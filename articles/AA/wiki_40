<doc id="5371" url="https://en.wikipedia.org/wiki?curid=5371" title="Concrete">
Concrete

Concrete is a composite material composed of fine and coarse aggregate bonded together with a fluid cement (cement paste) that hardens over time—most frequently in the past a lime-based cement binder, such as lime putty, but sometimes with other hydraulic cements, such as a calcium aluminate cement or with Portland cement to form Portland cement concrete (for its visual resemblance to Portland stone). Many other non-cementitious types of concrete exist with different methods of binding aggregate together, including asphalt concrete with a bitumen binder, which is frequently used for road surfaces, and polymer concretes that use polymers as a binder.

When aggregate is mixed with dry Portland cement and water, the mixture forms a fluid slurry that is easily poured and molded into shape. The cement reacts with the water and other ingredients to form a hard matrix that binds the materials together into a durable stone-like material that has many uses. Often, additives (such as pozzolans or superplasticizers) are included in the mixture to improve the physical properties of the wet mix or the finished material. Most concrete is poured with reinforcing materials (such as rebar) embedded to provide tensile strength, yielding reinforced concrete.

Concrete is one of the most frequently used building materials. Its usage worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminum combined. Globally, the ready-mix concrete industry, the largest segment of the concrete market, is projected to exceed $600 billion in revenue by 2025.

The word concrete comes from the Latin word ""concretus"" (meaning compact or condensed), the perfect passive participle of ""concrescere"", from ""con"-" (together) and ""crescere"" (to grow).

Small-scale production of concrete-like materials was pioneered by the Nabatean traders who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan from the 4th century BC. They discovered the advantages of hydraulic lime, with some self-cementing properties, by 700 BC. They built kilns to supply mortar for the construction of rubble masonry houses, concrete floors, and underground waterproof cisterns. They kept the cisterns secret as these enabled the Nabataeans to thrive in the desert. Some of these structures survive to this day.

In the Ancient Egyptian and later Roman eras, builders discovered that adding volcanic ash to the mix allowed it to set underwater.

Concrete floors were found in the royal palace of Tiryns, Greece, which dates roughly to 1400–1200 BC. Lime mortars were used in Greece, Crete, and Cyprus in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of waterproof concrete. Concrete was used for construction in many ancient structures.

The Romans used concrete extensively from 300 BC to 476 AD. During the Roman Empire, Roman concrete (or "opus caementicium") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman architectural revolution, freed Roman construction from the restrictions of stone and brick materials. It enabled revolutionary new designs in terms of both structural complexity and dimension. The Colosseum in Rome was built largely of concrete, and the concrete dome of the Pantheon is the world's largest unreinforced concrete dome. 

Concrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.

Modern tests show that "opus caementicium" had as much compressive strength as modern Portland-cement concrete (ca. ). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application was also different:

Modern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.

The long-term durability of Roman concrete structures has been found to be due to its use of pyroclastic (volcanic) rock and ash, whereby crystallization of strätlingite (a specific and complex calcium aluminosilicate hydrate) and the coalescence of this and similar calcium–aluminum-silicate–hydrate cementing binders helped give the concrete a greater degree of fracture resistance even in seismically active environments. Roman concrete is significantly more resistant to erosion by seawater than modern concrete; it used pyroclastic materials which react with seawater to form Al-tobermorite crystals over time.

The widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges, such as the magnificent Pont du Gard in southern France, have masonry cladding on a concrete core, as does the dome of the Pantheon.

After the Roman Empire collapsed, use of concrete became rare until the technology was redeveloped in the mid-18th century. Worldwide, concrete has overtaken steel in tonnage of material used.

After the Roman Empire, the use of burned lime and pozzolana was greatly reduced. Low kiln temperatures in the burning of lime, lack of pozzolana and poor mixing all contributed to a decline in the quality of concrete and mortar. From the 11th century, the increased use of stone in church and castle construction led to an increased demand for mortar. Quality began to improve in the 12th century through better grinding and sieving. Medieval lime mortars and concretes were non-hydraulic and were used for binding masonry, "hearting" (binding rubble masonry cores) and foundations. Bartholomaeus Anglicus in his "De proprietatibus rerum" (1240) describes the making of mortar. In an English translation of 1397, it reads "lyme ... is a stone brent; by medlynge thereof with sonde and water sement is made". From the 14th century the quality of mortar is again excellent, but only from the 17th century is pozzolana commonly added.

The "Canal du Midi" was built using concrete in 1670.

Perhaps the greatest step forward in the modern use of concrete was Smeaton's Tower, built by British engineer John Smeaton in Devon, England, between 1756 and 1759. This third Eddystone Lighthouse pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.

A method for producing Portland cement was developed in England and patented by Joseph Aspdin in 1824. Aspdin chose the name for its similarity to Portland stone, which was quarried on the Isle of Portland in Dorset, England. His son William continued developments into the 1840s, earning him recognition for the development of "modern" Portland cement.

Reinforced concrete was invented in 1849 by Joseph Monier. and the first house was built by François Coignet in 1853.
The first concrete reinforced bridge was designed and built by Joseph Monier in 1875.

Concrete is a composite material, comprising a matrix of aggregate (typically a rocky material) and a binder (typically Portland cement or asphalt), which holds the matrix together. Many types of concrete are available, determined by the formulations of binders and the types of aggregate used to suit the application for the material. These variables determine strength, density, as well as chemical and thermal resistance of the finished product.

Aggregate consists of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.

A cement, most commonly Portland cement, is the most prevalent kind of concrete binder. For cementitious binders, water is mixed with the dry powder and aggregate, which produces a semi-liquid slurry that can be shaped, typically by pouring it into a form. The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust stone-like material. Other cementitious materials, such as fly ash and slag cement, are sometimes added—either pre-blended with the cement or directly as a concrete component—and become a part of the binder for the aggregate. Admixtures are added to modify the cure rate or properties of the material.

Mineral admixtures use recycled materials as concrete ingredients. Conspicuous materials include fly ash, a by-product of coal-fired power plants; ground granulated blast furnace slag, a byproduct of steelmaking; and silica fume, a byproduct of industrial electric arc furnaces.

Structures employing Portland cement concrete usually include steel reinforcement because this type of concrete can be formulated with high compressive strength, but always has lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension, typically steel rebar.

Other materials can also be used as a concrete binder, the most prevalent alternative is asphalt, which is used as the binder in asphalt concrete.

The "mix design" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.

Portland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar and many plasters. British masonry worker Joseph Aspdin patented Portland cement in 1824. It was named because of the similarity of its color to Portland limestone, quarried from the English Isle of Portland and used extensively in London architecture. It consists of a mixture of calcium silicates (alite, belite), aluminates and ferrites—compounds which combine calcium, silicon, aluminum and iron in forms which will react with water. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay or shale (a source of silicon, aluminum and iron) and grinding this product (called "clinker") with a source of sulfate (most commonly gypsum).

In modern cement kilns many advanced features are used to lower the fuel consumption per ton of clinker produced. Cement kilns are extremely large, complex, and inherently dusty industrial installations, and have emissions which must be controlled. Of the various ingredients used to produce a given quantity of concrete, the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels.

Combining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.

As stated by Abrams' law, a lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. Impure water used to make concrete can cause problems when setting or in causing premature failure of the structure.

Hydration involves many different reactions, often occurring at the same time. As the reactions proceed, the products of the cement hydration process gradually bond together the individual sand and gravel particles and other components of the concrete to form a solid mass.

Reaction:

Fine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements for natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.

The size distribution of the aggregate determines how much binder is required. Aggregate with a very even size distribution has the biggest gaps whereas adding aggregate with smaller particles tends to fill these gaps. The binder must fill the gaps between the aggregate as well as paste the surfaces of the aggregate together, and is typically the most expensive component. Thus, variation in sizes of the aggregate reduces the cost of concrete. The aggregate is nearly always stronger than the binder, so its use does not negatively affect the strength of the concrete.

Redistribution of aggregates after compaction often creates inhomogeneity due to the influence of vibration. This can lead to strength gradients.

Decorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative "exposed aggregate" finish, popular among landscape designers.

Concrete is strong in compression, as the aggregate efficiently carries the compression load. However, it is weak in tension as the cement holding the aggregate in place can crack, allowing the structure to fail. Reinforced concrete adds either steel reinforcing bars, steel fibers, aramid fibers, carbon fibers, glass fibers, or plastic fibers to carry tensile loads.

Admixtures are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. Admixtures are defined as additions "made as the concrete mix is being prepared". The most common admixtures are retarders and accelerators. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See below.) The common types of admixtures are as follows:

Inorganic materials that have pozzolanic or latent hydraulic properties, these very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. This development is due to cement production being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions, as well as lowering costs, improving concrete properties, and recycling wastes.


Concrete production is the process of mixing together the various ingredients—water, aggregate, cement, and any additives—to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant.

In general usage, concrete plants come in two main types, ready mix plants and central mix plants. A ready-mix plant mixes all the ingredients except water, while a central mix plant mixes all the ingredients including water. A central-mix plant offers more accurate control of the concrete quality through better measurements of the amount of water added, but must be placed closer to the work site where the concrete will be used, since hydration begins at the plant.

A concrete plant consists of large storage hoppers for various reactive ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.

Modern concrete is usually prepared as a viscous fluid, so that it may be poured into forms, which are containers erected in the field to give the concrete its desired shape. Concrete formwork can be prepared in several ways, such as slip forming and steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture precast concrete products.

A wide variety of equipment is used for processing concrete, from hand tools to heavy industrial machinery. Whichever equipment builders use, however, the objective is to produce the desired building material; ingredients must be properly mixed, placed, shaped, and retained within time constraints. Any interruption in pouring the concrete can cause the initially placed material to begin to set before the next batch is added on top. This creates a horizontal plane of weakness called a "cold joint" between the two batches. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.

Thorough mixing is essential to produce uniform, high-quality concrete.

Concrete Mixes are primarily divided into two types, "nominal mix" and "design mix":

Nominal Mix ratios are given in volume of formula_1. Nominal mixes are a simple, fast way of getting a basic idea of the properties of the finished concrete without having to perform testing in advance.

Various governing bodies (such as British Standards) define nominal mix ratios into a number of grades, usually ranging from lower compressive strength to higher compressive strength. The grades usually indicate the 28-day cube strength.
For example, in Indian standards, the mixes of grades M10, M15, M20 and M25 correspond approximately to the mix proportions (1:3:6), (1:2:4), (1:1.5:3) and (1:1:2) respectively.

Design mix ratios are decided by an engineer after analyzing the properties of the specific ingredients being used. Instead of using a 'nominal mix' of 1 part cement, 2 parts sand, and 4 parts aggregate (the second example from above), a civil engineer will custom-design a concrete mix to exactly meet the requirements of the site and conditions, setting material ratios and often designing an admixture package to fine-tune the properties or increase the performance envelope of the mix. Design-mix concrete can have very broad specifications that cannot be met with more basic nominal mixes, but the involvement of the engineer often increases the cost of the concrete mix.

Workability is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (pouring, pumping, spreading, tamping, vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. The use of an aggregate blend with an undesirable gradation can result in a very harsh mix design with a very low slump, which cannot readily be made more workable by addition of reasonable amounts of water. An undesirable gradation can mean using a large aggregate that is too large for the size of the formwork, or which has too few smaller aggregate grades to serve to fill the gaps between the larger grades, or using too little or too much sand for the same reason, or using too little water, or too much cement, or even using jagged crushed stone instead of smoother round aggregate such as pebbles. Any combination of these factors and others may result in a mix which is too harsh, i.e., which does not flow or spread out smoothly, is difficult to get into the formwork, and which is difficult to surface finish.

Workability can be measured by the concrete slump test, a simple measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an "Abrams cone" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50 mm) out of one foot (305 mm). A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.

Slump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.

High-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.

After mixing, concrete is a fluid and can be pumped to the location where needed.

Concrete must be kept moist during curing in order to achieve optimal strength and durability. During curing hydration occurs, allowing calcium-silicate hydrate (C-S-H) to form. Over 90% of a mix's final strength is typically reached within four weeks, with the remaining 10% achieved over years or even decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO over several decades further strengthens the concrete and makes it more resistant to damage. This carbonation reaction, however, lowers the pH of the cement pore solution and can corrode the reinforcement bars.

Hydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation. Addition of short-cut polymer fibers can improve (reduce) shrinkage-induced stresses during curing and increase early and ultimate compression strength.

Properly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause scaling, reduced strength, poor abrasion resistance and cracking.

During the curing period, concrete is ideally maintained at controlled temperature and humidity. To ensure full hydration during curing, concrete slabs are often sprayed with "curing compounds" that create a water-retaining film over the concrete. Typical films are made of wax or related hydrophobic compounds. After the concrete is sufficiently cured, the film is allowed to abrade from the concrete through normal use.

Traditional conditions for curing involve by spraying or ponding the concrete surface with water. The adjacent picture shows one of many ways to achieve this, ponding—submerging setting concrete in water and wrapping in plastic to prevent dehydration. Additional common curing methods include wet burlap and plastic sheeting covering the fresh concrete.

For higher-strength applications, accelerated curing techniques may be applied to the concrete. A common technique involves heating the poured concrete with steam, which serves to both keep it damp and raise the temperature, so that the hydration process proceeds more quickly and more thoroughly.

"Asphalt concrete" (commonly called "asphalt", "blacktop", or "pavement" in North America, and "tarmac", "bitumen macadam", or "rolled asphalt" in the United Kingdom and the Republic of Ireland) is a composite material commonly used to surface roads, parking lots, airports, as well as the core of embankment dams. Asphalt mixtures have been used in pavement construction since the beginning of the twentieth century. It consists of mineral aggregate bound together with asphalt, laid in layers, and compacted. The process was refined and enhanced by Belgian inventor and U.S. immigrant Edward De Smedt.

The terms "asphalt" (or "asphaltic") "concrete", "bituminous asphalt concrete", and "bituminous mixture" are typically used only in engineering and construction documents, which define concrete as any composite material composed of mineral aggregate adhered with a binder. The abbreviation, "AC", is sometimes used for "asphalt concrete" but can also denote "asphalt content" or "asphalt cement", referring to the liquid asphalt portion of the composite material.

Pervious concrete is a mix of specially graded coarse aggregate, cement, water and little-to-no fine aggregates. This concrete is also known as "no-fines" or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totaling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze–thaw climates to minimize
the possibility of frost damage. Pervious concrete also permits rainwater to filter through roads and parking lots, to recharge aquifers, instead of contributing to runoff and flooding.

Nanoconcrete (also spelled "nano concrete"' or "nano-concrete") is a class of materials that contains Portland cement particles that are no greater than 100 μm and particles of silica no greater than 500 μm, which fill voids that would otherwise occur in normal concrete, thereby substantially increasing the material's strength. It is widely used in foot and highway bridges where high flexural and compressive strength are indicated.

Bacteria such as "Bacillus pasteurii", "Bacillus pseudofirmus", "Bacillus cohnii", "Sporosarcina pasteuri", and "Arthrobacter crystallopoietes" increase the compression strength of concrete through their biomass. Not all bacteria increase the strength of concrete significantly with their biomass. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. "Sporosarcina pasteurii" reduces water and chloride permeability. "B. pasteurii" increases resistance to acid. "Bacillus pasteurii" and "B. sphaericuscan" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.

Polymer concretes are mixtures of aggregate and any of various polymers and may be reinforced. The cement is costlier than lime-based cements, but polymer concretes nevertheless have advantages; they have significant tensile strength even without reinforcement, and they are largely impervious to water. Polymer concretes are frequently used for repair and construction of other applications, such as drains.

Grinding of concrete can produce hazardous dust. Exposure to cement dust can lead to issues such as silicosis, kidney disease, skin irritation and similar effects. The U.S. National Institute for Occupational Safety and Health in the United States recommends attaching local exhaust ventilation shrouds to electric concrete grinders to control the spread of this dust. In addition, the Occupational Safety and Health Administration (OSHA) has placed more stringent regulations on companies whose workers regularly come into contact with silica dust. An updated silica rule, which OSHA put into effect 23 September 2017 for construction companies, restricted the amount of respirable crystalline silica workers could legally come into contact with to 50 micrograms per cubic meter of air per 8-hour workday. That same rule went into effect 23 June 2018 for general industry, hydraulic fracturing and maritime. That the deadline was extended to 23 June 2021 for engineering controls in the hydraulic fracturing industry. Companies which fail to meet the tightened safety regulations can face financial charges and extensive penalties.

Concrete has relatively high compressive strength, but much lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.

Tests can be performed to ensure that the properties of concrete correspond to specifications for the application.
Different mixes of concrete ingredients produce different strengths. Concrete strength values are usually specified as the lower-bound compressive strength of either a cylindrical or cubic specimen as determined by standard test procedures.

Different strengths of concrete are used for different purposes. Very low-strength— or less—concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, to concrete is often used. concrete is readily commercially available as a more durable, although more expensive, option. Higher-strength concrete is often used for larger civil projects. Strengths above are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of or more, to keep the size of the columns small. Bridges may use long beams of high-strength concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as have been used commercially for these reasons.

Concrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other artificial material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.

Due to cement's exothermic chemical reaction while setting up, large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during hydration and associated expansion. To mitigate these effects, "post-cooling" is commonly applied during construction. An early example at Hoover Dam used a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.

Another approach to mass concrete structures that minimizes cement's thermal byproduct is the use of roller-compacted concrete, which uses a dry mix which has a much lower cooling requirement than conventional wet placement. It is deposited in thick layers as a semi-dry material then roller compacted into a dense, strong mass.

Advantage and Disadvantage of Concrete

Raw concrete surfaces tend to be porous and have a relatively uninteresting appearance. Many different finishes can be applied to improve the appearance and preserve the surface against staining, water penetration, and freezing.

Examples of improved appearance include stamped concrete where the wet concrete has a pattern impressed on the surface, to give a paved, cobbled or brick-like effect, and may be accompanied with coloration. Another popular effect for flooring and table tops is polished concrete where the concrete is polished optically flat with diamond abrasives and sealed with polymers or other sealants.

Other finishes can be achieved with chiseling, or more conventional techniques such as painting or covering it with other materials.

The proper treatment of the surface of concrete, and therefore its characteristics, is an important stage in the construction and renovation of architectural structures.

Prestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose tensile stresses experienced in use. This can greatly reduce the weight of beams or slabs, by
better distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.
In pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.

More than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. See Brutalism.

Extreme weather conditions (extreme heat or cold; windy condition, and humidity variations) can significantly alter the quality of concrete. Many precautions are observed in cold weather placement. Low temperatures significantly slow the chemical reactions involved in hydration of cement, thus affecting the strength development. Preventing freezing is the most important precaution, as formation of ice crystals can cause damage to the crystalline structure of the hydrated cement paste. If the surface of the concrete pour is insulated from the outside temperatures, the heat of hydration will prevent freezing.

The American Concrete Institute (ACI) definition of cold weather placement, ACI 306, is:
In Canada, where temperatures tend to be much lower during the cold season, the following criteria are used by CSA A23.1:

The minimum strength before exposing concrete to extreme cold is 500 psi (3.5 MPa). CSA A 23.1 specified a compressive strength of 7.0 MPa to be considered safe for exposure to freezing.

Concrete roads are more fuel efficient to drive on, more reflective and last significantly longer than other paving surfaces, yet have a much smaller market share than other paving solutions. Modern-paving methods and design practices have changed the economics of concrete paving, so that a well-designed and placed concrete pavement will be less expensive on initial costs and significantly less expensive over the life cycle. Another major benefit is that pervious concrete can be used, which eliminates the need to place storm drains near the road, and reducing the need for slightly sloped roadway to help rainwater to run off. No longer requiring discarding rainwater through use of drains also means that less electricity is needed (more pumping is otherwise needed in the water-distribution system), and no rainwater gets polluted as it no longer mixes with polluted water. Rather, it is immediately absorbed by the ground.

Energy requirements for transportation of concrete are low because it is produced locally from local resources, typically manufactured within 100 kilometers of the job site. Similarly, relatively little energy is used in producing and combining the raw materials (although large amounts of CO are produced by the chemical reactions in cement manufacture). The overall embodied energy of concrete at roughly 1 to 1.5 megajoules per kilogram is therefore lower than for most structural and construction materials.

Once in place, concrete offers great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.

Concrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.

Options for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.

Concrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However, this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is required to resist more extreme forces.

As discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).

Concrete can be damaged by many processes, such as the expansion of corrosion products of the steel reinforcement bars, freezing of trapped water, fire or radiant heat, aggregate expansion, sea water effects, bacterial corrosion, leaching, erosion by fast-flowing water, physical damage and chemical damage (from carbonatation, chlorides, sulfates and distillate water). The micro fungi Aspergillus Alternaria and Cladosporium were able to grow on samples of concrete used as a radioactive waste barrier in the Chernobyl reactor; leaching aluminum, iron, calcium, and silicon.

The manufacture and use of concrete produce a wide range of environmental and social consequences. Some are harmful, some welcome, and some both, depending on circumstances.

A major component of concrete is cement, which similarly exerts environmental and social effects. The cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas (the other two being the energy production and transportation industries). Every tonne of cement produced releases one tonne of CO into the atmosphere. As of 2019, the production of Portland cement contributed eight percent to global anthropogenic CO emissions, largely due to the sintering of limestone and clay at . Researchers have suggested a number of approaches to improving carbon sequestration relevant to concrete production. In August 2019, a reduced CO cement was announced which "reduces the overall carbon footprint in precast concrete by 70%."

Concrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution.

Concrete is a contributor to the urban heat island effect, though less so than asphalt.

Workers who cut, grind or polish concrete are at risk of inhaling airborne silica, which can lead to silicosis. This includes crew members who work in concrete chipping. 
The presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity.
Fresh concrete (before curing is complete) is highly alkaline and must be handled with proper protective equipment.
Concrete recycling is an increasingly common method for disposing of concrete structures. Concrete debris was once routinely shipped to landfills for disposal, but recycling is increasing due to improved environmental awareness, governmental laws and economic benefits.

The world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.

The world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .

The Polavaram dam works in Andhra Pradesh on 6 January 2019 entered the Guinness World Records by pouring 32,100 cubic metres of concrete in 24 hours. The world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.

The world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed in 30 hours, finished to a flatness tolerance of F 54.60 and a levelness tolerance of F 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.

The record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in 58.5 hours using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill & Monolith Project to be completed in the dry.


</doc>
<doc id="5373" url="https://en.wikipedia.org/wiki?curid=5373" title="Coitus interruptus">
Coitus interruptus

Coitus interruptus, also known as withdrawal or the pull-out method, is a method of birth control in which a man, during sexual intercourse, withdraws his penis from a woman's vagina prior to orgasm (and ejaculation) and then directs his ejaculate (semen) away from the vagina in an effort to avoid insemination.

This method of contraception, widely used for at least two millennia, is still in use today. This method was used by an estimated 38 million couples worldwide in 1991. "Coitus interruptus" does not protect against sexually transmitted infections (STIs/STDs).

Perhaps the oldest documentation of the use of the withdrawal method to avoid pregnancy is the story of Onan in the Torah and the Bible. This text is believed to have been written down over 2,500 years ago. Societies in the ancient civilizations of Greece and Rome preferred small families and are known to have practiced a variety of birth control methods. There are references that have led historians to believe withdrawal was sometimes used as birth control. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets).

After the decline of the Roman Empire in the 5th century AD, contraceptive practices fell out of use in Europe; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If withdrawal was used during the Roman Empire, knowledge of the practice may have been lost during its decline.

From the 18th century until the development of modern methods, withdrawal was one of the most popular methods of birth-control in Europe, North America, and elsewhere.

Like many methods of birth control, reliable effect is achieved only by correct and consistent use. Observed failure rates of withdrawal vary depending on the population being studied: studies have found actual failure rates of 15–28% per year. In comparison, the pill has an actual use failure rate of 2–8%, while intrauterine devices (IUDs) have an actual use failure rate of 0.8%. Condoms have an actual use failure rate of 10–18%. However, some authors suggest that actual effectiveness of withdrawal could be similar to effectiveness of condoms, and this area needs further research. (See Comparison of birth control methods.)

For couples that use "coitus interruptus" correctly at every act of intercourse, the failure rate is 4% per year. In comparison, the pill has a perfect-use failure rate of 0.3%, IUDs a rate of 0.6%, and condoms a rate of 2%.

It has been suggested that the pre-ejaculate ("Cowper's fluid") emitted by the penis prior to ejaculation normally contains spermatozoa (sperm cells), which would compromise the effectiveness of the method. However, several small studies have failed to find any viable sperm in the fluid. While no large conclusive studies have been done, it is believed by some that the cause of method (correct-use) failure is the pre-ejaculate fluid picking up sperm from a previous ejaculation. For this reason, it is recommended that the male partner urinate between ejaculations, to clear the urethra of sperm, and wash any ejaculate from objects that might come near the woman's vulva (e.g. hands and penis).

However, recent research suggests that this might not be accurate. A contrary, yet non-generalizable study that found mixed evidence, including individual cases of a high sperm concentration, was published in March 2011. A noted limitation to these previous studies' findings is that pre-ejaculate samples were analyzed after the critical two-minute point. That is, looking for motile sperm in small amounts of pre-ejaculate via microscope after two minutes – when the sample has most likely dried – makes examination and evaluation "extremely difficult". Thus, in March 2011 a team of researchers assembled 27 male volunteers and analyzed their pre-ejaculate samples within two minutes after producing them. The researchers found that 11 of the 27 men (41%) produced pre-ejaculatory samples that contained sperm, and 10 of these samples (37%) contained a "fair amount" of motile sperm (i.e. as few as 1 million to as many as 35 million).
This study therefore recommends, in order to minimise unintended pregnancy and disease transmission, the use of condoms from the first moment of genital contact.
As a point of reference, a study showed that, of couples who conceived within a year of trying, only 2.5% included a male partner with a total sperm count (per ejaculate) of 23 million sperm or less.
However, across a wide range of observed values, total sperm count (as with other identified semen and sperm characteristics) has weak power to predict which couples are at risk of pregnancy.

It is widely believed that urinating after an ejaculation will flush the urethra of remaining sperm. However, some of the subjects in the March 2011 study who produced sperm in their pre-ejaculate did urinate (sometimes more than once) before producing their sample. Therefore, some males can release the pre-ejaculate fluid containing sperm without a previous ejaculation.

The advantage of "coitus interruptus" is that it can be used by people who have objections to, or do not have access to, other forms of contraception. Some persons prefer it so they can avoid possible adverse effects of hormonal contraceptives or so that they can have a full experience and be able to "feel" their partner. Other reasons for the popularity of this method are it has no direct monetary cost, requires no artificial devices, has no physical side effects, can be practiced without a prescription or medical consultation, and provides no barriers to stimulation.

Compared to the other common reversible methods of contraception such as IUDs, hormonal contraceptives, and male condoms, "coitus interruptus" is less effective at preventing pregnancy. As a result, it is also less cost-effective than many more effective methods: although the method itself has no direct cost, users have a greater chance of incurring the risks and expenses of either child-birth or abortion. Only models that assume all couples practice perfect use of the method find cost savings associated with the choice of withdrawal as a birth control method.

The method is largely ineffective in the prevention of sexually transmitted infections (STIs/STDs), like HIV, since pre-ejaculate may carry viral particles or bacteria which may infect the partner if this fluid comes in contact with mucous membranes. However, a reduction in the volume of bodily fluids exchanged during intercourse may reduce the likelihood of disease transmission compared to using no method due to the smaller number of pathogens present.

Based on data from surveys conducted during the late 1990s, 3% of women of childbearing age worldwide rely on withdrawal as their primary method of contraception. Regional popularity of the method varies widely, from a low of 1% in Africa to 16% in Western Asia.

In the United States, a 2002 survey indicated 56% of women of reproductive age have had a partner use withdrawal, but only 2.5% were using withdrawal as their primary method of contraception.




</doc>
<doc id="5374" url="https://en.wikipedia.org/wiki?curid=5374" title="Condom">
Condom

A condom is a sheath-shaped barrier device used during sexual intercourse to reduce the probability of pregnancy or a sexually transmitted infection (STI). There are both male and female condoms. With proper use—and use at every act of intercourse—women whose partners use male condoms experience a 2% per-year pregnancy rate. With typical use the rate of pregnancy is 18% per-year. Their use greatly decreases the risk of gonorrhea, chlamydia, trichomoniasis, hepatitis B, and HIV/AIDS. They also to a lesser extent protect against genital herpes, human papillomavirus (HPV), and syphilis.
The male condom is rolled onto an erect penis before intercourse and works by forming a physical barrier which blocks semen from entering the body of a sexual partner. Male condoms are typically made from latex and, less commonly, from polyurethane, polyisoprene, or lamb intestine. Male condoms have the advantages of ease of use, easy to access, and few side effects. In those with a latex allergy a polyurethane or other synthetic version should be used. Female condoms are typically made from polyurethane and may be used multiple times.
Condoms as a method of preventing STIs have been used since at least 1564. Rubber condoms became available in 1855, followed by latex condoms in the 1920s. It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system. The wholesale cost in the developing world is about 0.03 to US$0.08 each. In the United States condoms usually cost less than US$1.00. Globally less than 10% of those using birth control are using the condom. Rates of condom use are higher in the developed world. In the United Kingdom the condom is the second most common method of birth control (22%) while in the United States it is the third most common (15%). About six to nine billion are sold a year.

The effectiveness of condoms, as of most forms of contraception, can be assessed two ways. "Perfect use" or "method" effectiveness rates only include people who use condoms properly and consistently. "Actual use", or "typical use" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.

The typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.

Condoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STD protection is also desired.

According to a 2000 report by the National Institutes of Health (NIH), consistent use of latex condoms reduces the risk of HIV/AIDS transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branch and the World Health Organization found similar risk reductions of 80–95%.

The 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus (HPV) to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2 also known as genital herpes, in both men and women.

Although a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases like HPV and herpes may be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STDs, however, is inconsistent use.

Condoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.

Condoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1–3% of women will test positive for semen residue after intercourse with a condom.

"Double bagging", using two condoms at once, is often believed to cause a higher rate of failure due to the friction of rubber on rubber. This claim is not supported by research. The limited studies that have been done found that the simultaneous use of multiple condoms decreases the risk of condom breakage.

Different modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins – such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.

Standard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer "snug" or "magnum" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.

It is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms "for greater durability, sensation, and comfort", but others warn that "the thinner the condom, the smaller the force required to break it".

Experienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure. An article in "Population Reports" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.

Among people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or simply dislike the feel of condoms and decide to "take a chance". This type of behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).

Another possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact on their effectiveness. Cases of such condom sabotage have occurred.

The use of latex condoms by people with an allergy to latex can cause allergic symptoms, such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause the development of a latex allergy in some people. Irritation may also occur due to spermicides that may be present.

Male condoms are usually packaged inside a foil or plastic wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that some space be left in the tip of the condom so that semen has a place to collect; otherwise it may be forced out of the base of the device. Most condoms have a teat end for this purpose. After use, it is recommended the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle. Condoms are used to reduce the likelihood of pregnancy during intercourse and to reduce the likelihood of contracting sexually-transmitted infections (STIs). Condoms are also used during fellatio to reduce the likelihood of contracting STIs.

Some couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.

In 2012 proponents gathered 372,000 voter signatures through a citizens' initiative in Los Angeles County to put Measure B on the 2012 ballot. As a result, Measure B, a law requiring the use of condoms in the production of pornographic films, was passed. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.

Condoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted diseases when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying ""comprehensive sexuality education programs... discuss the appropriate use of condoms"", and ""promote condom use for those who are sexually active"."

In the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.

Common procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special "collection condom" to collect semen during sexual intercourse.

Collection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Many men prefer collection condoms to masturbation, and some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.

For fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.

"Condom therapy" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.

Condoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.

Ongoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.

Condoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.

Medically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.

Condoms have also been used to protect scientific samples from the environment, and to waterproof microphones for underwater recording.

Most condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes, from snug to larger, and shapes. Width often varies from 49 mm to 56 mm. Sizes from 45 mm to 60 mm, however exist.

They also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.

Male condoms have a tight ring to form a seal around the penis while female condoms usually have a large stiff ring to prevent them from slipping into the body orifice. The Female Health Company produced a female condom that was initially made of polyurethane, but newer versions are made of nitrile. Medtech Products produces a female condom made of latex.

Latex has outstanding elastic properties: Its tensile strength exceeds 30 MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electric current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.

While the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009 the U.S. Food and Drug Administration granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.

The most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most polyisoprene.

Polyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04 mm and 0.07 mm thick.

Polyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.

However, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.

Polyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.

Condoms made from sheep intestines, labeled "lambskin", are also available. Although they are generally effective as a contraceptive by blocking sperm, it is presumed that they are likely less effective than latex in preventing the transmission of Sexually transmitted infections, because of pores in the material. This is based on the idea that intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses—such as HIV, herpes, and genital warts—are small enough to pass. However, there are to date no clinical data confirming or denying this theory. 

As a result of laboratory data on condom porosity, in 1989 the US Food and Drug Administration began requiring lambskin condom manufacturers to indicate that the products were not to be used for the prevention of sexually transmitted infections. This was based on the presumption that lambskin condoms would be less effective than latex in preventing HIV transmission, rather than a conclusion that lambskin condoms lack efficacy in STI prevention altogether. An FDA publication in 1992 states that lambskin condoms "...provide good birth control and a varying degree of protection against some, but not all, sexually transmitted diseases," and that the labelling requirement was decided upon because the FDA "...cannot expect people to know which STDs they need to be protected against," and since "the reality is that you don't know what your partner has, we wanted natural-membrane condoms to have labels that don't allow the user to assume they're effective against the small viral STDs."

Some believe that lambskin condoms provide a more "natural" sensation, and they lack the allergens that are inherent to latex, but because of their lesser protection against infection, other hypoallergenic materials such as polyurethane are recommended for latex-allergic users and/or partners. Lambskin condoms are also significantly more expensive than other types and as slaughter by-products they are also not vegetarian.

Some latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to Consumer Reports, condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary-tract infections in women. In contrast, application of separately packaged spermicide "is" believed to increase the contraceptive efficacy of condoms.

Nonoxynol-9 was once believed to offer additional protection against STDs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all. , nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.

Textured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the g-spot or frenulum. Many textured condoms which advertise "mutual pleasure" also are bulb-shaped at the top, to provide extra stimulation to the penis. Some women experience irritation during vaginal intercourse with studded condoms.

The anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.

A collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.

Some condom-like devices are intended for entertainment only, such as glow-in-the dark condoms. These novelty condoms may not provide protection against pregnancy and STDs.

The prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6–8% of married contraceptive users choose condoms.

Whether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, they were made of tortoise shell or animal horn.

In 16th-century Italy, anatomist and physician Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.

After this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication "De iustitia et iure" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of "condons", the first documented use of that word (or any similar spelling). (Other early spellings include "condam" and "quondam", from which the Italian derivation "guantone" has been suggested, from "guanto", "a glove.")
In addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from "fine leather" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.
Casanova in the 18th century was one of the first reported using "assurance caps" to prevent impregnating his mistresses.

From at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.

Despite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or "skin" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.

The early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other methods of birth control to the condom. By the late 19th century many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods which were controlled by women, such as diaphragms and spermicidal douches. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes, and often fell off or broke), but they discussed condoms as a good option for some, and as the only contraceptive that also protected from disease.

Many countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.
Beginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include effects of the American Civil War, and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught that abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma against victims of these diseases was so great that many hospitals refused to treat people who had syphilis.
The German military was the first to promote condom use among its soldiers, beginning in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.

In the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control on the grounds that their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all "unnatural means of conception avoidance". London's Bishop Arthur Winnington-Ingram complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.

However, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.

In 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844. The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original "capote" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.

For many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish-born inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called "cement dipping", this method required adding gasoline or benzene to the rubber to make it liquid. Latex, rubber suspended in water, was invented in 1920. Latex condoms required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).

Until the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.

In 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical "Casti connubii" affirming its opposition to all contraceptives, a stance it has never reversed. In the 1930s, legal restrictions on condoms began to be relaxed. But during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.

Throughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day. After the war, condom sales continued to grow. From 1955–1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950–1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 début, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the "world population crises": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)

In the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television: this policy remained in place until 1979.

After it was discovered in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.

Due to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Wal-Mart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called "prevention fatigue" or "condom fatigue". Observers have cited condom fatigue in both Europe and North America. As one response, manufacturers have changed the tone of their advertisements from scary to humorous.

New developments continued to occur in the condom market, with the first polyurethane condom—branded Avanti and produced by the manufacturer of Durex—introduced in the 1990s. Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. , condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on September 17, 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).

The term "condom" first appears in the early 18th century. Its etymology is unknown.
In popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one "Dr. Condom" or "Earl of Condom". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II ascended to the throne.

A variety of unproven Latin etymologies have been proposed, including "condon" (receptacle), "condamina" (house), and "cumdum" (scabbard or case). It has also been speculated to be from the Italian word "guantone", derived from "guanto", meaning glove. William E. Kruck wrote an article in 1981 concluding that, ""As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology."" Modern dictionaries may also list the etymology as "unknown".
Other terms are also commonly used to describe condoms. In North America condoms are also commonly known as "prophylactics", or "rubbers". In Britain they may be called "French letters". Additionally, condoms may be referred to using the manufacturer's name.

Some moral and scientific criticism of condoms exists despite the many benefits of condoms agreed on by scientific consensus and sexual health experts.

Condom usage is typically recommended for new couples who have yet to develop full trust in their partner with regard to STDs. Established couples on the other hand have few concerns about STDs, and can use other methods of birth control such as the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, who have different kinds of sex and have different risk consequences and factors.

Among the prime objections to condom usage is the blocking of erotic sensation, or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: "bareback") claim that the condom puts a barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.

The Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).

The use of condoms to prevent STI transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view—including all statements from the Vatican—is that condom-promotion programs encourage promiscuity, thereby actually increasing STI transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.

The Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.

In a November 2011 interview, Pope Benedict XVI discussed for the first time the use of condoms to prevent STI transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.

More generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Talc is known to be toxic if it enters the abdominal cavity (i.e., via the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.

Nitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.

In addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.

While biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.

In much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.

Cultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.

As an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the "Journal of Sex Health Research" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.

A telephone survey conducted by Rand Corporation and Oregon State University, and published in the "Journal of Acquired Immune Deficiency Syndromes" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.

In the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to "wasting" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an "elixir" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are "for prostitutes" and that respectable women should not use them. A few clerics even promote the lie that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.

In March 2013, technology mogul Bill Gates offered a US$100,000 grant through his foundation for a condom design that "significantly preserves or enhances pleasure" to encourage more males to adopt the use of condoms for safer sex. The grant information states: "The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?". The project has been named the "Next Generation Condom" and anyone who can provide a "testable hypothesis" is eligible to apply.

Middle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.

In 2017, India restricted TV advertisements for condoms to between the hours of 10PM to 6AM. Family planning advocates were against this, saying it was liable to "undo decades of progress on sexual and reproductive health".

One analyst described the size of the condom market as something that "boggles the mind". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.

A spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. , the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.

The Invisible Condom, developed at Université Laval in Quebec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. , the invisible condom is in the clinical trial phase, and has not yet been approved for use.

Also developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain his erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. , it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with "Excite Gel" which contains the amino acid l-arginine and is intended to improve the strength of the erectile response.



</doc>
<doc id="5375" url="https://en.wikipedia.org/wiki?curid=5375" title="Country code">
Country code

Country codes are short alphabetic or numeric geographical codes (geocodes) developed to represent countries and dependent areas, for use in data processing and communications. Several different systems have been developed to do this. The term "country code" frequently refers to ISO 3166-1 alpha-2 or international dialing codes, the E.164 country calling codes.

This standard defines for most of the countries and dependent areas in the world:

The two-letter codes are used as the basis for some other codes or applications, for example,
For more applications see ISO 3166-1 alpha-2.


The developers of ISO 3166 intended that in time it would replace other coding systems in existence.

The following can represent countries:

 -




</doc>
<doc id="5376" url="https://en.wikipedia.org/wiki?curid=5376" title="Cladistics">
Cladistics

Cladistics (, from Greek , "kládos", "branch") is an approach to biological classification in which organisms are categorized in groups ("clades") based on the most recent common ancestor. Hypothesized relationships are typically based on shared derived characteristics (synapomorphies")" that can be traced to the most recent common ancestor and are not present in more distant groups and ancestors. A key feature of a clade is that a common ancestor and all its descendants are part of the clade. Importantly, all descendants stay in their overarching ancestral clade. For example, if within a "strict" cladistic framework the terms "animals", "bilateria/worms", "fishes/vertebrata", or "monkeys/anthropoidea" were used, these terms would include humans. Many of these terms are normally used paraphyletically, outside of cladistics, e.g. as a 'grade'. Radiation results in the generation of new subclades by bifurcation, but in practice sexual hybridization may blur very closely related groupings.

The techniques and nomenclature of cladistics have been applied to disciplines other than biology. (See phylogenetic nomenclature.)

Cladistics is now the most commonly used method to classify organisms.

The original methods used in cladistic analysis and the school of taxonomy derived from the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); the terms "cladistics" and "clade" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.

What is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell for birds and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943.
The term "clade" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cuénot in 1940, "cladogenesis" in 1958, "cladistic" by Arthur Cain and Harrison in 1960, "cladist" (for an adherent of Hennig's school) by Ernst Mayr in 1965, and "cladistics" in 1966. Hennig referred to his own approach as "phylogenetic systematics". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to systematics with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal, and evolutionary taxonomy by Ernst Mayr.

Originally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for constructing phylogenies from morphological data.

In the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became popular in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.

The cladistic method interprets each character state transformation implied by the distribution of shared character states among taxa (or other terminals) as a potential piece of evidence for grouping. The outcome of a cladistic analysis is a cladogram – a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more "sophisticated" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified.

Every cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs), which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.

Until recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:

If this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the branch near the lived earlier than the last common ancestor of lizards and birds, near the . Most molecular evidence, however, produces cladograms more like this:

If this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms provide competing accounts of real events, at most one of them is correct.

The cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes and humans) are hypothesized to have had a common ancestor all of whose descendants were anthropoids, so they form the clade called Anthropoidea. The "prosimians", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the "prosimians" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.

The following terms, coined by Hennig, are used to identify shared or distinct character states among groups:

The terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.

It can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy, which may identify a monophyletic group, or whether it only appears to be the same and is thus a homoplasy, which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.

Mono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.

Cladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Transformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing phylogeny from cladistic analysis, but it has remained unpopular.

However, homology is usually determined from analysis of the results that are evaluated with homology measures, mainly the consistency index (CI) and retention index (RI), which, it has been claimed, makes the process objective. Also, homology can be equated to synapomorphy, which is what Patterson has done.

In organisms with sexual reproduction, incomplete lineage sorting may result in inconsistent phylogenetic trees, depending on which genes are assessed. It is also possible that multiple surviving lineages are generated while interbreeding is still significantly occurring (polytomy). Interbreeding is possible over periods of about 10 million years. Typically speciation occurs over only about 1 million years, which makes it less likely multiple long surviving lineages developed "simultaneously". Even so, interbreeding can result in a lineage being overwhelmed and absorbed by a related more numerous lineage.

The cladistic method does not identify fossils as actual ancestors. Instead, they are identified as separate extinct branches, which could be argued to be fine to take as the default position.

The comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.

Anthropology and archaeology: Cladistic methods have been used to reconstruct the development of cultures or artifacts using groups of cultural traits or artifact features.

Comparative mythology and folktale use cladistic methods to reconstruct the protoversion of many myths. Mythological phylogenies constructed with mythemes clearly support low horizontal transmissions (borrowings), historical (sometimes Palaeolithic) diffusions and punctuated evolution. They also are a powerful way to test hypotheses about cross-cultural relationships among folktales.

Literature: Cladistic methods have been used in the classification of the surviving manuscripts of the "Canterbury Tales", and the manuscripts of the Sanskrit "Charaka Samhita".

Historical linguistics: Cladistic methods have been used to reconstruct the phylogeny of languages using linguistic features. This is similar to the traditional comparative method of historical linguistics, but is more explicit in its use of parsimony and allows much faster analysis of large datasets (computational phylogenetics).

Textual criticism or stemmatics: Cladistic methods have been used to reconstruct the phylogeny of manuscripts of the same work (and reconstruct the lost original) using distinctive copying errors as apomorphies. This differs from traditional historical-comparative linguistics in enabling the editor to evaluate and place in genetic relationship large groups of manuscripts with large numbers of variants that would be impossible to handle manually. It also enables parsimony analysis of contaminated traditions of transmission that would be impossible to evaluate manually in a reasonable period of time.

Astrophysics infers the history of relationships between galaxies to create branching diagram hypotheses of galaxy diversification.




</doc>
<doc id="5377" url="https://en.wikipedia.org/wiki?curid=5377" title="Calendar">
Calendar

A calendar is a system of organizing days for social, religious, commercial or administrative purposes. This is done by giving names to periods of time, typically days, weeks, months and years. A date is the designation of a single, specific day within such a system. A calendar is also a physical record (often paper) of such a system. A calendar can also mean a list of planned events, such as a court calendar or a partly or fully chronological list of documents, such as a calendar of wills.

Periods in a calendar (such as years and months) are usually, though not necessarily, synchronized with the cycle of the sun or the moon. The most common type of pre-modern calendar was the lunisolar calendar, a lunar calendar that occasionally adds one intercalary month to remain synchronized with the solar year over the long term. 

The term "calendar" is taken from "calendae", the term for the first day of the month in the Roman calendar, related to the verb "calare" "to call out", referring to the "calling" of the new moon when it was first seen. Latin "calendarium" meant "account book, register" (as accounts were settled and debts were collected on the calends of each month). The Latin term was adopted in Old French as "calendier" and from there in Middle English as "calender" by the 13th century (the spelling "calendar" is early modern).

The course of the sun and the moon are the most salient natural, regularly recurring events useful for timekeeping, thus in pre-modern societies worldwide lunation and the year were most commonly used as time units. Nevertheless, the Roman calendar contained remnants of a very ancient pre-Etruscan 10-month solar year. The first recorded physical calendars, dependent on the development of writing in the Ancient Near East, are the Bronze Age Egyptian and Sumerian calendars.

A large number of Ancient Near East calendar systems based on the Babylonian calendar date from the Iron Age, among them the calendar system of the Persian Empire, which in turn gave rise to the Zoroastrian calendar and the Hebrew calendar.

A great number of Hellenic calendars developed in Classical Greece, and in the Hellenistic period gave rise to both the ancient Roman calendar and to various Hindu calendars.

Calendars in antiquity were lunisolar, depending on the introduction of intercalary months to align the solar and the lunar years. This was mostly based on observation, but there may have been early attempts to model the pattern of intercalation algorithmically, as evidenced in the fragmentary 2nd-century Coligny calendar.

The Roman calendar was reformed by Julius Caesar in 46 BC.. The Julian calendar was no longer dependent on the observation of the new moon but simply followed an algorithm of introducing a leap day every four years. This created a dissociation of the calendar month from the lunation.

The Islamic calendar is based on the prohibition of intercalation ("nasi'") by Muhammad, in Islamic tradition dated to a sermon held on 9 Dhu al-Hijjah AH 10 (Julian date: 6 March 632). This resulted in an observation-based lunar calendar that shifts relative to the seasons of the solar year.

The first calendar reform of the early modern era was the Gregorian calendar, introduced in 1582 based on the observation of a long-term shift between the Julian calendar and the solar year.

There have been a number of modern proposals for reform of the calendar, such as the World Calendar, International Fixed Calendar, Holocene calendar, and, recently, the Hanke-Henry Permanent Calendar. Such ideas are mooted from time to time but have failed to gain traction because of the loss of continuity, massive upheaval in implementation, and religious objections.

A full calendar system has a different calendar date for every day. Thus the week cycle is by itself not a full calendar system; neither is a system to name the days within a year without a system for identifying the years.

The simplest calendar system just counts time periods from a reference date. This applies for the Julian day or Unix Time. Virtually the only possible variation is using a different reference date, in particular, one less distant in the past to make the numbers smaller. Computations in these systems are just a matter of addition and subtraction.

Other calendars have one (or multiple) larger units of time.

Calendars that contain one level of cycles:

Calendars with two levels of cycles:

Cycles can be synchronized with periodic phenomena:


Very commonly a calendar includes more than one type of cycle or has both cyclic and non-cyclic elements.

Most calendars incorporate more complex cycles. For example, the vast majority of them track years, months, weeks and days. The seven-day week is practically universal, though its use varies. It has run uninterrupted for millennia.

Solar calendars assign a "date" to each solar day. A day may consist of the period between sunrise and sunset, with a following period of night, or it may be a period between successive events such as two sunsets. The length of the interval between two such successive events may be allowed to vary slightly during the year, or it may be averaged into a mean solar day. Other types of calendar may also use a solar day.

Not all calendars use the solar year as a unit. A lunar calendar is one in which days are numbered within each lunar phase cycle. Because the length of the lunar month is not an even fraction of the length of the tropical year, a purely lunar calendar quickly drifts against the seasons, which do not vary much near the equator. It does, however, stay constant with respect to other phenomena, notably tides. An example is the Islamic calendar.
Alexander Marshack, in a controversial reading, believed that marks on a bone baton (c. 25,000 BC) represented a lunar calendar. Other marked bones may also represent lunar calendars. Similarly, Michael Rappenglueck believes that marks on a 15,000-year-old cave painting represent a lunar calendar.

A lunisolar calendar is a lunar calendar that compensates by adding an extra month as needed to realign the months with the seasons. An example is the Hebrew calendar which uses a 19-year cycle.

Nearly all calendar systems group consecutive days into "months" and also into "years". In a "solar calendar" a "year" approximates Earth's tropical year (that is, the time it takes for a complete cycle of seasons), traditionally used to facilitate the planning of agricultural activities. In a "lunar calendar", the "month" approximates the cycle of the moon phase. Consecutive days may be grouped into other periods such as the week.

Because the number of days in the "tropical year" is not a whole number, a solar calendar must have a different number of days in different years. This may be handled, for example, by adding an extra day in leap years. The same applies to months in a lunar calendar and also the number of months in a year in a lunisolar calendar. This is generally known as intercalation. Even if a calendar is solar, but not lunar, the year cannot be divided entirely into months that never vary in length.

Cultures may define other units of time, such as the week, for the purpose of scheduling regular activities that do not easily coincide with months or years. Many cultures use different baselines for their calendars' starting years. Historically, several countries have based their calendars on regnal years, a calendar based on the reign of their current sovereign. For example, the year 2006 in Japan is year 18 Heisei, with Heisei being the era name of Emperor Akihito.

An "astronomical calendar" is based on ongoing observation; examples are the religious Islamic calendar and the old religious Jewish calendar in the time of the Second Temple. Such a calendar is also referred to as an "observation-based" calendar. The advantage of such a calendar is that it is perfectly and perpetually accurate. The disadvantage is that working out when a particular date would occur is difficult.

An "arithmetic calendar" is one that is based on a strict set of rules; an example is the current Jewish calendar. Such a calendar is also referred to as a "rule-based" calendar. The advantage of such a calendar is the ease of calculating when a particular date occurs. The disadvantage is imperfect accuracy. Furthermore, even if the calendar is very accurate, its accuracy diminishes slowly over time, owing to changes in Earth's rotation. This limits the lifetime of an accurate arithmetic calendar to a few thousand years. After then, the rules would need to be modified from observations made since the invention of the calendar.

Calendars may be either complete or incomplete. Complete calendars provide a way of naming each consecutive day, while incomplete calendars do not. The early Roman calendar, which had no way of designating the days of the winter months other than to lump them together as "winter", is an example of an incomplete calendar, while the Gregorian calendar is an example of a complete calendar.

The primary practical use of a calendar is to identify days: to be informed about or to agree on a future event and to record an event that has happened. Days may be significant for agricultural, civil, religious or social reasons. For example, a calendar provides a way to determine when to start planting or harvesting, which days are religious or civil holidays, which days mark the beginning and end of business accounting periods, and which days have legal significance, such as the day taxes are due or a contract expires. Also a calendar may, by identifying a day, provide other useful information about the day such as its season.

Calendars are also used to help people manage their personal schedules, time and activities, particularly when individuals have numerous work, school, and family commitments. People frequently use multiple systems and may keep both a business and family calendar to help prevent them from overcommitting their time.

Calendars are also used as part of a complete timekeeping system: date and time of day together specify a moment in time. In the modern world, timekeepers can show time, date and weekday. Some may also show lunar phase.

The Gregorian calendar is the "de facto" international standard and is used almost everywhere in the world for civil purposes. It is a purely solar calendar, with a cycle of leap days in a 400-year cycle designed to keep the duration of the year aligned with the solar year.

Each Gregorian year has either 365 or 366 days (the leap day being inserted as 29 February), amounting to an average Gregorian year of 365.2425 days (compared to a solar year of 365.2422 days). It was introduced in 1582 as a refinement to the Julian calendar which had been in use throughout the European Middle Ages, amounting to a 0.002% correction in the length of the year.

During the Early Modern period, however, its adoption was mostly limited to Roman Catholic nations, but by the 19th century, it became widely adopted worldwide for the sake of convenience in international trade. The last European country to adopt the reform was Greece, in 1923.

The calendar epoch used by the Gregorian calendar is inherited from the medieval convention established by Dionysius Exiguus and associated with the Julian calendar. The year number is variously given as AD (for "Anno Domini") or CE (for "Common Era" or, indeed, "Christian Era").

The most important use of pre-modern calendars is keeping track of the liturgical year and the observation of religious feast days.

While the Gregorian calendar is itself historically motivated in relation to the calculation of the Easter date, it is now in worldwide secular use as the "de facto" standard. Alongside the use of the Gregorian calendar for secular matters, there remain a number of calendars in use for religious purposes.

Eastern Christians, including the Orthodox Church, use the Julian calendar.

The Islamic calendar or Hijri calendar, is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to date events in most of the Muslim countries (concurrently with the Gregorian calendar), and used by Muslims everywhere to determine the proper day on which to celebrate Islamic holy days and festivals. Its epoch is the Hijra (corresponding to AD 622)
With an annual drift of 11 or 12 days, the seasonal relation is repeated approximately every 33 Islamic years.

Various Hindu calendars remain in use in the Indian subcontinent, including the Nepali calendar, Bengali calendar, Malayalam calendar, Tamil calendar, Vikrama Samvat used in Northern India, and Shalivahana calendar in the Deccan states.

The Buddhist calendar and the traditional lunisolar calendars of Cambodia, Laos, Myanmar, Sri Lanka and Thailand are also based on an older version of the Hindu calendar.

Most of the Hindu calendars are inherited from a system first enunciated in Vedanga Jyotisha of Lagadha, standardized in the "Sūrya Siddhānta" and subsequently reformed by astronomers such as Āryabhaṭa (AD 499), Varāhamihira (6th century) and Bhāskara II (12th century).

The Hebrew calendar is used by Jews worldwide for religious and cultural affairs, also influences civil matters in Israel (such as national holidays) and can be used business dealings (such as for the dating of cheques).

Bahá'ís worldwide use the Bahá'í calendar. The Baha'i Calendar, also known as the Badi Calendar was first established by the Bab in the Kitab-i-Asma. The Baha'i Calendar is also purely a solar calendar and comprises 19 months each having nineteen days.

The Chinese, Hebrew, Hindu, and Julian calendars are widely used for religious and social purposes.

The Iranian (Persian) calendar is used in Iran and some parts of Afghanistan. The Ethiopian calendar or Ethiopic calendar is the principal calendar used in Ethiopia and Eritrea, with the Oromo calendar also in use in some areas. In neighboring Somalia, the Somali calendar co-exists alongside the Gregorian and Islamic calendars. In Thailand, where the Thai solar calendar is used, the months and days have adopted the western standard, although the years are still based on the traditional Buddhist calendar.

A fiscal calendar generally means the accounting year of a government or a business. It is used for budgeting, keeping accounts and taxation. It is a set of 12 months that may start at any date in a year. The US government's fiscal year starts on 1 October and ends on 30 September. The government of India's fiscal year starts on 1 April and ends on 31 March. Small traditional businesses in India start the fiscal year on Diwali festival and end the day before the next year's Diwali festival.

In accounting (and particularly accounting software), a fiscal calendar (such as a 4/4/5 calendar) fixes each month at a specific number of weeks to facilitate comparisons from month to month and year to year. January always has exactly 4 weeks (Sunday through Saturday), February has 4 weeks, March has 5 weeks, etc. Note that this calendar will normally need to add a 53rd week to every 5th or 6th year, which might be added to December or might not be, depending on how the organization uses those dates. There exists an international standard way to do this (the ISO week). The ISO week starts on a Monday and ends on a Sunday. Week 1 is always the week that contains 4 January in the Gregorian calendar.

The term "calendar" applies not only to a given scheme of timekeeping but also to a specific record or device displaying such a scheme, for example, an appointment book in the form of a pocket calendar (or personal organizer), desktop calendar, a wall calendar, etc.

In a paper calendar, one or two sheets can show a single day, a week, a month, or a year. If a sheet is for a single day, it easily shows the date and the weekday. If a sheet is for multiple days it shows a conversion table to convert from weekday to date and back. With a special pointing device, or by crossing out past days, it may indicate the current date and weekday. This is the most common usage of the word.

In the US Sunday is considered the first day of the week and so appears on the far left and Saturday the last day of the week appearing on the far right. In Britain, the weekend may appear at the end of the week so the first day is Monday and the last day is Sunday. The US calendar display is also used in Britain.

It is common to display the Gregorian calendar in separate monthly grids of seven columns (from Monday to Sunday, or Sunday to Saturday depending on which day is considered to start the week – this varies according to country) and five to six rows (or rarely, four rows when the month of February contains 28 days beginning on the first day of the week), with the day of the month numbered in each cell, beginning with 1. The sixth row is sometimes eliminated by marking 23/30 and 24/31 together as necessary.

When working with weeks rather than months, a continuous format is sometimes more convenient, where no blank cells are inserted to ensure that the first day of a new month begins on a fresh row.

Calendaring software provides users with an electronic version of a calendar, and may additionally provide an appointment book, address book or contact list.
Calendaring is a standard feature of many PDAs, EDAs, and smartphones. The software may be a local package designed for individual use (e.g., Lightning extension for Mozilla Thunderbird, Microsoft Outlook without Exchange Server, or Windows Calendar) or may be a networked package that allows for the sharing of information between users (e.g., Mozilla Sunbird, Windows Live Calendar, Google Calendar, or Microsoft Outlook with Exchange Server).






</doc>
<doc id="5378" url="https://en.wikipedia.org/wiki?curid=5378" title="Physical cosmology">
Physical cosmology

Physical cosmology is a branch of cosmology concerned with the studies of the largest-scale structures and dynamics of the universe and with fundamental questions about its origin, structure, evolution, and ultimate fate. Cosmology as a science originated with the Copernican principle, which implies that celestial bodies obey identical physical laws to those on Earth, and Newtonian mechanics, which first allowed those physical laws to be understood. Physical cosmology, as it is now understood, began with the development in 1915 of Albert Einstein's general theory of relativity, followed by major observational discoveries in the 1920s: first, Edwin Hubble discovered that the universe contains a huge number of external galaxies beyond the Milky Way; then, work by Vesto Slipher and others showed that the universe is expanding. These advances made it possible to speculate about the origin of the universe, and allowed the establishment of the Big Bang theory, by Georges Lemaître, as the leading cosmological model. A few researchers still advocate a handful of alternative cosmologies; however, most cosmologists agree that the Big Bang theory explains the observations better.

Dramatic advances in observational cosmology since the 1990s, including the cosmic microwave background, distant supernovae and galaxy redshift surveys, have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations.

Cosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics. Areas relevant to cosmology include particle physics experiments and theory, theoretical and observational astrophysics, general relativity, quantum mechanics, and plasma physics.

Modern cosmology developed along tandem tracks of theory and observation. In 1916, Albert Einstein published his theory of general relativity, which provided a unified description of gravity as a geometric property of space and time. At the time, Einstein believed in a static universe, but found that his original formulation of the theory did not permit it. This is because masses distributed throughout the universe gravitationally attract, and move toward each other over time. However, he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale. Einstein published his first paper on relativistic cosmology in 1917, in which he added this "cosmological constant" to his field equations in order to force them to model a static universe. The Einstein model describes a static universe; space is finite and unbounded (analogous to the surface of a sphere, which has a finite area but no edges). However, this so-called Einstein model is unstable to small perturbations—it will eventually start to expand or contract. It was later realized that Einstein's model was just one of a larger set of possibilities, all of which were consistent with general relativity and the cosmological principle. The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s. His equations describe the Friedmann–Lemaître–Robertson–Walker universe, which may expand or contract, and whose geometry may be open, flat, or closed.

In the 1910s, Vesto Slipher (and later Carl Wilhelm Wirtz) interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth. However, it is difficult to determine the distance to astronomical objects. One way is to compare the physical size of an object to its angular size, but a physical size must be assumed to do this. Another method is to measure the brightness of an object and assume an intrinsic luminosity, from which the distance may be determined using the inverse square law. Due to the difficulty of using these methods, they did not realize that the nebulae were actually galaxies outside our own Milky Way, nor did they speculate about the cosmological implications. In 1927, the Belgian Roman Catholic priest Georges Lemaître independently derived the Friedmann–Lemaître–Robertson–Walker equations and proposed, on the basis of the recession of spiral nebulae, that the universe began with the "explosion" of a "primeval atom"—which was later called the Big Bang. In 1929, Edwin Hubble provided an observational basis for Lemaître's theory. Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars. He discovered a relationship between the redshift of a galaxy and its distance. He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance. This fact is now known as Hubble's law, though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten, due to not knowing about the types of Cepheid variables.

Given the cosmological principle, Hubble's law suggested that the universe was expanding. Two primary explanations were proposed for the expansion. One was Lemaître's Big Bang theory, advocated and developed by George Gamow. The other explanation was Fred Hoyle's steady state model in which new matter is created as the galaxies move away from each other. In this model, the universe is roughly the same at any point in time.

For a number of years, support for these theories was evenly divided. However, the observational evidence began to support the idea that the universe evolved from a hot dense state. The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model, and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s, few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos. One consequence of this is that in standard general relativity, the universe began with a singularity, as demonstrated by Roger Penrose and Stephen Hawking in the 1960s.

An alternative view to extend the Big Bang model, suggesting the universe had no beginning or singularity and the age of the universe is infinite, has been presented.

The lightest chemical elements, primarily hydrogen and helium, were created during the Big Bang through the process of nucleosynthesis. In a sequence of stellar nucleosynthesis reactions, smaller atomic nuclei are then combined into larger atomic nuclei, ultimately forming stable iron group elements such as iron and nickel, which have the highest nuclear binding energies. The net process results in a "later energy release", meaning subsequent to the Big Bang. Such reactions of nuclear particles can lead to "sudden energy releases" from cataclysmic variable stars such as novae. Gravitational collapse of matter into black holes also powers the most energetic processes, generally seen in the nuclear regions of galaxies, forming "quasars" and "active galaxies".

Cosmologists cannot explain all cosmic phenomena exactly, such as those related to the accelerating expansion of the universe, using conventional forms of energy. Instead, cosmologists propose a new form of energy called dark energy that permeates all space. One hypothesis is that dark energy is just the vacuum energy, a component of empty space that is associated with the virtual particles that exist due to the uncertainty principle.

There is no clear way to define the total energy in the universe using the most widely accepted theory of gravity, general relativity. Therefore, it remains controversial whether the total energy is conserved in an expanding universe. For instance, each photon that travels through intergalactic space loses energy due to the redshift effect. This energy is not obviously transferred to any other system, so seems to be permanently lost. On the other hand, some cosmologists insist that energy is conserved in some sense; this follows the law of conservation of energy.

Thermodynamics of the universe is a field of study that explores which form of energy dominates the cosmos – relativistic particles which are referred to as radiation, or non-relativistic particles referred to as matter. Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy, and so move at the speed of light or very close to it; non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light.

As the universe expands, both matter and radiation in it become diluted. However, the energy densities of radiation and matter dilute at different rates. As a particular volume expands, mass energy density is changed only by the increase in volume, but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up. Thus the energy of radiation becomes a smaller part of the universe's total energy than that of matter as it expands. The very early universe is said to have been 'radiation dominated' and radiation controlled the deceleration of expansion. Later, as the average energy per photon becomes roughly 10 eV and lower, matter dictates the rate of deceleration and the universe is said to be 'matter dominated'. The intermediate case is not treated well analytically. As the expansion of the universe continues, matter dilutes even further and the cosmological constant becomes dominant, leading to an acceleration in the universe's expansion.

The history of the universe is a central issue in cosmology. The history of the universe is divided into different periods called epochs, according to the dominant forces and processes in each period. The standard cosmological model is known as the Lambda-CDM model.

Within the standard cosmological model, the equations of motion governing the universe as a whole are derived from general relativity with a small, positive cosmological constant. The solution is an expanding universe; due to this expansion, the radiation and matter in the universe cool down and become diluted. At first, the expansion is slowed down by gravitation attracting the radiation and matter in the universe. However, as these become diluted, the cosmological constant becomes more dominant and the expansion of the universe starts to accelerate rather than decelerate. In our universe this happened billions of years ago.

During the earliest moments of the universe the average energy density was very high, making knowledge of particle physics critical to understanding this environment. Hence, scattering processes and decay of unstable elementary particles are important for cosmological models of this period.

As a rule of thumb, a scattering or a decay process is cosmologically important in a certain epoch if the time scale describing that process is smaller than, or comparable to, the time scale of the expansion of the universe. The time scale that describes the expansion of the universe is formula_1 with formula_2 being the Hubble parameter, which varies with time. The expansion timescale formula_1 is roughly equal to the age of the universe at each point in time.

Observations suggest that the universe began around 13.8 billion years ago. Since then, the evolution of the universe has passed through three phases. The very early universe, which is still poorly understood, was the split second in which the universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth. Therefore, while the basic features of this epoch have been worked out in the Big Bang theory, the details are largely based on educated guesses.
Following this, in the early universe, the evolution of the universe proceeded according to known high energy physics. This is when the first protons, electrons and neutrons formed, then nuclei and finally atoms. With the formation of neutral hydrogen, the cosmic microwave background was emitted. Finally, the epoch of structure formation began, when matter started to aggregate into the first stars and quasars, and ultimately galaxies, clusters of galaxies and superclusters formed. The future of the universe is not yet firmly known, but according to the ΛCDM model it will continue expanding forever.

Below, some of the most active areas of inquiry in cosmology are described, in roughly chronological order. This does not include all of the Big Bang cosmology, which is presented in "Timeline of the Big Bang."

The early, hot universe appears to be well explained by the Big Bang from roughly 10 seconds onwards, but there are several problems. One is that there is no compelling reason, using current particle physics, for the universe to be flat, homogeneous, and isotropic "(see the cosmological principle)". Moreover, grand unified theories of particle physics suggest that there should be magnetic monopoles in the universe, which have not been found. These problems are resolved by a brief period of cosmic inflation, which drives the universe to flatness, smooths out anisotropies and inhomogeneities to the observed level, and exponentially dilutes the monopoles. The physical model behind cosmic inflation is extremely simple, but it has not yet been confirmed by particle physics, and there are difficult problems reconciling inflation and quantum field theory. Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation.

Another major problem in cosmology is what caused the universe to contain far more matter than antimatter. Cosmologists can observationally deduce that the universe is not split into regions of matter and antimatter. If it were, there would be X-rays and gamma rays produced as a result of annihilation, but this is not observed. Therefore, some process in the early universe must have created a small excess of matter over antimatter, and this (currently not understood) process is called "baryogenesis". Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967, and requires a violation of the particle physics symmetry, called CP-symmetry, between matter and antimatter. However, particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry. Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early universe that might account for the baryon asymmetry.

Both the problems of baryogenesis and cosmic inflation are very closely related to particle physics, and their resolution might come from high energy theory and experiment, rather than through observations of the universe.

Big Bang nucleosynthesis is the theory of the formation of the elements in the early universe. It finished when the universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur. Big Bang nucleosynthesis had a brief period during which it could operate, so only the very lightest elements were produced. Starting from hydrogen ions (protons), it principally produced deuterium, helium-4, and lithium. Other elements were produced in only trace abundances. The basic theory of nucleosynthesis was developed in 1948 by George Gamow, Ralph Asher Alpher, and Robert Herman. It was used for many years as a probe of physics at the time of the Big Bang, as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early universe. Specifically, it can be used to test the equivalence principle, to probe dark matter, and test neutrino physics. Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth "sterile" species of neutrino.

The ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology.

The cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one part in 10. Cosmological perturbation theory, which describes the evolution of slight inhomogeneities in the early universe, has allowed cosmologists to precisely calculate the angular power spectrum of the radiation, and it has been measured by the recent satellite experiments (COBE and WMAP) and many ground and balloon-based experiments (such as Degree Angular Scale Interferometer, Cosmic Background Imager, and Boomerang). One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy, as well as to test the predictions of the Big Bang model and look for new physics. The results of measurements made by WMAP, for example, have placed limits on the neutrino masses.

Newer experiments, such as QUIET and the Atacama Cosmology Telescope, are trying to measure the polarization of the cosmic microwave background. These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation, and the so-called secondary anisotropies, such as the Sunyaev-Zel'dovich effect and Sachs-Wolfe effect, which are caused by interaction between galaxies and clusters with the cosmic microwave background.

On 17 March 2014, astronomers of the BICEP2 Collaboration announced the apparent detection of "B"-mode polarization of the CMB, considered to be evidence of primordial gravitational waves that are predicted by the theory of inflation to occur during the earliest phase of the Big Bang. However, later that year the Planck collaboration provided a more accurate measurement of cosmic dust, concluding that the B-mode signal from dust is the same strength as that reported from BICEP2. On 30 January 2015, a joint analysis of BICEP2 and Planck data was published and the European Space Agency announced that the signal can be entirely attributed to interstellar dust in the Milky Way.

Understanding the formation and evolution of the largest and earliest structures (i.e., quasars, galaxies, clusters and superclusters) is one of the largest efforts in cosmology. Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up, with smaller objects forming first, while the largest objects, such as superclusters, are still assembling. One way to study structure in the universe is to survey the visible galaxies, in order to construct a three-dimensional picture of the galaxies in the universe and measure the matter power spectrum. This is the approach of the "Sloan Digital Sky Survey" and the 2dF Galaxy Redshift Survey.

Another tool for understanding structure formation is simulations, which cosmologists use to study the gravitational aggregation of matter in the universe, as it clusters into filaments, superclusters and voids. Most simulations contain only non-baryonic cold dark matter, which should suffice to understand the universe on the largest scales, as there is much more dark matter in the universe than visible, baryonic matter. More advanced simulations are starting to include baryons and study the formation of individual galaxies. Cosmologists study these simulations to see if they agree with the galaxy surveys, and to understand any discrepancy.

Other, complementary observations to measure the distribution of matter in the distant universe and to probe reionization include:

These will help cosmologists settle the question of when and how structure formed in the universe.

Evidence from Big Bang nucleosynthesis, the cosmic microwave background, structure formation, and galaxy rotation curves suggests that about 23% of the mass of the universe consists of non-baryonic dark matter, whereas only 4% consists of visible, baryonic matter. The gravitational effects of dark matter are well understood, as it behaves like a cold, non-radiative fluid that forms haloes around galaxies. Dark matter has never been detected in the laboratory, and the particle physics nature of dark matter remains completely unknown. Without observational constraints, there are a number of candidates, such as a stable supersymmetric particle, a weakly interacting massive particle, a gravitationally-interacting massive particle, an axion, and a massive compact halo object. Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations (MOND) or an effect from brane cosmology.

If the universe is flat, there must be an additional component making up 73% (in addition to the 23% dark matter and 4% baryons) of the energy density of the universe. This is called dark energy. In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background, it must not cluster in haloes like baryons and dark matter. There is strong observational evidence for dark energy, as the total energy density of the universe is known through constraints on the flatness of the universe, but the amount of clustering matter is tightly measured, and is much less than this. The case for dark energy was strengthened in 1999, when measurements demonstrated that the expansion of the universe has begun to gradually accelerate.

Apart from its density and its clustering properties, nothing is known about dark energy. "Quantum field theory" predicts a cosmological constant (CC) much like dark energy, but 120 orders of magnitude larger than that observed. Steven Weinberg and a number of string theorists "(see string landscape)" have invoked the 'weak anthropic principle': i.e. the reason that physicists observe a universe with such a small cosmological constant is that no physicists (or any life) could exist in a universe with a larger cosmological constant. Many cosmologists find this an unsatisfying explanation: perhaps because while the weak anthropic principle is self-evident (given that living observers exist, there must be at least one universe with a cosmological constant which allows for life to exist) it does not attempt to explain the context of that universe. For example, the weak anthropic principle alone does not distinguish between:

Other possible explanations for dark energy include quintessence or a modification of gravity on the largest scales. The effect on cosmology of the dark energy that these models describe is given by the dark energy's equation of state, which varies depending upon the theory. The nature of dark energy is one of the most challenging problems in cosmology.

A better understanding of dark energy is likely to solve the problem of the ultimate fate of the universe. In the current cosmological epoch, the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming. It is not known whether the acceleration will continue indefinitely, perhaps even increasing until a big rip, or whether it will eventually reverse, lead to a big freeze, or follow some other scenario.

Gravitational waves are ripples in the curvature of spacetime that propagate as waves at the speed of light, generated in certain gravitational interactions that propagate outward from their source. Gravitational-wave astronomy is an emerging branch of observational astronomy which aims to use gravitational waves to collect observational data about sources of detectable gravitational waves such as binary star systems composed of white dwarfs, neutron stars, and black holes; and events such as supernovae, and the formation of the early universe shortly after the Big Bang.

In 2016, the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made the first observation of gravitational waves, originating from a pair of merging black holes using the Advanced LIGO detectors. On 15 June 2016, a second detection of gravitational waves from coalescing black holes was announced. Besides LIGO, many other gravitational-wave observatories (detectors) are under construction.

Cosmologists also study:





</doc>
<doc id="5382" url="https://en.wikipedia.org/wiki?curid=5382" title="Inflation (cosmology)">
Inflation (cosmology)

In physical cosmology, cosmic inflation, cosmological inflation, or just inflation, is a theory of exponential expansion of space in the early universe. The inflationary epoch lasted from 10 seconds after the conjectured Big Bang singularity to some time between 10 and 10 seconds after the singularity. Following the inflationary period, the universe continued to expand, but the expansion was no longer accelerating.

Inflation theory was developed in the late 1970s and early 80s, with notable contributions by several theoretical physicists, including Alexei Starobinsky at Landau Institute for Theoretical Physics, Alan Guth at Cornell University, and Andrei Linde at Lebedev Physical Institute. Alexei Starobinsky, Alan Guth, and Andrei Linde won the 2014 Kavli Prize "for pioneering the theory of cosmic inflation." It was developed further in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.

The detailed particle physics mechanism responsible for inflation is unknown. The basic inflationary paradigm is accepted by most physicists, as a number of inflation model predictions have been confirmed by observation; however, a substantial minority of scientists dissent from this position. The hypothetical field thought to be responsible for inflation is called the inflaton.

In 2002, three of the original architects of the theory were recognized for their major contributions; physicists Alan Guth of M.I.T., Andrei Linde of Stanford, and Paul Steinhardt of Princeton shared the prestigious Dirac Prize "for development of the concept of inflation in cosmology". In 2012, Alan Guth and Andrei Linde were awarded the Breakthrough Prize in Fundamental Physics for their invention and development of inflationary cosmology.

Around 1930, Edwin Hubble discovered that light from remote galaxies was redshifted; the more remote, the more shifted. This was quickly interpreted as meaning galaxies were receding from Earth. If Earth is not in some special, privileged, central position in the universe, then it would mean all galaxies are moving apart, and the further away, the faster they are moving away. It is now understood that the universe is expanding, carrying the galaxies with it, and causing this observation. Many other observations agree, and also lead to the same conclusion. However, for many years it was not clear why or how the universe might be expanding, or what it might signify.

Based on a huge amount of experimental observation and theoretical work, it is now believed that the reason for the observation is that "space itself is expanding", and that it expanded very rapidly within the first fraction of a second after the Big Bang. This kind of expansion is known as a ""metric"" expansion. In the terminology of mathematics and physics, a "metric" is a measure of distance that satisfies a specific list of properties, and the term implies that "the sense of distance within the universe is itself changing", although at this time it is far too small an effect to see on less than an intergalactic scale.

The modern explanation for the metric expansion of space was proposed by physicist Alan Guth in 1979, while investigating the problem of why no magnetic monopoles are seen today. He found that if the universe contained a field in a positive-energy false vacuum state, then according to general relativity it would generate an exponential expansion of space. It was very quickly realized that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does "today", the Universe would have to have started from very finely tuned, or "special" initial conditions at the Big Bang. Inflation theory largely resolves these problems as well, thus making a universe like ours much more likely in the context of Big Bang theory.

No physical field has yet been discovered that is responsible for this inflation. However such a field would be scalar and the first relativistic scalar field proven to exist, the Higgs field, was only discovered in 2012–2013 and is still being researched. So it is not seen as problematic that a field responsible for cosmic inflation and the metric expansion of space has not yet been discovered. The proposed field and its quanta (the subatomic particles related to it) have been named the inflaton. If this field did not exist, scientists would have to propose a different explanation for all the observations that strongly suggest a metric expansion of space has occurred, and is still occurring (much more slowly) today.

An expanding universe generally has a cosmological horizon, which, by analogy with the more familiar horizon caused by the curvature of Earth's surface, marks the boundary of the part of the Universe that an observer can see. Light (or other radiation) emitted by objects beyond the cosmological horizon in an accelerating universe never reaches the observer, because the space in between the observer and the object is expanding too rapidly.

The observable universe is one "causal patch" of a much larger unobservable universe; other parts of the Universe cannot communicate with Earth yet. These parts of the Universe are outside our current cosmological horizon. In the standard hot big bang model, without inflation, the cosmological horizon moves out, bringing new regions into view. Yet as a local observer sees such a region for the first time, it looks no different from any other region of space the local observer has already seen: its background radiation is at nearly the same temperature as the background radiation of other regions, and its space-time curvature is evolving lock-step with the others. This presents a mystery: how did these new regions know what temperature and curvature they were supposed to have? They couldn't have learned it by getting signals, because they were not previously in communication with our past light cone.

Inflation answers this question by postulating that all the regions come from an earlier era with a big vacuum energy, or cosmological constant. A space with a cosmological constant is qualitatively different: instead of moving outward, the cosmological horizon stays put. For any one observer, the distance to the cosmological horizon is constant. With exponentially expanding space, two nearby observers are separated very quickly; so much so, that the distance between them quickly exceeds the limits of communications. The spatial slices are expanding very fast to cover huge volumes. Things are constantly moving beyond the cosmological horizon, which is a fixed distance away, and everything becomes homogeneous.

As the inflationary field slowly relaxes to the vacuum, the cosmological constant goes to zero and space begins to expand normally. The new regions that come into view during the normal expansion phase are exactly the same regions that were pushed out of the horizon during inflation, and so they are at nearly the same temperature and curvature, because they come from the same originally small patch of space.

The theory of inflation thus explains why the temperatures and curvatures of different regions are so nearly equal. It also predicts that the total curvature of a space-slice at constant global time is zero. This prediction implies that the total ordinary matter, dark matter and residual vacuum energy in the Universe have to add up to the critical density, and the evidence supports this. More strikingly, inflation allows physicists to calculate the minute differences in temperature of different regions from quantum fluctuations during the inflationary era, and many of these quantitative predictions have been confirmed.

In a space that expands exponentially (or nearly exponentially) with time, any pair of free-floating objects that are initially at rest will move apart from each other at an accelerating rate, at least as long as they are not bound together by any force. From the point of view of one such object, the spacetime is something like an inside-out Schwarzschild black hole—each object is surrounded by a spherical event horizon. Once the other object has fallen through this horizon it can never return, and even light signals it sends will never reach the first object (at least so long as the space continues to expand exponentially).

In the approximation that the expansion is exactly exponential, the horizon is static and remains a fixed physical distance away. This patch of an inflating universe can be described by the following metric:

This exponentially expanding spacetime is called a de Sitter space, and to sustain it there must be a cosmological constant, a vacuum energy density that is constant in space and time and proportional to Λ in the above metric. For the case of exactly exponential expansion, the vacuum energy has a negative pressure "p" equal in magnitude to its energy density "ρ"; the equation of state is "p=−ρ".

Inflation is typically not an exactly exponential expansion, but rather quasi- or near-exponential. In such a universe the horizon will slowly grow with time as the vacuum energy density gradually decreases.

Because the accelerating expansion of space stretches out any initial variations in density or temperature to very large length scales, an essential feature of inflation is that it smooths out inhomogeneities and anisotropies, and reduces the curvature of space. This pushes the Universe into a very simple state in which it is completely dominated by the inflaton field and the only significant inhomogeneities are tiny quantum fluctuations. Inflation also dilutes exotic heavy particles, such as the magnetic monopoles predicted by many extensions to the Standard Model of particle physics. If the Universe was only hot enough to form such particles "before" a period of inflation, they would not be observed in nature, as they would be so rare that it is quite likely that there are none in the observable universe. Together, these effects are called the inflationary "no-hair theorem" by analogy with the no hair theorem for black holes.

The "no-hair" theorem works essentially because the cosmological horizon is no different from a black-hole horizon, except for philosophical disagreements about what is on the other side. The interpretation of the no-hair theorem is that the Universe (observable and unobservable) expands by an enormous factor during inflation. In an expanding universe, energy densities generally fall, or get diluted, as the volume of the Universe increases. For example, the density of ordinary "cold" matter (dust) goes down as the inverse of the volume: when linear dimensions double, the energy density goes down by a factor of eight; the radiation energy density goes down even more rapidly as the Universe expands since the wavelength of each photon is stretched (redshifted), in addition to the photons being dispersed by the expansion. When linear dimensions are doubled, the energy density in radiation falls by a factor of sixteen (see the solution of the energy density continuity equation for an ultra-relativistic fluid). During inflation, the energy density in the inflaton field is roughly constant. However, the energy density in everything else, including inhomogeneities, curvature, anisotropies, exotic particles, and standard-model particles is falling, and through sufficient inflation these all become negligible. This leaves the Universe flat and symmetric, and (apart from the homogeneous inflaton field) mostly empty, at the moment inflation ends and reheating begins.

A key requirement is that inflation must continue long enough to produce the present observable universe from a single, small inflationary Hubble volume. This is necessary to ensure that the Universe appears flat, homogeneous and isotropic at the largest observable scales. This requirement is generally thought to be satisfied if the Universe expanded by a factor of at least 10 during inflation.

Inflation is a period of supercooled expansion, when the temperature drops by a factor of 100,000 or so. (The exact drop is model-dependent, but in the first models it was typically from 10 K down to 10 K.) This relatively low temperature is maintained during the inflationary phase. When inflation ends the temperature returns to the pre-inflationary temperature; this is called "reheating" or thermalization because the large potential energy of the inflaton field decays into particles and fills the Universe with Standard Model particles, including electromagnetic radiation, starting the radiation dominated phase of the Universe. Because the nature of the inflation is not known, this process is still poorly understood, although it is believed to take place through a parametric resonance.

Inflation resolves several problems in Big Bang cosmology that were discovered in the 1970s. Inflation was first proposed by Alan Guth in 1979 while investigating the problem of why no magnetic monopoles are seen today; he found that a positive-energy false vacuum would, according to general relativity, generate an exponential expansion of space. It was very quickly realised that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does "today", the Universe would have to have started from very finely tuned, or "special" initial conditions at the Big Bang. Inflation attempts to resolve these problems by providing a dynamical mechanism that drives the Universe to this special state, thus making a universe like ours much more likely in the context of the Big Bang theory.

The horizon problem is the problem of determining why the Universe appears statistically homogeneous and isotropic in accordance with the cosmological principle. For example, molecules in a canister of gas are distributed homogeneously and isotropically because they are in thermal equilibrium: gas throughout the canister has had enough time to interact to dissipate inhomogeneities and anisotropies. The situation is quite different in the big bang model without inflation, because gravitational expansion does not give the early universe enough time to equilibrate. In a big bang with only the matter and radiation known in the Standard Model, two widely separated regions of the observable universe cannot have equilibrated because they move apart from each other faster than the speed of light and thus have never come into causal contact. In the early Universe, it was not possible to send a light signal between the two regions. Because they have had no interaction, it is difficult to explain why they have the same temperature (are thermally equilibrated). Historically, proposed solutions included the "Phoenix universe" of Georges Lemaître, the related oscillatory universe of Richard Chase Tolman, and the Mixmaster universe of Charles Misner. Lemaître and Tolman proposed that a universe undergoing a number of cycles of contraction and expansion could come into thermal equilibrium. Their models failed, however, because of the buildup of entropy over several cycles. Misner made the (ultimately incorrect) conjecture that the Mixmaster mechanism, which made the Universe "more" chaotic, could lead to statistical homogeneity and isotropy.

The flatness problem is sometimes called one of the Dicke coincidences (along with the cosmological constant problem). It became known in the 1960s that the density of matter in the Universe was comparable to the critical density necessary for a flat universe (that is, a universe whose large scale geometry is the usual Euclidean geometry, rather than a non-Euclidean hyperbolic or spherical geometry).

Therefore, regardless of the shape of the universe the contribution of spatial curvature to the expansion of the Universe could not be much greater than the contribution of matter. But as the Universe expands, the curvature redshifts away more slowly than matter and radiation. Extrapolated into the past, this presents a fine-tuning problem because the contribution of curvature to the Universe must be exponentially small (sixteen orders of magnitude less than the density of radiation at Big Bang nucleosynthesis, for example). This problem is exacerbated by recent observations of the cosmic microwave background that have demonstrated that the Universe is flat to within a few percent.

The magnetic monopole problem, sometimes called the exotic-relics problem, says that if the early universe were very hot, a large number of very heavy, stable magnetic monopoles would have been produced. This is a problem with Grand Unified Theories, which propose that at high temperatures (such as in the early universe) the electromagnetic force, strong, and weak nuclear forces are not actually fundamental forces but arise due to spontaneous symmetry breaking from a single gauge theory. These theories predict a number of heavy, stable particles that have not been observed in nature. The most notorious is the magnetic monopole, a kind of stable, heavy "charge" of magnetic field. Monopoles are predicted to be copiously produced following Grand Unified Theories at high temperature, and they should have persisted to the present day, to such an extent that they would become the primary constituent of the Universe. Not only is that not the case, but all searches for them have failed, placing stringent limits on the density of relic magnetic monopoles in the Universe. A period of inflation that occurs below the temperature where magnetic monopoles can be produced would offer a possible resolution of this problem: monopoles would be separated from each other as the Universe around them expands, potentially lowering their observed density by many orders of magnitude. Though, as cosmologist Martin Rees has written, "Skeptics about exotic physics might not be hugely impressed by a theoretical argument to explain the absence of particles that are themselves only hypothetical. Preventive medicine can readily seem 100 percent effective against a disease that doesn't exist!"

In the early days of General Relativity, Albert Einstein introduced the cosmological constant to allow a static solution, which was a three-dimensional sphere with a uniform density of matter. Later, Willem de Sitter found a highly symmetric inflating universe, which described a universe with a cosmological constant that is otherwise empty. It was discovered that Einstein's universe is unstable, and that small fluctuations cause it to collapse or turn into a de Sitter universe.

In the early 1970s Zeldovich noticed the flatness and horizon problems of Big Bang cosmology; before his work, cosmology was presumed to be symmetrical on purely philosophical grounds. In the Soviet Union, this and other considerations led Belinski and Khalatnikov to analyze the chaotic BKL singularity in General Relativity. Misner's Mixmaster universe attempted to use this chaotic behavior to solve the cosmological problems, with limited success.

In the late 1970s, Sidney Coleman applied the instanton techniques developed by Alexander Polyakov and collaborators to study the fate of the false vacuum in quantum field theory. Like a metastable phase in statistical mechanics—water below the freezing temperature or above the boiling point—a quantum field would need to nucleate a large enough bubble of the new vacuum, the new phase, in order to make a transition. Coleman found the most likely decay pathway for vacuum decay and calculated the inverse lifetime per unit volume. He eventually noted that gravitational effects would be significant, but he did not calculate these effects and did not apply the results to cosmology.

In the Soviet Union, Alexei Starobinsky noted that quantum corrections to general relativity should be important for the early universe. These generically lead to curvature-squared corrections to the Einstein–Hilbert action and a form of "f"("R") modified gravity. The solution to Einstein's equations in the presence of curvature squared terms, when the curvatures are large, leads to an effective cosmological constant. Therefore, he proposed that the early universe went through an inflationary de Sitter era. This resolved the cosmology problems and led to specific predictions for the corrections to the microwave background radiation, corrections that were then calculated in detail. Starobinsky used the action 
which corresponds to the potential
in the Einstein frame. This results in the observables:
formula_4

In 1978, Zeldovich noted the monopole problem, which was an unambiguous quantitative version of the horizon problem, this time in a subfield of particle physics, which led to several speculative attempts to resolve it. In 1980 Alan Guth realized that false vacuum decay in the early universe would solve the problem, leading him to propose a scalar-driven inflation. Starobinsky's and Guth's scenarios both predicted an initial de Sitter phase, differing only in mechanistic details.

Guth proposed inflation in January 1981 to explain the nonexistence of magnetic monopoles; it was Guth who coined the term "inflation". At the same time, Starobinsky argued that quantum corrections to gravity would replace the initial singularity of the Universe with an exponentially expanding de Sitter phase. In October 1980, Demosthenes Kazanas suggested that exponential expansion could eliminate the particle horizon and perhaps solve the horizon problem, while Sato suggested that an exponential expansion could eliminate domain walls (another kind of exotic relic). In 1981 Einhorn and Sato published a model similar to Guth's and showed that it would resolve the puzzle of the magnetic monopole abundance in Grand Unified Theories. Like Guth, they concluded that such a model not only required fine tuning of the cosmological constant, but also would likely lead to a much too granular universe, i.e., to large density variations resulting from bubble wall collisions.

Guth proposed that as the early universe cooled, it was trapped in a false vacuum with a high energy density, which is much like a cosmological constant. As the very early universe cooled it was trapped in a metastable state (it was supercooled), which it could only decay out of through the process of bubble nucleation via quantum tunneling. Bubbles of true vacuum spontaneously form in the sea of false vacuum and rapidly begin expanding at the speed of light. Guth recognized that this model was problematic because the model did not reheat properly: when the bubbles nucleated, they did not generate any radiation. Radiation could only be generated in collisions between bubble walls. But if inflation lasted long enough to solve the initial conditions problems, collisions between bubbles became exceedingly rare. In any one causal patch it is likely that only one bubble would nucleate.

The bubble collision problem was solved by Linde and independently by Andreas Albrecht and Paul Steinhardt in a model named "new inflation" or "slow-roll inflation" (Guth's model then became known as "old inflation"). In this model, instead of tunneling out of a false vacuum state, inflation occurred by a scalar field rolling down a potential energy hill. When the field rolls very slowly compared to the expansion of the Universe, inflation occurs. However, when the hill becomes steeper, inflation ends and reheating can occur.

Eventually, it was shown that new inflation does not produce a perfectly symmetric universe, but that quantum fluctuations in the inflaton are created. These fluctuations form the primordial seeds for all structure created in the later universe. These fluctuations were first calculated by Viatcheslav Mukhanov and G. V. Chibisov in analyzing Starobinsky's similar model. In the context of inflation, they were worked out independently of the work of Mukhanov and Chibisov at the three-week 1982 Nuffield Workshop on the Very Early Universe at Cambridge University. The fluctuations were calculated by four groups working separately over the course of the workshop: Stephen Hawking; Starobinsky; Guth and So-Young Pi; and Bardeen, Steinhardt and Turner.

Inflation is a mechanism for realizing the cosmological principle, which is the basis of the standard model of physical cosmology: it accounts for the homogeneity and isotropy of the observable universe. In addition, it accounts for the observed flatness and absence of magnetic monopoles. Since Guth's early work, each of these observations has received further confirmation, most impressively by the detailed observations of the cosmic microwave background made by the Planck spacecraft. This analysis shows that the Universe is flat to within 0.5 percent, and that it is homogeneous and isotropic to one part in 100,000.

Inflation predicts that the structures visible in the Universe today formed through the gravitational collapse of perturbations that were formed as quantum mechanical fluctuations in the inflationary epoch. The detailed form of the spectrum of perturbations, called a nearly-scale-invariant Gaussian random field is very specific and has only two free parameters. One is the amplitude of the spectrum and the "spectral index", which measures the slight deviation from scale invariance predicted by inflation (perfect scale invariance corresponds to the idealized de Sitter universe). The other free parameter is the tensor to scalar ratio. The simplest inflation models, those without fine-tuning, predict a tensor to scalar ratio near 0.1.

Inflation predicts that the observed perturbations should be in thermal equilibrium with each other (these are called "adiabatic" or "isentropic" perturbations). This structure for the perturbations has been confirmed by the Planck spacecraft, WMAP spacecraft and other cosmic microwave background (CMB) experiments, and galaxy surveys, especially the ongoing Sloan Digital Sky Survey. These experiments have shown that the one part in 100,000 inhomogeneities observed have exactly the form predicted by theory. There is evidence for a slight deviation from scale invariance. The "spectral index", "n" is one for a scale-invariant Harrison–Zel'dovich spectrum. The simplest inflation models predict that "n" is between 0.92 and 0.98. This is the range that is possible without fine-tuning of the parameters related to energy. From Planck data it can be inferred that "n"=0.968 ± 0.006, and a tensor to scalar ratio that is less than 0.11. These are considered an important confirmation of the theory of inflation.

Various inflation theories have been proposed that make radically different predictions, but they generally have much more fine tuning than should be necessary. As a physical model, however, inflation is most valuable in that it robustly predicts the initial conditions of the Universe based on only two adjustable parameters: the spectral index (that can only change in a small range) and the amplitude of the perturbations. Except in contrived models, this is true regardless of how inflation is realized in particle physics.

Occasionally, effects are observed that appear to contradict the simplest models of inflation. The first-year WMAP data suggested that the spectrum might not be nearly scale-invariant, but might instead have a slight curvature. However, the third-year data revealed that the effect was a statistical anomaly. Another effect remarked upon since the first cosmic microwave background satellite, the Cosmic Background Explorer is that the amplitude of the quadrupole moment of the CMB is unexpectedly low and the other low multipoles appear to be preferentially aligned with the ecliptic plane. Some have claimed that this is a signature of non-Gaussianity and thus contradicts the simplest models of inflation. Others have suggested that the effect may be due to other new physics, foreground contamination, or even publication bias.

An experimental program is underway to further test inflation with more precise CMB measurements. In particular, high precision measurements of the so-called "B-modes" of the polarization of the background radiation could provide evidence of the gravitational radiation produced by inflation, and could also show whether the energy scale of inflation predicted by the simplest models (10–10 GeV) is correct. In March 2014, the BICEP2 team announced B-mode CMB polarization confirming inflation had been demonstrated. The team announced the tensor-to-scalar power ratio formula_5 was between 0.15 and 0.27 (rejecting the null hypothesis; formula_5 is expected to be 0 in the absence of inflation). However, on 19 June 2014, lowered confidence in confirming the findings was reported; on 19 September 2014, a further reduction in confidence was reported and, on 30 January 2015, even less confidence yet was reported. By 2018, additional data suggested, with 95% confidence, that formula_5 is 0.06 or lower: consistent with the null hypothesis, but still also consistent with many remaining models of inflation.

Other potentially corroborating measurements are expected from the Planck spacecraft, although it is unclear if the signal will be visible, or if contamination from foreground sources will interfere. Other forthcoming measurements, such as those of 21 centimeter radiation (radiation emitted and absorbed from neutral hydrogen before the first stars formed), may measure the power spectrum with even greater resolution than the CMB and galaxy surveys, although it is not known if these measurements will be possible or if interference with radio sources on Earth and in the galaxy will be too great.

In Guth's early proposal, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field although the recent discovery of the Higgs boson has increased the number of works considering the Higgs field as inflaton. One problem of this identification is the current tension with experimental data at the electroweak scale, which is currently under study at the Large Hadron Collider (LHC). Other models of inflation relied on the properties of Grand Unified Theories. Since the simplest models of grand unification have failed, it is now thought by many physicists that inflation will be included in a supersymmetric theory such as string theory or a supersymmetric grand unified theory. At present, while inflation is understood principally by its detailed predictions of the initial conditions for the hot early universe, the particle physics is largely "ad hoc" modelling. As such, although predictions of inflation have been consistent with the results of observational tests, many open questions remain.

One of the most severe challenges for inflation arises from the need for fine tuning. In new inflation, the "slow-roll conditions" must be satisfied for inflation to occur. The slow-roll conditions say that the inflaton potential must be flat (compared to the large vacuum energy) and that the inflaton particles must have a small mass. New inflation requires the Universe to have a scalar field with an especially flat potential and special initial conditions. However, explanations for these fine-tunings have been proposed. For example, classically scale invariant field theories, where scale invariance is broken by quantum effects, provide an explanation of the flatness of inflationary potentials, as long as the theory can be studied through perturbation theory.

Linde proposed a theory known as "chaotic inflation" in which he suggested that the conditions for inflation were actually satisfied quite generically. Inflation will occur in virtually any universe that begins in a chaotic, high energy state that has a scalar field with unbounded potential energy. However, in his model the inflaton field necessarily takes values larger than one Planck unit: for this reason, these are often called "large field" models and the competing new inflation models are called "small field" models. In this situation, the predictions of effective field theory are thought to be invalid, as renormalization should cause large corrections that could prevent inflation. This problem has not yet been resolved and some cosmologists argue that the small field models, in which inflation can occur at a much lower energy scale, are better models. While inflation depends on quantum field theory (and the semiclassical approximation to quantum gravity) in an important way, it has not been completely reconciled with these theories.

Brandenberger commented on fine-tuning in another situation. The amplitude of the primordial inhomogeneities produced in inflation is directly tied to the energy scale of inflation. This scale is suggested to be around 10 GeV or 10 times the Planck energy. The natural scale is naïvely the Planck scale so this small value could be seen as another form of fine-tuning (called a hierarchy problem): the energy density given by the scalar potential is down by 10 compared to the Planck density. This is not usually considered to be a critical problem, however, because the scale of inflation corresponds naturally to the scale of gauge unification.

In many models, the inflationary phase of the Universe's expansion lasts forever in at least some regions of the Universe. This occurs because inflating regions expand very rapidly, reproducing themselves. Unless the rate of decay to the non-inflating phase is sufficiently fast, new inflating regions are produced more rapidly than non-inflating regions. In such models, most of the volume of the Universe is continuously inflating at any given time.

All models of eternal inflation produce an infinite, hypothetical multiverse, typically a fractal. The multiverse theory has created significant dissension in the scientific community about the viability of the inflationary model.

Paul Steinhardt, one of the original architects of the inflationary model, introduced the first example of eternal inflation in 1983. He showed that the inflation could proceed forever by producing bubbles of non-inflating space filled with hot matter and radiation surrounded by empty space that continues to inflate. The bubbles could not grow fast enough to keep up with the inflation. Later that same year, Alexander Vilenkin showed that eternal inflation is generic.

Although new inflation is classically rolling down the potential, quantum fluctuations can sometimes lift it to previous levels. These regions in which the inflaton fluctuates upwards expand much faster than regions in which the inflaton has a lower potential energy, and tend to dominate in terms of physical volume. It has been shown that any inflationary theory with an unbounded potential is eternal. There are well-known theorems that this steady state cannot continue forever into the past. Inflationary spacetime, which is similar to de Sitter space, is incomplete without a contracting region. However, unlike de Sitter space, fluctuations in a contracting inflationary space collapse to form a gravitational singularity, a point where densities become infinite. Therefore, it is necessary to have a theory for the Universe's initial conditions.

In eternal inflation, regions with inflation have an exponentially growing volume, while regions that are not inflating don't. This suggests that the volume of the inflating part of the Universe in the global picture is always unimaginably larger than the part that has stopped inflating, even though inflation eventually ends as seen by any single pre-inflationary observer. Scientists disagree about how to assign a probability distribution to this hypothetical anthropic landscape. If the probability of different regions is counted by volume, one should expect that inflation will never end or applying boundary conditions that a local observer exists to observe it, that inflation will end as late as possible.

Some physicists believe this paradox can be resolved by weighting observers by their pre-inflationary volume. Others believe that there is no resolution to the paradox and that the multiverse is a critical flaw in the inflationary paradigm. Paul Steinhardt, who first introduced the eternal inflationary model, later became one of its most vocal critics for this reason.

Some physicists have tried to avoid the initial conditions problem by proposing models for an eternally inflating universe with no origin. These models propose that while the Universe, on the largest scales, expands exponentially it was, is and always will be, spatially infinite and has existed, and will exist, forever.

Other proposals attempt to describe the ex nihilo creation of the Universe based on quantum cosmology and the following inflation. Vilenkin put forth one such scenario. Hartle and Hawking offered the no-boundary proposal for the initial creation of the Universe in which inflation comes about naturally.

Guth described the inflationary universe as the "ultimate free lunch": new universes, similar to our own, are continually produced in a vast inflating background. Gravitational interactions, in this case, circumvent (but do not violate) the first law of thermodynamics (energy conservation) and the second law of thermodynamics (entropy and the arrow of time problem). However, while there is consensus that this solves the initial conditions problem, some have disputed this, as it is much more likely that the Universe came about by a quantum fluctuation. Don Page was an outspoken critic of inflation because of this anomaly. He stressed that the thermodynamic arrow of time necessitates low entropy initial conditions, which would be highly unlikely. According to them, rather than solving this problem, the inflation theory aggravates it – the reheating at the end of the inflation era increases entropy, making it necessary for the initial state of the Universe to be even more orderly than in other Big Bang theories with no inflation phase.

Hawking and Page later found ambiguous results when they attempted to compute the probability of inflation in the Hartle-Hawking initial state. Other authors have argued that, since inflation is eternal, the probability doesn't matter as long as it is not precisely zero: once it starts, inflation perpetuates itself and quickly dominates the Universe. However, Albrecht and Lorenzo Sorbo argued that the probability of an inflationary cosmos, consistent with today's observations, emerging by a random fluctuation from some pre-existent state is much higher than that of a non-inflationary cosmos. This is because the "seed" amount of non-gravitational energy required for the inflationary cosmos is so much less than that for a non-inflationary alternative, which outweighs any entropic considerations.

Another problem that has occasionally been mentioned is the trans-Planckian problem or trans-Planckian effects. Since the energy scale of inflation and the Planck scale are relatively close, some of the quantum fluctuations that have made up the structure in our universe were smaller than the Planck length before inflation. Therefore, there ought to be corrections from Planck-scale physics, in particular the unknown quantum theory of gravity. Some disagreement remains about the magnitude of this effect: about whether it is just on the threshold of detectability or completely undetectable.

Another kind of inflation, called "hybrid inflation", is an extension of new inflation. It introduces additional scalar fields, so that while one of the scalar fields is responsible for normal slow roll inflation, another triggers the end of inflation: when inflation has continued for sufficiently long, it becomes favorable to the second field to decay into a much lower energy state.

In hybrid inflation, one scalar field is responsible for most of the energy density (thus determining the rate of expansion), while another is responsible for the slow roll (thus determining the period of inflation and its termination). Thus fluctuations in the former inflaton would not affect inflation termination, while fluctuations in the latter would not affect the rate of expansion. Therefore, hybrid inflation is not eternal. When the second (slow-rolling) inflaton reaches the bottom of its potential, it changes the location of the minimum of the first inflaton's potential, which leads to a fast roll of the inflaton down its potential, leading to termination of inflation.

Dark energy is broadly similar to inflation and is thought to be causing the expansion of the present-day universe to accelerate. However, the energy scale of dark energy is much lower, 10 GeV, roughly 27 orders of magnitude less than the scale of inflation.

The discovery of flux compactifications opened the way for reconciling inflation and string theory. "Brane inflation" suggests that inflation arises from the motion of D-branes in the compactified geometry, usually towards a stack of anti-D-branes. This theory, governed by the "Dirac-Born-Infeld action", is different from ordinary inflation. The dynamics are not completely understood. It appears that special conditions are necessary since inflation occurs in tunneling between two vacua in the string landscape. The process of tunneling between two vacua is a form of old inflation, but new inflation must then occur by some other mechanism.

When investigating the effects the theory of loop quantum gravity would have on cosmology, a loop quantum cosmology model has evolved that provides a possible mechanism for cosmological inflation. Loop quantum gravity assumes a quantized spacetime. If the energy density is larger than can be held by the quantized spacetime, it is thought to bounce back.

Other models explain some of the observations explained by inflation. However none of these "alternatives" has the same breadth of explanation and still require inflation for a more complete fit with observation. They should therefore be regarded as adjuncts to inflation, rather than as alternatives.

The big bounce hypothesis attempts to replace the cosmic singularity with a cosmic contraction and bounce, thereby explaining the initial conditions that led to the big bang. The flatness and horizon problems are naturally solved in the Einstein-Cartan-Sciama-Kibble theory of gravity, without needing an exotic form of matter or free parameters. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. The minimal coupling between torsion and Dirac spinors generates a spin-spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical Big Bang singularity, replacing it with a cusp-like bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the Big Bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.

The ekpyrotic and cyclic models are also considered adjuncts to inflation. These models solve the horizon problem through an expanding epoch well "before" the Big Bang, and then generate the required spectrum of primordial density perturbations during a contracting phase leading to a Big Crunch. The Universe passes through the Big Crunch and emerges in a hot Big Bang phase. In this sense they are reminiscent of Richard Chace Tolman's oscillatory universe; in Tolman's model, however, the total age of the Universe is necessarily finite, while in these models this is not necessarily so. Whether the correct spectrum of density fluctuations can be produced, and whether the Universe can successfully navigate the Big Bang/Big Crunch transition, remains a topic of controversy and current research. Ekpyrotic models avoid the magnetic monopole problem as long as the temperature at the Big Crunch/Big Bang transition remains below the Grand Unified Scale, as this is the temperature required to produce magnetic monopoles in the first place. As things stand, there is no evidence of any 'slowing down' of the expansion, but this is not surprising as each cycle is expected to last on the order of a trillion years.

Another adjunct, the varying speed of light model was offered by Jean-Pierre Petit in 1988, John Moffat in 1992, and the two-man team of Andreas Albrecht and João Magueijo in 1998. Instead of superluminal expansion the speed of light was 60 orders of magnitude faster than its current value solving the horizon and homogeneity problems in the early universe.

String theory requires that, in addition to the three observable spatial dimensions, additional dimensions exist that are curled up or compactified (see also Kaluza–Klein theory). Extra dimensions appear as a frequent component of supergravity models and other approaches to quantum gravity. This raised the contingent question of why four space-time dimensions became large and the rest became unobservably small. An attempt to address this question, called "string gas cosmology", was proposed by Robert Brandenberger and Cumrun Vafa. This model focuses on the dynamics of the early universe considered as a hot gas of strings. Brandenberger and Vafa show that a dimension of spacetime can only expand if the strings that wind around it can efficiently annihilate each other. Each string is a one-dimensional object, and the largest number of dimensions in which two strings will generically intersect (and, presumably, annihilate) is three. Therefore, the most likely number of non-compact (large) spatial dimensions is three. Current work on this model centers on whether it can succeed in stabilizing the size of the compactified dimensions and produce the correct spectrum of primordial density perturbations. Supporters admit that their model "does not solve the entropy and flatness problems of standard cosmology ... and we can provide no explanation for why the current universe is so close to being spatially flat".

Since its introduction by Alan Guth in 1980, the inflationary paradigm has become widely accepted. Nevertheless, many physicists, mathematicians, and philosophers of science have voiced criticisms, claiming untestable predictions and a lack of serious empirical support. In 1999, John Earman and Jesús Mosterín published a thorough critical review of inflationary cosmology, concluding, "we do not think that there are, as yet, good grounds for admitting any of the models of inflation into the standard core of cosmology."

In order to work, and as pointed out by Roger Penrose from 1986 on, inflation requires extremely specific initial conditions of its own, so that the problem (or pseudo-problem) of initial conditions is not solved: "There is something fundamentally misconceived about trying to explain the uniformity of the early universe as resulting from a thermalization process. [...] For, if the thermalization is actually doing anything [...] then it represents a definite increasing of the entropy. Thus, the universe would have been even more special before the thermalization than after." The problem of specific or "fine-tuned" initial conditions would not have been solved; it would have gotten worse. At a conference in 2015, Penrose said that "inflation isn't falsifiable, it's falsified. [...] BICEP did a wonderful service by bringing all the Inflation-ists out of their shell, and giving them a black eye."

A recurrent criticism of inflation is that the invoked inflaton field does not correspond to any known physical field, and that its potential energy curve seems to be an ad hoc contrivance to accommodate almost any data obtainable. Paul Steinhardt, one of the founding fathers of inflationary cosmology, has recently become one of its sharpest critics. He calls 'bad inflation' a period of accelerated expansion whose outcome conflicts with observations, and 'good inflation' one compatible with them: "Not only is bad inflation more likely than good inflation, but no inflation is more likely than either [...] Roger Penrose considered all the possible configurations of the inflaton and gravitational fields. Some of these configurations lead to inflation [...] Other configurations lead to a uniform, flat universe directly – without inflation. Obtaining a flat universe is unlikely overall. Penrose's shocking conclusion, though, was that obtaining a flat universe without inflation is much more likely than with inflation – by a factor of 10 to the googol (10 to the 100) power!" Together with Anna Ijjas and Abraham Loeb, he wrote articles claiming that the inflationary paradigm is in trouble in view of the data from the Planck satellite. Counter-arguments were presented by Alan Guth, David Kaiser, and Yasunori Nomura and by Andrei Linde, saying that "cosmic inflation is on a stronger footing than ever before".





</doc>
<doc id="5385" url="https://en.wikipedia.org/wiki?curid=5385" title="Candela">
Candela

The candela ( or ; symbol: cd) is the base unit of luminous intensity in the International System of Units (SI); that is, luminous power per unit solid angle emitted by a point light source in a particular direction. Luminous intensity is analogous to radiant intensity, but instead of simply adding up the contributions of every wavelength of light in the source's spectrum, the contribution of each wavelength is weighted by the standard luminosity function (a model of the sensitivity of the human eye to different wavelengths). A common wax candle emits light with a luminous intensity of roughly one candela. If emission in some directions is blocked by an opaque barrier, the emission would still be approximately one candela in the directions that are not obscured.

The word "candela" is Latin for "candle". The old name "candle" is still sometimes used, as in "foot-candle" and the modern definition of "candlepower".

The 26th General Conference on Weights and Measures (CGPM) redefined the candela in 2018. The new definition, which took effect on 20 May 2019, is:

The candela [...] is defined by taking the fixed numerical value of the luminous efficacy of monochromatic radiation of frequency 540 × 10 Hz, "K", to be 683 when expressed in the unit lm W, which is equal to , or , where the kilogram, metre and second are defined in terms of "h", "c" and Δ"ν".

The frequency chosen is in the visible spectrum near green, corresponding to a wavelength of about 555 nanometres. The human eye, when adapted for bright conditions, is most sensitive near this frequency. At other frequencies, more radiant intensity is required to achieve the same luminous intensity, according to the frequency response of the human eye. The luminous intensity for light of a particular wavelength "λ" is given by
where "I"("λ") is the luminous intensity, "I"("λ") is the radiant intensity and formula_2 is the photopic luminosity function. If more than one wavelength is present (as is usually the case), one must integrate over the spectrum of wavelengths to get the total luminous intensity.


Prior to 1948, various standards for luminous intensity were in use in a number of countries. These were typically based on the brightness of the flame from a "standard candle" of defined composition, or the brightness of an incandescent filament of specific design. One of the best-known of these was the English standard of candlepower. One candlepower was the light produced by a pure spermaceti candle weighing one sixth of a pound and burning at a rate of 120 grains per hour. Germany, Austria and Scandinavia used the Hefnerkerze, a unit based on the output of a Hefner lamp.

It became clear that a better-defined unit was needed. Jules Violle had proposed a standard based on the light emitted by 1 cm of platinum at its melting point (or freezing point), calling this the Violle. The light intensity was due to the Planck radiator (a black body) effect, and was thus independent of the construction of the device. This made it easy for anyone to measure the standard, as high-purity platinum was widely available and easily prepared.

The Commission Internationale de l'Éclairage (International Commission on Illumination) and the CIPM proposed a “new candle” based on this basic concept. However, the value of the new unit was chosen to make it similar to the earlier unit candlepower by dividing the Violle by 60. The decision was promulgated by the CIPM in 1946:
The value of the new candle is such that the brightness of the full radiator at the temperature of solidification of platinum is 60 new candles per square centimetre.

It was then ratified in 1948 by the 9th CGPM which adopted a new name for this unit, the "candela". In 1967 the 13th CGPM removed the term "new candle" and gave an amended version of the candela definition, specifying the atmospheric pressure applied to the freezing platinum:
The candela is the luminous intensity, in the perpendicular direction, of a surface of square metre of a black body at the temperature of freezing platinum under a pressure of  newtons per square metre.

In 1979, because of the difficulties in realizing a Planck radiator at high temperatures and the new possibilities offered by radiometry, the 16th CGPM adopted a new definition of the candela:
The candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency and that has a radiant intensity in that direction of  watt per steradian.
The definition describes how to produce a light source that (by definition) emits one candela, but does not specify the luminosity function for weighting radiation at other frequencies. Such a source could then be used to calibrate instruments designed to measure luminous intensity with reference to a specified luminosity function. An appendix to the SI Brochure makes it clear that the luminosity function is not uniquely specified, but must be selected to fully define the candela.

The arbitrary (1/683) term was chosen so that the new definition would precisely match the old definition. Although the candela is now defined in terms of the second (an SI base unit) and the watt (a derived SI unit), the candela remains a base unit of the SI system, by definition.

The 26th CGPM approved the modern definition of the candela in 2018 as part of the 2019 redefinition of SI base units, which redefined the SI base units in terms of fundamental physical constants.

If a source emits a known luminous intensity "I" (in candelas) in a well-defined cone, the total luminous flux "Φ" in lumens is given by

where "A" is the "radiation angle" of the lamp—the full vertex angle of the emission cone. For example, a lamp that emits 590 cd with a radiation angle of 40° emits about 224 lumens. See MR16 for emission angles of some common lamps.

If the source emits light uniformly in all directions, the flux can be found by multiplying the intensity by 4π: a uniform 1 candela source emits 12.6 lumens.

For the purpose of measuring illumination, the candela is not a practical unit, as it only applies to idealized point light sources, each approximated by a source small compared to the distance from which its luminous radiation is measured, also assuming that it is done so in the absence of other light sources. What gets directly measured by a light meter is incident light on a sensor of finite area, i.e. illuminance in lm/m (lux). However, if designing illumination from many point light sources, like light bulbs, of known approximate omnidirectionally-uniform intensities, the contributions to illuminance from incoherent light being additive, it is mathematically estimated as follows. If r is the position of the "i"-th source of uniform intensity "I", and â is the unit vector normal to the illuminated elemental opaque area "dA" being measured, and provided that all light sources lie in the same half-space divided by the plane of this area,

In the case of a single point light source of intensity "I", at a distance "r" and normally incident, this reduces to


</doc>
<doc id="5387" url="https://en.wikipedia.org/wiki?curid=5387" title="Condensed matter physics">
Condensed matter physics

Condensed matter physics is the field of physics that deals with the macroscopic and microscopic physical properties of matter. In particular it is concerned with the "condensed" phases that appear whenever the number of constituents in a system is extremely large and the interactions between the constituents are strong. The most familiar examples of condensed phases are solids and liquids, which arise from the electromagnetic forces between atoms. Condensed matter physicists seek to understand the behavior of these phases by using physical laws. In particular, they include the laws of quantum mechanics, electromagnetism and statistical mechanics.

More exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature, the ferromagnetic and antiferromagnetic phases of spins on crystal lattices of atoms, and the Bose–Einstein condensate found in ultracold atomic systems. The study of condensed matter physics involves measuring various material properties via experimental probes along with using methods of theoretical physics to develop mathematical models that help in understanding physical behavior.

The diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists self-identify as condensed matter physicists, and the Division of Condensed Matter Physics is the largest division at the American Physical Society. The field overlaps with chemistry, materials science, and nanotechnology, and relates closely to atomic physics and biophysics. The theoretical physics of condensed matter shares important concepts and methods with that of particle physics and nuclear physics.

A variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as "solid state physics". Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the new, related specialty of condensed matter physics. According to physicist Philip Warren Anderson, the term was coined by him and Volker Heine, when they changed the name of their group at the Cavendish Laboratories, Cambridge from "Solid state theory" to "Theory of Condensed Matter" in 1967, as they felt it did not exclude their interests in the study of liquids, nuclear matter, and so on. Although Anderson and Heine helped popularize the name "condensed matter", it had been present in Europe for some years, most prominently in the form of a journal published in English, French, and German by Springer-Verlag titled "Physics of Condensed Matter", which was launched in 1963. The funding environment and Cold War politics of the 1960s and 1970s were also factors that lead some physicists to prefer the name "condensed matter physics", which emphasized the commonality of scientific problems encountered by physicists working on solids, liquids, plasmas, and other complex matter, over "solid state physics", which was often associated with the industrial applications of metals and semiconductors. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics.

References to "condensed" state can be traced to earlier sources. For example, in the introduction to his 1947 book "Kinetic Theory of Liquids", Yakov Frenkel proposed that "The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact, it would be more correct to unify them under the title of 'condensed bodies'".

One of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the nineteenth century. Davy observed that of the forty chemical elements known at the time, twenty-six had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in John Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.

In 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, except for nitrogen, hydrogen, and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and Heike Kamerlingh Onnes were successfully able to liquefy hydrogen and then newly discovered helium, respectively.

Paul Drude in 1900 proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann–Franz law. However, despite the success of Drude's free electron model, it had one notable problem: it was unable to correctly explain the electronic contribution to the specific heat and magnetic properties of metals, and the temperature dependence of resistivity at low temperatures.

In 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that "with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas."

Drude's classical model was augmented by Wolfgang Pauli, Arnold Sommerfeld, Felix Bloch and other physicists. Pauli realized that the free electrons in metal must obey the Fermi–Dirac statistics. Using this idea, he developed the theory of paramagnetism in 1926. Shortly after, Sommerfeld incorporated the Fermi–Dirac statistics into the free electron model and made it better to explain the heat capacity. Two years later, Bloch used quantum mechanics to describe the motion of an electron in a periodic lattice. The mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series "International Tables of Crystallography", first published in 1935. Band structure calculations was first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.
In 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered a voltage developed across conductors transverse to an electric current in the conductor and magnetic field perpendicular to the current. This phenomenon arising due to the nature of charge carriers in the conductor came to be termed the Hall effect, but it was not properly explained at the time, since the electron was not experimentally discovered until 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 developed the theory of Landau quantization and laid the foundation for the theoretical explanation for the quantum Hall effect discovered half a century later.

Magnetism as a property of matter has been known in China since 4000 BC. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included classifying materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization cannot occur in one dimension but is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and Néel on antiferromagnetism led to developing new magnetic materials with applications to magnetic storage devices.

The Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective excitation modes of solids and the important notion of a quasiparticle. Russian physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now termed Landau-quasiparticles. Landau also developed a mean field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1965, John Bardeen, Leon Cooper and John Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons of opposite spin mediated by phonons in the lattice can give rise to a bound state called a Cooper pair.
The study of phase transition and the critical behavior of observables, termed critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and widom scaling. These ideas were unified by Kenneth G. Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.

The quantum Hall effect was discovered by Klaus von Klitzing in 1980 when he observed the Hall conductance to be integer multiples of a fundamental constant formula_1.(see figure) The effect was observed to be independent of parameters such as system size and impurities. In 1981, theorist Robert Laughlin proposed a theory explaining the unanticipated precision of the integral plateau. It also implied that the Hall conductance can be characterized in terms of a topological invariable called Chern number. Shortly after, in 1982, Horst Störmer and Daniel Tsui observed the fractional quantum Hall effect where the conductance was now a rational multiple of a constant. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational method solution, named the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research.
In 1986, Karl Müller and Johannes Bednorz discovered the first high temperature superconductor, a material which was superconducting at temperatures as high as 50 kelvins. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron–electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.
In 2009, David Field and researchers at Aarhus University discovered spontaneous electric fields when creating prosaic films of various gases. This has more recently expanded to form the research area of spontelectrics.

In 2012 several groups released preprints which suggest that samarium hexaboride has the properties of a topological insulator in accord with the earlier theoretical predictions. Since samarium hexaboride is an established Kondo insulator, i.e. a strongly correlated electron material, the existence of a topological surface state in this material would lead to a topological insulator with strong electronic correlations.

Theoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg–Landau theory, critical exponents and the use of mathematical methods of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases, and gauge symmetries.

Theoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are understood poorly, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum aluminate-strontium titanate interface, where two non-magnetic insulators are joined to create conductivity, superconductivity, and ferromagnetism.

The metallic state has historically been an important building block for studying properties of solids. The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. He was able to derive the empirical Wiedemann-Franz law and get results in close agreement with the experiments. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi–Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann–Franz law. In 1912, The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. In 1928, Swiss physicist Felix Bloch provided a wave function solution to the Schrödinger equation with a periodic potential, called the Bloch wave.

Calculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation methods are needed to obtain meaningful predictions. The Thomas–Fermi theory, developed in the 1920s, was used to estimate system energy and electronic density by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree–Fock wavefunction as an improvement over the Thomas–Fermi model. The Hartree–Fock method accounted for exchange statistics of single particle electron wavefunctions. In general, it's very difficult to solve the Hartree–Fock equation. Only the free electron gas case can be solved exactly. Finally in 1964–65, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory which gave realistic descriptions for bulk and surface properties of metals. The density functional theory (DFT) has been widely used since the 1970s for band structure calculations of variety of solids.

Some states of matter exhibit "symmetry breaking", where the relevant laws of physics possess some form of symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) phase rotational symmetry.

Goldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.

Phase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature. Classical phase transition occurs at finite temperature when the order of the system was destroyed. For example, when ice melts and becomes water, the ordered crystal structure is destroyed. 

In quantum phase transitions, the temperature is set to absolute zero, and the non-thermal control parameter, such as pressure or magnetic field, causes the phase transitions when order is destroyed by quantum fluctuations originating from the Heisenberg uncertainty principle. Here, the different quantum phases of the system refer to distinct ground states of the Hamiltonian matrix. Understanding the behavior of quantum phase transition is important in the difficult tasks of explaining the properties of rare-earth magnetic insulators, high-temperature superconductors, and other substances.

Two classes of phase transitions occur: "first-order transitions" and "second-order" or "continuous transitions". For the latter, the two phases involved do not co-exist at the transition temperature, also called the critical point. Near the critical point, systems undergo critical behavior, wherein several of their properties such as correlation length, specific heat, and magnetic susceptibility diverge exponentially. These critical phenomena present serious challenges to physicists because normal macroscopic laws are no longer valid in the region, and novel ideas and methods must be invented to find the new laws that can describe the system.

The simplest theory that can describe continuous phase transitions is the Ginzburg–Landau theory, which works in the so-called mean field approximation. However, it can only roughly explain continuous phase transition for ferroelectrics and type I superconductors which involves long range microscopic interactions. For other types of systems that involves short range interactions near the critical point, a better theory is needed.

Near the critical point, the fluctuations happen over broad range of size scales while the feature of the whole system is scale invariant. Renormalization group methods successively average out the shortest wavelength fluctuations in stages while retaining their effects into the next stage. Thus, the changes of a physical system as viewed at different size scales can be investigated systematically. The methods, together with powerful computer simulation, contribute greatly to the explanation of the critical phenomena associated with continuous phase transition.

Experimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Such probes include effects of electric and magnetic fields, measuring response functions, transport properties and thermometry. Commonly used experimental methods include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measuring transport via thermal and heat conduction.
Several condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 electron volt (eV) and is used as a scattering probe to measure variations in material properties such as dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density.

Neutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy. 

In experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a method by which external magnetic fields are used to find resonance modes of individual electrons, thus giving information about the atomic, molecular, and bond structure of their neighborhood. NMR experiments can be made in magnetic fields with strengths up to 60 Tesla. Higher magnetic fields can improve the quality of NMR measurement data. Quantum oscillations is another experimental method where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. High magnetic fields will be useful in experimentally testing of the various theoretical predictions such as the quantized magnetoelectric effect, image magnetic monopole, and the half-integer quantum Hall effect.

The local structure, the structure of the nearest neighbour atoms, of condensed matter can be investigated with methods of nuclear spectroscopy, which are very sensitive to small changes. Using specific and radioactive nuclei, the nucleus becomes the probe that interacts with its sourrounding electric and magnetic fields (hyperfine interactions). The methods are suitable to study defects, diffusion, phase change, magnetism. Common methods are e.g. NMR, Mössbauer spectroscopy, or perturbed angular correlation (PAC). Especially PAC is ideal for the study of phase changes at extreme temperature above 2000°C due to no temperature dependence of the method.

Ultracold atom trapping in optical lattices is an experimental tool commonly used in condensed matter physics, and in atomic, molecular, and optical physics. The method involves using optical lasers to form an interference pattern, which acts as a "lattice", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as "quantum simulators", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters, and to study phase transitions for antiferromagnetic and spin liquid ordering.

In 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose–Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy one quantum state.

Research in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, laser technology, and several phenomena studied in the context of nanotechnology. Methods such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication.

In quantum computation, information is represented by quantum bits, or qubits. The qubits may decohere quickly before useful computation is completed. This serious problem must be solved before quantum computing may be realized. To solve this problem, several promising approaches are proposed in condensed matter physics, including Josephson junction qubits, spintronic qubits using the spin orientation of magnetic materials, or the topological non-Abelian anyons from fractional quantum Hall effect states.

Condensed matter physics also has important uses for biophysics, for example, the experimental method of magnetic resonance imaging, which is widely used in medical diagnosis.



</doc>
<doc id="5388" url="https://en.wikipedia.org/wiki?curid=5388" title="Cultural anthropology">
Cultural anthropology

Cultural anthropology is a branch of anthropology focused on the study of cultural variation among humans. It is in contrast to social anthropology, which perceives cultural variation as a subset of the anthropological constant.

Cultural anthropology has a rich methodology, including participant observation (often called fieldwork because it requires the anthropologist spending an extended period of time at the research location), interviews, and surveys.
One of the earliest articulations of the anthropological meaning of the term "culture" came from Sir Edward Tylor who writes on the first page of his 1871 book: "Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society." The term "civilization" later gave way to definitions given by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.

The anthropological concept of "culture" reflects in part a reaction against earlier Western discourses based on an opposition between "culture" and "nature", according to which some human beings lived in a "state of nature". Anthropologists have argued that culture "is" "human nature", and that all people have a capacity to classify experiences, encode classifications symbolically (i.e. in language), and teach such abstractions to others.

Since humans acquire culture through the learning processes of enculturation and socialization, people living in different places or different circumstances develop different cultures. Anthropologists have also pointed out that through culture people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).

The rise of cultural anthropology took place within the context of the late 19th century, when questions regarding which cultures were "primitive" and which were "civilized" occupied the minds of not only Marx and Freud, but many others. Colonialism and its processes increasingly brought European thinkers into direct or indirect contact with "primitive others." The relative status of various humans, some of whom had modern advanced technologies that included engines and telegraphs, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle, was of interest to the first generation of cultural anthropologists.

Parallel with the rise of cultural anthropology in the United States, social anthropology, in which "sociality" is the central concept and which focuses on the study of social statuses and roles, groups, institutions, and the relations among them—developed as an academic discipline in Britain and in France. The umbrella term socio-cultural anthropology draws upon both cultural and social anthropology traditions.

Anthropology is concerned with the lives of people in different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must have learned from one another somehow, however indirectly; in other words, they argued that cultural traits spread from one place to another, or "diffused".
Other ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated "independent invention", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.

20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw "diffusion" and "independent invention" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so highlighting the role of Ethics in modern anthropology.

Accordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of "cultural relativism", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which he or she lived or lives.

Others, such as Claude Lévi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.

Cultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: "...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes." Although Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race. Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.

Cultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.

Boas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography: basically, they advocated living with people of another culture for an extended period of time, so that they could learn the local language and be enculturated, at least partially, into that culture. In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, "Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation."



Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from "savagery", to "barbarism", to "civilization". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.

Franz Boas (1858–1942) established academic anthropology in the United States in opposition to Morgan's evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.

Influenced by the German tradition, Boas argued that the world was full of distinct "cultures," rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.

In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.

Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.

The publication of Alfred Kroeber's textbook "Anthropology" (1923) marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.

Though such works as Mead's "Coming of Age in Samoa" (1928) and Benedict's "The Chrysanthemum and the Sword" (1946) remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined in favor of Ralph Linton, and Mead was limited to her offices at the AMNH.

In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris.

Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis.

In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as "Reinventing Anthropology" worried about anthropology's relevance.

Since the 1980s issues of power, such as those examined in Eric Wolf's "Europe and the People Without History", have been central to the discipline. In the 1980s books like "Anthropology and the Colonial Encounter" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins, who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between symbolic meaning, sociocultural structure, and individual agency in the processes of historical transformation. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.

Many anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:

Geertz's interpretive method involved what he called "thick description." The cultural symbols of rituals, political and economic action, and of kinship, are "read" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the "experience-near" but foreign concepts of the other culture, into the "experience-distant" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of "religion as a cultural system" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential. Schneider demonstrated that the American folk-cultural emphasis on "blood connections" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic but a cultural relationship established on very different terms in different societies.

Prominent British symbolic anthropologists include Victor Turner and Mary Douglas.

In the late 1980s and 1990s authors such as James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theory and methods became "de rigueur" as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive, explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.



Modern cultural anthropology has its origins in, and developed in reaction to, 19th century ethnology, which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others – usually missionaries, traders, explorers, or colonial officials – earning them the moniker of "arm-chair anthropologists".

Participant observation is one of the principle research methods of cultural anthropology. It relies on the assumption that the best way to understand a group of people is to interact with them closely over a long period of time. The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of Sociology. Historically, the group of people being studied was a small, non-Western society. However, today it may be a specific corporation, a church group, a sports team, or a small town. There are no restrictions as to what the subject of participant observation can be, as long as the group of people is studied intimately by the observing anthropologist over a long period of time. This allows the anthropologist to develop trusting relationships with the subjects of study and receive an inside perspective on the culture, which helps him or her to give a richer description when writing about the culture later. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time, and researchers can discover discrepancies between what participants say—and often believe—should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.

Interactions between an ethnographer and a cultural informant must go both ways. Just as an ethnographer may be naive or curious about a culture, the members of that culture may be curious about the ethnographer. To establish connections that will eventually lead to a better understanding of the cultural context of a situation, an anthropologist must be open to becoming part of the group, and willing to develop meaningful relationships with its members. One way to do this is to find a small area of common experience between an anthropologist and his or her subjects, and then to expand from this common ground into the larger area of difference. Once a single connection has been established, it becomes easier to integrate into the community, and more likely that accurate and complete information is being shared with the anthropologist.

Before participant observation can begin, an anthropologist must choose both a location and a focus of study. This focus may change once the anthropologist is actively observing the chosen group of people, but having an idea of what one wants to study before beginning fieldwork allows an anthropologist to spend time researching background information on their topic. It can also be helpful to know what previous research has been conducted in one's chosen location or on similar topics, and if the participant observation takes place in a location where the spoken language is not one the anthropologist is familiar with, he or she will usually also learn that language. This allows the anthropologist to become better established in the community. The lack of need for a translator makes communication more direct, and allows the anthropologist to give a richer, more contextualized representation of what they witness. In addition, participant observation often requires permits from governments and research institutions in the area of study, and always needs some form of funding.

The majority of participant observation is based on conversation. This can take the form of casual, friendly dialogue, or can also be a series of more structured interviews. A combination of the two is often used, sometimes along with photography, mapping, artifact collection, and various other methods. In some cases, ethnographers also turn to structured observation, in which an anthropologist's observations are directed by a specific set of questions he or she is trying to answer. In the case of structured observation, an observer might be required to record the order of a series of events, or describe a certain part of the surrounding environment. While the anthropologist still makes an effort to become integrated into the group they are studying, and still participates in the events as they observe, structured observation is more directed and specific than participant observation in general. This helps to standardize the method of study when ethnographic data is being compared across several groups or is needed to fulfill a specific purpose, such as research for a governmental policy decision.

One common criticism of participant observation is its lack of objectivity. Because each anthropologist has his or her own background and set of experiences, each individual is likely to interpret the same culture in a different way. Who the ethnographer is has a lot to do with what he or she will eventually write about a culture, because each researcher is influenced by his or her own perspective. This is considered a problem especially when anthropologists write in the ethnographic present, a present tense which makes a culture seem stuck in time, and ignores the fact that it may have interacted with other cultures or gradually evolved since the anthropologist made observations. To avoid this, past ethnographers have advocated for strict training, or for anthropologists working in teams. However, these approaches have not generally been successful, and modern ethnographers often choose to include their personal experiences and possible biases in their writing instead.

Participant observation has also raised ethical questions, since an anthropologist is in control of what he or she reports about a culture. In terms of representation, an anthropologist has greater power than his or her subjects of study, and this has drawn criticism of participant observation in general. Additionally, anthropologists have struggled with the effect their presence has on a culture. Simply by being present, a researcher causes changes in a culture, and anthropologists continue to question whether or not it is appropriate to influence the cultures they study, or possible to avoid having influence.

In the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.

Numerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.

Bronisław Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown´s students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.

In the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European "social anthropologists" focused on observed social behaviors and on "social structure", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).

American "cultural anthropologists" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors, and have an equal interest in what people do and in what people say.

One means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called "human universals" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not.
The Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF's precursor, the "Cross-Cultural Survey" (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. "eHRAF World Cultures" includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, "eHRAF Archaeology", covers major archaeological traditions and many more sub-traditions and sites around the world.

Comparison across cultures includies the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small scale societies are:
Ethnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.

A growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, "Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.

Also emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a "thing," such as a particular commodity, as it is transported through the networks of global capitalism.

Multi-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.

Sociocultural anthropologists have increasingly turned their investigative eye on to "Western" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for "In Search of Respect", a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.

Kinship refers to the anthropological study of the ways in which humans form and maintain relationships with one another, and further, how those relationships operate within and define social organization.

Research in kinship studies often crosses over into different anthropological subfields including medical, feminist, and public anthropology. This is likely due to its fundamental concepts, as articulated by linguistic anthropologist Patrick McConvell: Kinship is the bedrock of all human societies that we know. All humans recognize fathers and mothers, sons and daughters, brothers and sisters, uncles and aunts, husbands and wives, grandparents, cousins, and often many more complex types of relationships in the terminologies that they use. That is the matrix into which human children are born in the great majority of cases, and their first words are often kinship terms.Throughout history, kinship studies have primarily focused on the topics of marriage, descent, and procreation. Anthropologists have written extensively on the variations within marriage across cultures and its legitimacy as a human institution. There are stark differences between communities in terms of marital practice and value, leaving much room for anthropological fieldwork. For instance, the Nuer of Sudan and the Brahmans of Nepal practice polygyny, where one man has several marriages to two or more women. The Nyar of India and Nyimba of Tibet and Nepal practice polyandry, where one woman is often married to two or more men. The marital practice found in most cultures, however, is monogamy, where one woman is married to one man. Anthropologists also study different marital taboos across cultures, most commonly the incest taboo of marriage within sibling and parent-child relationships. It has been found that all cultures have an incest taboo to some degree, but the taboo shifts between cultures when the marriage extends beyond the nuclear family unit.

There are similar foundational differences where the act of procreation is concerned. Although anthropologists have found that biology is acknowledged in every cultural relationship to procreation, there are differences in the ways in which cultures assess the constructs of parenthood. For example, in the Nuyoo municipality of Oaxaca, Mexico, it is believed that a child can have partible maternity and partible paternity. In this case, a child would have multiple biological mothers in the case that it is born of one woman and then breastfed by another. A child would have multiple biological fathers in the case that the mother had sex with multiple men, following the commonplace belief in Nuyoo culture that pregnancy must be preceded by sex with multiple men in order have the necessary accumulation of semen.

In the twenty-first century, Western ideas of kinship have evolved beyond the traditional assumptions of the nuclear family, raising anthropological questions of consanguinity, lineage, and normative marital expectation. The shift can be traced back to the 1960s, with the reassessment of kinship's basic principles offered by Edmund Leach, Rodney Neeham, David Schneider, and others. Instead of relying on narrow ideas of Western normalcy, kinship studies increasingly catered to "more ethnographic voices, human agency, intersecting power structures, and historical contex". The study of kinship evolved to accommodate for the fact that it cannot be separated from its institutional roots and must pay respect to the society in which it lives, including that society's contradictions, hierarchies, and individual experiences of those within it. This shift was progressed further by the emergence of second-wave feminism in the early 1970s, which introduced ideas of marital oppression, sexual autonomy, and domestic subordination. Other themes that emerged during this time included the frequent comparisons between Eastern and Western kinship systems and the increasing amount of attention paid to anthropologists' own societies, a swift turn from the focus that had traditionally been paid to largely "foreign", non-Western communities.

Kinship studies began to gain mainstream recognition in the late 1990s with the surging popularity of feminist anthropology, particularly with its work related to biological anthropology and the intersectional critique of gender relations. At this time, there was the arrival of "Third World feminism", a movement that argued kinship studies could not examine the gender relations of developing countries in isolation, and must pay respect to racial and economic nuance as well. This critique became relevant, for instance, in the anthropological study of Jamaica: race and class were seen as the primary obstacles to Jamaican liberation from economic imperialism, and gender as an identity was largely ignored. Third World feminism aimed to combat this in the early twenty-first century by promoting these categories as coexisting factors. In Jamaica, marriage as an institution is often substituted for a series of partners, as poor women cannot rely on regular financial contributions in a climate of economic instability. In addition, there is a common practice of Jamaican women artificially lightening their skin tones in order to secure economic survival. These anthropological findings, according to Third World feminism, cannot see gender, racial, or class differences as separate entities, and instead must acknowledge that they interact together to produce unique individual experiences.

Kinship studies have also experienced a rise in the interest of reproductive anthropology with the advancement of assisted reproductive technologies (ARTs), including in vitro fertilization (IVF). These advancements have led to new dimensions of anthropological research, as they challenge the Western standard of biogenetically based kinship, relatedness, and parenthood. According to anthropologists Maria C. Inhorn and Daphna Birenbaum-Carmeli, "ARTs have pluralized notions of relatedness and led to a more dynamic notion of "kinning" namely, kinship as a process, as something under construction, rather than a natural given". With this technology, questions of kinship have emerged over the difference between biological and genetic relatedness, as gestational surrogates can provide a biological environment for the embryo while the genetic ties remain with a third party. If genetic, surrogate, and adoptive maternities are involved, anthropologists have acknowledged that there can be the possibility for three "biological" mothers to a single child. With ARTs, there are also anthropological questions concerning the intersections between wealth and fertility: ARTs are generally only available to those in the highest income bracket, meaning the infertile poor are inherently devalued in the system. There have also been issues of reproductive tourism and bodily commodification, as individuals seek economic security through hormonal stimulation and egg harvesting, which are potentially harmful procedures. With IVF, specifically, there have been many questions of embryotic value and the status of life, particularly as it relates to the manufacturing of stem cells, testing, and research.

Current issues in kinship studies, such as adoption, have revealed and challenged the Western cultural disposition towards the genetic, "blood" tie. Western biases against single parent homes have also been explored through similar anthropological research, uncovering that a household with a single parent experiences "greater levels of scrutiny and [is] routinely seen as the 'other' of the nuclear, patriarchal family". The power dynamics in reproduction, when explored through a comparative analysis of "conventional" and "unconventional" families, have been used to dissect the Western assumptions of child bearing and child rearing in contemporary kinship studies.

Kinship, as an anthropological field of inquiry, has been heavily criticized across the discipline. One critique is that, as its inception, the framework of kinship studies was far too structured and formulaic, relying on dense language and stringent rules. Another critique, explored at length by American anthropologist David Schneider, argues that kinship has been limited by its inherent Western ethnocentrism. Schneider proposes that kinship is not a field that can be applied cross-culturally, as the theory itself relies on European assumptions of normalcy. He states in the widely circulated 1984 book "A critique of the study of kinship" that "[K]inship has been defined by European social scientists, and European social scientists use their own folk culture as the source of many, if not all of their ways of formulating and understanding the world about them". However, this critique has been challenged by the argument that it is linguistics, not cultural divergence, that has allowed for a European bias, and that the bias can be lifted by centering the methodology on fundamental human concepts. Polish anthropologist Anna Wierzbicka argues that "mother" and "father" are examples of such fundamental human concepts, and can only be Westernized when conflated with English concepts such as "parent" and "sibling".

A more recent critique of kinship studies is its solipsistic focus on privileged, Western human relations and its promotion of normative ideals of human exceptionalism. In "Critical Kinship Studies", social psychologists Elizabeth Peel and Damien Riggs argue for a move beyond this human-centered framework, opting instead to explore kinship through a "posthumanist" vantage point where anthropologists focus on the intersecting relationships of human animals, non-human animals, technologies and practices.

The role of anthropology in institutions has expanded significantly since the end of the 20th century. Much of this development can be attributed to the rise in anthropologists working outside of academia and the increasing importance of globalization in both institutions and the field of anthropology. Anthropologists can be employed by institutions such as for-profit business, nonprofit organizations, and governments. For instance, cultural anthropologists are commonly employed by the United States federal government.

The two types of institutions defined in the field of anthropology are total institutions and social institutions. Total institutions are places that comprehensively coordinate the actions of people within them, and examples of total institutions include prisons, convents, and hospitals. Social institutions, on the other hand, are constructs that regulate individuals' day-to-day lives, such as kinship, religion, and economics. Anthropology of institutions may analyze labor unions, businesses ranging from small enterprises to corporations, government, medical organizations, education, prisons, and financial institutions. Nongovernmental organizations have garnered particular interest in the field of institutional anthropology because of they are capable of fulfilling roles previously ignored by governments, or previously realized by families or local groups, in an attempt to mitigate social problems.

The types and methods of scholarship performed in the anthropology of institutions can take a number of forms. Institutional anthropologists may study the relationship between organizations or between an organization and other parts of society. Institutional anthropology may also focus on the inner workings of an institution, such as the relationships, hierarchies and cultures formed, and the ways that these elements are transmitted and maintained, transformed, or abandoned over time. Additionally, some anthropology of institutions examines the specific design of institutions and their corresponding strength. More specifically, anthropologists may analyze specific events within an institution, perform semiotic investigations, or analyze the mechanisms by which knowledge and culture are organized and dispersed.

In all manifestations of institutional anthropology, participant observation is critical to understanding the intricacies of the way an institution works and the consequences of actions taken by individuals within it. Simultaneously, anthropology of institutions extends beyond examination of the commonplace involvement of individuals in institutions to discover how and why the organizational principles evolved in the manner that they did.

Common considerations taken by anthropologists in studying institutions include the physical location at which a researcher places themselves, as important interactions often take place in private, and the fact that the members of an institution are often being examined in their workplace and may not have much idle time to discuss the details of their everyday endeavors. The ability of individuals to present the workings of an institution in a particular light or frame must additionally be taken into account when using interviews and document analysis to understand an institution, as the involvement of an anthropologist may be met with distrust when information being released to the public is not directly controlled by the institution and could potentially be damaging.



</doc>
<doc id="5390" url="https://en.wikipedia.org/wiki?curid=5390" title="Conversion of units">
Conversion of units

Conversion of units is the conversion between different units of measurement for the same quantity, typically through multiplicative conversion factors.

The process of conversion depends on the specific situation and the intended purpose. This may be governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:

Some conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes called "soft conversion". It does not involve changing the physical configuration of the item being measured.

By contrast, a "hard conversion" or an "adaptive conversion" may not be exactly equivalent. It changes the measurement to convenient and workable numbers and units in the new system. It sometimes involves a slightly different configuration, or size substitution, of the item. Nominal values are sometimes allowed and used.

A conversion factor is used to change the units of a measured quantity without changing its value. The unity bracket method of unit conversion consists of a fraction in which the denominator is equal to the numerator, but they are in different units. Because of the identity property of multiplication, the value of a quantity will not change as long as it is multiplied by one. Also, if the numerator and denominator of a fraction are equal to each other, then the fraction is equal to one. So as long as the numerator and denominator of the fraction are equivalent, they will not affect the value of the measured quantity.

The following example demonstrates how the unity bracket method is used to convert the rate 5 kilometers per second to meters per second. The symbols km, m, and s represent kilometer, meter, and second, respectively.

formula_1formula_2formula_3formula_4formula_5

Thus, it is found that 5 kilometers per second is equal to 5000 meters per second.

There are many conversion tools. They are found in the function libraries of applications such as spreadsheets databases, in calculators, and in macro packages and plugins for many other applications such as the mathematical, scientific and technical applications.

There are many standalone applications that offer the thousands of the various units with conversions. For example, the free software movement offers a command line utility GNU units for Linux and Windows.

This article gives lists of conversion factors for each of a number of physical quantities, which are listed in the index. For each physical quantity, a number of different units (some only of historical interest) are shown and expressed in terms of the corresponding SI unit. Conversions between units in the metric system are defined by their prefixes (for example, 1 kilogram = 1000 grams, 1 milligram = 0.001 grams) and are thus not listed in this article. Exceptions are made if the unit is commonly known by another name (for example, 1 micron = 10 metre). Within each table, the units are listed alphabetically, and the SI units (base or derived) are highlighted.

Notes:

A velocity consists of a speed combined with a direction; the speed part of the velocity takes units of speed.

"See also:" Conversion between weight (force) and mass

Modern standards (such as ISO 80000) prefer the shannon to the bit as a unit for a quantity of information entropy, whereas the (discrete) storage space of digital devices is measured in bits. Thus, uncompressed redundant data occupy more than one bit of storage per shannon of information entropy. The multiples of a bit listed above are usually used with this meaning.

The candela is the preferred nomenclature for the SI unit.

Although becquerel (Bq) and hertz (Hz) both ultimately refer to the same SI base unit (s), Hz is used only for periodic phenomena (i.e. repetitions at regular intervals), and Bq is only used for stochastic processes (i.e. at random intervals) associated with radioactivity.

The roentgen is not an SI unit and the NIST strongly discourages its continued use.

Although the definitions for sievert (Sv) and gray (Gy) would seem to indicate that they measure the same quantities, this is not the case. The effect of receiving a certain dose of radiation (given as Gy) is variable and depends on many factors, thus a new unit was needed to denote the biological effectiveness of that dose on the body; this is known as the equivalent dose and is shown in Sv. The general relationship between absorbed dose and equivalent dose can be represented as
where "H" is the equivalent dose, "D" is the absorbed dose, and "Q" is a dimensionless quality factor. Thus, for any quantity of "D" measured in Gy, the numerical value for "H" measured in Sv may be different.



</doc>
<doc id="5391" url="https://en.wikipedia.org/wiki?curid=5391" title="City">
City

A city is a large human settlement. It can be defined as a permanent and densely settled place with administratively defined boundaries whose members work primarily on non-agricultural tasks. Cities generally have extensive systems for housing, transportation, sanitation, utilities, land use, and communication. Their density facilitates interaction between people, government organisations and businesses, sometimes benefiting different parties in the process, such as improving efficiency of goods and service distribution. This concentration also can have significant negative consequences, such as forming urban heat islands, concentrating pollution, and stressing water supplies and other resources. 

Historically, city-dwellers have been a small proportion of humanity overall, but following two centuries of unprecedented and rapid urbanisation, roughly half of the world population now lives in cities, which has had profound consequences for global sustainability. Present-day cities usually form the core of larger metropolitan areas and urban areas—creating numerous commuters traveling towards city centres for employment, entertainment, and edification. However, in a world of intensifying globalisation, all cities are in different degree also connected globally beyond these regions. This increased influence means that cities also have significant influences on global issues, such as sustainable development, global warming and global health.

The world powers feature several renowned cities, the cities of the United States, include New York City and Los Angeles, and other major cities such as Chicago, Houston, and Philadelphia. While the cities in China, rank among the most populated, for example city proper is Chongqing and the Shanghai area. While the most populous metropolitan areas are the Greater Tokyo Area, and the Jakarta metropolitan area.

Other important traits of cities, besides population, also includes the capital status and relative continued occupation of the city. For example, country capitals such as Abu Dhabi, Beijing, Berlin, Cairo, Dubai, London, Moscow, Paris, Rome, Seoul, Tokyo, and Washington D.C. reflect their nation's identity. Some historic capitals, such as Kyoto, maintain their reflection of cultural identity even without modern capital status. Religious holy sites offer another example of capital status within a religion, Jerusalem, Mecca, and Varanasi each hold significance. The cities of Faiyum, Damascus, and Argos are among those laying claim to the longest continual inhabitation. In terms of relative age, the oldest cities in the Americas are Cholula near Puebla, Florés in Petén, and Acoma near Albuquerque, while the oldest capital cities in the Americas are Mexico City, Santo Domingo, and San Juan. Another example of relative age, is in the age of the oldest capital cities of the superpower and emerging superpower, they are the U.S. state capital of Santa Fe, New Mexico, and the Chinese prefecture capital of Xi'an, Shaanxi.

A city is distinguished from other human settlements by its relatively great size, but also by its functions and its special symbolic status, which may be conferred by a central authority. The term can also refer either to the physical streets and buildings of the city or to the collection of people who dwell there, and can be used in a general sense to mean urban rather than rural territory.

National censuses use a variety of definitions - invoking factors such as population, population density, number of dwellings, economic function, and infrastructure - to classify populations as urban. Typical working definitions for small-city populations start at around 100,000 people. Common population definitions for an urban area (city or town) range between 1,500 and 50,000 people, with most U.S states using a minimum between 1,500 and 5,000 inhabitants. Some jurisdictions set no such minima. In the United Kingdom, city status is awarded by the Crown and then remains permanently. (Historically, the qualifying factor was the presence of a cathedral, resulting in some very small cities such as Wells, with a population 12,000 and St Davids, with a population of 1,841 .) According to the "functional definition" a city is not distinguished by size alone, but also by the role it plays within a larger political context. Cities serve as administrative, commercial, religious, and cultural hubs for their larger surrounding areas. Examples of settlements with "city” in their names which may not meet any of the traditional criteria to be named such include Broad Top City, Pennsylvania (population 452), and City Dulas, Anglesey, a hamlet.

The presence of a literate elite is sometimes included in the definition. A typical city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to support the government workers. (This arrangement contrasts with the more typically horizontal relationships in a tribe or village accomplishing common goals through informal agreements between neighbors, or through leadership of a chief.) The governments may be based on heredity, religion, military power, work systems such as canal-building, food-distribution, land-ownership, agriculture, commerce, manufacturing, finance, or a combination of these. Societies that live in cities are often called civilizations.

The word "city" and the related "civilization" come, via Old French, from the Latin root "civitas", originally meaning citizenship or community member and eventually coming to correspond with urbs, meaning "city" in a more physical sense. The Roman "civitas" was closely linked with the Greek "polis"—another common root appearing in English words such as "metropolis".

Urban geography deals both with cities in their larger context and with their internal structure.

Town siting has varied through history according to natural, technological, economic, and military contexts. Access to water has long been a major factor in city placement and growth, and despite exceptions enabled by the advent of rail transport in the nineteenth century, through the present most of the world's urban population lives near the coast or on a river.

Urban areas as a rule cannot produce their own food and therefore must develop some relationship with a hinterland which sustains them. Only in special cases such as mining towns which play a vital role in long-distance trade, are cities disconnected from the countryside which feeds them. Thus, centrality within a productive region influences siting, as economic forces would in theory favor the creation of market places in optimal mutually reachable locations.

The vast majority of cities have a central area containing buildings with special economic, political, and religious significance. Archaeologists refer to this area by the Greek term temenos or if fortified as a citadel. These spaces historically reflect and amplify the city's centrality and importance to its wider sphere of influence. Today cities have a city center or downtown, sometimes coincident with a central business district.

Cities typically have public spaces where anyone can go. These include privately owned spaces open to the public as well as forms of public land such as public domain and the commons. Western philosophy since the time of the Greek agora has considered physical public space as the substrate of the symbolic public sphere. Public art adorns (or disfigures) public spaces. Parks and other natural sites within cities provide residents with relief from the hardness and regularity of typical built environments.

Urban structure generally follows one or more basic patterns: geomorphic, radial, concentric, rectilinear, and curvilinear. Physical environment generally constrains the form in which a city is built. If located on a mountainside, urban structure may rely on terraces and winding roads. It may be adapted to its means of subsistence (e.g. agriculture or fishing). And it may be set up for optimal defense given the surrounding landscape. Beyond these "geomorphic" features, cities can develop internal patterns, due to natural growth or to city planning.

In a radial structure, main roads converge on a central point. This form could evolve from successive growth over a long time, with concentric traces of town walls and citadels marking older city boundaries. In more recent history, such forms were supplemented by ring roads moving traffic around the outskirts of a town. Dutch cities such as Amsterdam and Haarlem are structured as a central square surrounded by concentric canals marking every expansion. In cities such as and also Moscow, this pattern is still clearly visible.

A system of rectilinear city streets and land plots, known as the grid plan, has been used for millennia in Asia, Europe, and the Americas. The Indus Valley Civilisation built Mohenjo-Daro, Harappa and other cities on a grid pattern, using ancient principles described by Kautilya, and aligned with the compass points. The ancient Greek city of Priene exemplifies a grid plan with specialized districts used across the Hellenistic Mediterranean.

Urban-type settlement extends far beyond the traditional boundaries of the city proper in a form of development sometimes described critically as urban sprawl. Decentralization and dispersal of city functions (commercial, industrial, residential, cultural, political) has transformed the very meaning of the term and has challenged geographers seeking to classify territories according to an urban-rural binary.

Metropolitan areas include suburbs and exurbs organized around the needs of commuters, and sometimes edge cities characterized by a degree of economic and political independence. (In the US these are grouped into metropolitan statistical areas for purposes of demography and marketing.) Some cities are now part of a continuous urban landscape called urban agglomeration, conurbation, or megalopolis (exemplified by the BosWash corridor of the Northeastern United States.)

Cities, characterized by population density, symbolic function, and urban planning, have existed for thousands of years. In the conventional view, civilization and the city both followed from the development of agriculture, which enabled production of surplus food, and thus a social division of labour (with concomitant social stratification) and trade. Early cities often featured granaries, sometimes within a temple. A minority viewpoint considers that cities may have arisen without agriculture, due to alternative means of subsistence (fishing), to use as communal seasonal shelters, to their value as bases for defensive and offensive military organization, or to their inherent economic function. Cities played a crucial role in the establishment of political power over an area, and ancient leaders such as Alexander the Great founded and created them with zeal.

Jericho and Çatalhöyük, dated to the eighth millennium BC, are among the earliest proto-cities known to archaeologists.

In the fourth and third millennium BC, complex civilizations flourished in the river valleys of Mesopotamia, India, China, and Egypt. Excavations in these areas have found the ruins of cities geared variously towards trade, politics, or religion. Some had large, dense populations, but others carried out urban activities in the realms of politics or religion without having large associated populations. Among the early Old World cities, Mohenjo-daro of the Indus Valley Civilization in present-day Pakistan, existing from about 2600 BC, was one of the largest, with a population of 50,000 or more and a sophisticated sanitation system. China's planned cities were constructed according to sacred principles to act as celestial microcosms. The Ancient Egyptian cities known physically by archaeologists are not extensive. They include (known by their Arab names) El Lahun, a workers' town associated with the pyramid of Senusret II, and the religious city Amarna built by Akhenaten and abandoned. These sites appear planned in a highly regimented and stratified fashion, with a minimalistic grid of rooms for the workers and increasingly more elaborate housing available for higher classes.

In Mesopotamia, the civilization of Sumer, followed by Assyria and Babylon, gave rise to numerous cities, governed by kings and fostering multiple languages written in cuneiform. The Phoenician trading empire, flourishing around the turn of the first millennium BC, encompassed numerous cities extending from Tyre, Cydon, and Byblos to Carthage and Cádiz.

In the following centuries, independent city-states of Greece developed the "polis", an association of male landowning citizens who collectively constituted the city. The agora, meaning "gathering place" or "assembly", was the center of athletic, artistic, spiritual and political life of the polis. Rome's rise to power brought its population to one million. Under the authority of its empire, Rome transformed and founded many cities ("coloniae"), and with them brought its principles of urban architecture, design, and society.

In the ancient Americas, early urban traditions developed in the Andes and Mesoamerica. In the Andes, the first urban centers developed in the Norte Chico civilization, Chavin and Moche cultures, followed by major cities in the Huari, Chimu and Inca cultures. The Norte Chico civilization included as many as 30 major population centers in what is now the Norte Chico region of north-central coastal Peru. It is the oldest known civilization in the Americas, flourishing between the 30th century BC and the 18th century BC. Mesoamerica saw the rise of early urbanism in several cultural regions, beginning with the Olmec and spreading to the Preclassic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec, Andean civilization, Mayan, Mound Builders, and Pueblo peoples drew on these earlier urban traditions. Many of their ancient cities continue to be inhabited, including major metropolitan cities such as Mexico City, in the same location as Tenochtitlan; while ancient continuously inhabited Pueblos are near modern urban areas in New Mexico, such as Acoma Pueblo near the Albuquerque metropolitan area and Taos Pueblo near Taos; while others like Lima are located nearby ancient Peruvian sites such as Pachacamac.

Jenné-Jeno, located in present-day Mali and dating to the third century BC, lacked monumental architecture and a distinctive elite social class—but nevertheless had specialized production and relations with a hinterland. Pre-Arabic trade contacts probably existed between Jenné-Jeno and North Africa. Other early urban centers in sub-Saharan Africa, dated to around 500 AD, include Awdaghust, Kumbi-Saleh the ancient capital of Ghana, and Maranda a center located on a trade route between Egypt and Gao.

In the first millennium AD, Angkor in the Khmer Empire grew into one of the most extensive cities in the world and may have supported up to one million people.

In the remnants of the Roman Empire, cities of late antiquity gained independence but soon lost population and importance. The locus of power in the West shifted to Constantinople and to the ascendant Islamic civilization with its major cities Baghdad, Cairo, and Córdoba. From the 9th through the end of the 12th century, Constantinople, capital of the Eastern Roman Empire, was the largest and wealthiest city in Europe, with a population approaching 1 million. The Ottoman Empire gradually gained control over many cities in the Mediterranean area, including Constantinople in 1453.

In the Holy Roman Empire, beginning in the 12th. century, free imperial cities such as Nuremberg, Strasbourg, Frankfurt, Zurich, Nijmegen became a privileged elite among towns having won self-governance from their local lay or secular lord or having been granted self-governanace by the emperor and being placed under his immediate protection. By 1480, these cities, as far as still part of the empire, became part of the Imperial Estates governing the empire with the emperor through the Imperial Diet.

By the thirteenth and fourteenth centuries, some cities become powerful states, taking surrounding areas under their control or establishing extensive maritime empires. In Italy medieval communes developed into city-states including the Republic of Venice and the Republic of Genoa. In Northern Europe, cities including Lübeck and Bruges formed the Hanseatic League for collective defense and commerce. Their power was later challenged and eclipsed by the Dutch commercial cities of Ghent, Ypres, and Amsterdam. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed a considerable autonomy in late medieval Japan.

In the West, nation-states became the dominant unit of political organization following the Peace of Westphalia in the seventeenth century. Western Europe's larger capitals (London and Paris) benefited from the growth of commerce following the emergence of an Atlantic trade. However, most towns remained small.

During the Spanish colonization of the Americas the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories, and were bound to several laws regarding administration, finances and urbanism.

The growth of modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. England led the way as London became the capital of a world empire and cities across the country grew in locations strategic for manufacturing. In the United States from 1860 to 1910, the introduction of railroads reduced transportation costs, and large manufacturing centers began to emerge, fueling migration from rural to city areas.

Industrialized cities became deadly places to live, due to health problems resulting from overcrowding, occupational hazards of industry, contaminated water and air, poor sanitation, and communicable diseases such as typhoid and cholera. Factories and slums emerged as regular features of the urban landscape.

In the second half of the twentieth century, deindustrialization (or "economic restructuring") in the West led to poverty, homelessness, and urban decay in formerly prosperous cities. America's "Steel Belt" became a "Rust Belt" and cities such as Detroit, Michigan, and Gary, Indiana began to shrink, contrary to the global trend of massive urban expansion. Such cities have shifted with varying success into the service economy and public-private partnerships, with concomitant gentrification, uneven revitalization efforts, and selective cultural development. Under the Great Leap Forward and subsequent five-year plans continuing today, the People's Republic of China has undergone concomitant urbanization and industrialization and to become the world's leading manufacturer.

Amidst these economic changes, high technology and instantaneous telecommunication enable select cities to become centers of the knowledge economy. A new smart city paradigm, supported by institutions such as the RAND Corporation and IBM, is bringing computerized surveillance, data analysis, and governance to bear on cities and city-dwellers. Some companies are building brand new masterplanned cities from scratch on greenfield sites.

Urbanization is the process of migration from rural into urban areas, driven by various political, economic, and cultural factors. Until the 18th century, an equilibrium existed between the rural agricultural population and towns featuring markets and small-scale manufacturing. With the agricultural and industrial revolutions urban population began its unprecedented growth, both through migration and through demographic expansion. In England the proportion of the population living in cities jumped from 17% in 1801 to 72% in 1891. In 1900, 15% of the world population lived in cities. The cultural appeal of cities also plays a role in attracting residents.

Urbanization rapidly spread across the Europe and the Americas and since the 1950s has taken hold in Asia and Africa as well. The Population Division of the United Nations Department of Economic and Social Affairs, reported in 2014 that for the first time more than half of the world population lives in cities. Latin America is the most urban continent, with four fifths of its population living in cities, including one fifth of the population said to live in shantytowns (favelas, poblaciones callampas, etc.). Batam, Indonesia, Mogadishu, Somalia, Xiamen, China and Niamey, Niger, are considered among the world's fastest-growing cities, with annual growth rates of 5–8%. In general, the more developed countries of the "Global North" remain more urbanized than the less developed countries of the "Global South"—but the difference continues to shrink because urbanization is happening faster in the latter group. Asia is home to by far the greatest absolute number of city-dwellers: over two billion and counting. The UN predicts an additional 2.5 billion citydwellers (and 300 million fewer countrydwellers) worldwide by 2050, with 90% of urban population expansion occurring in Asia and Africa.

Megacities, cities with population in the multi-millions, have proliferated into the dozens, arising especially in Asia, Africa, and Latin America. Economic globalization fuels the growth of these cities, as new torrents of foreign capital arrange for rapid industrialization, as well as relocation of major businesses from Europe and North America, attracting immigrants from near and far. A deep gulf divides rich and poor in these cities, with usually contain a super-wealthy elite living in gated communities and large masses of people living in substandard housing with inadequate infrastructure and otherwise poor conditions.

Cities around the world have expanded physically as they grow in population, with increases in their surface extent, with the creation of high-rise buildings for residential and commercial use, and with development underground.

Urbanization can create rapid demand for water resources management, as formerly good sources of freshwater become overused and polluted, and the volume of sewage begins to exceed manageable levels.

Local government of cities takes different forms including prominently the municipality (especially in England, in the United States, in India, and in other British colonies; legally, the municipal corporation; "municipio" in Spain and in Portugal, and, along with "municipalidad", in most former parts of the Spanish and Portuguese empires) and the "commune" (in France and in Chile; or "comune" in Italy).

The chief official of the city has the title of mayor. Whatever their true degree of political authority, the mayor typically acts as the figurehead or personification of their city.

City governments have authority to make laws governing activity within cities, while its jurisdiction is generally considered subordinate (in ascending order) to state/provincial, national, and perhaps international law. This hierarchy of law is not enforced rigidly in practice—for example in conflicts between municipal regulations and national principles such as constitutional rights and property rights. Legal conflicts and issues arise more frequently in cities than elsewhere due to the bare fact of their greater density. Modern city governments thoroughly regulate everyday life in many dimensions, including public and personal health, transport, burial, resource use and extraction, recreation, and the nature and use of buildings. Technologies, techniques, and laws governing these areas—developed in cities—have become ubiquitous in many areas.
Municipal officials may be appointed from a higher level of government or elected locally.

Cities typically provide municipal services such as education, through school systems; policing, through police departments; and firefighting, through fire departments; as well as the city's basic infrastructure. These are provided more or less routinely, in a more or less equal fashion. Responsibility for administration usually falls on the city government, though some services may be operated by a higher level of government, while others may be privately run. Armies may assume responsibility for policing cities in states of domestic turmoil such as America's King assassination riots of 1968.

The traditional basis for municipal finance is local property tax levied on real estate within the city. Local government can also collect revenue for services, or by leasing land that it owns. However, financing municipal services, as well as urban renewal and other development projects, is a perennial problem, which cities address through appeals to higher governments, arrangements with the private sector, and techniques such as privatization (selling services into the private sector), corporatization (formation of quasi-private municipally-owned corporations), and financialization (packaging city assets into tradable financial instruments and derivatives). This situation has become acute in deindustrialized cities and in cases where businesses and wealthier citizens have moved outside of city limits and therefore beyond the reach of taxation. Cities in search of ready cash increasingly resort to the municipal bond, essentially a loan with interest and a repayment date. City governments have also begun to use tax increment financing, in which a development project is financed by loans based on future tax revenues which it is expected to yield. Under these circumstances, creditors and consequently city governments place a high importance on city credit ratings.

Governance includes government but refers to a wider domain of social control functions implemented by many actors including nongovernmental organizations. The impact of globalization and the role of multinational corporations in local governments worldwide, has led to a shift in perspective on urban governance, away from the "urban regime theory" in which a coalition of local interests functionally govern, toward a theory of outside economic control, widely associated in academics with the philosophy of neoliberalism. In the neoliberal model of governance, public utilities are privatized, industry is deregulated, and corporations gain the status of governing actors—as indicated by the power they wield in public-private partnerships and over business improvement districts, and in the expectation of self-regulation through corporate social responsibility. The biggest investors and real estate developers act as the city's de facto urban planners.

The related concept of good governance places more emphasis on the state, with the purpose of assessing urban governments for their suitability for development assistance. The concepts of governance and good governance are especially invoked in the emergent megacities, where international organizations consider existing governments inadequate for their large populations.

Urban planning, the application of forethought to city design, involves optimizing land use, transportation, utilities, and other basic systems, in order to achieve certain objectives. Urban planners and scholars have proposed overlapping theories as ideals for how plans should be formed. Planning tools, beyond the original design of the city itself, include public capital investment in infrastructure and land-use controls such as zoning. The continuous process of comprehensive planning involves identifying general objectives as well as collecting data to evaluate progress and inform future decisions.

Government is legally the final authority on planning but in practice the process involves both public and private elements. The legal principle of eminent domain is used by government to divest citizens of their property in cases where its use is required for a project. Planning often involves tradeoffs—decisions in which some stand to gain and some to lose—and thus is closely connected to the prevailing political situation.

The history of urban planning dates to some of the earliest known cities, especially in the Indus Valley and Mesoamerican civilizations, which built their cities on grids and apparently zoned different areas for different purposes. The effects of planning, ubiquitous in today's world, can be seen most clearly in the layout of planned communities, fully designed prior to construction, often with consideration for interlocking physical, economic, and cultural systems.

Urban society is typically stratified. Spatially, cities are formally or informally segregated along ethnic, economic and racial lines. People living relatively close together may live, work, and play, in separate areas, and associate with different people, forming ethnic or lifestyle enclaves or, in areas of concentrated poverty, ghettoes. While in the US and elsewhere poverty became associated with the inner city, in France it has become associated with the "banlieues", areas of urban development which surround the city proper. Meanwhile, across Europe and North America, the racially white majority is empirically the most segregated group. Suburbs in the west, and, increasingly, gated communities and other forms of "privatopia" around the world, allow local elites to self-segregate into secure and exclusive neighborhoods.

Landless urban workers, contrasted with peasants and known as the proletariat, form a growing stratum of society in the age of urbanization. In Marxist doctrine, the proletariat will inevitably revolt against the bourgeoisie as their ranks swell with disenfranchised and disaffected people lacking all stake in the status quo. The global urban proletariat of today, however, generally lacks the status as factory workers which in the nineteenth century provided access to the means of production.

Historically, cities rely on rural areas for intensive farming to yield surplus crops, in exchange for which they provide money, political administration, manufactured goods, and culture. Urban economics tends to analyze larger agglomerations, stretching beyond city limits, in order to reach a more complete understanding of the local labor market.

As hubs of trade cities have long been home to retail commerce and consumption through the interface of shopping. In the 20th century, department stores using new techniques of advertising, public relations, decoration, and design, transformed urban shopping areas into fantasy worlds encouraging self-expression and escape through consumerism.

In general, the density of cities expedites commerce and facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Population density enables also sharing of common infrastructure and production facilities, however in very dense cities, increased crowding and waiting times may lead to some negative effects.

Although manufacturing fueled the growth of cities, many now rely on a tertiary or service economy. The services in question range from tourism, hospitality, entertainment, housekeeping and prostitution to grey-collar work in law, finance, and administration.

Cities are typically hubs for education and the arts, supporting universities, museums, temples, and other cultural institutions. They feature impressive displays of architecture ranging from small to enormous and ornate to brutal; skyscrapers, providing thousands of offices or homes within a small footprint, and visible from miles away, have become iconic urban features. Cultural elites tend to live in cities, bound together by shared cultural capital, and themselves playing some role in governance. By virtue of their status as centers of culture and literacy, cities can be described as the locus of civilization, world history, and social change.

Density makes for effective mass communication and transmission of news, through heralds, printed proclamations, newspapers, and digital media. These communication networks, though still using cities as hubs, penetrate extensively into all populated areas. In the age of rapid communication and transportation, commentators have described urban culture as nearly ubiquitous or as no longer meaningful.

Today, a city's promotion of its cultural activities dovetails with place branding and city marketing, public diplomacy techniques used to inform development strategy; to attract businesses, investors, residents, and tourists; and to create a shared identity and sense of place within the metropolitan area. Physical inscriptions, plaques, and monuments on display physically transmit a historical context for urban places. Some cities, such as Jerusalem, Mecca, and Rome have indelible religious status and for hundreds of years have attracted pilgrims. Patriotic tourists visit Agra to see the Taj Mahal, or New York City to visit the World Trade Center. Elvis lovers visit Memphis to pay their respects at Graceland. Place brands (which include place satisfaction and place loyalty) have great economic value (comparable to the value of commodity brands) because of their influence on the decision-making process of people thinking about doing business in—"purchasing" (the brand of)—a city.

Bread and circuses among other forms of cultural appeal, attract and entertain the masses. Sports also play a major role in city branding and local identity formation. Cities go to considerable lengths in competing to host the Olympic Games, which bring global attention and tourism.

Cities play a crucial strategic role in warfare due to their economic, demographic, symbolic, and political centrality. For the same reasons, they are targets in asymmetric warfare. Many cities throughout history were founded under military auspices, a great many have incorporated fortifications, and military principles continue to influence urban design. Indeed, war may have served as the social rationale and economic basis for the very earliest cities.

Powers engaged in geopolitical conflict have established fortified settlements as part of military strategies, as in the case of garrison towns, America's Strategic Hamlet Program during the Vietnam War, and Israeli settlements in Palestine. While occupying the Philippines, the US Army ordered local people concentrated into cities and towns, in order to isolate committed insurgents and battle freely against them in the countryside.

During World War II, national governments on occasion declared certain cities open, effectively surrendering them to an advancing enemy in order to avoid damage and bloodshed. Urban warfare proved decisive, however, in the Battle of Stalingrad, where Soviet forces repulsed German occupiers, with extreme casualties and destruction. In an era of low-intensity conflict and rapid urbanization, cities have become sites of long-term conflict waged both by foreign occupiers and by local governments against insurgency. Such warfare, known as counterinsurgency, involves techniques of surveillance and psychological warfare as well as close combat, functionally extends modern urban crime prevention, which already uses concepts such as defensible space.

Although capture is the more common objective, warfare has in some cases spelt complete destruction for a city. Mesopotamian tablets and ruins attest to such destruction, as does the Latin motto "Carthago delenda est". Since the atomic bombing of Hiroshima and Nagasaki and throughout the Cold War, nuclear strategists continued to contemplate the use of "countervalue" targeting: crippling an enemy by annihilating its valuable cities, rather than aiming primarily at its military forces.

Cities are responsible for a substantial portion of the emissions responsible for global warming. Over half of the world population is in cities, and cities have outside influence on construction and transportation-- two of the key contributors to global warming emissions. A report by the C40 Cities Climate Leadership Group described consumption based emissions as having significantly more impact than production-based emissions within cities. The report estimates that 85% of the emissions associated with goods within a city is generated outside of that city.
Urban infrastructure involves various physical networks and spaces necessary for transportation, water use, energy, recreation, and public functions. Infrastructure carries a high initial cost in fixed capital (pipes, wires, plants, vehicles, etc.) but lower marginal costs and thus positive economies of scale. Because of the higher barriers to entry, these networks have been classified as natural monopolies, meaning that economic logic favors control of each network by a single organization, public or private.

Infrastructure in general (if not every infrastructure project) plays a vital role in a city's capacity for economic activity and expansion, underpinning the very survival of the city's inhabitants, as well as technological, commercial, industrial, and social activities. Structurally, many infrastructure systems take the form of networks with redundant links and multiple pathways, so that the system as a whole continue to operate even if parts of it fail. The particulars of a city's infrastructure systems have historical path dependence because new development must build from what exists already.

Megaprojects such as the construction of airports, power plants, and railways require large upfront investments and thus tend to require funding from national government or the private sector. Privatization may also extend to all levels of infrastructure construction and maintenance.

Urban infrastructure ideally serves all residents equally but in practice may prove uneven—with, in some cities, clear first-class and second-class alternatives.

Public utilities (literally, useful things with general availability) include basic and essential infrastructure networks, chiefly concerned with the supply of water, electricity, and telecommunications capability to the populace.

Sanitation, necessary for good health in crowded conditions, requires water supply and waste management as well as individual hygiene. Urban water systems include principally a water supply network and a network for wastewater including sewage and stormwater. Historically, either local governments or private companies have administered urban water supply, with a tendency toward government water supply in the 20th century and a tendency toward private operation at the turn of the twenty-first. The market for private water services is dominated by two French companies, Veolia Water (formerly Vivendi) and Engie (formerly Suez), said to hold 70% of all water contracts worldwide.

Modern urban life relies heavily on the energy transmitted through electricity for the operation of electric machines (from household appliances to industrial machines to now-ubiquitous electronic systems used in communications, business, and government) and for traffic lights, streetlights and indoor lighting. Cities rely to a lesser extent on hydrocarbon fuels such as gasoline and natural gas for transportation, heating, and cooking. Telecommunications infrastructure such as telephone lines and coaxial cables also traverse cities, forming dense networks for mass and point-to-point communications.

Because cities rely on specialization and an economic system based on wage labour, their inhabitants must have the ability to regularly travel between home, work, commerce, and entertainment. Citydwellers travel foot or by wheel on roads and walkways, or use special rapid transit systems based on underground, overground, and elevated rail. Cities also rely on long-distance transportation (truck, rail, and airplane) for economic connections with other cities and rural areas.

Historically, city streets were the domain of horses and their riders and pedestrians, who only sometimes had sidewalks and special walking areas reserved for them. In the west, bicycles or (velocipedes), efficient human-powered machines for short- and medium-distance travel, enjoyed a period of popularity at the beginning of the twentieth century before the rise of automobiles. Soon after, they gained a more lasting foothold in Asian and African cities under European influence. In western cities, industrializing, expanding, and electrifying at this time, public transit systems and especially streetcars enabled urban expansion as new residential neighborhoods sprung up along transit lines and workers rode to and from work downtown.

Since the mid-twentieth century, cities have relied heavily on motor vehicle transportation, with major implications for their layout, environment, and aesthetics. (This transformation occurred most dramatically in the US—where corporate and governmental policies favored automobile transport systems—and to a lesser extent in Europe.) The rise of personal cars accompanied the expansion of urban economic areas into much larger metropolises, subsequently creating ubiquitous traffic issues with accompanying construction of new highways, wider streets, and alternative walkways for pedestrians. However, severe traffic jams still occur regularly in cities around the world, as private car ownership and urbanization continue to increase, overwhelming existing urban street networks.

The urban bus system, the world's most common form of public transport, uses a network of scheduled routes to move people through the city, alongside cars, on the roads. Economic function itself also became more decentralized as concentration became impractical and employers relocated to more car-friendly locations (including edge cities). Some cities have introduced bus rapid transit systems which include exclusive bus lanes and other methods for prioritizing bus traffic over private cars. Many big American cities still operate conventional public transit by rail, as exemplified by the ever-popular New York City Subway system. Rapid transit is widely used in Europe and has increased in Latin America and Asia.

Walking and cycling ("non-motorized transport") enjoy increasing favor (more pedestrian zones and bike lanes) in American and Asian urban transportation planning, under the influence of such trends as the Healthy Cities movement, the drive for sustainable development, and the idea of a carfree city. Techniques such as road space rationing and road use charges have been introduced to limit urban car traffic.

Housing of residents presents one of the major challenges every city must face. Adequate housing entails not only physical shelters but also the physical systems necessary to sustain life and economic activity. Home ownership represents status and a modicum of economic security, compared to renting which may consume much of the income of low-wage urban workers. Homelessness, or lack of housing, is a challenge currently faced by millions of people in countries rich and poor.

Urban ecosystems, influenced as they are by the density of human buildings and activities differ considerably from those of their rural surroundings. Anthropogenic buildings and waste, as well as cultivation in gardens, create physical and chemical environments which have no equivalents in wilderness, in some cases enabling exceptional biodiversity. They provide homes not only for immigrant humans but also for immigrant plants, bringing about interactions between species which never previously encountered each other. They introduce frequent disturbances (construction, walking) to plant and animal habitats, creating opportunities for recolonization and thus favoring young ecosystems with r-selected species dominant. On the whole, urban ecosystems are less complex and productive than others, due to the diminished absolute amount of biological interactions.

Typical urban fauna include insects (especially ants), rodents (mice, rats), and birds, as well as cats and dogs (domesticated and feral). Large predators are scarce.

Cities generate considerable ecological footprints, locally and at longer distances, due to concentrated populations and technological activities. From one perspective, cities are not ecologically sustainable due to their resource needs. From another, proper management may be able to ameliorate a city's ill effects. Air pollution arises from various forms of combustion, including fireplaces, wood or coal-burning stoves, other heating systems, and internal combustion engines. Industrialized cities, and today third-world megacities, are notorious for veils of smog (industrial haze) which envelop them, posing a chronic threat to the health of their millions of inhabitants. Urban soil contains higher concentrations of heavy metals (especially lead, copper, and nickel) and has lower pH than soil in comparable wilderness.

Modern cities are known for creating their own microclimates, due to concrete, asphalt, and other artificial surfaces, which heat up in sunlight and channel rainwater into underground ducts. The temperature in New York City exceeds nearby rural temperatures by an average of 2–3 °C and at times 5–10 °C differences have been recorded. This effect varies nonlinearly with population changes (independently of the city's physical size). Aerial particulates increase rainfall by 5–10%. Thus, urban areas experience unique climates, with earlier flowering and later leaf dropping than in nearby country.

Poor and working-class people face disproportionate exposure to environmental risks (known as environmental racism when intersecting also with racial segregation). For example, within the urban microclimate, less-vegetated poor neighborhoods bear more of the heat (but have fewer means of coping with it).

One of the main methods of improving the urban ecology is including in the cities more natural areas: Parks, Gardens, Lawns, and Trees. These areas improve the health, the well being of the human, animal, and plant population of the cities. Generally they are called Urban open space (although this word not always mean green space), Green space, Urban greening. Well-maintained urban trees can provide many social, ecological, and physical benefits to the residents of the city.

A study published in Nature's Scientific Reports journal in 2019 found that people who spent at least two hours per week in nature, were 23 percent more likely to be satisfied with their life and were 59 percent more likely to be in good health than those who had zero exposure. The study used data from almost 20,000 people in the UK. Benefits increased for up to 300 minutes of exposure. The benefits applied to men and women of all ages, as well as across different ethnicities, socioeconomic status, and even those with long-term illnesses and disabilities.

People who did not get at least two hours — even if they surpassed an hour per week — did not get the benefits.

The study is the latest addition to a compelling body of evidence for the health benefits of nature. Many doctors already give nature prescriptions to their patients.

The study didn't count time spent in a person's own yard or garden as time in nature, but the majority of nature visits in the study took place within two miles from home. "Even visiting local urban green spaces seems to be a good thing," Dr. White said in a press release. "Two hours a week is hopefully a realistic target for many people, especially given that it can be spread over an entire week to get the benefit."

As the world becomes more closely linked through economics, politics, technology, and culture (a process called globalization), cities have come to play a leading role in transnational affairs, exceeding the limitations of international relations conducted by national governments. This phenomenon, resurgent today, can be traced back to the Silk Road, Phoenicia, and the Greek city-states, through the Hanseatic League and other alliances of cities. Today the information economy based on high-speed internet infrastructure enables instantaneous telecommunication around the world, effectively eliminating the distance between cities for the purposes of stock markets and other high-level elements of the world economy, as well as personal communications and mass media.

A global city, also known as a world city, is a prominent centre of trade, banking, finance, innovation, and markets. Saskia Sassen used the term "global city" in her 1991 work, "The Global City: New York, London, Tokyo" to refer to a city's power, status, and cosmopolitanism, rather than to its size. Following this view of cities, it is possible to rank the world's cities hierarchically. Global cities form the capstone of the global hierarchy, exerting command and control through their economic and political influence. Global cities may have reached their status due to early transition to post-industrialism or through inertia which has enabled them to maintain their dominance from the industrial era. This type of ranking exemplifies an emerging discourse in which cities, considered variations on the same ideal type, "must" compete with each other globally to achieve prosperity.

Critics of the notion point to the different realms of power and interchange. The term "global city" is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant. Paul James, for example argues that the term is "reductive and skewed" in its focus on financial systems.

Multinational corporations and banks make their headquarters in global cities and conduct much of their business within this context. American firms dominate the international markets for law and engineering and maintain branches in the biggest foreign global cities.

Global cities feature concentrations of extremely wealthy and extremely poor people. Their economies are lubricated by their capacity (limited by the national government's immigration policy, which functionally defines the supply side of the labor market) to recruit low- and high-skilled immigrant workers from poorer areas. More and more cities today draw on this globally available labor force.

Cities increasingly participate in world political activities independently of their enclosing nation-states. Early examples of this phenomenon are the sister city relationship and the promotion of multi-level governance within the European Union as a technique for European integration. Cities including Hamburg, Prague, Amsterdam, The Hague, and City of London maintain their own embassies to the European Union at Brussels.

New urban dwellers may increasingly not simply as immigrants but as transmigrants, keeping one foot each (through telecommunications if not travel) in their old and their new homes.

Cities participate in global governance by various means including membership in global networks which transmit norms and regulations. At the general, global level, United Cities and Local Governments (UCLG) is a significant umbrella organization for cities; regionally and nationally, Eurocities, Asian Network of Major Cities 21, the Federation of Canadian Municipalities the National League of Cities, and the United States Conference of Mayors play similar roles. UCLG took responsibility for creating Agenda 21 for culture, a program for cultural policies promoting sustainable development, and has organized various conferences and reports for its furtherance.

Networks have become especially prevalent in the arena of environmentalism and specifically climate change following the adoption of Agenda 21. Environmental city networks include the C40 Cities Climate Leadership Group, World Association of Major Metropolises ("Metropolis"), the United Nations Global Compact Cities Programme, the Carbon Neutral Cities Alliance (CNCA), the Covenant of Mayors and the Compact of Mayors, ICLEI – Local Governments for Sustainability, and the Transition Towns network.

Cities with world political status as meeting places for advocacy groups, non-governmental organizations, lobbyists, educational institutions, intelligence agencies, military contractors, information technology firms, and other groups with a stake in world policymaking. They are consequently also sites for symbolic protest.

The United Nations System has been involved in a series of events and declarations dealing with the development of cities during this period of rapid urbanization.

UN-Habitat coordinates the UN urban agenda, working with the UN Environmental Programme, the UN Development Programme, the Office of the High Commissioner for Human Rights, the World Health Organization, and the World Bank.

The World Bank, a United Nations specialized agency, has been a primary force in promoting the Habitat conferences, and since the first Habitat conference has used their declarations as a framework for issuing loans for urban infrastructure. The bank's structural adjustment programs contributed to urbanization in the Third World by creating incentives to move to cities. The World Bank and UN-Habitat in 1999 jointly established the Cities Alliance (based at the World Bank headquarters in Washington, D.C.) to guide policymaking, knowledge sharing, and grant distribution around the issue of urban poverty. (UN-Habitat plays an advisory role in evaluating the quality of a locality's governance.) The Bank's policies have tended to focus on bolstering real estate markets through credit and technical assistance.

The United Nations Educational, Scientific and Cultural Organization, UNESCO has increasingly focused on cities as key sites for influencing cultural governance. It has developed various city networks including the International Coalition of Cities against Racism and the Creative Cities Network. UNESCO's capacity to select World Heritage Sites and maintain them through Public/social/private partnerships gives the organization significant influence over cultural capital, tourism, and historic preservation funding.

Cities figure prominently in traditional Western culture, appearing in the Bible in both evil and holy forms, symbolized by Babylon and Jerusalem. Cain and Nimrod are the first city builders in the Book of Genesis. In Sumerian mythology Gilgamesh built the walls of Uruk.

Cities can be perceived in terms of extremes or opposites: at once liberating and oppressive, wealthy and poor, organized and chaotic. The name anti-urbanism refers to various types of ideological opposition to cities, whether because of their culture or their political relationship with the country. Such opposition may result from identification of cities with oppression and the ruling elite. This and other political ideologies strongly influence narratives and themes in discourse about cities. In turn, cities symbolize their home societies.

Writers, painters, and filmmakers have produced innumerable works of art concerning the urban experience. Classical and medieval literature includes a genre of "descriptiones" which treat of city features and history. Modern authors such as Charles Dickens and James Joyce are famous for evocative descriptions of their home cities. Fritz Lang conceived the idea for his influential 1927 film "Metropolis" while visiting Times Square and marveling at the nighttime neon lighting. Other early cinematic representations of cities in the twentieth century generally depicted them as technologically efficient spaces with smoothly functioning systems of automobile transport. By the 1960s, however, traffic congestion began to appear in such films as "The Fast Lady" (1962) and "Playtime" (1967).

Literature, film, and other forms of popular culture have supplied visions of future cities both utopian and dystopian. The prospect of expanding, communicating, and increasingly interdependent world cities has given rise to images such as Nylonkong (NY, London, Hong Kong) and visions of a single world-encompassing ecumenopolis.

Bibliography
Further reading



</doc>
<doc id="5394" url="https://en.wikipedia.org/wiki?curid=5394" title="Chervil">
Chervil

Chervil (; Anthriscus cerefolium), sometimes called French parsley or garden chervil (to distinguish it from similar plants also called chervil), is a delicate annual herb related to parsley. It is commonly used to season mild-flavoured dishes and is a constituent of the French herb mixture .

The name "chervil" is from Anglo-Norman, from Latin or , ultimately from Ancient Greek (), meaning "leaves of joy".

A member of the Apiaceae, chervil is native to the Caucasus but was spread by the Romans through most of Europe, where it is now naturalised. It is also grown frequently in the United States, where it sometimes escapes cultivation. Such escape can be recognized, however, as garden chervil is distinguished from all other Anthriscus species growing in North America (i.e., A. caucalis and A. sylvestris) by its having lanceolate-linear bracteoles and a fruit with a relatively long beak. 

The plants grow to , with tripinnate leaves that may be curly. The small white flowers form small umbels, across. The fruit is about 1 cm long, oblong-ovoid with a slender, ridged beak.
Chervil is used, particularly in France, to season poultry, seafood, young spring vegetables (such as carrots), soups, and sauces. More delicate than parsley, it has a faint taste of liquorice or aniseed.

Chervil is one of the four traditional French , along with tarragon, chives, and parsley, which are essential to French cooking. Unlike the more pungent, robust herbs such as thyme and rosemary, which can take prolonged cooking, the are added at the last minute, to salads, omelettes, and soups.

According to some, slugs are attracted to chervil and the plant is sometimes used to bait them.

Chervil has had various uses in folk medicine. It was claimed to be useful as a digestive aid, for lowering high blood pressure, and, infused with vinegar, for curing hiccups. Besides its digestive properties, it is used as a mild stimulant.

Chervil has also been implicated in "strimmer dermatitis", another name for phytophotodermatitis, due to spray from weed trimmers and similar forms of contact. Other plants in the family Apiaceae can have similar effects.

Transplanting chervil can be difficult, due to the long taproot. It prefers a cool and moist location; otherwise, it rapidly goes to seed (also known as bolting). It is usually grown as a cool-season crop, like lettuce, and should be planted in early spring and late fall or in a winter greenhouse. Regular harvesting of leaves also helps to prevent bolting. If plants bolt despite precautions, the plant can be periodically re-sown throughout the growing season, thus producing fresh plants as older plants bolt and go out of production.

Chervil grows to a height of , and a width of .




</doc>
<doc id="5395" url="https://en.wikipedia.org/wiki?curid=5395" title="Chives">
Chives

Chives, scientific name Allium schoenoprasum, are an edible species of the genus "Allium". Their close relatives include the common onions, garlic, shallot, leek, scallion, and Chinese onion.

A perennial plant, it is widespread in nature across much of Europe, Asia, and North America.

"A. schoenoprasum" is the only species of "Allium" native to both the New and the Old Worlds.

Chives are a commonly used herb and can be found in grocery stores or grown in home gardens. In culinary use, the green stalks (scapes) and the unopened, immature flower buds are diced and used as an ingredient for omelettes, fish, potatoes, soups, and many other dishes. The edible flowers can be used in salads. Chives have insect-repelling properties that can be used in gardens to control pests.

The plant provides a great deal of nectar for pollinators. It was rated in the top 10 for most nectar production (nectar per unit cover per year) in a UK plants survey conducted by the AgriLand project which is supported by the UK Insect Pollinators Initiative.

Chives are a bulb-forming herbaceous perennial plant, growing to tall. The bulbs are slender, conical, long and broad, and grow in dense clusters from the roots. The scapes (or stems) are hollow and tubular, up to long and across, with a soft texture, although, prior to the emergence of a flower, they may appear stiffer than usual. The grass-like leaves, which are shorter than the scapes, are also hollow and tubular, or terete, (round in cross-section) which distinguishes it at a glance from garlic chives ("Allium tuberosum"). The flowers are pale purple, and star-shaped with six petals, wide, and produced in a dense inflorescence of 10-30 together; before opening, the inflorescence is surrounded by a papery bract. The seeds are produced in a small, three-valved capsule, maturing in summer. The herb flowers from April to May in the southern parts of its habitat zones and in June in the northern parts.

Chives are the only species of "Allium" native to both the New and the Old Worlds. Sometimes, the plants found in North America are classified as "A. schoenoprasum" var. "sibiricum", although this is disputed. Differences between specimens are significant. One example was found in northern Maine growing solitary, instead of in clumps, also exhibiting dingy grey flowers.

Although chives are repulsive to insects in general, due to their sulfur compounds, their flowers attract bees, and they are at times kept to increase desired insect life.

It was formally described by the Swedish botanist Carl Linnaeus in his seminal publication "Species Plantarum" in 1753.

The name of the species derives from the Greek σχοίνος, "skhoínos" (sedge or rush) and πράσον, "práson" (leek). Its English name, chives, derives from the French word "cive", from "cepa", the Latin word for onion. In the Middle Ages, it was known as 'rush leek'.

It has two known subspecies; "Allium schoenoprasum" subsp. "gredense" and "Allium schoenoprasum" subsp. "latiorifolium" 

Chives are native to temperate areas of Europe, Asia and North America.

It is found in Asia within the Caucasus (in Armenia, Azerbaijan and Georgia), also in China, Iran, Iraq, Japan (within the provinces of Hokkaido and Honshu), Kazakhstan, Kyrgyzstan, Mongolia, Pakistan, Russian Federation (within the provinces of Kamchatka, Khabarovsk, and Primorye) Siberia and Turkey.

In middle Europe, it is found within Austria, the Czech Republic, Germany, the Netherlands, Poland and Switzerland. In northern Europe, in Denmark, Finland, Norway, Sweden and the United Kingdom. In southeastern Europe, within Bulgaria, Greece, Italy and Romania. It is also found in southwestern Europe, in France, Portugal and Spain.

In Northern America, it is found in Canada (within the provinces of Alberta, British Columbia, Nova Scotia, New Brunswick, Newfoundland, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan and Yukon ), in the United States (with the states of Alaska, Colorado, Connecticut, Idaho, Maine, Maryland, Massachusetts, Michigan, Minnesota, Montana, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, Rhode Island, Vermont, Washington, West Virginia, Wisconsin and Wyoming).
Chives are grown for their scapes and leaves, which are used for culinary purposes as a flavoring herb, and provide a somewhat milder flavor than those of other "Allium" species.

Chives have a wide variety of culinary uses, such as in traditional dishes in France, Sweden, and elsewhere. In his 1806 book "Attempt at a Flora" ("Försök til en flora"), Retzius describes how chives are used with pancakes, soups, fish, and sandwiches. They are also an ingredient of the "gräddfil" sauce with the traditional herring dish served at Swedish midsummer celebrations. The flowers may also be used to garnish dishes. In Poland and Germany, chives are served with quark. Chives are one of the "fines herbes" of French cuisine, the others being tarragon, chervil and parsley. Chives can be found fresh at most markets year-round, making them readily available; they can also be dry-frozen without much impairment to the taste, giving home growers the opportunity to store large quantities harvested from their own gardens.

Retzius also describes how farmers would plant chives between the rocks making up the borders of their flowerbeds, to keep the plants free from pests (such as Japanese beetles). The growing plant repels unwanted insect life, and the juice of the leaves can be used for the same purpose, as well as fighting fungal infections, mildew, and scab.

Chives are cultivated both for their culinary uses and their ornamental value; the violet flowers are often used in ornamental dry bouquets. The flowers are also edible and are used in salads, or used to make Blossom vinegars.

Chives thrive in well-drained soil, rich in organic matter, with a pH of 6-7 and full sun. They can be grown from seed and mature in summer, or early the following spring. Typically, chives need to be germinated at a temperature of 15 to 20 °C (60-70 °F) and kept moist. They can also be planted under a cloche or germinated indoors in cooler climates, then planted out later. After at least four weeks, the young shoots should be ready to be planted out. They are also easily propagated by division.

In cold regions, chives die back to the underground bulbs in winter, with the new leaves appearing in early spring. Chives starting to look old can be cut back to about 2–5 cm. When harvesting, the needed number of stalks should be cut to the base. During the growing season, the plant continually regrows leaves, allowing for a continuous harvest.

Chives are susceptible to damage by leek moth larvae, which bore into the leaves or bulbs of the plant.

Chives have been cultivated in Europe since the Middle Ages (fifth until the 15th centuries), although their usage dates back 5000 years. They were sometimes referred to as "rush leeks".

It was mentioned in 80 A.D. by Marcus Valerius Martialis in his "Epigrams".
The Romans believed chives could relieve the pain from sunburn or a sore throat. They believed eating chives could increase blood pressure and act as a diuretic.

Romani have used chives in fortune telling. Bunches of dried chives hung around a house were believed to ward off disease and evil.

In the 19th century, Dutch farmers fed cattle on the herb to give a different taste to milk.



</doc>
<doc id="5397" url="https://en.wikipedia.org/wiki?curid=5397" title="Chris Morris (satirist)">
Chris Morris (satirist)

Christopher J Morris (born 15 June 1962) is an English comedian, writer, director, actor, voice actor, and producer. He is known for his black humour, surrealism, and controversial subject matters, and has been hailed for his "uncompromising, moralistic drive" by the British Film Institute.

In the early 1990s, Morris teamed up with his radio producer, Armando Iannucci, to create "On the Hour", a satire of news programmes. This was expanded into a television spin off, "The Day Today", which launched the career of Steve Coogan, and has since been hailed as one of the most important satirical shows of the 1990s. Morris further developed the satirical news format with "Brass Eye", which lampooned celebrities whilst focusing on themes such as crime and drugs. For many, the apotheosis of Morris' career was a "Brass Eye" special, which dealt with the moral panic surrounding paedophilia. It quickly became one of the most complained about programmes in British television history, leading the "Daily Mail" to describe him as "the most loathed man on TV".

Meanwhile, Morris's postmodern sketch comedy and ambient music radio show "Blue Jam", which had seen controversy similar to Brass Eye, helped him to gain a cult following. "Blue Jam" was adapted into the TV series "Jam", which some hailed as "the most radical and original television programme broadcast in years", and he went on to win a BAFTA for Best Short Film after expanding a "Blue Jam" sketch into "My Wrongs 8245–8249 & 117", which starred Paddy Considine. This was followed by "Nathan Barley", a sitcom written in collaboration with a then little-known Charlie Brooker that satirised hipsters, which had low ratings but found success upon its DVD release. Morris followed this by joining the cast of the sitcom "The IT Crowd", his first project in which he did not have writing or producing input.

In 2010, Morris directed his first feature-length film, "Four Lions", which satirised Islamic terrorism through a group of inept British Pakistanis. Reception of the film was largely positive, earning Morris his second BAFTA, for "Outstanding Debut". Since 2012, he has directed four episodes of Iannucci's political comedy "Veep" and appeared onscreen in "The Double" and "Stewart Lee's Comedy Vehicle".

Morris was born in Colchester, Essex, to father Paul Michael Morris, a GP, and mother Rosemary Parrington and grew up in a Victorian farmhouse in the village Buckden, Huntingdonshire, which he describes as "very dull".

He has two younger brothers, including theatre director Tom Morris. From an early age he was a prankster, and also had a passion for radio. From the age of 10 he was educated at Stonyhurst College, an independent Jesuit boarding school in Lancashire. He went to study zoology at the University of Bristol, where he gained a 2:1.

On graduating, Morris pursued a career as a musician in various bands, for which he played the bass guitar. He then went to work for Radio West, a local radio station in Bristol. He then took up a news traineeship with BBC Radio Cambridgeshire, where he took advantage of access to editing and recording equipment to create elaborate spoofs and parodies. He also spent time in early 1987 hosting a 2–4pm afternoon show and finally ended up presenting Saturday morning show "I.T."

In July 1987, he moved on to BBC Radio Bristol to present his own show "No Known Cure", broadcast on Saturday and Sunday mornings. The show was surreal and satirical, with odd interviews conducted with unsuspecting members of the public. He was fired from Bristol in 1990 after "talking over the news bulletins and making silly noises". In 1988 he also joined, from its launch, Greater London Radio (GLR). He presented "The Chris Morris Show" on GLR until 1993, when one show got suspended after a sketch was broadcast involving a child "outing" celebrities.

In 1991, Morris joined Armando Iannucci's spoof news project "On the Hour". Broadcast on BBC Radio 4, it saw him work alongside Iannucci, Steve Coogan, Stewart Lee, Richard Herring and Rebecca Front. In 1992, Morris hosted Danny Baker's Radio 5 Morning Edition show for a week whilst Baker was on holiday. In 1994, Morris began a weekly evening show, the "Chris Morris Music Show", on BBC Radio 1 alongside Peter Baynham and 'man with a mobile phone' Paul Garner. In the shows, Morris perfected the spoof interview style that would become a central component of his "Brass Eye" programme. In the same year, Morris teamed up with Peter Cook (as Sir Arthur Streeb-Greebling), in a series of improvised conversations for BBC Radio 3 entitled "Why Bother?".

In 1994, a BBC 2 television series based on "On the Hour" was broadcast under the name "The Day Today". "The Day Today" made a star of Morris, and marked the television debut of Steve Coogan's Alan Partridge character. The programme ended on a high after just one series, with Morris winning the 1994 British Comedy Award for Best Newcomer for his lead role as the Paxmanesque news anchor.

In 1996, Morris appeared on the daytime programme The Time, The Place, posing as an academic, Thurston Lowe, in a discussion entitled "Are British Men Lousy Lovers?", but was found out when a producer alerted the show's host, John Stapleton.

In 1997, the black humour which had featured in "On the Hour" and "The Day Today" became more prominent in "Brass Eye", another spoof current affairs television documentary, shown on Channel 4. "Brass Eye" became known for tricking celebrities and politicians into throwing support behind public awareness campaigns for made-up issues that were often absurd or surreal (such as a drug called "cake" and an elephant with its trunk stuck up its anus).

From 1997 to 1999 Morris created "Blue Jam" for BBC Radio 1, a surreal taboo-breaking radio show set to an ambient soundtrack. In 2000 this was followed by "Jam", a television reworking. Morris released a 'remix' version of this, entitled "Jaaaaam".

In 2001, a special episode of "Brass Eye" on the moral panic that surrounds paedophilia attracted a record-breaking number of complaints – the total remains the third highest on UK television after "Celebrity Big Brother 2007" and "" – as well as heated discussion in the press. Many complainants, some of whom later admitted to not having seen the programme (notably Beverley Hughes, a government minister), felt the satire was directed at the victims of paedophilia, which Morris denies. Channel 4 defended the show, insisting the target was the media and its hysterical treatment of paedophilia, and not victims of crime.

In 2002, Morris ventured into film, directing the short "My Wrongs#8245–8249 & 117", adapted from a "Blue Jam" monologue about a man led astray by a sinister talking dog. It was the first film project of Warp Films, a branch of Warp Records. In 2002 it won the BAFTA for best short film. In 2005 Morris worked on a sitcom entitled "Nathan Barley", based on the character created by Charlie Brooker for his website TVGoHome (Morris had contributed to TVGoHome on occasion, under the pseudonym 'Sid Peach'). Co-written by Brooker and Morris, the series was broadcast on Channel 4 in early 2005.

Morris was a cast member in "The IT Crowd", a Channel 4 sitcom which focused on the information technology department of the fictional company Reynholm Industries. The series was written and directed by Graham Linehan (writer of "Father Ted" and "Black Books", with whom Morris collaborated on "The Day Today", "Brass Eye" and "Jam") and produced by Ash Atalla ("The Office"). Morris played Denholm Reynholm, the eccentric managing director of the company. This marked the first time Morris has acted in a substantial role in a project which he has not developed himself. Morris' character appeared to leave the series during episode two of the second series. His character made a brief return in the first episode of the third series.

In November 2007, Morris wrote an article for "The Observer" in response to Ronan Bennett's article published six days earlier in "The Guardian". Bennett's article, "Shame on us", accused the novelist Martin Amis of racism. Morris' response, "The absurd world of Martin Amis", was also highly critical of Amis; although he did not accede to Bennett's accusation of racism, Morris likened Amis to the Muslim cleric Abu Hamza (who was jailed for inciting racial hatred in 2006), suggesting that both men employ "mock erudition, vitriol and decontextualised quotes from the Qu'ran" to incite hatred.

Morris served as script editor for the 2009 series "Stewart Lee's Comedy Vehicle", working with former colleagues Stewart Lee, Kevin Eldon and Armando Iannucci. He maintained this role for the second (2011) and third series (2014), also appearing as a mock interviewer dubbed the "hostile interrogator" in the third and fourth series.

Morris completed his debut feature film "Four Lions" in late 2009, a satire based on a group of Islamist terrorists in Sheffield.
It premiered at the Sundance Film Festival in January 2010 and was short-listed for the festival's World Cinema Narrative prize. The film (working title "Boilerhouse") was picked up by Film Four. Morris told "The Sunday Times" that the film sought to do for Islamic terrorism what "Dad's Army", the classic BBC comedy, did for the Nazis by showing them as "scary but also ridiculous".

In 2012, Morris directed the seventh and penultimate episode of the first season of "Veep", an Armando Iannucci-devised American version of "The Thick of It". In 2013, he returned to direct two episodes for the second season of "Veep", and a further episode for season three in 2014.

In 2013, Morris appeared briefly in Richard Ayoade's "The Double", a black comedy film based on the Fyodor Dostoyevsky novella of the same name. Morris had previously worked with Ayoade on "Nathan Barley" and "The IT Crowd".

In February 2014, Morris made a surprise appearance at the beginning of a Stewart Lee live show, introducing the comedian with fictional anecdotes about their work together. The following month, Morris appeared in the third series of "Stewart Lee's Comedy Vehicle" as a "hostile interrogator", a role previously occupied by Armando Iannucci.

In December 2014, it was announced that a short radio collaboration with Noel Fielding and Richard Ayoade would be broadcast on BBC Radio 6. According to Fielding, the work had been in progress since around 2006. However, in January 2015 it was decided, 'in consultation with [Morris]', that the project was not yet complete, and so the intended broadcast did not go ahead.

A statement released by Film4 in February 2016 made reference to funding what would be Morris's second feature-film. In November 2017 it was reported that Morris had shot the movie, starring Anna Kendrick, in the Dominican Republic but the title was not made public. It was later reported in January 2018 that Jim Gaffigan and Rupert Friend had joined the cast of the still-untitled film, and that the plot would revolve around an FBI hostage situation gone wrong. The completed film, titled "The Day Shall Come", had its world premiere at South by Southwest on 11 March 2019.

Morris often co-writes and performs incidental music for his television shows, notably with "Jam" and the 'extended remix' version, "Jaaaaam". In the early 1990s Morris contributed a Pixies parody track entitled "Motherbanger" to a flexi-disc given away with an edition of Select music magazine. Morris supplied sketches for British band Saint Etienne's 1993 single "You're in a Bad Way" (the sketch 'Spongbake' appears at the end of the 4th track on the CD single).
In 2000, he collaborated by mail with Amon Tobin to create the track "Bad Sex", which was released as a B-side on the Tobin single "Slowly".
British band Stereolab's song "Nothing to Do with Me" from their 2001 album "Sound-Dust" featured various lines from Chris Morris sketches as lyrics.

In 2003, Morris was listed in "The Observer" as one of the 50 funniest acts in British comedy. In 2005, Channel 4 aired a show called "The Comedian's Comedian" in which foremost writers and performers of comedy ranked their 50 favourite acts. Morris was at number eleven. Morris won the BAFTA for outstanding debut with his film "Four Lions". Adeel Akhtar and Nigel Lindsay collected the award in his absence. Lindsay stated that Morris had sent him a text message before they collected the award reading, 'Doused in petrol, Zippo at the ready'. In June 2012 Morris was placed at number 16 in the Top 100 People in UK Comedy.

In 2010, a biography, "Disgusting Bliss: The Brass Eye of Chris Morris", was published. Written by Lucian Randall, the book depicted Morris as "brilliant but uncompromising", and a "frantic-minded perfectionist".

In November 2014, a three-hour retrospective of Morris's radio career was broadcast on BBC Radio 4 Extra under the title 'Raw Meat Radio', presented by Mary Anne Hobbs and featuring interviews with Armando Iannucci, Peter Baynham, Paul Garner, and others.

Morris won the Best TV Comedy Newcomer award from the British Comedy Awards in 1998 for his performance in "The Day Today". He has won two BAFTA awards: the BAFTA Award for Best Short Film in 2002 for "My Wrongs#8245–8249 & 117", and the BAFTA Award for Outstanding Debut by a British director, writer or producer in 2011 for "Four Lions".

Morris lives in Brixton, with his wife, the actress turned literary agent Jo Unwin. The pair met in 1984 at the Edinburgh Festival, when he was playing bass guitar for the Cambridge Footlights Revue and she was in a comedy troupe called the Millies. They have two sons, Charles and Frederick, both of whom were born in Lambeth. 

Until the release of "Four Lions" he gave very few interviews and little had been published about Morris's personal life. In 2010 he made numerous media appearances to promote and support the film, both in the UK and US, at one point appearing as a guest on "Late Night with Jimmy Fallon". In 2019, two lengthy interviews with Morris conducted by fellow British comedian Adam Buxton for "The Adam Buxton Podcast" were released in the run up to the release of Morris’ new movie "The Day Shall Come".

Morris can be heard as himself in a 2008 podcast for CERN, being taken on a tour of the facility by the physicist Brian Cox. 

Morris has a large birthmark on his face, which he usually covers with makeup when acting.




</doc>
<doc id="5399" url="https://en.wikipedia.org/wiki?curid=5399" title="Colorado">
Colorado

Colorado (, other variants) is a state of the Western United States encompassing most of the southern Rocky Mountains as well as the northeastern portion of the Colorado Plateau and the western edge of the Great Plains. It is the 8th most extensive and 21st most populous U.S. state. The estimated population of Colorado is 5,770,545 as of 2019, an increase of 14.7% since the 2010 United States Census.

The state was named for the Colorado River, which early Spanish explorers named the "Río Colorado" ("Red River") for the ruddy silt the river carried from the mountains. The Territory of Colorado was organized on February 28, 1861, and on August 1, 1876, U.S. President Ulysses S. Grant signed Proclamation 230 admitting Colorado to the Union as the 38th state. Colorado is nicknamed the "Centennial State" because it became a state one century after the signing of the United States Declaration of Independence.

Colorado is bordered by Wyoming to the north, Nebraska to the northeast, Kansas to the east, Oklahoma to the southeast, New Mexico to the south, Utah to the west, and touches Arizona to the southwest at the Four Corners. Colorado is noted for its vivid landscape of mountains, forests, high plains, mesas, canyons, plateaus, rivers and desert lands. Colorado is part of the western and southwestern United States and is one of the Mountain States.

Denver is the capital and most populous city of Colorado. Residents of the state are known as Coloradans, although the antiquated term "Coloradoan" is occasionally used.

Colorado is notable for its diverse geography, which includes alpine mountains, high plains, deserts with huge sand dunes, and deep canyons. In 1861, the United States Congress defined the boundaries of the new Territory of Colorado exclusively by lines of latitude and longitude, stretching from 37°N to 41°N latitude, and from 102°02'48"W to 109°02'48"W longitude (25°W to 32°W from the Washington Meridian). After years of government surveys, the borders of Colorado are now officially defined by 697 boundary markers and 697 straight boundary lines. Colorado, Wyoming, and Utah are the only states that have their borders defined solely by straight boundary lines with no natural features. The southwest corner of Colorado is the Four Corners Monument at 36°59'56"N, 109°2'43"W. This border delineating Colorado, New Mexico, Arizona, and Utah is the only place in the United States where four states meet.

The summit of Mount Elbert at elevation in Lake County is the highest point in Colorado and the Rocky Mountains of North America. Colorado is the only U.S. state that lies entirely above 1,000 meters elevation. The point where the Arikaree River flows out of Yuma County, Colorado, and into Cheyenne County, Kansas, is the lowest point in Colorado at elevation. This point, which holds the distinction of being the highest low elevation point of any state, is higher than the high elevation points of 18 states and the District of Columbia.

A little less than half of Colorado is flat and rolling land. East of the Rocky Mountains are the Colorado Eastern Plains of the High Plains, the section of the Great Plains within Nebraska at elevations ranging from roughly . The Colorado plains are mostly prairies but also include deciduous forests, buttes, and canyons. Precipitation averages annually.

Eastern Colorado is presently mainly farmland and rangeland, along with small farming villages and towns. Corn, wheat, hay, soybeans, and oats are all typical crops. Most villages and towns in this region boast both a water tower and a grain elevator. Irrigation water is available from both surface and subterranean sources. Surface water sources include the South Platte, the Arkansas River, and a few other streams. Subterranean water is generally accessed through artesian wells. Heavy usage of these wells for irrigation purposes caused underground water reserves to decline in the region. Eastern Colorado also hosts a considerable amount and range of livestock, such as cattle ranches and hog farms.

Roughly 70% of Colorado's population resides along the eastern edge of the Rocky Mountains in the Front Range Urban Corridor between Cheyenne, Wyoming, and Pueblo, Colorado. This region is partially protected from prevailing storms that blow in from the Pacific Ocean region by the high Rockies in the middle of Colorado. The "Front Range" includes Denver, Boulder, Fort Collins, Loveland, Castle Rock, Colorado Springs, Pueblo, Greeley, Lone Tree, and other townships and municipalities in between. On the other side of the Rockies, the significant population centers in Western Colorado (which is not considered the "Front Range") are the cities of Grand Junction, Durango, and Montrose.

The Continental Divide of the Americas extends along the crest of the Rocky Mountains. The area of Colorado to the west of the Continental Divide is called the Western Slope of Colorado. West of the Continental Divide, water flows to the southwest via the Colorado River and the Green River into the Gulf of California.

Within the interior of the Rocky Mountains are several large parks which are high broad basins. In the north, on the east side of the Continental Divide is the North Park of Colorado. The North Park is drained by the North Platte River, which flows north into Wyoming and Nebraska. Just to the south of North Park, but on the western side of the Continental Divide, is the Middle Park of Colorado, which is drained by the Colorado River. The South Park of Colorado is the region of the headwaters of the South Platte River.

In southmost Colorado is the large San Luis Valley, where the headwaters of the Rio Grande are located. The valley sits between the Sangre De Cristo Mountains and San Juan Mountains, and consists of large desert lands that eventually run into the mountains. The Rio Grande drains due south into New Mexico, Mexico, and Texas. Across the Sangre de Cristo Range to the east of the San Luis Valley lies the Wet Mountain Valley. These basins, particularly the San Luis Valley, lie along the Rio Grande Rift, a major geological formation of the Rocky Mountains, and its branches.

To the west of the Great Plains of Colorado rises the eastern slope of the Rocky Mountains. Notable peaks of the Rocky Mountains include Longs Peak, Mount Evans, Pikes Peak, and the Spanish Peaks near Walsenburg, in southern Colorado. This area drains to the east and the southeast, ultimately either via the Mississippi River or the Rio Grande into the Gulf of Mexico.
The Rocky Mountains within Colorado contain 53 peaks that are or higher in elevation above sea level, known as fourteeners. These mountains are largely covered with trees such as conifers and aspens up to the tree line, at an elevation of about in southern Colorado to about in northern Colorado. Above this tree line only alpine vegetation grows. Only small parts of the Colorado Rockies are snow-covered year round.

Much of the alpine snow melts by mid-August with the exception of a few snow-capped peaks and a few small glaciers. The Colorado Mineral Belt, stretching from the San Juan Mountains in the southwest to Boulder and Central City on the front range, contains most of the historic gold- and silver-mining districts of Colorado. Mount Elbert is the highest summit of the Rocky Mountains. The 30 highest major summits of the Rocky Mountains of North America all lie within the state.

The Western Slope area of Colorado includes the western face of the Rocky Mountains and all of the state to the western border. This area includes several terrains and climates from alpine mountains to arid deserts. The Western Slope includes many ski resort towns in the Rocky Mountains and towns west of the mountains. It is less populous than the Front Range but includes a large number of national parks and monuments.

From west to east, the land of Colorado consists of desert lands, desert plateaus, alpine mountains, National Forests, relatively flat grasslands, scattered forests, buttes, and canyons in the western edge of the Great Plains. The famous Pikes Peak is located just west of Colorado Springs. Its isolated peak is visible from nearly the Kansas border on clear days, and also far to the north and the south. The northwestern corner of Colorado is a sparsely populated region, and it contains part of the noted Dinosaur National Monument, which is not only a paleontological area, but is also a scenic area of rocky hills, canyons, arid desert, and streambeds. Here, the Green River briefly crosses over into Colorado. Desert lands in Colorado are located in and around areas such as the Pueblo, Canon City, Florence, Great Sand Dunes National Park and Preserve, San Luis Valley, Cortez, Canyon of the Ancients National Monument, Hovenweep National Monument, Ute Mountain, Delta, Grand Junction, Colorado National Monument, and other areas surrounding the Uncompahgre Plateau and Uncompahgre National Forest.

The Western Slope of Colorado is drained by the Colorado River and its tributaries (primarily the Gunnison River, Green River, and the San Juan River), or by evaporation in its arid areas. The Colorado River flows through Glenwood Canyon, and then through an arid valley made up of desert from Rifle to Parachute, through the desert canyon of De Beque Canyon, and into the arid desert of Grand Valley, where the city of Grand Junction is located. Also prominent in or near the southern portion of the Western Slope are the Grand Mesa, which lies to the southeast of Grand Junction; the high San Juan Mountains, a rugged mountain range; and to the west of the San Juan Mountains, the Colorado Plateau, a high arid region that borders Southern Utah.

Grand Junction, Colorado is the largest city on the Western Slope. Grand Junction and Durango are the only major centers of television broadcasting west of the Continental Divide in Colorado, though most mountain resort communities publish daily newspapers. Grand Junction is located along Interstate 70, the only major highway in Western Colorado. Grand Junction is also along the major railroad of the Western Slope, the Union Pacific. This railroad also provides the tracks for Amtrak's California Zephyr passenger train, which crosses the Rocky Mountains between Denver and Grand Junction via a route on which there are no continuous highways.

The Western Slope includes multiple notable destinations in the Colorado Rocky Mountains, including Glenwood Springs, with its resort hot springs, and the ski resorts of Aspen, Breckenridge, Vail, Crested Butte, Steamboat Springs, and Telluride.

Higher education in and near the Western Slope can be found at Colorado Mesa University in Grand Junction, Western Colorado University in Gunnison, Fort Lewis College in Durango, and Colorado Mountain College in Glenwood Springs and Steamboat Springs.

The Four Corners Monument in the southwest corner of Colorado marks the common boundary of Colorado, New Mexico, Arizona, and Utah; the only such place in the United States.

The climate of Colorado is more complex than states outside of the Mountain States region. Unlike most other states, southern Colorado is not always warmer than northern Colorado. Most of Colorado is made up of mountains, foothills, high plains, and desert lands. Mountains and surrounding valleys greatly affect local climate.

As a general rule, with an increase in elevation comes a decrease in temperature and an increase in precipitation. Northeast, east, and southeast Colorado are mostly the high plains, while Northern Colorado is a mix of high plains, foothills, and mountains. Northwest and west Colorado are predominantly mountainous, with some desert lands mixed in. Southwest and southern Colorado are a complex mixture of desert and mountain areas.

The climate of the Eastern Plains is semiarid (Köppen climate classification: "BSk") with low humidity and moderate precipitation, usually from annually. The area is known for its abundant sunshine and cool, clear nights, which give this area a great average diurnal temperature range. The difference between the highs of the days and the lows of the nights can be considerable as warmth dissipates to space during clear nights, the heat radiation not being trapped by clouds. The Front Range urban corridor, where most of the population of Colorado resides, lies in a pronounced precipitation shadow as a result of being on the lee side of the Rocky Mountains.

In summer, this area can have many days above 95 °F (35 °C) and often 100 °F (38 °C). On the plains, the winter lows usually range from 25 to −10 °F (−4 to −23 °C). About 75% of the precipitation falls within the growing season, from April to September, but this area is very prone to droughts. Most of the precipitation comes from thunderstorms, which can be severe, and from major snowstorms that occur in the winter and early spring. Otherwise, winters tend to be mostly dry and cold.

In much of the region, March is the snowiest month. April and May are normally the rainiest months, while April is the wettest month overall. The Front Range cities closer to the mountains tend to be warmer in the winter due to Chinook winds which warm the area, sometimes bringing temperatures of 70 °F (21 °C) or higher in the winter. The average July temperature is 55 °F (13 °C) in the morning and 90 °F (32 °C) in the afternoon. The average January temperature is 18 °F (−8 °C) in the morning and 48 °F (9 °C) in the afternoon, although variation between consecutive days can be 40 °F (20 °C).

Just west of the plains and into the foothills, there are a wide variety of climate types. Locations merely a few miles apart can experience entirely different weather depending on the topography. Most valleys have a semi-arid climate not unlike the eastern plains, which transitions to an alpine climate at the highest elevations. Microclimates also exist in local areas that run nearly the entire spectrum of climates, including subtropical highland ("Cfb/Cwb"), humid subtropical ("Cfa"), humid continental ("Dfa/Dfb"), Mediterranean ("Csa/Csb") and subarctic ("Dfc").

Extreme weather changes are common in Colorado, although a significant portion of the extreme weather occurs in the least populated areas of the state. Thunderstorms are common east of the Continental Divide in the spring and summer, yet are usually brief. Hail is a common sight in the mountains east of the Divide and across the eastern Plains, especially the northeast part of the state. Hail is the most commonly reported warm season severe weather hazard. The eastern Plains are subject to some of the biggest hail storms in North America. Notable examples are the severe hailstorms that hit Denver on July 11, 1990 and May 8, 2017, the latter being the costliest ever in the state.

The Eastern Plains are part of the extreme western portion of Tornado Alley; some damaging tornadoes in the Eastern Plains include the 1990 Limon F3 tornado and the 2008 Windsor EF3 tornado, which devastated the small town. Portions of the eastern Plains see especially frequent tornadoes, both those spawned from mesocyclones in supercell thunderstorms and from less intense landspouts, such as within the Denver convergence vorticity zone (DCVZ).

The Plains are also susceptible to occasional floods and particularly severe flash floods, which are caused both by thunderstorms and by the rapid melting of snow in the mountains during warm weather. Notable examples include the 1965 Denver Flood, the Big Thompson River flooding of 1976 and the 2013 Colorado floods. Hot weather is common during summers in Denver. The city's record in 1901 for the number of consecutive days above 90 °F (32 °C) was broken during the summer of 2008. The new record of 24 consecutive days surpassed the previous record by almost a week.

Much of Colorado is very dry, with the state averaging only of precipitation per year statewide. The state rarely experiences a time when some portion is not in some degree of drought. The lack of precipitation contributes to the severity of wildfires in the state, such as the Hayman Fire of 2002, one of the largest wildfires in American history, and the Fourmile Canyon Fire of 2010, which until the Waldo Canyon Fire and High Park Fire of June 2012, and the Black Forest Fire of June 2013, was the most destructive wildfire in Colorado's recorded history.
However, some of the mountainous regions of Colorado receive a huge amount of moisture from winter snowfalls. The spring melts of these snows often cause great waterflows in the Yampa River, the Colorado River, the Rio Grande, the Arkansas River, the North Platte River, and the South Platte River.

Water flowing out of the Colorado Rocky Mountains is a very significant source of water for the farms, towns, and cities of the southwest states of New Mexico, Arizona, Utah, and Nevada, as well as the Midwest, such as Nebraska and Kansas, and the southern states of Oklahoma and Texas. A significant amount of water is also diverted for use in California; occasionally (formerly naturally and consistently), the flow of water reaches northern Mexico.

The highest official ambient air temperature ever recorded in Colorado was on July 20, 2019, at John Martin Dam. The lowest official air temperature was on February 1, 1985, at Maybell.

Despite its mountainous terrain, Colorado is relatively quiet seismically. The U.S. National Earthquake Information Center is located in Golden.

On August 22, 2011, a 5.3 magnitude earthquake occurred west-southwest of the city of Trinidad. There were no casualties and only a small amount of damage was reported. It was the second-largest earthquake in Colorado's history. A magnitude 5.7 earthquake was recorded in 1973.

In early morning hours of August 24, 2018, four minor earthquakes rattled the state of Colorado ranging from magnitude 2.9 to 4.3.

Colorado has recorded 525 earthquakes since 1973, a majority of which range 2to 3.5 on the Richter scale.

The region that is today the state of Colorado has been inhabited by Native Americans for more than 13,000 years. The Lindenmeier Site in Larimer County contains artifacts dating from approximately 11200 BC to 3000 BC. The eastern edge of the Rocky Mountains was a major migration route that was important to the spread of early peoples throughout the Americas. The Ancient Pueblo peoples lived in the valleys and mesas of the Colorado Plateau. The Ute Nation inhabited the mountain valleys of the Southern Rocky Mountains and the Western Rocky Mountains, even as far east as the Front Range of present day. The Apache and the Comanche also inhabited Eastern and Southeastern parts of the state. At times, the Arapaho Nation and the Cheyenne Nation moved west to hunt across the High Plains.
The Spanish Empire claimed Colorado as part of its New Mexico province prior to U.S. involvement in the region. The U.S. acquired a territorial claim to the eastern Rocky Mountains with the Louisiana Purchase from France in 1803. This U.S. claim conflicted with the claim by Spain to the upper Arkansas River Basin as the exclusive trading zone of its colony of Santa Fé de Nuevo México. In 1806, Zebulon Pike led a U.S. Army reconnaissance expedition into the disputed region. Colonel Pike and his men were arrested by Spanish cavalrymen in the San Luis Valley the following February, taken to Chihuahua, and expelled from Mexico the following July.

The U.S. relinquished its claim to all land south and west of the Arkansas River and south of 42nd parallel north and west of the 100th meridian west as part of its purchase of Florida from Spain with the Adams-Onís Treaty of 1819. The treaty took effect February 22, 1821. Having settled its border with Spain, the U.S. admitted the southeastern portion of the Territory of Missouri to the Union as the state of Missouri on August 10, 1821. The remainder of Missouri Territory, including what would become northeastern Colorado, became unorganized territory, and remained so for 33 years over the question of slavery. After 11 years of war, Spain finally recognized the independence of Mexico with the Treaty of Córdoba signed on August 24, 1821. Mexico eventually ratified the Adams-Onís Treaty in 1831. The Texian Revolt of 1835–36 fomented a dispute between the U.S. and Mexico which eventually erupted into the Mexican–American War in 1846. Mexico surrendered its northern territory to the U.S. with the Treaty of Guadalupe Hidalgo at the conclusion of the war in 1848.
Most American settlers traveling overland west to the Oregon Country, the new goldfields of California, or the new Mormon settlements of the State of Deseret in the Salt Lake Valley, avoided the rugged Southern Rocky Mountains, and instead followed the North Platte River and Sweetwater River to South Pass (Wyoming), the lowest crossing of the Continental Divide between the Southern Rocky Mountains and the Central Rocky Mountains. In 1849, the Mormons of the Salt Lake Valley organized the extralegal State of Deseret, claiming the entire Great Basin and all lands drained by the rivers Green, Grand, and Colorado. The federal government of the U.S. flatly refused to recognize the new Mormon government, because it was theocratic and sanctioned plural marriage. Instead, the Compromise of 1850 divided the Mexican Cession and the northwestern claims of Texas into a new state and two new territories, the state of California, the Territory of New Mexico, and the Territory of Utah. On April 9, 1851, Mexican American settlers from the area of Taos settled the village of San Luis, then in the New Mexico Territory, later to become Colorado's first permanent Euro-American settlement.
In 1854, Senator Stephen A. Douglas persuaded the U.S. Congress to divide the unorganized territory east of the Continental Divide into two new organized territories, the Territory of Kansas and the Territory of Nebraska, and an unorganized southern region known as the Indian territory. Each new territory was to decide the fate of slavery within its boundaries, but this compromise merely served to fuel animosity between free soil and pro-slavery factions.

The gold seekers organized the Provisional Government of the Territory of Jefferson on August 24, 1859, but this new territory failed to secure approval from the Congress of the United States embroiled in the debate over slavery. The election of Abraham Lincoln for the President of the United States on November 6, 1860, led to the secession of nine southern slave states and the threat of civil war among the states. Seeking to augment the political power of the Union states, the Republican Party-dominated Congress quickly admitted the eastern portion of the Territory of Kansas into the Union as the free State of Kansas on January 29, 1861, leaving the western portion of the Kansas Territory, and its gold-mining areas, as unorganized territory.

Thirty days later on February 28, 1861, outgoing U.S. President James Buchanan signed an Act of Congress organizing the free Territory of Colorado. The original boundaries of Colorado remain unchanged except for government survey amendments. The name Colorado was chosen because it was commonly believed that the Colorado River originated in the territory. In 1776, Spanish priest Silvestre Vélez de Escalante recorded that Native Americans in the area knew the river as "" for the red-brown silt that the river carried from the mountains. In 1859, a U.S. Army topographic expedition led by Captain John Macomb located the confluence of the Green River with the Grand River in what is now Canyonlands National Park in Utah. The Macomb party designated the confluence as the source of the Colorado River.

On April 12, 1861, South Carolina artillery opened fire on Fort Sumter to start the American Civil War. While many gold seekers held sympathies for the Confederacy, the vast majority remained fiercely loyal to the Union cause.

In 1862, a force of Texas cavalry invaded the Territory of New Mexico and captured Santa Fe on March 10. The object of this Western Campaign was to seize or disrupt the gold fields of Colorado and California and to seize ports on the Pacific Ocean for the Confederacy. A hastily organized force of Colorado volunteers force-marched from Denver City, Colorado Territory, to Glorieta Pass, New Mexico Territory, in an attempt to block the Texans. On March 28, the Coloradans and local New Mexico volunteers stopped the Texans at the Battle of Glorieta Pass, destroyed their cannon and supply wagons, and dispersed 500 of their horses and mules. The Texans were forced to retreat to Santa Fe. Having lost the supplies for their campaign and finding little support in New Mexico, the Texans abandoned Santa Fe and returned to San Antonio in defeat. The Confederacy made no further attempts to seize the Southwestern United States.

In 1864, Territorial Governor John Evans appointed the Reverend John Chivington as Colonel of the Colorado Volunteers with orders to protect white settlers from Cheyenne and Arapaho warriors who were accused of stealing cattle. Colonel Chivington ordered his men to attack a band of Cheyenne and Arapaho encamped along Sand Creek. Chivington reported that his troops killed more than 500 warriors. The militia returned to Denver City in triumph, but several officers reported that the so-called battle was a blatant massacre of Indians at peace, that most of the dead were women and children, and that bodies of the dead had been hideously mutilated and desecrated. Three U.S. Army inquiries condemned the action, and incoming President Andrew Johnson asked Governor Evans for his resignation, but none of the perpetrators was ever punished. This event is now known as the Sand Creek massacre.
In the midst and aftermath of Civil War, many discouraged prospectors returned to their homes, but a few stayed and developed mines, mills, farms, ranches, roads, and towns in Colorado Territory. On September 14, 1864, James Huff discovered silver near Argentine Pass, the first of many silver strikes. In 1867, the Union Pacific Railroad laid its tracks west to Weir, now Julesburg, in the northeast corner of the Territory. The Union Pacific linked up with the Central Pacific Railroad at Promontory Summit, Utah, on May 10, 1869, to form the First Transcontinental Railroad. The Denver Pacific Railway reached Denver in June the following year, and the Kansas Pacific arrived two months later to forge the second line across the continent. In 1872, rich veins of silver were discovered in the San Juan Mountains on the Ute Indian reservation in southwestern Colorado. The Ute people were removed from the San Juans the following year.

The United States Congress passed an enabling act on March 3, 1875, specifying the requirements for the Territory of Colorado to become a state. On August 1, 1876 (four weeks after the Centennial of the United States), U.S. President Ulysses S. Grant signed a proclamation admitting Colorado to the Union as the 38th state and earning it the moniker "Centennial State".

The discovery of a major silver lode near Leadville in 1878 triggered the Colorado Silver Boom. The Sherman Silver Purchase Act of 1890 invigorated silver mining, and Colorado's last, but greatest, gold strike at Cripple Creek a few months later lured a new generation of gold seekers. Colorado women were granted the right to vote on November 7, 1893, making Colorado the second state to grant universal suffrage and the first one by a popular vote (of Colorado men). The repeal of the Sherman Silver Purchase Act in 1893 led to a staggering collapse of the mining and agricultural economy of Colorado, but the state slowly and steadily recovered. Between the 1880s and 1930s, Denver's floriculture industry developed into a major industry in Colorado. This period became known locally as the Carnation Gold Rush.

Poor labor conditions and discontent among miners resulted in several major clashes between strikers and the Colorado National Guard, including the 1903-1904 Western Federation of Miners Strike and Colorado Coalfield War, the latter of which included the Ludlow massacre that killed a dozen women and children. In 1927, the Columbine Mine massacre resulted in six dead strikers following a confrontation with Colorado Rangers. More than 5,000 Colorado miners—many immigrants—are estimated to have died in accidents since records began to be formally collected following an accident in Crested Butte that killed 59 in 1884.

Colorado became the first western state to host a major political convention when the Democratic Party met in Denver in 1908. By the U.S. Census in 1930, the population of Colorado first exceeded one million residents. Colorado suffered greatly through the Great Depression and the Dust Bowl of the 1930s, but a major wave of immigration following World War II boosted Colorado's fortune. Tourism became a mainstay of the state economy, and high technology became an important economic engine. The United States Census Bureau estimated that the population of Colorado exceeded five million in 2009.

Three warships of the U.S. Navy have been named the USS "Colorado". The first USS "Colorado" was named for the Colorado River. The later two ships were named in honor of the state, including the battleship USS "Colorado" which served in World War II in the Pacific beginning in 1941. At the time of the attack on Pearl Harbor, this USS "Colorado" was located at the naval base in San Diego, Calif. and hence went unscathed.

On September 11, 1957, a plutonium fire occurred at the Rocky Flats Plant, which resulted in the significant plutonium contamination of surrounding populated areas.

Since extirpation by trapping and poisoning of the gray wolf ("Canis lupus") from Colorado in the 1930's, a wolf pack recolonized Moffat County, Colorado in northwestern Colorado in 2019.

The United States Census Bureau estimates that the population of Colorado was 5,758,736 as of 2019, a 14.7% increase since the 2010 United States Census. Colorado's most populous city and capital, is Denver. The Greater Denver Metropolitan Area, with an estimated 2017 population of 3,515,374, is considered the largest metropolitan area within the state and is found within the larger Front Range Urban Corridor, home to around 5,000,000 people.

The largest increases are expected in the Front Range Urban Corridor, especially in the Denver metropolitan area. The state's fastest-growing counties are Douglas and Weld. The center of population of Colorado is located just north of the village of Critchell in Jefferson County.

According to the 2010 United States Census, Colorado had a population of 5,029,196. Racial composition of the state's population was:

People of Hispanic and Latino American (of any race made) heritage made up 20.7% of the population. According to the 2000 Census, the largest ancestry groups in Colorado are German (22%) including of Swiss and Austrian nationalities, Mexican (18%), Irish (12%), and English (12%). Persons reporting German ancestry are especially numerous in the Front Range, the Rockies (west-central counties), and Eastern parts/High Plains.

Colorado has a high proportion of Hispanic, mostly Mexican-American, citizens in Metropolitan Denver, Colorado Springs, as well as the smaller cities of Greeley and Pueblo, and elsewhere. Southern, Southwestern, and Southeastern Colorado has a large number of Hispanos, the descendants of the early Mexican settlers of colonial Spanish origin. In 1940, the Census Bureau reported Colorado's population as 8.2% Hispanic and 90.3% non-Hispanic white. The Hispanic population of Colorado has continued to grow quickly over the past decades. By 2019, Hispanics made up 22% of Colorado's population, and Non-Hispanic Whites made up 70%. Spoken English in Colorado has many Spanish idioms.

Colorado also has some large African-American communities located in Denver, in the neighborhoods of Montbello, Five Points, Whittier, and many other East Denver areas. The state has sizable numbers of Asian-Americans of Mongolian, Chinese, Filipino, Korean, Southeast Asian, and Japanese descent. The highest population of Asian Americans can be found on the south and southeast side of Denver, as well as some on Denver's southwest side. The Denver metropolitan area is considered more liberal and diverse than much of the state when it comes to political issues and environmental concerns.

There were a total of 70,331 births in Colorado in 2006. (Birth rate of 14.6 per thousand.) In 2007, non-Hispanic whites were involved in 59.1% of all the births. Some 14.06% of those births involved a non-Hispanic white person and someone of a different ethnicity, most often with a couple including one Hispanic. A birth where at least one Hispanic person was involved counted for 43% of the births in Colorado. As of the 2010 Census, Colorado has the seventh highest percentage of Hispanics (20.7%) in the U.S. behind New Mexico (46.3%), California (37.6%), Texas (37.6%), Arizona (29.6%), Nevada (26.5%), and Florida (22.5%). Per the 2000 census, the Hispanic population is estimated to be 918,899 or approximately 20% of the state total population. Colorado has the 5th-largest population of Mexican-Americans, behind California, Texas, Arizona, and Illinois. In percentages, Colorado has the 6th-highest percentage of Mexican-Americans, behind New Mexico, California, Texas, Arizona, and Nevada.

In 2011, 46% of Colorado's population younger than the age of one were minorities, meaning they had at least one parent who was not non-Hispanic white.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


In 2017, Colorado recorded the second-lowest fertility rate in the United States outside of New England, after Oregon, at 1.63 children per woman. Significant, contributing factors to the decline in pregnancies were the Title X Family Planning Program and an Intrauterine device grant from Warren Buffett's family.

Spanish is the second-most spoken language in Colorado, after English. There is one Native Coloradan language still spoken in Colorado, Colorado River Numic (Ute).

Major religious affiliations of the people of Colorado are 64% Christian, of whom there are 44% Protestant, 16% Roman Catholic, 3% Mormon, and 1% Eastern Orthodox. Other religious breakdowns are 1% Jewish, 1% Muslim, 1% Buddhist and 4% other. The religiously unaffiliated make up 29% of the population.

The largest denominations by number of adherents in 2010 were the Catholic Church with 811,630; non-denominational Evangelical Protestants with 229,981; and The Church of Jesus Christ of Latter-day Saints with 151,433.

According to several studies, Coloradans have the lowest rates of obesity of any state in the U.S. , 18% of the population was considered medically obese, and while the lowest in the nation, the percentage had increased from 17% in 2004. According to a report in the Journal of the American Medical Association, residents of Colorado had a 2014 life expectancy of 80.21 years, the longest of any U.S. state.



A number of film productions have shot on location in Colorado, especially prominent Westerns like "True Grit", "The Searchers", and "Butch Cassidy and the Sundance Kid". A number of historic military forts, railways with trains still operating, mining ghost towns have been utilized and transformed for historical accuracy in well known films. There are also a number of scenic highways and mountain passes that helped to feature the open road in films such as "Vanishing Point", "Bingo" and "Starman". Some Colorado landmarks have been featured in films, such as The Stanley Hotel in "Dumb and Dumber" and "The Shining" and the Sculptured House in "Sleeper". In 2015, "Furious 7" was to film driving sequences on Pikes Peak Highway in Colorado. The TV Series, "Good Luck Charlie" was being filmed in Denver, Colorado. The Colorado Office of Film and Television has noted that more than 400 films have been shot in Colorado.

There are also a number of established film festivals in Colorado, including Aspen Shortsfest, Boulder International Film Festival, Castle Rock Film Festival, Denver Film Festival, Festivus Film Festival (ended in 2013), Mile High Horror Film Festival, Moondance International Film Festival, Mountainfilm in Telluride, Rocky Mountain Women's Film Festival, and Telluride Film Festival.

Colorado is known for its Southwest and Rocky Mountain cuisine. Mexican restaurants are prominent throughout the state.

Boulder, Colorado was named America's Foodiest Town 2010 by Bon Appétit. Boulder, and Colorado in general, is home to a number of national food and beverage companies, top-tier restaurants and farmers' markets. Boulder, Colorado also has more Master Sommeliers per capita than any other city, including San Francisco and New York.

The Food & Wine Classic is held annually each June in Aspen, Colorado. Aspen also has a reputation as the culinary capital of the Rocky Mountain region.

Denver is known for steak, but now has a diverse culinary scene with many restaurants.

Colorado wines include award-winning varietals that have attracted favorable notice from outside the state. With wines made from traditional "Vitis vinifera" grapes along with wines made from cherries, peaches, plums and honey, Colorado wines have won top national and international awards for their quality. Colorado's grape growing regions contain the highest elevation vineyards in the United States, with most viticulture in the state practiced between above sea level. The mountain climate ensures warm summer days and cool nights. Colorado is home to two designated American Viticultural Areas of the Grand Valley AVA and the West Elks AVA, where most of the vineyards in the state are located. However, an increasing number of wineries are located along the Front Range. In 2018, Wine Enthusiast Magazine named Colorado's Grand Valley AVA in Mesa County, Colorado, as one of the Top Ten wine travel destinations in the world.

Colorado is home to many nationally praised microbreweries, including New Belgium Brewing Company, Odell Brewing Company, Great Divide Brewing Company, and Bristol Brewing Company. The area of northern Colorado near and between the cities of Denver, Boulder, and Fort Collins is known as the "Napa Valley of Beer" due to its high density of craft breweries.

Colorado is open to cannabis (marijuana) tourism. With the adoption of their 64th state amendment in 2013, Colorado became the first state in the union to legalize the medicinal (2000), industrial (2013), and recreational (2014) use of marijuana. Colorado's marijuana industry sold $1.31 billion worth of marijuana in 2016 and $1.26 billion in the first three-quarters of 2017. The state generated tax, fee, and license revenue of $194 million in 2016 on legal marijuana sales. Colorado regulates hemp as any part of the plant with less than 0.3% THC.

Amendment 64, adopted by the voters in the 2012 general election, forces the Colorado state legislature to enact legislation governing the cultivation, processing and sale of recreational marijuana and industrial hemp. On April 4, 2014, Senate Bill 14–184 addressing oversight of Colorado's industrial hemp program was first introduced, ultimately being signed into law by Governor John Hickenlooper on May 31, 2014.

On November 7, 2000, 54% of Colorado voters passed Amendment 20, which amends the Colorado State constitution to allow the medical use of marijuana. A patient's medical use of marijuana, within the following limits, is lawful:
Currently Colorado has listed "eight medical conditions for which patients can use marijuana—cancer, glaucoma, HIV/AIDS, muscle spasms, seizures, severe pain, severe nausea and cachexia, or dramatic weight loss and muscle atrophy". Colorado Governor John Hickenlooper has allocated about half of the state's $13 million "Medical Marijuana Program Cash Fund" to medical research in the 2014 budget.

On November 6, 2012, voters amended the state constitution to protect "personal use" of marijuana for adults, establishing a framework to regulate marijuana in a manner similar to alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.

Colorado has five major professional sports leagues, all based in the Denver metropolitan area. Colorado is the least populous state with a franchise in each of the major professional sports leagues.

The Pikes Peak International Hill Climb is a major hillclimbing motor race held at the Pikes Peak Highway.

The Cherry Hills Country Club has hosted several professional golf tournaments, including the U.S. Open, U.S. Senior Open, U.S. Women's Open, PGA Championship and BMW Championship.

The following universities and colleges participate in the National Collegiate Athletic Association Division I. The most popular college sports program is the University of Colorado Buffaloes, who used to play in the Big-12 but now play in the Pac-12. They have won the 1957 and 1991 Orange Bowl, 1995 Fiesta Bowl, and 1996 Cotton Bowl Classic.

CNBC's list of "Top States for Business for 2010" has recognized Colorado as the third-best state in the nation, falling short to only Texas and Virginia.
Total employment 2016

Number of employer establishments 

The total state product in 2015 was $318,600 million. Median Annual Household Income in 2016 was $70,666, 8th in the nation. Per capita personal income in 2010 was $51,940, ranking Colorado 11th in the nation. The state's economy broadened from its mid-19th-century roots in mining when irrigated agriculture developed, and by the late 19th century, raising livestock had become important. Early industry was based on the extraction and processing of minerals and agricultural products. Current agricultural products are cattle, wheat, dairy products, corn, and hay.

The federal government is also a major economic force in the state with many important federal facilities including NORAD (North American Aerospace Defense Command), United States Air Force Academy, Schriever Air Force Base located approximately 10 miles (16 kilometers) east of Peterson Air Force Base, and Fort Carson, both located in Colorado Springs within El Paso County; NOAA, the National Renewable Energy Laboratory (NREL) in Golden, and the National Institute of Standards and Technology in Boulder; U.S. Geological Survey and other government agencies at the Denver Federal Center near Lakewood; the Denver Mint, Buckley Air Force Base, the Tenth Circuit Court of Appeals, and the Byron G. Rogers Federal Building and United States Courthouse in Denver; and a federal Supermax Prison and other federal prisons near Cañon City. In addition to these and other federal agencies, Colorado has abundant National Forest land and four National Parks that contribute to federal ownership of of land in Colorado, or 37% of the total area of the state.
In the second half of the 20th century, the industrial and service sectors have expanded greatly. The state's economy is diversified, and is notable for its concentration of scientific research and high-technology industries. Other industries include food processing, transportation equipment, machinery, chemical products, the extraction of metals such as gold (see Gold mining in Colorado), silver, and molybdenum. Colorado now also has the largest annual production of beer of any state. Denver is an important financial center.

A number of nationally known brand names have originated in Colorado factories and laboratories. From Denver came the forerunner of telecommunications giant Qwest in 1879, Samsonite luggage in 1910, Gates belts and hoses in 1911, and Russell Stover Candies in 1923. Kuner canned vegetables began in Brighton in 1864. From Golden came Coors beer in 1873, CoorsTek industrial ceramics in 1920, and Jolly Rancher candy in 1949. CF&I railroad rails, wire, nails, and pipe debuted in Pueblo in 1892. Holly Sugar was first milled from beets in Holly in 1905, and later moved its headquarters to Colorado Springs. The present-day Swift packed meat of Greeley evolved from Monfort of Colorado, Inc., established in 1930. Estes model rockets were launched in Penrose in 1958. Fort Collins has been the home of Woodward Governor Company's motor controllers (governors) since 1870, and Waterpik dental water jets and showerheads since 1962. Celestial Seasonings herbal teas have been made in Boulder since 1969. Rocky Mountain Chocolate Factory made its first candy in Durango in 1981.

Colorado has a flat 4.63% income tax, regardless of income level. Unlike most states, which calculate taxes based on federal "adjusted gross income", Colorado taxes are based on "taxable income"—income after federal exemptions and federal itemized (or standard) deductions. Colorado's state sales tax is 2.9% on retail sales. When state revenues exceed state constitutional limits, according to Colorado's Taxpayer Bill of Rights legislation, full-year Colorado residents can claim a sales tax refund on their individual state income tax return. Many counties and cities charge their own rates, in addition to the base state rate. There are also certain county and special district taxes that may apply.

Real estate and personal business property are taxable in Colorado. The state's senior property tax exemption was temporarily suspended by the Colorado Legislature in 2003. The tax break was scheduled to return for assessment year 2006, payable in 2007.

, the state's unemployment rate was 4.2%.

The West Virginia teachers' strike in 2018 inspired teachers in other states, including Colorado, to take similar action.

Major philanthropic organizations based in Colorado include the Daniels Fund, the Anschutz Family Foundation, the Gates Family Foundation, the El Pomar Foundation, and the Boettcher Foundation grant each year from approximately $7 billion of assets.

Colorado has significant hydrocarbon resources. According to the Energy Information Administration, Colorado hosts seven of the Nation's 100 largest natural gas fields, and two of its 100 largest oil fields. Conventional and unconventional natural gas output from several Colorado basins typically account for more than five percent of annual U.S. natural gas production. Colorado's oil shale deposits hold an estimated of oil—nearly as much oil as the entire world's proven oil reserves; the economic viability of the oil shale, however, has not been demonstrated. Substantial deposits of bituminous, subbituminous, and lignite coal are found in the state.

Uranium mining in Colorado goes back to 1872, when pitchblende ore was taken from gold mines near Central City, Colorado. The Colorado uranium industry has seen booms and busts, but continues to this day. Not counting byproduct uranium from phosphate, Colorado is considered to have the third-largest uranium reserves of any U.S. state, behind Wyoming and New Mexico.

Uranium price increases from 2001 to 2007 prompted a number of companies to revive uranium mining in Colorado. Price drops and financing problems in late 2008 forced these companies to cancel or scale back uranium-mining project. Currently, there are no uranium producing mines in Colorado.

Colorado's high Rocky Mountain ridges and eastern plains offer wind power potential, and geologic activity in the mountain areas provides potential for geothermal power development. Much of the state is sunny, and could produce solar power. Major rivers flowing from the Rocky Mountains offer hydroelectric power resources. Corn grown in the flat eastern part of the state offers potential resources for ethanol production.

Colorado's primary mode of transportation (in terms of passengers) is its highway system. Interstate 25 (I-25) is the primary north–south highway in the state, connecting Pueblo, Colorado Springs, Denver, and Fort Collins, and extending north to Wyoming and south to New Mexico. I-70 is the primary east–west corridor. It connects Grand Junction and the mountain communities with Denver, and enters Utah and Kansas. The state is home to a network of U.S. and Colorado highways that provide access to all principal areas of the state. Many smaller communities are connected to this network only via county roads.
Denver International Airport (DIA) is the fifth-busiest domestic U.S. airport and twentieth busiest airport in the world by passenger traffic. DIA handles by far the largest volume of commercial air traffic in Colorado, and is the busiest U.S. hub airport between Chicago and the Pacific coast, making Denver the most important airport for connecting passenger traffic in the western United States.

Extensive public transportation bus services are offered both intra-city and inter-city—including the Denver metro area's extensive RTD services. The Regional Transportation District (RTD) operates the popular RTD Bus & Rail transit system in the Denver Metropolitan Area. the RTD rail system had 170 light-rail vehicles, serving of track.
Amtrak operates two passenger rail lines in Colorado, the California Zephyr and Southwest Chief. Colorado's contribution to world railroad history was forged principally by the Denver and Rio Grande Western Railroad which began in 1870 and wrote the book on mountain railroading. In 1988 the "Rio Grande" acquired, but was merged into, the Southern Pacific Railroad by their joint owner Philip Anschutz. On September 11, 1996, Anschutz sold the combined company to the Union Pacific Railroad, creating the largest railroad network in the United States. The Anschutz sale was partly in response to the earlier merger of Burlington Northern and Santa Fe which formed the large Burlington Northern and Santa Fe Railway (BNSF), Union Pacific's principal competitor in western U.S. railroading. Both Union Pacific and BNSF have extensive freight operations in Colorado.

Colorado's freight railroad network consists of 2,688 miles of Class I trackage. It is integral to the U.S. economy, being a critical artery for the movement of energy, agriculture, mining, and industrial commodities as well as general freight and manufactured products between the East and Midwest and the Pacific coast states.
In August 2014, Colorado began to issue driver licenses to aliens not lawfully in the United States who lived in Colorado. In September 2014, KCNC reported that 524 non-citizens were issued Colorado driver licenses that are normally issued to U.S. citizens living in Colorado.

Like the federal government and all other U.S. states, Colorado's state constitution provides for three branches of government: the legislative, the executive, and the judicial branches.

The Governor of Colorado heads the state's executive branch. The current governor is Jared Polis, a Democrat. Colorado's other statewide elected executive officers are the Lieutenant Governor of Colorado (elected on a ticket with the Governor), Secretary of State of Colorado, Colorado State Treasurer, and Attorney General of Colorado, all of whom serve four-year terms.

The seven-member Colorado Supreme Court is the highest judicial court in the state.
The state legislative body is the Colorado General Assembly, which is made up of two houses, the House of Representatives and the Senate. The House has 65 members and the Senate has 35. , the Democratic Party holds a 19 to 16 majority in the Senate and a 41 to 24 majority in the House.

Most Coloradans are native to other states (nearly 60% according to the 2000 census), and this is illustrated by the fact that the state did not have a native-born governor from 1975 (when John David Vanderhoof left office) until 2007, when Bill Ritter took office; his election the previous year marked the first electoral victory for a native-born Coloradan in a gubernatorial race since 1958 (Vanderhoof had ascended from the Lieutenant Governorship when John Arthur Love was given a position in Richard Nixon's administration in 1973). In the 2016 election, the Democratic party won the Colorado electoral college votes.

Tax is collected by the Colorado Department of Revenue.

The State of Colorado is divided into 64 counties. Counties are important units of government in Colorado since the state has no secondary civil subdivisions such as townships. Two of these counties, the City and County of Denver and the City and County of Broomfield, have consolidated city and county governments.

Nine Colorado counties have a population in excess of 250,000 each, while eight Colorado counties have a population of less than 2,500 each. The ten most populous Colorado counties are all located in the Front Range Urban Corridor.

The United States Office of Management and Budget (OMB) has defined one combined statistical area (CSA), seven Metropolitan Statistical Areas (MSAs), and seven Micropolitan Statistical Areas (μSAs) in the state of Colorado.

The most populous of the 14 Core Based Statistical Areas in Colorado is the Denver-Aurora-Broomfield, CO Metropolitan Statistical Area. This area had an estimated population of 2,888,227 on July 1, 2017, an increase of +13.55% since the 2010 United States Census.

The more extensive Denver-Aurora-Boulder, CO Combined Statistical Area had an estimated population of 3,515,374 on July 1, 2017, an increase of +13.73% since the 2010 United States Census.

The most populous extended metropolitan region in Rocky Mountain Region is the Front Range Urban Corridor along the northeast face of the Southern Rocky Mountains. This region with Denver at its center had an estimated population of 4,495,181 on July 1, 2012, an increase of +3.73% since the 2010 United States Census.

The state of Colorado currently has 271 active incorporated municipalities, including 196 towns, 73 cities, and two consolidated city and county governments.

Colorado municipalities operate under one of five types of municipal governing authority. Colorado has one town with a territorial charter, 160 statutory towns, 12 statutory cities, 96 home rule municipalities (61 cities and 35 towns), and two consolidated city and county governments.

In addition to its 271 municipalities, Colorado has 187 unincorporated Census Designated Places and many other small communities.

The state of Colorado has more than 3,000 districts with taxing authority. These districts may provide schools, law enforcement, fire protection, water, sewage, drainage, irrigation, transportation, recreation, infrastructure, cultural facilities, business support, redevelopment, or other services.

Some of these districts have authority to levy sales tax and well as property tax and use fees. This has led to a hodgepodge of sales tax and property tax rates in Colorado. There are some street intersections in Colorado with a different sales tax rate on each corner, sometimes substantially different.

Some of the more notable Colorado districts are:

Colorado is considered a swing state in both state and federal elections. Coloradans have elected 17 Democrats and 12 Republicans to the governorship in the last 100 years. In presidential politics, Colorado was considered a reliably Republican state during the post-World WarII era, voting for the Democratic candidate only in 1948, 1964, and 1992. However, it became a competitive swing state by the turn of the century, and voted consecutively for Democrat Barack Obama in 2008 and 2012, as well as Democrat Hillary Clinton in 2016.

Colorado politics has the contrast of conservative cities such as Colorado Springs and liberal cities such as Boulder and Denver. Democrats are strongest in metropolitan Denver, the college towns of Fort Collins and Boulder, southern Colorado (including Pueblo), and a few western ski resort counties. The Republicans are strongest in the Eastern Plains, Colorado Springs, Greeley, and far Western Colorado near Grand Junction.

The state of Colorado is represented by its two United States Senators:
Colorado is represented by seven Representatives to the United States House of Representatives:

On the November 8, 1932 ballot, Colorado approved the repeal of alcohol prohibition more than a year before the Twenty-first Amendment to the United States Constitution was ratified.

In 2012, voters amended the state constitution protecting "personal use" of marijuana for adults, establishing a framework to regulate cannabis in a manner similar to alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.

On May 29, 2019, Governor Jared Polis signed House Bill 1124 immediately prohibiting law enforcement officials in Colorado from holding undocumented immigrants solely on the basis of a request from U.S. Immigration and Customs Enforcement.

Colleges and universities in Colorado:


Colorado is currently the home of seven major military bases and installations.
Former Military installations and outposts include:

Colorado is home to 4 national parks, 8 national monuments, 2 national recreation areas, 2 national historic sites, 3 national historic trails, a national scenic trail, 11 national forests, 2 national grasslands, 42 national wilderness areas, 2 national conservation areas, 8 national wildlife refuges, 44 state parks, 307 state wildlife areas, and numerous other scenic, historic, and recreational areas.

Units of the National Park System in Colorado:

State government
Federal government
Other


</doc>
<doc id="5401" url="https://en.wikipedia.org/wiki?curid=5401" title="Carboniferous">
Carboniferous

The Carboniferous ( ) is a geologic period and system that spans 60 million years from the end of the Devonian Period million years ago (Mya), to the beginning of the Permian Period, Mya. The name "Carboniferous" means "coal-bearing" and derives from the Latin words "carbō" ("coal") and "ferō" ("I bear, I carry"), and was coined by geologists William Conybeare and William Phillips in 1822.

Based on a study of the British rock succession, it was the first of the modern 'system' names to be employed, and reflects the fact that many coal beds were formed globally during that time. The Carboniferous is often treated in North America as two geological periods, the earlier Mississippian and the later Pennsylvanian. Terrestrial animal life was well established by the Carboniferous period. Amphibians were the dominant land vertebrates, of which one branch would eventually evolve into amniotes, the first solely terrestrial vertebrates.

Arthropods were also very common, and many (such as "Meganeura") were much larger than those of today. Vast swaths of forest covered the land, which would eventually be laid down and become the coal beds characteristic of the Carboniferous stratigraphy evident today. The atmospheric content of oxygen also reached its highest levels in geological history during the period, 35% compared with 21% today, allowing terrestrial invertebrates to evolve to great size.

The later half of the period experienced glaciations, low sea level, and mountain building as the continents collided to form Pangaea. A minor marine and terrestrial extinction event, the Carboniferous rainforest collapse, occurred at the end of the period, caused by climate change.

In the United States the Carboniferous is usually broken into Mississippian (earlier) and Pennsylvanian (later) subperiods. The Mississippian is about twice as long as the Pennsylvanian, but due to the large thickness of coal-bearing deposits with Pennsylvanian ages in Europe and North America, the two subperiods were long thought to have been more or less equal in duration.

In Europe the Lower Carboniferous sub-system is known as the Dinantian, comprising the Tournaisian and Visean Series, dated at 362.5-332.9 Ma, and the Upper Carboniferous sub-system is known as the Silesian, comprising the Namurian, Westphalian, and Stephanian Series, dated at 332.9-298.9 Ma. The Silesian is roughly contemporaneous with the late Mississippian Serpukhovian plus the Pennsylvanian. In Britain the Dinantian is traditionally known as the Carboniferous Limestone, the Namurian as the Millstone Grit, and the Westphalian as the Coal Measures and Pennant Sandstone.

The International Commission on Stratigraphy (ICS) faunal stages (in bold) from youngest to oldest, together with some of their regional subdivisions, are:
Late Pennsylvanian: Gzhelian (most recent)

Late Pennsylvanian: Kasimovian

Middle Pennsylvanian: Moscovian

Early Pennsylvanian: Bashkirian / Morrowan

Late Mississippian: Serpukhovian

Middle Mississippian: Visean

Early Mississippian: Tournaisian (oldest)
A global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread inland seas and the carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwanaland was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush swamps, later to become coal, flourished to within 30 degrees of the northernmost glaciers.

Mid-Carboniferous, a drop in sea level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea level drop and the associated unconformity in North America separate the Mississippian subperiod from the Pennsylvanian subperiod. This happened about 323 million years ago, at the onset of the Permo-Carboniferous Glaciation.

The Carboniferous was a time of active mountain-building as the supercontinent Pangaea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America–Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural Mountains. Most of the Mesozoic supercontinent of Pangea was now assembled, although North China (which would collide in the Latest Carboniferous), and South China continents were still separated from Laurasia. The Late Carboniferous Pangaea was shaped like an "O."

There were two major oceans in the Carboniferous—Panthalassa and Paleo-Tethys, which was inside the "O" in the Carboniferous Pangaea. Other minor oceans were shrinking and eventually closed - Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean (closed by North China collision with Siberia/Kazakhstania).

Average global temperatures in the Early Carboniferous Period were high: approximately 20 °C (68 °F). However, cooling during the Middle Carboniferous reduced average global temperatures to about 12 °C (54 °F). Lack of growth rings of fossilized trees suggest a lack of seasons of a tropical climate. Glaciations in Gondwana, triggered by Gondwana's southward movement, continued into the Permian and because of the lack of clear markers and breaks, the deposits of this glacial period are often referred to as Permo-Carboniferous in age.

The cooling and drying of the climate led to the Carboniferous Rainforest Collapse (CRC) during the late Carboniferous. Tropical rainforests fragmented and then were eventually devastated by climate change.

Carboniferous rocks in Europe and eastern North America largely consist of a repeated sequence of limestone, sandstone, shale and coal beds. In North America, the early Carboniferous is largely marine limestone, which accounts for the division of the Carboniferous into two periods in North American schemes. The Carboniferous coal beds provided much of the fuel for power generation during the Industrial Revolution and are still of great economic importance.

The large coal deposits of the Carboniferous may owe their existence primarily to two factors. The first of these is the appearance of wood tissue and bark-bearing trees. The evolution of the wood fiber lignin and the bark-sealing, waxy substance suberin variously opposed decay organisms so effectively that dead materials accumulated long enough to fossilise on a large scale. The second factor was the lower sea levels that occurred during the Carboniferous as compared to the preceding Devonian period. This promoted the development of extensive lowland swamps and forests in North America and Europe. Based on a genetic analysis of mushroom fungi, it was proposed that large quantities of wood were buried during this period because animals and decomposing bacteria and fungi had not yet evolved enzymes that could effectively digest the resistant phenolic lignin polymers and waxy suberin polymers. They suggest that fungi that could break those substances down effectively only became dominant towards the end of the period, making subsequent coal formation much rarer.

The Carboniferous trees made extensive use of lignin. They had bark to wood ratios of 8 to 1, and even as high as 20 to 1. This compares to modern values less than 1 to 4. This bark, which must have been used as support as well as protection, probably had 38% to 58% lignin. Lignin is insoluble, too large to pass through cell walls, too heterogeneous for specific enzymes, and toxic, so that few organisms other than Basidiomycetes fungi can degrade it. To oxidize it requires an atmosphere of greater than 5% oxygen, or compounds such as peroxides. It can linger in soil for thousands of years and its toxic breakdown products inhibit decay of other substances. One possible reason for its high percentages in plants at that time was to provide protection from insects in a world containing very effective insect herbivores (but nothing remotely as effective as modern plant eating insects) and probably many fewer protective toxins produced naturally by plants than exist today. As a result, undegraded carbon built up, resulting in the extensive burial of biologically fixed carbon, leading to an increase in oxygen levels in the atmosphere; estimates place the peak oxygen content as high as 35%, as compared to 21% today. This oxygen level may have increased wildfire activity. It also may have promoted gigantism of insects and amphibians — creatures that have been constrained in size by respiratory systems that are limited in their physiological ability to transport and distribute oxygen at the lower atmospheric concentrations that have since been available.

In eastern North America, marine beds are more common in the older part of the period than the later part and are almost entirely absent by the late Carboniferous. More diverse geology existed elsewhere, of course. Marine life is especially rich in crinoids and other echinoderms. Brachiopods were abundant. Trilobites became quite uncommon. On land, large and diverse plant populations existed. Land vertebrates included large amphibians.

Early Carboniferous land plants, some of which were preserved in coal balls, were very similar to those of the preceding Late Devonian, but new groups also appeared at this time.
The main Early Carboniferous plants were the Equisetales (horse-tails), Sphenophyllales (scrambling plants), Lycopodiales (club mosses), Lepidodendrales (scale trees), Filicales (ferns), Medullosales (informally included in the "seed ferns", an artificial assemblage of a number of early gymnosperm groups) and the Cordaitales. These continued to dominate throughout the period, but during late Carboniferous, several other groups, Cycadophyta (cycads), the Callistophytales (another group of "seed ferns"), and the Voltziales (related to and sometimes included under the conifers), appeared.

The Carboniferous lycophytes of the order Lepidodendrales, which are cousins (but not ancestors) of the tiny club-moss of today, were huge trees with trunks 30 meters high and up to 1.5 meters in diameter. These included "Lepidodendron" (with its cone called Lepidostrobus), "Anabathra", "Lepidophloios" and "Sigillaria". The roots of several of these forms are known as Stigmaria. Unlike present-day trees, their secondary growth took place in the cortex, which also provided stability, instead of the xylem. The Cladoxylopsids were large trees, that were ancestors of ferns, first arising in the Carboniferous.

The fronds of some Carboniferous ferns are almost identical with those of living species. Probably many species were epiphytic. Fossil ferns and "seed ferns" include "Pecopteris", "Cyclopteris", "Neuropteris", "Alethopteris", and "Sphenopteris"; "Megaphyton" and "Caulopteris" were tree ferns.

The Equisetales included the common giant form "Calamites", with a trunk diameter of 30 to and a height of up to . "Sphenophyllum" was a slender climbing plant with whorls of leaves, which was probably related both to the calamites and the lycopods.

"Cordaites", a tall plant (6 to over 30 meters) with strap-like leaves, was related to the cycads and conifers; the catkin-like reproductive organs, which bore ovules/seeds, is called "Cardiocarpus". These plants were thought to live in swamps. True coniferous trees ("Walchia", of the order Voltziales) appear later in the Carboniferous, and preferred higher drier ground.

In the oceans the marine invertebrate groups are the Foraminifera, corals, Bryozoa, Ostracoda, brachiopods, ammonoids, hederelloids, microconchids and echinoderms (especially crinoids). For the first time foraminifera take a prominent part in the marine faunas. The large spindle-shaped genus "Fusulina" and its relatives were abundant in what is now Russia, China, Japan, North America; other important genera include "Valvulina", "Endothyra", "Archaediscus", and "Saccammina" (the latter common in Britain and Belgium). Some Carboniferous genera are still extant.

The microscopic shells of radiolarians are found in cherts of this age in the Culm of Devon and Cornwall, and in Russia, Germany and elsewhere. Sponges are known from spicules and anchor ropes, and include various forms such as the Calcispongea "Cotyliscus" and "Girtycoelia", the demosponge "Chaetetes", and the genus of unusual colonial glass sponges "Titusvillia".

Both reef-building and solitary corals diversify and flourish; these include both rugose (for example, "Caninia", "Corwenia", "Neozaphrentis"), heterocorals, and tabulate (for example, "Chladochonus", "Michelinia") forms. Conularids were well represented by "Conularia"

Bryozoa are abundant in some regions; the fenestellids including "Fenestella", "Polypora", and "Archimedes", so named because it is in the shape of an Archimedean screw. Brachiopods are also abundant; they include productids, some of which (for example, "Gigantoproductus") reached very large (for brachiopods) size and had very thick shells, while others like "Chonetes" were more conservative in form. Athyridids, spiriferids, rhynchonellids, and terebratulids are also very common. Inarticulate forms include "Discina" and "Crania". Some species and genera had a very wide distribution with only minor variations.

Annelids such as "Serpulites" are common fossils in some horizons. Among the mollusca, the bivalves continue to increase in numbers and importance. Typical genera include "Aviculopecten", "Posidonomya", "Nucula", "Carbonicola", "Edmondia", and "Modiola". Gastropods are also numerous, including the genera "Murchisonia", "Euomphalus", "Naticopsis". Nautiloid cephalopods are represented by tightly coiled nautilids, with straight-shelled and curved-shelled forms becoming increasingly rare. Goniatite ammonoids are common.

Trilobites are rarer than in previous periods, on a steady trend towards extinction, represented only by the proetid group. Ostracoda, a class of crustaceans, were abundant as representatives of the meiobenthos; genera included "Amphissites", "Bairdia", "Beyrichiopsis", "Cavellina", "Coryellina", "Cribroconcha", "Hollinella", "Kirkbya", "Knoxiella", and "Libumella".

Amongst the echinoderms, the crinoids were the most numerous. Dense submarine thickets of long-stemmed crinoids appear to have flourished in shallow seas, and their remains were consolidated into thick beds of rock. Prominent genera include "Cyathocrinus", "Woodocrinus", and "Actinocrinus". Echinoids such as "Archaeocidaris" and "Palaeechinus" were also present. The blastoids, which included the Pentreinitidae and Codasteridae and superficially resembled crinoids in the possession of long stalks attached to the seabed, attain their maximum development at this time.

Freshwater Carboniferous invertebrates include various bivalve molluscs that lived in brackish or fresh water, such as "Anthraconaia", "Naiadites", and "Carbonicola"; diverse crustaceans such as "Candona", "Carbonita", "Darwinula", "Estheria", "Acanthocaris", "Dithyrocaris", and "Anthrapalaemon".
The eurypterids were also diverse, and are represented by such genera as "Adelophthalmus", "Megarachne" (originally misinterpreted as a giant spider, hence its name) and the specialised very large "Hibbertopterus". Many of these were amphibious.

Frequently a temporary return of marine conditions resulted in marine or brackish water genera such as "Lingula", Orbiculoidea, and "Productus" being found in the thin beds known as marine bands.

Fossil remains of air-breathing insects, myriapods and arachnids are known from the late Carboniferous, but so far not from the early Carboniferous. The first true priapulids appeared during this period. Their diversity when they do appear, however, shows that these arthropods were both well developed and numerous. Their large size can be attributed to the moistness of the environment (mostly swampy fern forests) and the fact that the oxygen concentration in the Earth's atmosphere in the Carboniferous was much higher than today. This required less effort for respiration and allowed arthropods to grow larger with the up to millipede-like "Arthropleura" being the largest-known land invertebrate of all time. Among the insect groups are the huge predatory Protodonata (griffinflies), among which was "Meganeura", a giant dragonfly-like insect and with a wingspan of ca. —the largest flying insect ever to roam the planet. Further groups are the Syntonopterodea (relatives of present-day mayflies), the abundant and often large sap-sucking Palaeodictyopteroidea, the diverse herbivorous Protorthoptera, and numerous basal Dictyoptera (ancestors of cockroaches). Many insects have been obtained from the coalfields of Saarbrücken and Commentry, and from the hollow trunks of fossil trees in Nova Scotia. Some British coalfields have yielded good specimens: "Archaeoptitus", from the Derbyshire coalfield, had a spread of wing extending to more than ; some specimens ("Brodia") still exhibit traces of brilliant wing colors. In the Nova Scotian tree trunks land snails ("Archaeozonites", "Dendropupa") have been found.

Many fish inhabited the Carboniferous seas; predominantly Elasmobranchs (sharks and their relatives). These included some, like "Psammodus", with crushing pavement-like teeth adapted for grinding the shells of brachiopods, crustaceans, and other marine organisms. Other sharks had piercing teeth, such as the Symmoriida; some, the petalodonts, had peculiar cycloid cutting teeth. Most of the sharks were marine, but the Xenacanthida invaded fresh waters of the coal swamps. Among the bony fish, the Palaeonisciformes found in coastal waters also appear to have migrated to rivers. Sarcopterygian fish were also prominent, and one group, the Rhizodonts, reached very large size.

Most species of Carboniferous marine fish have been described largely from teeth, fin spines and dermal ossicles, with smaller freshwater fish preserved whole.

Freshwater fish were abundant, and include the genera "Ctenodus", "Uronemus", "Acanthodes", "Cheirodus", and "Gyracanthus".

Sharks (especially the "Stethacanthids") underwent a major evolutionary radiation during the Carboniferous. It is believed that this evolutionary radiation occurred because the decline of the placoderms at the end of the Devonian period caused many environmental niches to become unoccupied and allowed new organisms to evolve and fill these niches. As a result of the evolutionary radiation Carboniferous sharks assumed a wide variety of bizarre shapes including "Stethacanthus" which possessed a flat brush-like dorsal fin with a patch of denticles on its top. "Stethacanthus" unusual fin may have been used in mating rituals.

Carboniferous amphibians were diverse and common by the middle of the period, more so than they are today; some were as long as 6 meters, and those fully terrestrial as adults had scaly skin. They included a number of basal tetrapod groups classified in early books under the Labyrinthodontia. These had long bodies, a head covered with bony plates and generally weak or undeveloped limbs. The largest were over 2 meters long. They were accompanied by an assemblage of smaller amphibians included under the Lepospondyli, often only about long. Some Carboniferous amphibians were aquatic and lived in rivers ("Loxomma", "Eogyrinus", "Proterogyrinus"); others may have been semi-aquatic ("Ophiderpeton", "Amphibamus", "Hyloplesion") or terrestrial ("Dendrerpeton", "Tuditanus", "Anthracosaurus").

The Carboniferous Rainforest Collapse slowed the evolution of amphibians who could not survive as well in the cooler, drier conditions. Reptiles, however, prospered due to specific key adaptations. One of the greatest evolutionary innovations of the Carboniferous was the amniote egg, which allowed the laying of eggs in a dry environment, allowing for the further exploitation of the land by certain tetrapods. These included the earliest sauropsid reptiles ("Hylonomus"), and the earliest known synapsid ("Archaeothyris"). These small lizard-like animals quickly gave rise to many descendants, reptiles, birds, and mammals.

Reptiles underwent a major evolutionary radiation in response to the drier climate that preceded the rainforest collapse. By the end of the Carboniferous period, amniotes had already diversified into a number of groups, including protorothyridids, captorhinids, araeoscelids, and several families of pelycosaurs.

Because plants and animals were growing in size and abundance in this time (for example, "Lepidodendron"), land fungi diversified further. Marine fungi still occupied the oceans. All modern classes of fungi were present in the Late Carboniferous (Pennsylvanian Epoch).

The first 15 million years of the Carboniferous had very limited terrestrial fossils. This gap in the fossil record is called Romer's gap after the American palaentologist Alfred Romer. While it has long been debated whether the gap is a result of fossilisation or relates to an actual event, recent work indicates the gap period saw a drop in atmospheric oxygen levels, indicating some sort of ecological collapse. The gap saw the demise of the Devonian fish-like ichthyostegalian labyrinthodonts, and the rise of the more advanced temnospondyl and reptiliomorphan amphibians that so typify the Carboniferous terrestrial vertebrate fauna.

Before the end of the Carboniferous Period, an extinction event occurred. On land this event is referred to as the Carboniferous Rainforest Collapse (CRC). Vast tropical rainforests collapsed suddenly as the climate changed from hot and humid to cool and arid. This was likely caused by intense glaciation and a drop in sea levels.

The new climatic conditions were not favorable to the growth of rainforest and the animals within them. Rainforests shrank into isolated islands, surrounded by seasonally dry habitats. Towering lycopsid forests with a heterogeneous mixture of vegetation were replaced by much less diverse tree-fern dominated flora.
Amphibians, the dominant vertebrates at the time, fared poorly through this event with large losses in biodiversity; reptiles continued to diversify due to key adaptations that let them survive in the drier habitat, specifically the hard-shelled egg and scales, both of which retain water better than their amphibian counterparts.





</doc>
<doc id="5403" url="https://en.wikipedia.org/wiki?curid=5403" title="Comoros">
Comoros

The Comoros (; , '), officially the Union of the Comoros (Comorian: "Udzima wa Komori," , '), is an island country in the Indian Ocean located at the northern end of the Mozambique Channel off the eastern coast of Africa between northeastern Mozambique, the French region of Mayotte, and northwestern Madagascar. The capital and largest city in Comoros is Moroni. The religion of the majority of the population is Sunni Islam. As a member of the Arab League, the Comoros is the only country in the Arab world which is entirely in the Southern Hemisphere.

At , excluding the contested island of Mayotte, the Comoros is the fourth-smallest African nation by area. The population, excluding Mayotte, is estimated at . As a nation formed at a crossroads of different civilisations, the archipelago is noted for its diverse culture and history. The archipelago was first inhabited by Bantu speakers who came from East Africa, supplemented by Arab and Austronesian immigration.

The sovereign state is an archipelago consisting of three major islands and numerous smaller islands, all in the volcanic Comoro Islands. The major islands are commonly known by their French names: northwestern-most Grande Comore (Ngazidja), Mohéli (Mwali), and Anjouan (Nzwani). In addition, the country has a claim on a fourth major island, southeastern-most Mayotte (Maore), though Mayotte voted against independence from France in 1974, has never been administered by an independent Comoros government, and continues to be administered by France (currently as an overseas department). France has vetoed United Nations Security Council resolutions that would affirm Comorian sovereignty over the island. In addition, Mayotte became an overseas department and a region of France in 2011 following a referendum passed overwhelmingly.

It became part of the French colonial empire in the end of 19th century before becoming independent in 1975. Since declaring independence, the country has experienced more than 20 coups d'état or attempted coups, with various heads of state assassinated. Along with this constant political instability, the population of the Comoros lives with the worst income inequality of any nation, with a Gini coefficient over 60%, while also ranking in the worst quartile on the Human Development Index. about half the population lived below the international poverty line of US$1.25 a day. The French insular region of Mayotte, which is the most prosperous territory in the Mozambique Channel, is the major destination for Comorian illegal immigrants who flee their country. The Comoros is a member state of the Arab League, the African Union, Francophonie, the Organisation of Islamic Cooperation, and the Indian Ocean Commission. Other countries near the Comoros are Tanzania to the northwest and the Seychelles to the northeast. Its capital is Moroni, on Grande Comore. The Union of the Comoros has three official languages—Comorian, Arabic, and French.

The name "Comoros" derives from the Arabic word "qamar" ("moon").

The first human inhabitants of the Comoro Islands are thought to have been Austronesian settlers travelling by boat from islands in Southeast Asia. These people arrived no later than the sixth century AD, the date of the earliest known archaeological site, found on Nzwani, although settlement beginning as early as the first century has been postulated.

The islands of the Comoros were populated by a succession of peoples from the coast of Africa, the Arabian Peninsula and the Persian Gulf, the Malay Archipelago, and Madagascar. Bantu-speaking settlers reached the islands as a part of the greater Bantu expansion that took place in Africa throughout the first millennium.

According to pre-Islamic mythology, a jinni (spirit) dropped a jewel, which formed a great circular inferno. This became the Karthala volcano, which created the island of Grande Comoro.

Development of the Comoros is divided into phases. The earliest reliably recorded phase is the Dembeni phase (ninth to tenth centuries), during which each island maintained a single, central village. From the eleventh to the fifteenth centuries, trade with the island of Madagascar and merchants from the Middle East flourished, smaller villages emerged, and existing towns expanded. Many Comorians can trace their genealogies to ancestors from Yemen, mainly Hadhramaut, and Oman.

According to legend, in 632, upon hearing of Islam, islanders are said to have dispatched an emissary, Mtswa-Mwindza, to Mecca—but by the time he arrived there, the Prophet Muhammad had died. Nonetheless, after a stay in Mecca, he returned to Ngazidja and led the gradual conversion of his islanders to Islam.

Among the earliest accounts of East Africa, the works of Al-Masudi describe early Islamic trade routes, and how the coast and islands were frequently visited by Muslims including Persian and Arab merchants and sailors in search of coral, ambergris, ivory, tortoiseshell, gold and slaves. They also brought Islam to the people of the Zanj including the Comoros. As the importance of the Comoros grew along the East African coast, both small and large mosques were constructed. Despite its distance from the coast, the Comoros is situated along the Swahili Coast in East Africa. It was a major hub of trade and an important location in a network of trading towns that included Kilwa, in present-day Tanzania, Sofala (an outlet for Zimbabwean gold), in Mozambique, and Mombasa in Kenya.

After the arrival of the Portuguese in the early 15th century and subsequent collapse of the East African sultanates, the powerful Omani Sultan Saif bin Sultan began to defeat the Dutch and the Portuguese. His successor Said bin Sultan increased Omani Arab influence in the region, moving his administration to nearby Zanzibar, which came under Omani rule. Nevertheless, the Comoros remained independent, and although the three smaller islands were usually politically unified, the largest island, Ngazidja, was divided into a number of autonomous kingdoms ("ntsi").

By the time Europeans showed interest in the Comoros, the islanders were well placed to take advantage of their needs, initially supplying ships of the route to India and, later, slaves to the plantation islands in the Mascarenes.

Portuguese explorers first visited the archipelago in 1503. The islands provided provisions to the Portuguese fort at Mozambique throughout the 16th century.

In 1793, Malagasy warriors from Madagascar first started raiding the islands for slaves. On the Comoros, it was estimated in 1865 that as much as 40% of the population consisted of slaves. France first established colonial rule in the Comoros in 1841. The first French colonists landed in Mayotte, and "Andriantsoly" (also known as Andrian Tsouli, the Sakalava Dia-Ntsoli, the Sakalava of Boina, and the Malagasy King of Mayotte) signed the Treaty of April 1841, which ceded the island to the French authorities.

The Comoros served as a way station for merchants sailing to the Far East and India until the opening of the Suez Canal significantly reduced traffic passing through the Mozambique Channel. The native commodities exported by the Comoros were coconuts, cattle and tortoiseshell. French settlers, French-owned companies, and wealthy Arab merchants established a plantation-based economy that used about one-third of the land for export crops. After its annexation, France converted Mayotte into a sugar plantation colony. The other islands were soon transformed as well, and the major crops of ylang-ylang, vanilla, coffee, cocoa beans, and sisal were introduced.

In 1886, Mohéli was placed under French protection by its Sultan Mardjani Abdou Cheikh. That same year, despite having no authority to do so, Sultan Said Ali of Bambao, one of the sultanates on Ngazidja, placed the island under French protection in exchange for French support of his claim to the entire island, which he retained until his abdication in 1910. In 1908 the islands were unified under a single administration ("Colonie de Mayotte et dépendances") and placed under the authority of the French colonial governor general of Madagascar. In 1909, Sultan Said Muhamed of Anjouan abdicated in favour of French rule. In 1912 the colony and the protectorates were abolished and the islands became a province of the colony of Madagascar.

Agreement was reached with France in 1973 for the Comoros to become independent in 1978. The deputies of Mayotte abstained. Referendums were held on all four of the islands. Three voted for independence by large margins, while Mayotte voted against, and remains under French administration. On 6 July 1975, however, the Comorian parliament passed a unilateral resolution declaring independence. Ahmed Abdallah proclaimed the independence of the Comorian State ("État comorien"; دولة القمر) and became its first president.

The next 30 years were a period of political turmoil. On 3 August 1975, less than one month after independence, president Ahmed Abdallah was removed from office in an armed coup and replaced with United National Front of the Comoros (FNUK) member Prince Said Mohamed Jaffar. Months later, in January 1976, Jaffar was ousted in favour of his Minister of Defense Ali Soilih.

At this time, the population of Mayotte voted against independence from France in two referenda. The first, held on 22 December 1974, won 63.8% support for maintaining ties with France, while the second, held in February 1976, confirmed that vote with an overwhelming 99.4%. The three remaining islands, ruled by President Soilih, instituted a number of socialist and isolationist policies that soon strained relations with France. On 13 May 1978, Bob Denard returned to overthrow President Soilih and reinstate Abdallah with the support of the French, Rhodesian and South African governments. During Soilih's brief rule, he faced seven additional coup attempts until he was finally forced from office and killed.

In contrast to Soilih, Abdallah's presidency was marked by authoritarian rule and increased adherence to traditional Islam and the country was renamed the Federal Islamic Republic of the Comoros ("République Fédérale Islamique des Comores"; جمهورية القمر الإتحادية الإسلامية). Abdallah continued as president until 1989 when, fearing a probable coup d'état, he signed a decree ordering the Presidential Guard, led by Bob Denard, to disarm the armed forces. Shortly after the signing of the decree, Abdallah was allegedly shot dead in his office by a disgruntled military officer, though later sources claim an antitank missile was launched into his bedroom and killed him. Although Denard was also injured, it is suspected that Abdallah's killer was a soldier under his command.

A few days later, Bob Denard was evacuated to South Africa by French paratroopers. Said Mohamed Djohar, Soilih's older half-brother, then became president, and served until September 1995, when Bob Denard returned and attempted another coup. This time France intervened with paratroopers and forced Denard to surrender. The French removed Djohar to Reunion, and the Paris-backed Mohamed Taki Abdoulkarim became president by election. He led the country from 1996, during a time of labour crises, government suppression, and secessionist conflicts, until his death November 1998. He was succeeded by Interim President Tadjidine Ben Said Massounde.

The islands of Anjouan and Mohéli declared their independence from the Comoros in 1997, in an attempt to restore French rule. But France rejected their request, leading to bloody confrontations between federal troops and rebels. In April 1999, Colonel Azali Assoumani, Army Chief of Staff, seized power in a bloodless coup, overthrowing the Interim President Massounde, citing weak leadership in the face of the crisis. This was the Comoros' 18th coup, or attempted coup d'état since independence in 1975.

Azali failed to consolidate power and reestablish control over the islands, which was the subject of international criticism. The African Union, under the auspices of President Thabo Mbeki of South Africa, imposed sanctions on Anjouan to help broker negotiations and effect reconciliation. The official name of the country was changed to the Union of the Comoros and a new system of political autonomy was instituted for each island, plus a union government for the three islands was added.

Azali stepped down in 2002 to run in the democratic election of the President of the Comoros, which he won. Under ongoing international pressure, as a military ruler who had originally come to power by force, and was not always democratic while in office, Azali led the Comoros through constitutional changes that enabled new elections. A "Loi des compétences" law was passed in early 2005 that defines the responsibilities of each governmental body, and is in the process of implementation. The elections in 2006 were won by Ahmed Abdallah Mohamed Sambi, a Sunni Muslim cleric nicknamed the "Ayatollah" for his time spent studying Islam in Iran. Azali honoured the election results, thus allowing the first peaceful and democratic exchange of power for the archipelago.

Colonel Mohammed Bacar, a French-trained former gendarme, seized power as President in Anjouan in 2001. He staged a vote in June 2007 to confirm his leadership that was rejected as illegal by the Comoros federal government and the African Union. On 25 March 2008 hundreds of soldiers from the African Union and the Comoros seized rebel-held Anjouan, generally welcomed by the population: there have been reports of hundreds, if not thousands, of people tortured during Bacar's tenure.
Some rebels were killed and injured, but there are no official figures. At least 11 civilians were wounded. Some officials were imprisoned. Bacar fled in a speedboat to the French Indian Ocean territory of Mayotte to seek asylum. Anti-French protests followed in the Comoros (see 2008 invasion of Anjouan).

Since independence from France, the Comoros experienced more than 20 coups or attempted coups.

Following elections in late 2010, former Vice-President Ikililou Dhoinine was inaugurated as President on 26 May 2011. A member of the ruling party, Dhoinine was supported in the election by the incumbent President Ahmed Abdallah Mohamed Sambi. Dhoinine, a pharmacist by training, is the first President of the Comoros from the island of Mohéli. Following the 2016 elections, Azali Assoumani became president for a third term.

The Comoros is formed by Ngazidja (Grande Comore), Mwali (Mohéli) and Nzwani (Anjouan), three major islands in the Comoros Archipelago, as well as many minor islets. The islands are officially known by their Comorian language names, though international sources still use their French names (given in parentheses above). The capital and largest city, Moroni, is located on Ngazidja. The archipelago is situated in the Indian Ocean, in the Mozambique Channel, between the African coast (nearest to Mozambique and Tanzania) and Madagascar, with no land borders.

At , it is one of the smallest countries in the world. The Comoros also has claim to of territorial seas. The interiors of the islands vary from steep mountains to low hills.

Ngazidja is the largest of the Comoros Archipelago, approximately equal in area to the other islands combined. It is also the most recent island, and therefore has rocky soil. The island's two volcanoes, Karthala (active) and La Grille (dormant), and the lack of good harbours are distinctive characteristics of its terrain. Mwali, with its capital at Fomboni, is the smallest of the four major islands. Nzwani, whose capital is Mutsamudu, has a distinctive triangular shape caused by three mountain chains – Sima, Nioumakélé and Jimilimé – emanating from a central peak, Mount N'Tingui ().

The islands of the Comoros Archipelago were formed by volcanic activity. Mount Karthala, an active shield volcano located on Ngazidja, is the country's highest point, at . It contains the Comoros' largest patch of disappearing rainforest. Karthala is currently one of the most active volcanoes in the world, with a minor eruption in May 2006, and prior eruptions as recently as April 2005 and 1991. In the 2005 eruption, which lasted from 17 to 19 April, 40,000 citizens were evacuated, and the crater lake in the volcano's caldera was destroyed.

The Comoros also lays claim to the "Îles Éparses" or "Îles éparses de l'océan indien" (Scattered Islands in the Indian Ocean) – Glorioso Islands, comprising Grande Glorieuse, Île du Lys, Wreck Rock, South Rock, Verte Rocks (three islets) and three unnamed islets – one of France's overseas districts. The Glorioso Islands were administered by the colonial Comoros before 1975, and are therefore sometimes considered part of the Comoros Archipelago. Banc du Geyser, a former island in the Comoros Archipelago, now submerged, is geographically located in the "Îles Éparses", but was annexed by Madagascar in 1976 as an unclaimed territory. The Comoros and France each still view the Banc du Geyser as part of the Glorioso Islands and, thus, part of its particular exclusive economic zone.

The climate is generally tropical and mild, and the two major seasons are distinguishable by their raininess. The temperature reaches an average of in March, the hottest month in the rainy season (called kashkazi/kaskazi [meaning north monsoon], which runs from December to April), and an average low of in the cool, dry season (kusi (meaning south monsoon), which proceeds from May to November). The islands are rarely subject to cyclones.

The Comoros constitute an ecoregion in their own right, Comoros forests.

In December 1952 a specimen of the Coelacanth fish was re-discovered off the Comoros coast. The 66 million year old species was thought to have been long extinct until its first recorded appearance in 1938 off the South African coast. Between 1938 and 1975, 84 specimens were caught and recorded.

Politics of the Comoros takes place in a framework of a federal presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. The Constitution of the Union of the Comoros was ratified by referendum on 23 December 2001, and the islands' constitutions and executives were elected in the following months. It had previously been considered a military dictatorship, and the transfer of power from Azali Assoumani to Ahmed Abdallah Mohamed Sambi in May 2006 was a watershed moment as it was the first peaceful transfer in Comorian history.

Executive power is exercised by the government. Federal legislative power is vested in both the government and parliament. The preamble of the constitution guarantees an Islamic inspiration in governance, a commitment to human rights, and several specific enumerated rights, democracy, "a common destiny" for all Comorians. Each of the islands (according to Title II of the Constitution) has a great amount of autonomy in the Union, including having their own constitutions (or Fundamental Law), president, and Parliament. The presidency and Assembly of the Union are distinct from each of the islands' governments. The presidency of the Union rotates between the islands. Mohéli holds the current presidency rotation, and so Ikililou Dhoinine is President of the Union; Grand Comore and Anjouan follow in four-year terms.

The Comorian legal system rests on Islamic law, an inherited French (Napoleonic Code) legal code, and customary law (mila na ntsi). Village elders, kadis or civilian courts settle most disputes. The judiciary is independent of the legislative and the executive. The Supreme Court acts as a Constitutional Council in resolving constitutional questions and supervising presidential elections. As High Court of Justice, the Supreme Court also arbitrates in cases where the government is accused of malpractice. The Supreme Court consists of two members selected by the president, two elected by the Federal Assembly, and one by the council of each island.

Around 80 percent of the central government's annual budget is spent on the country's complex electoral system which provides for a semi-autonomous government and president for each of the three islands and a rotating presidency for the overarching Union government. A referendum took place on 16 May 2009 to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councillors.

In November 1975, the Comoros became the 143rd member of the United Nations. The new nation was defined as comprising the entire archipelago, although the citizens of Mayotte chose to become French citizens and keep their island as a French territory.

The Comoros has repeatedly pressed its claim to Mayotte before the United Nations General Assembly, which adopted a series of resolutions under the caption "Question of the Comorian Island of Mayotte", opining that Mayotte belongs to the Comoros under the principle that the territorial integrity of colonial territories should be preserved upon independence. As a practical matter, however, these resolutions have little effect and there is no foreseeable likelihood that Mayotte will become "de facto" part of the Comoros without its people's consent. More recently, the Assembly has maintained this item on its agenda but deferred it from year to year without taking action. Other bodies, including the Organization of African Unity, the Movement of Non-Aligned Countries and the Organisation of Islamic Cooperation, have similarly questioned French sovereignty over Mayotte. To close the debate and to avoid being integrated by force in the Union of the Comoros, the population of Mayotte overwhelmingly chose to become an overseas department and a region of France in a 2009 referendum. The new status was effective on 31 March 2011 and Mayotte has been recognised as an outermost region by the European Union on 1 January 2014. This decision integrates Mayotte in the French Republic legally « one and indivisible ».

The Comoros is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission and the African Development Bank. On 10 April 2008, the Comoros became the 179th nation to accept the Kyoto Protocol to the United Nations Framework Convention on Climate Change. The Comoros signed the UN treaty on the Prohibition of Nuclear Weapons.

In May 2013 the Union of the Comoros became known for filing a referral to the Office of the Prosecutor of the International Criminal Court (ICC) regarding the events of "the 31 May 2010 Israeli raid on the Humanitarian Aid Flotilla bound for [the] Gaza Strip". In November 2014 the ICC Prosecutor eventually decided that the events did constitute war crimes but did not meet the gravity standards of bringing the case before ICC. The emigration rate of skilled workers was about 21.2% in 2000.

The military resources of the Comoros consist of a small standing army and a 500-member police force, as well as a 500-member defence force. A defence treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a few senior officers presence in the Comoros at government request. France maintains a small maritime base and a Foreign Legion Detachment (DLEM) on Mayotte.

Once the new government was installed in May–June 2011, an expert mission from UNREC (Lomé) came to the Comoros and produced guidelines for the elaboration of a national security policy, which were discussed by different actors, notably the national defence authorities and civil society. By the end of the programme in end March 2012, a normative framework agreed upon by all entities involved in SSR will have been established. This will then have to be adopted by Parliament and implemented by the authorities.

Both male and female same-sex sexual acts are illegal in Comoros. Such acts are punished with up to five years imprisonment.

The level of poverty in the Comoros is high, but "judging by the international poverty threshold of $1.9 per person per day, only two out of every ten Comorians could be classified as poor, a rate that places the Comoros ahead of other low-income countries and 30 percentage points ahead of other countries in Sub-Saharan Africa." Poverty declined by about 10% between 2014 and 2018, and living conditions generally improved. Economic inequality remains widespread, with a major gap between rural and urban areas. Remittances through the sizable Comorian diaspora form a substantial part of the country's GDP and have contributed to decreases in poverty and increases in living standards. 

According to ILO's ILOSTAT statistical database, between 1991 and 2019 the unemployment rate as a percent of the total labor force ranged from. An October 2005 paper by the Comoros Ministry of Planning and Regional Development, however, reported that "registered unemployment rate is 14.3 percent, distributed very unevenly among and within the islands, but with marked incidence in urban areas."

In 2019, more than 56% of the labor force was employed in agriculture, with 29% employed in industry and 14% employed in services. The islands' agricultural sector is based on the export of spices, including vanilla, cinnamon, and cloves, and thus susceptible to price fluctuations in the volatile world commodity market for these goods. The Comoros is the world's largest producer of ylang-ylang, a plant whose extracted essential oil is used in the perfume industry; some 80% of the world's supply comes from the Comoros.

High population densities, as much as 1000 per square kilometre in the densest agricultural zones, for what is still a mostly rural, agricultural economy may lead to an environmental crisis in the near future, especially considering the high rate of population growth. In 2004 the Comoros' real GDP growth was a low 1.9% and real GDP per capita continued to decline. These declines are explained by factors including declining investment, drops in consumption, rising inflation, and an increase in trade imbalance due in part to lowered cash crop prices, especially vanilla.

Fiscal policy is constrained by erratic fiscal revenues, a bloated civil service wage bill, and an external debt that is far above the HIPC threshold. Membership in the franc zone, the main anchor of stability, has nevertheless helped contain pressures on domestic prices.

The Comoros has an inadequate transportation system, a young and rapidly increasing population, and few natural resources. The low educational level of the labour force contributes to a subsistence level of economic activity, high unemployment, and a heavy dependence on foreign grants and technical assistance. Agriculture contributes 40% to GDP and provides most of the exports. 

The government is struggling to upgrade education and technical training, to privatise commercial and industrial enterprises, to improve health services, to diversify exports, to promote tourism, and to reduce the high population growth rate.

The Comoros is a member of the Organization for the Harmonization of Business Law in Africa (OHADA).

With fewer than a million people, the Comoros is one of the least populous countries in the world, but is also one of the most densely populated, with an average of . In 2001, 34% of the population was considered urban, but that is expected to grow, since rural population growth is negative, while overall population growth is still relatively high.

Almost half the population of the Comoros is under the age of 15. Major urban centres include Moroni, Mutsamudu, Domoni, Fomboni, and Tsémbéhou. There are between 200,000 and 350,000 Comorians in France.

The islands of the Comoros share mostly African-Arab origins. One of the largest ethnic groups on the various islands of Comoros remain the Shirazi people. Minorities include Malagasy (Christian) and Indian (mostly Ismaili), as well as other minorities mostly descended from early French settlers. Chinese people are also present in parts of Grande Comore (especially Moroni). A small white minority of French with other European (i.e. Dutch, British and Portuguese) ancestry lives in the Comoros. Most French left after independence in 1975.

The most common language in the Comoros is Comorian, or "Shikomori". It is a language related to Swahili, with four different variants (Shingazidja, Shimwali, Shinzwani and Shimaore) being spoken on each of the four islands. Arabic and Latin scripts are both used, Arabic being the more widely used, and an official orthography has recently been developed for the Latin script.

Arabic and French are also official languages, along with Comorian. Arabic is widely known as a second language, being the language of Quranic teaching. French is the administrative language and the language of all non-Quranic formal education.

Sunni Islam is the dominant religion, representing as much as 99% of the population. A minority of the population of the Comoros, mostly immigrants from metropolitan France, are Roman Catholic. Comoros is the only Muslim-majority country in Southern Africa and the second southernmost Muslim-majority territory after the French territory of Mayotte.

There are 15 physicians per 100,000 people. The fertility rate was 4.7 per adult woman in 2004. Life expectancy at birth is 67 for females and 62 for males.

Almost all of the educated populace of the Comoros have attended Quranic schools at some point in their lives, often before regular schooling. Here, boys and girls are taught about the Qur'an, and memorise it. Some parents specifically choose this early schooling to offset French schools children usually attend later. Since independence and the ejection of French teachers, the education system has been plagued by poor teacher training and poor results, though recent stability may allow for substantial improvements.

Pre-colonization education systems in Comoros focused on necessary skills such as agriculture, caring for livestock and completing household tasks. Religious education also taught children the virtues of Islam. The education system underwent a transformation during colonization in the early 1900s which brought secular education based on the French system. This was mainly for children of the elite. After Comoros gained independence in 1975, the education system changed again. Funding for teachers' salaries was lost, and many went on strike. Thus, the public education system was not functioning between 1997 and 2001. Since gaining independence, the education system has also undergone a democratization and options exist for those other than the elite. Enrollment has also grown.

In 2000, 44.2% of children ages 5 to 14 years were attending school. There is a general lack of facilities, equipment, qualified teachers, textbooks and other resources. Salaries for teachers are often so far in arrears that many refuse to work.

Prior to 2000, students seeking a university education had to attend school outside of the country, however in the early 2000s a university was created in the country. This served to help economic growth and to fight the "flight" of many educated people who were not returning to the islands to work.

About fifty-seven percent of the population is literate in the Latin script while more than 90% are literate in the Arabic script; total literacy is estimated at 77.8%. Comorian has no native script, but both Arabic and Latin scripts are used.

Traditional Comorian women wear colourful sari-like dresses called shiromani, and apply a paste of ground sandalwood and coral called msinzano to their faces. Traditional male clothing is a colourful long skirt and a long white shirt.

There are two types of marriages in Comoros, the "Mna daho" (little marriage) and the "ada" (grand marriage). The little marriage is a simple legal marriage. It is small, intimate, and inexpensive. The bride's dowry is nominal. The little marriage, however, is just a placeholder until the couple can afford the "ada", or grand marriage. The hallmarks of the grand marriage are dazzling gold jewelry, two weeks of celebration and an enormous bridal dowry. The groom must pay most of the expenses for this event, and the bride's family typically pays only a third of that of the groom's. The grand wedding can cost up to £55,000 (74,000 US dollars). Many men cannot afford this until their late 40s, if ever.

The grand marriage is a symbol of social status on the Comoros islands. The completion of an "ada" marriage also greatly increases a man's standing in the Comorian hierarchy. A Comorian man can only wear certain elements of the national dress or stand in the first line at the mosque if he has had a grand marriage. Also, one is not fully considered a man until he has had an "ada" marriage.

The continuation of the grand marriage tradition is criticized because of its great expense and Comoros's intense poverty.

Comorian society has a bilateral descent system. Lineage membership and inheritance of immovable goods (land, housing) is matrilineal, passed in the maternal line, similar to many Bantu peoples who are also matrilineal, while other goods and patronymics are passed in the male line. However, there are differences between the islands, the matrilineal element being stronger on Ngazidja. 

Zanzibar's taarab music remains the most influential genre on the islands. 

There is a government-owned national newspaper in Comoros, "Al-Watwan", published in Moroni. Radio Comoros is the national radio service and Comoros National TV is the television service.


This article incorporates text from the Library of Congress Country Studies, which is in the public domain.



</doc>
<doc id="5404" url="https://en.wikipedia.org/wiki?curid=5404" title="Critical philosophy">
Critical philosophy

Attributed to Immanuel Kant, the critical philosophy () movement sees the primary task of philosophy as criticism rather than justification of knowledge; criticism, for Kant, meant judging as to the possibilities of knowledge before advancing to knowledge itself (from the Greek "kritike (techne)", or "art of judgment"). The basic task of philosophers, according to this view, is not to establish and demonstrate theories about reality, but rather to subject all theories—including those about philosophy itself—to critical review, and measure their validity by how well they withstand criticism.

"Critical philosophy" is also used as another name for Kant's philosophy itself. Kant said that philosophy's proper enquiry is not about what is out there in reality, but rather about the character and foundations of experience itself. We must first judge how human reason works, and within what limits, so that we can afterwards correctly apply it to sense experience and determine whether it can be applied at all to metaphysical objects.




</doc>
<doc id="5405" url="https://en.wikipedia.org/wiki?curid=5405" title="China">
China

China (), officially the People's Republic of China (PRC), is a country in East Asia and is the world's most populous country, with a population of around /1e9 round 3 billion in 2017. Covering approximately , it is the third-largest or the fourth-largest country by area. Governed by the Communist Party of China, the state exercises jurisdiction over 22 provinces, five autonomous regions, four direct-controlled municipalities (Beijing, Tianjin, Shanghai, and Chongqing), and the special administrative regions of Hong Kong and Macau.

China emerged as one of the world's first civilizations, in the fertile basin of the Yellow River in the North China Plain. For millennia, China's political system was based on hereditary monarchies, or dynasties, beginning with the semi-mythical Xia dynasty in 21st century BCE. Since then, China has expanded, fractured, and re-unified numerous times. In the 3rd century BCE, the Qin reunited core China and established the first Chinese empire. The succeeding Han dynasty, which ruled from 206 BCE until 220 CE, saw some of the most advanced technology at that time, including papermaking and the compass, along with agricultural and medical improvements. The invention of gunpowder and movable type in the Tang dynasty (618–907) and Northern Song (960–1127) completed the Four Great Inventions. Tang culture spread widely in Asia, as the new Silk Route brought traders to as far as Mesopotamia and the Horn of Africa. Dynastic rule ended in 1912 with the Xinhai Revolution, when the Republic of China (ROC) replaced the Qing dynasty. China, as a whole, was ravaged by feudal warlordism and Japan during World War II. The subsequent Chinese Civil War resulted in a division of territory in 1949 when the Communist Party of China led by Mao Zedong established the People's Republic of China on mainland China while the Kuomintang-led nationalist government retreated to the island of Taiwan where it governed until 1996 when Taiwan transitioned to democracy. The political status of Taiwan remains disputed to this day.

China is a unitary one-party socialist republic and is one of the few existing socialist states. Political dissidents and human rights groups have denounced and criticized the Chinese government for human rights abuses, suppression of religious and ethnic minorities, censorship and mass surveillance, and cracking down on protests such as in 1989. The Chinese government says that the right to subsistence and economic development is a prerequisite to other types of human rights and that the notion of human rights should take into account a country's present economic level.

Since the introduction of economic reforms in 1978, China's economy has been one of the world's fastest-growing with annual growth rates consistently above 6 percent. According to the World Bank, China's GDP grew from $150 billion in 1978 to $12.24 trillion by 2017. According to official data, China's GDP in 2018 was 90 trillion Yuan ($13.5 trillion). Since 2010, China has been the world's second-largest economy by nominal GDP, and since 2014, the largest economy in the world by purchasing power parity. China is also the world's largest exporter and second-largest importer of goods. China is a recognized nuclear weapons state and has the world's largest standing army, the People's Liberation Army, and the second-largest defense budget. The PRC is a permanent member of the United Nations Security Council as it replaced the ROC in 1971, as well as an active global partner of ASEAN Plus mechanism. Since 2019, China has the highest number of rich people in the world. China has been characterized as a potential superpower, mainly because of its massive population, economy, and military.
 
The word "China" has been used in English since the 16th century. However, it was not a word used by the Chinese themselves during the period. Its origin has been traced through Portuguese, Malay, and Persian back to the Sanskrit word "Cīna", used in ancient India.

"China" appears in Richard Eden's 1555 translation of the 1516 journal of the Portuguese explorer Duarte Barbosa. Barbosa's usage was derived from Persian "Chīn" (), which was in turn derived from Sanskrit "Cīna" (). "Cīna" was first used in early Hindu scripture, including the "Mahābhārata" (5th century ) and the "Laws of Manu" (2nd century ). In 1655, Martino Martini suggested that the word China is derived ultimately from the name of the Qin dynasty (221–206 BCE), which existed as a state since the 9th century BC at the furthest west of ancient China. Although this derivation is still given in various sources, the origin of the Sanskrit word is still a matter of debate, according to the "Oxford English Dictionary". Other suggestions include the names for Yelang and the state of Jing (, another name for Chu).

The official name of the modern state is the "People's Republic of China" (). The shorter form is "China" ' from ' ("central") and "" ("state"), a term which developed under the Western Zhou dynasty in reference to its royal demesne. It was then applied to the area around Luoyi (present-day Luoyang) during the Eastern Zhou and then to China's Central Plain before being used as an occasional synonym for the state under the Qing. It was often used as a cultural concept to distinguish the Huaxia people from perceived "barbarians". The name "Zhongguo" is also translated as in English.

Archaeological evidence suggests that early hominids inhabited China between 2.24 million and 250,000 years ago. The hominid fossils of Peking Man, a "Homo erectus" who used fire, were discovered in a cave at Zhoukoudian near Beijing; they have been dated to between 680,000 and 780,000 years ago. The fossilized teeth of "Homo sapiens" (dated to 125,000–80,000 years ago) have been discovered in Fuyan Cave in Dao County, Hunan. Chinese proto-writing existed in Jiahu around 7000 , Damaidi around 6000 , Dadiwan from 5800–5400 , and Banpo dating from the 5th millennium . Some scholars have suggested that the Jiahu symbols (7th millennium ) constituted the earliest Chinese writing system.

According to Chinese tradition, the first dynasty was the Xia, which emerged around 2100 . Xia dynasty marked the beginning of China's political system based on hereditary monarchies, or dynasties, which lasted for a millennia. The dynasty was considered mythical by historians until scientific excavations found early Bronze Age sites at Erlitou, Henan in 1959. It remains unclear whether these sites are the remains of the Xia dynasty or of another culture from the same period. The succeeding Shang dynasty is the earliest to be confirmed by contemporary records. The Shang ruled the plain of the Yellow River in eastern China from the 17th to the 11th century . Their oracle bone script (from  ) represents the oldest form of Chinese writing yet found, and is a direct ancestor of modern Chinese characters.

The Shang was conquered by the Zhou, who ruled between the 11th and 5th centuries , though centralized authority was slowly eroded by feudal warlords. Some principalities eventually emerged from the weakened Zhou, no longer fully obeyed the Zhou king and continually waged war with each other in the 300-year Spring and Autumn period. By the time of the Warring States period of the 5th–3rd centuries , there were only seven powerful states left.

The Warring States period ended in 221  after the state of Qin conquered the other six kingdoms, reunited China and established the dominant order of autocracy. King Zheng of Qin proclaimed himself the First Emperor of the Qin dynasty. He enacted Qin's legalist reforms throughout China, notably the forced standardization of Chinese characters, measurements, road widths (i.e., cart axles' length), and currency. His dynasty also conquered the Yue tribes in Guangxi, Guangdong, and Vietnam. The Qin dynasty lasted only fifteen years, falling soon after the First Emperor's death, as his harsh authoritarian policies led to widespread rebellion.

Following a widespread civil war during which the imperial library at Xianyang was burned, the Han dynasty emerged to rule China between 206  and  220, creating a cultural identity among its populace still remembered in the ethnonym of the Han Chinese. The Han expanded the empire's territory considerably, with military campaigns reaching Central Asia, Mongolia, South Korea, and Yunnan, and the recovery of Guangdong and northern Vietnam from Nanyue. Han involvement in Central Asia and Sogdia helped establish the land route of the Silk Road, replacing the earlier path over the Himalayas to India. Han China gradually became the largest economy of the ancient world. Despite the Han's initial decentralization and the official abandonment of the Qin philosophy of Legalism in favor of Confucianism, Qin's legalist institutions and policies continued to be employed by the Han government and its successors.

After the end of the Han dynasty, a period of strife known as Three Kingdoms followed, whose central figures were later immortalized in one of the Four Classics of Chinese literature. At its end, Wei was swiftly overthrown by the Jin dynasty. The Jin fell to civil war upon the ascension of a developmentally-disabled emperor; the Five Barbarians then invaded and ruled northern China as the Sixteen States. The Xianbei unified them as the Northern Wei, whose Emperor Xiaowen reversed his predecessors' apartheid policies and enforced a drastic sinification on his subjects, largely integrating them into Chinese culture. In the south, the general Liu Yu secured the abdication of the Jin in favor of the Liu Song. The various successors of these states became known as the Northern and Southern dynasties, with the two areas finally reunited by the Sui in 581. The Sui restored the Han to power through China, reformed its agriculture, economy and imperial examination system, constructed the Grand Canal, and patronized Buddhism. However, they fell quickly when their conscription for public works and a failed war in northern Korea provoked widespread unrest.

Under the succeeding Tang and Song dynasties, Chinese economy, technology, and culture entered a golden age. The Tang Empire returned control of the Western Regions and the Silk Road, brought traders to as far as Mesopotamia and the Horn of Africa, and made the capital Chang'an a cosmopolitan urban center. However, it was devastated and weakened by the An Shi Rebellion in the 8th century. In 907, the Tang disintegrated completely when the local military governors became ungovernable. The Song dynasty ended the separatist situation in 960, leading to a balance of power between the Song and Khitan Liao. The Song was the first government in world history to issue paper money and the first Chinese polity to establish a permanent standing navy which was supported by the developed shipbuilding industry along with the sea trade.

Between the 10th and 11th centuries, the population of China doubled in size to around 100 million people, mostly because of the expansion of rice cultivation in central and southern China, and the production of abundant food surpluses. The Song dynasty also saw a revival of Confucianism, in response to the growth of Buddhism during the Tang, and a flourishing of philosophy and the arts, as landscape art and porcelain were brought to new levels of maturity and complexity. However, the military weakness of the Song army was observed by the Jurchen Jin dynasty. In 1127, Emperor Huizong of Song and the capital Bianjing were captured during the Jin–Song Wars. The remnants of the Song retreated to southern China.

The 13th century brought the Mongol conquest of China. In 1271, the Mongol leader Kublai Khan established the Yuan dynasty; the Yuan conquered the last remnant of the Song dynasty in 1279. Before the Mongol invasion, the population of Song China was 120 million citizens; this was reduced to 60 million by the time of the census in 1300. A peasant named Zhu Yuanzhang overthrew the Yuan in 1368 and founded the Ming dynasty as the Hongwu Emperor. Under the Ming dynasty, China enjoyed another golden age, developing one of the strongest navies in the world and a rich and prosperous economy amid a flourishing of art and culture. It was during this period that admiral Zheng He led the Ming treasure voyages throughout the Indian Ocean, reaching as far as East Africa.

In the early years of the Ming dynasty, China's capital was moved from Nanjing to Beijing. With the budding of capitalism, philosophers such as Wang Yangming further critiqued and expanded Neo-Confucianism with concepts of individualism and equality of four occupations. The scholar-official stratum became a supporting force of industry and commerce in the tax boycott movements, which, together with the famines and defense against Japanese invasions of Korea (1592–1598) and Manchu invasions led to an exhausted treasury.

In 1644, Beijing was captured by a coalition of peasant rebel forces led by Li Zicheng. The Chongzhen Emperor committed suicide when the city fell. The Manchu Qing dynasty, then allied with Ming dynasty general Wu Sangui, overthrew Li's short-lived Shun dynasty and subsequently seized control of Beijing, which became the new capital of the Qing dynasty.

The Qing dynasty, which lasted from 1644 until 1912, was the last imperial dynasty of China. Its conquest of the Ming (1618–1683) cost 25 million lives and the economy of China shrank drastically. After the Southern Ming ended, the further conquest of the Dzungar Khanate added Mongolia, Tibet and Xinjiang to the empire. The centralized autocracy was strengthened to crack down on anti-Qing sentiment with the policy of valuing agriculture and restraining commerce, the "Haijin" ("sea ban"), and ideological control as represented by the literary inquisition, causing social and technological stagnation. In the mid-19th century, the dynasty experienced Western imperialism in the Opium Wars with Britain and France. China was forced to pay compensation, open treaty ports, allow extraterritoriality for foreign nationals, and cede Hong Kong to the British under the 1842 Treaty of Nanking, the first of the Unequal Treaties. The First Sino-Japanese War (1894–95) resulted in Qing China's loss of influence in the Korean Peninsula, as well as the cession of Taiwan to Japan.
The Qing dynasty also began experiencing internal unrest in which tens of millions of people died, especially in the White Lotus Rebellion, the failed Taiping Rebellion that ravaged southern China in the 1850s and 1860s and the Dungan Revolt (1862–77) in the northwest. The initial success of the Self-Strengthening Movement of the 1860s was frustrated by a series of military defeats in the 1880s and 1890s.

In the 19th century, the great Chinese diaspora began. Losses due to emigration were added to by conflicts and catastrophes such as the Northern Chinese Famine of 1876–79, in which between 9 and 13 million people died. The Guangxu Emperor drafted a reform plan in 1898 to establish a modern constitutional monarchy, but these plans were thwarted by the Empress Dowager Cixi. The ill-fated anti-foreign Boxer Rebellion of 1899–1901 further weakened the dynasty. Although Cixi sponsored a program of reforms, the Xinhai Revolution of 1911–12 brought an end to the Qing dynasty and established the Republic of China.

On 1 January 1912, the Republic of China was established, and Sun Yat-sen of the Kuomintang (the KMT or Nationalist Party) was proclaimed provisional president. However, the presidency was later given to Yuan Shikai, a former Qing general who in 1915 proclaimed himself Emperor of China. In the face of popular condemnation and opposition from his own Beiyang Army, he was forced to abdicate and re-establish the republic.

After Yuan Shikai's death in 1916, China was politically fragmented. Its Beijing-based government was internationally recognized but virtually powerless; regional warlords controlled most of its territory. In the late 1920s, the Kuomintang, under Chiang Kai-shek, the then Principal of the Republic of China Military Academy, was able to reunify the country under its own control with a series of deft military and political manoeuvrings, known collectively as the Northern Expedition. The Kuomintang moved the nation's capital to Nanjing and implemented "political tutelage", an intermediate stage of political development outlined in Sun Yat-sen's San-min program for transforming China into a modern democratic state. The political division in China made it difficult for Chiang to battle the communist People's Liberation Army (PLA), against whom the Kuomintang had been warring since 1927 in the Chinese Civil War. This war continued successfully for the Kuomintang, especially after the PLA retreated in the Long March, until Japanese aggression and the 1936 Xi'an Incident forced Chiang to confront Imperial Japan.
The Second Sino-Japanese War (1937–1945), a theater of World War II, forced an uneasy alliance between the Kuomintang and the PLA. Japanese forces committed numerous war atrocities against the civilian population; in all, as many as 20 million Chinese civilians died. An estimated 40,000 to 300,000 Chinese were massacred in the city of Nanjing alone during the Japanese occupation. During the war, China, along with the UK, the US, and the Soviet Union, were referred to as "trusteeship of the powerful" and were recognized as the Allied "Big Four" in the Declaration by United Nations. Along with the other three great powers, China was one of the four major Allies of World War II, and was later considered one of the primary victors in the war. After the surrender of Japan in 1945, Taiwan, including the Pescadores, was returned to Chinese control. China emerged victorious but war-ravaged and financially drained. The continued distrust between the Kuomintang and the Communists led to the resumption of civil war. Constitutional rule was established in 1947, but because of the ongoing unrest, many provisions of the ROC constitution were never implemented in mainland China.

Major combat in the Chinese Civil War ended in 1949 with the Communist Party in control of most of mainland China, and the Kuomintang retreating offshore, reducing its territory to only Taiwan, Hainan, and their surrounding islands. On 21 September 1949, Communist Party Chairman Mao Zedong proclaimed the establishment of the People's Republic of China with a speech at the First Plenary Session of the Chinese People's Political Consultative Conference. This was followed by a mass celebration in Tiananmen Square on 1 October, at which the proclamation was made publicly by Mao at the Tiananmen Gate, the date becoming the new country's first National Day. In 1950, the People's Liberation Army captured Hainan from the ROC and incorporated Tibet. However, remaining Kuomintang forces continued to wage an insurgency in western China throughout the 1950s.

The regime consolidated its popularity among the peasants through land reform, which included the execution of between 1 and 2 million landlords. China developed an independent industrial system and its own nuclear weapons. The Chinese population increased from 550 million in 1950 to 900 million in 1974. However, the Great Leap Forward, an idealistic massive reform project, resulted in an estimated 15 to 35 million deaths between 1958 and 1961, mostly from starvation. In 1966, Mao and his allies launched the Cultural Revolution, sparking a decade of political recrimination and social upheaval that lasted until Mao's death in 1976. In October 1971, the PRC replaced the Republic in the United Nations, and took its seat as a permanent member of the Security Council.

After Mao's death, the Gang of Four was quickly arrested and held responsible for the excesses of the Cultural Revolution. Deng Xiaoping took power in 1978, and instituted significant economic reforms. The Party loosened governmental control over citizens' personal lives, and the communes were gradually disbanded in favor of working contracted to households. This marked China's transition from a planned economy to a mixed economy with an increasingly open-market environment. China adopted its current constitution on 4 December 1982. In 1989, the violent suppression of student protests in Tiananmen Square brought sanctions against the Chinese government from various countries.

Jiang Zemin, Li Peng and Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. The country joined the World Trade Organization in 2001, and maintained its high rate of economic growth under Hu Jintao and Wen Jiabao's leadership in the 2000s. However, the growth also severely impacted the country's resources and environment, and caused major social displacement. Living standards continued to improve rapidly despite the late-2000s recession, but political control remained tight.

Preparations for a decadal leadership change in 2012 were marked by factional disputes and political scandals. During the 18th National Communist Party Congress in November 2012, Hu Jintao was replaced as General Secretary of the Communist Party by Xi Jinping. Under Xi, the Chinese government began large-scale efforts to reform its economy, which has suffered from structural instabilities and slowing growth. The Xi–Li Administration also announced major reforms to the one-child policy and prison system.

China's landscape is vast and diverse, ranging from the Gobi and Taklamakan Deserts in the arid north to the subtropical forests in the wetter south. The Himalaya, Karakoram, Pamir and Tian Shan mountain ranges separate China from much of South and Central Asia. The Yangtze and Yellow Rivers, the third- and sixth-longest in the world, respectively, run from the Tibetan Plateau to the densely populated eastern seaboard. China's coastline along the Pacific Ocean is long and is bounded by the Bohai, Yellow, East China and South China seas. China connects through the Kazakh border to the Eurasian Steppe which has been an artery of communication between East and West since the Neolithic through the Steppe route – the ancestor of the terrestrial Silk Road(s).

The territory of China lies between latitudes 18° and 54° N, and longitudes 73° and 135° E. China's landscapes vary significantly across its vast territory. In the east, along the shores of the Yellow Sea and the East China Sea, there are extensive and densely populated alluvial plains, while on the edges of the Inner Mongolian plateau in the north, broad grasslands predominate. Southern China is dominated by hills and low mountain ranges, while the central-east hosts the deltas of China's two major rivers, the Yellow River and the Yangtze River. Other major rivers include the Xi, Mekong, Brahmaputra and Amur. To the west sit major mountain ranges, most notably the Himalayas. High plateaus feature among the more arid landscapes of the north, such as the Taklamakan and the Gobi Desert. The world's highest point, Mount Everest (8,848 m), lies on the Sino-Nepalese border. The country's lowest point, and the world's third-lowest, is the dried lake bed of Ayding Lake (−154m) in the Turpan Depression.
China's climate is mainly dominated by dry seasons and wet monsoons, which lead to pronounced temperature differences between winter and summer. In the winter, northern winds coming from high-latitude areas are cold and dry; in summer, southern winds from coastal areas at lower latitudes are warm and moist. The climate in China differs from region to region because of the country's highly complex topography.

A major environmental issue in China is the continued expansion of its deserts, particularly the Gobi Desert. Although barrier tree lines planted since the 1970s have reduced the frequency of sandstorms, prolonged drought and poor agricultural practices have resulted in dust storms plaguing northern China each spring, which then spread to other parts of East Asia, including Japan and Korea. China's environmental watchdog, SEPA, stated in 2007 that China is losing per year to desertification. Water quality, erosion, and pollution control have become important issues in China's relations with other countries. Melting glaciers in the Himalayas could potentially lead to water shortages for hundreds of millions of people.
China has a very agriculturally suitable climate and has been the largest producer of rice, wheat, tomatoes, brinjal, grapes, water melon, spinach in the world.

China is one of 17 megadiverse countries, lying in two of the world's major ecozones: the Palearctic and the Indomalaya. By one measure, China has over 34,687 species of animals and vascular plants, making it the third-most biodiverse country in the world, after Brazil and Colombia. The country signed the Rio de Janeiro Convention on Biological Diversity on 11 June 1992, and became a party to the convention on 5 January 1993. It later produced a National Biodiversity Strategy and Action Plan, with one revision that was received by the convention on 21 September 2010.

China is home to at least 551 species of mammals (the third-highest such number in the world), 1,221 species of birds (eighth), 424 species of reptiles (seventh) and 333 species of amphibians (seventh). Wildlife in China share habitat with and bear acute pressure from the world's largest population of "Homo sapiens". At least 840 animal species are threatened, vulnerable or in danger of local extinction in China, due mainly to human activity such as habitat destruction, pollution and poaching for food, fur and ingredients for traditional Chinese medicine. Endangered wildlife is protected by law, and , the country has over 2,349 nature reserves, covering a total area of 149.95 million hectares, 15 percent of China's total land area. The Baiji was confirmed extinct on 12 December 2006.

China has over 32,000 species of vascular plants, and is home to a variety of forest types. Cold coniferous forests predominate in the north of the country, supporting animal species such as moose and Asian black bear, along with over 120 bird species. The understorey of moist conifer forests may contain thickets of bamboo. In higher montane stands of juniper and yew, the bamboo is replaced by rhododendrons. Subtropical forests, which are predominate in central and southern China, support as many as 146,000 species of flora. Tropical and seasonal rainforests, though confined to Yunnan and Hainan Island, contain a quarter of all the animal and plant species found in China. China has over 10,000 recorded species of fungi, and of them, nearly 6,000 are higher fungi.

In recent decades, China has suffered from severe environmental deterioration and pollution. While regulations such as the 1979 Environmental Protection Law are fairly stringent, they are poorly enforced, as they are frequently disregarded by local communities and government officials in favor of rapid economic development. Urban air pollution is a severe health issue in the country; the World Bank estimated in 2013 that 16 of the world's 20 most-polluted cities are located in China. And China is the country with the highest death toll because of air pollution. There are 1.14 million deaths caused by exposure to ambient air pollution. China is the world's largest carbon dioxide emitter. The country also has significant water pollution problems: 40% of China's rivers had been polluted by industrial and agricultural waste by late 2011. In 2014, the internal freshwater resources per capita of China reduced to 2,062m, and it was below 500m in the North China Plain, while 5,920m in the world.
In China, heavy metals also cause environmental pollution. Heavy metal pollution is an inorganic chemical hazard, which is mainly caused by lead (Pb), chromium (Cr), arsenic (As), cadmium (Cd), mercury (Hg), zinc (Zn), copper (Cu), cobalt (Co), and nickel (Ni). Five metals among them, Pb, Cr, As, Cd, and Hg, are the key heavy metal pollutants in China. Heavy metal pollutants mainly come from mining, sewage irrigation, the manufacturing of metal-containing products, and other related production activities. High level of heavy metal exposure can also cause permanent intellectual and developmental disabilities, including reading and learning disabilities, behavioral problems, hearing loss, attention problems, and disruption in the development of visual and motor function. According to the data of a national census of pollution, China has more than 1.5 million sites of heavy metals exposure. The total volume of discharged heavy metals in the waste water, waste gas and solid wastes are around 900,000 tons each year from 2005–2011.
However, China is the world's leading investor in renewable energy and its commercialization, with $52 billion invested in 2011 alone; it is a major manufacturer of renewable energy technologies and invests heavily in local-scale renewable energy projects. By 2015, over 24% of China's energy was derived from renewable sources, while most notably from hydroelectric power: a total installed capacity of 197 GW makes China the largest hydroelectric power producer in the world. China also has the largest power capacity of installed solar photovoltaics system and wind power system in the world. In 2011, the Chinese government announced plans to invest four trillion yuan (US$619 billion) in water infrastructure and desalination projects over a ten-year period, and to complete construction of a flood prevention and anti-drought system by 2020. In 2013, China began a five-year, US$277 billion effort to reduce air pollution, particularly in the north of the country.

The People's Republic of China is the second-largest country in the world by land area after Russia, and is the third largest by total area, after Russia and Canada. China's total area is generally stated as being approximately . Specific area figures range from according to the "Encyclopædia Britannica", to according to the UN Demographic Yearbook, and the CIA World Factbook.

China has the longest combined land border in the world, measuring from the mouth of the Yalu River (Amnok River) to the Gulf of Tonkin. China borders 14 nations, more than any other country except Russia, which also borders 14. China extends across much of East Asia, bordering Vietnam, Laos, and Myanmar (Burma) in Southeast Asia; India, Bhutan, Nepal, Afghanistan, and Pakistan in South Asia; Tajikistan, Kyrgyzstan and Kazakhstan in Central Asia; and Russia, Mongolia, and North Korea in Inner Asia and Northeast Asia. Additionally, China shares maritime boundaries with South Korea, Japan, Vietnam, and the Philippines.

China's constitution states that The People's Republic of China "is a socialist state under the people's democratic dictatorship led by the working class and based on the alliance of workers and peasants," and that the state organs "apply the principle of democratic centralism." The PRC is one of the world's only socialist states explicitly aiming to build communism. The Chinese government has been variously described as communist and socialist, but also as authoritarian and corporatist, with heavy restrictions in many areas, most notably against free access to the Internet, freedom of the press, freedom of assembly, the right to have children, free formation of social organizations and freedom of religion. Its current political, ideological and economic system has been termed by its leaders as the "people's democratic dictatorship", "socialism with Chinese characteristics" (which is Marxism adapted to Chinese circumstances) and the "socialist market economy" respectively.

Since 2018, the main body of the Chinese constitution declares that "the defining feature of socialism with Chinese characteristics is the leadership of the Communist Party of China (CPC)." The 2018 amendments constitutionalized the "de facto" one-party state status of China, wherein the General Secretary (party leader) holds ultimate power and authority over state and government and serves as the paramount leader of China. The electoral system is pyramidal. Local People's Congresses are directly elected, and higher levels of People's Congresses up to the National People's Congress (NPC) are indirectly elected by the People's Congress of the level immediately below. The political system is decentralized, and provincial and sub-provincial leaders have a significant amount of autonomy. Another eight political parties, have representatives in the NPC and the Chinese People's Political Consultative Conference (CPPCC). China supports the Leninist principle of "democratic centralism", but critics describe the elected National People's Congress as a "rubber stamp" body.

The President is the titular head of state, elected by the National People's Congress. The Premier is the head of government, presiding over the State Council composed of four vice premiers and the heads of ministries and commissions. The incumbent president is Xi Jinping, who is also the General Secretary of the Communist Party of China and the Chairman of the Central Military Commission, making him China's paramount leader. The incumbent premier is Li Keqiang, who is also a senior member of the CPC Politburo Standing Committee, China's "de facto" top decision-making body.

There have been some moves toward political liberalization, in that open contested elections are now held at the village and town levels. However, the party retains effective control over government appointments: in the absence of meaningful opposition, the CPC wins by default most of the time. Political concerns in China include the growing gap between rich and poor and government corruption. Nonetheless, the level of public support for the government and its management of the nation is high, with 80–95% of Chinese citizens expressing satisfaction with the central government, according to a 2011 survey.

The People's Republic of China is divided into 22 provinces, five autonomous regions (each with a designated minority group), and four municipalities—collectively referred to as "mainland China"—as well as the special administrative regions (SARs) of Hong Kong and Macau. Geographically, all 31 provincial divisions of mainland China can be grouped into six regions: North China, Northeast China, East China, South Central China, Southwest China, and Northwest China.

China considers Taiwan to be its 23rd province, although Taiwan is governed by the Republic of China (ROC), which rejects the PRC's claim. Conversely, the ROC claims sovereignty over all divisions governed by the PRC.

The PRC has diplomatic relations with 175 countries and maintains embassies in 162. Its legitimacy is disputed by the Republic of China and a few other countries; it is thus the largest and most populous state with limited recognition. In 1971, the PRC replaced the Republic of China as the sole representative of China in the United Nations and as one of the five permanent members of the United Nations Security Council. China was also a former member and leader of the Non-Aligned Movement, and still considers itself an advocate for developing countries. Along with Brazil, Russia, India and South Africa, China is a member of the BRICS group of emerging major economies and hosted the group's third official summit at Sanya, Hainan in April 2011.

Under its interpretation of the One-China policy, Beijing has made it a precondition to establishing diplomatic relations that the other country acknowledges its claim to Taiwan and severs official ties with the government of the Republic of China. Chinese officials have protested on numerous occasions when foreign countries have made diplomatic overtures to Taiwan, especially in the matter of armament sales.

Much of current Chinese foreign policy is reportedly based on Premier Zhou Enlai's Five Principles of Peaceful Coexistence, and is also driven by the concept of "harmony without uniformity", which encourages diplomatic relations between states despite ideological differences. This policy may have led China to support states that are regarded as dangerous or repressive by Western nations, such as Zimbabwe, North Korea and Iran. China has a close economic and military relationship with Russia, and the two states often vote in unison in the UN Security Council.

China became the world's largest trading nation in 2013, as measured by the sum of imports and exports. By 2016, China was the largest trading partner of 124 other countries. In 2019, China's exports of goods and services was $2.5 trillion and imports was $2.1 trillion, thus resulting in $4.6 trillion of foreign trade. In recent decades, China has played an increasing role in calling for free trade areas and security pacts amongst its Asia-Pacific neighbours. China became a member of the World Trade Organization (WTO) on 11 December 2001. In 2004, it proposed an entirely new East Asia Summit (EAS) framework as a forum for regional security issues. The EAS, which includes ASEAN Plus Three, India, Australia and New Zealand, held its inaugural summit in 2005. China is also a founding member of the Shanghai Cooperation Organization (SCO), along with Russia and the Central Asian republics.

China has had a long and complex trade relationship with the United States. In 2000, the United States Congress approved "permanent normal trade relations" (PNTR) with China, allowing Chinese exports in at the same low tariffs as goods from most other countries. China has a significant trade surplus with the United States, its most important export market. In the early 2010s, US politicians argued that the Chinese yuan was significantly undervalued, giving China an unfair trade advantage.

Since the turn of the century, China has followed a policy of engaging with African nations for trade and bilateral co-operation; in 2012, Sino-African trade totalled over US$160 billion. China maintains healthy and highly diversified trade links with the European Union. China has furthermore strengthened its ties with major South American economies, becoming the largest trading partner of Brazil and building strategic links with Argentina.

China's Belt and Road Initiative has expanded significantly over the last six years and, as of 2019, includes 137 countries and 30 international organizations.

Ever since its establishment after the second Chinese Civil War, the PRC has claimed the territories governed by the Republic of China (ROC), a separate political entity today commonly known as Taiwan, as a part of its territory. It regards the island of Taiwan as its Taiwan Province, Kinmen and Matsu as a part of Fujian Province and islands the ROC controls in the South China Sea as a part of Hainan Province and Guangdong Province. These claims are controversial because of the complicated Cross-Strait relations, with the PRC treating the One-China policy as one of its most important diplomatic principles.

In addition to Taiwan, China is also involved in other international territorial disputes. Since the 1990s, China has been involved in negotiations to resolve its disputed land borders, including a disputed border with India and an undefined border with Bhutan. China is additionally involved in multilateral disputes over the ownership of several small islands in the East and South China Seas, such as the Senkaku Islands and the Scarborough Shoal. On 21 May 2014 Xi Jinping, speaking at a conference in Shanghai, pledged to settle China's territorial disputes peacefully. "China stays committed to seeking peaceful settlement of disputes with other countries over territorial sovereignty and maritime rights and interests", he said.

China is regularly hailed as a potential new superpower, with certain commentators citing its rapid economic progress, growing military might, very large population, and increasing international influence as signs that it will play a prominent global role in the 21st century. Others, however, warn that economic bubbles and demographic imbalances could slow or even halt China's growth as the century progresses.
Some authors also question the definition of "superpower", arguing that China's large economy alone would not qualify it as a superpower, and noting that it lacks the military power and cultural influence of the United States. Others — namely, IRENA — have argued that China's investment in renewable energy has positioned it to be a likely superpower in that area, especially due to a shift in global trade relations.

The Chinese democracy movement, social activists, and some members of the Communist Party of China have all identified the need for social and political reform. While economic and social controls have been significantly relaxed in China since the 1970s, political freedom is still tightly restricted. The Constitution of the People's Republic of China states that the "fundamental rights" of citizens include freedom of speech, freedom of the press, the right to a fair trial, freedom of religion, universal suffrage, and property rights. However, in practice, these provisions do not afford significant protection against criminal prosecution by the state. Although some criticisms of government policies and the ruling Communist Party are tolerated, censorship of political speech and information, most notably on the Internet, are routinely used to prevent collective action. By 2020, China plans to give all its citizens a personal "Social Credit" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology. In 2005, Reporters Without Borders ranked China 159th out of 167 states in its Annual World Press Freedom Index, which indicates a very low level of press freedom. In 2014, China ranked 175th out of 180 countries.

Rural migrants to China's cities often find themselves treated as second class citizens by the "hukou" household registration system, which controls access to state benefits. Property rights are often poorly protected, However, a number of rural taxes have been reduced or abolished since the early 2000s, and additional social services provided to rural dwellers.

A number of foreign governments, foreign press agencies, and NGOs also routinely criticize China's human rights record, alleging widespread civil rights violations such as detention without trial, forced abortions, forced confessions, torture, restrictions of fundamental rights, and excessive use of the death penalty. The government suppresses popular protests and demonstrations that it considers a potential threat to "social stability", as was the case with the Tiananmen Square protests of 1989.
Falun Gong was first taught publicly in 1992. In 1999, when there were about 70 million practitioners, the persecution of Falun Gong began, resulting in mass arrests, extralegal detention, and alleged reports of torture and deaths in custody. The Chinese state is regularly accused of large-scale repression and human rights abuses in Tibet and Xinjiang, including violent police crackdowns and religious suppression. At least one million members of China's Muslim Uyghur minority have been detained in mass detention camps, termed "Vocational Education and Training Centers", aimed at changing the political thinking of detainees, their identities, and their religious beliefs. The camps were established under General Secretary Xi Jinping's administration. In January 2019 the United Nations asked for direct access to the detention camps after a panel said it had received “credible reports” that 1.1 million Uygur, Kazakhs, Hui and other ethnic minorities had been detained in these camps. The state has also sought to control offshore reporting of tensions in Xinjiang, intimidating foreign-based reporters by detaining their family members.

The Chinese government has responded to foreign criticism by arguing that the right to subsistence and economic development is a prerequisite to other types of human rights, and that the notion of human rights should take into account a country's present level of economic development. It emphasizes the rise in the Chinese standard of living, literacy rate, and average life expectancy since the 1970s, as well as improvements in workplace safety and efforts to combat natural disasters such as the perennial Yangtze River floods. Furthermore, some Chinese politicians have spoken out in support of democratization, although others remain more conservative. Some major reform efforts have been conducted. For instance, in November 2013 the government announced plans to relax the one-child policy and abolish the much-criticized re-education through labour program, although human rights groups note that reforms to the latter have been largely cosmetic. During the 2000s and early 2010s, the Chinese government was increasingly tolerant of NGOs that offer practical, efficient solutions to social problems, but such "third sector" activity remained heavily regulated. After Xi Jinping succeeded General Secretary of the Communist Party of China in 2012, human rights in China have become worse.
The Global Slavery Index estimated that in 2016 more than 3.8 million people were living in "conditions of modern slavery", or 0.25% of the population, including victims of human trafficking, forced labor, forced marriage, child labor, and state-imposed forced labor. All except the last category are illegal. The state-imposed forced system was formally abolished in 2013 but it is not clear the extent to which its various practices have stopped. The Chinese penal system includes labor prison factories, detention centers, and re-education camps, which fall under the heading Laogai ("reform through labor"). The Laogai Research Foundation in the United States estimated that there were over a thousand slave labour prisons and camps, known collectively as the Laogai. Prisoners are not paid at all, and need their families to send money to them. Prisoners who refuse to work are beaten, and some are beaten to death. Many of the prisoners are political or religious dissidents, and some are recognized internationally as prisoners of conscience. A Chinese leader said that they want to see two products coming out of the prisons: the man who has been reformed, and the product made by the man. Harry Wu, himself a former prisoner of the Laogai, filmed undercover footage of the Laogai, and was charged with stealing state secrets. For this, Harry Wu was sentenced to 15 years in prison, but only served 66 days before being deported to the United States.

In 2019 a world-first study called for the mass retraction of more than 400 scientific papers on organ transplantation, because of fears the organs were obtained unethically from Chinese prisoners. The study was published in the medical journal BMJ Open. A report published in 2016 found a large discrepancy between official transplant figures from the Chinese government and the number of transplants reported by hospitals. While the government says 10,000 transplants occur each year, hospital data shows between 60,000 and 100,000 organs are transplanted each year. The report provided evidence that this gap is being made up by executed prisoners of conscience.

With 2.3 million active troops, the People's Liberation Army (PLA) is the largest standing military force in the world, commanded by the Central Military Commission (CMC). China has the second-biggest military reserve force, only behind North Korea. The PLA consists of the Ground Force (PLAGF), the Navy (PLAN), the Air Force (PLAAF), and the People's Liberation Army Rocket Force (PLARF). According to the Chinese government, China's military budget for 2017 totalled US$151.5 billion, constituting the world's second-largest military budget, although the military expenditures-GDP ratio with 1.3% of GDP is below world average. However, many authorities – including SIPRI and the U.S. Office of the Secretary of Defense – argue that China does not report its real level of military spending, which is allegedly much higher than the official budget.

As a recognized nuclear weapons state, China is considered both a major regional military power and a potential military superpower. According to a 2013 report by the US Department of Defense, China fields between 50 and 75 nuclear ICBMs, along with a number of SRBMs. However, compared with the other four UN Security Council Permanent Members, China has relatively limited power projection capabilities. To offset this, it has developed numerous power projection assets since the early 2000s – its first aircraft carrier entered service in 2012, and it maintains a substantial fleet of submarines, including several nuclear-powered attack and ballistic missile submarines. China has furthermore established a network of foreign military relationships along critical sea lanes.

China has made significant progress in modernising its air force in recent decades, purchasing Russian fighter jets such as the Sukhoi Su-30, and also manufacturing its own modern fighters, most notably the Chengdu J-10, J-20 and the Shenyang J-11, J-15, J-16, and J-31. China is furthermore engaged in developing an indigenous stealth aircraft and numerous combat drones. Air and Sea denial weaponry advances have increased the regional threat from the perspective of Japan as well as Washington. China has also updated its ground forces, replacing its ageing Soviet-derived tank inventory with numerous variants of the modern Type 99 tank, and upgrading its battlefield C3I and C4I systems to enhance its network-centric warfare capabilities. In addition, China has developed or acquired numerous advanced missile systems, including anti-satellite missiles, cruise missiles and submarine-launched nuclear ICBMs. According to the Stockholm International Peace Research Institute's data, China became the world's third largest exporter of major arms in 2010–14, an increase of 143 percent from the period 2005–09. SIPRI also calculated that China surpassed Russia to become the world's second largest arms exporter by 2020. Chinese officials stated that spending on the military will rise to U.S. $173B in 2018. In the period of 2014-2018, China was the fifth largest exporter of major arms in the world, with an increase of 2.7% compared to the previous period.
In August 2018, China tested its first hypersonic flight. The China Academy of Aerospace Aerodynamics (CAAA) claims to have successfully conducted the test with the aircraft Starry Sky-2 that touched a speed of Mach 6 – which is six times the speed of sound, that can carry nuclear missiles.

As of 2018, China had the world's second-largest economy in terms of nominal GDP, totaling approximately US$13.5 trillion (90 trillion Yuan). In terms of purchasing power parity (PPP GDP), China's economy has been the largest in the world since 2014, according to the World Bank. According to the World Bank, China's GDP grew from $150 billion in 1978 to $13.6 trillion by 2018. China's economic growth has been consistently above 6 percent since the introduction of economic reforms in 1978. China is also the world's largest exporter and second-largest importer of goods. Between 2010 and 2019, China's contribution to global GDP growth has been 25% to 39%.

China had the largest economy in the world for most of the past two thousand years, during which it has seen cycles of prosperity and decline. Since economic reforms began in 1978, China has developed into a highly diversified economy and one of the most consequential players in international trade. Major sectors of competitive strength include manufacturing, retail, mining, steel, textiles, automobiles, energy generation, green energy, banking, electronics, telecommunications, real estate, e-commerce, and tourism. China has three out of the ten largest stock exchanges in the world—Shanghai, Hong Kong and Shenzhen—that together have a market capitalization of over $10 trillion, as of 2019.

China has been the world's #1 manufacturer since 2010, after overtaking the US, which had been #1 for the previous hundred years. China has also been #2 in high-tech manufacturing since 2012, according to US National Science Foundation. China is the second largest retail market in the world, next to the United States. China leads the world in e-commerce, accounting for 40% of the global market share in 2016 and more than 50% of the global market share in 2019. China is the world's leader in electric vehicles, manufacturing and buying half of all the plug-in electric cars (BEV and PHEV) in the world in 2018. China had 174 GW of installed solar capacity by the end of 2018, which amounts to more than 40% of the global solar capacity.
China has been the world's second-largest economy in terms of nominal GDP since 2010. In terms of purchasing power parity (PPP) GDP, China's economy has been the largest in the world since 2014. As of 2018, China was second in the world in total number of billionaires and millionaires—there were 338 Chinese billionaires and 3.5 million millionaires. However, it ranks behind over 70 countries (out of around 180) in per capita economic output, making it a middle income country. Additionally, its development is highly uneven. Its major cities and coastal areas are far more prosperous compared to rural and interior regions. China brought more people out of extreme poverty than any other country in history—between 1978 and 2018, China reduced extreme poverty by 800 million. China reduced the extreme poverty rate—per international standard, it refers to an income of less than $1.90/day—from 88% in 1981 to 1.85% by 2013. According to the World Bank, the number of Chinese in extreme poverty fell from 756 million to 25 million between 1990 and 2013. China's own national poverty standards are higher and thus the national poverty rates were 3.1% in 2017 and 1% in 2018.

In 2019, China overtook the US as the home to the highest number of rich people in the world, according to the global wealth report by Credit Suisse. In other words, as of 2019, 100 million Chinese are in the top 10% of the wealthiest individuals in the world—those who have a net personal wealth of at least $110,000.

From its founding in 1949 until late 1978, the People's Republic of China was a Soviet-style centrally planned economy. Following Mao's death in 1976 and the consequent end of the Cultural Revolution, Deng Xiaoping and the new Chinese leadership began to reform the economy and move towards a more market-oriented mixed economy under one-party rule. Agricultural collectivization was dismantled and farmlands privatized, while foreign trade became a major new focus, leading to the creation of Special Economic Zones (SEZs). Inefficient state-owned enterprises (SOEs) were restructured and unprofitable ones were closed outright, resulting in massive job losses. Modern-day China is mainly characterized as having a market economy based on private property ownership, and is one of the leading examples of state capitalism. The state still dominates in strategic "pillar" sectors such as energy production and heavy industries, but private enterprise has expanded enormously, with around 30 million private businesses recorded in 2008. In 2018, private enterprises in China accounted for 60% of GDP, 80% of urban employment and 90% of new jobs.

In 2015, China's Middle Class became the largest in the world. By 2019, there were more Chinese than Americans in the richest 10% of the people in the world. Since economic liberalization began in 1978, China has been among the world's fastest-growing economies, relying originally on investment- and export-led growth, but now shifting more to consumption. In 2018, 76% of growth in China's GDP came from consumption. According to the IMF, China's annual average GDP growth between 2001 and 2010 was 10.5%. In the years immediately following the financial crisis of 2007, China's economic growth rate was equivalent to all of the G7 countries' growth combined. According to the Global Growth Generators index announced by Citigroup in February 2011, China has a very high 3G growth rating. Its high productivity, low labor costs and relatively good infrastructure have made it a global leader in manufacturing.

China ranks #1 in the production of steel, aluminum and automobiles—China's global market shares are 50% in steel, 50% in aluminum and 30% in automobile manufacturing. China has also been increasingly turning to automation, becoming the world's largest market for industrial robots in 2013. Between 2010 and 2015, China installed 90,000 industrial robots, or one-third of the world's total. In 2017, China bought 36% of all the new industrial robots in the world. China's plan is to also domestically design and manufacture 100,000 industrial robots by 2020. However, the Chinese economy is highly energy-intensive and inefficient; China became the world's largest energy consumer in 2010, relies on coal to supply over 70% of its energy needs, and surpassed the US to become the world's largest oil importer in 2013.

In the last decade, China has become #1 in the world in terms of installed solar power capacity, hydro-power and wind power. According to the World Economic Forum, China will account for 40% of the global renewable energy by 2022. In addition, official GDP figures are seen as unreliable and there have been several well-publicized cases of data manipulation. In the early 2010s, China's economic growth rate began to slow amid domestic credit troubles, weakening international demand for Chinese exports and fragility in the global economy. China's GDP was smaller than Germany's in 2007; however, by 2017, China's $12.2 trillion-economy became larger than those of Germany, UK, France and Italy combined. In 2018, the IMF reiterated its forecast that China will overtake the US in terms of nominal GDP by the year 2030. Economists also expect China's middle class to expand to 600 million people by 2025.

Tourism is a major contributor to the economy. In 2017, this sector contributed about CNY 8.77 trillion (US$1.35 trillion), 11.04% of the GDP, and contributed direct and indirect employment of up to 28.25 million people. There were 139.48 million inbound trips and five billion domestic trips. China is now #1 in the number of skyscrapers (buildings taller than 200m), accounting for about 50% of world's total. In four years—2015 through 2018—China built 310 skyscrapers, while the corresponding number for the US was 33.

China is one of the world's most technologically advanced nations. It is the world's largest e-commerce market, amounting to 42% of the global market by 2016 and is expected to account for 55% of global e-commerce retail sales in 2019 (more than three times as large as the US market). China's e-commerce market had online sales of more than $1 trillion in 2018, according to PWC and is expected to be just under $2 trillion in 2019. China's e-commerce industry took off in 2009, marked by the growth of internet giants Tencent and Alibaba – purveyors of products such as WeChat and Tmall – that have become ubiquitous in contemporary Chinese life. Tencent's WeChat Pay and Alibaba's Ali Pay have helped China become a world leader in mobile payments, which amounted to about $30 trillion in China in 2017 and more than $40 trillion in 2018.

China is also second only to the United States in venture capital activity and is home to a large number of unicorn startup companies. In 2018, China attracted $105 billion of venture capital investments, amounting to 38% of global VC investments that year. In late 2018, the world's most valuable startup was ByteDance, a Chinese company; and the two most valuable artificial intelligence (AI) startups in the world were SenseTime and Face++, both from China. In 2017, China's State Council released its Artificial Intelligence Development Plan, which declared AI technology a priority economic growth and investment sector. In 2018, China created 97 "unicorns" – startups that are worth more than $1 billion – which amounted to 1 unicorn every 3.8 days. Chinese smartphone brands – Huawei, Xiaomi, Oppo, Vivo, OnePlus etc. – have captured more than 40% of the global market. By 2019, Huawei had become the world's largest telecom infrastructure provider, surpassing Nokia and Ericsson, and had taken the lead in 5G technology. The company also entered the consumer smartphone and enterprise services markets, and is the world's third-largest smartphone company, after Apple and Samsung.

Other Chinese tech giants include DJI (world's largest drone maker with 70% market share), BOE (world's largest flat-panel display maker), Didi (world's largest and most valuable ride-sharing company) and BYD (world's largest plug-in vehicle maker—including both BEV and PHEV) China also the most number of supercomputers—227 out of the Top 500 (as of 2019).

China is also the world leader in patents, accounting for 1.54 million patents or almost half of all global patents in 2018. China's spending on R&D has been growing rapidly in the last decade, amounting to $277 billion in 2017.

China is a member of the WTO and is the world's largest trading power, with a total international trade value of US$4.62 trillion in 2018. Its foreign exchange reserves reached US$3.1 trillion as of 2019, making its reserves by far the world's largest. In 2012, China was the world's largest recipient of inward foreign direct investment (FDI), attracting $253 billion. In 2014, China's foreign exchange remittances were $US64 billion making it the second largest recipient of remittances in the world. China also invests abroad, with a total outward FDI of $62.4 billion in 2012, and a number of major takeovers of foreign firms by Chinese companies. China is a major owner of US public debt, holding trillions of dollars worth of U.S. Treasury bonds. China's undervalued exchange rate has caused friction with other major economies, and it has also been widely criticized for manufacturing large quantities of counterfeit goods.

China ranks 17th in the world in Global Innovation Index, not too far from the US, which ranks 6th. China ranks 27th out of 137 countries in the 2017-2018 Global Competitiveness Index, above many advanced economies and making it by far the most competitive major emerging economy. This is largely owing to its strength in infrastructure and wide adoption of communication and information technology. However, it lags behind advanced economies in labor market efficiency, institutional strength, and openness of market competition, especially for foreign players attempting to enter the domestic market. In 2018, "Fortune"'s Global 500 list of the world's largest corporations included 120 Chinese companies. Many of the largest public companies in the world were Chinese, including the world's largest bank by total assets, the Industrial and Commercial Bank of China.

Following the 2007-08 financial crisis, Chinese authorities sought to actively wean off of its dependence on the U.S. Dollar as a result of perceived weaknesses of the international monetary system. To achieve those ends, China took a series of actions to further the internationalization of the Renminbi. In 2008, China established dim sum bond market and expanded the Cross-Border Trade RMB Settlement Pilot Project, which helps establish pools of offshore RMB liquidity. This was followed with bilateral agreements to settle trades directly in renminbi with Russia, Japan, Australia, Singapore, the United Kingdom, and Canada. As a result of the rapid internationalization of the renminbi, it became the eighth-most-traded currency in the world, an emerging international reserve currency, and a component of the IMF's special drawing rights; however, partly due to capital controls that make the renminbi fall short of being a fully convertible currency, it remains far behind the Euro, Dollar and Japanese Yen in international trade volumes.

China has had the world's largest middle class population since 2015, and the middle class grew to a size of 400 million by 2018. China's middle-class population (if defined as those with annual income of between US$10,000 and US$60,000) had reached more than 300 million by 2012. Wages in China have grown exponentially in the last 40 years—real (inflation-adjusted) wages grew seven-fold from 1978 to 2007. By 2018, median wages in Chinese cities such as Shanghai were about the same as or higher than the wages in Eastern European countries. More than 75 percent of China's urban consumers are expected to earn between 60.000 and 229.000 RMB per year by 2022. China has the world's second-highest number of billionaires, with nearly 400 as of 2018, increasing at the rate of roughly two per week. China's domestic retail market was worth over 20 trillion yuan (US$3.2 trillion) in 2012 and is growing at over 12% annually , while the country's luxury goods market has expanded immensely, with 27.5% of the global share. However, in recent years, China's rapid economic growth has contributed to severe consumer inflation, leading to increased government regulation. China has a high level of economic inequality, which has increased in the past few decades. In 2012, China's official Gini coefficient was 0.474. A study conducted by Southwestern University of Finance and Economics showed that China's Gini coefficient actually had reached 0.61 in 2012, and top 1% Chinese held more than 25% of China's wealth. By 2015, China's GINI index had fallen to 0.38, according to World Bank.

China was once a world leader in science and technology up until the Ming dynasty. Ancient Chinese discoveries and inventions, such as papermaking, printing, the compass, and gunpowder (the Four Great Inventions), became widespread across East Asia, the Middle East and later to Europe. Chinese mathematicians were the first to use negative numbers. By the 17th century, Europe and the Western world surpassed China in scientific and technological advancement. The causes of this early modern Great Divergence continue to be debated by scholars to this day.

After repeated military defeats by the European colonial powers and Japan in the 19th century, Chinese reformers began promoting modern science and technology as part of the Self-Strengthening Movement. After the Communists came to power in 1949, efforts were made to organize science and technology based on the model of the Soviet Union, in which scientific research was part of central planning. After Mao's death in 1976, science and technology was established as one of the Four Modernizations, and the Soviet-inspired academic system was gradually reformed.

Since the end of the Cultural Revolution, China has made significant investments in scientific research and is quickly catching up with the US in R&D spending. In 2017, China spent $279 billion on scientific research and development. According to OECD, China spent 2.11% of its GDP on Research and Development (R&D) in 2016. Science and technology are seen as vital for achieving China's economic and political goals, and are held as a source of national pride to a degree sometimes described as "techno-nationalism". Nonetheless, China's investment in basic and applied scientific research remains behind that of leading technological powers such as the United States and Japan. According to the US National Science Board, China had, for the first time, more science and engineering publications than the US, in 2016. Also, in 2016, China spent $409 billion (by PPP) on Research and Development. In 2018, China is estimated to have spent $475 billion (by PPP), second only to the USA. In 2017, China was #2 in international patents application, behind the US but ahead of Japan. Chinese tech companies Huawei and ZTE were the top 2 filers of international patents in 2017. Chinese-born scientists have won the Nobel Prize in Physics four times, the Nobel Prize in Chemistry and Physiology or Medicine once respectively, though most of these scientists conducted their Nobel-winning research in western nations.

China is developing its education system with an emphasis on science, mathematics and engineering; in 2009, China graduated over 10,000 Ph.D. engineers, and as many as 500,000 BSc graduates, more than any other country. In 2016, there were 4.7 million STEM (Science, Technology, Engineering and Mathematics) graduates in China, which was more than eight times the corresponding number for the US. China also became the world's largest publisher of scientific papers, by 2016. Chinese technology companies such as Huawei and Lenovo have become world leaders in telecommunications and personal computing, and Chinese supercomputers are consistently ranked among the world's most powerful. China is also expanding its use of industrial robots; from 2008 to 2011, the installation of multi-role robots in Chinese factories rose by 136 percent. China has been the world's largest market for industrial robots since 2013 and will account for 45% of newly installed robots from 2019-2021.

The Chinese space program is one of the world's most active, and is a major source of national pride. In 2018, China successfully launched more satellites (35) than any other country, including the USA (30). In 1970, China launched its first satellite, Dong Fang Hong I, becoming the fifth country to do so independently. In 2003, China became the third country to independently send humans into space, with Yang Liwei's spaceflight aboard Shenzhou 5; , ten Chinese nationals have journeyed into space, including two women. In 2011, China's first space station module, Tiangong-1, was launched, marking the first step in a project to assemble a large manned station by the early 2020s. In 2013, China successfully landed the Chang'e 3 lander and Yutu rover onto the lunar surface. In 2016, China's 2nd space station module, Tiangong-2, was launched from Jiuquan aboard a Long March 2F rocket on 15 September 2016. Then Shenzhou 11 successfully docked with Tiangong-2 on 19 October 2016. In 2019, China became the first country to land a probe—Chang'e 4—on the far side of the moon.

A 2016 report by McKinsey consulting group, revealed that China has been annually spending more on infrastructure than North America and Western Europe combined.

China is the largest telecom market in the world and currently has the largest number of active cellphones of any country in the world, with over 1.5 billion subscribers, as of 2018. It also has the world's largest number of internet and broadband users, with over 800 million Internet users —equivalent to around 60% of its population—and almost all of them being mobile as well. Almost entire China's population had access to 4G network by 2017. By 2018, China had more than 1 billion 4G users, accounting for 40% of world's total. In terms of unique mobile subscribers as percentage of population, China came in at 82%, placing the country #3 in the world (as of 2018). As of early 2019, the average mobile connection speed in China was 30 Mbit/s (megabits per second), which is 9% slower than the US. As for fixed broadband in China, the average download speed was 76 Mbit/s; and 60% of fixed broadband Chinese users (or 200 million Chinese households) were able to access the Internet at 100 Mbit/s or higher (as of 2018). China is making rapid progress in 1 Gbit/s (1000 Mbit/s) internet, and 42% of Chinese homes are expected to have 1 Gbit/s broadband link by 2023. In 2018, China had 378 million fixed broadband users and 87% of them were fiber-optic users, making China #1 in the world in deployment of fiber-optic cables for broadband. By the end of 2017, China had 29 million kilometers of fiber-optic cable. In 2019, China is expected to account for 24% of the world's spending on IoT or internet-connected devices. Since 2011 China has been the nation with the most installed telecommunication bandwidth in the world. By 2014, China hosted more than twice as much national bandwidth potential than the U.S., the historical leader in terms of installed telecommunication bandwidth (China: 29% versus US:13% of the global total). China is making rapid advances in 5G—by late 2018, China had started large-scale and commercial 5G trials. In early 2019, Shanghai railway station introduced 5G WiFi that has an internet speed of 1,200 Mbit/s.

China Mobile, China Unicom and China Telecom, are the three large providers of mobile and internet in China. China Telecom alone served more than 145 million broadband subscribers and 300 million mobile users; China Unicom had about 300 million subscribers; and China Mobile, the biggest of them all, had 925 million users, as of 2018. Combined, the three operators had over 3.4 million 4G base-stations in China. Several Chinese telecommunications companies, most notably Huawei and ZTE, have been accused of spying for the Chinese military. British intelligence—GCHQ and NCSC—said in 2019 that there have been no evidence of malicious activity or spying by Huawei.

China is developing its own satellite navigation system, dubbed Beidou, which began offering commercial navigation services across Asia in 2012 and it started providing global services by the end of 2018. Now China belongs to the elite group of three countries—US and Russia being the other two members—that provide global satellite navigation.

Since the late 1990s, China's national road network has been significantly expanded through the creation of a network of national highways and expressways. In 2018, China's highways had reached a total length of , making it the longest highway system in the world; and China's railways reached a total length of 127,000 km by 2017. By the end of 2018, China's high-speed railway network reached a length of 29,000 km, representing more than 60% of the world's total. In 1991, there were only six bridges across the main stretch of the Yangtze River, which bisects the country into northern and southern halves. By October 2014, there were 81 such bridges and tunnels.
China has the world's largest market for automobiles, having surpassed the United States in both auto sales and production. Sales of passenger cars in 2016 exceeded 24 million. A side-effect of the rapid growth of China's road network has been a significant rise in traffic accidents, with poorly enforced traffic laws cited as a possible cause—in 2011 alone, around 62,000 Chinese died in road accidents. However, the Chinese government has taken a lot of steps to address this problem and has reduced the number of fatalities in traffic accidents by 20% from 2007 to 2017. In urban areas, bicycles remain a common mode of transport, despite the increasing prevalence of automobiles – , there are approximately 470 million bicycles in China.

China's railways, which are state-owned, are among the busiest in the world, handling a quarter of the world's rail traffic volume on only 6 percent of the world's tracks in 2006. , the country had of railways, the second longest network in the world. The railways strain to meet enormous demand particularly during the Chinese New Year holiday, when the world's largest annual human migration takes place. In 2013, Chinese railways delivered 2.106 billion passenger trips, generating 1,059.56 billion passenger-kilometers and carried 3.967 billion tons of freight, generating 2,917.4 billion cargo tons-kilometers.

China's high-speed rail (HSR) system started construction in the early 2000s. By the end of 2018, high speed rail in China had over of dedicated lines alone, a length that exceeds rest of the world's high-speed rail tracks combined, making it the longest HSR network in the world. With an annual ridership of over 1.1 billion passengers in 2015 it is the world's busiest. The network includes the Beijing–Guangzhou–Shenzhen High-Speed Railway, the single longest HSR line in the world, and the Beijing–Shanghai High-Speed Railway, which has three of longest railroad bridges in the world. The HSR track network is set to reach approximately by the end of 2019. The Shanghai Maglev Train, which reaches , is the fastest commercial train service in the world. In May 2019, China released a prototype for a maglev high-speed train that would reach a speed of 600 km/hr (375 mph); and it is expected to go into commercial production by 2021. 

Since 2000, the growth of rapid transit systems in Chinese cities has accelerated. , 26 Chinese cities have urban mass transit systems in operation and 39 more have metro systems approved with a dozen more to join them by 2020. The Shanghai Metro, Beijing Subway, Guangzhou Metro, Hong Kong MTR and Shenzhen Metro are among the longest and busiest in the world.

There were approximately 229 airports in 2017, with around 240 planned by 2020. More than two-thirds of the airports under construction worldwide in 2013 were in China, and Boeing expects that China's fleet of active commercial aircraft in China will grow from 1,910 in 2011 to 5,980 in 2031. In just five years—from 2013 to 2018—China bought 1000 planes from Boeing. With rapid expansion in civil aviation, the largest airports in China have also joined the ranks of the busiest in the world. In 2018, Beijing's Capital Airport ranked second in the world by passenger traffic (it was 26th in 2002). Since 2010, the Hong Kong International Airport and Shanghai Pudong International Airport have ranked first and third in air cargo tonnage.

Some 80% of China's airspace remains restricted for military use, and Chinese airlines made up eight of the 10 worst-performing Asian airlines in terms of delays.
China has over 2,000 river and seaports, about 130 of which are open to foreign shipping. In 2017, the Ports of Shanghai, Hong Kong, Shenzhen, Ningbo-Zhoushan, Guangzhou, Qingdao and Tianjin ranked in the Top 10 in the world in container traffic and cargo tonnage.
Water supply and sanitation infrastructure in China is facing challenges such as rapid urbanization, as well as water scarcity, contamination, and pollution. According to data presented by the Joint Monitoring Program for Water Supply and Sanitation of WHO and UNICEF in 2015, about 36% of the rural population in China still did not have access to improved sanitation. In June 2010, there were 1,519 sewage treatment plants in China and 18 plants were added each week. The ongoing South–North Water Transfer Project intends to abate water shortage in the north.

The national census of 2010 recorded the population of the People's Republic of China as approximately 1,370,536,875. About 16.60% of the population were 14 years old or younger, 70.14% were between 15 and 59 years old, and 13.26% were over 60 years old. The population growth rate for 2013 is estimated to be 0.46%. China used to make up much of the world's poor; now it makes up much of the world's middle class. Although a middle-income country by Western standards, China's rapid growth has pulled hundreds of millions—800 million, to be more precise—of its people out of poverty since 1978. By 2013, less than 2% of the Chinese population lived below the international poverty line of US$1.9 per day, down from 88% in 1981. China's own standards for poverty are higher and still the country is on its way to eradicate national poverty completely by 2019. From 2009–2018, the unemployment rate in China has averaged about 4%.

Given concerns about population growth, China implemented a two-child limit during the 1970s, and, in 1979, began to advocate for an even stricter limit of one child per family. Beginning in the mid 1980s, however, given the unpopularity of the strict limits, China began to allow some major exemptions, particularly in rural areas, resulting in what was actually a "1.5"-child policy from the mid-1980s to 2015 (ethnic minorities were also exempt from one child limits). The next major loosening of the policy was enacted in December 2013, allowing families to have two children if one parent is an only child. In 2016, the one-child policy was replaced in favor of a two-child policy. Data from the 2010 census implies that the total fertility rate may be around 1.4, although due to underreporting of births it may be closer to 1.5–1.6.

According to one group of scholars, one-child limits had little effect on population growth or the size of the total population. However, these scholars have been challenged. Their own counterfactual model of fertility decline without such restrictions implies that China averted more than 500 million births between 1970 and 2015, a number which may reach one billion by 2060 given all the lost descendants of births averted during the era of fertility restrictions, with one-child restrictions accounting for the great bulk of that reduction.

The policy, along with traditional preference for boys, may have contributed to an imbalance in the sex ratio at birth. According to the 2010 census, the sex ratio at birth was 118.06 boys for every 100 girls, which is beyond the normal range of around 105 boys for every 100 girls. The 2010 census found that males accounted for 51.27 percent of the total population. However, China's sex ratio is more balanced than it was in 1953, when males accounted for 51.82 percent of the total population.

China legally recognizes 56 distinct ethnic groups, who altogether comprise the "Zhonghua Minzu". The largest of these nationalities are the ethnic Chinese or "Han", who constitute more than 80% of the total
population. The Han Chinese – the world's largest single ethnic group – outnumber other ethnic groups in every provincial-level division except Tibet and Xinjiang. Ethnic minorities account for about less than 25% of the population of China, according to
the 2010 census. Compared with the 2000 population census, the Han population increased by 66,537,177 persons, or 5.74%, while the population of the 55 national minorities combined increased by 7,362,627 persons, or 6.92%. The 2010 census recorded a total of 593,832 foreign nationals living in China. The largest such groups were from South Korea (120,750), the
United States (71,493) and Japan (66,159).

There are as many as 292 living languages in China. The languages most commonly spoken belong to the Sinitic branch of the Sino-Tibetan language family, which contains Mandarin (spoken by 70% of the population), and other varieties of Chinese language: Yue (including Cantonese and Taishanese), Wu (including Shanghainese and Suzhounese), Min (including Fuzhounese, Hokkien and Teochew), Xiang, Gan and Hakka. Languages of the Tibeto-Burman branch, including Tibetan, Qiang, Naxi and Yi, are spoken across the Tibetan and Yunnan–Guizhou Plateau. Other ethnic minority languages in southwest China include Zhuang, Thai, Dong and Sui of the Tai-Kadai family, Miao and Yao of the Hmong–Mien family, and Wa of the Austroasiatic family. Across northeastern and northwestern China, local ethnic groups speak Altaic languages including Manchu, Mongolian and several Turkic languages: Uyghur, Kazakh, Kyrgyz, Salar and Western Yugur. Korean is spoken natively along the border with North Korea. Sarikoli, the language of Tajiks in western Xinjiang, is an Indo-European language. Taiwanese aborigines, including a small population on the mainland, speak Austronesian languages.

Standard Mandarin, a variety of Mandarin based on the Beijing dialect, is the official national language of China and is used as a lingua franca in the country between people of different linguistic backgrounds.

Chinese characters have been used as the written script for the Sinitic languages for thousands of years. They allow speakers of mutually unintelligible Chinese varieties to communicate with each other through writing. In 1956, the government introduced simplified characters, which have supplanted the older traditional characters in mainland China. Chinese characters are romanized using the Pinyin system. Tibetan uses an alphabet based on an Indic script. Uyghur is most commonly written in Persian alphabet based Uyghur Arabic alphabet. The Mongolian script used in China and the Manchu script are both derived from the Old Uyghur alphabet. Zhuang uses both an official Latin alphabet script and a traditional Chinese character script.

China has urbanized significantly in recent decades. The percent of the country's population living in urban areas increased from 20% in 1980 to over 55% in 2016. It is estimated that China's urban population will reach one billion by 2030, potentially equivalent to one-eighth of the world population. , there are more than 262 million migrant workers in China, mostly rural migrants seeking work in cities.

China has over 160 cities with a population of over one million, including the seven megacities (cities with a population of over 10 million) of Chongqing, Shanghai, Beijing, Guangzhou, Tianjin, Shenzhen, and Wuhan. By 2025, it is estimated that the country will be home to 221 cities with over a million inhabitants. The figures in the table below are from the 2010 census, and are only estimates of the urban populations within administrative city limits; a different ranking exists when considering the total municipal populations (which includes suburban and rural populations). The large "floating populations" of migrant workers make conducting censuses in urban areas difficult; the figures below include only long-term residents.
Since 1986, compulsory education in China comprises primary and junior secondary school, which together last for nine years. In 2010, about 82.5 percent of students continued their education at a three-year senior secondary school. The Gaokao, China's national university entrance exam, is a prerequisite for entrance into most higher education institutions. In 2010, 27 percent of secondary school graduates are enrolled in higher education. This number increased significantly over the last years, reaching a tertiary school enrollment of 50 percent in 2018. Vocational education is available to students at the secondary and tertiary level.

In February 2006, the government pledged to provide completely free nine-year education, including textbooks and fees. Annual education investment went from less than US$50 billion in 2003 to more than US$250 billion in 2011. However, there remains an inequality in education spending. In 2010, the annual education expenditure per secondary school student in Beijing totalled ¥20,023, while in Guizhou, one of the poorest provinces in China, only totalled ¥3,204. Free compulsory education in China consists of primary school and junior secondary school between the ages of 6 and 15. In 2011, around 81.4% of Chinese have received secondary education. By 2007, there were 396,567 primary schools, 94,116 secondary schools, and 2,236 higher education institutions in China.

, 96% of the population over age 15 are literate. In 1949, only 20% of the population could read, compared to 65.5% thirty years later. In 2009, Chinese students from Shanghai achieved the world's best results in mathematics, science and literacy, as tested by the Programme for International Student Assessment (PISA), a worldwide evaluation of 15-year-old school pupils' scholastic performance. Despite the high results, Chinese education has also faced both native and international criticism for its emphasis on rote memorization and its gap in quality from rural to urban areas.

The National Health and Family Planning Commission, together with its counterparts in the local commissions, oversees the health needs of the Chinese population. An emphasis on public health and preventive medicine has characterized Chinese health policy since the early 1950s. At that time, the Communist Party started the Patriotic Health Campaign, which was aimed at improving sanitation and hygiene, as well as treating and preventing several diseases. Diseases such as cholera, typhoid and scarlet fever, which were previously rife in China, were nearly eradicated by the campaign. After Deng Xiaoping began instituting economic reforms in 1978, the health of the Chinese public improved rapidly because of better nutrition, although many of the free public health services provided in the countryside disappeared along with the People's Communes. Healthcare in China became mostly privatized, and experienced a significant rise in quality. In 2009, the government began a 3-year large-scale healthcare provision initiative worth US$124 billion. By 2011, the campaign resulted in 95% of China's population having basic health insurance coverage. In 2011, China was estimated to be the world's third-largest supplier of pharmaceuticals, but its population has suffered from the development and distribution of counterfeit medications. 

, the average life expectancy at birth in China is 76 years, and the infant mortality rate is 7 per thousand. Both have improved significantly since the 1950s. Rates of stunting, a condition caused by malnutrition, have declined from 33.1% in 1990 to 9.9% in 2010. Despite significant improvements in health and the construction of advanced medical facilities, China has several emerging public health problems, such as respiratory illnesses caused by widespread air pollution, hundreds of millions of cigarette smokers, and an increase in obesity among urban youths. China's large population and densely populated cities have led to serious disease outbreaks in recent years, such as the 2003 outbreak of SARS, although this has since been largely contained. In 2010, air pollution caused 1.2 million premature deaths in China.

The government of the People's Republic of China officially espouses state atheism, and has conducted antireligious campaigns to this end. Religious affairs and issues in the country are overseen by the State Administration for Religious Affairs. Freedom of religion is guaranteed by China's constitution, although religious organizations that lack official approval can be subject to state persecution.

Over the millennia, Chinese civilization has been influenced by various religious movements. The "three teachings", including Confucianism, Taoism, and Buddhism (Chinese Buddhism), historically have a significant role in shaping Chinese culture, enriching a theological and spiritual framework which harkens back to the early Shang and Zhou dynasty. Chinese popular or folk religion, which is framed by the three teachings and other traditions, consists in allegiance to the "shen" (), a character that signifies the "energies of generation", who can be deities of the environment or ancestral principles of human groups, concepts of civility, culture heroes, many of whom feature in Chinese mythology and history. Among the most popular cults are those of Mazu (goddess of the seas), Huangdi (one of the two divine patriarchs of the Chinese race), Guandi (god of war and business), Caishen (god of prosperity and richness), Pangu and many others. China is home to many of the world's tallest religious statues, including the tallest of all, the Spring Temple Buddha in Henan.

Clear data on religious affiliation in China is difficult to gather due to varying definitions of "religion" and the unorganized, diffusive nature of Chinese religious traditions. Scholars note that in China there is no clear boundary between three teachings religions and local folk religious practice. A 2015 poll conducted by Gallup International found that 61% of Chinese people self-identified as "convinced atheist", though it is worthwhile to note that Chinese religions or some of their strands are definable as non-theistic and humanistic religions, since they do not believe that divine creativity is completely transcendent, but it is inherent in the world and in particular in the human being. According to a 2014 study, approximately 74% are either non-religious or practise Chinese folk belief, 16% are Buddhists, 2% are Christians, 1% are Muslims, and 8% adhere to other religions including Taoists and folk salvationism. In addition to Han people's local religious practices, there are also various ethnic minority groups in China who maintain their traditional autochthone religions. The various folk religions today comprise 2–3% of the population, while Confucianism as a religious self-identification is common within the intellectual class. Significant faiths specifically connected to certain ethnic groups include Tibetan Buddhism and the Islamic religion of the Hui, Uyghur, Kazakh, Kyrgyz and other peoples in Northwest China.

Since ancient times, Chinese culture has been heavily influenced by Confucianism. For much of the country's dynastic era, opportunities for social advancement could be provided by high performance in the prestigious imperial examinations, which have their origins in the Han dynasty. The literary emphasis of the exams affected the general perception of cultural refinement in China, such as the belief that calligraphy, poetry and painting were higher forms of art than dancing or drama. Chinese culture has long emphasized a sense of deep history and a largely inward-looking national perspective. Examinations and a culture of merit remain greatly valued in China today.

The first leaders of the People's Republic of China were born into the traditional imperial order, but were influenced by the May Fourth Movement and reformist ideals. They sought to change some traditional aspects of Chinese culture, such as rural land tenure, sexism, and the Confucian system of education, while preserving others, such as the family structure and culture of obedience to the state. Some observers see the period following the establishment of the PRC in 1949 as a continuation of traditional Chinese dynastic history, while others claim that the Communist Party's rule has damaged the foundations of Chinese culture, especially through political movements such as the Cultural Revolution of the 1960s, where many aspects of traditional culture were destroyed, having been denounced as "regressive and harmful" or "vestiges of feudalism". Many important aspects of traditional Chinese morals and culture, such as Confucianism, art, literature, and performing arts like Peking opera, were altered to conform to government policies and propaganda at the time. Access to foreign media remains heavily restricted.

Today, the Chinese government has accepted numerous elements of traditional Chinese culture as being integral to Chinese society. With the rise of Chinese nationalism and the end of the Cultural Revolution, various forms of traditional Chinese art, literature, music, film, fashion and architecture have seen a vigorous revival, and folk and variety art in particular have sparked interest nationally and even worldwide. China is now the third-most-visited country in the world, with 55.7 million inbound international visitors in 2010. It also experiences an enormous volume of domestic tourism; an estimated 740 million Chinese holidaymakers travelled within the country in October 2012.

Chinese literature is based on the literature of the Zhou dynasty. Concepts covered within the Chinese classic texts present a wide range of thoughts and subjects including calendar, military, astrology, herbology, geography and many others. Some of the most important early texts include the "I Ching" and the "Shujing" within the Four Books and Five Classics which served as the Confucian authoritative books for the state-sponsored curriculum in dynastic era. Inherited from the "Classic of Poetry", classical Chinese poetry developed to its floruit during the Tang dynasty. Li Bai and Du Fu opened the forking ways for the poetic circles through romanticism and realism respectively. Chinese historiography began with the "Shiji", the overall scope of the historiographical tradition in China is termed the Twenty-Four Histories, which set a vast stage for Chinese fictions along with Chinese mythology and folklore. Pushed by a burgeoning citizen class in the Ming dynasty, Chinese classical fiction rose to a boom of the historical, town and gods and demons fictions as represented by the Four Great Classical Novels which include "Water Margin", "Romance of the Three Kingdoms", "Journey to the West" and "Dream of the Red Chamber". Along with the wuxia fictions of Jin Yong and Liang Yusheng, it remains an enduring source of popular culture in the East Asian cultural sphere.

In the wake of the New Culture Movement after the end of the Qing dynasty, Chinese literature embarked on a new era with written vernacular Chinese for ordinary citizens. Hu Shih and Lu Xun were pioneers in modern literature. Various literary genres, such as misty poetry, scar literature, young adult fiction and the xungen literature, which is influenced by magic realism, emerged following the Cultural Revolution. Mo Yan, a xungen literature author, was awarded the Nobel Prize in Literature in 2012.

Chinese cuisine is highly diverse, drawing on several millennia of culinary history and geographical variety, in which the most influential are known as the "Eight Major Cuisines", including Sichuan, Cantonese, Jiangsu, Shandong, Fujian, Hunan, Anhui, and Zhejiang cuisines. All of them are featured by the precise skills of shaping, heating, colorway and flavoring. Chinese cuisine is also known for its width of cooking methods and ingredients, as well as food therapy that is emphasized by traditional Chinese medicine. Generally, China's staple food is rice in the south, wheat based breads and noodles in the north. The diet of the common people in pre-modern times was largely grain and simple vegetables, with meat reserved for special occasions. And the bean products, such as tofu and soy milk, remain as a popular source of protein. Pork is now the most popular meat in China, accounting for about three-fourths of the country's total meat consumption. While pork dominates the meat market, there is also the vegetarian Buddhist cuisine and the pork-free Chinese Islamic cuisine. Southern cuisine, due to the area's proximity to the ocean and milder climate, has a wide variety of seafood and vegetables; it differs in many respects from the wheat-based diets across dry northern China. Numerous offshoots of Chinese food, such as Hong Kong cuisine and American Chinese food, have emerged in the nations that play host to the Chinese diaspora.

China has become a prime sports destination worldwide. The country gained the hosting rights for several major global sports tournaments including the 2008 Summer Olympics, the 2015 World Championships in Athletics, the 2019 FIBA Basketball World Cup and the upcoming 2022 Winter Olympics.

China has one of the oldest sporting cultures in the world. There is evidence that archery ("shèjiàn") was practiced during the Western Zhou dynasty. Swordplay ("jiànshù") and cuju, a sport loosely related to association football date back to China's early dynasties as well.

Physical fitness is widely emphasized in Chinese culture, with morning exercises such as qigong and t'ai chi ch'uan widely practiced, and commercial gyms and private fitness clubs are gaining popularity across the country. Basketball is currently the most popular spectator sport in China. The Chinese Basketball Association and the American National Basketball Association have a huge following among the people, with native or ethnic Chinese players such as Yao Ming and Yi Jianlian held in high esteem. China's professional football league, now known as Chinese Super League, was established in 1994, it is the largest football market in Asia. Other popular sports in the country include martial arts, table tennis, badminton, swimming and snooker. Board games such as go (known as "wéiqí" in Chinese), xiangqi, mahjong, and more recently chess, are also played at a professional level. In addition, China is home to a huge number of cyclists, with an estimated 470 million bicycles . Many more traditional sports, such as dragon boat racing, Mongolian-style wrestling and horse racing are also popular.

China has participated in the Olympic Games since 1932, although it has only participated as the PRC since 1952. China hosted the 2008 Summer Olympics in Beijing, where its athletes received 51 gold medals – the highest number of gold medals of any participating nation that year. China also won the most medals of any nation at the 2012 Summer Paralympics, with 231 overall, including 95 gold medals. In 2011, Shenzhen in Guangdong, China hosted the 2011 Summer Universiade. China hosted the 2013 East Asian Games in Tianjin and the 2014 Summer Youth Olympics in Nanjing. Beijing and its nearby city Zhangjiakou of Hebei province will also collaboratively host the 2022 Olympic Winter Games, which will make Beijing the first city in the world to hold both the Summer Olympics and the Winter Olympics.









</doc>
<doc id="5407" url="https://en.wikipedia.org/wiki?curid=5407" title="California">
California

California is a state in the Pacific Region of the United States. With 39.5 million residents across a total area of about , California is the most populous U.S. state and the third-largest by area. The state capital is Sacramento. The Greater Los Angeles Area and the San Francisco Bay Area are the nation's second- and fifth-most populous urban regions, with 18.7 million and 9.7 million residents respectively. Los Angeles is California's most populous city, and the country's second-most populous, after New York City. California also has the nation's most populous county, Los Angeles County, and its largest county by area, San Bernardino County. The City and County of San Francisco is both the country's second most densely populated major city after New York City and the fifth most densely populated county, behind only four of the five New York City boroughs.

California's economy, with a gross state product of $3.0 trillion, is the largest sub-national economy in the world. If it were a country, California would be the fifth-largest economy in the world (larger than the United Kingdom, France, or India), and the 36th-most populous . The Greater Los Angeles Area and the San Francisco Bay Area are the nation's second- and third-largest urban economies ($1.3 trillion and $1.0 trillion respectively ), after the New York metropolitan area. The San Francisco Bay Area PSA had the nation's highest gross domestic product per capita in 2018 ($106,757) among large primary statistical areas, and is home to three of the world's ten largest companies by market capitalization and three of the world's ten richest people.

California culture is considered a global trendsetter in popular culture, communication, information, innovation, environmentalism, economics, politics, and entertainment. As a result of the state's diversity and migration, California integrates foods, languages, and traditions from other areas across the country and around the globe. It is considered the origin of the American film industry, the hippie counterculture, fast food, beach and car culture, the Internet, and the personal computer, among others. The San Francisco Bay Area and the Greater Los Angeles Area are widely seen as centers of the global technology and entertainment industries, respectively. California's economy is very diverse: 58% of it is based on finance, government, real estate services, technology, and professional, scientific, and technical business services. Although it accounts for only 1.5% of the state's economy, California's agriculture industry has the highest output of any U.S. state.

California shares a border with Oregon to the north, Nevada and Arizona to the east, and the Mexican state of Baja California to the south. The state's diverse geography ranges from the Pacific Coast in the west to the Sierra Nevada mountain range in the east, and from the redwood and Douglas fir forests in the northwest to the Mojave Desert in the southeast. The Central Valley, a major agricultural area, dominates the state's center. Although California is well-known for its warm Mediterranean climate, the large size of the state results in climates that vary from moist temperate rainforest in the north to arid desert in the interior, as well as snowy alpine in the mountains. Over time, drought and wildfires have become more frequent challenges.

What is now California was first settled by various Native Californian tribes before being explored by a number of European expeditions during the 16th and 17th centuries. The Spanish Empire then claimed and conquered it. In 1804 it was included in Alta California province, within Spanish New Spain Viceroyalty. The area became a part of Mexico in 1821 following its successful war for independence but was ceded to the United States in 1848 after the Mexican–American War. The western portion of Alta California was then organized and admitted as the 31st state on September 9, 1850. The California Gold Rush starting in 1848 led to dramatic social and demographic changes, with large-scale emigration from the east and abroad with an accompanying economic boom.

The Spaniards gave the name to the peninsula of Baja California and to Alta California, the region that became the present-day states of California, Nevada, and Utah, and parts of Arizona, New Mexico, Texas, and Wyoming.

The name likely derived from the mythical island of California in the fictional story of Queen Calafia, as recorded in a 1510 work "The Adventures of Esplandián" by Garci Rodríguez de Montalvo. This work was the fifth in a popular Spanish chivalric romance series that began with "Amadis de Gaula". Queen Calafia's kingdom was said to be a remote land rich in gold and pearls, inhabited by beautiful black women who wore gold armor and lived like Amazons, as well as griffins and other strange beasts. In the fictional paradise, the ruler Queen Calafia fought alongside Muslims and her name may have been chosen to echo the title of a Muslim leader, the Caliph. It is possible the name California was meant to imply the island was a Caliphate.
Shortened forms of the state's name include CA, Cal., Calif., and US-CA.

Settled by successive waves of arrivals during the last 10,000 years, California was one of the most culturally and linguistically diverse areas in pre-Columbian North America. Various estimates of the native population range from 100,000 to 300,000. The Indigenous peoples of California included more than 70 distinct ethnic groups of Native Americans, ranging from large, settled populations living on the coast to groups in the interior. California groups also were diverse in their political organization with bands, tribes, villages, and on the resource-rich coasts, large chiefdoms, such as the Chumash, Pomo and Salinan. Trade, intermarriage and military alliances fostered many social and economic relationships among the diverse groups.

The first European to explore the coast as far north as the Russian River was a Spanish sailing expedition, led by Spanish captain Juan Rodríguez Cabrillo, in 1542. Some 37 years later English explorer Francis Drake also explored and claimed an undefined portion of the California coast in 1579. Spanish traders made unintended visits with the Manila galleons on their return trips from the Philippines beginning in 1565. The first Asians to set foot on what would be the United States occurred in 1587, when Filipino sailors arrived in Spanish ships at Morro Bay. Sebastián Vizcaíno explored and mapped the coast of California in 1602 for New Spain.

Despite the on-the-ground explorations of California in the 16th century, Rodríguez's idea of California as an island persisted. That depiction appeared on many European maps well into the 18th century.

After the Portolà expedition of 1769–70, Spanish missionaries began setting up 21 California Missions on or near the coast of Alta (Upper) California, beginning in San Diego. During the same period, Spanish military forces built several forts ("presidios") and three small towns ("pueblos"). The San Francisco Mission grew into the city of San Francisco, and two of the pueblos grew into the cities of Los Angeles and San Jose. Several other smaller cities and towns also sprang up surrounding the various Spanish missions and pueblos, which remain to this day.

The Spanish colonization began decimating the natives through epidemics of various diseases for which the indigenous peoples had no natural immunity, such as measles and diphtheria. The establishment of the Spanish systems of government and social structure, which the Spanish settlers had brought with them, also technologically and culturally overwhelmed the societies of the earlier indigenous peoples.

During this same period, Russian ships also explored along the California coast and in 1812 established a trading post at Fort Ross. Russia's early 19th-century coastal settlements in California were positioned just north of the northernmost edge of the area of Spanish settlement in San Francisco Bay, and were the southernmost Russian settlements in North America. The Russian settlements associated with Fort Ross were spread from Point Arena to Tomales Bay.

In 1821, the Mexican War of Independence gave Mexico (including California) independence from Spain. For the next 25 years, Alta California remained as a remote, sparsely populated, northwestern administrative district of the newly independent country of Mexico.

After Mexican independence from Spain, the missions, which controlled most of the best land in the state, were secularized by 1834 and became the property of the Mexican government. The governor granted many square leagues of land to others with political influence. These huge "ranchos" or cattle ranches emerged as the dominant institutions of Mexican California. The ranchos developed under ownership by Californios (Hispanics native of California) who traded cowhides and tallow with Boston merchants. Beef did not become a commodity until the 1849 gold Rush.

From the 1820s, trappers and settlers from the United States and the future Canada arrived in Northern California. These new arrivals used the Siskiyou Trail, California Trail, Oregon Trail and Old Spanish Trail to cross the rugged mountains and harsh deserts in and surrounding California.

The early government of the newly independent Mexico was highly unstable, and in a reflection of this, from 1831 onwards, California also experienced a series of armed disputes, both internal and with the central Mexican government. During this tumultuous political period Juan Bautista Alvarado was able to secure the governorship during 1836–1842. The military action which first brought Alvarado to power had momentarily declared California to be an independent state, and had been aided by American and British residents of California, including Isaac Graham. In 1840, one hundred of those residents who did not have passports were arrested, leading to the Graham affair.

One of the largest ranchers in California was John Marsh. After failing to obtain justice against squatters on his land from the Mexican courts, he determined that California should become part of the United States. Marsh conducted a letter-writing campaign espousing the California climate, the soil, and other reasons to settle there, as well as the best route to follow, which became known as "Marsh's route". His letters were read, reread, passed around, and printed in newspapers throughout the country, and started the first wagon trains rolling to California. He invited immigrants to stay on his ranch until they could get settled, and assisted in their obtaining passports.

After ushering in the period of organized emigration to California, Marsh helped end the rule of the last Mexican governor of California, thereby paving the way to California's ultimate acquisition by the United States.

In 1846, a group of American settlers in and around Sonoma rebelled against Mexican rule during the Bear Flag Revolt. Afterwards, rebels raised the Bear Flag (featuring a bear, a star, a red stripe and the words "California Republic") at Sonoma. The Republic's only president was William B. Ide, who played a pivotal role during the Bear Flag Revolt. This revolt by American settlers served as a prelude to the later American military invasion of California, and was closely coordinated with nearby American military commanders.

The California Republic was short lived; the same year marked the outbreak of the Mexican–American War (1846–48). When Commodore John D. Sloat of the United States Navy sailed into Monterey Bay and began the military occupation of California by the United States, Northern California capitulated in less than a month to the United States forces. After a series of defensive battles in Southern California, the Treaty of Cahuenga was signed by the Californios on January 13, 1847, securing American control in California.

Following the Treaty of Guadalupe Hidalgo (February 2, 1848) that ended the war, the westernmost portion of the annexed Mexican territory of Alta California soon became the American state of California, and the remainder of the old territory was then subdivided into the new American Territories of Arizona, Nevada, Colorado and Utah. The lightly populated and arid lower region of old Baja California remained as a part of Mexico. In 1846, the total settler population of the western part of the old Alta California had been estimated to be no more than 8,000, plus about 100,000 Native Americans, down from about 300,000 before Hispanic settlement in 1769.

In 1848, only one week before the official American annexation of the area, gold was discovered in California, this being an event which was to forever alter both the state's demographics and its finances. Soon afterward, a massive influx of immigration into the area resulted, as prospectors and miners arrived by the thousands. The population burgeoned with United States citizens, Europeans, Chinese and other immigrants during the great California Gold Rush. By the time of California's application for statehood in 1850, the settler population of California had multiplied to 100,000. By 1854, more than 300,000 settlers had come. Between 1847 and 1870, the population of San Francisco increased from 500 to 150,000. California was suddenly no longer a sparsely populated backwater, but seemingly overnight it had grown into a major population center.

The seat of government for California under Spanish and later Mexican rule had been located in Monterey from 1777 until 1845. Pio Pico, last Mexican governor of Alta California, had briefly moved the capital to Los Angeles in 1845. The United States consulate had also been located in Monterey, under consul Thomas O. Larkin.

In 1849, a state Constitutional Convention was first held in Monterey. Among the first tasks of the Convention was a decision on a location for the new state capital. The first full legislative sessions were held in San Jose (1850–1851). Subsequent locations included Vallejo (1852–1853), and nearby Benicia (1853–1854); these locations eventually proved to be inadequate as well. The capital has been located in Sacramento since 1854 with only a short break in 1862 when legislative sessions were held in San Francisco due to flooding in Sacramento.

Once the state's Constitutional Convention had finalized its state constitution, it applied to the U.S. Congress for admission to statehood. On September 9, 1850, as part of the Compromise of 1850, California became a free state and September9 a state holiday.

During the American Civil War (1861–1865), California was able to send gold shipments eastwards to Washington in support of the Union cause; however, due to the existence of a large contingent of pro-South sympathizers within the state, the state was not able to muster any full military regiments to send eastwards to officially serve in the Union war effort. Still, several smaller military units within the Union army were unofficially associated with the state of California, such as the "California 100 Company", due to a majority of their members being from California.

At the time of California's admission into the Union, travel between California and the rest of the continental United States had been a time consuming and dangerous feat. Nineteen years afterwards, in 1869, shortly after the conclusion of the Civil War, a more direct connection was developed with the completion of the First Transcontinental Railroad in 1869. California was then easy to reach.

Much of the state was extremely well suited to fruit cultivation and agriculture in general. Vast expanses of wheat, other cereal crops, vegetable crops, cotton, and nut and fruit trees were grown (including oranges in Southern California), and the foundation was laid for the state's prodigious agricultural production in the Central Valley and elsewhere.

Under earlier Spanish and Mexican rule, California's original native population had precipitously declined, above all, from Eurasian diseases to which the indigenous people of California had not yet developed a natural immunity. Under its new American administration, California's harsh governmental policies towards its own indigenous people did not improve. As in other American states, many of the native inhabitants were soon forcibly removed from their lands by incoming American settlers such as miners, ranchers, and farmers. Although California had entered the American union as a free state, the "loitering or orphaned Indians" were de facto enslaved by their new Anglo-American masters under the 1853 "Act for the Government and Protection of Indians". There were also massacres in which hundreds of indigenous people were killed.

Between 1850 and 1860, the California state government paid around 1.5 million dollars (some 250,000 of which was reimbursed by the federal government) to hire militias whose purpose was to protect settlers from the indigenous populations. In later decades, the native population was placed in reservations and rancherias, which were often small and isolated and without enough natural resources or funding from the government to sustain the populations living on them. As a result, the rise of California was a calamity for the native inhabitants. Several scholars and Native American activists, including Benjamin Madley and Ed Castillo, have described the actions of the California government as a genocide.

Migration to California accelerated during the early 20th century with the completion of major transcontinental highways like the Lincoln Highway and Route 66. In the period from 1900 to 1965, the population grew from fewer than one million to the greatest in the Union. In 1940, the Census Bureau reported California's population as 6.0% Hispanic, 2.4% Asian, and 89.5% non-Hispanic white.

To meet the population's needs, major engineering feats like the California and Los Angeles Aqueducts; the Oroville and Shasta Dams; and the Bay and Golden Gate Bridges were built across the state. The state government also adopted the California Master Plan for Higher Education in 1960 to develop a highly efficient system of public education.

Meanwhile, attracted to the mild Mediterranean climate, cheap land, and the state's wide variety of geography, filmmakers established the studio system in Hollywood in the 1920s. California manufactured 8.7 percent of total United States military armaments produced during World War II, ranking third (behind New York and Michigan) among the 48 states. California however easily ranked first in production of military ships during the war (transport, cargo, [merchant ships] such as Liberty ships, Victory ships, and warships) at drydock facilities in San Diego, Los Angeles, and the San Francisco Bay Area. After World War II, California's economy greatly expanded due to strong aerospace and defense industries, whose size decreased following the end of the Cold War. Stanford University and its Dean of Engineering Frederick Terman began encouraging faculty and graduates to stay in California instead of leaving the state, and develop a high-tech region in the area now known as Silicon Valley. As a result of these efforts, California is regarded as a world center of the entertainment and music industries, of technology, engineering, and the aerospace industry, and as the United States center of agricultural production. Just before the Dot Com Bust, California had the fifth-largest economy in the world among nations. Yet since 1991, and starting in the late 1980s in Southern California, California has seen a net loss of domestic migrants in most years. This is often referred to by the media as the California exodus.

During the 20th century, two great disasters happened in California. The 1906 San Francisco earthquake and 1928 St. Francis Dam flood remain the deadliest in U.S history.

Although air pollution problems have been reduced, health problems associated with pollution have continued. The brown haze known as "smog" has been substantially abated after the passage of federal and state restrictions on automobile exhaust.

An energy crisis in 2001 led to rolling blackouts, soaring power rates, and the importation of electricity from neighboring states. Southern California Edison and Pacific Gas and Electric Company came under heavy criticism.

Housing prices in urban areas continued to increase; a modest home which in the 1960s cost $25,000 would cost half a million dollars or more in urban areas by 2005. More people commuted longer hours to afford a home in more rural areas while earning larger salaries in the urban areas. Speculators bought houses they never intended to live in, expecting to make a huge profit in a matter of months, then rolling it over by buying more properties. Mortgage companies were compliant, as everyone assumed the prices would keep rising. The bubble burst in 2007-8 as housing prices began to crash and the boom years ended. Hundreds of billions in property values vanished and foreclosures soared as many financial institutions and investors were badly hurt.

California is the 3rd largest state in the United States in area, after Alaska and Texas. California is often geographically bisected into two regions, Southern California, comprising the 10 southernmost counties, and Northern California, comprising the 48 northernmost counties. It is bordered by Oregon to the north, Nevada to the east and northeast, Arizona to the southeast, the Pacific Ocean to the west and it shares an international border with the Mexican state of Baja California to the south (with which it makes up part of The Californias region of North America, alongside Baja California Sur).

In the middle of the state lies the California Central Valley, bounded by the Sierra Nevada in the east, the coastal mountain ranges in the west, the Cascade Range to the north and by the Tehachapi Mountains in the south. The Central Valley is California's productive agricultural heartland.

Divided in two by the Sacramento-San Joaquin River Delta, the northern portion, the Sacramento Valley serves as the watershed of the Sacramento River, while the southern portion, the San Joaquin Valley is the watershed for the San Joaquin River. Both valleys derive their names from the rivers that flow through them. With dredging, the Sacramento and the San Joaquin Rivers have remained deep enough for several inland cities to be seaports.

The Sacramento-San Joaquin River Delta is a critical water supply hub for the state. Water is diverted from the delta and through an extensive network of pumps and canals that traverse nearly the length of the state, to the Central Valley and the State Water Projects and other needs. Water from the Delta provides drinking water for nearly 23 million people, almost two-thirds of the state's population as well as water for farmers on the west side of the San Joaquin Valley.

Suisun Bay lies at the confluence of the Sacramento and San Joaquin Rivers. The water is drained by the Carquinez Strait, which flows into San Pablo Bay, a northern extension of San Francisco Bay, which then connects to the Pacific Ocean via the Golden Gate strait.

The Channel Islands are located off the Southern coast, while the Farallon Islands lie west of San Francisco.

The Sierra Nevada (Spanish for "snowy range") includes the highest peak in the contiguous 48 states, Mount Whitney, at . The range embraces Yosemite Valley, famous for its glacially carved domes, and Sequoia National Park, home to the giant sequoia trees, the largest living organisms on Earth, and the deep freshwater lake, Lake Tahoe, the largest lake in the state by volume.

To the east of the Sierra Nevada are Owens Valley and Mono Lake, an essential migratory bird habitat. In the western part of the state is Clear Lake, the largest freshwater lake by area entirely in California. Although Lake Tahoe is larger, it is divided by the California/Nevada border. The Sierra Nevada falls to Arctic temperatures in winter and has several dozen small glaciers, including Palisade Glacier, the southernmost glacier in the United States.

About 45 percent of the state's total surface area is covered by forests, and California's diversity of pine species is unmatched by any other state. California contains more forestland than any other state except Alaska. Many of the trees in the California White Mountains are the oldest in the world; an individual bristlecone pine is over 5,000 years old.

In the south is a large inland salt lake, the Salton Sea. The south-central desert is called the Mojave; to the northeast of the Mojave lies Death Valley, which contains the lowest and hottest place in North America, the Badwater Basin at . The horizontal distance from the bottom of Death Valley to the top of Mount Whitney is less than . Indeed, almost all of southeastern California is arid, hot desert, with routine extreme high temperatures during the summer. The southeastern border of California with Arizona is entirely formed by the Colorado River, from which the southern part of the state gets about half of its water.

A majority of California's cities are located in either the San Francisco Bay Area or the Sacramento metropolitan area in Northern California; or the Los Angeles area, the Riverside-San Bernardino-Inland Empire, or the San Diego metropolitan area in Southern California. The Los Angeles Area, the Bay Area, and the San Diego metropolitan area are among several major metropolitan areas along the California coast.

As part of the Ring of Fire, California is subject to tsunamis, floods, droughts, Santa Ana winds, wildfires, landslides on steep terrain, and has several volcanoes. It has many earthquakes due to several faults running through the state, the largest being the San Andreas Fault. About 37,000 earthquakes are recorded each year, but most are too small to be felt.

Although most of the state has a Mediterranean climate, due to the state's large size the climate ranges from polar to subtropical. The cool California Current offshore often creates summer fog near the coast. Farther inland, there are colder winters and hotter summers. The maritime moderation results in the shoreline summertime temperatures of Los Angeles and San Francisco being the coolest of all major metropolitan areas of the United States and uniquely cool compared to areas on the same latitude in the interior and on the east coast of the North American continent. Even the San Diego shoreline bordering Mexico is cooler in summer than most areas in the contiguous United States. Just a few miles inland, summer temperature extremes are significantly higher, with downtown Los Angeles being several degrees warmer than at the coast. The same microclimate phenomenon is seen in the climate of the Bay Area, where areas sheltered from the sea experience significantly hotter summers than nearby areas closer to the ocean.

Northern parts of the state have more rain than the south. California's mountain ranges also influence the climate: some of the rainiest parts of the state are west-facing mountain slopes. Northwestern California has a temperate climate, and the Central Valley has a Mediterranean climate but with greater temperature extremes than the coast. The high mountains, including the Sierra Nevada, have an alpine climate with snow in winter and mild to moderate heat in summer.

California's mountains produce rain shadows on the eastern side, creating extensive deserts. The higher elevation deserts of eastern California have hot summers and cold winters, while the low deserts east of the Southern California mountains have hot summers and nearly frostless mild winters. Death Valley, a desert with large expanses below sea level, is considered the hottest location in the world; the highest temperature in the world, , was recorded there on July 10, 1913. The lowest temperature in California was in 1937 in Boca.

The table below lists average temperatures for January and August in a selection of places throughout the state; some highly populated and some not. This includes the relatively cool summers of the Humboldt Bay region around Eureka, the extreme heat of Death Valley, and the mountain climate of Mammoth in the Sierra Nevadas.

California is one of the richest and most diverse parts of the world, and includes some of the most endangered ecological communities. California is part of the Nearctic ecozone and spans a number of terrestrial ecoregions.

California's large number of endemic species includes relict species, which have died out elsewhere, such as the Catalina ironwood ("Lyonothamnus floribundus"). Many other endemics originated through differentiation or adaptive radiation, whereby multiple species develop from a common ancestor to take advantage of diverse ecological conditions such as the California lilac ("Ceanothus"). Many California endemics have become endangered, as urbanization, logging, overgrazing, and the introduction of exotic species have encroached on their habitat.

California boasts several superlatives in its collection of flora: the largest trees, the tallest trees, and the oldest trees. California's native grasses are perennial plants. After European contact, these were generally replaced by invasive species of European annual grasses; and, in modern times, California's hills turn a characteristic golden-brown in summer.

Because California has the greatest diversity of climate and terrain, the state has six life zones which are the lower Sonoran (desert); upper Sonoran (foothill regions and some coastal lands), transition (coastal areas and moist northeastern counties); and the Canadian, Hudsonian, and Arctic Zones, comprising the state's highest elevations.

Plant life in the dry climate of the lower Sonoran zone contains a diversity of native cactus, mesquite, and paloverde. The Joshua tree is found in the Mojave Desert. Flowering plants include the dwarf desert poppy and a variety of asters. Fremont cottonwood and valley oak thrive in the Central Valley. The upper Sonoran zone includes the chaparral belt, characterized by forests of small shrubs, stunted trees, and herbaceous plants. "Nemophila", mint, "Phacelia", "Viola", and the California poppy ("Eschscholzia californica", the state flower) also flourish in this zone, along with the lupine, more species of which occur here than anywhere else in the world.

The transition zone includes most of California's forests with the redwood ("Sequoia sempervirens") and the "big tree" or giant sequoia ("Sequoiadendron giganteum"), among the oldest living things on earth (some are said to have lived at least 4,000 years). Tanbark oak, California laurel, sugar pine, madrona, broad-leaved maple, and Douglas-fir also grow here. Forest floors are covered with swordfern, alumnroot, barrenwort, and trillium, and there are thickets of huckleberry, azalea, elder, and wild currant. Characteristic wild flowers include varieties of mariposa, tulip, and tiger and leopard lilies.

The high elevations of the Canadian zone allow the Jeffrey pine, red fir, and lodgepole pine to thrive. Brushy areas are abundant with dwarf manzanita and ceanothus; the unique Sierra puffball is also found here. Right below the timberline, in the Hudsonian zone, the whitebark, foxtail, and silver pines grow. At about , begins the Arctic zone, a treeless region whose flora include a number of wildflowers, including Sierra primrose, yellow columbine, alpine buttercup, and alpine shooting star.

Common plants that have been introduced to the state include the eucalyptus, acacia, pepper tree, geranium, and Scotch broom. The species that are federally classified as endangered are the Contra Costa wallflower, Antioch Dunes evening primrose, Solano grass, San Clemente Island larkspur, salt marsh bird's beak, McDonald's rock-cress, and Santa Barbara Island liveforever. , 85 plant species were listed as threatened or endangered.

In the deserts of the lower Sonoran zone, the mammals include the jackrabbit, kangaroo rat, squirrel, and opossum. Common birds include the owl, roadrunner, cactus wren, and various species of hawk. The area's reptilian life include the sidewinder viper, desert tortoise, and horned toad. The upper Sonoran zone boasts mammals such as the antelope, brown-footed woodrat, and ring-tailed cat. Birds unique to this zone are the California thrasher, bushtit, and California condor.

In the transition zone, there are Colombian black-tailed deer, black bears, gray foxes, cougars, bobcats, and Roosevelt elk. Reptiles such as the garter snakes and rattlesnakes inhabit the zone. In addition, amphibians such as the water puppy and redwood salamander are common too. Birds such as the kingfisher, chickadee, towhee, and hummingbird thrive here as well.

The Canadian zone mammals include the mountain weasel, snowshoe hare, and several species of chipmunks. Conspicuous birds include the blue-fronted jay, Sierra chickadee, Sierra hermit thrush, water ouzel, and Townsend's solitaire. As one ascends into the Hudsonian zone, birds become scarcer. While the Sierra rosy finch is the only bird native to the high Arctic region, other bird species such as the hummingbird and Clark's nutcracker. Principal mammals found in this region include the Sierra coney, white-tailed jackrabbit, and the bighorn sheep. , the bighorn sheep was listed as endangered by the U.S. Fish and Wildlife Service. The fauna found throughout several zones are the mule deer, coyote, mountain lion, northern flicker, and several species of hawk and sparrow.

Aquatic life in California thrives, from the state's mountain lakes and streams to the rocky Pacific coastline. Numerous trout species are found, among them rainbow, golden, and cutthroat. Migratory species of salmon are common as well. Deep-sea life forms include sea bass, yellowfin tuna, barracuda, and several types of whale. Native to the cliffs of northern California are seals, sea lions, and many types of shorebirds, including migratory species.

, 118 California animals were on the federal endangered list; 181 plants were listed as endangered or threatened. Endangered animals include the San Joaquin kitfox, Point Arena mountain beaver, Pacific pocket mouse, salt marsh harvest mouse, Morro Bay kangaroo rat (and five other species of kangaroo rat), Amargosa vole, California least tern, California condor, loggerhead shrike, San Clemente sage sparrow, San Francisco garter snake, five species of salamander, three species of chub, and two species of pupfish. Eleven butterflies are also endangered and two that are threatened are on the federal list. Among threatened animals are the coastal California gnatcatcher, Paiute cutthroat trout, southern sea otter, and northern spotted owl. California has a total of of National Wildlife Refuges. , 123 California animals were listed as either endangered or threatened on the federal list. Also, , 178 species of California plants were listed either as endangered or threatened on this federal list.

The most prominent river system within California is formed by the Sacramento River and San Joaquin River, which are fed mostly by snowmelt from the west slope of the Sierra Nevada, and respectively drain the north and south halves of the Central Valley. The two rivers join in the Sacramento–San Joaquin River Delta, flowing into the Pacific Ocean through San Francisco Bay. Many major tributaries feed into the Sacramento–San Joaquin system, including the Pit River, Feather River and Tuolumne River.

The Klamath and Trinity Rivers drain a large area in far northwestern California. The Eel River and Salinas River each drain portions of the California coast, north and south of San Francisco Bay, respectively. The Mojave River is the primary watercourse in the Mojave Desert, and the Santa Ana River drains much of the Transverse Ranges as it bisects Southern California. The Colorado River forms the state's southeast border with Arizona.

Most of California's major rivers are dammed as part of two massive water projects: the Central Valley Project, providing water for agriculture in the Central Valley, and the California State Water Project diverting water from northern to southern California. The state's coasts, rivers, and other bodies of water are regulated by the California Coastal Commission.

The United States Census Bureau estimates that the population of California was 39,512,223 on July 1, 2019, a 6.06% increase since the 2010 United States Census. The population is projected to reach 40 million by 2020 and 50 million by 2060.

Between 2000 and 2009, there was a natural increase of 3,090,016 (5,058,440 births minus 2,179,958 deaths). During this time period, international migration produced a net increase of 1,816,633 people while domestic migration produced a net decrease of 1,509,708, resulting in a net in-migration of 306,925 people. The state of California's own statistics show a population of 38,292,687 for January 1, 2009. However, according to the Manhattan Institute for Policy Research, since 1990 almost 3.4 million Californians have moved to other states, with most leaving to Texas, Nevada, and Arizona.

Within the Western hemisphere California is the second most populous sub-national administrative entity (behind the state of São Paulo in Brazil) and third most populous sub-national entity of any kind outside Asia (in which wider category it also ranks behind England in the United Kingdom, which has no administrative functions). California's population is greater than that of all but 34 countries of the world. The Greater Los Angeles Area is the 2nd-largest metropolitan area in the United States, after the New York metropolitan area, while Los Angeles, with nearly half the population of New York City, is the second-largest city in the United States. Conversely, San Francisco, with nearly one-quarter the population density of Manhattan, is the most densely populated city in California and one of the most densely populated cities in the United States. Also, Los Angeles County has held the title of most populous United States county for decades, and it alone is more populous than 42 United States states. Including Los Angeles, four of the top 15 most populous cities in the U.S. are in California: Los Angeles (2nd), San Diego (8th), San Jose (10th), and San Francisco (13th). The center of population of California is located in the town of Buttonwillow, Kern County.

The state has 482 incorporated cities and towns, of which 460 are cities and 22 are towns. Under California law, the terms "city" and "town" are explicitly interchangeable; the name of an incorporated municipality in the state can either be "City of (Name)" or "Town of (Name)".

Sacramento became California's first incorporated city on February 27, 1850. San Jose, San Diego, and Benicia tied for California's second incorporated city, each receiving incorporation on March 27, 1850. Jurupa Valley became the state's most recent and 482nd incorporated municipality on July 1, 2011.

The majority of these cities and towns are within one of five metropolitan areas: the Los Angeles Metropolitan Area, the San Francisco Bay Area, the Riverside-San Bernardino Area, the San Diego metropolitan area, or the Sacramento metropolitan area.

Starting in the year 2010, for the first time since the California Gold Rush, California-born residents make up the majority of the state's population. Along with the rest of the United States, California's immigration pattern has also shifted over the course of the late 2000s to early 2010s. Immigration from Latin American countries has dropped significantly with most immigrants now coming from Asia. In total for 2011, there were 277,304 immigrants. 57% came from Asian countries vs. 22% from Latin American countries. Net immigration from Mexico, previously the most common country of origin for new immigrants, has dropped to zero/less than zero since more Mexican nationals are departing for their home country than immigrating. As a result it is projected that Hispanic citizens will constitute 49% of the population by 2060, instead of the previously projected 2050, due primarily to domestic births.

The state's population of undocumented immigrants has been shrinking in recent years, due to increased enforcement and decreased job opportunities for lower-skilled workers. The number of migrants arrested attempting to cross the Mexican border in the Southwest decreased from a high of 1.1 million in 2005 to 367,000 in 2011. Despite these recent trends, illegal aliens constituted an estimated 7.3 percent of the state's population, the third highest percentage of any state in the country, totaling nearly 2.6 million. In particular, illegal immigrants tended to be concentrated in Los Angeles, Monterey, San Benito, Imperial, and Napa Counties—the latter four of which have significant agricultural industries that depend on manual labor. More than half of illegal immigrants originate from Mexico. The state of California and some California cities, including Los Angeles, Oakland and San Francisco, have adopted sanctuary policies.

According to the United States Census Bureau in 2016 the population self-identifies as (alone or in combination):

By ethnicity, in 2016 the population was 61.1% non-Hispanic (of any race) and 38.9% Hispanic or Latino (of any race). Hispanics are the largest single ethnic group in California. Non-Hispanic whites constituted 37.7% of the state's population. "Californios" are the Hispanic residents native to California, who are culturally or genetically descended from the Spanish-speaking community which has existed in California since 1542, of varying Mexican American/Chicano, Criollo Spaniard, and Mestizo origin.

, 75.1% of California's population younger than age 1 were minorities, meaning they had at least one parent who was not non-Hispanic white (white Hispanics are counted as minorities).

In terms of total numbers, California has the largest population of White Americans in the United States, an estimated 22,200,000 residents. The state has the 5th largest population of African Americans in the United States, an estimated 2,250,000 residents. California's Asian American population is estimated at 4.4 million, constituting a third of the nation's total. California's Native American population of 285,000 is the most of any state.

According to estimates from 2011, California has the largest minority population in the United States by numbers, making up 60% of the state population. Over the past 25 years, the population of non-Hispanic whites has declined, while Hispanic and Asian populations have grown. Between 1970 and 2011, non-Hispanic whites declined from 80% of the state's population to 40%, while Hispanics grew from 32% in 2000 to 38% in 2011. It is currently projected that Hispanics will rise to 49% of the population by 2060, primarily due to domestic births rather than immigration. With the decline of immigration from Latin America, Asian Americans now constitute the fastest growing racial/ethnic group in California; this growth is primarily driven by immigration from China, India and the Philippines, respectively.

English serves as California's de jure and de facto official language. In 2010, the Modern Language Association of America estimated that 57.02% (19,429,309) of California residents age5 and older spoke only English at home, while 42.98% spoke another primary language at home. According to the 2007 American Community Survey, 73% of people who speak a language other than English at home are able to speak English well or very well, with 9.8% not speaking English at all. Like most U.S. states (32 out of 50), California law enshrines English as its official language, and has done so since the passage of Proposition 63 by California voters. Various government agencies do, and are often required to, furnish documents in the various languages needed to reach their intended audiences.

In total, 16 languages other than English were spoken as primary languages at home by more than 100,000 persons, more than any other state in the nation. New York State, in second place, had nine languages other than English spoken by more than 100,000 persons. The most common language spoken besides English was Spanish, spoken by 28.46% (9,696,638) of the population. With Asia contributing most of California's new immigrants, California had the highest concentration nationwide of Vietnamese and Chinese speakers, the second highest concentration of Korean, and the third highest concentration of Tagalog speakers.

California has historically been one of the most linguistically diverse areas in the world, with more than 70 indigenous languages derived from 64 root languages in six language families. A survey conducted between 2007 and 2009 identified 23 different indigenous languages among California farmworkers. All of California's indigenous languages are endangered, although there are now efforts toward language revitalization.

As a result of the state's increasing diversity and migration from other areas across the country and around the globe, linguists began noticing a noteworthy set of emerging characteristics of spoken American English in California since the late 20th century. This variety, known as California English, has a vowel shift and several other phonological processes that are different from varieties of American
English used in other regions of the United States.

The culture of California is a Western culture and most clearly has its modern roots in the culture of the United States, but also, historically, many Hispanic Californio and Mexican influences. As a border and coastal state, Californian culture has been greatly influenced by several large immigrant populations, especially those from Latin America and Asia.

California has long been a subject of interest in the public mind and has often been promoted by its boosters as a kind of paradise. In the early 20th century, fueled by the efforts of state and local boosters, many Americans saw the Golden State as an ideal resort destination, sunny and dry all year round with easy access to the ocean and mountains. In the 1960s, popular music groups such as The Beach Boys promoted the image of Californians as laid-back, tanned beach-goers.

The California Gold Rush of the 1850s is still seen as a symbol of California's economic style, which tends to generate technology, social, entertainment, and economic fads and booms and related busts.

The largest religious denominations by number of adherents as a percentage of California's population in 2014 were the Catholic Church with 28 percent, Evangelical Protestants with 20 percent, and Mainline Protestants with 10 percent. Together, all kinds of Protestants accounted for 32 percent. Those unaffiliated with any religion represented 27 percent of the population. The breakdown of other religions is 1% Muslim, 2% Hindu and 2% Buddhist. This is a change from 2008, when the population identified their religion with the Catholic Church with 31 percent; Evangelical Protestants with 18 percent; and Mainline Protestants with 14 percent. In 2008, those unaffiliated with any religion represented 21 percent of the population. The breakdown of other religions in 2008 was 0.5% Muslim, 1% Hindu and 2% Buddhist. The "American Jewish Year Book" placed the total Jewish population of California at about 1,194,190 in 2006. According to the Association of Religion Data Archives (ARDA) the largest denominations by adherents in 2010 were the Roman Catholic Church with 10,233,334; The Church of Jesus Christ of Latter-day Saints with 763,818; and the Southern Baptist Convention with 489,953.

The first priests to come to California were Roman Catholic missionaries from Spain. Roman Catholics founded 21 missions along the California coast, as well as the cities of Los Angeles and San Francisco. California continues to have a large Roman Catholic population due to the large numbers of Mexicans and Central Americans living within its borders. California has twelve dioceses and two archdioceses, the Archdiocese of Los Angeles and the Archdiocese of San Francisco, the former being the largest archdiocese in the United States.

A Pew Research Center survey revealed that California is somewhat less religious than the rest of the states: 62 percent of Californians say they are "absolutely certain" of their belief in God, while in the nation 71 percent say so. The survey also revealed 48 percent of Californians say religion is "very important", compared to 56 percent nationally.

California has nineteen major professional sports league franchises, far more than any other state. The San Francisco Bay Area has six major league teams spread in its three major cities: San Francisco, San Jose, and Oakland, while the Greater Los Angeles Area is home to ten major league franchises. San Diego and Sacramento each have one major league team. The NFL Super Bowl has been hosted in California 11 times at four different stadiums: Los Angeles Memorial Coliseum, the Rose Bowl, Stanford Stadium, and San Diego's Qualcomm Stadium. A twelfth, Super Bowl 50, was held at Levi's Stadium in Santa Clara on February 7, 2016.

California has long had many respected collegiate sports programs. California is home to the oldest college bowl game, the annual Rose Bowl, among others.

California is the only U.S. state to have hosted both the Summer and Winter Olympics. The 1932 and 1984 summer games were held in Los Angeles. Squaw Valley Ski Resort in the Lake Tahoe region hosted the 1960 Winter Olympics. Los Angeles will host the 2028 Summer Olympics, marking the fourth time that California will have hosted the Olympic Games. Multiple games during the 1994 FIFA World Cup took place in California, with the Rose Bowl hosting eight matches (including the final), while Stanford Stadium hosted six matches.
Below is a list of major league sports teams in California:

Public secondary education consists of high schools that teach elective courses in trades, languages, and liberal arts with tracks for gifted, college-bound and industrial arts students. California's public educational system is supported by a unique constitutional amendment that requires a minimum annual funding level for grades K–12 and community colleges that grow with the economy and student enrollment figures.

In 2016, California's K–12 public school per-pupil spending was ranked 22nd in the nation ($11,500 per student vs. $11,800 for the U.S. average).

For 2012, California's K–12 public schools ranked 48th in the number of employees per student, at 0.102 (the U.S. average was 0.137), while paying the 7th most per employee, $49,000 (the U.S. average was $39,000).

A 2007 study concluded that California's public school system was "broken" in that it suffered from over-regulation.

California's public postsecondary education offers three separate systems:

California is also home to such notable private universities as Stanford University, the University of Southern California, the California Institute of Technology, and the Claremont Colleges. California has hundreds of other private colleges and universities, including many religious and special-purpose institutions.

California has twinning arrangements with the region of Catalonia in Spain
and with the Province of Alberta in Canada.

California's economy ranks among the largest in the world. , the gross state product (GSP) was $3.0 trillion ($76,000 per capita), the largest in the United States. California is responsible for 1/7 of the United States' approximate $21 trillion gross domestic product (GDP). , California's nominal GDP is larger than all but four countries (the United States, China, Japan and Germany). In terms of Purchasing Power Parity, it is larger than all but eight countries (the United States, China, India, Japan, Germany, Russia, Brazil and Indonesia). California's economy is larger than Africa and Australia and is almost as large as South America.

-Total Non farm Employment 2016
-Total employer establishments 2016

The five largest sectors of employment in California are trade, transportation, and utilities; government; professional and business services; education and health services; and leisure and hospitality. In output, the five largest sectors are financial services, followed by trade, transportation, and utilities; education and health services; government; and manufacturing. , California has an unemployment rate of 5.5%.

California's economy is dependent on trade and international related commerce accounts for about one-quarter of the state's economy. In 2008, California exported $144 billion worth of goods, up from $134 billion in 2007 and $127 billion in 2006.
Computers and electronic products are California's top export, accounting for 42 percent of all the state's exports in 2008.

Agriculture is an important sector in California's economy. Farming-related sales more than quadrupled over the past three decades, from $7.3 billion in 1974 to nearly $31 billion in 2004. This increase has occurred despite a 15 percent decline in acreage devoted to farming during the period, and water supply suffering from chronic instability. Factors contributing to the growth in sales-per-acre include more intensive use of active farmlands and technological improvements in crop production. In 2008, California's 81,500 farms and ranches generated $36.2 billion products revenue. In 2011, that number grew to $43.5 billion products revenue. The Agriculture sector accounts for two percent of the state's GDP and employs around three percent of its total workforce. According to the USDA in 2011, the three largest California agricultural products by value were milk and cream, shelled almonds, and grapes.

Per capita GDP in 2007 was $38,956, ranking eleventh in the nation. Per capita income varies widely by geographic region and profession. The Central Valley is the most impoverished, with migrant farm workers making less than minimum wage. According to a 2005 report by the Congressional Research Service, the San Joaquin Valley was characterized as one of the most economically depressed regions in the United States, on par with the region of Appalachia. California has a poverty rate of 23.5%, the highest of any state in the country. However, that's using the supplemental poverty measure (SPM); when the poverty rate using the official measure was 13.3% as of 2017. Many coastal cities include some of the wealthiest per-capita areas in the United States. The high-technology sectors in Northern California, specifically Silicon Valley, in Santa Clara and San Mateo counties, have emerged from the economic downturn caused by the dot-com bust.

In 2010, there were more than 663,000 millionaires in the state, more than any other state in the nation. In 2010, California residents were ranked first among the states with the best average credit score of 754.

State spending increased from $56 billion in 1998 to $127 billion in 2011. California, with 12% of the United States population, has one-third of the nation's welfare recipients. California has the third highest per capita spending on welfare among the states, as well as the highest spending on welfare at $6.67 billion. In January 2011 the California's total debt was at least $265 billion. On June 27, 2013, Governor Jerry Brown signed a balanced budget (no deficit) for the state, its first in decades; however the state's debt remains at $132 billion.

With the passage of Proposition 30 in 2012 and Proposition 55 in 2016, California now levies a 13.3% maximum marginal income tax rate with ten tax brackets, ranging from 1% at the bottom tax bracket of $0 annual individual income to 13.3% for annual individual income over $1,000,000. Proposition 30 enacted a minimum state sales tax of 7.5%. The sales tax increase was temporary for a four-year period and afterwards reverted to a previous minimum state sales tax rate of 7.25%. Local governments can and do levy additional sales taxes in addition to this minimum rate. Proposition 55 extended the higher income tax rates enacted by Proposition 30 until 2030.

All real property is taxable annually; the ad valorem tax is based on the property's fair market value at the time of purchase or the value of new construction. Property tax increases are capped at 2% annually or the rate of inflation (whichever is lower), per Proposition 13.

Because it is the most populous state in the United States, California is one of the country's largest users of energy. However because of its high energy rates, conservation mandates, mild weather in the largest population centers and strong environmental movement, its "per capita" energy use is one of the smallest of any state in the United States. Due to the high electricity demand, California imports more electricity than any other state, primarily hydroelectric power from states in the Pacific Northwest (via Path 15 and Path 66) and coal- and natural gas-fired production from the desert Southwest via Path 46.

As a result of the state's strong environmental movement, California has some of the most aggressive renewable energy goals in the United States, with a target for California to obtain a third of its electricity from renewables by 2020. Currently, several solar power plants such as the Solar Energy Generating Systems facility are located in the Mojave Desert. California's wind farms include Altamont Pass, San Gorgonio Pass, and Tehachapi Pass. Several dams across the state provide hydro-electric power. It would be possible to convert the total supply to 100% renewable energy, including heating, cooling and mobility, by 2050.

The state's crude oil and natural gas deposits are located in the Central Valley and along the coast, including the large Midway-Sunset Oil Field. Natural gas-fired power plants typically account for more than one-half of state electricity generation.

California is also home to two major nuclear power plants: Diablo Canyon and San Onofre, the latter having been shut down in 2013. Voters banned the approval of new nuclear power plants since the late 1970s because of concerns over radioactive waste disposal. In addition, several cities such as Oakland, Berkeley and Davis have declared themselves as nuclear-free zones.

California's vast terrain is connected by an extensive system of controlled-access highways ('freeways'), limited-access roads ('expressways'), and highways. California is known for its car culture, giving California's cities a reputation for severe traffic congestion. Construction and maintenance of state roads and statewide transportation planning are primarily the responsibility of the California Department of Transportation, nicknamed "Caltrans". The rapidly growing population of the state is straining all of its transportation networks, and California has some of the worst roads in the United States. The Reason Foundation's 19th Annual Report on the Performance of State Highway Systems ranked California's highways the third-worst of any state, with Alaska second, and Rhode Island first.

The state has been a pioneer in road construction. One of the state's more visible landmarks, the Golden Gate Bridge, was the longest suspension bridge main span in the world at between 1937 (when it opened) and 1964. With its orange paint and panoramic views of the bay, this highway bridge is a popular tourist attraction and also accommodates pedestrians and bicyclists. The San Francisco–Oakland Bay Bridge (often abbreviated the "Bay Bridge"), completed in 1936, transports about 280,000 vehicles per day on two-decks. Its two sections meet at Yerba Buena Island through the world's largest diameter transportation bore tunnel, at wide by high. The Arroyo Seco Parkway, connecting Los Angeles and Pasadena, opened in 1940 as the first freeway in the Western United States. It was later extended south to the Four Level Interchange in downtown Los Angeles, regarded as the first stack interchange ever built.

Los Angeles International Airport (LAX), the 6th busiest airport in the world, and San Francisco International Airport (SFO), the 23rd busiest airport in the world, are major hubs for trans-Pacific and transcontinental traffic. There are about a dozen important commercial airports and many more general aviation airports throughout the state.

California also has several important seaports. The giant seaport complex formed by the Port of Los Angeles and the Port of Long Beach in Southern California is the largest in the country and responsible for handling about a fourth of all container cargo traffic in the United States. The Port of Oakland, fourth largest in the nation, also handles trade entering from the Pacific Rim to the rest of the country. The Port of Stockton is the farthest inland port on the west coast of the United States.

The California Highway Patrol is the largest statewide police agency in the United States in employment with more than 10,000 employees. They are responsible for providing any police-sanctioned service to anyone on California's state-maintained highways and on state property.

The California Department of Motor Vehicles is by far the largest in North America. By the end of 2009, the California DMV had 26,555,006 driver's licenses and ID cards on file. In 2010, there were 1.17 million new vehicle registrations in force.

Inter-city rail travel is provided by Amtrak California; the three routes, the "Capitol Corridor", "Pacific Surfliner", and "San Joaquin", are funded by Caltrans. These services are the busiest intercity rail lines in the United States outside the Northeast Corridor and ridership is continuing to set records. The routes are becoming increasingly popular over flying, especially on the LAX-SFO route. Integrated subway and light rail networks are found in Los Angeles (Metro Rail) and San Francisco (MUNI Metro). Light rail systems are also found in San Jose (VTA), San Diego (San Diego Trolley), Sacramento (RT Light Rail), and Northern San Diego County (Sprinter). Furthermore, commuter rail networks serve the San Francisco Bay Area (ACE, BART, Caltrain, SMART), Greater Los Angeles (Metrolink), and San Diego County (Coaster).

The California High-Speed Rail Authority was created in 1996 by the state to implement an extensive rail system. Construction was approved by the voters during the November 2008 general election, with the first phase of construction estimated to cost $64.2 billion.

Nearly all counties operate bus lines, and many cities operate their own city bus lines as well. Intercity bus travel is provided by Greyhound, Megabus, and Amtrak Thruway Motorcoach.

California's interconnected water system is the world's largest, managing over of water per year, centered on six main systems of aqueducts and infrastructure projects. Water use and conservation in California is a politically divisive issue, as the state experiences periodic droughts and has to balance the demands of its large agricultural and urban sectors, especially in the arid southern portion of the state. The state's widespread redistribution of water also invites the frequent scorn of environmentalists.

The California Water Wars, a conflict between Los Angeles and the Owens Valley over water rights, is one of the most well-known examples of the struggle to secure adequate water supplies. Former California Governor Arnold Schwarzenegger said: "We've been in crisis for quite some time because we're now 38 million people and not anymore 18 million people like we were in the late 60s. So it developed into a battle between environmentalists and farmers and between the south and the north and between rural and urban. And everyone has been fighting for the last four decades about water."

The capital of California is located within Sacramento.
The state is organized into three branches of government—the executive branch consisting of the Governor and the other independently elected constitutional officers; the legislative branch consisting of the Assembly and Senate; and the judicial branch consisting of the Supreme Court of California and lower courts. The state also allows ballot propositions: direct participation of the electorate by initiative, referendum, recall, and ratification. Before the passage of California Proposition 14 (2010), California allowed each political party to choose whether to have a closed primary or a primary where only party members and independents vote. After June 8, 2010, when Proposition 14 was approved, excepting only the United States President and county central committee offices, all candidates in the primary elections are listed on the ballot with their preferred party affiliation, but they are not the official nominee of that party. At the primary election, the two candidates with the top votes will advance to the general election regardless of party affiliation. If at a special primary election, one candidate receives more than 50% of all the votes cast, they are elected to fill the vacancy and no special general election will be held.

The California executive branch consists of the Governor of California and seven other elected constitutional officers: Lieutenant Governor, Attorney General, Secretary of State, State Controller, State Treasurer, Insurance Commissioner, and State Superintendent of Public Instruction. They serve four-year terms and may be re-elected only once.

The California State Legislature consists of a 40-member Senate and 80-member Assembly. Senators serve four-year terms and Assembly members two. Members of the Assembly are subject to term limits of three terms, and members of the Senate are subject to term limits of two terms.

California's legal system is explicitly based upon English common law (as is the case with all other states except Louisiana) but carries a few features from Spanish civil law, such as community property. California's prison population grew from 25,000 in 1980 to over 170,000 in 2007. Capital punishment is a legal form of punishment and the state has the largest "Death Row" population in the country (though Oklahoma and Texas are far more active in carrying out executions).

California's judiciary system is the largest in the United States with a total of 1,600 judges (the federal system has only about 840). At the apex is the seven-member Supreme Court of California, while the California Courts of Appeal serve as the primary appellate courts and the California Superior Courts serve as the primary trial courts. Justices of the Supreme Court and Courts of Appeal are appointed by the Governor, but are subject to retention by the electorate every 12 years. The administration of the state's court system is controlled by the Judicial Council, composed of the Chief Justice of the California Supreme Court, 14 judicial officers, four representatives from the State Bar of California, and one member from each house of the state legislature.

California is divided into 58 counties. Per Article 11, Section 1, of the Constitution of California, they are the legal subdivisions of the state. The county government provides countywide services such as law enforcement, jails, elections and voter registration, vital records, property assessment and records, tax collection, public health, health care, social services, libraries, flood control, fire protection, animal control, agricultural regulations, building inspections, ambulance services, and education departments in charge of maintaining statewide standards. In addition, the county serves as the local government for all unincorporated areas. Each county is governed by an elected board of supervisors.

Incorporated cities and towns in California are either charter or general-law municipalities. General-law municipalities owe their existence to state law and are consequently governed by it; charter municipalities are governed by their own city or town charters. Municipalities incorporated in the 19th century tend to be charter municipalities. All ten of the state's most populous cities are charter cities. Most small cities have a council–manager form of government, where the elected city council appoints a city manager to supervise the operations of the city. Some larger cities have a directly-elected mayor who oversees the city government. In many council-manager cities, the city council selects one of its members as a mayor, sometimes rotating through the council membership—but this type of mayoral position is primarily ceremonial.

The Government of San Francisco is the only consolidated city-county in California, where both the city and county governments have been merged into one unified jurisdiction. The San Francisco Board of Supervisors also acts as the city council and the Mayor of San Francisco also serves as the county administrative officer.

About 1,102 school districts, independent of cities and counties, handle California's public education. California school districts may be organized as elementary districts, high school districts, unified school districts combining elementary and high school grades, or community college districts.

There are about 3,400 special districts in California. A special district, defined by California Government Code § 16271(d) as "any agency of the state for the local performance of governmental or proprietary functions within limited boundaries", provides a limited range of services within a defined geographic area. The geographic area of a special district can spread across multiple cities or counties, or could consist of only a portion of one. Most of California's special districts are "single-purpose districts", and provide one service.

The state of California sends 53 members to the House of Representatives, the nation's largest congressional state delegation. Consequently California also has the largest number of electoral votes in national presidential elections, with 55. The current Speaker of the House of Representatives is the representative of California's 12th district, Nancy Pelosi; Kevin McCarthy, representing the state's 23rd district, is the House Minority Leader.

California's U.S. Senators are Dianne Feinstein, a native and former mayor of San Francisco, and Kamala Harris, a native, former District Attorney from San Francisco and former Attorney General of California. In the 1992 U.S. Senate election, California became the first state to elect a Senate delegation entirely composed of women, due to the victories of Feinstein and Barbara Boxer.

In California, , the U.S. Department of Defense had a total of 117,806 active duty servicemembers of which 88,370 were Sailors or Marines, 18,339 were Airmen, and 11,097 were Soldiers, with 61,365 Department of Defense civilian employees. Additionally, there were a total of 57,792 Reservists and Guardsman in California.

In 2010, Los Angeles County was the largest origin of military recruits in the United States by county, with 1,437 individuals enlisting in the military. However, , Californians were relatively under-represented in the military as a proportion to its population.

In 2000, California, had 2,569,340 veterans of United States military service: 504,010 served in World War II, 301,034 in the Korean War, 754,682 during the Vietnam War, and 278,003 during 1990–2000 (including the Persian Gulf War). , there were 1,942,775 veterans living in California, of which 1,457,875 served during a period of armed conflict, and just over four thousand served before World WarII (the largest population of this group of any state).

California's military forces consist of the Army and Air National Guard, the naval and state military reserve (militia), and the California Cadet Corps.

On August 5, 1950, a nuclear-capable United States Air Force Boeing B-29 Superfortress bomber carrying a nuclear bomb crashed shortly after takeoff from Fairfield-Suisun Air Force Base. Brigadier General Robert F. Travis, command pilot of the bomber, was among the dead.

California has an idiosyncratic political culture compared to the rest of the country, and is sometimes regarded as a trendsetter. In socio-cultural mores and national politics, Californians are perceived as more liberal than other Americans, especially those who live in the inland states. As of the 2016 presidential election, California was the second most Democratic state behind Hawaii. According to the Cook Political Report, California contains five of the 15 most Democratic congressional districts in the United States.

Among the political idiosyncrasies and trendsetting, California was the second state to recall their state governor, the second state to legalize abortion, and the only state to ban marriage for gay couples twice by vote (including Proposition8 in 2008). Voters also passed Proposition 71 in 2004 to fund stem cell research, and Proposition 14 in 2010 to completely change the state's primary election process. California has also experienced disputes over water rights; and a tax revolt, culminating with the passage of Proposition 13 in 1978, limiting state property taxes.

The state's trend towards the Democratic Party and away from the Republican Party can be seen in state elections. From 1899 to 1939, California had Republican governors. Since 1990, California has generally elected Democratic candidates to federal, state and local offices, including current Governor Gavin Newsom; however, the state has elected Republican Governors, though many of its Republican Governors, such as Arnold Schwarzenegger, tend to be considered moderate Republicans and more centrist than the national party.

The Democrats also now hold a supermajority in both houses of the state legislature. There are 60 Democrats and 20 Republicans in the Assembly; and 29 Democrats and 11 Republicans in the Senate.

The trend towards the Democratic Party is most obvious in presidential elections. From 1960 to 1988, California was a Republican leaning state, with the party carrying the state's electoral votes in every election except for 1964. California Republicans Richard Nixon and Ronald Reagan were elected as the 37th and the 40th U.S. Presidents, respectively. However, Democrats have won all of California's electoral votes since 1992.

In the United States House, the Democrats held a 34–19 edge in the CA delegation of the 110th United States Congress in 2007. As the result of gerrymandering, the districts in California were usually dominated by one or the other party, and few districts were considered competitive. In 2008, Californians passed Proposition 20 to empower a 14-member independent citizen commission to redraw districts for both local politicians and Congress. After the 2012 elections, when the new system took effect, Democrats gained four seats and held a 38–15 majority in the delegation. Following the 2018 midterm House elections, Democrats won 46 out of 53 congressional house seats in California, leaving Republicans with seven.

In general, Democratic strength is centered in the populous coastal regions of the Los Angeles metropolitan area and the San Francisco Bay Area. Republican strength is still greatest in eastern parts of the state. Orange County had remained largely Republican until the 2016 and 2018 elections, in which a majority of the county's votes were cast for Democratic candidates. One study ranked Berkeley, Oakland, Inglewood and San Francisco in the top 20 most liberal American cities; and Bakersfield, Orange, Escondido, Garden Grove, and Simi Valley in the top 20 most conservative cities.

In October 2012, out of the 23,802,577 people eligible to vote, 18,245,970 people were registered to vote. Of the people registered, the three largest registered groups were Democrats (7,966,422), Republicans (5,356,608), and Decline to State (3,820,545). Los Angeles County had the largest number of registered Democrats (2,430,612) and Republicans (1,037,031) of any county in the state.






</doc>
<doc id="5408" url="https://en.wikipedia.org/wiki?curid=5408" title="Columbia River">
Columbia River

The Columbia River is the largest river in the Pacific Northwest region of North America. The river rises in the Rocky Mountains of British Columbia, Canada. It flows northwest and then south into the US state of Washington, then turns west to form most of the border between Washington and the state of Oregon before emptying into the Pacific Ocean. The river is long, and its largest tributary is the Snake River. Its drainage basin is roughly the size of France and extends into seven US states and a Canadian province. The fourth-largest river in the United States by volume, the Columbia has the greatest flow of any North American river entering the Pacific.

The Columbia and its tributaries have been central to the region's culture and economy for thousands of years. They have been used for transportation since ancient times, linking the region's many cultural groups. The river system hosts many species of anadromous fish, which migrate between freshwater habitats and the saline waters of the Pacific Ocean. These fish—especially the salmon species—provided the core subsistence for native peoples.

In the late 18th century, a private American ship became the first non-indigenous vessel to enter the river; it was followed by a British explorer, who navigated past the Oregon Coast Range into the Willamette Valley. In the following decades, fur trading companies used the Columbia as a key transportation route. Overland explorers entered the Willamette Valley through the scenic but treacherous Columbia River Gorge, and pioneers began to settle the valley in increasing numbers. Steamships along the river linked communities and facilitated trade; the arrival of railroads in the late 19th century, many running along the river, supplemented these links.

Since the late 19th century, public and private sectors have heavily developed the river. To aid ship and barge navigation, locks have been built along the lower Columbia and its tributaries, and dredging has opened, maintained, and enlarged shipping channels. Since the early 20th century, dams have been built across the river for power generation, navigation, irrigation, and flood control. The 14 hydroelectric dams on the Columbia's main stem and many more on its tributaries produce more than 44 percent of total US hydroelectric generation. Production of nuclear power has taken place at two sites along the river. Plutonium for nuclear weapons was produced for decades at the Hanford Site, which is now the most contaminated nuclear site in the US. These developments have greatly altered river environments in the watershed, mainly through industrial pollution and barriers to fish migration.

The Columbia begins its journey in the southern Rocky Mountain Trench in British Columbia (BC). Columbia Lake – above sea level – and the adjoining Columbia Wetlands form the river's headwaters. The trench is a broad, deep, and long glacial valley between the Canadian Rockies and the Columbia Mountains in BC. For its first , the Columbia flows northwest along the trench through Windermere Lake and the town of Invermere, a region known in British Columbia as the Columbia Valley, then northwest to Golden and into Kinbasket Lake. Rounding the northern end of the Selkirk Mountains, the river turns sharply south through a region known as the Big Bend Country, passing through Revelstoke Lake and the Arrow Lakes. Revelstoke, the Big Bend, and the Columbia Valley combined are referred to in BC parlance as the Columbia Country. Below the Arrow Lakes, the Columbia passes the cities of Castlegar, located at the Columbia's confluence with the Kootenay River, and Trail, two major population centers of the West Kootenay region. The Pend Oreille River joins the Columbia about north of the US–Canada border.

The Columbia enters eastern Washington flowing south and turning to the west at the Spokane River confluence. It marks the southern and eastern borders of the Colville Indian Reservation and the western border of the Spokane Indian Reservation. The river turns south after the Okanogan River confluence, then southeasterly near the confluence with the Wenatchee River in central Washington. This C‑shaped segment of the river is also known as the "Big Bend". During the Missoula Floods 10,000 to 15,000 years ago, much of the floodwater took a more direct route south, forming the ancient river bed known as the Grand Coulee. After the floods, the river found its present course, and the Grand Coulee was left dry. The construction of the Grand Coulee Dam in the mid-20th century impounded the river, forming Lake Roosevelt, from which water was pumped into the dry coulee, forming the reservoir of Banks Lake.

The river flows past The Gorge Amphitheatre, a prominent concert venue in the Northwest, then through Priest Rapids Dam, and then through the Hanford Nuclear Reservation. Entirely within the reservation is Hanford Reach, the only US stretch of the river that is completely free-flowing, unimpeded by dams and not a tidal estuary. The Snake River and Yakima River join the Columbia in the Tri‑Cities population center. The Columbia makes a sharp bend to the west at the Washington–Oregon border. The river defines that border for the final of its journey.

The Deschutes River joins the Columbia near The Dalles. Between The Dalles and Portland, the river cuts through the Cascade Range, forming the dramatic Columbia River Gorge. No other rivers except for the Klamath and Pit River completely breach the Cascades—the other rivers that flow through the range also originate in or very near the mountains. The headwaters and upper course of the Pit River are on the Modoc Plateau; downstream the Pit cuts a canyon through the southern reaches of the Cascades. In contrast, the Columbia cuts through the range nearly a thousand miles from its source in the Rocky Mountains. The gorge is known for its strong and steady winds, scenic beauty, and its role as an important transportation link. The river continues west, bending sharply to the north-northwest near Portland and Vancouver, Washington, at the Willamette River confluence. Here the river slows considerably, dropping sediment that might otherwise form a river delta. Near Longview, Washington and the Cowlitz River confluence, the river turns west again. The Columbia empties into the Pacific Ocean just west of Astoria, Oregon, over the Columbia Bar, a shifting sandbar that makes the river's mouth one of the most hazardous stretches of water to navigate in the world. Because of the danger and the many shipwrecks near the mouth, it acquired a reputation as the "Graveyard of Ships".

The Columbia drains an area of about . Its drainage basin covers nearly all of Idaho, large portions of British Columbia, Oregon, and Washington, ultimately all of Montana west of the Continental Divide, and small portions of Wyoming, Utah, and Nevada; the total area is similar to the size of France. Roughly of the river's length and 85 percent of its drainage basin are in the US. The Columbia is the twelfth-longest river and has the sixth-largest drainage basin in the United States. In Canada, where the Columbia flows for and drains , the river ranks 23rd in length, and the Canadian part of its basin ranks 13th in size among Canadian basins. 
The Columbia shares its name with nearby places, such as British Columbia, as well as with landforms and bodies of water.

With an average flow at the mouth of about , the Columbia is the largest river by discharge flowing into the Pacific from North America and is the fourth-largest by volume in the US. The average flow where the river crosses the international border between Canada and the United States is from a drainage basin of . This amounts to about 15 percent of the entire Columbia watershed. The Columbia's highest recorded flow, measured at The Dalles, was in June 1894, before the river was dammed. The lowest flow recorded at The Dalles was on April 16, 1968, and was caused by the initial closure of the John Day Dam, upstream. The Dalles is about from the mouth; the river at this point drains about or about 91 percent of the total watershed. Flow rates on the Columbia are affected by many large upstream reservoirs, many diversions for irrigation, and, on the lower stretches, reverse flow from the tides of the Pacific Ocean. The National Ocean Service observes water levels at six tide gauges and issues tide forecasts for twenty-two additional locations along the river between the entrance at the North Jetty and the base of Bonneville Dam, the head of tide.

When the rifting of Pangaea, due to the process of plate tectonics, pushed North America away from Europe and Africa and into the Panthalassic Ocean (ancestor to the modern Pacific Ocean), the Pacific Northwest was not part of the continent. As the North American continent moved westward, the Farallon Plate subducted under its western margin. As the plate subducted, it carried along island arcs which were accreted to the North American continent, resulting in the creation of the Pacific Northwest between 150 and 90 million years ago. The general outline of the Columbia Basin was not complete until between 60 and 40 million years ago, but it lay under a large inland sea later subject to uplift. Between 50 and 20 million years ago, from the Eocene through the Miocene eras, tremendous volcanic eruptions frequently modified much of the landscape traversed by the Columbia. The lower reaches of the ancestral river passed through a valley near where Mount Hood later arose. Carrying sediments from erosion and erupting volcanoes, it built a thick delta that underlies the foothills on the east side of the Coast Range near Vernonia in northwestern Oregon. Between 17 million and 6 million years ago, huge outpourings of flood basalt lava covered the Columbia River Plateau and forced the lower Columbia into its present course. The modern Cascade Range began to uplift 5 to 4 million years ago. Cutting through the uplifting mountains, the Columbia River significantly deepened the Columbia River Gorge.

The river and its drainage basin experienced some of the world's greatest known catastrophic floods toward the end of the last ice age. The periodic rupturing of ice dams at Glacial Lake Missoula resulted in the Missoula Floods, with discharges exceeding the combined flow of all the other rivers in the world, dozens of times over thousands of years. The exact number of floods is unknown, but geologists have documented at least 40; evidence suggests that they occurred between about 19,000 and 13,000 years ago.

The floodwaters rushed across eastern Washington, creating the channeled scablands, which are a complex network of dry canyon-like channels, or coulees that are often braided and sharply gouged into the basalt rock underlying the region's deep topsoil. Numerous flat-topped buttes with rich soil stand high above the chaotic scablands. Constrictions at several places caused the floodwaters to pool into large temporary lakes, such as Lake Lewis, in which sediments were deposited. Water depths have been estimated at at Wallula Gap and over modern Portland, Oregon. Sediments were also deposited when the floodwaters slowed in the broad flats of the Quincy, Othello, and Pasco Basins. The floods' periodic inundation of the lower Columbia River Plateau deposited rich sediments; 21st-century farmers in the Willamette Valley "plow fields of fertile Montana soil and clays from Washington's Palouse".

Over the last several thousand years a series of large landslides have occurred on the north side of the Columbia River Gorge, sending massive amounts of debris south from Table Mountain and Greenleaf Peak into the gorge near the present site of Bonneville Dam. The most recent and significant is known as the Bonneville Slide, which formed a massive earthen dam, filling of the river's length. Various studies have placed the date of the Bonneville Slide anywhere between 1060 and 1760 AD; the idea that the landslide debris present today was formed by more than one slide is relatively recent and may explain the large range of estimates. It has been suggested that if the later dates are accurate there may be a link with the 1700 Cascadia earthquake. The pile of debris resulting from the Bonneville Slide blocked the river until rising water finally washed away the sediment. It is not known how long it took the river to break through the barrier; estimates range from several months to several years. Much of the landslide's debris remained, forcing the river about south of its previous channel and forming the Cascade Rapids. In 1938, the construction of Bonneville Dam inundated the rapids as well as the remaining trees that could be used to refine the estimated date of the landslide.

In 1980, the eruption of Mount St. Helens deposited large amounts of sediment in the lower Columbia, temporarily reducing the depth of the shipping channel by .

Humans have inhabited the Columbia's watershed for more than 15,000 years, with a transition to a sedentary lifestyle based mainly on salmon starting about 3,500 years ago. In 1962, archaeologists found evidence of human activity dating back 11,230 years at the Marmes Rockshelter, near the confluence of the Palouse and Snake rivers in eastern Washington. In 1996 the skeletal remains of a 9,000-year-old prehistoric man (dubbed Kennewick Man) were found near Kennewick, Washington. The discovery rekindled debate in the scientific community over the origins of human habitation in North America and sparked a protracted controversy over whether the scientific or Native American community was entitled to possess and/or study the remains.

Many different Native Americans and First Nations peoples have a historical and continuing presence on the Columbia. South of the Canada–US border, the Colville, Spokane, Coeur d'Alene, Yakama, Nez Perce, Cayuse, Palus, Umatilla, Cowlitz, and the Confederated Tribes of Warm Springs live along the US stretch. Along the upper Snake River and Salmon River, the Shoshone Bannock tribes are present. The Sinixt or Lakes people lived on the lower stretch of the Canadian portion, while above that the Shuswap people (Secwepemc in their own language) reckon the whole of the upper Columbia east to the Rockies as part of their territory. The Canadian portion of the Columbia Basin outlines the traditional homelands of the Canadian Kootenay–Ktunaxa.

The Chinook tribe, which is not federally recognized, who live near the lower Columbia River, call it ' or ' in the Upper Chinook (Kiksht) language, and it is "Nch’i-Wàna" or "Nchi wana" to the Sahaptin (Ichishkíin Sɨ́nwit)-speaking peoples of its middle course in present-day Washington. The river is known as "" by the Sinixt people, who live in the area of the Arrow Lakes in the river's upper reaches in Canada. All three terms essentially mean "the big river".

Oral histories describe the formation and destruction of the Bridge of the Gods, a land bridge that connected the Oregon and Washington sides of the river in the Columbia River Gorge. The bridge, which aligns with geological records of the Bonneville Slide, was described in some stories as the result of a battle between gods, represented by Mount Adams and Mount Hood, in their competition for the affection of a goddess, represented by Mount St. Helens. Native American stories about the bridge differ in their details but agree in general that the bridge permitted increased interaction between tribes on the north and south sides of the river.

Horses, originally acquired from Spanish New Mexico, spread widely via native trade networks, reaching the Shoshone of the Snake River Plain by 1700. The Nez Perce, Cayuse, and Flathead people acquired their first horses around 1730. Along with horses came aspects of the emerging plains culture, such as equestrian and horse training skills, greatly increased mobility, hunting efficiency, trade over long distances, intensified warfare, the linking of wealth and prestige to horses and war, and the rise of large and powerful tribal confederacies. The Nez Perce and Cayuse kept large herds and made annual long-distance trips to the Great Plains for bison hunting, adopted the plains culture to a significant degree, and became the main conduit through which horses and the plains culture diffused into the Columbia River region. Other peoples acquired horses and aspects of the plains culture unevenly. The Yakama, Umatilla, Palus, Spokane, and Coeur d'Alene maintained sizable herds of horses and adopted some of the plains cultural characteristics, but fishing and fish-related economies remained important. Less affected groups included the Molala, Klickitat, Wenatchi, Okanagan, and Sinkiuse-Columbia peoples, who owned small numbers of horses and adopted few plains culture features. Some groups remained essentially unaffected, such as the Sanpoil and Nespelem people, whose culture remained centered on fishing.

Natives of the region encountered foreigners at several times and places during the 18th and 19th centuries. European and American vessels explored the coastal area around the mouth of the river in the late 18th century, trading with local natives. The contact would prove devastating to the Indian tribes; a large portion of their population was wiped out by a smallpox epidemic. Canadian explorer Alexander Mackenzie crossed what is now interior British Columbia in 1793. From 1805 to 1807, the Lewis and Clark Expedition entered the Oregon Country along the Clearwater and Snake rivers, and encountered numerous small settlements of natives. Their records recount tales of hospitable traders who were not above stealing small items from the visitors. They also noted brass teakettles, a British musket, and other artifacts that had been obtained in trade with coastal tribes. From the earliest contact with westerners, the natives of the mid- and lower Columbia were not tribal, but instead congregated in social units no larger than a village, and more often at a family level; these units would shift with the season as people moved about, following the salmon catch up and down the river's tributaries.

Sparked by the 1848 Whitman Massacre, a number of violent battles were fought between American settlers and the region's natives. The subsequent Indian Wars, especially the Yakima War, decimated the native population and removed much land from native control. As years progressed, the right of natives to fish along the Columbia became the central issue of contention with the states, commercial fishers, and private property owners. The US Supreme Court upheld fishing rights in landmark cases in 1905 and 1918, as well as the 1974 case "United States v. Washington", commonly called the Boldt Decision.

Fish were central to the culture of the region's natives, both as sustenance and as part of their religious beliefs. Natives drew fish from the Columbia at several major sites, which also served as trading posts. Celilo Falls, located east of the modern city of The Dalles, was a vital hub for trade and the interaction of different cultural groups, being used for fishing and trading for 11,000 years. Prior to contact with westerners, villages along this stretch may have at times had a population as great as 10,000. The site drew traders from as far away as the Great Plains.

The Cascades Rapids of the Columbia River Gorge, and Kettle Falls and Priest Rapids in eastern Washington, were also major fishing and trading sites.

In prehistoric times the Columbia's salmon and steelhead runs numbered an estimated annual average of 10 to 16 million fish. In comparison, the largest run since 1938 was in 1986, with 3.2 million fish entering the Columbia. The annual catch by natives has been estimated at . The most important and productive native fishing site was located at Celilo Falls, which was perhaps the most productive inland fishing site in North America. The falls were located at the border between Chinookan- and Sahaptian-speaking peoples and served as the center of an extensive trading network across the Pacific Plateau. Celilo was the oldest continuously inhabited community on the North American continent.

Salmon canneries established by white settlers beginning in 1866 had a strong negative impact on the salmon population, and in 1908 US President Theodore Roosevelt observed that the salmon runs were but a fraction of what they had been 25 years prior.

As river development continued in the 20th century, each of these major fishing sites was flooded by a dam, beginning with Cascades Rapids in 1938. The development was accompanied by extensive negotiations between natives and US government agencies. The Confederated Tribes of Warm Springs, a coalition of various tribes, adopted a constitution and incorporated after the 1938 completion of the Bonneville Dam flooded Cascades Rapids; Still, in the 1930s, there were natives who lived along the river and fished year round, moving along with the fish's migration patterns throughout the seasons. The Yakama were slower to do so, organizing a formal government in 1944. In the 21st century, the Yakama, Nez Perce, Umatilla, and Warm Springs tribes all have treaty fishing rights along the Columbia and its tributaries.

In 1957 Celilo Falls was submerged by the construction of The Dalles Dam, and the native fishing community was displaced. The affected tribes received a $26.8 million settlement for the loss of Celilo and other fishing sites submerged by The Dalles Dam. The Confederated Tribes of Warm Springs used part of its $4 million settlement to establish the Kah-Nee-Ta resort south of Mount Hood.

Some historians believe that Japanese or Chinese vessels blown off course reached the Northwest Coast long before Europeans—possibly as early as 219 BCE. Historian Derek Hayes claims that "It is a near certainty that Japanese or Chinese people arrived on the northwest coast long before any European." It is unknown whether they landed near the Columbia. Evidence exists that Spanish castaways reached the shore in 1679 and traded with the Clatsop; if these were the first Europeans to see the Columbia, they failed to send word home to Spain.
In the 18th century, there was strong interest in discovering a Northwest Passage that would permit navigation between the Atlantic (or inland North America) and the Pacific Ocean. Many ships in the area, especially those under Spanish and British command, searched the northwest coast for a large river that might connect to Hudson Bay or the Missouri River. The first documented European discovery of the Columbia River was that of Bruno de Heceta, who in 1775 sighted the river's mouth. On the advice of his officers, he did not explore it, as he was short-staffed and the current was strong. He considered it a bay, and called it "Ensenada de Asunción". Later Spanish maps based on his discovery showed a river, labeled "Rio de San Roque", or an entrance, called "Entrada de Hezeta". Following Heceta's reports, British maritime fur trader Captain John Meares searched for the river in 1788 but concluded that it did not exist. He named Cape Disappointment for the non-existent river, not realizing the cape marks the northern edge of the river's mouth.

What happened next would form the basis for decades of both cooperation and dispute between British and American exploration of, and ownership claim to, the region. Royal Navy commander George Vancouver sailed past the mouth in April 1792 and observed a change in the water's color, but he accepted Meares' report and continued on his journey northward. Later that month, Vancouver encountered the American captain Robert Gray at the Strait of Juan de Fuca. Gray reported that he had seen the entrance to the Columbia and had spent nine days trying but failing to enter.

On May 12, 1792, Gray returned south and crossed the Columbia Bar, becoming the first known explorer of European descent to enter the river. Gray's fur trading mission had been financed by Boston merchants, who outfitted him with a private vessel named "Columbia Rediviva"; he named the river after the ship on May 18. Gray spent nine days trading near the mouth of the Columbia, then left without having gone beyond upstream. The farthest point reached was Grays Bay at the mouth of Grays River. Gray's discovery of the Columbia River was later used by the United States to support its claim to the Oregon Country, which was also claimed by Russia, Great Britain, Spain and other nations.

In October 1792, Vancouver sent Lieutenant William Robert Broughton, his second-in-command, up the river. Broughton got as far as the Sandy River at the western end of the Columbia River Gorge, about upstream, sighting and naming Mount Hood. Broughton formally claimed the river, its drainage basin, and the nearby coast for Britain. In contrast, Gray had not made any formal claims on behalf of the United States.

Because the Columbia was at the same latitude as the headwaters of the Missouri River, there was some speculation that Gray and Vancouver had discovered the long-sought Northwest Passage. A 1798 British map showed a dotted line connecting the Columbia with the Missouri. When the American explorers Meriwether Lewis and William Clark charted the vast, unmapped lands of the American West in their overland expedition (1803–05), they found no passage between the rivers. After crossing the Rocky Mountains, Lewis and Clark built dugout canoes and paddled down the Snake River, reaching the Columbia near the present-day Tri-Cities, Washington. They explored a few miles upriver, as far as Bateman Island, before heading down the Columbia, concluding their journey at the river's mouth and establishing Fort Clatsop, a short-lived establishment that was occupied for less than three months.

Canadian explorer David Thompson, of the North West Company, spent the winter of 1807–08 at Kootanae House near the source of the Columbia at present-day Invermere, British Columbia. Over the next few years he explored much of the river and its northern tributaries. In 1811 he traveled down the Columbia to the Pacific Ocean, arriving at the mouth just after John Jacob Astor's Pacific Fur Company had founded Astoria. On his return to the north, Thompson explored the one remaining part of the river he had not yet seen, becoming the first Euro-descended person to travel the entire length of the river.

In 1825, the Hudson's Bay Company (HBC) established Fort Vancouver on the bank of the Columbia, in what is now Vancouver, Washington, as the headquarters of the company's Columbia District, which encompassed everything west of the Rocky Mountains. Chief Factor John McLoughlin, a physician who had been in the fur trade since 1804, was appointed superintendent of the Columbia District. The HBC reoriented its Columbia District operations toward the Pacific Ocean via the Columbia, which became the region's main trunk route. In the early 1840s Americans began to colonize the Oregon country in large numbers via the Oregon Trail, despite the HBC's efforts to discourage American settlement in the region. For many the final leg of the journey involved travel down the lower Columbia River to Fort Vancouver. This part of the Oregon Trail, the treacherous stretch from The Dalles to below the Cascades, could not be traversed by horses or wagons (only watercraft, at great risk). This prompted the 1846 construction of the Barlow Road.

In the Treaty of 1818 the United States and Britain agreed that both nations were to enjoy equal rights in Oregon Country for 10 years. By 1828, when the so-called "joint occupation" was renewed for an indefinite period, it seemed probable that the lower Columbia River would in time become the border between the two nations. For years the Hudson's Bay Company successfully maintained control of the Columbia River and American attempts to gain a foothold were fended off. In the 1830s, American religious missions were established at several locations in the lower Columbia River region. In the 1840s a mass migration of American settlers undermined British control. The Hudson's Bay Company tried to maintain dominance by shifting from the fur trade, which was in decline, to exporting other goods such as salmon and lumber. Colonization schemes were attempted, but failed to match the scale of American settlement. Americans generally settled south of the Columbia, mainly in the Willamette Valley. The Hudson's Bay Company tried to establish settlements north of the river, but nearly all the British colonists moved south to the Willamette Valley. The hope that the British colonists might dilute the American presence in the valley failed in the face of the overwhelming number of American settlers. These developments rekindled the issue of "joint occupation" and the boundary dispute. While some British interests, especially the Hudson's Bay Company, fought for a boundary along the Columbia River, the Oregon Treaty of 1846 set the boundary at the 49th parallel. As part of the treaty, the British retained all areas north of the line while the U.S. acquired the south. The Columbia River became much of the border between the U.S. territories of Oregon and Washington. Oregon became a U.S. state in 1859, while Washington later entered into the Union in 1889.

By the turn of the 20th century, the difficulty of navigating the Columbia was seen as an impediment to the economic development of the Inland Empire region east of the Cascades. The dredging and dam building that followed would permanently alter the river, disrupting its natural flow but also providing electricity, irrigation, navigability and other benefits to the region.

American captain Robert Gray and British captain George Vancouver, who explored the river in 1792, proved that it was possible to cross the Columbia Bar. Many of the challenges associated with that feat remain today; even with modern engineering alterations to the mouth of the river, the strong currents and shifting sandbar make it dangerous to pass between the river and the Pacific Ocean.

The use of steamboats along the river, beginning with the British "Beaver" in 1836 and followed by American vessels in 1850, contributed to the rapid settlement and economic development of the region. Steamboats operated in several distinct stretches of the river: on its lower reaches, from the Pacific Ocean to Cascades Rapids; from the Cascades to Celilo Falls; from Celilo to the confluence with the Snake River; on the Wenatchee Reach of eastern Washington; on British Columbia's Arrow Lakes; and on tributaries like the Willamette, the Snake and Kootenay Lake. The boats, initially powered by burning wood, carried passengers and freight throughout the region for many years. Early railroads served to connect steamboat lines interrupted by waterfalls on the river's lower reaches. In the 1880s, railroads maintained by companies such as the Oregon Railroad and Navigation Company began to supplement steamboat operations as the major transportation links along the river.

As early as 1881, industrialists proposed altering the natural channel of the Columbia to improve navigation. Changes to the river over the years have included the construction of jetties at the river's mouth, dredging, and the construction of canals and navigation locks. Today, ocean freighters can travel upriver as far as Portland and Vancouver, and barges can reach as far inland as Lewiston, Idaho.

The shifting Columbia Bar makes passage between the river and the Pacific Ocean difficult and dangerous, and numerous rapids along the river hinder navigation. "Pacific Graveyard," a 1964 book by James A. Gibbs, describes the many shipwrecks near the mouth of the Columbia. Jetties, first constructed in 1886, extend the river's channel into the ocean. Strong currents and the shifting sandbar remain a threat to ships entering the river and necessitate continuous maintenance of the jetties.

In 1891 the Columbia was dredged to enhance shipping. The channel between the ocean and Portland and Vancouver was deepened from to . "The Columbian" called for the channel to be deepened to as early as 1905, but that depth was not attained until 1976.

Cascade Locks and Canal were first constructed in 1896 around the Cascades Rapids, enabling boats to travel safely through the Columbia River Gorge. The Celilo Canal, bypassing Celilo Falls, opened to river traffic in 1915. In the mid-20th century, the construction of dams along the length of the river submerged the rapids beneath a series of reservoirs. An extensive system of locks allowed ships and barges to pass easily from one reservoir to the next. A navigation channel reaching to Lewiston, Idaho, along the Columbia and Snake rivers, was completed in 1975. Among the main commodities are wheat and other grains, mainly for export. As of 2016, the Columbia ranked third, behind the Mississippi and Paraná rivers, among the world's largest export corridors for grain.

The 1980 eruption of Mount St. Helens caused mudslides in the area, which reduced the Columbia's depth by for a stretch, disrupting Portland's economy.

Efforts to maintain and improve the navigation channel have continued to the present day. In 1990 a new round of studies examined the possibility of further dredging on the lower Columbia. The plans were controversial from the start because of economic and environmental concerns.

In 1999, Congress authorized deepening the channel between Portland and Astoria from , which will make it possible for large container and grain ships to reach Portland and Vancouver. The project has met opposition because of concerns about stirring up toxic sediment on the riverbed. Portland-based Northwest Environmental Advocates brought a lawsuit against the Army Corps of Engineers, but it was rejected by the Ninth U.S. Circuit Court of Appeals in August 2006. The project includes measures to mitigate environmental damage; for instance, the US Army Corps of Engineers must restore 12 times the area of wetland damaged by the project. In early 2006, the Corps spilled of hydraulic oil into the Columbia, drawing further criticism from environmental organizations.

Work on the project began in 2005 and concluded in 2010. The project's cost is estimated at $150 million. The federal government is paying 65 percent, Oregon and Washington are paying $27 million each, and six local ports are also contributing to the cost.

In 1902, the United States Bureau of Reclamation was established to aid in the economic development of arid western states. One of its major undertakings was building Grand Coulee Dam to provide irrigation for the of the Columbia Basin Project in central Washington. With the onset of World War II, the focus of dam construction shifted to production of hydroelectricity. Irrigation efforts resumed after the war.

River development occurred within the structure of the 1909 International Boundary Waters Treaty between the US and Canada. The United States Congress passed the Rivers and Harbors Act of 1925, which directed the Army Corps of Engineers and the Federal Power Commission to explore the development of the nation's rivers. This prompted agencies to conduct the first formal financial analysis of hydroelectric development; the reports produced by various agencies were presented in House Document 308. Those reports, and subsequent related reports, are referred to as 308 Reports.

In the late 1920s, political forces in the Northwestern United States generally favored private development of hydroelectric dams along the Columbia. But the overwhelming victories of gubernatorial candidate George W. Joseph in the 1930 Republican primary, and later his law partner Julius Meier, were understood to demonstrate strong public support for public ownership of dams. In 1933, President Franklin D. Roosevelt signed a bill that enabled the construction of the Bonneville and Grand Coulee dams as public works projects. The legislation was attributed to the efforts of Oregon Senator Charles McNary, Washington Senator Clarence Dill, and Oregon Congressman Charles Martin, among others.

In 1948 floods swept through the Columbia watershed, destroying Vanport, then the second largest city in Oregon, and impacting cities as far north as Trail, British Columbia. The flooding prompted the United States Congress to pass the Flood Control Act of 1950, authorizing the federal development of additional dams and other flood control mechanisms. By that time local communities had become wary of federal hydroelectric projects, and sought local control of new developments; a public utility district in Grant County, Washington, ultimately began construction of the dam at Priest Rapids.

In the 1960s, the United States and Canada signed the Columbia River Treaty, which focused on flood control and the maximization of downstream power generation. Canada agreed to build dams and provide reservoir storage, and the United States agreed to deliver to Canada one-half of the increase in US downstream power benefits as estimated five years in advance. Canada's obligation was met by building three dams (two on the Columbia, and one on the Duncan River), the last of which was completed in 1973.

Today the main stem of the Columbia River has 14 dams, of which three are in Canada and 11 in the US. Four mainstem dams and four lower Snake River dams contain navigation locks to allow ship and barge passage from the ocean as far as Lewiston, Idaho. The river system as a whole has more than 400 dams for hydroelectricity and irrigation. The dams address a variety of demands, including flood control, navigation, stream flow regulation, storage and delivery of stored waters, reclamation of public lands and Indian reservations, and the generation of hydroelectric power.

The larger US dams are owned and operated by the federal government (some by the Army Corps of Engineers and some by the Bureau of Reclamation), while the smaller dams are operated by public utility districts, and private power companies. The federally operated system is known as the Federal Columbia River Power System, which includes 31 dams on the Columbia and its tributaries. The system has altered the seasonal flow of the river in order to meet higher electricity demands during the winter. At the beginning of the 20th century, roughly 75 percent of the Columbia's flow occurred in the summer, between April and September. By 1980, the summer proportion had been lowered to about 50 percent, essentially eliminating the seasonal pattern.

The installation of dams dramatically altered the landscape and ecosystem of the river. At one time, the Columbia was one of the top salmon-producing river systems in the world. Previously active fishing sites, such as Celilo Falls in the eastern Columbia River Gorge, have exhibited a sharp decline in fishing along the Columbia in the last century, and salmon populations have been dramatically reduced. Fish ladders have been installed at some dam sites to help the fish journey to spawning waters. Chief Joseph Dam has no fish ladders and completely blocks fish migration to the upper half of the Columbia River system.

The Bureau of Reclamation's Columbia Basin Project focused on the generally dry region of central Washington known as the Columbia Basin, which features rich loess soil. Several groups developed competing proposals, and in 1933, President Franklin D. Roosevelt authorized the Columbia Basin Project. The Grand Coulee Dam was the project's central component; upon completion, it pumped water up from the Columbia to fill the formerly dry Grand Coulee, forming Banks Lake. By 1935, the intended height of the dam was increased from a range between to , a height that would extend the lake impounded by the dam all the way to the Canada–US border; the project had grown from a local New Deal relief measure to a major national project.

The project's initial purpose was irrigation, but the onset of World War II created a high demand for electricity, mainly for aluminum production and for the development of nuclear weapons at the Hanford Site. Irrigation began in 1951. The project provides water to more than of fertile but arid land in central Washington, transforming the region into a major agricultural center. Important crops include orchard fruit, potatoes, alfalfa, mint, beans, beets, and wine grapes.

Since 1750, the Columbia has experienced six multi-year droughts. The longest, lasting 12 years in the mid‑19th century, reduced the river's flow to 20 percent below average. Scientists have expressed concern that a similar drought would have grave consequences in a region so dependent on the Columbia. In 1992–1993, a lesser drought affected farmers, hydroelectric power producers, shippers, and wildlife managers.

Many farmers in central Washington build dams on their property for irrigation and to control frost on their crops. The Washington Department of Ecology, using new techniques involving aerial photographs, estimated there may be as many as a hundred such dams in the area, most of which are illegal. Six such dams have failed in recent years, causing hundreds of thousands of dollars of damage to crops and public roads. Fourteen farms in the area have gone through the permitting process to build such dams legally.

The Columbia's heavy flow and large elevation drop over a short distance, , give it tremendous capacity for hydroelectricity generation. In comparison, the Mississippi drops less than . The Columbia alone possesses one-third of the United States's hydroelectric potential. In 2012, the river and its tributaries accounted for 29 GW of hydroelectric generating capacity, contributing 44 percent of the total hydroelectric generation in the nation.
The largest of the 150 hydroelectric projects, the Grand Coulee Dam and the Chief Joseph Dam, are also the largest in the United States. As of 2017, Grand Coulee is the fifth largest hydroelectric plant in the world.

Inexpensive hydropower supported the location of a large aluminum industry in the region, because its reduction from bauxite requires large amounts of electricity. Until 2000, the Northwestern United States produced up to 17 percent of the world's aluminum and 40 percent of the aluminum produced in the United States. The commoditization of power in the early 21st century, coupled with drought that reduced the generation capacity of the river, damaged the industry and by 2001, Columbia River aluminum producers had idled 80 percent of its production capacity. By 2003, the entire United States produced only 15 percent of the world's aluminum, and many smelters along the Columbia had gone dormant or out of business.

Power remains relatively inexpensive along the Columbia, and since the mid-2000 several global enterprises have moved server farm operations into the area to avail themselves of cheap power. Downriver of Grand Coulee, each dam's reservoir is closely regulated by the Bonneville Power Administration (BPA), the U.S. Army Corps of Engineers, and various Washington public utility districts to ensure flow, flood control, and power generation objectives are met. Increasingly, hydro-power operations are required to meet standards under the US Endangered Species Act and other agreements to manage operations to minimize impacts on salmon and other fish, and some conservation and fishing groups support removing four dams on the lower Snake River, the largest tributary of the Columbia.

In 1941, the BPA hired Oklahoma folksinger Woody Guthrie to write songs for a documentary film promoting the benefits of hydropower. In the month he spent traveling the region Guthrie wrote 26 songs, which have become an important part of the cultural history of the region.

The Columbia supports several species of anadromous fish that migrate between the Pacific Ocean and fresh water tributaries of the river. Sockeye salmon, Coho and Chinook (also known as "king") salmon, and steelhead, all of the genus "Oncorhynchus", are ocean fish that migrate up the rivers at the end of their life cycles to spawn. White sturgeon, which take 15 to 25 years to mature, typically migrate between the ocean and the upstream habitat several times during their lives.

Salmon populations declined dramatically after the establishment of canneries in 1867. In 1879 it was reported that 545,450 salmon, with an average weight of were caught (in a recent season) and mainly canned for export to England. A can weighing could be sold for 8d or 9d. By 1908, there was widespread concern about the decline of salmon and sturgeon. In that year, the people of Oregon passed two laws under their newly instituted program of citizens' initiatives limiting fishing on the Columbia and other rivers. Then in 1948, another initiative banned the use of seine nets (devices already used by Native Americans, and refined by later settlers) altogether.

Dams interrupt the migration of anadromous fish. Salmon and steelhead return to the streams in which they were born to spawn; where dams prevent their return, entire populations of salmon die. Some of the Columbia and Snake River dams employ fish ladders, which are effective to varying degrees at allowing these fish to travel upstream. Another problem exists for the juvenile salmon headed downstream to the ocean. Previously, this journey would have taken two to three weeks. With river currents slowed by the dams, and the Columbia converted from wild river to a series of slackwater pools, the journey can take several months, which increases the mortality rate. In some cases, the Army Corps of Engineers transports juvenile fish downstream by truck or river barge. The Chief Joseph Dam and several dams on the Columbia's tributaries entirely block migration, and there are no migrating fish on the river above these dams. Sturgeon have different migration habits and can survive without ever visiting the ocean. In many upstream areas cut off from the ocean by dams, sturgeon simply live upstream of the dam.

Not all fish have suffered from the modifications to the river; the northern pikeminnow (formerly known as the "squawfish") thrives in the warmer, slower water created by the dams. Research in the mid-1980s found that juvenile salmon were suffering substantially from the predatory pikeminnow, and in 1990, in the interest of protecting salmon, a "bounty" program was established to reward anglers for catching pikeminnow.

In 1994, the salmon catch was smaller than usual in the rivers of Oregon, Washington, and British Columbia, causing concern among commercial fishermen, government agencies, and tribal leaders. US government intervention, to which the states of Alaska, Idaho, and Oregon objected, included an 11-day closure of an Alaska fishery. In April 1994 the Pacific Fisheries Management Council unanimously approved the strictest regulations in 18 years, banning all commercial salmon fishing for that year from Cape Falcon north to the Canada–US border. In the winter of 1994, the return of coho salmon far exceeded expectations, which was attributed in part to the fishing ban.

Also in 1994, United States Secretary of the Interior Bruce Babbitt first proposed the removal of several Pacific Northwest dams because of their impact on salmon spawning. The Northwest Power Planning Council approved a plan that provided more water for fish and less for electricity, irrigation, and transportation. Environmental advocates have called for the removal of certain dams in the Columbia system in the years since. Of the 227 major dams in the Columbia River drainage basin, the four Washington dams on the lower Snake River are often identified for removal, for example in an ongoing lawsuit concerning a Bush administration plan for salmon recovery. These dams and reservoirs limit the recovery of upriver salmon runs to Idaho's Salmon and Clearwater rivers. Historically, the Snake produced over 1.5 million spring and summer Chinook salmon, a number that has dwindled to several thousand in recent years. Idaho Power Company's Hells Canyon dams have no fish ladders (and do not pass juvenile salmon downstream), and thus allow no steelhead or salmon to migrate above Hells Canyon. In 2007, the destruction of the Marmot Dam on the Sandy River was the first dam removal in the system. Other Columbia Basin dams that have been removed include Condit Dam on Washington's White Salmon River, and the Milltown Dam on the Clark Fork in Montana.

In southeastern Washington, a stretch of the river passes through the Hanford Site, established in 1943 as part of the Manhattan Project. The site served as a plutonium production complex, with nine nuclear reactors and related facilities along the banks of the river. From 1944 to 1971, pump systems drew cooling water from the river and, after treating this water for use by the reactors, returned it to the river. Before being released back into the river, the used water was held in large tanks known as retention basins for up to six hours. Longer-lived isotopes were not affected by this retention, and several terabecquerels entered the river every day. By 1957, the eight plutonium production reactors at Hanford dumped a daily average of 50,000 curies of radioactive material into the Columbia. These releases were kept secret by the federal government until the release of declassified documents in the late 1980s. Radiation was measured downstream as far west as the Washington and Oregon coasts.

The nuclear reactors were decommissioned at the end of the Cold War, and the Hanford site is the focus of one of the world's largest environmental cleanup, managed by the Department of Energy under the oversight of the Washington Department of Ecology and the Environmental Protection Agency. Nearby aquifers contain an estimated 270 billion US gallons (1 billion m) of groundwater contaminated by high-level nuclear waste that has leaked out of Hanford's underground storage tanks. , 1 million US gallons (3,785 m) of highly radioactive waste is traveling through groundwater toward the Columbia River. This waste is expected to reach the river in 12 to 50 years if cleanup does not proceed on schedule.

In addition to concerns about nuclear waste, numerous other pollutants are found in the river. These include chemical pesticides, bacteria, arsenic, dioxins, and polychlorinated biphenyls (PCB).

Studies have also found significant levels of toxins in fish and the waters they inhabit within the basin. Accumulation of toxins in fish threatens the survival of fish species, and human consumption of these fish can lead to health problems. Water quality is also an important factor in the survival of other wildlife and plants that grow in the Columbia River drainage basin. The states, Indian tribes, and federal government are all engaged in efforts to restore and improve the water, land, and air quality of the Columbia River drainage basin and have committed to work together to enhance and accomplish critical ecosystem restoration efforts. A number of cleanup efforts are currently underway, including Superfund projects at Portland Harbor, Hanford, and Lake Roosevelt.

Timber industry activity further contaminates river water, for example in the increased sediment runoff that results from clearcuts. The Northwest Forest Plan, a piece of federal legislation from 1994, mandated that timber companies consider the environmental impacts of their practices on rivers like the Columbia.

On July 1, 2003, Christopher Swain of Portland, Oregon, became the first person to swim the Columbia River's entire length, in an effort to raise public awareness about the river's environmental health.

Both natural and anthropogenic processes are involved in the cycling of nutrients in the Columbia River basin. Natural processes in the system include estuarine mixing of fresh and ocean waters, and climate variability patterns such as the Pacific Decadal Oscillation and the El Nino Southern Oscillation (both climatic cycles that affect the amount of regional snowpack and river discharge). Natural sources of nutrients in the Columbia River include weathering, leaf litter, salmon carcasses, runoff from its tributaries, and ocean estuary exchange. Major anthropogenic impacts to nutrients in the basin are due to fertilizers from agriculture, sewage systems, logging, and the construction of dams.

Nutrients dynamics vary in the river basin from the headwaters to the main river and dams, to finally reaching the Columbia River estuary and ocean. Upstream in the headwaters, salmon runs are the main source of nutrients. Dams along the river impact nutrient cycling by increasing residence time of nutrients, and reducing the transport of silicate to the estuary, which directly impacts diatoms, a type of phytoplankton. The dams are also a barrier to salmon migration, and can increase the amount of methane locally produced. The Columbia River estuary exports high rates of nutrients into the Pacific Ocean; with the exception of nitrogen, which is delivered into the estuary by ocean upwelling sources.

Most of the Columbia's drainage basin (which, at , is about the size of France) lies roughly between the Rocky Mountains on the east and the Cascade Mountains on the west. In the United States and Canada the term watershed is often used to mean drainage basin. The term "Columbia Basin" is used to refer not only to the entire drainage basin but also to subsets of the river's full watershed, such as the relatively flat and unforested area in eastern Washington bounded by the Cascades, the Rocky Mountains, and the Blue Mountains. Within the watershed are diverse landforms including mountains, arid plateaus, river valleys, rolling uplands, and deep gorges. Grand Teton National Park lies in the watershed, as well as parts of Yellowstone National Park, Glacier National Park, Mount Rainier National Park, and North Cascades National Park. Canadian National Parks in the watershed include Kootenay National Park, Yoho National Park, Glacier National Park, and Mount Revelstoke National Park. Hells Canyon, the deepest gorge in North America, and the Columbia Gorge are in the watershed. Vegetation varies widely, ranging from western hemlock and western redcedar in the moist regions to sagebrush in the arid regions. The watershed provides habitat for 609 known fish and wildlife species, including the bull trout, bald eagle, gray wolf, grizzly bear, and Canada lynx.

The World Wide Fund for Nature (WWF) divides the waters of the Columbia and its tributaries into three freshwater ecoregions, naming them Columbia Glaciated, Columbia Unglaciated, and Upper Snake. The Columbia Glaciated ecoregion, making up about a third of the total watershed, lies in the north and was covered with ice sheets during the Pleistocene. The ecoregion includes the mainstem Columbia north of the Snake River and tributaries such as the Yakima, Okanagan, Pend Oreille, Clark Fork, and Kootenay rivers. The effects of glaciation include a number of large lakes and a relatively low diversity of freshwater fish. The Upper Snake ecoregion is defined as the Snake River watershed above Shoshone Falls, which totally blocks fish migration. This region has 14 species of fish, many of which are endemic. The Columbia Unglaciated ecoregion makes up the rest of the watershed. It includes the mainstem Columbia below the Snake River and tributaries such as the Salmon, John Day, Deschutes, and lower Snake Rivers. Of the three ecoregions it is the richest in terms of freshwater species diversity. There are 35 species of fish, of which four are endemic. There are also high levels of mollusk endemism.

In 2016, over eight million people lived within the Columbia's drainage basin. Of this total about 3.5 million people lived in Oregon, 2.1 million in Washington, 1.7 million in Idaho, half a million in British Columbia, and 0.4 million in Montana. Population in the watershed has been rising for many decades and is projected to rise to about 10 million by 2030. The highest population densities are found west of the Cascade Mountains along the I-5 corridor, especially in the Portland-Vancouver urban area. High densities are also found around Spokane, Washington, and Boise, Idaho. Although much of the watershed is rural and sparsely populated, areas with recreational and scenic values are growing rapidly. The central Oregon county of Deschutes is the fastest-growing in the state. Populations have also been growing just east of the Cascades in central Washington around the city of Yakima and the Tri-Cities area. Projections for the coming decades assume growth throughout the watershed, including the interior. The Canadian part of the Okanagan subbasin is also growing rapidly.

Climate varies greatly from place to place within the watershed. Elevation ranges from sea level at the river mouth to more than in the mountains, and temperatures vary with elevation. The highest peak is Mount Rainier, at . High elevations have cold winters and short cool summers; interior regions are subject to great temperature variability and severe droughts. Over some of the watershed, especially west of the Cascade Mountains, precipitation maximums occur in winter, when Pacific storms come ashore. Atmospheric conditions block the flow of moisture in summer, which is generally dry except for occasional thunderstorms in the interior. In some of the eastern parts of the watershed, especially shrub-steppe regions with Continental climate patterns, precipitation maximums occur in early summer. Annual precipitation varies from more than a year in the Cascades to less than in the interior. Much of the watershed gets less than a year.

Several major North American drainage basins and many minor ones share a common border with the Columbia River's drainage basin. To the east, in northern Wyoming and Montana, the Continental Divide separates the Columbia watershed from the Mississippi-Missouri watershed, which empties into the Gulf of Mexico. To the northeast, mostly along the southern border between British Columbia and Alberta, the Continental Divide separates the Columbia watershed from the Nelson-Lake Winnipeg-Saskatchewan watershed, which empties into Hudson Bay. The Mississippi and Nelson watersheds are separated by the Laurentian Divide, which meets the Continental Divide at Triple Divide Peak near the headwaters of the Columbia's Flathead River tributary. This point marks the meeting of three of North America's main drainage patterns, to the Pacific Ocean, to Hudson Bay, and to the Atlantic Ocean via the Gulf of Mexico.

Further north along the Continental Divide, a short portion of the combined Continental and Laurentian divides separate the Columbia watershed from the MacKenzie-Slave-Athabasca watershed, which empties into the Arctic Ocean. The Nelson and Mackenzie watersheds are separated by a divide between streams flowing to the Arctic Ocean and those of the Hudson Bay watershed. This divide meets the Continental Divide at Snow Dome (also known as Dome), near the northernmost bend of the Columbia River.

To the southeast, in western Wyoming, another divide separates the Columbia watershed from the Colorado–Green watershed, which empties into the Gulf of California. The Columbia, Colorado, and Mississippi watersheds meet at Three Waters Mountain in the Wind River Range of . To the south, in Oregon, Nevada, Utah, Idaho, and Wyoming, the Columbia watershed is divided from the Great Basin, whose several watersheds are endorheic, not emptying into any ocean but rather drying up or sinking into sumps. Great Basin watersheds that share a border with the Columbia watershed include Harney Basin, Humboldt River, and Great Salt Lake. The associated triple divide points are Commissary Ridge North, Wyoming, and Sproats Meadow Northwest, Oregon. To the north, mostly in British Columbia, the Columbia watershed borders the Fraser River watershed. To the west and southwest the Columbia watershed borders a number of smaller watersheds that drain to the Pacific Ocean, such as the Klamath River in Oregon and California and the Puget Sound Basin in Washington.

The Columbia receives more than 60 significant tributaries. The four largest that empty directly into the Columbia (measured either by discharge or by size of watershed) are the Snake River (mostly in Idaho), the Willamette River (in northwest Oregon), the Kootenay River (mostly in British Columbia), and the Pend Oreille River (mostly in northern Washington and Idaho, also known as the lower part of the Clark Fork). Each of these four averages more than and drains an area of more than .

The Snake is by far the largest tributary. Its watershed of is larger than the state of Idaho. Its discharge is roughly a third of the Columbia's at the rivers' confluence but compared to the Columbia upstream of the confluence the Snake is longer (113%) and has a larger drainage basin (104%).

The Pend Oreille River system (including its main tributaries, the Clark Fork and Flathead rivers) is also similar in size to the Columbia at their confluence. Compared to the Columbia River above the two rivers' confluence, the Pend Oreille-Clark-Flathead is nearly as long (about 86%), its basin about three-fourths as large (76%), and its discharge over a third (37%).




</doc>
<doc id="5409" url="https://en.wikipedia.org/wiki?curid=5409" title="Commelinales">
Commelinales

Commelinales is the botanical name of an order of flowering plants. It comprises five families: Commelinaceae, Haemodoraceae, Hanguanaceae, Philydraceae, and Pontederiaceae. All the families combined contain over 885 species in about 70 genera; the majority of species are in the Commelinaceae. Plants in the order share a number of synapomorphies that tie them together, such as a lack of mycorrhizal associations and tapetal raphides. Estimates differ as to when the Comminales evolved, but most suggest an origin and diversification sometime during the mid- to late Cretaceous. Depending on the methods used, studies suggest a range of origin between 123 and 73 million years, with diversification occurring within the group 110 to 66 million years ago. The order's closest relatives are in the Zingiberales, which includes ginger, bananas, cardamom, and others.

According to the most recent classification scheme, the APG IV of 2016, the order includes five families:

This is unchanged from the APG III of 2009 and the APG II of 2003, but different from the older APG system of 1998, which did not include Hanguanaceae.

The older Cronquist system of 1981, which was based purely on morphological data, placed the order in subclass Commelinidae of class Liliopsida and included the families Commelinaceae, Mayacaceae, Rapateaceae and Xyridaceae. These families are now known to be only distantly related.
In the classification system of Dahlgren the Commelinales were one of four orders in the superorder Commeliniflorae (also called Commelinanae), and contained five families, of which only Commelinaceae has been retained by the Angiosperm Phylogeny Group (APG).


</doc>
<doc id="5411" url="https://en.wikipedia.org/wiki?curid=5411" title="Cucurbitales">
Cucurbitales

The Cucurbitales are an order of flowering plants, included in the rosid group of dicotyledons. This order mostly belongs to tropical areas, with limited presence in subtropic and temperate regions. The order includes shrubs and trees, together with many herbs and climbers. One major characteristic of the Cucurbitales is the presence of unisexual flowers, mostly pentacyclic, with thick pointed petals (whenever present). The pollination is usually performed by insects, but wind pollination is also present (in Coriariaceae and Datiscaceae).

The order consists of roughly 2600 species in eight families. The largest families are Begoniaceae (begonia family) with around 1500 species and Cucurbitaceae (gourd family) with around 900 species. These two families include the only economically important plants. Specifically, the Cucurbitaceae (gourd family) include some food species, such as squash, pumpkin (both from "Cucurbita"), watermelon ("Citrullus vulgaris"), and cucumber and melons ("Cucumis"). The Begoniaceae are known for their horticultural species, of which there are over 130 with many more varieties.

The Cucurbitales are an order of plants with a cosmopolitan distribution, particularly diverse in the tropics. Most are herbs, climber herbs, woody lianas or shrubs but some genera include canopy-forming evergreen lauroid trees. Members of the Cucurbitales form an important component of low to montane tropical forest with greater representation in terms of the number of species. Although not known with certainty the total number of species in the order, conservative estimates indicate about 2600 species worldwide, distributed in 109 genera. Compared to other flowering plant orders, the taxonomy is poorly understood due to their great diversity, difficulty in identification, and limited study.

The order Cucurbitales in the eurosid I clade comprises almost 2600 species in 109 or 110 genera in eight families, tropical and temperate, of very different sizes, morphology, and ecology. It is a case of divergent evolution. In contrast, there is convergent evolution with other groups not related due to ecological or physical drivers toward a similar solution, including analogous structures.
Some species are trees that have similar foliage to the true laurels due to convergent evolution.

The patterns of speciation in the Cucurbitales are diversified in a high number of species. They have a pantropical distribution with centers of diversity in Africa, South America, and Southeast Asia. They most likely originated in West Gondwana 67–107 million years ago, so the oldest split could relate to the break-up of Gondwana in the middle Eocene to late Oligocene, 45–24 million years ago. The group reached their current distribution by multiple intercontinental dispersal events. One factor was product of aridification, other groups responded to favorable climatic periods and expanded across the available habitat, occurring as opportunistic species across wide distribution; other groups diverged over long periods within isolated areas.

The Cucurbitales comprise the families: Apodanthaceae, Anisophylleaceae, Begoniaceae, Coriariaceae, Corynocarpaceae, Cucurbitaceae, Tetramelaceae, and Datiscaceae. Some of the synapomorphies of the order are: leaves in spiral with secondary veins palmated, calyx or perianth valvate, and the elevated stomatal calyx/perianth bearing separate styles. The two whorls are similar in texture.

"Tetrameles nudiflora" is a tree of immense proportions of height and width; Tetramelaceae, Anisophylleaceae, and Corynocarpaceae are tall canopy trees in temperate and tropical forests. The genus "Dendrosicyos", with the only species being the cucumber tree, is adapted to the arid semidesert island of Socotra. Deciduous perennial Cucurbitales lose all of their leaves for part of the year depending on variations in rainfall. The leaf loss coincides with the dry season in tropical, subtropical and arid regions. In temperate or polar climates, the dry season is due to the inability of the plant to absorb water available in the form of ice. Apodanthaceae are obligatory endoparasites that only emerge once a year in the form of small flowers that develop into small berries, however taxonomists have not agreed on the exact placement of this family within the Cucurbitales.
Over half of the known members of this order belong to the greatly diverse begonia family Begoniaceae, with around 1500 species in two genera. Before modern DNA-molecular classifications, some Cucurbitales species were assigned to orders as diverse as Ranunculales, Malpighiales, Violales, and Rafflesiales. Early molecular studies revealed several surprises, such as the nonmonophyly of the traditional Datiscaceae, including "Tetrameles" and "Octomeles", but the exact relationships among the families remain unclear.
The lack of knowledge about the order in general is due to many species being found in countries with limited economic means or unstable political environments, factors unsuitable for plant collection and detailed study. Thus the vast majority of species remain poorly determined, and a future increase in the number of species is expected.

Under the Cronquist system, the families Begoniaceae, Cucurbitaceae, and Datiscaceae were placed in the order Violales, within the subclass Dilleniidae, with the Tetramelaceae subsumed into the Datiscaceae. Corynocarpaceae was placed in order Celastrales, and Anisophylleaceae in order Rosales, both under subclass Rosidae. Coriariaceae was placed in Ranunculaceae, subclass Magnoliidae. Apodanthaceae was not recognised as a family, its genera being assigned to another parasitic plant family, the Rafflesiaceae. The present classification is due to APG III (2009).

Modern molecular phylogenetics suggest the following relationships:



</doc>
<doc id="5412" url="https://en.wikipedia.org/wiki?curid=5412" title="Contra dance">
Contra dance

Contra dance (also contradance, contra-dance and other variant spellings) is a folk dance made up of long lines of couples. 
It has mixed origins from English country dance, Scottish country dance, and French dance styles in the 17th century. Sometimes described as New England folk dance or Appalachian folk dance, contra dances can be found around the world, but are most common in the United States (periodically held in nearly every state), Canada, and other Anglophone countries.

Contra dancing is a social dance that one can attend without a partner. The dancers form couples, and the couples form sets of two couples in long lines starting from the stage and going down the length of the dance hall. Throughout the course of a dance, couples progress up and down these lines, dancing with each other couple in the line. The dance is led by a caller who teaches the sequence of figures in the dance before the music starts. Callers describe the series of steps called "figures", and in a single dance, a caller may include anywhere from 6–12 figures which are repeated as couples progress up and down the lines. Each time through the dance takes 64 beats, after which the pattern is repeated.

Almost all contra dances are danced to live music. The music played includes, but is not limited to, Irish, Scottish, old-time and French-Canadian folk tunes. The fiddle is considered the core instrument, though other stringed instruments can be used, such as the guitar, banjo, bass and mandolin, as well as the piano, accordion, flute, clarinet and more. Some contra dances are even done to techno music. Music in a dance can consist of a single tune or a medley of tunes, and key changes during the course of a dance are common.

Many callers and bands perform for local contra dances, and some are hired to play for dances around the U.S. and Canada. Many dancers travel regionally (or even nationally) to contra dance weekends and week-long contra dance camps, where they can expect to find other dedicated dancers, great callers, and great bands.

Contra Dancing is a popular form of recreation enjoyed by people of all ages in over 100 cities and towns across the United states (as of 2019), yet it also has a long history that includes European origins, and over 100 years of cultural influences from many different sources.

At the end of the 17th century, English country dances were taken up by French dance masters. The French called these dances "contra-dances" or "contredanses" (which roughly translated "opposites dance"), as indicated in a 1710 dance book called "Recuil de Contredance". As time progressed, these dances returned to England and were spread and reinterpreted in the United States, and eventually the French form of the name came to be associated with the American folk dances, where they were alternatively called "country dances" or in some parts of New England such as New Hampshire, "contradances".

Contra dances were fashionable in the United States and were considered one of the most popular social dances across class lines in the late 18th century, though these events were usually referred to as "country dances" until the 1780s, when the term contra dance became more common to describe these events. In the mid-19th century, group dances started to decline in popularity in favor of quadrilles, lancers, and couple dances such as the waltz and polka.

Henry Ford, founder of the Ford Motor Company, had a role in preserving contra and American folk dancing generally, in part as a response in opposition to modern jazz influences in the United States. In the 1920s, he asked friend and dance coordinator in Massachusetts, Benjamin Lovett, to come to Michigan to begin a dance program. Initially, Lovett could not as he was under contract at a local inn; consequently, Ford bought the property rights to the inn. Lovett and Ford initiated a dance program in Dearborn, Michigan that included several folk dances, including contras. Ford also published a book titled "Good Morning: After a Sleep of Twenty-Five Years, Old-Fashioned Dancing Is Being Revived" in 1926 detailing steps for some contra dances.

In the 1930s and 1940s, the popularity of Jazz, Swing, and "Big Band" music caused contra dance to be forsaken in several parts of the USA, and were primarily held in towns within the Northeastern portions of North America, such as Ohio, the Maritime provinces of Canada, and particularly in New England. Ralph Page almost single-handedly maintained the New England tradition until it was revitalized in the 1950s and 1960s, particularly by Ted Sannella and Dudley Laufman. The New England contra dance tradition was also maintained in Vermont by the Ed Larkin Old Time Contra Dancers, formed by Edwin Loyal Larkin in 1934. The group he founded is still performing, teaching the dances, and holding monthly open house dances in Tunbridge, VT.

By then, early dance camps, retreats, and weekends had emerged, such as Pinewoods Camp, in Plymouth, Massachusetts, which became primarily a music and dance camp in 1933, and NEFFA, the New England Folk Festival, also in Massachusetts, which began in 1944. Pittsburgh Contra Dance celebrated its 100th anniversary in 2015. These and others continue to be popular and some offer other dances and activities besides contra dancing.

In the 1970s, Sannella and other callers introduced movements from English Country Dance, such as heys and gypsies, to the contra dances. New dances, such as "Shadrack's Delight" by Tony Parkes, featured symmetrical dancing by all couples. (Previously, the actives and inactives —see "Progression" below— had significantly different roles). Double progression dances, popularized by Herbie Gaudreau, added to the aerobic nature of the dances, and one caller, Gene Hubert, wrote a quadruple progression dance, "Contra Madness". Becket formation was introduced, with partners next to each other in the line instead of opposite. The Brattleboro Dawn Dance started in 1976, and continues to run semiannually.

In the early 1980s, Tod Whittemore started the first Saturday dance in the Peterborough Town House, which remains one of the more popular regional dances. The Peterborough dance influenced Bob McQuillen, who became a notable musician in New England. As musicians and callers moved to other locations, they founded contra dances in Michigan, Washington, Oregon, California, Texas, and elsewhere.

For the last 20 years, contra dancing has enjoyed renewed popularity, as more people have been able <br> to learn about this form of entertainment and exercise through internet web sites, such as: 


... as well as the availability of Contra Dance videos, showing a variety of example dances, like:

Contra dance events are open to all, regardless of experience unless explicitly labeled otherwise, and it is common to see dancers anywhere from 15 to 70 years of age, and with a variety of backgrounds. Contra dances are family-friendly, and alcohol consumption is not part of the culture. Many events offer beginner-level instructions prior to the dance. A typical evening of contra dance is three hours long, including an intermission. The event consists of a number of individual "contra dances", each lasting about 15 minutes, and typically a band intermission with some waltzes, schottisches, polkas, or Swedish hambos. In some places, square dances are thrown into the mix, sometimes at the discretion of the caller. Music for the evening is typically performed by a live band, playing jigs and reels from Ireland, Scotland, Canada, or the USA. The tunes may range from traditional originating a century ago, to modern compositions including electric guitar, synth keyboard, and driving percussion - so long as the music fits the timing for Contra dance patterns. Sometimes, a rock tune will be woven in, to the delight of the dancers.

Generally, a leader, known as a caller, will teach each individual dance just before the music for that dance begins. During this introductory walk-through, participants learn the dance by walking through the steps and formations, following the caller's instructions. The caller gives the instructions orally, and sometimes augments them with demonstrations of steps by experienced dancers in the group. The walk-through usually proceeds in the order of the moves as they will be done with the music; in some dances, the caller may vary the order of moves during the dance, a fact that is usually explained as part of the caller's instructions.

After the walk-through, the music begins and the dancers repeat that sequence some number of times before that dance ends, often 10 to 15 minutes, depending on the length of the contra lines. Calls are normally given at least the first few times through, and often for the last. At the end of each dance, the dancers thank their partners. The contra dance tradition in North America is to change partners for every dance, while in the United Kingdom typically people dance with the same partner the entire evening. One who attends an evening of contra dances in North America does not need to bring his or her own partner. In the short break between individual dances, the dancers invite each other to dance. Booking ahead by asking partner or partners ahead of time for each individual dance is common at some venues, but has been discouraged by some.

Most contra dances do not have an expected dress code. No special outfits are worn, but comfortable and loose-fitting clothing that does not restrict movement is usually recommended. Lightweight skirts are often worn, at some dances by men as well as women, as these have a very pretty effect when swinging or twirling. However, low heeled, broken-in, soft-soled, non-marking shoes, such as dance shoes, sneakers, or sandals, are recommended and, in some places, required. As dancing can be aerobic, dancers are sometimes encouraged to bring a change of clothes.

As in any social dance, cooperation is vital to contra dancing. Since over the course of any single dance, individuals interact with not just their partners but everyone else in the set, contra dancing might be considered a group activity. As will necessarily be the case when beginners are welcomed in by more practiced dancers, mistakes are made; most dancers are willing to help beginners in learning the steps. However, because the friendly, social nature of the dances can be misinterpreted or even abused, some groups have created anti-harassment policies.

Contra dances are arranged in long lines of couples. A pair of lines is called a "set". Sets are generally arranged so they run the length of the hall, with the "top" of the set being the end closest to the band and caller. Correspondingly, the "bottom" of the set is the end farthest from the caller.

Couples consist of two people, traditionally (but not necessarily) one male (typically referred to by the caller as the "gent", "lead", "man", "lark", or "left") and one female (typically referred to by the caller as the "lady", "follow", "woman", "raven", "robin", or "right"). Couples interact primarily with an adjacent couple for each round of the dance. Each sub-group of two interacting couples is known to choreographers as a "minor set" and to dancers as a "foursome" or "hands four". Couples in the same minor set are "neighbors". Minor sets originate at the head of the set, starting with the topmost dancers as the 1s (the "active couple" or "actives"); the other couple are "2s" (or "inactives"). The 1s are said to be "above" their neighboring 2s; 2s are "below". If there is an uneven number of couples dancing, the bottom-most couple will "wait out" the first time through the dance.

There are four common ways of arranging couples in the minor sets: "proper", "improper", "Becket", and "triple" formations. There are many additional forms a contra dance may take. Five of them are: "triplet", "indecent", "four-face-four", and "whole-set". (For diagrams and full descriptions, see Contra Dance Form main article.)

A fundamental aspect of Contra Dancing is that each dancer interacts with several different people within the span of each song. During a single dance, the same pattern is repeated over and over (one time through lasts roughly 30 seconds), but each time you and your partner will dance with new neighbors. Dancers do not need to memorize these patterns in advance, since the dance leader, or "Caller", will generally explain the pattern for each song before the music begins, and give people a chance to "walk through" the pattern so both new and experienced dancers can learn the moves. The "walk through" also helps understand how the dance pattern leads toward new people each time. Once music starts, the Caller will continue to talk on their microphone and describe each move until the dancers are comfortable with that dance pattern. The dance "progression" toward new people is built into the Contra Dance pattern as continuous motion with the music, and does not interrupt the dancing. While all dancers in the room are part of the same dance pattern, half of the couples in the room are moving toward the band/music at any moment and half are facing away from the music, so when everybody steps forward, they will find new people to dance with for the next 30-40 seconds. This effect is almost like having a checker board with red & black pieces evenly arranged across the whole board, and all red pieces gradually moving toward one side while all black pieces progressing toward the other side. Once people reach the edge of the room or "set" they may switch direction, rest/wait one 30-second cycle of the dance, and step back into the dance set to continue as long as music is still being played. 

Contra dances patterns usually organize the entire room of dancers into smaller groups of four people, or two couples. While teaching each dance pattern, the Caller may refer to the people who start closer to the band as "1s" and the people who started a few steps further from the music (but are facing toward the band) as "2s" (or the second couple). As the dance pattern "progresses" the "1s" within each group of four people will move "Down the Hall" away from the music to find new dancers, while the "2s" in each group of four will progress/move "UP the Hall" toward the Caller and the music. In the span of 10-15 minutes, you often dance with everybody in the entire set of dancers. (See the article on contra dance form for full characterizations of the progression in the eight dance forms mentioned above.) While this may sound complicated "in print" as shown above... In practice it's easier to "just do it", with the caller explaining things, and other dancers helping point the way, and the overall effect is that you enjoy dancing with one "set" of four people for 30 seconds, and then step forward and find "new people to play with" as you move across the room.

A single dance runs around ten minutes, long enough to progress at least 15-20 times. If the sets are short to medium length the caller will often try to run the dance until each couple has danced with every other couple both as a 1 and a 2 and returned to where they started. A typical room of Contra dancers may include about 120 people; but this varies from 30 people in smaller towns, to over 300 people in cities like Washington DC, Los Angeles, or New York. With longer sets (more than ≈60 people) time for one song does not allow dancing with every dancer in the group; but people do still enjoy a variety of dance personalities as they interact with most couples in the same set.

Contra dance choreography specifies the dance formation, the "figures", and the sequence of those figures in a dance. Notably, contra dance figures (with a few exceptions) do not have defined footwork; within the limits of the music and the comfort of their fellow dancers, individuals move according to their own taste.

Most contra dances consist of a sequence of about 6 to 12 individual figures, prompted by the caller in time to the music as the figures are danced. As the sequence repeats, the caller may cut down his or her prompting, and eventually drop out, leaving the dancers to each other and the music.

A figure is a pattern of movement that typically takes eight "counts", although figures with four or 16 counts are also common. Each dance is a collection of figures assembled to allow the dancers to progress along the set (see "Progression," above).

A count (as used above) is one half of a musical measure, such as one quarter note in time or three eighth notes in time. A count may also be called a "step", as contra dance is a walking form, and each count of a dance typically matches a single physical step in a figure.

Typical contra dance choreography comprises four parts, each 16 counts (8 measures) long. The parts are called A1, A2, B1 and B2. This nomenclature stems from the music: Most contra dance tunes (as written) have two parts (A and B), each 8 measures long, and each fitting one part of the dance. The A and B parts are each played twice in a row, hence, A1, A2, B1, B2. While the same music is generally played in, for example, parts A1 and A2, distinct choreography is followed in those parts. Thus, a contra dance is typically 64 "counts", and goes with a 32 "measure" tune. Tunes of this form are called "square"; tunes that deviate from this form are called "crooked".

Sample contra dances:


Many modern contra dances have these characteristics:

An event which consists primarily (or solely) of dances in this style is sometimes referred to as a Modern Urban Contra Dance.

The most common contra dance repertoire is rooted in the Anglo-Celtic tradition as it developed in North America. Irish, Scottish, French Canadian, and Old-time tunes are common, and Klezmer tunes have also been used. The old-time repertoire includes very few of the jigs common in the others.

Tunes used for a contra dance are nearly always "square" 64-beat tunes, in which one time through the tune is each of two 16-beat parts played twice (this is notated AABB). However, any 64-beat tune will do; for instance, three 8-beat parts could be played AABB AACC, or two 8-beat parts and one 16-beat part could be played AABB CC. Tunes not 64 beats long are called "crooked" and are almost never used for contra dancing, although a few crooked dances have been written as novelties. Contra tunes are played at a narrow range of tempos, between 108 and 132 bpm.

In terms of instrumentation, fiddles are considered to be the primary melody instrument in contra dancing, though other stringed instruments can also be used, such as the mandolin or banjo, in addition to a few wind instruments, for example, the accordion. The piano, guitar, and double bass are frequently found in the rhythm section of a contra dance band. Occasionally, percussion instruments are also used in contra dancing, such as the Irish bodhran or less frequently, the dumbek or washboard. The last few years have seen some of the bands incorporate the Quebecois practice of tapping feet on a board while playing an instrument (often the fiddle).

Until the 1970s it was traditional to play a single tune for the duration of a contra dance (about 5 to 10 minutes). Since then, contra dance musicians have typically played tunes in sets of two or three related (and sometimes contrasting) tunes, though single-tune dances are again becoming popular with some northeastern bands. In the Celtic repertoires it is common to change keys with each tune. A set might start with a tune in G, switch to a tune in D, and end with a tune in Bm. Here, D is related to G as its dominant (5th), while D and Bm share a key signature of two sharps. In the old-time tradition the musicians will either play the same tune for the whole dance, or switch to tunes in the same key. This is because the tunings of the five-string banjo are key-specific. An old-time band might play a set of tunes in D, then use the time between dances to retune for a set of tunes in A. (Fiddlers also may take this opportunity to retune; tune- or key-specific fiddle tunings are uncommon in American Anglo-Celtic traditions other than old-time.)

In the Celtic repertoires it is most common for bands to play sets of reels and sets of jigs. However, since the underlying beat structure of jigs and reels is the same (two "counts" per bar) bands will occasionally mix jigs and reels in a set.

In recent years, younger contra dancers have begun establishing "crossover contra" or "techno contra" — contra dancing to techno, hip-hop, and other modern forms of music. While challenging for DJs and callers, the fusion of contra patterns with moves from hip-hop, tango, and other forms of dance has made this form of contra dance a rising trend since 2008. Techno differs from other contra dancing in that it is usually done to recorded music, although there are some bands that play live for techno dances. Techno has become especially prevalent in Asheville, NC, but regular techno contra dance series are spreading up the East Coast to locales such as Charlottesville, VA, Washington, D.C., Amherst, MA, Greenfield, MA, and various North Carolina dance communities, with one-time or annual events cropping up in locations farther west, including California, Portland, OR, and Washington state. They also sometimes appear as late night events during contra dance weekends. In response to the demand for tecno contra, a number of contra dance callers have developed repertoires of recorded songs to play that go well with particular contra dances; these callers are known as DJs A kind of techno/traditional contra fusion has arisen, with at least one band, Buddy System, playing live music melded with synth sounds for techno contra dances.

Some of the most popular contra dance bands in recent years are Great Bear, Perpetual E-Motion, Buddy System, Crowfoot, Elixir, the Mean Lids, Nor'easter, Nova, Pete's Posse, the Stringrays, the Syncopaths, and Wild Asparagus.








</doc>
<doc id="5413" url="https://en.wikipedia.org/wiki?curid=5413" title="Coin collecting">
Coin collecting

Coin collecting is the collecting of coins or other forms of minted legal tender.

Coins of interest to collectors often include those that circulated for only a brief time, coins with mint errors and especially beautiful or historically significant pieces. Coin collecting can be differentiated from numismatics, in that the latter is the systematic study of currency.

A coin's grade is a main determinant of its value. For a tiered fee, a third party certification service like PCGS or NGC will grade, authenticate, attribute, and encapsulate most U.S. and foreign coins. Over 80 million coins have been certified by the four largest services.

People have hoarded coins for their bullion value for as long as coins have been minted. However, the collection of coins for their artistic value was a later development. Evidence from the archaeological and historical record of Ancient Rome and medieval Mesopotamia indicates that coins were collected and catalogued by scholars and state treasuries. It also seems probable that individual citizens collected old, exotic or commemorative coins as an affordable, portable form of art. According to Suetonius in his "De vita Caesarum" ("The Lives of the Twelve Caesars"), written in the first century CE, the emperor Augustus sometimes presented old and exotic coins to friends and courtiers during festivals and other special occasions.

Contemporary coin collecting and appreciation began around the fourteenth century. During the Renaissance, it became a fad among some members of the privileged classes, especially kings and queens. The Italian scholar and poet Petrarch is credited with being the pursuit's first and most famous aficionado. Following his lead, many European kings, princes, and other nobility kept collections of ancient coins. Some notable collectors were Pope Boniface VIII, Emperor Maximilian I of the Holy Roman Empire, Louis XIV of France, Ferdinand I, Henry IV of France and Elector Joachim II of Brandenburg, who started the Berlin Coin Cabinet (German: "Münzkabinett Berlin"). Perhaps because only the very wealthy could afford the pursuit, in Renaissance times coin collecting became known as the "Hobby of Kings."

During the 17th and 18th centuries coin collecting remained a pursuit of the well-to-do. But rational, Enlightenment thinking led to a more systematic approach to accumulation and study. Numismatics as an academic discipline emerged in these centuries at the same time as coin collecting became a leisure pursuit of a growing middle class, eager to prove their wealth and sophistication. During the 19th and 20th centuries, coin collecting increased further in popularity. The market for coins expanded to include not only antique coins, but foreign or otherwise exotic currency. Coin shows, trade associations, and regulatory bodies emerged during these decades. The first international convention for coin collectors was held 15–18 August 1962, in Detroit, Michigan, and was sponsored by the American Numismatic Association and the Royal Canadian Numismatic Association. Attendance was estimated at 40,000. As one of the oldest and most popular world pastimes, coin collecting is now often referred to as the "King of Hobbies".

The motivations for collecting vary from one person to another. Possibly the most common type of collectors are the hobbyists, who amass a collection purely for the pleasure of it with no real expectation of profit.

Another frequent reason for purchasing coins is as an investment. As with stamps, precious metals or other commodities, coin prices are periodical based on supply and demand. Prices drop for coins that are not in long-term demand, and increase along with a coin's perceived or intrinsic value. Investors buy with the expectation that the value of their purchase will increase over the long term. As with all types of investment, the principle of "caveat emptor" applies and study is recommended before buying. Likewise, as with most collectibles, a coin collection does not produce income until it is sold, and may even incur costs (for example, the cost of safe deposit box storage) in the interim.

Coin hoarders may be similar to investors in the sense that they accumulate coins for potential long-term profit. However, unlike investors, they typically do not take into account aesthetic considerations; rather they gather whatever quantity of coins they can and hold them. This is most common with coins whose metal value exceeds their spending value.

Speculators, be they amateurs or commercial buyers, generally purchase coins in bulk and often act with the expectation of short-term profit. They may wish to take advantage of a spike in demand for a particular coin (for example, during the annual release of Canadian numismatic collectibles from the Royal Canadian Mint). The speculator might hope to buy the coin in large lots and sell at profit within weeks or months. Speculators may also buy common circulation coins for their intrinsic metal value. Coins without collectible value may be melted down or distributed as bullion for commercial purposes. Typically they purchase coins that are composed of rare or precious metals, or coins that have a high purity of a specific metal.

A final type of collector is the inheritor, an accidental collector who acquires coins from another person as part of an inheritance. The inheritor type may not necessarily have an interest in or know anything about numismatics at the time of the acquisition.

Coinciding with National Coin Week (April 21–28), hundreds of the country’s leading coin dealers and collectors will be also celebrating the first Great American Coin Hunt by releasing a treasure trove of collectible vintage coins and paper money notes into circulation. This will mark the first time since the 1960s that it will be possible to find many of these coins in pocket change".

Casual coin collectors often begin the hobby by saving notable coins found by chance. These coins may be pocket change left from an international trip or an old coin found in circulation.

Usually, if the enthusiasm of the novice increases over time, random coins found in circulation are not enough to satisfy their interest. The hobbyist may then trade coins in a coin club or buy coins from dealers or mints. Their collection then takes on a more specific focus.

Some enthusiasts become "generalists" and accumulate a few examples from a broad variety of historical or geographically significant coins. Given enough resources, this can result in a vast collection. King Farouk of Egypt was a generalist with a collection famous for its scope and variety.

Most collectors decide to focus their financial resources on a narrower, "specialist" interest. Some collectors focus on coins of a certain nation or historic period. Some collect coins by themes (or 'subjects') that are featured on the artwork displayed on the coin. Others will seek error coins. Still others might focus on exonumia such as medals, tokens or challenge coins.
Some collectors are "completists" and seek an example of every type of coin within a certain category. Perhaps the most famous of these is Louis Eliasberg, the only collector thus far to assemble a complete set of known coins of the United States.

Coin collecting can become a "competitive" activity, as prompted by the recent emergence of PCGS (Professional Coin Grading Service) and NGC (Numismatic Guarantee Corporation) Registry Sets. Registry Sets are private collections of coins verified for ownership and quality by numismatic grading services. The grading services assess collections, seal the coins in clear plastic holders, then register and publish the results. This can lead to very high prices as dedicated collectors compete for the very best specimens of, for example, each date and mint mark combination.

A few common themes are often combined into a collection goal:
Collecting counterfeits and forgeries is a controversial area because of the possibility that counterfeits might someday reenter the coin market as authentic coins, but US statutory and case law do not explicitly prohibit possession of counterfeit coins.

In coin collecting, the condition of a coin (its grade) is paramount to its value; a high-quality example is often worth many times more than a poor example. Collectors have created systems to describe the overall condition of coins.

In the early days of coin collecting—before the development of a large international coin market—extremely precise grades were not needed. Coins were described using only three adjectives: "good," "fine" or "uncirculated". By the mid 20th century, with the growing market for rare coins, the American Numismatic Association helps identify most coins in North America. It uses a 1–70 numbering scale, where 70 represents a perfect specimen and 1 represents a barely identifiable coin. Descriptions and numeric grades for coins (from highest to lowest) is as follows:

In addition to the rating of coins by their wear, Proof coinage occurs as a separate category. These are specimens struck from polished dies and are often packaged and sold by mints. This is frequently done for Commemorative coins, though annual proof sets of circulating coinage may be issued as well. Unless mishandled, they will stay in Mint State. Collectors often desire both the proof and regular ("business strike") issues of a coin, though the difference in price between the two may be significant. Additionally, proof coins follow the same 1–70 grading scale as business strike coins, though they use the PR or PF prefix.

Coin experts in Europe and elsewhere often shun the numerical system, preferring to rate specimens on a purely descriptive, or adjectival, scale. Nevertheless, most grading systems use similar terminology and values and remain mutually intelligible.
When evaluating a coin, the following—often subjective—factors may be considered:

Damage of any sort (e.g., holes, edge dents, repairs, cleaning, re-engraving or gouges) can substantially reduce the value of a coin. Specimens are occasionally cleaned or polished in an attempt to pass them off as higher grades or as uncirculated strikes. Because of the substantially lower prices for cleaned or damaged coins, some enthusiasts specialize in their collection.

Third-party grading (TPG), aka "coin certification services", emerged in the 1980s with the goals of standardizing grading, exposing alterations, and eliminating counterfeits. For tiered fees, certification services grade, authenticate, attribute, and encapsulate coins in clear, plastic holders. There are four coin certification services which eBay, the largest coin marketplace, deems acceptable to include in its listings: Professional Coin Grading Service (PCGS), Numismatic Guaranty Corporation (NGC), Independent Coin Graders (ICG), and ANACS. Experts consider these to be the most credible and popular services. Together they have certified over 80 million coins.

In 2007, the rare coin industry's leading dealer association, the Professional Numismatists Guild (PNG), released the results of a survey of major coin dealers who gave their professional opinions about 11 certification services. PCGS and NGC were rated "Superior" overall, with ANACS and ICG deemed "Good". PCI and SEGS were listed as "Poor", while called "Unacceptable" were Accugrade (ACG), Numistrust Corporation (NTC), Hallmark Coin Grading Service (HCGS), American Coin Club Grading Service (ACCGS), and Star Grading Services (SGS).

Certified Acceptance Corporation (CAC) is a Far Hills, New Jersey coin certification company started in 2007 by coin dealer John Albanese. The firm evaluates most numismatically valuable U.S. coins already certified by NGC or PCGS. Coins which CAC deems high-end for their grades receive green stickers. Coins which are at least high end for the next grade up are bestowed gold stickers. CAC-certified coins usually trade for a premium. CAC buys and sells CAC-certified coins via their affiliated trading network, Coinplex.

Coin certification has greatly reduced the number of counterfeits and grossly overgraded coins, and improved buyer confidence. Certification services can sometimes be controversial because grading is subjective; coins may be graded differently by different services or even upon resubmission to the same service. The numeric grade alone does not represent all of a coin's characteristics, such as toning, strike, brightness, color, luster, and attractiveness. Due to potentially large differences in value over slight differences in a coin's condition, some submitters will repeatedly resubmit a coin to a grading service in the hope of receiving a higher grade. Because fees are charged for certification, submitters must funnel money away from purchasing additional coins.

Coin collector clubs offer variety of benefits to members. They usually serve as a source of information and unification of people interested in coins. Collector clubs are popular both offline and online.




</doc>
<doc id="5415" url="https://en.wikipedia.org/wiki?curid=5415" title="Crokinole">
Crokinole

Crokinole ( ) is a dexterity board game similar in various ways to pitchnut, carrom, marbles, shove ha'penny, knipsbrat, with elements of shuffleboard and curling reduced to table-top size. Players take turns shooting discs across the circular playing surface, trying to have their discs land in the higher-scoring regions of the board, while also attempting to knock away opposing discs.

Board dimensions vary with a playing surface typically of polished wood or laminate approximately in diameter. The arrangement is 3 concentric rings worth 5, 10, and 15 points as you move in from the outside. There is a shallow 20-point hole at the center. The inner 15-point ring is guarded with 8 small bumpers or posts. The outer ring of the board is divided into four quadrants. The outer edge of the board is raised slightly to keep errant shots from flying out, with a gutter between the playing surface and the edge to collect discarded pieces. Crokinole boards are typically octagonal or round in shape. The discs are roughly checker-sized, slightly smaller in diameter than the board's central hole, and may have convex faces to reduce sliding friction. Alternatively, the game may be played with ring-shaped pieces with a central hole.

The use of any lubricating powder in crokinole is controversial, with some purists reviling the practice.

Powder is used to ensure pieces slide smoothly on the surface. According to carrom rules, the powder must be of high quality to keep the surface smooth and dry, and shall not be wet. Pouches and containers are used to spread the powder over the playing surface. There must be no impurity in the powder. Boric acid powder is mostly used for this purpose.

In the UK, many players use a version of anti-set-off spray powder, from the printing industry, which has specific electrostatic properties, with particles of 50-micrometre diameter (). The powder is made of pure food-grade plant/vegetable starch.

Crokinole is most commonly played by two players, or by four players in teams of two, with partners sitting across the board from each other. Players take turns flicking their discs from the outer edge of their quadrant of the board onto the playfield. Shooting is usually done by flicking the disc with a finger, though sometimes small cue sticks may be used. If there are any enemy discs on the board, a player must make contact, directly or indirectly, with an enemy disc during the shot. If unsuccessful, the shot disc is "fouled" and removed from the board, along with any of the player's other discs that were moved during the shot.

When there are no enemy discs on the board, many (but not all) rules also state that a player must shoot for the centre of the board, and a shot disc must finish either completely inside the 15-point guarded ring line, or (depending on the specifics of the rules) be inside or touching this line. This is often called the "no hiding" rule, since it prevents players from placing their first shots where their opponent must traverse completely though the guarded centre ring to hit them and avoid fouling. When playing without this rule, a player may generally make any shot desired, and as long as a disc remains completely inside the outer line of the playfield, it remains on the board. During any shot, any disc that falls completely into the recessed central "20" hole (a.k.a. the "Toad" or "Dukie") is removed from play, and counts as twenty points for the owner of the disc at the end of the round, assuming the shot is valid.

Scoring occurs after all pieces (generally 12 per player or team) have been played, and is differential: i.e., the player or team with higher score is awarded the difference between the higher and lower scores for the round, thus only one team or player each round gains points. Play continues until a predetermined winning score is reached.

The earliest known crokinole board was made by craftsman Eckhardt Wettlaufer in 1876 in Perth County, Ontario, Canada. It is said Wettlaufer crafted the board as a fifth birthday present for his son Adam, which is now part of the collection at the Joseph Schneider Haus, a national historic site in Kitchener, Ontario, with a focus on Germanic folk art. Several other home-made boards dating from southwestern Ontario in the 1870s have been discovered since the 1990s. A board game similar to crokinole was patented on 20 April 1880 by Joshua K. Ingalls (US Patent No. 226,615)

Crokinole is often believed to be of Mennonite or Amish origins, but there is no factual data to support such a claim. The reason for this misconception may be due to its popularity in Mennonite and Amish groups. The game was viewed as a rather innocuous pastime – unlike the perception that diversions such as card playing or dancing were considered "works of the Devil" as held by many 19th-century Protestant groups.
The oldest roots of crokinole, from the 1860s, suggest the British and South Asian games are the most likely antecedents of what became crokinole.

In 2006, a documentary film called "Crokinole" was released. The world premiere occurred at the Princess Cinema in Waterloo, Ontario, in early 2006. The movie follows some of the competitors of the 2004 World Crokinole Championship as they prepare for the event.

The name "crokinole" derives from , a French word today designating:
It also used to designate the action of flicking with the finger (Molière, "Le malade imaginaire"; or Voltaire, "Lettre à Frédéric II Roi de Prusse"; etc.), and this seems the most likely origin of the name of the game. was also a synonym of , a word that gave its name to the different but related games of pichenotte and pitchnut.

Crokinole is called ('flick-board') in the Low German spoken by Mennonites.

The World Crokinole Championship (WCC) tournament has been held annually since 1999 on the first Saturday of June in Tavistock, Ontario. Tavistock was chosen as the host city because it was the home of Eckhardt Wettlaufer, the maker of the earliest known board. The tournament has seen registration from every Canadian province, several American states, Germany, Australia, Spain and the UK.

The reigning world adult singles crokinole champion is Michael Davidson from Halifax, Nova Scotia. The reigning world adult doubles champions are Jason Beierling of Kitchener, Ontario, and Ray Beierling of Dorchester, Ontario.

The WCC singles competition begins with a qualifying round in which competitors play 10 matches against randomly assigned opponents. The qualifying round is played in a large randomly determined competition. At the end of the opening round, the top 16 competitors move on to the playoffs. The top four in the playoffs advance to a final round robin to play each other, and the top two compete in the finals. The WCC doubles competition begins with a qualifying round of 8 matches against randomly assigned opponents with the top six teams advancing to a playoff round robin to determine the champions.

The WCC has multiple divisions, including a singles finger-shooting category for competitive players (adult singles), novices (recreational), and younger players (intermediate, 11–14 yrs; junior, 6–10 yrs), as well as a division for cue-shooters (cues singles). The WCC also awards a prize for the top 20-hole shooter in the qualifying round of competitive singles, recreational singles, cues singles, intermediate singles, and in the junior singles. The tournament also holds doubles divisions for competitive fingers-shooting (competitive doubles), novices (recreational doubles), younger players (youth doubles, 6–16yrs), and cues-shooting (cues doubles).

The National Crokinole Association (NCA) is a federation that supports existing, and the development of new, crokinole clubs and tournaments. While the majority of NCA events are based in Ontario, Canada, the NCA has held sanctioned events in the Canadian provinces of PEI and BC, as well as in New York State.

The collection of NCA tournaments is referred to as the NCA Tour. Each NCA Tour season begins at the Tavistock World Crokinole Championships in June, and concludes at the Ontario Singles Crokinole Championship in May of the following years. The results of each tournament award points for each player, as they compete for their season-ending ranking classification. The NCA Tour includes both doubles and singles events.

The 2017-2018 NCA Tour Champion was Justin Slater, followed by Nathan Walsh in 2nd and Jon Conrad in 3rd.



</doc>
<doc id="5416" url="https://en.wikipedia.org/wiki?curid=5416" title="Capitalism">
Capitalism

Capitalism is an economic system based on the private ownership of the means of production and their operation for profit. Characteristics central to capitalism include private property, capital accumulation, wage labor, voluntary exchange, a price system and competitive markets. In a capitalist market economy, decision-making and investments are determined by every owner of wealth, property or production ability in financial and capital markets, whereas prices and the distribution of goods and services are mainly determined by competition in goods and services markets.

Economists, political economists, sociologists and historians have adopted different perspectives in their analyses of capitalism and have recognized various forms of it in practice. These include "laissez-faire" or free-market capitalism, welfare capitalism and state capitalism. Different forms of capitalism feature varying degrees of free markets, public ownership, obstacles to free competition and state-sanctioned social policies. The degree of competition in markets, the role of intervention and regulation, and the scope of state ownership vary across different models of capitalism. The extent to which different markets are free as well as the rules defining private property are matters of politics and policy. Most existing capitalist economies are mixed economies which combine elements of free markets with state intervention and in some cases economic planning.

Market economies have existed under many forms of government and in many different times, places and cultures. Modern capitalist societies—marked by a universalization of money-based social relations, a consistently large and system-wide class of workers who must work for wages, and a capitalist class which owns the means of production—developed in Western Europe in a process that led to the Industrial Revolution. Capitalist systems with varying degrees of direct government intervention have since become dominant in the Western world and continue to spread. Over time, capitalist countries have experienced consistent economic growth and an increase in the standard of living.

Critics of capitalism argue that it establishes power in the hands of a minority capitalist class that exists through the exploitation of the majority working class and their labor; it prioritizes profit over social good, natural resources and the environment; and it is an engine of inequality, corruption and economic instabilities. Supporters argue that it provides better products and innovation through competition, disperses wealth to all productive people, promotes pluralism and decentralization of power, creates strong economic growth and yields productivity and prosperity that greatly benefit society.

The term "capitalist", meaning an owner of capital, appears earlier than the term "capitalism" and dates to the mid-17th century. "Capitalism" is derived from "capital", which evolved from "capitale", a late Latin word based on "caput", meaning "head"—which is also the origin of "chattel" and "cattle" in the sense of movable property (only much later to refer only to livestock). "Capitale" emerged in the 12th to 13th centuries to refer to funds, stock of merchandise, sum of money or money carrying interest. By 1283, it was used in the sense of the capital assets of a trading firm and was often interchanged with other words—wealth, money, funds, goods, assets, property and so on.

The "Hollandische Mercurius" uses "capitalists" in 1633 and 1654 to refer to owners of capital. In French, Étienne Clavier referred to "capitalistes" in 1788, six years before its first recorded English usage by Arthur Young in his work "Travels in France" (1792). In his "Principles of Political Economy and Taxation" (1817), David Ricardo referred to "the capitalist" many times. English poet Samuel Taylor Coleridge used "capitalist" in his work "Table Talk" (1823). Pierre-Joseph Proudhon used the term in his first work, "What is Property?" (1840), to refer to the owners of capital. Benjamin Disraeli used the term in his 1845 work "Sybil".

The initial use of the term "capitalism" in its modern sense is attributed to Louis Blanc in 1850 ("What I call 'capitalism' that is to say the appropriation of capital by some to the exclusion of others") and Pierre-Joseph Proudhon in 1861 ("Economic and social regime in which capital, the source of income, does not generally belong to those who make it work through their labor"). Karl Marx and Friedrich Engels referred to the "capitalistic system" and to the "capitalist mode of production" in "" (1867). The use of the word "capitalism" in reference to an economic system appears twice in Volume I of "Capital", p. 124 (German Edition) and in "Theories of Surplus Value", tome II, p. 493 (German Edition). Marx did not extensively use the form "capitalism", but instead those of capitalist and capitalist mode of production"," which appear more than 2,600 times in the trilogy The "Capital." According to the "Oxford English Dictionary" (OED), the term "capitalism" first appeared in English in 1854 in the novel "The Newcomes" by novelist William Makepeace Thackeray, where he meant "having ownership of capital". Also according to the OED, Carl Adolph Douai, a German American socialist and abolitionist, used the phrase "private capitalism" in 1863.

Capitalism in its modern form can be traced to the emergence of agrarian capitalism and mercantilism in the early Renaissance, in city-states like Florence.
Capital has existed incipiently on a small scale for centuries in the form of merchant, renting and lending activities and occasionally as small-scale industry with some wage labour. Simple commodity exchange and consequently simple commodity production, which is the initial basis for the growth of capital from trade, have a very long history. Arabs promulgated capitalist economic policies such as free trade and banking. Their use of Indo-Arabic numerals facilitated bookkeeping. These innovations migrated to Europe through trade partners in cities such as Venice and Pisa. The Italian mathematician Fibonacci traveled the Mediterranean talking to Arab traders and returned to popularize the use of Indo-Arabic numerals in Europe.

Capital and commercial trade thus existed for much of history, but it did not lead to industrialization or dominate the production process of society. That required a set of conditions, including specific technologies of mass production, the ability to independently and privately own and trade in means of production, a class of workers willing to sell their labor power for a living, a legal framework promoting commerce, a physical infrastructure allowing the circulation of goods on a large scale and security for private accumulation. Many of these conditions do not currently exist in many Third World countries, although there is plenty of capital and labor. The obstacles for the development of capitalist markets are therefore less technical and more social, cultural and political.

The economic foundations of the feudal agricultural system began to shift substantially in 16th-century England as the manorial system had broken down and land began to become concentrated in the hands of fewer landlords with increasingly large estates. Instead of a serf-based system of labor, workers were increasingly employed as part of a broader and expanding money-based economy. The system put pressure on both landlords and tenants to increase the productivity of agriculture to make profit; the weakened coercive power of the aristocracy to extract peasant surpluses encouraged them to try better methods, and the tenants also had incentive to improve their methods in order to flourish in a competitive labor market. Terms of rent for land were becoming subject to economic market forces rather than to the previous stagnant system of custom and feudal obligation.

By the early 17th century, England was a centralized state in which much of the feudal order of Medieval Europe had been swept away. This centralization was strengthened by a good system of roads and by a disproportionately large capital city, London. The capital acted as a central market hub for the entire country, creating a very large internal market for goods, contrasting with the fragmented feudal holdings that prevailed in most parts of the Continent.

The economic doctrine prevailing from the 16th to the 18th centuries is commonly called mercantilism. This period, the Age of Discovery, was associated with the geographic exploration of the foreign lands by merchant traders, especially from England and the Low Countries. Mercantilism was a system of trade for profit, although commodities were still largely produced by non-capitalist methods. Most scholars consider the era of merchant capitalism and mercantilism as the origin of modern capitalism, although Karl Polanyi argued that the hallmark of capitalism is the establishment of generalized markets for what he called the "fictitious commodities", i.e. land, labor and money. Accordingly, he argued that "not until 1834 was a competitive labor market established in England, hence industrial capitalism as a social system cannot be said to have existed before that date".
England began a large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). A systematic and coherent explanation of balance of trade was made public through Thomas Mun's argument "England's Treasure by Forraign Trade, or the Balance of our Forraign Trade is The Rule of Our Treasure." It was written in the 1620s and published in 1664.

European merchants, backed by state controls, subsidies and monopolies, made most of their profits by buying and selling goods. In the words of Francis Bacon, the purpose of mercantilism was "the opening and well-balancing of trade; the cherishing of manufacturers; the banishing of idleness; the repressing of waste and excess by sumptuary laws; the improvement and husbanding of the soil; the regulation of prices...".

After the period of the proto-industrialization, the British East India Company and the Dutch East India Company, after massive contributions from the Mughal Bengal, inaugurated an expansive era of commerce and trade. These companies were characterized by their colonial and expansionary powers given to them by nation-states. During this era, merchants, who had traded under the previous stage of mercantilism, invested capital in the East India Companies and other colonies, seeking a return on investment.

In the mid-18th century a group of economic theorists, led by David Hume (1711–1776) and Adam Smith (1723–1790), challenged fundamental mercantilist doctrines - such as the belief that the world's wealth remained constant and that a state could only increase its wealth at the expense of another state.

During the Industrial Revolution, industrialists replaced merchants as a dominant factor in the capitalist system and affected the decline of the traditional handicraft skills of artisans, guilds and journeymen. Also during this period, the surplus generated by the rise of commercial agriculture encouraged increased mechanization of agriculture. Industrial capitalism marked the development of the factory system of manufacturing, characterized by a complex division of labor between and within work process and the routine of work tasks; and eventually established the domination of the capitalist mode of production.

Industrial Britain eventually abandoned the protectionist policy formerly prescribed by mercantilism. In the 19th century, Richard Cobden (1804–1865) and John Bright (1811–1889), who based their beliefs on the Manchester School, initiated a movement to lower tariffs. In the 1840s Britain adopted a less protectionist policy, with the 1846 repeal of the Corn Laws and the 1849 repeal of the Navigation Acts. Britain reduced tariffs and quotas, in line with David Ricardo's advocacy of free trade.

Capitalism was carried across the world by broader processes of globalization and by the beginning of the nineteenth century a series of loosely connected market systems had come together as a relatively integrated global system, in turn intensifying processes of economic and other globalization. Later in the 20th century, capitalism overcame a challenge by centrally-planned economies and is now the encompassing system worldwide, with the mixed economy being its dominant form in the industrialized Western world.

Industrialization allowed cheap production of household items using economies of scale while rapid population growth created sustained demand for commodities. Globalization in this period was decisively shaped by 18th-century imperialism.

After the First and Second Opium Wars and the completion of the British conquest of India, vast populations of these regions became ready consumers of European exports. Also in this period, areas of sub-Saharan Africa and the Pacific islands were colonized. The conquest of new parts of the globe, notably sub-Saharan Africa, by Europeans yielded valuable natural resources such as rubber, diamonds and coal and helped fuel trade and investment between the European imperial powers, their colonies and the United States: The inhabitant of London could order by telephone, sipping his morning tea, the various products of the whole earth, and reasonably expect their early delivery upon his doorstep. Militarism and imperialism of racial and cultural rivalries were little more than the amusements of his daily newspaper. What an extraordinary episode in the economic progress of man was that age which came to an end in August 1914.

In this period, the global financial system was mainly tied to the gold standard. The United Kingdom first formally adopted this standard in 1821. Soon to follow were Canada in 1853, Newfoundland in 1865, the United States and Germany ("de jure") in 1873. New technologies, such as the telegraph, the transatlantic cable, the radiotelephone, the steamship and railway allowed goods and information to move around the world at an unprecedented degree.
In the period following the global depression of the 1930s, the state played an increasingly prominent role in the capitalistic system throughout much of the world. The postwar boom ended in the late 1960s and early 1970s and the situation was worsened by the rise of stagflation. Monetarism, a modification of Keynesianism that is more compatible with "laissez-faire", gained increasing prominence in the capitalist world, especially under the leadership of Ronald Reagan in the United States and Margaret Thatcher in the United Kingdom in the 1980s. Public and political interest began shifting away from the so-called collectivist concerns of Keynes's managed capitalism to a focus on individual choice, called "remarketized capitalism".

According to Harvard academic Shoshana Zuboff, a new genus of capitalism, surveillance capitalism, monetizes data acquired through surveillance. She states it was first discovered and consolidated at Google, emerged due to the "coupling of the vast powers of the digital with the radical indifference and intrinsic narcissism of the financial capitalism and its neoliberal vision that have dominated commerce for at least three decades, especially in the Anglo economies" and depends on the global architecture of computer mediation which produces a distributed and largely uncontested new expression of power she calls "Big Other".

The relationship between democracy and capitalism is a contentious area in theory and in popular political movements. The extension of universal adult male suffrage in 19th-century Britain occurred along with the development of industrial capitalism and democracy became widespread at the same time as capitalism, leading capitalists to posit a causal or mutual relationship between them. However, according to some authors in the 20th-century, capitalism also accompanied a variety of political formations quite distinct from liberal democracies, including fascist regimes, absolute monarchies and single-party states. Democratic peace theory asserts that democracies seldom fight other democracies, but critics of that theory suggest that this may be because of political similarity or stability rather than because they are democratic or capitalist. Moderate critics argue that though economic growth under capitalism has led to democracy in the past, it may not do so in the future as authoritarian regimes have been able to manage economic growth using some of capitalism's competitive principles without making concessions to greater political freedom.

Milton Friedman, one of the biggest supporters of the idea that capitalism promotes political freedom, argued that competitive capitalism allows economic and political power to be separate, ensuring that they do not clash with one another. Moderate critics have recently challenged this, stating that the current influence lobbying groups have had on policy in the United States is a contradiction, given the approval of Citizens United. This has led people to question the idea that competitive capitalism promotes political freedom. The ruling on Citizens United allows corporations to spend undisclosed and unregulated amounts of money on political campaigns, shifting outcomes to the interests and undermining true democracy. As explained in Robin Hahnel’s writings, the centerpiece of the ideological defense of the free market system is the concept of economic freedom and that supporters equate economic democracy with economic freedom and claim that only the free market system can provide economic freedom. According to Hahnel, there are a few objections to the premise that capitalism offers freedom through economic freedom. These objections are guided by critical questions about who or what decides whose freedoms are more protected. Often, the question of inequality is brought up when discussing how well capitalism promotes democracy. An argument that could stand is that economic growth can lead to inequality given that capital can be acquired at different rates by different people. In "Capital in the Twenty-First Century", Thomas Piketty of the Paris School of Economics asserts that inequality is the inevitable consequence of economic growth in a capitalist economy and the resulting concentration of wealth can destabilize democratic societies and undermine the ideals of social justice upon which they are built.

States with capitalistic economic systems have thrived under political regimes deemed to be authoritarian or oppressive. Singapore has a successful open market economy as a result of its competitive, business-friendly climate and robust rule of law. Nonetheless, it often comes under fire for its brand of government which though democratic and consistently one of the least corrupt it also operates largely under a one-party rule and does not vigorously defend freedom of expression given its government-regulated press as well as penchant for upholding laws protecting ethnic and religious harmony, judicial dignity and personal reputation. The private (capitalist) sector in the People's Republic of China has grown exponentially and thrived since its inception, despite having an authoritarian government. Augusto Pinochet's rule in Chile led to economic growth and high levels of inequality by using authoritarian means to create a safe environment for investment and capitalism. Similarly, Suharto's authoritarian reign and extirpation of the Communist Party of Indonesia allowed for the expansion of capitalism in Indonesia.

In general, capitalism as an economic system and mode of production can be summarised by the following:

In free market and "laissez-faire" forms of capitalism, markets are used most extensively with minimal or no regulation over the pricing mechanism. In mixed economies, which are almost universal today, markets continue to play a dominant role, but they are regulated to some extent by the state in order to correct market failures, promote social welfare, conserve natural resources, fund defense and public safety or other rationale. In state capitalist systems, markets are relied upon the least, with the state relying heavily on state-owned enterprises or indirect economic planning to accumulate capital.

Supply is the amount of a good or service that is available for purchase or sale. Demand is the measure of value for a good that people are willing to buy at a given time. Prices tend to rise when demand for an available resource increases or its supply diminishes and fall with demand or when supply increases.

Competition arises when more than one producer is trying to sell the same or similar products to the same buyers. In capitalist theory, competition leads to innovation and more affordable prices. Without competition, a monopoly or cartel may develop. A monopoly occurs when a firm is granted exclusivity over a market. Hence the firm can engage in rent seeking behaviors such as limiting output and raising prices because it has no fear of competition. A cartel is a group of firms that act together in a monopolistic manner to control output and prices.

Governments have implemented legislation for the purpose of preventing the creation of monopolies and cartels. In 1890, the Sherman Antitrust Act became the first legislation passed by the United States Congress to limit monopolies.

The profit motive, in the theory in capitalism, is the desire to earn income in the form of profit. Stated differently, the reason for a business's existence is to turn a profit. The profit motive functions according to rational choice theory, or the theory that individuals tend to pursue what is in their own best interests. Accordingly, businesses seek to benefit themselves and/or their shareholders by maximizing profit.

In capitalist theoretics, the profit motive is said to ensure that resources are being allocated efficiently. For instance, Austrian economist Henry Hazlitt explains: "If there is no profit in making an article, it is a sign that the labor and capital devoted to its production are misdirected: the value of the resources that must be used up in making the article is greater than the value of the article itself". In other words, profits let companies know whether an item is worth producing. Theoretically, in free and competitive markets maximising profit ensures that resources are not wasted.

The relationship between the state, its formal mechanisms and capitalist societies has been debated in many fields of social and political theory, with active discussion since the 19th century. Hernando de Soto is a contemporary Peruvian economist who has argued that an important characteristic of capitalism is the functioning state protection of property rights in a formal property system where ownership and transactions are clearly recorded.

According to de Soto, this is the process by which physical assets are transformed into capital, which in turn may be used in many more ways and much more efficiently in the market economy. A number of Marxian economists have argued that the Enclosure Acts in England and similar legislation elsewhere were an integral part of capitalist primitive accumulation and that specific legal frameworks of private land ownership have been integral to the development of capitalism.

In capitalist economics, market competition is the rivalry among sellers trying to achieve such goals as increasing profits, market share and sales volume by varying the elements of the marketing mix: price, product, distribution and promotion. Merriam-Webster defines competition in business as "the effort of two or more parties acting independently to secure the business of a third party by offering the most favourable terms". It was described by Adam Smith in "The Wealth of Nations" (1776) and later economists as allocating productive resources to their most highly valued uses and encouraging efficiency. Smith and other classical economists before Antoine Augustine Cournot were referring to price and non-price rivalry among producers to sell their goods on best terms by bidding of buyers, not necessarily to a large number of sellers nor to a market in final equilibrium. Competition is widespread throughout the market process. It is a condition where "buyers tend to compete with other buyers, and sellers tend to compete with other sellers". In offering goods for exchange, buyers competitively bid to purchase specific quantities of specific goods which are available, or might be available if sellers were to choose to offer such goods. Similarly, sellers bid against other sellers in offering goods on the market, competing for the attention and exchange resources of buyers. Competition results from scarcity—there is never enough to satisfy all conceivable human wants—and occurs "when people strive to meet the criteria that are being used to determine who gets what".

Historically, capitalism has an ability to promote economic growth as measured by gross domestic product (GDP), capacity utilization or standard of living. This argument was central, for example, to Adam Smith's advocacy of letting a free market control production and price and allocate resources. Many theorists have noted that this increase in global GDP over time coincides with the emergence of the modern world capitalist system.

Between 1000 and 1820, the world economy grew sixfold, a faster rate than the population growth, so individuals enjoyed, on average, a 50% increase in income. Between 1820 and 1998, world economy grew 50-fold, a much faster rate than the population growth, so individuals enjoyed on average a 9-fold increase in income. Over this period, in Europe, North America and Australasia the economy grew 19-fold per person, even though these regions already had a higher starting level; and in Japan, which was poor in 1820, the increase per person was 31-fold. In the Third World, there was an increase, but only 5-fold per person.

The capitalist mode of production refers to the systems of organising production and distribution within capitalist societies. Private money-making in various forms (renting, banking, merchant trade, production for profit and so on) preceded the development of the capitalist mode of production as such. The capitalist mode of production proper based on wage-labour and private ownership of the means of production and on industrial technology began to grow rapidly in Western Europe from the Industrial Revolution, later extending to most of the world.

The term capitalist mode of production is defined by private ownership of the means of production, extraction of surplus value by the owning class for the purpose of capital accumulation, wage-based labour and at least as far as commodities are concerned being market-based.

Capitalism in the form of money-making activity has existed in the shape of merchants and money-lenders who acted as intermediaries between consumers and producers engaging in simple commodity production (hence the reference to "merchant capitalism") since the beginnings of civilisation. What is specific about the "capitalist mode of production" is that most of the inputs and outputs of production are supplied through the market (i.e. they are commodities) and essentially all production is in this mode. For example, in flourishing feudalism most or all of the factors of production including labour are owned by the feudal ruling class outright and the products may also be consumed without a market of any kind, it is production for use within the feudal social unit and for limited trade. This has the important consequence that the whole organisation of the production process is reshaped and re-organised to conform with economic rationality as bounded by capitalism, which is expressed in price relationships between inputs and outputs (wages, non-labour factor costs, sales and profits) rather than the larger rational context faced by society overall—that is, the whole process is organised and re-shaped in order to conform to "commercial logic". Essentially, capital accumulation comes to define economic rationality in capitalist production.

A society, region or nation is capitalist if the predominant source of incomes and products being distributed is capitalist activity, but even so this does not yet mean necessarily that the capitalist mode of production is dominant in that society.

In capitalist economic structures, supply and demand is an economic model of price determination in a market. It concludes that in a competitive market, the unit price for a particular good will vary until it settles at a point where the quantity demanded by consumers (at the current price) will equal the quantity supplied by producers (at the current price), resulting in an economic equilibrium for price and quantity.

The four basic laws of supply and demand are:

Although it is normal to regard the quantity demanded and the quantity supplied as functions of the price of the goods, the standard graphical representation, usually attributed to Alfred Marshall, has price on the vertical axis and quantity on the horizontal axis, the opposite of the standard convention for the representation of a mathematical function.

Since determinants of supply and demand other than the price of the goods in question are not explicitly represented in the supply-demand diagram, changes in the values of these variables are represented by moving the supply and demand curves (often described as "shifts" in the curves). By contrast, responses to changes in the price of the good are represented as movements along unchanged supply and demand curves.

A supply schedule is a table that shows the relationship between the price of a good and the quantity supplied. Under the assumption of perfect competition, supply is determined by marginal cost. That is: firms will produce additional output while the cost of producing an extra unit of output is less than the price they would receive.

A hike in the cost of raw goods would decrease supply and shifting costs up while a discount would increase supply, shifting costs down and hurting producers as producer surplus decreases.

By its very nature, conceptualising a supply curve requires the firm to be a perfect competitor (i.e. to have no influence over the market price). This is true because each point on the supply curve is the answer to the question "If this firm is "faced with" this potential price, how much output will it be able to and willing to sell?". If a firm has market power, its decision of how much output to provide to the market influences the market price, therefore the firm is not "faced with" any price and the question becomes less relevant.

Economists distinguish between the supply curve of an individual firm and the market supply curve. The market supply curve is obtained by summing the quantities supplied by all suppliers at each potential price, thus in the graph of the supply curve individual firms' supply curves are added horizontally to obtain the market supply curve.

Economists also distinguish the short-run market supply curve from the long-run market supply curve. In this context, two things are assumed constant by definition of the short run: the availability of one or more fixed inputs (typically physical capital) and the number of firms in the industry. In the long-run, firms can adjust their holdings of physical capital, enabling them to better adjust their quantity supplied at any given price. Furthermore, in the long-run potential competitors can enter or exit the industry in response to market conditions. For both of these reasons, long-run market supply curves are generally flatter than their short-run counterparts.

The determinants of supply are:

A demand schedule, depicted graphically as the demand curve, represents the amount of some goods that buyers are willing and able to purchase at various prices, assuming all determinants of demand other than the price of the good in question, such as income, tastes and preferences, the price of substitute goods and the price of complementary goods, remain the same. According to the law of demand, the demand curve is almost always represented as downward-sloping, meaning that as price decreases, consumers will buy more of the good.

Just like the supply curves reflect marginal cost curves, demand curves are determined by marginal utility curves. Consumers will be willing to buy a given quantity of a good at a given price, if the marginal utility of additional consumption is equal to the opportunity cost determined by the price—that is, the marginal utility of alternative consumption choices. The demand schedule is defined as the willingness and ability of a consumer to purchase a given product in a given frame of time.

While the aforementioned demand curve is generally downward-sloping, there may be rare examples of goods that have upward-sloping demand curves. Two different hypothetical types of goods with upward-sloping demand curves are Giffen goods (an inferior, but staple good) and Veblen goods (goods made more fashionable by a higher price).

By its very nature, conceptualising a demand curve requires that the purchaser be a perfect competitor—that is, that the purchaser has no influence over the market price. This is true because each point on the demand curve is the answer to the question "If this buyer is faced with this potential price, how much of the product will it purchase?". If a buyer has market power, so its decision of how much to buy influences the market price, then the buyer is not "faced with" any price and the question is meaningless.

Like with supply curves, economists distinguish between the demand curve of an individual and the market demand curve. The market demand curve is obtained by summing the quantities demanded by all consumers at each potential price, thus in the graph of the demand curve individuals' demand curves are added horizontally to obtain the market demand curve.

The determinants of demand are:

In the context of supply and demand, economic equilibrium refers to a state where economic forces such as supply and demand are balanced and in the absence of external influences the (equilibrium) values of economic variables will not change. For example, in the standard text-book model of perfect competition equilibrium occurs at the point at which quantity demanded and quantity supplied are equal. Market equilibrium in this case refers to a condition where a market price is established through competition such that the amount of goods or services sought by buyers is equal to the amount of goods or services produced by sellers. This price is often called the competitive price or market clearing price and will tend not to change unless demand or supply changes and the quantity is called "competitive quantity" or market clearing quantity.

Partial equilibrium, as the name suggests, takes into consideration only a part of the market to attain equilibrium.

Jain proposes (attributed to George Stigler): "A partial equilibrium is one which is based on only a restricted range of data, a standard example is price of a single product, the prices of all other products being held fixed during the analysis".

The supply and demand model is a partial equilibrium model of economic equilibrium, where the clearance on the market of some specific goods is obtained independently from prices and quantities in other markets. In other words, the prices of all substitutes and complements as well as income levels of consumers are constant. This makes analysis much simpler than in a general equilibrium model which includes an entire economy.

Here the dynamic process is that prices adjust until supply equals demand. It is a powerfully simple technique that allows one to study equilibrium, efficiency and comparative statics. The stringency of the simplifying assumptions inherent in this approach make the model considerably more tractable, but it may produce results which while seemingly precise do not effectively model real world economic phenomena.

Partial equilibrium analysis examines the effects of policy action in creating equilibrium only in that particular sector or market which is directly affected, ignoring its effect in any other market or industry assuming that they being small will have little impact if any.

Hence this analysis is considered to be useful in constricted markets.

Léon Walras first formalised the idea of a one-period economic equilibrium of the general economic system, but it was French economist Antoine Augustin Cournot and English political economist Alfred Marshall who developed tractable models to analyse an economic system.

Demand and supply relations in a market can be statistically estimated from price, quantity and other data with sufficient information in the model. This can be done with simultaneous-equation methods of estimation in econometrics. Such methods allow solving for the model-relevant "structural coefficients", the estimated algebraic counterparts of the theory. The parameter identification problem is a common issue in "structural estimation". Typically, data on exogenous variables (that is, variables other than price and quantity, both of which are endogenous variables) are needed to perform such an estimation. An alternative to "structural estimation" is reduced-form estimation, which regresses each of the endogenous variables on the respective exogenous variables.

Demand and supply have also been generalised to explain macroeconomic variables in a market economy, including the quantity of total output and the general price level. The Aggregate Demand–Aggregate Supply model may be the most direct application of supply and demand to macroeconomics, but other macroeconomic models also use supply and demand. Compared to microeconomic uses of demand and supply, different (and more controversial) theoretical considerations apply to such macroeconomic counterparts as aggregate demand and aggregate supply. Demand and supply are also used in macroeconomic theory to relate money supply and money demand to interest rates and to relate labor supply and labor demand to wage rates.

According to Hamid S. Hosseini, the power of supply and demand was understood to some extent by several early Muslim scholars, such as fourteenth-century Mamluk scholar Ibn Taymiyyah, who wrote: "If desire for goods increases while its availability decreases, its price rises. On the other hand, if availability of the good increases and the desire for it decreases, the price comes down".
John Locke's 1691 work "Some Considerations on the Consequences of the Lowering of Interest and the Raising of the Value of Money" includes an early and clear description of supply and demand and their relationship. In this description, demand is rent: "The price of any commodity rises or falls by the proportion of the number of buyer and sellers" and "that which regulates the price... [of goods] is nothing else but their quantity in proportion to their rent".

The phrase "supply and demand" was first used by James Denham-Steuart in his "Inquiry into the Principles of Political Economy", published in 1767. Adam Smith used the phrase in his 1776 book "The Wealth of Nations", and David Ricardo titled one chapter of his 1817 work "Principles of Political Economy and Taxation" "On the Influence of Demand and Supply on Price".

In "The Wealth of Nations", Smith generally assumed that the supply price was fixed, but that its "merit" (value) would decrease as its "scarcity" increased, in effect what was later called the law of demand also. In "Principles of Political Economy and Taxation", Ricardo more rigorously laid down the idea of the assumptions that were used to build his ideas of supply and demand. Antoine Augustin Cournot first developed a mathematical model of supply and demand in his 1838 "Researches into the Mathematical Principles of Wealth", including diagrams.

During the late 19th century, the marginalist school of thought emerged. This field mainly was started by Stanley Jevons, Carl Menger and Léon Walras. The key idea was that the price was set by the most expensive price—that is, the price at the margin. This was a substantial change from Adam Smith's thoughts on determining the supply price.

In his 1870 essay "On the Graphical Representation of Supply and Demand", Fleeming Jenkin in the course of "introduc[ing] the diagrammatic method into the English economic literature" published the first drawing of supply and demand curves therein, including comparative statics from a shift of supply or demand and application to the labor market. The model was further developed and popularized by Alfred Marshall in the 1890 textbook "Principles of Economics".

In a capitalist system, the government protects private property and guarantees the right of citizens to choose their job. In most cases, the government does not prevent firms from determining what wages they will pay and what prices they will charge for their products. However, many countries have minimum wage laws and minimum safety standards.

Under some versions of capitalism, the government carries out a number of economic functions, such as issuing money, supervising public utilities, and enforcing private contracts. Many countries have competition laws that prohibit monopolies and cartels. Despite anti-monopoly laws, large corporations can form near-monopolies in some industries. Such firms can temporarily drop prices and accept losses to prevent competition from entering the market and then raise them again once the threat of competition is reduced. In many countries, public utilities such as electricity, heating fuel and communications are able to operate as a monopoly under government regulation due to high economies of scale.

Government agencies regulate the standards of service in many industries, such as airlines and broadcasting, as well as financing a wide range of programs. In addition, the government regulates the flow of capital and uses financial tools such as the interest rate to control such factors as inflation and unemployment.

In his book "The Road to Serfdom", Friedrich Hayek asserts that the economic freedom of capitalism is a requisite of political freedom. He argues that the market mechanism is the only way of deciding what to produce and how to distribute the items without using coercion. Milton Friedman, Andrew Brennan and Ronald Reagan also promoted this view. Friedman claimed that centralized economic operations are always accompanied by political repression. In his view, transactions in a market economy are voluntary and that the wide diversity that voluntary activity permits is a fundamental threat to repressive political leaders and greatly diminishes their power to coerce. Some of Friedman's views were shared by John Maynard Keynes, who believed that capitalism is vital for freedom to survive and thrive. Freedom House, an American think tank that conducts international research on, and advocates for, democracy, political freedom and human rights, has argued "there is a high and statistically significant correlation between the level of political freedom as measured by Freedom House and economic freedom as measured by the Wall Street Journal/Heritage Foundation survey".

There are many variants of capitalism in existence that differ according to country and region. They vary in their institutional makeup and by their economic policies. The common features among all the different forms of capitalism is that they are based on the production of goods and services for profit, predominantly market-based allocation of resources and they are structured upon the accumulation of capital. They include advanced capitalism, corporate capitalism, finance capitalism, free-market capitalism, mercantilism, social capitalism, state capitalism and welfare capitalism. Other variants of capitalism include anarcho-capitalism, community capitalism, humanistic capitalism, neo-capitalism, state monopoly capitalism, supercapitalism and technocapitalism.

Advanced capitalism is the situation that pertains to a society in which the capitalist model has been integrated and developed deeply and extensively for a prolonged period. Various writers identify Antonio Gramsci as an influential early theorist of advanced capitalism, even if he did not use the term himself. In his writings, Gramsci sought to explain how capitalism had adapted to avoid the revolutionary overthrow that had seemed inevitable in the 19th century. At the heart of his explanation was the decline of raw coercion as a tool of class power, replaced by use of civil society institutions to manipulate public ideology in the capitalists' favour.

Jürgen Habermas has been a major contributor to the analysis of advanced-capitalistic societies. Habermas observed four general features that characterise advanced capitalism:

Corporate capitalism is a free or mixed-market capitalist economy characterized by the dominance of hierarchical, bureaucratic corporations.

Finance capitalism is the subordination of processes of production to the accumulation of money profits in a financial system. In their critique of capitalism, Marxism and Leninism both emphasise the role of finance capital as the determining and ruling-class interest in capitalist society, particularly in the latter stages.

Rudolf Hilferding is credited with first bringing the term finance capitalism into prominence through "Finance Capital", his 1910 study of the links between German trusts, banks and monopolies—a study subsumed by Vladimir Lenin into "Imperialism, the Highest Stage of Capitalism" (1917), his analysis of the imperialist relations of the great world powers. Lenin concluded that the banks at that time operated as "the chief nerve centres of the whole capitalist system of national economy". For the Comintern (founded in 1919), the phrase "dictatorship of finance capitalism" became a regular one.

Fernand Braudel would later point to two earlier periods when finance capitalism had emerged in human history—with the Genoese in the 16th century and with the Dutch in the 17th and 18th centuries—although at those points it developed from commercial capitalism. Giovanni Arrighi extended Braudel's analysis to suggest that a predominance of finance capitalism is a recurring, long-term phenomenon, whenever a previous phase of commercial/industrial capitalist expansion reaches a plateau.

A capitalist free-market economy is an economic system where prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets and private ownership of the means of production. "Laissez-faire" capitalism is a more extensive form of this free-market economy, but one in which the role of the state is limited to protecting property rights. In anarcho-capitalist theory, property rights are protected by private firms and market-generated law. According to anarcho-capitalists, this entails property rights without statutory law through market-generated tort, contract and property law as well as the self-sustaining private industry.

Mercantilism is a nationalist form of early capitalism that came into existence approximately in the late 16th century. It is characterized by the intertwining of national business interests to state-interest and imperialism; and consequently, the state apparatus is utilized to advance national business interests abroad. An example of this is colonists living in America who were only allowed to trade with and purchase goods from their respective mother countries (e.g. Britain, France and Portugal). Mercantilism was driven by the belief that the wealth of a nation is increased through a positive balance of trade with other nations—it corresponds to the phase of capitalist development sometimes called the primitive accumulation of capital.

A social market economy is a free-market or mixed-market capitalist system, sometimes classified as a coordinated market economy, where government intervention in price formation is kept to a minimum, but the state provides significant services in the area of social security, unemployment benefits and recognition of labor rights through national collective bargaining arrangements. This model is prominent in Western and Northern European countries as well as Japan, albeit in slightly different configurations. The vast majority of enterprises are privately owned in this economic model.

Rhine capitalism is the contemporary model of capitalism and adaptation of the social market model that exists in continental Western Europe today.

State capitalism is a capitalist market economy dominated by state-owned enterprises, where the state enterprises are organized as commercial, profit-seeking businesses. The designation has been used broadly throughout the 20th century to designate a number of different economic forms, ranging from state-ownership in market economies to the command economies of the former Eastern Bloc. According to Aldo Musacchio, a professor at Harvard Business School, state capitalism is a system in which governments, whether democratic or autocratic, exercise a widespread influence on the economy either through direct ownership or various subsidies. Musacchio notes a number of differences between today's state capitalism and its predecessors. In his opinion, gone are the days when governments appointed bureaucrats to run companies: the world's largest state-owned enterprises are now traded on the public markets and kept in good health by large institutional investors. Contemporary state capitalism is associated with the East Asian model of capitalism, dirigisme and the economy of Norway. Alternatively, Merriam-Webster defines state capitalism as "an economic system in which private capitalism is modified by a varying degree of government ownership and control".

In "Socialism: Utopian and Scientific", Friedrich Engels argued that state-owned enterprises would characterize the final stage of capitalism, consisting of ownership and management of large-scale production and communication by the bourgeois state. In his writings, Vladimir Lenin characterized the economy of Soviet Russia as state capitalist, believing state capitalism to be an early step toward the development of socialism.

Some economists and left-wing academics including Richard D. Wolff and Noam Chomsky argue that the economies of the former Soviet Union and Eastern Bloc represented a form of state capitalism because their internal organization within enterprises and the system of wage labor remained intact.

The term is not used by Austrian School economists to describe state ownership of the means of production. The economist Ludwig von Mises argued that the designation of state capitalism was simply a new label for the old labels of state socialism and planned economy and differed only in non-essentials from these earlier designations.

The debate between proponents of private versus state capitalism is centered around questions of managerial efficacy, productive efficiency and fair distribution of wealth.

Welfare capitalism is capitalism that includes social welfare policies. Today, welfare capitalism is most often associated with the models of capitalism found in Central Mainland and Northern Europe such as the Nordic model, social market economy and Rhine capitalism. In some cases, welfare capitalism exists within a mixed economy, but welfare states can and do exist independently of policies common to mixed economies such as state interventionism and extensive regulation.

A mixed economy is a largely market-based capitalist economy consisting of both private and public ownership of the means of production and economic interventionism through macroeconomic policies intended to correct market failures, reduce unemployment and keep inflation low. The degree of intervention in markets varies among different countries. Some mixed economies such as France under dirigisme also featured a degree of indirect economic planning over a largely capitalist-based economy.

Most modern capitalist economies are defined as mixed economies to some degree.

The accumulation of capital is the process of "making money", or growing an initial sum of money through investment in production. Capitalism is based on the accumulation of capital, whereby financial capital is invested in order to make a profit and then reinvested into further production in a continuous process of accumulation. In Marxian economic theory, this dynamic is called the law of value. Capital accumulation forms the basis of capitalism, where economic activity is structured around the accumulation of capital, defined as investment in order to realize a financial profit. In this context, "capital" is defined as money or a financial asset invested for the purpose of making more money (whether in the form of profit, rent, interest, royalties, capital gain or some other kind of return).

In mainstream economics, accounting and Marxian economics, capital accumulation is often equated with investment of profit income or saving, especially in real capital goods. The concentration and centralisation of capital are two of the results of such accumulation. In modern macroeconomics and econometrics, the phrase "capital formation" is often used in preference to "accumulation", though the United Nations Conference on Trade and Development (UNCTAD) refers nowadays to "accumulation". The term “accumulation” is occasionally used in national accounts.

Accumulation can be measured as the monetary value of investments, the amount of income that is reinvested, or as the change in the value of assets owned (the increase in the value of the capital stock). Using company balance sheets, tax data and direct surveys as a basis, government statisticians estimate total investments and assets for the purpose of national accounts, national balance of payments and flow of funds statistics. The Reserve Banks and the Treasury usually provide interpretations and analysis of this data. Standard indicators include capital formation, gross fixed capital formation, fixed capital, household asset wealth and foreign direct investment.

Organisations such as the International Monetary Fund, the UNCTAD, the World Bank Group, the OECD and the Bank for International Settlements used national investment data to estimate world trends. The Bureau of Economic Analysis, Eurostat and the Japan Statistical Office provide data on the United States, Europe and Japan respectively. Other useful sources of investment information are business magazines such as "Fortune, Forbes, The Economist, Business Week" and so on as well as various corporate "watchdog" organisations and non-governmental organisation publications. A reputable scientific journal is the "Review of Income & Wealth". In the case of the United States, the "Analytical Perspectives" document (an annex to the yearly budget) provides useful wealth and capital estimates applying to the whole country.

In Karl Marx' economic theory, capital accumulation refers to the operation whereby profits are reinvested increasing the total quantity of capital. Capital is viewed by Marx as expanding value, that is, in other terms, as a sum of capital, usually expressed in money, that is transformed through human labor into a larger value, extracted as profits and expressed as money. Here, capital is defined essentially as economic or commercial asset value in search of additional value or surplus-value. This requires property relations which enable objects of value to be appropriated and owned, and trading rights to be established. Capital accumulation has a double origin, namely in trade and in expropriation, both of a legal or illegal kind. The reason is that a stock of capital can be increased through a process of exchange or "trading up", but also through directly taking an asset or resource from someone else without compensation. David Harvey calls this accumulation by dispossession.

The continuation and progress of capital accumulation depends on the removal of obstacles to the expansion of trade and this has historically often been a violent process. As markets expand, more and more new opportunities develop for accumulating capital because more and more types of goods and services can be traded in. However, capital accumulation may also confront resistance when people refuse to sell, or refuse to buy (for example a strike by investors or workers, or consumer resistance).

According to Marx, capital has the tendency for concentration and centralization in the hands of the wealthy. Marx explains: "It is concentration of capitals already formed, destruction of their individual independence, expropriation of capitalist by capitalist, transformation of many small into few large capitals. [...] Capital grows in one place to a huge mass in a single hand, because it has in another place been lost by many. [...] The battle of competition is fought by cheapening of commodities. The cheapness of commodities demands, "caeteris paribus", on the productiveness of labour, and this again on the scale of production. Therefore, the larger capitals beat the smaller. It will further be remembered that, with the development of the capitalist mode of production, there is an increase in the minimum amount of individual capital necessary to carry on a business under its normal conditions. The smaller capitals, therefore, crowd into spheres of production which Modern Industry has only sporadically or incompletely got hold of. Here competition rages [...] It always ends in the ruin of many small capitalists, whose capitals partly pass into the hands of their conquerors, partly vanish".

In Marxian economics, the rate of accumulation is defined as (1) the value of the real net increase in the stock of capital in an accounting period; and (2) the proportion of realised surplus-value or profit-income which is reinvested, rather than consumed. This rate can be expressed by means of various ratios between the original capital outlay, the realised turnover, surplus-value or profit and reinvestments (e.g. the writings of the economist Michał Kalecki).

Other things being equal, the greater the amount of profit-income that is disbursed as personal earnings and used for consumptive purposes, the lower the savings rate and the lower the rate of accumulation is likely to be. However, earnings spent on consumption can also stimulate market demand and higher investment. This is the cause of endless controversies in economic theory about "how much to spend, and how much to save".

In a boom period of capitalism, the growth of investments is cumulative, i.e. one investment leads to another, leading to a constantly expanding market, an expanding labor force and an increase in the standard of living for the majority of the people.

In a stagnating, decadent capitalism, the accumulation process is increasingly oriented towards investment on military and security forces, real estate, financial speculation and luxury consumption. In that case, income from value-adding production will decline in favour of interest, rent and tax income, with as a corollary an increase in the level of permanent unemployment. The more capital one owns, the more capital one can also borrow. The inverse is also true and this is one factor in the widening gap between the rich and the poor.

Ernest Mandel emphasised that the rhythm of capital accumulation and growth depended critically on (1) the division of a society's social product between "necessary product" and "surplus product"; and (2) the division of the surplus product between investment and consumption. In turn, this allocation pattern reflected the outcome of competition among capitalists, competition between capitalists and workers and competition between workers. The pattern of capital accumulation can therefore never be simply explained by commercial factors as it also involved social factors and power relationships.

Strictly speaking, capital has accumulated only when realised profit income has been reinvested in capital assets. As suggested in the first volume of Marx' "Das Kapital", the process of capital accumulation in production has at least seven distinct but linked moments:

All of these moments do not refer simply to an "economic" or commercial process. Rather, they assume the existence of legal, social, cultural and economic power conditions, without which creation, distribution and circulation of the new wealth could not occur. This becomes especially clear when the attempt is made to create a market where none exists, or where people refuse to trade.

In the second volume of "Das Kapital", Marx continues the story and shows that with the aid of bank credit capital in search of growth can more or less smoothly mutate from one form to another, alternately taking the form of money capital (liquid deposits, securities and so on), commodity capital (tradable products, real estate and the like), or production capital (means of production and labor power).

His discussion of the simple and expanded reproduction of the conditions of production offers a more sophisticated model of the parameters of the accumulation process as a whole. At simple reproduction, a sufficient amount is produced to sustain society at the given living standard; the stock of capital stays constant. At expanded reproduction, more product-value is produced than is necessary to sustain society at a given living standard (a surplus product); the additional product-value is available for investments which enlarge the scale and variety of production.

The bourgeois claim there is no economic law according to which capital is necessarily re-invested in the expansion of production, that such depends on anticipated profitability, market expectations and perceptions of investment risk. Such statements only explain the subjective experiences of investors and ignore the objective realities which would influence such opinions. As Marx states in the second volume of "Das Kapital", simple reproduction only exists if the variable and surplus capital realised by Dept. 1—producers of means of production—exactly equals that of the constant capital of Dept. 2, producers of articles of consumption (p. 524). Such equilibrium rests on various assumptions, such as a constant labor supply (no population growth). Accumulation does not imply a necessary change in total magnitude of value produced, but can simply refer to a change in the composition of an industry (p. 514).

Ernest Mandel introduced the additional concept of contracted economic reproduction, i.e. reduced accumulation where business operating at a loss outnumbers growing business, or economic reproduction on a decreasing scale, for example due to wars, natural disasters or devalorisation.

Balanced economic growth requires that different factors in the accumulation process expand in appropriate proportions. However, markets themselves cannot spontaneously create that balance and in fact what drives business activity is precisely the imbalances between supply and demand: inequality is the motor of growth. This partly explains why the worldwide pattern of economic growth is very uneven and unequal, even although markets have existed almost everywhere for a very long-time. Some people argue that it also explains government regulation of market trade and protectionism.

"Accumulation of capital" sometimes also refers in Marxist writings to the reproduction of capitalist social relations (institutions) on a larger scale over time, i.e. the expansion of the size of the proletariat and of the wealth owned by the bourgeoisie.

This interpretation emphasises that capital ownership, predicated on command over labor, is a social relation: the growth of capital implies the growth of the working class (a "law of accumulation"). In the first volume of "Das Kapital", Marx had illustrated this idea with reference to Edward Gibbon Wakefield's theory of colonisation: 

In the third volume of "Das Kapital", Marx refers to the "fetishism of capital" reaching its highest point with interest-bearing capital because now capital seems to grow of its own accord without anybody doing anything: 

Wage labour refers to the sale of labour under a formal or informal employment contract to an employer. These transactions usually occur in a labour market where wages are market determined. Individuals who possess and supply financial capital or labor to productive ventures often become owners, either jointly (as shareholders) or individually. In Marxist economics, these owners of the means of production and suppliers of capital are generally called capitalists. The description of the role of the capitalist has shifted, first referring to a useless intermediary between producers to an employer of producers and eventually came to refer to owners of the means of production. Labor includes all physical and mental human resources, including entrepreneurial capacity and management skills, which are needed to produce products and services. Production is the act of making goods or services by applying labor power.

Critics of the capitalist mode of production see wage labour as a major, if not defining, aspect of hierarchical industrial systems. Most opponents of the institution support worker self-management and economic democracy as alternatives to both wage labour and to capitalism. While most opponents of the wage system blame the capitalist owners of the means of production for its existence, most anarchists and other libertarian socialists also hold the state as equally responsible as it exists as a tool utilised by capitalists to subsidise themselves and protect the institution of private ownership of the means of production. As some opponents of wage labour take influence from Marxist propositions, many are opposed to private property, but maintain respect for personal property.

The most common form of wage labour currently is ordinary direct, or "full-time", employment in which a free worker sells his or her labour for an indeterminate time (from a few years to the entire career of the worker) in return for a money-wage or salary and a continuing relationship with the employer which it does not in general offer contractors or other irregular staff. However, wage labour takes many other forms and explicit as opposed to implicit (i.e. conditioned by local labour and tax law) contracts are not uncommon. Economic history shows a great variety of ways in which labour is traded and exchanged. The differences show up in the form of:

War typically causes the diversion, destruction and creation of capital assets as capital assets are both destroyed or consumed and diverted to types of production needed to fight the war. Many assets are wasted and in some few cases created specifically to fight a war. War driven demands may be a powerful stimulus for the accumulation of capital and production capability in limited areas and market expansion outside the immediate theatre of war. Often this has induced laws against perceived and real war profiteering.

The total hours worked in the United States rose by 34 percent during World War II, even though the military draft reduced the civilian labor force by 11 percent.

War destruction can be illustrated by looking at World War II. Industrial war damage was heaviest in Japan, where 1/4 of factory buildings and 1/3 of plant and equipment were destroyed; 1/7 of electric power-generating capacity was destroyed and 6/7 of oil refining capacity. The Japanese merchant fleet lost 80% of their ships. In Germany in 1944, when air attacks were heaviest, 6.5% of machine tools were damaged or destroyed, but around 90% were later repaired. About 10% of steel production capacity was lost. In Europe, the United States and the Soviet Union enormous resources were accumulated and ultimately dissipated as planes, ships, tanks and so on were built and then lost or destroyed.

Germany's total war damage was estimated at about 17.5% of the pre-war total capital stock by value, i.e. about 1/6. In the Berlin area alone, there were 8 million refugees lacking basic necessities. In 1945, less than 10% of the railways were still operating. 2,395 rail bridges were destroyed and a total of 7,500 bridges, 10,000 locomotives and more than 100,000 goods wagons were destroyed. Less than 40% of the remaining locomotives were operational.

However, by the first quarter of 1946 European rail traffic, which was given assistance and preferences (by Western appointed military governors) for resources and material as an essential asset, regained its prewar operational level. At the end of the year, 90% of Germany's railway lines were operating again. In retrospect, the rapidity of infrastructure reconstruction appears astonishing.

Initially, in May 1945 newly installed United States president Harry S. Truman's directive had been that no steps would be taken towards economic rehabilitation of Germany. In fact, the initial industry plan of 1946 prohibited production in excess of half of the 1938 level; the iron and steel industry was allowed to produce only less than a third of pre-war output. These plans were rapidly revised and better plans were instituted. In 1946, over 10% of Germany's physical capital stock (plant and equipment) was also dismantled and confiscated, most of it going to the Soviet Union. By 1947, industrial production in Germany was at 1/3 of the 1938 level and industrial investment at about 1/2 the 1938 level.

The first big strike-wave in the Ruhr occurred in early 1947—it was about food rations and housing, but soon there were demands for nationalisation. However, the United States appointed military governor (Newman) stated at the time that he had the power to break strikes by withholding food rations. The clear message was "no work, no eat". As the military controls in Western Germany were nearly all relinquished and the Germans were allowed to rebuild their own economy with Marshall Plan aid things rapidly improved. By 1951, German industrial production had overtaken the prewar level. The Marshall Aid funds were important, but after the currency reform (which permitted German capitalists to revalue their assets) and the establishment of a new political system much more important was the commitment of the United States to rebuilding German capitalism and establishing a free market economy and government, rather than keeping Germany in a weak position. Initially, average real wages remained low, lower even than in 1938, until the early 1950s while profitability was unusually high. So the total investment fund, aided by credits, was also high, resulting in a high rate of capital accumulation which was nearly all reinvested in new construction or new tools. This was called the German economic miracle or "Wirtschaftswunder".

In Italy, the victorious Allies did three things in 1945: they imposed their absolute military authority; they quickly disarmed the Italian partisans from a very large stock of weapons; and they agreed to a state guarantee of wage payments as well as a veto on all sackings of workers from their jobs. Although the Italian Communist Party grew very large immediately after the war ended—it achieved a membership of 1.7 million people in a population of 45 million—it was outmaneuvered through a complicated political battle by the Christian Democrats after three years. In the 1950s, an economic boom began in Italy, at first fueled by internal demand and then also by exports.

In modern times, it has often been possible to rebuild physical capital assets destroyed in wars completely within the space of about 10 years, except in cases of severe pollution by chemical warfare or other kinds of irreparable devastation. However, damage to human capital has been much more devastating in terms of fatalities (in the case of World War II, about 55 million deaths), permanent physical disability, enduring ethnic hostility and psychological injuries which have effects for at least several generations.

Critics of capitalism associate the economic system with social inequality; unfair distribution of wealth and power; materialism; repression of workers and trade unionists; social alienation; economic inequality; unemployment; and economic instability. Many socialists consider capitalism to be irrational in that production and the direction of the economy are unplanned, creating many inconsistencies and internal contradictions. Capitalism and individual property rights have been associated with the tragedy of the anticommons where owners are unable to agree. Marxian economist Richard D. Wolff postulates that capitalist economies prioritize profits and capital accumulation over the social needs of communities and capitalist enterprises rarely include the workers in the basic decisions of the enterprise. Democratic socialists argue that the role of the state in a capitalist society is to defend the interests of the bourgeoisie. These states take actions to implement such things as unified national markets, national currencies and customs system. Capitalism and capitalist governments have also been criticized as oligarchic in nature due to the inevitable inequality characteristic of economic progress.

Some labor historians and scholars have argued that unfree labor—by slaves, indentured servants, prisoners or other coerced persons—is compatible with capitalist relations. Tom Brass argued that unfree labor is acceptable to capital. Historian Greg Grandin argues that capitalism has its origins in slavery, saying that "[w]hen historians talk about the Atlantic market revolution, they are talking about capitalism. And when they are talking about capitalism, they are talking about slavery." Some historians, including Edward E. Baptist and Sven Beckert, assert that slavery was an integral component in the violent development of American and global capitalism. The Slovenian continental philosopher Slavoj Žižek posits that the new era of global capitalism has ushered in new forms of contemporary slavery, including migrant workers deprived of basic civil rights on the Arabian Peninsula, the total control of workers in Asian sweatshops, and the use of forced labor in the exploitation of natural resources in Central Africa.

According to Immanuel Wallerstein, institutional racism has been "one of the most significant pillars" of the capitalist system and serves as "the ideological justification for the hierarchization of the work-force and its highly unequal distributions of reward".

Many aspects of capitalism have come under attack from the anti-globalization movement, which is primarily opposed to corporate capitalism. Environmentalists have argued that capitalism requires continual economic growth and that it will inevitably deplete the finite natural resources of Earth and cause mass extinctions of animal and plant life. Such critics argue that while neoliberalism, the ideological backbone of contemporary globalized capitalism, has indeed increased global trade, it has also destroyed traditional ways of life, exacerbated inequality, increased global poverty, and that environmental indicators indicate massive environmental degradation since the late 1970s.
Some scholars blame the financial crisis of 2007–2008 on the neoliberal capitalist model. Following the banking crisis of 2007, Alan Greenspan told the United States Congress on 23 October 2008 that "[t]his modern risk-management paradigm held sway for decades. The whole intellectual edifice, however, collapsed in the summer of last year", and that "I made a mistake in presuming that the self-interests of organizations, specifically banks and others, were such that they were best capable of protecting their own shareholders and their equity in firms [...] I was shocked".

Many religions have criticized or opposed specific elements of capitalism. Traditional Judaism, Christianity, and Islam forbid lending money at interest, although alternative methods of banking have been developed. Some Christians have criticized capitalism for its materialist aspects and its inability to account for the wellbeing of all people. Many of Jesus' parables deal with economic concerns: farming, shepherding, being in debt, doing hard labor, being excluded from banquets and the houses of the rich and have implications for wealth and power distribution. Catholic scholars and clergy have often criticized capitalism because of its disenfranchisement of the poor, often promoting distributism as an alternative. In his 84-page apostolic exhortation , Catholic Pope Francis described unfettered capitalism as "a new tyranny" and called on world leaders to fight rising poverty and inequality:

Proponents of capitalism argue that it creates more prosperity than any other economic system and that its benefits are mainly to the ordinary person. Critics of capitalism variously associate it with economic instability, an inability to provide for the well-being of all people and an unsustainable danger to the natural environment. Socialists maintain that although capitalism is superior to all previously existing economic systems (such as feudalism or slavery), the contradiction between class interests will only be resolved by advancing into a completely new social system of production and distribution in which all persons have an equal relationship to the means of production.

The term capitalism in its modern sense is often attributed to Karl Marx. In his "Das Kapital", Marx analyzed the "capitalist mode of production" using a method of understanding today known as Marxism. However, Marx himself rarely used the term "capitalism" while it was used twice in the more political interpretations of his work, primarily authored by his collaborator Friedrich Engels. In the 20th century, defenders of the capitalist system often replaced the term capitalism with phrases such as free enterprise and private enterprise and replaced capitalist with rentier and investor in reaction to the negative connotations associated with capitalism.

The majority of criticisms against the profit motive centre on the idea that the profit motive encourages selfishness and greed, rather than serve the public good or necessarily creating an increase in net wealth. Critics of the profit motive contend that companies disregard morals or public safety in the pursuit of profits.

Free market economists counter that the profit motive, coupled with competition, actually reduces the final price of an item for consumption, rather than raising it. They argue that businesses profit by selling a good at a lower price and at a greater volume than the competition. Economist Thomas Sowell uses supermarkets as an example to illustrate this point: "It has been estimated that a supermarket makes a clear profit of about a penny on a dollar of sales. If that sounds pretty skimpy, remember that it is collecting that penny on every dollar at several cash registers simultaneously and, in many cases, around the clock".

American economist Milton Friedman has argued that greed and self-interest are universal human traits. On a 1979 episode of "The Phil Donahue Show", Friedman states: "The world runs on individuals pursuing their separate interests". He continues by explaining that only in capitalist countries, where individuals can pursue their own self-interest, people have been able to escape from "grinding poverty".

Wage labor has long been compared to slavery. As a result, the phrase "wage slavery" is often utilized as a pejorative for wage labor. Similarly, advocates of slavery looked upon the "comparative evils of Slave Society and of Free Society, of slavery to human Masters and slavery to Capital" and proceeded to argue that wage slavery was actually worse than chattel slavery. Slavery apologists like George Fitzhugh contended that workers only accepted wage labor with the passage of time as they became "familiarised and inattentive to the infected social atmosphere they continually inhale". Scholars have debated the exact relationship between wage labor, slavery, and capitalism at length, especially for the Antebellum South.

Similarities between wage labor and slavery were noted as early as Cicero in Ancient Rome, such as in "De Officiis". With the advent of the Industrial Revolution, thinkers such as Pierre-Joseph Proudhon and Karl Marx elaborated the comparison between wage labor and slavery in the context of a critique of societal property not intended for active personal use while Luddites emphasized the dehumanisation brought about by machines. Before the American Civil War, Southern defenders of African American slavery invoked the concept of wage slavery to favorably compare the condition of their slaves to workers in the North. The United States abolished slavery during the Civil War, but labor union activists found the metaphor useful. According to Lawrence Glickman, in the Gilded Age "references abounded in the labor press, and it is hard to find a speech by a labour leader without the phrase".
According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book "On the Limits of State Action", liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the laborer works under external control, "we may admire what he does, but we despise what he is". Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations. Additionally, as per anthropologist David Graeber, the earliest wage labor contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money and the slave another, with which to maintain his or her living expenses). According to Graeber, such arrangements were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued in "The Black Jacobins" that most of the techniques of human organisation employed on factory workers during the Industrial Revolution were first developed on slave plantations.
Some anti-capitalist thinkers claim that the elite maintain wage slavery and a divided working class through their influence over the media and entertainment industry, educational institutions, unjust laws, nationalist and corporate propaganda, pressures and incentives to internalize values serviceable to the power structure, state violence, fear of unemployment and a historical legacy of exploitation and profit accumulation/transfer under prior systems, which shaped the development of economic theory:

Adam Smith noted that employers often conspire together to keep wages low: 

Aristotle made the statement that "the citizens must not live a mechanic or a mercantile life (for such a life is ignoble and inimical to virtue), nor yet must those who are to be citizens in the best state be tillers of the soil (for leisure is needed both for the development of virtue and for active participation in politics)", often paraphrased as "all paid jobs absorb and degrade the mind". Cicero wrote in 44 BC that "vulgar are the means of livelihood of all hired workmen whom we pay for mere manual labour, not for artistic skill; for in their case the very wage they receive is a pledge of their slavery". Somewhat similar criticisms have also been expressed by some proponents of liberalism, like Henry George, Silvio Gesell and Thomas Paine as well as the Distributist school of thought within the Roman Catholic Church.

To Marxist and anarchist thinkers like Mikhail Bakunin and Peter Kropotkin, wage slavery was a class condition in place due to the existence of private property and the state. This class situation rested primarily on:

For Marxists, labor as commodity, which is how they regard wage labor, provides a fundamental point of attack against capitalism. "It can be persuasively argued", noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatization of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it". That this objection is fundamental follows immediately from Marx's conclusion that wage labor is the very foundation of capitalism: "Without a class dependent on wages, the moment individuals confront each other as free persons, there can be no production of surplus value; without the production of surplus-value there can be no capitalist production, and hence no capital and no capitalist!".

Marx considered capitalism to be a historically specific mode of production (the way in which the productive property is owned and controlled, combined with the corresponding social relations between individuals based on their connection with the process of production).

The "capitalistic era" according to Karl Marx dates from 16th-century merchants and small urban workshops. Marx knew that wage labour existed on a modest scale for centuries before capitalist industry. For Marx, the capitalist stage of development or "bourgeois society" represented the most advanced form of social organization to date, but he also thought that the working classes would come to power in a worldwide socialist or communist transformation of human society as the end of the series of first aristocratic, then capitalist and finally working class rule was reached.

Following Adam Smith, Marx distinguished the use value of commodities from their exchange value in the market. According to Marx, capital is created with the purchase of commodities for the purpose of creating new commodities with an exchange value higher than the sum of the original purchases. For Marx, the use of labor power had itself become a commodity under capitalism and the exchange value of labor power, as reflected in the wage, is less than the value it produces for the capitalist.

This difference in values, he argues, constitutes surplus value, which the capitalists extract and accumulate. In his book "Capital", Marx argues that the capitalist mode of production is distinguished by how the owners of capital extract this surplus from workers—all prior class societies had extracted surplus labor, but capitalism was new in doing so via the sale-value of produced commodities. He argues that a core requirement of a capitalist society is that a large portion of the population must not possess sources of self-sustenance that would allow them to be independent and are instead forced to sell their labor for a wage.

In conjunction with his criticism of capitalism was Marx's belief that the working class, due to its relationship to the means of production and numerical superiority under capitalism, would be the driving force behind the socialist revolution. This argument is intertwined with Marx' version of the labor theory of value arguing that labor is the source of all value and thus of profit.

In "Imperialism, the Highest Stage of Capitalism" (1916), Vladimir Lenin further developed Marxist theory and argued that capitalism necessarily led to monopoly capitalism and the export of capital—which he also called "imperialism"—to find new markets and resources, representing the last and highest stage of capitalism. Some 20th century Marxian economists consider capitalism to be a social formation where capitalist class processes dominate, but are not exclusive.

To these thinkers, capitalist class processes are simply those in which surplus labor takes the form of surplus value, usable as capital; other tendencies for utilization of labor nonetheless exist simultaneously in existing societies where capitalist processes predominate. However, other late Marxian thinkers argue that a social formation as a whole may be classed as capitalist if capitalism is the mode by which a surplus is extracted, even if this surplus is not produced by capitalist activity as when an absolute majority of the population is engaged in non-capitalist economic activity.

In "Limits to Capital" (1982), David Harvey outlines an overdetermined, "spatially restless" capitalism coupled with the spatiality of crisis formation and resolution. Harvey used Marx's theory of crisis to aid his argument that capitalism must have its "fixes", but that we cannot predetermine what fixes will be implemented, nor in what form they will be. His work on contractions of capital accumulation and international movements of capitalist modes of production and money flows has been influential. According to Harvey, capitalism creates the conditions for volatile and geographically uneven development 

Sociologists such as Ulrich Beck envisioned the society of risk as a new cultural value which saw risk as a commodity to be exchanged in globalized economies. This theory suggested that disasters and capitalist economy were inevitably entwined. Disasters allow the introduction of economic programs which otherwise would be rejected as well as decentralizing the class structure in production.

Scholars argue that the capitalist approach to environmental economics does not take into consideration the preserving of natural resources. Capitalism creates three ecological problems: growth, technology, and consumption. The growth problem results from the nature of capitalism, as it focuses around the accumulation of Capital.  The innovation of new technologies has an impact on the environmental future as they serve as a capitalist tool in which environmental technologies can result in the expansion of the system. Consumption is focused around the capital accumulation of commodities and neglects the use-value of production.  

At least two assumptions are necessary for the validity of the standard model: first, that supply and demand are independent; and second, that supply is "constrained by a fixed resource". If these conditions do not hold, then the Marshallian model cannot be sustained. Sraffa's critique focused on the inconsistency (except in implausible circumstances) of partial equilibrium analysis and the rationale for the upward slope of the supply curve in a market for a produced consumption good. The notability of Sraffa's critique is also demonstrated by Paul A. Samuelson's comments and engagements with it over many years, for example:

Aggregate excess demand in a market is the difference between the quantity demanded and the quantity supplied as a function of price. In the model with an upward-sloping supply curve and downward-sloping demand curve, the aggregate excess demand function only intersects the axis at one point, namely at the point where the supply and demand curves intersect. The Sonnenschein–Mantel–Debreu theorem shows that the standard model cannot be rigorously derived in general from general equilibrium theory.

The model of prices being determined by supply and demand assumes perfect competition. However, "economists have no adequate model of how individuals and firms adjust prices in a competitive model. If all participants are price-takers by definition, then the actor who adjusts prices to eliminate excess demand is not specified". Goodwin, Nelson, Ackerman and Weisskopf write:

Market failure occurs when an externality is present and a market will often either under-produce a product with a positive externalisation or overproduce a product that generates a negative externalisation. Air pollution, for instance, is a negative externalisation that cannot be easily incorporated into markets as the world's air is not owned and then sold for use to polluters. So too much pollution could be emitted and people not involved in the production pay the cost of the pollution instead of the firm that initially emitted the air pollution. Critics of market failure theory, like Ronald Coase, Harold Demsetz and James M. Buchanan, argue that government programs and policies also fall short of absolute perfection. While all nations currently have some kind of market regulations, the desirable degree of regulation is disputed.

Austrian School economists have argued that capitalism can organize itself into a complex system without an external guidance or central planning mechanism. Friedrich Hayek considered the phenomenon of self-organisation as underpinning capitalism. Prices serve as a signal as to the urgent and unfilled wants of people and the opportunity to earn profits if successful, or absorb losses if resources are used poorly or left idle, gives entrepreneurs incentive to use their knowledge and resources to satisfy those wants. Thus the activities of millions of people, each seeking his own interest, are coordinated.

The novelist and philosopher Ayn Rand made positive moral defenses of "laissez-faire" capitalism, most notably in her 1957 novel "Atlas Shrugged" and in her 1966 collection of essays "". She argued that capitalism should be supported on moral grounds, not just on the basis of practical benefits. Her ideas have had significant influence over conservative and libertarian supporters of capitalism, especially within the American Tea Party movement. Rand defined capitalism as "a social system based on the recognition of individual rights, including property rights, in which all property is privately owned". According to Rand, the role of government in a capitalist state has three broad categories of proper functions: first, the police "to protect men from criminals"; second, the armed services "to protect men from foreign invaders"; and third, the law courts "to settle disputes among men according to objective laws".





</doc>
<doc id="5420" url="https://en.wikipedia.org/wiki?curid=5420" title="Cross ownership">
Cross ownership

Cross ownership is a method of reinforcing business relationships by owning stock in the companies with which a given company does business. Heavy cross ownership is referred to as circular ownership.

In the US, "cross ownership" also refers to a type of investment in different mass-media properties in one market.

Some countries where cross ownership of shares is a major part of the business culture are:

Positives of cross ownership:

Cross ownership of shares is criticized for:

A major factor in perpetuating cross ownership of shares is a high capital gains tax rate. A company has less incentive to sell cross owned shares if taxes are high because of the immediate reduction in the value of the assets.

For example, a company owns $1000 of stock in another company that was originally purchased for $200. If the capital gains tax rate is 25% (like Germany) and the company sells the stock,
the company has $800 which is 20 percent less than before it sold the stock.

Long term cross ownership of shares combined with a high capital tax rate greatly increases periods of asset deflation both in time and in severity.

Cross ownership also refers to a type of media ownership in which one type of communications (say a newspaper) owns or is the sister company of another type of medium (such as a radio or TV station). One example is "The New York Times" 's former ownership of WQXR Radio and the "Chicago Tribune"'s similar relationship with WGN Radio (WGN-AM) and Television (WGN-TV).

The Federal Communications Commission generally does not allow cross ownership, to keep from one license holder having too much local media ownership, unless the license holder obtains a waiver, such as News Corporation and the Tribune Company have in New York.

The mid-1970s cross-ownership guidelines grandfathered already-existing crossownerships, such as "Tribune"-WGN, "New York Times"-WQXR and the "New York Daily News" ownership of WPIX Television and Radio.


</doc>
<doc id="5421" url="https://en.wikipedia.org/wiki?curid=5421" title="Cardiology">
Cardiology

Cardiology (from Greek "kardiā", "heart" and "-logia", "study") is a branch of medicine that deals with the disorders of the heart as well as some parts of the circulatory system. The field includes medical diagnosis and treatment of congenital heart defects, coronary artery disease, heart failure, valvular heart disease and electrophysiology. Physicians who specialize in this field of medicine are called cardiologists, a specialty of internal medicine. Pediatric cardiologists are pediatricians who specialize in cardiology. Physicians who specialize in cardiac surgery are called cardiothoracic surgeons or cardiac surgeons, a specialty of general surgery.

Although the cardiovascular system is inextricably linked to blood, cardiology is relatively unconcerned with hematology and its diseases. Some obvious exceptions that affect the function of the heart would be blood tests (electrolyte disturbances, troponins), decreased oxygen carrying capacity (anemia, hypovolemic shock), and coagulopathies.

All cardiologists study the disorders of the heart, but the study of adult and child heart disorders are through different training pathways. Therefore, an adult cardiologist (often simply called "cardiologist") is inadequately trained to take care of children, and pediatric cardiologists are not trained to take care of adult heart disease. The surgical aspects are not included in cardiology and are in the domain of cardiothoracic surgery. For example, coronary artery bypass surgery (CABG), cardiopulmonary bypass and valve replacement are surgical procedures performed by surgeons, not cardiologists. However the insertion of stents and pacemakers is performed by cardiologists

Cardiology is a specialty of internal medicine. To be a cardiologist in the United States, a three-year residency in internal medicine is followed by a three-year fellowship in cardiology. It is possible to specialize further in a sub-specialty. Recognized sub-specialties in the United States by the ACGME are cardiac electrophysiology, echocardiography, interventional cardiology, and nuclear cardiology. Recognized subspecialties in the United States by the American Osteopathic Association Bureau of Osteopathic Specialists (AOABOS) include clinical cardiac electrophysiology and interventional cardiology.

Per doximity, adult cardiologists make an average of $436,849 in the United States.

Cardiac electrophysiology is the science of elucidating, diagnosing, and treating the electrical activities of the heart. The term is usually used to describe studies of such phenomena by invasive (intracardiac) catheter recording of spontaneous activity as well as of cardiac responses to programmed electrical stimulation (PES). These studies are performed to assess complex arrhythmias, elucidate symptoms, evaluate abnormal electrocardiograms, assess risk of developing arrhythmias in the future, and design treatment. These procedures increasingly include therapeutic methods (typically radiofrequency ablation, or cryoablation) in addition to diagnostic and prognostic procedures. Other therapeutic modalities employed in this field include antiarrhythmic drug therapy and implantation of pacemakers and automatic implantable cardioverter-defibrillators (AICD).

The cardiac electrophysiology study (EPS) typically measures the response of the injured or cardiomyopathic myocardium to PES on specific pharmacological regimens in order to assess the likelihood that the regimen will successfully prevent potentially fatal sustained ventricular tachycardia (VT) or ventricular fibrillation VF (VF) in the future. Sometimes a "series" of EPS drug trials must be conducted to enable the cardiologist to select the one regimen for long-term treatment that best prevents or slows the development of VT or VF following PES. Such studies may also be conducted in the presence of a newly implanted or newly replaced cardiac pacemaker or AICD.

Clinical cardiac electrophysiology is a branch of the medical specialty of cardiology and is concerned with the study and treatment of rhythm disorders of the heart. Cardiologists with expertise in this area are usually referred to as electrophysiologists. Electrophysiologists are trained in the mechanism, function, and performance of the electrical activities of the heart. Electrophysiologists work closely with other cardiologists and cardiac surgeons to assist or guide therapy for heart rhythm disturbances (arrhythmias). They are trained to perform interventional and surgical procedures to treat cardiac arrhythmia.

The training required to become an electrophysiologist is long and requires 7 to 8 years after medical school (within the U.S.). Three years of internal medicine residency, three years of Clinical Cardiology fellowship, and one to two (in most instances) years of clinical cardiac electrophysiology.

Cardiogeriatrics, or geriatric cardiology, is the branch of cardiology and geriatric medicine that deals with the cardiovascular disorders in elderly people.

Cardiac disorders such as coronary heart disease, including myocardial infarction, heart failure, cardiomyopathy, and arrhythmias such as atrial fibrillation, are common and are a major cause of mortality in elderly people. Vascular disorders such as atherosclerosis and peripheral arterial disease cause significant morbidity and mortality in aged people.

Echocardiography uses standard two-dimensional, three-dimensional, and Doppler ultrasound to create images of the heart.

Echocardiography has become routinely used in the diagnosis, management, and follow-up of patients with any suspected or known heart diseases. It is one of the most widely used diagnostic tests in cardiology. It can provide a wealth of helpful information, including the size and shape of the heart (internal chamber size quantification), pumping capacity, and the location and extent of any tissue damage. An echocardiogram can also give physicians other estimates of heart function, such as a calculation of the cardiac output, ejection fraction, and diastolic function (how well the heart relaxes).

Echocardiography can help detect cardiomyopathies, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, and many others. The use of stress echocardiography may also help determine whether any chest pain or associated symptoms are related to heart disease. The biggest advantage to echocardiography is that it is not invasive (does not involve breaking the skin or entering body cavities) and has no known risks or side effects.

Interventional cardiology is a branch of cardiology that deals specifically with the catheter based treatment of structural heart diseases. A large number of procedures can be performed on the heart by catheterization. This most commonly involves the insertion of a sheath into the femoral artery (but, in practice, any large peripheral artery or vein) and cannulating the heart under X-ray visualization (most commonly Fluoroscopy).

The main advantages of using the interventional cardiology or radiology approach are the avoidance of the scars and pain, and long post-operative recovery. Additionally, interventional cardiology procedure of primary angioplasty is now the gold standard of care for an acute Myocardial infarction. This procedure can also be done proactively, when areas of the vascular system become occluded from Atherosclerosis. The Cardiologist will thread this sheath through the vascular system to access the heart. This sheath has a balloon and a tiny wire mesh tube wrapped around it, and if the cardiologist finds a blockage or Stenosis, they can inflate the balloon at the occlusion site in the vascular system to flatten or compress the plaque against the vascular wall. Once that is complete a Stent is placed as a type of scaffold to hold the vasculature open permanently.

In recent times, the focus is gradually shifting to Preventive cardiology due to increased Cardiovascular Disease burden at an early age. As per WHO, 37% of all premature deaths are due to cardiovascular diseases and out of this, 82% are in low and middle income countries. Clinical cardiology is the sub specialty of Cardiology which looks after preventive cardiology and cardiac rehabilitation. Preventive cardiology also deals with routine preventive checkup though non invasive tests specifically Electrocardiography, Stress Tests, Lipid Profile and General Physical examination to detect any cardiovascular diseases at an early age while cardiac rehabilitation is the upcoming branch of cardiology which helps a person regain his overall strength and live a normal life after a cardiovascular event.

Helen B. Taussig is known as the founder of pediatric cardiology. She became famous through her work with Tetralogy of Fallot, a congenital heart defect in which oxygenated and deoxygenated blood enters the circulatory system resulting from a ventricular septal defect (VSD) right beneath the aorta. This condition causes newborns to have a bluish-tint, cyanosis, and have a deficiency of oxygen to their tissues, hypoxemia. She worked with Alfred Blalock and Vivien Thomas at the Johns Hopkins Hospital where they experimented with dogs to look at how they would attempt to surgically cure these "blue babies." They eventually figured out how to do just that by the anastomosis of the systemic artery to the pulmonary artery and called this the Blalock-Taussig Shunt.

Tetralogy of Fallot, pulmonary atresia, double outlet right ventricle, transposition of the great arteries, persistent truncus arteriosus, and Ebsteins anomaly are various congenital cyanotic heart diseases. Congenital cyanotic heart diseases is where something is wrong with the heart of a newborn and it is not oxygenating the blood efficiently.
Tetralogy of Fallot is the most common congenital heart disease arising in 1–3 cases per 1,000 births. The cause of this defect is a ventricular septal defect (VSD) and an overriding aorta. These two defects combined causes deoxygenated blood to bypass the lungs and going right back into the circulatory system. The modified Blalock-Taussig shunt is usually used to fix the circulation. This procedure is done by placing a graft between the subclavian artery and the ipsilateral pulmonary artery to restore the correct blood flow.

Pulmonary Atresia happens in 7–8 per 100,000 births and is characterized by the aorta branching out of the right ventricle. This causes the deoxygenated blood to bypass the lungs and enter the circulatory system. Surgeries can fix this by redirecting the aorta and fixing the right ventricle and pulmonary artery connection.

There are two types of pulmonary atresia, classified by whether or not the baby also has a ventricular septal defect.

Double outlet right ventricle is when both great arteries, the pulmonary artery and the aorta, are connected to the right ventricle. There is usually a VSD in different particular places depending on the variations of DORV, typically 50% are subaortic and 30%. The surgeries that can be done to fix this defect can vary due to the different physiology and blood flow in the defected heart. One way it can be cured is by a VSD closure and placing conduits to restart the blood flow between the left ventricle and the aorta and between the right ventricle and the pulmonary artery. Another way is systemic-to-pulmonary artery shunt in cases associated with pulmonary stenosis. Also, a balloon atrial septostomy can be done to fix DORV with the Taussig-Bing anomaly.

There are two different types of transposition of the great arteries, Dextro-transposition of the great arteries and Levo-transposition of the great arteries, depending on where the chambers and vessels connect. Dextro-transposition happens in about 1 in 4,000 newborns and is when the right ventricle pumps blood into the aorta and deoxygenated blood enters the blood stream. The temporary procedure is to create an atrial septal defect (ASD). A permanent fix is more complicated and involves redirecting the pulmonary return to the right atrium and the systemic return to the left atrium, which is known as the Senning procedure. The Rastelli procedure can also be done by rerouting the left ventricular outflow, dividing the pulmonary trunk, and placing a conduit in between the right ventricle and pulmonary trunk. Levo-transposition happens in about 1 in 13,000 newborns and is characterized by the left ventricle pumping blood into the lungs and the right ventricle pumping the blood into the aorta. This may not produce problems at the beginning, but will eventually due to the different pressures each ventricle uses to pump blood. Switching the left ventricle to be the systemic ventricle and the right ventricle to pump blood into the pulmonary artery can repair levo-transposition.

Persistent truncus arteriosus is when the truncus arteriosus fails to split into the aorta and pulmonary trunk. This occurs in about 1 in 11,000 live births and allows both oxygenated and deoxygenated blood into the body. The repair consists of a VSD closure and the Rastelli procedure.

Ebstein's anomaly is characterized by a right atrium that is significantly enlarged and a heart that is shaped like a box. This is very rare and happens in less than 1% of congenital heart disease cases. The surgical repair varies depending on the severity of the disease.

Pediatric cardiology is a sub-specialty of pediatrics. To become a pediatric cardiologist in the United States, one must complete a three-year residency in pediatrics, followed by a three-year fellowship in pediatric cardiology. Per doximity, pediatric cardiologists make an average of $303,917 in the United States.

As the center focus of cardiology, the heart has numerous anatomical features (e.g., atria, ventricles, heart valves) and numerous physiological features (e.g., systole, heart sounds, afterload) that have been encyclopedically documented for many centuries.

Disorders of the heart lead to heart disease and cardiovascular disease and can lead to a significant number of deaths: cardiovascular disease is the leading cause of death in the United States and caused 24.95% of total deaths in 2008.

The primary responsibility of the heart is to pump blood throughout the body.
It pumps blood from the body — called the systemic circulation — through the lungs — called the pulmonary circulation — and then back out to the body.
This means that the heart is connected to and affects the entirety of the body. Simplified, the heart is a circuit of the Circulation.
While plenty is known about the healthy heart, the bulk of study in cardiology is in disorders of the heart and restoration, and where possible, of function.

The heart is a muscle that squeezes blood and functions like a pump.
Each part of the heart is susceptible to failure or dysfunction and the heart can be divided into the mechanical and the electrical parts.

The electrical part of the heart is centered on the periodic contraction (squeezing) of the muscle cells that is caused by the cardiac pacemaker located in the sinoatrial node.
The study of the electrical aspects is a sub-field of electrophysiology called cardiac electrophysiology and is epitomized with the electrocardiogram (ECG/EKG).
The action potentials generated in the pacemaker propagate throughout the heart in a specific pattern. The system that carries this potential is called the electrical conduction system.
Dysfunction of the electrical system manifests in many ways and may include Wolff–Parkinson–White syndrome, ventricular fibrillation, and heart block.

The mechanical part of the heart is centered on the fluidic movement of blood and the functionality of the heart as a pump.
The mechanical part is ultimately the purpose of the heart and many of the disorders of the heart disrupt the ability to move blood.
Failure to move sufficient blood can result in failure in other organs and may result in death if severe.
Heart failure is one condition in which the mechanical properties of the heart have failed or are failing, which means insufficient blood is being circulated.

Coronary circulation is the circulation of blood in the blood vessels of the heart muscle (myocardium). The vessels that deliver oxygen-rich blood to the myocardium are known as coronary arteries. The vessels that remove the deoxygenated blood from the heart muscle are known as cardiac veins. These include the great cardiac vein, the middle cardiac vein, the small cardiac vein and the anterior cardiac veins.

As the left and right coronary arteries run on the surface of the heart, they can be called epicardial coronary arteries. These arteries, when healthy, are capable of autoregulation to maintain coronary blood flow at levels appropriate to the needs of the heart muscle. These relatively narrow vessels are commonly affected by atherosclerosis and can become blocked, causing angina or a heart attack. (See also: circulatory system.) The coronary arteries that run deep within the myocardium are referred to as subendocardial.

The coronary arteries are classified as "end circulation", since they represent the only source of blood supply to the myocardium; there is very little redundant blood supply, which is why blockage of these vessels can be so critical.

The cardiac examination (also called the "precordial exam"), is performed as part of a physical examination, or when a patient presents with chest pain suggestive of a cardiovascular pathology. It would typically be modified depending on the indication and integrated with other examinations especially the respiratory examination.

Like all medical examinations, the cardiac examination follows the standard structure of inspection, palpation and auscultation.

Cardiology is concerned with the normal functionality of the heart and the deviation from a healthy heart.
Many disorders involve the heart itself but some are outside of the heart and in the vascular system.
Collectively, the two together are termed the cardiovascular system and diseases of one part tend to affect the other.

Hypertension, also known as "high blood pressure"", is a long term medical condition in which the blood pressure in the arteries is persistently elevated. High blood pressure usually does not cause symptoms. Long term high blood pressure, however, is a major risk factor for coronary artery disease, stroke, heart failure, peripheral vascular disease, vision loss, and chronic kidney disease.
Lifestyle factors can increase the risk of hypertension. These include excess salt in the diet, excess body weight, smoking, and alcohol. Hypertension can also be caused by other diseases, or as a side-effect of drugs.
Blood pressure is expressed by two measurements, the systolic and diastolic pressures, which are the maximum and minimum pressures, respectively. Normal blood pressure at rest is within the range of 100–140 millimeters mercury (mmHg) systolic and 60–90 mmHg diastolic. High blood pressure is present if the resting blood pressure is persistently at or above 140/90 mmHg for most adults. Different numbers apply to children. Ambulatory blood pressure monitoring over a 24-hour period appears more accurate than office best blood pressure measurement.
Lifestyle changes and medications can lower blood pressure and decrease the risk of health complications. Lifestyle changes include weight loss, decreased salt intake, physical exercise, and a healthy diet. If lifestyle changes are not sufficient then blood pressure medications are used. Up to three medications can control blood pressure in 90% of people. The treatment of moderate to severe high arterial blood pressure (defined as >160/100 mmHg) with medications is associated with an improved life expectancy and reduced morbidity. The effect of treatment of blood pressure between 140/90 mmHg and 160/100 mmHg is less clear, with some reviews finding benefit and others finding a lack of evidence for benefit. High blood pressure affects between 16 and 37% of the population globally. In 2010 hypertension was believed to have been a factor in 18% (9.4 million) deaths.

Essential hypertension is the form of hypertension that by definition has no identifiable cause. It is the most common type of hypertension, affecting 95% of hypertensive patients, it tends to be familial and is likely to be the consequence of an interaction between environmental and genetic factors. Prevalence of essential hypertension increases with age, and individuals with relatively high blood pressure at younger ages are at increased risk for the subsequent development of hypertension.
Hypertension can increase the risk of cerebral, cardiac, and renal events.

Secondary hypertension is a type of hypertension which is caused by an identifiable underlying secondary cause. It is much less common than essential hypertension, affecting only 5% of hypertensive patients. It has many different causes including endocrine diseases, kidney diseases, and tumors. It also can be a side effect of many medications.

Complications of hypertension are clinical outcomes that result from persistent elevation of blood pressure. Hypertension is a risk factor for all clinical manifestations of atherosclerosis since it is a risk factor for atherosclerosis itself. It is an independent predisposing factor for heart failure, coronary artery disease, stroke, renal disease, and peripheral arterial disease. It is the most important risk factor for cardiovascular morbidity and mortality, in industrialized countries.

Cardiac arrhythmia
Cardiac arrhythmia, also known as "cardiac dysrhythmia" or "irregular heartbeat", is a group of conditions in which the heartbeat is irregular, too fast, or too slow. A heart rate that is too fast – above 100 beats per minute in adults – is called tachycardia and a heart rate that is too slow – below 60 beats per minute – is called bradycardia. Many types of arrhythmia have no symptoms. When symptoms are present these may include palpitations or feeling a pause between heartbeats. More seriously there may be lightheadedness, passing out, shortness of breath, or chest pain. While most types of arrhythmia are not serious, some predispose a person to complications such as stroke or heart failure. Others may result in cardiac arrest.
There are four main types of arrhythmia: extra beats, supraventricular tachycardias, ventricular arrhythmias, and bradyarrhythmias. Extra beats include premature atrial contractions, premature ventricular contractions, and premature junctional contractions. Supraventricular tachycardias include atrial fibrillation, atrial flutter, and paroxysmal supraventricular tachycardia. Ventricular arrhythmias include ventricular fibrillation and ventricular tachycardia. Arrhythmias are due to problems with the electrical conduction system of the heart. Arrhythmias may occur in children; however, the normal range for the heart rate is different and depends on age. A number of tests can help with diagnosis including an electrocardiogram (ECG) and Holter monitor.
Most arrhythmias can be effectively treated. Treatments may include medications, medical procedures such as a pacemaker, and surgery. Medications for a fast heart rate may include beta blockers or agents that attempt to restore a normal heart rhythm such as procainamide. This later group may have more significant side effects especially if taken for a long period of time. Pacemakers are often used for slow heart rates. Those with an irregular heartbeat are often treated with blood thinners to reduce the risk of complications. Those who have severe symptoms from an arrhythmia may receive urgent treatment with a jolt of electricity in the form of cardioversion or defibrillation.
Arrhythmia affects millions of people. In Europe and North America, as of 2014, atrial fibrillation affects about 2% to 3% of the population. Atrial fibrillation and atrial flutter resulted in 112,000 deaths in 2013, up from 29,000 in 1990. Sudden cardiac death is the cause of about half of deaths due to cardiovascular disease or about 15% of all deaths globally. About 80% of sudden cardiac death is the result of ventricular arrhythmias. Arrhythmias may occur at any age but are more common among older people.

Coronary artery disease, also known as "ischemic heart disease", is a group of diseases that includes: stable angina, unstable angina, myocardial infarction, and sudden cardiac death. It is within the group of cardiovascular diseases of which it is the most common type. A common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck, or jaw. Occasionally it may feel like heartburn. Usually symptoms occur with exercise or emotional stress, last less than a few minutes, and get better with rest. Shortness of breath may also occur and sometimes no symptoms are present. The first sign is occasionally a heart attack. Other complications include heart failure or an irregular heartbeat.

Risk factors include: high blood pressure, smoking, diabetes, lack of exercise, obesity, high blood cholesterol, poor diet, and excessive alcohol, among others. Other risks include depression. The underlying mechanism involves atherosclerosis of the arteries of the heart. A number of tests may help with diagnoses including: electrocardiogram, cardiac stress testing, coronary computed tomographic angiography, and coronary angiogram, among others.
Prevention is by eating a healthy diet, regular exercise, maintaining a healthy weight and not smoking. Sometimes medication for diabetes, high cholesterol, or high blood pressure are also used. There is limited evidence for screening people who are at low risk and do not have symptoms. Treatment involves the same measures as prevention. Additional medications such as antiplatelets including aspirin, beta blockers, or nitroglycerin may be recommended. Procedures such as percutaneous coronary intervention (PCI) or coronary artery bypass surgery (CABG) may be used in severe disease. In those with stable CAD it is unclear if PCI or CABG in addition to the other treatments improve life expectancy or decreases heart attack risk.
In 2013 CAD was the most common cause of death globally, resulting in 8.14 million deaths (16.8%) up from 5.74 million deaths (12%) in 1990. The risk of death from CAD for a given age has decreased between 1980 and 2010 especially in developed countries. The number of cases of CAD for a given age has also decreased between 1990 and 2010. In the United States in 2010 about 20% of those over 65 had CAD, while it was present in 7% of those 45 to 64, and 1.3% of those 18 to 45. Rates are higher among men than women of a given age.

Cardiac arrest is a sudden stop in effective blood flow due to the failure of the heart to contract effectively. Symptoms include loss of consciousness and abnormal or absent breathing. Some people may have chest pain, shortness of breath, or nausea before this occurs. If not treated within minutes, death usually occurs.
The most common cause of cardiac arrest is coronary artery disease. Less common causes include major blood loss, lack of oxygen, very low potassium, heart failure, and intense physical exercise. A number of inherited disorders may also increase the risk including long QT syndrome. The initial heart rhythm is most often ventricular fibrillation. The diagnosis is confirmed by finding no pulse. While a cardiac arrest may be caused by heart attack or heart failure these are not the same.
Prevention includes not smoking, physical activity, and maintaining a healthy weight. Treatment for cardiac arrest is immediate cardiopulmonary resuscitation (CPR) and, if a shockable rhythm is present, defibrillation. Among those who survive targeted temperature management may improve outcomes. An implantable cardiac defibrillator may be placed to reduce the chance of death from recurrence.
In the United States, cardiac arrest outside of hospital occurs in about 13 per 10,000 people per year (326,000 cases). In hospital cardiac arrest occurs in an additional 209,000 Cardiac arrest becomes more common with age. It affects males more often than females. The percentage of people who survive with treatment is about 8%. Many who survive have significant disability. Many U.S. television shows, however, have portrayed unrealistically high survival rates of 67%.

A congenital heart defect, also known as a "congenital heart anomaly" or "congenital heart disease", is a problem in the structure of the heart that is present at birth. Signs and symptoms depend on the specific type of problem. Symptoms can vary from none to life-threatening. When present they may include rapid breathing, bluish skin, poor weight gain, and feeling tired. It does not cause chest pain. Most congenital heart problems do not occur with other diseases. Complications that can result from heart defects include heart failure.
The cause of a congenital heart defect is often unknown. Certain cases may be due to infections during pregnancy such as rubella, use of certain medications or drugs such as alcohol or tobacco, parents being closely related, or poor nutritional status or obesity in the mother. Having a parent with a congenital heart defect is also a risk factor. A number of genetic conditions are associated with heart defects including Down syndrome, Turner syndrome, and Marfan syndrome. Congenital heart defects are divided into two main groups: cyanotic heart defects and non-cyanotic heart defects, depending on whether the child has the potential to turn bluish in color. The problems may involve the interior walls of the heart, the heart valves, or the large blood vessels that lead to and from the heart.
Congenital heart defects are partly preventable through rubella vaccination, the adding of iodine to salt, and the adding of folic acid to certain food products. Some defects do not need treatment. Other may be effectively treated with catheter based procedures or heart surgery. Occasionally a number of operations may be needed. Occasionally heart transplantation is required. With appropriate treatment outcomes, even with complex problems, are generally good.
Heart defects are the most common birth defect. In 2013 they were present in 34.3 million people globally. They affect between 4 and 75 per 1,000 live births depending upon how they are diagnosed. About 6 to 19 per 1,000 cause a moderate to severe degree of problems. Congenital heart defects are the leading cause of birth defect-related deaths. In 2013 they resulted in 323,000 deaths down from 366,000 deaths in 1990.

Diagnostic tests in cardiology are the methods of identifying heart conditions associated with healthy vs. unhealthy, pathologic heart function. The starting point is obtaining a medical history, followed by Auscultation. Then blood tests, electrophysiological procedures, and cardiac imaging can be ordered for further analysis. Electrophysiological procedures include electrocardiogram, cardiac monitoring, cardiac stress testing, and the electrophysiology study.

Also, HeartScore exists





London Cardiologists



</doc>
<doc id="5422" url="https://en.wikipedia.org/wiki?curid=5422" title="Capcom">
Capcom

Capcom's predecessor, I.R.M. Corporation, was founded on May 30, 1979 by Kenzo Tsujimoto. Tsujimoto was still president of Irem Corporation when he founded I.R.M. Tsujimoto worked concomitantly in both companies until leaving the former in 1983.

The original companies that spawned Capcom's Japanese branch were I.R.M. as well as its subsidiary Japan Capsule Computers Co., Ltd., both of which were devoted to the manufacturing and distribution of electronic game machines. The two companies underwent a name change to Sambi Co., Ltd. in September 1981, while Capcom Co., Ltd. was first established on June 11, 1983 by Kenzo Tsujimoto, for the purpose of taking over the internal sales department.

In January 1989, the old affiliate company Capcom Co., Ltd. merged with Sambi Co., Ltd., resulting in the current Japanese branch. The name Capcom is a clipped compound of "Capsule Computers", a term coined by the company to describe the arcade machines it solely manufactured in its early years, designed to set themselves apart from personal computers that were becoming widespread at that time. The word capsule alludes to how Capcom likened its game software to "a capsule packed to the brim with gaming fun", as well as to the company's desire to protect its intellectual property with a hard outer shell, preventing illegal copies and inferior imitations.

While Capcom's first product was the coin-operated "Little League" from July 1983, its first real video game, the arcade title "Vulgus", was released in May 1984. Beginning with a Nintendo Entertainment System port of "1942" published in December 1985, the company started to venture into the market of home console video games, which became its main business segment a few years later. Its division Capcom USA had a brief stint in the late 1980s as a video game publisher for the Commodore 64 and IBM PC DOS computers although the development of these arcade ports were handled by other companies. Capcom has created 15 multi-million-selling game series, the most successful of which is "Resident Evil".

Capcom has been noted as the last major publisher to be committed to 2D games, though this was not entirely by choice. The company's commitment to the Super Nintendo Entertainment System as its platform of choice caused them to lag behind other leading publishers in developing 3D-capable arcade boards. In addition, the 2D animated cartoon-style graphics seen in games such as "" and "" proved popular, leading Capcom to adopt it as a signature style and use it in more games.

In 1994, Capcom adapted its "Street Fighter" series of fighting games into a film of the same name. While commercially successful, it was critically panned. A 2002 adaptation of its "Resident Evil" series faced similar criticism but was also successful in theaters. The company sees films as a way to build sales for its video games.

Capcom partnered with Nyu Media in 2011 to publish and distribute the Japanese independent (dōjin soft) games that Nyu localized into the English language. The company works with the Polish localization company QLoc to port Capcom's games to other platforms, notably examples are ""s PC version and its PlayStation 4 and Xbox One remasters, "Dragon's Dogma"s PC version released in January 2016, and "Dead Rising"s version on PlayStation 4, Xbox One and PC released on September 13, 2016.

On August 27, 2014, Capcom filed a patent infringement lawsuit against Koei Tecmo Games at the Osaka District Court for 980 million yen in damage. Capcom claimed Koei Tecmo Games infringed a patent it obtained in 2002 regarding a play feature in video games.

In the first few years after its establishment, the Japanese branch of Capcom had three development groups referred to as "Planning Rooms", led by Tokuro Fujiwara, Takashi Nishiyama and Yoshiki Okamoto, respectively. Later, games developed internally used to be created by several numbered "Production Studios", each assigned to different games. Starting in 2002, the development process was reformed to better share technologies and expertise, and all of the individual studios were gradually restructured into bigger departments responsible for different tasks. While there are self-contained departments for the creation of arcade, pachinko and pachislo, online, and mobile games, the Consumer Games R&D Division instead is an amalgamation of subsections in charge of various game development stages.

Capcom has three internal divisions to make games. Those are Consumer games division 1 with "Resident Evil", "Devil May Cry", "Dead Rising", and other worldwide franchises (usually targeted towards North American and European audiences), Consumer games division 2 with "Street Fighter", "Marvel vs. Capcom", and other online focused franchises (usually targeted towards worldwide audiences), and Consumer games division 3 with "Monster Hunter", "Ace Attorney", and other franchises with more traditional IP (usually targeted towards Japanese audiences).

In addition to these internal teams, Capcom also commissions outside development studios to ensure a steady output of titles. However, following poor sales of "Dark Void" and "Bionic Commando", the company's management has decided to limit outsourcing to sequels and newer versions of installments in existing franchises, reserving the development of original titles for its in-house teams. The production of games, budgets, and platforms supported are decided upon in development approval meetings, attended by the company management and the marketing, sales, and quality control departments.

Apart from the head office building and the R&D building of Capcom Co., Ltd., both located in Chūō-ku, Osaka, the Japanese parent company also has a branch office in the Shinjuku Mitsui Building in Nishi-Shinjuku, Shinjuku, Tokyo. It also has the Ueno Facility, a branch office in Iga, Mie Prefecture.

The international Capcom Group encompasses 15 subsidiaries in Japan, North America, Europe, and East Asia. Affiliated companies include Koko Capcom Co., Ltd. in South Korea, Street Fighter Film, LLC in the United States, and Dellgamadas Co., Ltd.

In addition to the development and publishing of home, online, mobile, arcade, pachinko, and pachislo games, the company publishes strategy guides, maintains its own arcade centers in Japan known as "Plaza Capcom", and licenses its franchise and character properties for use in tie-in products, movies, television series, and stage performances.

Suleputer, an in-house marketing and music label established in cooperation with Sony Music Entertainment Intermedia in 1998, publishes CDs, DVDs, and other media based on Capcom's games. An annual private media summit called Captivate, renamed from Gamers Day in 2008, is traditionally used as a platform for new game and business announcements.

Capcom started its "Street Fighter" franchise in 1987. The series of fighting games are among the most popular in their genre. Having sold over 30 million units, the series serves as Capcom's flagship franchise. That same year, the company introduced its "Mega Man" series, which also sells nearly 30 million units.

The company released the first entry in its "Resident Evil" survival horror series in 1996. The series has achieved financial success, selling over 90 million units. Following work on the second entry in the "Resident Evil" series, Capcom began work on a "Resident Evil" game for the PlayStation 2. Radically different from the existing series, Capcom decided to spin off the game into its own series, "Devil May Cry". While it released the first two entries exclusively for the PlayStation 2, the company brought further entries to non-Sony consoles. The series as a whole has seen sales in excess of 10 million units. Capcom began its "Monster Hunter" series in 2004. The series has seen sales of over 45 million units on a variety of consoles.

Although the company often relies on existing franchises, it also published and developed several titles for the Xbox 360, PlayStation 3 and Wii, based on original intellectual property: "", "Dead Rising", "Dragon's Dogma", "Asura's Wrath" and "Zack and Wiki". During this period, Capcom also helped publish several original titles from up and coming Western developers with titles like "Remember Me", "Dark Void" and "Spyborgs", titles that many other publishers were not willing to take a chance on. Also of note are the titles "Ōkami", "Ōkamiden" and "". Currently, Capcom is working on its latest new intellectual property, "Deep Down", for the PlayStation 4.

Capcom compiles a list, which is updated on quarterly basis, of its games that have exceeded one million copies sold, called "Platinum Titles". The list contains over 90 video games, here are the top ten titles by sold copies as of September 30, 2019.

In 2012, Capcom was criticized for controversial sales tactics, such as having to pay for additional content which is already available within the game's files, most notably in "Street Fighter X Tekken". Capcom has defended the practice. The company has been criticized for other business decisions, such as not releasing certain games outside Japan (most notably the "Sengoku BASARA" series), abruptly cancelling anticipated projects (most notably "Mega Man Legends 3"), and shutting down Clover Studio. In 2015, the company pulled the PlayStation 4 version of "Ultra Street Fighter IV" from the Capcom Pro Tour due to numerous technical issues and gameplay bugs. In 2016, Capcom released "Street Fighter V" with very limited single player content. At launch, there were stability issues with the game's network that booted players mid-game even when they were not playing in an online mode. "Street Fighter V" failed to meet its sales target of 2 million in March 2016.




</doc>
<doc id="5428" url="https://en.wikipedia.org/wiki?curid=5428" title="History of Cambodia">
History of Cambodia

The history of Cambodia, a country in mainland Southeast Asia, can be traced back to at least the 5th millennium BCE. Detailed records of a political structure on the territory of what is now Cambodia first appear in Chinese annals in reference to Funan, a polity that encompassed the southernmost part of the Indochinese peninsula during the 1st to 6th centuries. Centered at the lower Mekong, Funan is noted as the oldest regional Hindu culture, which suggests prolonged socio-economic interaction with maritime trading partners of the Indosphere in the west. By the 6th century a civilisation, called Chenla or Zhenla in Chinese annals, firmly replaced Funan, as it controlled larger, more undulating areas of Indochina and maintained more than a singular centre of power.

The Khmer Empire was established by the early 9th century. Sources refer here to a mythical initiation and consecration ceremony to claim political legitimacy by founder Jayavarman II at Mount Kulen (Mount Mahendra) in 802 CE. A succession of powerful sovereigns, continuing the Hindu devaraja cult tradition, reigned over the classical era of Khmer civilization until the 11th century. A new dynasty of provincial origin introduced Buddhism, which according to some scholars resulted in royal religious discontinuities and general decline. The royal chronology ends in the 14th century. Great achievements in administration, agriculture, architecture, hydrology, logistics, urban planning and the arts are testimony to a creative and progressive civilisation - in its complexity a cornerstone of Southeast Asian cultural legacy.

The decline continued through a transitional period of approximately 100 years followed by the Middle Period of Cambodian history, also called the Dark ages of Cambodia, beginning in the mid 15th century. Although the Hindu cults had by then been all but replaced, the monument sites at the old capital remained an important spiritual centre.
Yet since the mid 15th century the core population steadily moved to the east and – with brief exceptions – settled at the confluence of the Mekong and Tonle Sap rivers at Chaktomuk, Longvek and Oudong.

Maritime trade was the basis for a very prosperous 16th century. But, as a result foreigners – Muslim Malays and Cham, Christian European adventurers and missionaries – increasingly disturbed and influenced government affairs. Ambiguous fortunes, a robust economy on the one hand and a disturbed culture and compromised royalty on the other were constant features of the Longvek era.

By the 15th century, the Khmers' traditional neighbours, the Mon people in the west and the Cham people in the east had gradually been pushed aside or replaced by the resilient Siamese/Thai and Annamese/Vietnamese, respectively. These powers had perceived, understood and increasingly followed the imperative of controlling the lower Mekong basin as the key to control all Indochina. A weak Khmer kingdom only encouraged the strategists in Ayutthaya (later in Bangkok) and in Huế. Attacks on and conquests of Khmer royal residences left sovereigns without a ceremonial and legitimate power base. Interference in succession and marriage policies added to the decay of royal prestige. Oudong was established in 1601 as the last royal residence of the Middle Period.

The 19th-century arrival of then technologically more advanced and ambitious European colonial powers with concrete policies of global control put an end to regional feuds and as Siam/Thailand, although humiliated and on the retreat, escaped colonisation as a buffer state, Vietnam was to be the focal point of French colonial ambition.

After 80 years of colonial hibernation, the brief episode of Japanese occupation during World War II, that coincided with the investiture of king Sihanouk was the opening act for the irreversible process towards re-emancipation and modern Cambodian history.
The Kingdom of Cambodia (1953–70), independent since 1953, struggled to remain neutral in a world shaped by polarisation of the nuclear powers USA and Soviet Union.
As the Indochinese war escalates, Cambodia becomes increasingly involved, the Khmer Republic is one of the results in 1970, another is civil war. 1975, abandoned and in the hands of the Khmer Rouge, Cambodia endures its darkest hour – Democratic Kampuchea and its long aftermath of Vietnamese occupation, the People's Republic of Kampuchea and the UN Mandate towards Modern Cambodia since 1993.

Radiocarbon dating of a cave at Laang Spean in Battambang Province, northwest Cambodia confirmed the presence of Hoabinhian stone tools from 6000–7000 BCE and pottery from 4200 BCE. Starting in 2009 archaeological research of the "Franco-Cambodian Prehistoric Mission" has documented a complete cultural sequence from 71.000 years BP to the Neolithic period in the cave. Finds since 2012 lead to the common interpretation, that the cave contains the archaeological remains of a first occupation by hunter and gatherer groups, followed by Neolithic people with highly developed hunting strategies and stone tool making techniques, as well as highly artistic pottery making and design, and with elaborate social, cultural, symbolic and exequial practices.

Skulls and human bones found at Samrong Sen in Kampong Chhnang Province date from 1500 BCE. Heng Sophady (2007) has drawn comparisons between Samrong Sen and the circular earthwork sites of eastern Cambodia. These people may have migrated from South-eastern China to the Indochinese Peninsula. Scholars trace the first cultivation of rice and the first bronze making in Southeast Asia to these people.

2010 Examination of skeletal material from graves at Phum Snay in north-west Cambodia revealed an exceptionally high number of injuries, especially to the head, likely to have been caused by interpersonal violence. The graves also contain a quantity of swords and other offensive weapons used in conflict.

The Iron Age period of Southeast Asia begins around 500 BCE and lasts until the end of the Funan era - around 500 A.D. as it provides the first concrete evidence for sustained maritime trade and socio-political interaction with India and South Asia. By the 1st century settlers have developed complex, organised societies and a varied religious cosmology, that required advanced spoken languages very much related to those of the present day. The most advanced groups lived along the coast and in the lower Mekong River valley and the delta regions in houses on stilts where they cultivated rice, fished and kept domesticated animals.

Chinese annals contain detailed records of the first known organised polity, the Kingdom of Funan, on Cambodian and Vietnamese territory characterised by "high population and urban centers, the production of surplus food...socio-political stratification [and] legitimized by Indian religious ideologies". Centered around the lower Mekong and Bassac rivers from the first to sixth century CE with "walled and moated cities" such as Angkor Borei in Takeo Province and Óc Eo in modern An Giang Province, Vietnam.

Early Funan was composed of loose communities, each with its own ruler, linked by a common culture and a shared economy of rice farming people in the hinterland and traders in the coastal towns, who were economically interdependent, as surplus rice production found its way to the ports.

By the second century CE Funan controlled the strategic coastline of Indochina and the maritime trade routes. Cultural and religious ideas reached Funan via the Indian Ocean trade route. Trade with India had commenced well before 500 BCE as Sanskrit hadn't yet replaced Pali. Funan's language has been determined as to have been an early form of Khmer and its written form was Sanskrit.

In the period 245–250 CE dignitaries of the Chinese Kingdom of Wu visited the Funan city Vyadharapura. Envoys Kang Tai and Zhu Ying defined Funan as to be a distinct Hindu culture. Trade with China had begun after the southward expansion of the Han Dynasty, around the 2nd century BCE Effectively Funan "controlled strategic land routes in addition to coastal areas" and occupied a prominent position as an "economic and administrative hub" between The Indian Ocean trade network and China, collectively known as the Maritime Silk Road. Trade routes, that eventually ended in distant Rome are corroborated by Roman and Persian coins and artefacts, unearthed at archaeological sites of 2nd and 3rd century settlements.

Funan is associated with myths, such as the Kattigara legend and the Khmer founding legend in which an Indian Brahman or prince named Preah Thaong in Khmer, Kaundinya in Sanskrit and Hun-t’ien in Chinese records marries the local ruler, a princess named Nagi Soma (Lieu-Ye in Chinese records), thus establishing the first Cambodian royal dynasty.

Scholars debate as to how deep the narrative is rooted in actual events and on Kaundinya's origin and status. A Chinese document, that underwent 4 alterations and a 3rd-century epigraphic inscription of Champa are the contemporary sources. Some scholars consider the story to be simply an allegory for the diffusion of Indic Hindu and Buddhist beliefs into ancient local cosmology and culture whereas some historians dismiss it chronologically.

Chinese annals report that Funan reached its territorial climax in the early 3rd century under the rule of king Fan Shih-man, extending as far south as Malaysia and as far west as Burma. A system of mercantilism in commercial monopolies was established. Exports ranged from forest products to precious metals and commodities such as gold, elephants, ivory, rhinoceros horn, kingfisher feathers, wild spices like cardamom, lacquer, hides and aromatic wood. Under Fan Shih-man Funan maintained a formidable fleet and was administered by an advanced bureaucracy, based on a "tribute-based economy, that produced a surplus which was used to support foreign traders along its coasts and ostensibly to launch expansionist missions to the west and south".

Historians maintain contradicting ideas about Funan's political status and integrity. Miriam T. Stark calls it simply Funan: [The]"notion of Fu Nan as an early "state"...has been built largely by historians using documentary and historical evidence" and Michael Vickery remarks: "Nevertheless, it is...unlikely that the several ports constituted a unified state, much less an 'empire'". Other sources though, imply imperial status: "Vassal kingdoms spread to southern Vietnam in the east and to the Malay peninsula in the west" and "Here we will look at two empires of this period...Funan and Srivijaya".

The question of how Funan came to an end is in the face of almost universal scholarly conflict impossible to pin down. Chenla is the name of Funan's successor in Chinese annals, first appearing in 616/617 CE

The archaeological approach to and interpretation of the entire early historic period is considered to be a decisive supplement for future research. The "Lower Mekong Archaeological Project" focuses on the development of political complexity in this region during the early historic period. LOMAP survey results of 2003 to 2005, for example, have helped to determine that "...the region’s importance continued unabated throughout the pre-Angkorian period...and that at least three [surveyed areas] bear Angkorian-period dates and suggest the continued importance of the delta."

The History of the Chinese Sui dynasty contains records that a state called Chenla sent an embassy to China in 616 or 617 CE It says, that Chenla was a vassal of Funan, but under its ruler Citrasena-Mahendravarman conquered Funan and gained independence.

Most of the Chinese recordings on Chenla, including that of Chenla conquering Funan, have been contested since the 1970s as they are generally based on single remarks in the Chinese annals, as author Claude Jacques emphasised the very vague character of the Chinese terms 'Funan' and 'Chenla', while more domestic epigraphic sources become available. Claude Jacques summarises: "Very basic historical mistakes have been made" because "the history of pre-Angkorean Cambodia was reconstructed much more on the basis of Chinese records than on that of [Cambodian] inscriptions" and as new inscriptions were discovered, researchers "preferred to adjust the newly discovered facts to the initial outline rather than to call the Chinese reports into question".

The notion of Chenla's centre being in modern Laos has also been contested. "All that is required is that it be inland from Funan." The most important political record of pre-Angkor Cambodia, the inscription K53 from Ba Phnom, dated 667 CE does not indicate any political discontinuity, either in royal succession of kings Rudravarman, Bhavavarman I, Mahendravarman [Citrasena], Īśānavarman, and Jayavarman I or in the status of the family of officials who produced the inscription. Another inscription of a few years later, K44, 674 CE, commemorating a foundation in Kampot province under the patronage of Jayavarman I, refers to an earlier foundation in the time of King Raudravarma, presumably Rudravarman of Funan, and again there is no suggestion of political discontinuity.

The History of the T'ang asserts that shortly after 706 the country was split into Land Chenla and Water Chenla. The names signify a northern and a southern half, which may conveniently be referred to as Upper and Lower Chenla.

By the late 8th century Water Chenla had become a vassal of the Sailendra dynasty of Java – the last of its kings were killed and the polity incorporated into the Javanese monarchy around 790 CE. Land Chenla acquired independence under Jayavarman II in 802 CE

The Khmers, vassals of Funan, reached the Mekong river from the northern Menam River via the Mun River Valley. Chenla, their first independent state developed out of Funanese influence.

Ancient Chinese records mention two kings, Shrutavarman and Shreshthavarman who ruled at the capital Shreshthapura located in modern-day southern Laos. The immense influence on the identity of Cambodia to come was wrought by the Khmer Kingdom of Bhavapura, in the modern day Cambodian city of Kampong Thom. Its legacy was its most important sovereign, Ishanavarman who completely conquered the kingdom of Funan during 612–628. He chose his new capital at the Sambor Prei Kuk, naming it Ishanapura.

The six centuries of the Khmer Empire are characterised by unparalleled technical and artistic progress and achievements, political integrity and administrative stability. The empire represents the cultural and technical apogee of the Cambodian and Southeast Asian pre-industrial civilisation.

The Khmer Empire was preceded by Chenla, a polity with shifting centres of power, which was split into Land Chenla and Water Chenla in the early 8th century. By the late 8th century Water Chenla was absorbed by the Malays of the Srivijaya Empire and the Javanese of the Shailandra Empire and eventually incorporated into Java and Srivijaya.
Jayavarman II, ruler of Land Chenla, initiates a mythical Hindu consecration ceremony at Mount Kulen (Mount Mahendra) in 802 CE, intended to proclaim political autonomy and royal legitimacy. As he declared himself devaraja - god-king, divinely appointed and uncontested, he simultaneously declares independence from Shailandra and Srivijaya. He established Hariharalaya, the first capital of the Angkorean area near the modern town of Roluos.

Indravarman I (877–889) and his son and successor Yasovarman I (889–900), who established the capital Yasodharapura ordered the construction of huge water reservoirs (barays) north of the capital. The water management network depended on elaborate configurations of channels, ponds, and embankments built from huge quantities of clayey sand, the available bulk material on the Angkor plain. Dikes of the East Baray still exist today, which are more than long and wide. The largest component is the West Baray, a reservoir about long and across, containing approximately 50 million m of water.

Royal administration was based on the religious idea of the Shivaite Hindu state and the central cult of the sovereign as warlord and protector – the "Varman". This centralised system of governance appointed royal functionaries to provinces. The Mahidharapura dynasty – its first king was Jayavarman VI (1080 to 1107), which originated west of the Dângrêk Mountains in the Mun river valley discontinued the old "ritual policy", genealogical traditions and crucially, Hinduism as exclusive state religion. Some historians relate the empires' decline to these religious discontinuities.

The area that comprises the various capitals was spread out over around , it is nowadays commonly called Angkor. The combination of sophisticated wet-rice agriculture, based on an engineered irrigation system and the Tonlé Sap's spectacular abundance in fish and aquatic fauna, as protein source guaranteed a regular food surplus. Recent Geo-surveys have confirmed that Angkor maintained the largest pre-industrial settlement complex worldwide during the 12th and 13th centuries – some three quarters of a million people lived there. Sizeable contingents of the public workforce were to be redirected to monument building and infrastructure maintenance. A growing number of researchers relates the progressive over-exploitation of the delicate local eco-system and its resources alongside large scale deforestation and resulting erosion to the empires' eventual decline.
Under king Suryavarman II (1113–1150) the empire reached its greatest geographic extent as it directly or indirectly controlled Indochina, the Gulf of Thailand and large areas of northern maritime Southeast Asia. Suryavarman II commissioned the temple of Angkor Wat, built in a period of 37 years, its five towers representing Mount Meru is considered to be the most accomplished expression of classical Khmer architecture. However, territorial expansion ended when Suryavarman II was killed in battle attempting to invade Đại Việt. It was followed by a period of dynastic upheaval and a Cham invasion that culminated in the sack of Angkor in 1177.
King Jayavarman VII (reigned 1181–1219) is generally considered to be Cambodia's greatest King. A Mahayana Buddhist, he initiates his reign by striking back against Champa in a successful campaign. During his nearly forty years in power he becomes the most prolific monument builder, who establishes the city of Angkor Thom with its central temple the Bayon. Further outstanding works are attributed to him – Banteay Kdei, Ta Prohm, Neak Pean and Sra Srang. The construction of an impressive number of utilitarian and secular projects and edifices, such as maintenance of the extensive road network of Suryavarman I, in particular the royal road to Phimai and the many rest houses, bridges and hospitals make Jayavarman VII unique among all imperial rulers.

In August 1296, the Chinese diplomat Zhou Daguan arrived at Angkor and remained at the court of king Srindravarman until July 1297. He wrote a detailed report, "The Customs of Cambodia", on life in Angkor. His portrayal is one of the most important sources of understanding historical Angkor as the text offers valuable information on the everyday life and the habits of the inhabitants of Angkor.

The last Sanskrit inscription is dated 1327, and records the succession of Indrajayavarman by Jayavarman IX Parameshwara (1327–1336).

The empire was an agrarian state that consisted essentially of three social classes, the elite, workers and slaves. The elite included advisers, military leaders, courtiers, priests, religious ascetics and officials. Workers included agricultural labourers and also a variety of craftsman for construction projects. Slaves were often captives from military campaigns or distant villages. Coinage did not exist and the barter economy was based on agricultural produce, principally rice, with regional trade as an insignificant part of the economy.

The term "Dark ages of Cambodia", also the "Middle Period" refers to the historical era from the early 15th century to 1863, the beginning of the French Protectorate of Cambodia. Reliable sources – particularly for the 15th and 16th century – are very rare. A conclusive explanation that relates to concrete events manifesting the decline of the Khmer Empire has not yet been produced. However, most modern historians consent that several distinct and gradual changes of religious, dynastic, administrative and military nature, environmental problems and ecological imbalance coincided with shifts of power in Indochina and must all be taken into account to make an interpretation. In recent years, focus has notably shifted towards studies on climate changes, human–environment interactions and the ecological consequences.

Epigraphy in temples, ends in the third decade of the fourteenth, and does not resume until the mid-16th century. Recording of the Royal Chronology discontinues with King Jayavarman IX Parameshwara (or Jayavarma-Paramesvara) – there exists not a single contemporary record of even a king’s name for over 200 years. Construction of monumental temple architecture had come to a standstill after Jayavarman VIIth reign. According to author Michael Vickery there only exist external sources for Cambodia’s 15th century, the Chinese Ming Shilu annals and the earliest Royal Chronicle of Ayutthaya. Wang Shi-zhen (王世貞), a Chinese scholar of the 16th century, remarked: "The official historians are unrestrained and are skilful at concealing the truth; but the memorials and statutes they record and the documents they copy cannot be discarded."

The central reference point for the entire 15th century is a Siamese intervention of some undisclosed nature at the capital Yasodharapura (Angkor Thom) around the year 1431. Historians relate the event to the shift of Cambodia's political centre southward to the region of Phnom Penh, Longvek and later Oudong.

Sources for the 16th century are more numerous. The kingdom is centred at the Mekong, prospering as an integral part of the Asian maritime trade network, via which the first contact with European explorers and adventurers does occur. Wars with the Siamese result in loss of territory and eventually the conquest of the capital Longvek in 1594. The Vietnamese on their "Southward March" reach Prei Nokor/Saigon at the Mekong Delta in the 17th century. This event initiates the slow process of Cambodia losing access to the seas and independent marine trade.

Siamese and Vietnamese dominance intensified during the 17th and 18th century, resulting in frequent displacements of the seat of power as the Khmer royal authority decreased to the state of a vassal. In the early 19th century with dynasties in Vietnam and Siam firmly established, Cambodia was placed under joint suzerainty, having lost its national sovereignty. British agent John Crawfurd states: "...the King of that ancient Kingdom is ready to throw himself under the protection of any European nation..." To save Cambodia from being incorporated into Vietnam and Siam, King Ang Duong agreed to colonial France's offers of protection, which took effect with King Norodom Prohmbarirak signing and officially recognising the French protectorate on 11 August 1863.

In August 1863 King Norodom signed an agreement with the French placing the kingdom under the protection of France. The original treaty left Cambodian sovereignty intact, but French control gradually increased, with important landmarks in 1877, 1884, and 1897, until by the end of the century the king's authority no longer existed outside the palace. Norodom died in 1904, and his two successors, Sisowath and Monivong, were content to allow the French to control the country, but in 1940 France was defeated in a brief border war with Thailand and forced to surrender the provinces of Battambang and Angkor (the ancient site of Angkor itself was retained). King Monivong died in April 1941 and the French placed the obscure Prince Sihanouk on the throne as king, believing that the inexperienced 18-year old would be more pliable than Monivong's middle-aged son, Prince Monireth. 

Cambodia's situation at the end of the war was chaotic. The Free French, under General Charles de Gaulle, were determined to recover Indochina, though they offered Cambodia and the other Indochinese protectorates a carefully circumscribed measure of self-government. Convinced that they had a "civilizing mission", they envisioned Indochina's participation in a French Union of former colonies that shared the common experience of French culture.

On 9 March 1945, during the Japanese occupation of Cambodia, young king Norodom Sihanouk proclaimed an independent Kingdom of Kampuchea, following a formal request by the Japanese. Shortly thereafter the Japanese government nominally ratified the independence of Cambodia and established a consulate in Phnom Penh. The new government did away with the romanisation of the Khmer language that the French colonial administration was beginning to enforce and officially reinstated the Khmer script. This measure taken by the short-lived governmental authority would be popular and long-lasting, for since then no government in Cambodia has tried to romanise the Khmer language again.
After Allied military units entered Cambodia, the Japanese military forces present in the country were disarmed and repatriated. The French were able to reimpose the colonial administration in Phnom Penh in October the same year.
Sihanouk's "royal crusade for independence" resulted in grudging French acquiescence to his demands for a transfer of sovereignty. A partial agreement was struck in October 1953. Sihanouk then declared that independence had been achieved and returned in triumph to Phnom Penh. As a result of the Geneva Conference on Indochina, Cambodia was able to bring about the withdrawal of the Viet Minh troops from its territory and to withstand any residual impingement upon its sovereignty by external powers.

Neutrality was the central element of Cambodian foreign policy during the 1950s and 1960s. By the mid-1960s, parts of Cambodia's eastern provinces were serving as bases for North Vietnamese Army and National Liberation Front (NVA/NLF) forces operating against South Vietnam, and the port of Sihanoukville was being used to supply them. As NVA/VC activity grew, the United States and South Vietnam became concerned, and in 1969, the United States began a 14-month-long series of bombing raids targeted at NVA/VC elements, contributing to destabilisation. The bombing campaign took place no further than ten, and later twenty miles (32 km) inside the Cambodian border, areas where the Cambodian population had been evicted by the NVA. Prince Sihanouk, fearing that the conflict between communist North Vietnam and South Vietnam might spill over to Cambodia, publicly opposed the idea of a bombing campaign by the United States along the Vietnam–Cambodia border and inside Cambodian territory. However Peter Rodman claimed, "Prince Sihanouk complained bitterly to us about these North Vietnamese bases in his country and invited us to attack them". In December 1967 "Washington Post" journalist Stanley Karnow was told by Sihanouk that if the US wanted to bomb the Vietnamese communist sanctuaries, he would not object, unless Cambodians were killed. The same message was conveyed to US President Johnson's emissary Chester Bowles in January 1968. So the US had no real motivation to overthrow Sihanouk. However Prince Sihanouk wanted Cambodia to stay out of the North Vietnam–South Vietnam conflict and was very critical of the United States government and its allies (the South Vietnamese government). Prince Sihanouk, facing internal struggles of his own, due to the rise of the Khmer Rouge, did not want Cambodia to be involved in the conflict. Sihanouk wanted the United States and its allies (South Vietnam) to keep the war away from the Cambodian border. Sihanouk did not allow the United States to use Cambodian air space and airports for military purposes. This upset the United States greatly and contributed to their view that of Prince Sihanouk as a North Vietnamese sympathiser and a thorn on the United States. However, declassified documents indicate that, as late as March 1970, the Nixon administration was hoping to garner "friendly relations" with Sihanouk.

Throughout the 1960s, domestic Cambodian politics became polarised. Opposition to the government grew within the middle class and leftists including Paris-educated leaders like Son Sen, Ieng Sary, and Saloth Sar (later known as Pol Pot), who led an insurgency under the clandestine Communist Party of Kampuchea (CPK). Sihanouk called these insurgents the Khmer Rouge, literally the "Red Khmer". But the 1966 national assembly elections showed a significant swing to the right, and General Lon Nol formed a new government, which lasted until 1967. During 1968 and 1969, the insurgency worsened. However members of the government and army, who resented Sihanouk's ruling style as well as his tilt away from the United States, did have a motivation to overthrow him.

While visiting Beijing in 1970 Sihanouk was ousted by a military coup led by Prime Minister General Lon Nol and Prince Sisowath Sirik Matak in the early hours of 18 March 1970.
However, as early as 12 March 1970, the CIA Station Chief told Washington that based on communications from Sirik Matak, Lon Nol's cousin, that "the (Cambodian) army was ready for a coup". Lon Nol assumed power after the military coup and immediately allied Cambodia with the United States. Son Ngoc Thanh, an opponent of Pol Pot, announced his support for the new government. On 9 October, the Cambodian monarchy was abolished, and the country was renamed the Khmer Republic. The new regime immediately demanded that the Vietnamese communists leave Cambodia.

Hanoi rejected the new republic's request for the withdrawal of NVA troops. In response, the United States moved to provide material assistance to the new government's armed forces, which were engaged against both CPK insurgents and NVA forces. The North Vietnamese and Viet Cong forces, desperate to retain their sanctuaries and supply lines from North Vietnam, immediately launched armed attacks on the new government. The North Vietnamese quickly overran large parts of eastern Cambodia, reaching to within of Phnom Penh. The North Vietnamese turned the newly won territories over to the Khmer Rouge. The king urged his followers to help in overthrowing this government, hastening the onset of civil war.

In April 1970, US President Richard Nixon announced to the American public that US and South Vietnamese ground forces had entered Cambodia in a campaign aimed at destroying NVA base areas in Cambodia (see Cambodian Incursion). The US had already been bombing Vietnamese positions in Cambodia for well over a year by that point. Although a considerable quantity of equipment was seized or destroyed by US and South Vietnamese forces, containment of North Vietnamese forces proved elusive.

The Khmer Republic's leadership was plagued by disunity among its three principal figures: Lon Nol, Sihanouk's cousin Sirik Matak, and National Assembly leader In Tam. Lon Nol remained in power in part because none of the others were prepared to take his place. In 1972, a constitution was adopted, a parliament elected, and Lon Nol became president. But disunity, the problems of transforming a 30,000-man army into a national combat force of more than 200,000 men, and spreading corruption weakened the civilian administration and army.

The Khmer Rouge insurgency inside Cambodia continued to grow, aided by supplies and military support from North Vietnam. Pol Pot and Ieng Sary asserted their dominance over the Vietnamese-trained communists, many of whom were purged. At the same time, the Khmer Rouge (CPK) forces became stronger and more independent of their Vietnamese patrons. By 1973, the CPK were fighting battles against government forces with little or no North Vietnamese troop support, and they controlled nearly 60% of Cambodia's territory and 25% of its population.

The government made three unsuccessful attempts to enter into negotiations with the insurgents, but by 1974, the CPK was operating openly as divisions, and some of the NVA combat forces had moved into South Vietnam. Lon Nol's control was reduced to small enclaves around the cities and main transportation routes. More than two million refugees from the war lived in Phnom Penh and other cities.

On New Year's Day 1975, Communist troops launched an offensive which, in 117 days of the hardest fighting of the war, caused the collapse of the Khmer Republic. Simultaneous attacks around the perimeter of Phnom Penh pinned down Republican forces, while other CPK units overran fire bases controlling the vital lower Mekong resupply route. A US-funded airlift of ammunition and rice ended when Congress refused additional aid for Cambodia. The Lon Nol government in Phnom Penh surrendered on 17 April 1975, just five days after the US mission evacuated Cambodia.

The relationship between the massive carpet bombing of Cambodia by the United States and the growth of the Khmer Rouge, in terms of recruitment and popular support, has been a matter of interest to historians. Some historians, including Michael Ignatieff, Adam Jones and Greg Grandin, have cited the United States intervention and bombing campaign (spanning 1965–1973) as a significant factor which lead to increased support for the Khmer Rouge among the Cambodian peasantry. According to Ben Kiernan, the Khmer Rouge "would not have won power without U.S. economic and military destabilization of Cambodia. ... It used the bombing's devastation and massacre of civilians as recruitment propaganda and as an excuse for its brutal, radical policies and its purge of moderate communists and Sihanoukists." Pol Pot biographer David P. Chandler writes that the bombing "had the effect the Americans wanted – it broke the Communist encirclement of Phnom Penh", but it also accelerated the collapse of rural society and increased social polarization. Peter Rodman and Michael Lind claimed that the United States intervention saved the Lon Nol regime from collapse in 1970 and 1973. Craig Etcheson acknowledged that U.S. intervention increased recruitment for the Khmer Rouge but disputed that it was a primary cause of the Khmer Rouge victory. William Shawcross wrote that the United States bombing and ground incursion plunged Cambodia into the chaos that Sihanouk had worked for years to avoid.

By 1973, Vietnamese support of the Khmer Rouge had largely disappeared. China "armed and trained" the Khmer Rouge both during the civil war and the years afterward.

Owing to Chinese, U.S., and Western support, the Khmer Rouge-dominated Coalition Government of Democratic Kampuchea (CGDK) held Cambodia's UN seat until 1993, long after the Cold War had ended. China has defended its ties with the Khmer Rouge. Chinese Foreign Ministry spokeswoman Jiang Yu said that "the government of Democratic Kampuchea had a legal seat at the United Nations, and had established broad foreign relations with more than 70 countries".

Immediately after its victory, the CPK ordered the evacuation of all cities and towns, sending the entire urban population into the countryside to work as farmers, as the CPK was trying to reshape society into a model that Pol Pot had conceived.

The new government sought to completely restructure Cambodian society. Remnants of the old society were abolished and religion was suppressed. Agriculture was collectivised, and the surviving part of the industrial base was abandoned or placed under state control. Cambodia had neither a currency nor a banking system.

Democratic Kampuchea's relations with Vietnam and Thailand worsened rapidly as a result of border clashes and ideological differences. While communist, the CPK was fiercely nationalistic, and most of its members who had lived in Vietnam were purged. Democratic Kampuchea established close ties with the People's Republic of China, and the Cambodian-Vietnamese conflict became part of the Sino-Soviet rivalry, with Moscow backing Vietnam. Border clashes worsened when the Democratic Kampuchea military attacked villages in Vietnam. The regime broke off relations with Hanoi in December 1977, protesting Vietnam's alleged attempt to create an Indochina Federation. In mid-1978, Vietnamese forces invaded Cambodia, advancing about before the arrival of the rainy season.

The reasons for Chinese support of the CPK was to prevent a pan-Indochina movement, and maintain Chinese military superiority in the region. The Soviet Union supported a strong Vietnam to maintain a second front against China in case of hostilities and to prevent further Chinese expansion. Since Stalin's death, relations between Mao-controlled China and the Soviet Union had been lukewarm at best. In February to March 1979, China and Vietnam would fight the brief Sino-Vietnamese War over the issue.

In December 1978, Vietnam announced the formation of the Kampuchean United Front for National Salvation (KUFNS) under Heng Samrin, a former DK division commander. It was composed of Khmer Communists who had remained in Vietnam after 1975 and officials from the eastern sector—like Heng Samrin and Hun Sen—who had fled to Vietnam from Cambodia in 1978. In late December 1978, Vietnamese forces launched a full invasion of Cambodia, capturing Phnom Penh on 7 January 1979 and driving the remnants of Democratic Kampuchea's army westward toward Thailand.

Within the CPK, the Paris-educated leadership—Pol Pot, Ieng Sary, Nuon Chea, and Son Sen—were in control. A new constitution in January 1976 established Democratic Kampuchea as a Communist People's Republic, and a 250-member Assembly of the Representatives of the People of Kampuchea (PRA) was selected in March to choose the collective leadership of a State Presidium, the chairman of which became the head of state.

Prince Sihanouk resigned as head of state on 4 April. On 14 April, after its first session, the PRA announced that Khieu Samphan would chair the State Presidium for a 5-year term. It also picked a 15-member cabinet headed by Pol Pot as prime minister. Prince Sihanouk was put under virtual house arrest.

20,000 people died of exhaustion or disease during the evacuation of Phnom Penh and its aftermath. Many of those forced to evacuate the cities were resettled in newly created villages, which lacked food, agricultural implements, and medical care. Many who lived in cities had lost the skills necessary for survival in an agrarian environment. Thousands starved before the first harvest. Hunger and malnutrition—bordering on starvation—were constant during those years. Most military and civilian leaders of the former regime who failed to disguise their pasts were executed.

Some of the ethnicities in Cambodia, such as the Cham and Vietnamese, suffered specific and targeted and violent persecutions, to the point of some international sources referring to it as the "Cham genocide". Entire families and towns were targeted and attacked with the goal of significantly diminishing their numbers and eventually eliminated them. Life in 'Democratic Kampuchea' was strict and brutal. In many areas of the country people were rounded up and executed for speaking a foreign language, wearing glasses, scavenging for food, absent for government assigned work, and even crying for dead loved ones. Former businessmen and bureaucrats were hunted down and killed along with their entire families; the Khmer Rouge feared that they held beliefs that could lead them to oppose their regime. A few Khmer Rouge loyalists were even killed for failing to find enough 'counter-revolutionaries' to execute.

When Cambodian socialists began to rebel in the eastern zone of Cambodia, Pol Pot ordered his armies to exterminate 1.5 million eastern Cambodians which he branded as "Cambodians with Vietnamese minds" in the area. The purge was done speedily and efficiently as Pol Pot's soldiers quickly killed at least more than 100,000 to 250,000 eastern Cambodians right after deporting them to execution site locations in Central, North and North-Western Zones within a month's time, making it the most bloodiest episode of mass murder under Pol Pot's regime

Religious institutions were not spared by the Khmer Rouge as well, in fact religion was so viciously persecuted to such a terrifying extent that the vast majority of Cambodia's historic architecture, 95% of Cambodia's Buddhist temples, was completely destroyed.

Ben Kiernan estimates that 1.671 million to 1.871 million Cambodians died as a result of Khmer Rouge policy, or between 21% and 24% of Cambodia's 1975 population. A study by French demographer Marek Sliwinski calculated slightly fewer than 2 million unnatural deaths under the Khmer Rouge out of a 1975 Cambodian population of 7.8 million; 33.5% of Cambodian men died under the Khmer Rouge compared to 15.7% of Cambodian women. According to a 2001 academic source, the most widely-accepted estimates of excess deaths under the Khmer Rouge range from 1.5 million to 2 million, although figures as low as 1 million and as high as 3 million have been cited; conventionally accepted estimates of deaths due to Khmer Rouge executions range from 500,000 to 1 million, "a third to one half of excess mortality during the period." However, a 2013 academic source (citing research from 2009) indicates that execution may have accounted for as much as 60% of the total, with 23,745 mass graves containing approximately 1.3 million suspected victims of execution. While considerably higher than earlier and more widely-accepted estimates of Khmer Rouge executions, the Documentation Center of Cambodia (DC-Cam)'s Craig Etcheson defended such estimates of over one million executions as "plausible, given the nature of the mass grave and DC-Cam's methods, which are more likely to produce an under-count of bodies rather than an over-estimate." Demographer Patrick Heuveline estimated that between 1.17 million and 3.42 million Cambodians died unnatural deaths between 1970 and 1979, with between 150,000 and 300,000 of those deaths occurring during the civil war. Heuveline's central estimate is 2.52 million excess deaths, of which 1.4 million were the direct result of violence. Despite being based on a house-to-house survey of Cambodians, the estimate of 3.3 million deaths promulgated by the Khmer Rouge's successor regime, the People's Republic of Kampuchea (PRK), is generally considered to be an exaggeration; among other methodological errors, the PRK authorities added the estimated number of victims that had been found in the partially-exhumed mass graves to the raw survey results, meaning that some victims would have been double-counted.

An estimated 300,000 Cambodians starved to death between 1979 and 1980, largely as a result of the after-effects of Khmer Rouge policies.

On 10 January 1979, after the Vietnamese army and the KUFNS (Kampuchean United Front for National Salvation) invaded Cambodia and overthrowing the Khmer Rouge, the new People's Republic of Kampuchea (PRK) was established with Heng Samrin as head of state. Pol Pot's Khmer Rouge forces retreated rapidly to the jungles near the Thai border. The Khmer Rouge and the PRK began a costly struggle that played into the hands of the larger powers China, the United States and the Soviet Union. The Khmer People’s Revolutionary Party’s rule gave rise to a guerrilla movement of three major resistance groups – the FUNCINPEC (Front Uni National pour un Cambodge Indépendant, Neutre, Pacifique, et Coopératif), the KPLNF (Khmer People’s National Liberation Front) and the PDK (Party of Democratic Kampuchea, the Khmer Rouge under the nominal presidency of Khieu Samphan). "All held dissenting perceptions concerning the purposes and modalities of Cambodia’s future". Civil war displaced 600,000 Cambodians, who fled to refugee camps along the border to Thailand and tens of thousands of people were murdered throughout the country.

Peace efforts began in Paris in 1989 under the State of Cambodia, culminating two years later in October 1991 in a comprehensive peace settlement. The United Nations was given a mandate to enforce a ceasefire and deal with refugees and disarmament known as the United Nations Transitional Authority in Cambodia (UNTAC).

On 23 October 1991, the Paris Conference reconvened to sign a comprehensive settlement giving the UN full authority to supervise a cease-fire, repatriate the displaced Khmer along the border with Thailand, disarm and demobilise the factional armies, and prepare the country for free and fair elections. Prince Sihanouk, President of the Supreme National Council of Cambodia (SNC), and other members of the SNC returned to Phnom Penh in November 1991, to begin the resettlement process in Cambodia. The UN Advance Mission for Cambodia (UNAMIC) was deployed at the same time to maintain liaison among the factions and begin demining operations to expedite the repatriation of approximately 370,000 Cambodians from Thailand.

On 16 March 1992, the UN Transitional Authority in Cambodia (UNTAC) arrived in Cambodia to begin implementation of the UN settlement plan and to become operational on 15 March 1992 under Yasushi Akashi, the Special Representative of the U.N. Secretary General. UNTAC grew into a 22,000-strong civilian and military peacekeeping force tasked to ensure the conduct of free and fair elections for a constituent assembly.

Over 4 million Cambodians (about 90% of eligible voters) participated in the May 1993 elections. Pre-election violence and intimidation was widespread, caused by SOC (State of Cambodia – made up largely of former PDK cadre) security forces, mostly against the FUNCINPEC and BLDP parties according to UNTAC. The Khmer Rouge or Party of Democratic Kampuchea (PDK), whose forces were never actually disarmed or demobilized blocked local access to polling places. Prince Ranariddh's (son of Norodom Sihanouk) royalist Funcinpec Party was the top vote recipient with 45.5% of the vote, followed by Hun Sen's Cambodian People's Party and the Buddhist Liberal Democratic Party, respectively. Funcinpec then entered into a coalition with the other parties that had participated in the election. A coalition government resulted between the Cambodian People’s Party and FUNCINPEC, with two co-prime ministers – Hun Sen, since 1985 the prime minister in the Communist government, and Norodom Ranariddh.

The parties represented in the 120-member assembly proceeded to draft and approve a new constitution, which was promulgated 24 September 1993. It established a multiparty liberal democracy in the framework of a constitutional monarchy, with the former Prince Sihanouk elevated to King. Prince Ranariddh and Hun Sen became First and Second Prime Ministers, respectively, in the Royal Cambodian Government (RGC). The constitution provides for a wide range of internationally recognised human rights.

Hun Sen and his government have seen much controversy. Hun Sen was a former Khmer Rouge commander who was originally installed by the Vietnamese and, after the Vietnamese left the country, maintains his strong man position by violence and oppression when deemed necessary. In 1997, fearing the growing power of his co-Prime Minister, Prince Norodom Ranariddh, Hun launched a coup, using the army to purge Ranariddh and his supporters. Ranariddh was ousted and fled to Paris while other opponents of Hun Sen were arrested, tortured and some summarily executed.

On 4 October 2004, the Cambodian National Assembly ratified an agreement with the United Nations on the establishment of a tribunal to try senior leaders responsible for the atrocities committed by the Khmer Rouge. International donor countries have pledged a US$43 Million share of the three-year tribunal budget as Cambodia contributes US$13.3 Million. The tribunal has sentenced several senior Khmer Rouge leaders since 2008.

Cambodia is still infested with countless land mines, indiscriminately planted by all warring parties during the decades of war and upheaval.




</doc>
<doc id="5429" url="https://en.wikipedia.org/wiki?curid=5429" title="Geography of Cambodia">
Geography of Cambodia

Cambodia is a country in mainland Southeast Asia, bordering Thailand, Laos, Vietnam, the Gulf of Thailand and covers a total area of . The country is situated in its entirety inside the tropical Indomalayan ecozone and the (ICT).

Cambodia's main geographical features are the low lying Central Plain that includes the Tonlé Sap basin, the lower Mekong River flood-plains and the Bassac River plain surrounded by mountain ranges to the north, east, in the south-west and south. The central lowlands extend into Vietnam to the south-east. The south and south-west of the country constitute a long coast at the Gulf of Thailand, characterized by sizable mangrove marshes, peninsulas, sandy beaches and headlands and bays. Cambodia's territorial waters account for over 50 islands. The highest peak is Phnom Aural, sitting above sea level.

The landmass is bisected by the Mekong river, which at is the longest river in Cambodia. After extensive rapids, turbulent sections and cataracts in Laos, the river enters the country at Stung Treng province, is predominantly calm and navigable during the entire year as it widens considerably in the lowlands. The Mekong's waters disperse into the surrounding wetlands of central Cambodia and strongly affect the seasonal nature of the Tonlé Sap lake.

Two third of the country's population live in the lowlands, where the rich sediment deposited during the Mekong's annual flooding makes the agricultural lands highly fertile. As deforestation and over-exploitation affected Cambodia only in recent decades, forests, low mountain ranges and local eco-regions still retain much of their natural potential and although still home to the largest areas of contiguous and intact forests in mainland Southeast Asia, multiple serious environmental issues persist and accumulate, which are closely related to rapid population growth, uncontrolled globalization and inconsequential administration.

The majority of the country lies within the Tropical savanna climate zone, as the coastal areas in the South and West receive noticeably more and steady rain before and during the wet season. These areas constitute the easternmost fringes of the south-west monsoon, determined to be inside the Tropical monsoon climate. Countrywide there are two seasons of relatively equal length, defined by varying precipitation as temperatures and humidity are generally high and steady throughout the entire year.

Southeast Asia consists of allochthonous continental blocks from Gondwanaland. These include the South China, Indochina, Sibumasu, and West Burma blocks, which amalgamated to form the Southeast Asian continent during the Paleozoic and Mesozoic periods.

The current geological structure of South China and South-East Asia is determined to be the response to the "Indo-sinian" collision in South-East Asia during the Carboniferous. The Indo-Sinian orogeny was followed by extension of the Indo-Chinese block, the formation of rift basins and thermal subsidence during the early Triassic.

The Indochina continental block, which is separated from the South China Block by the Jinshajiang-Ailaoshan Suture zone, is an amalgamation of the Viet-Lao, Khorat-Kontum, Uttaradit (UTD), and Chiang Mai-West Kachin terranes, all of which are separated by suture zones or ductile shear zones.
The Khorat-Kontum terrane, which includes western Laos, Cambodia and southern Vietnam, consists of the Kontum metamorphic complex, Paleozoic shallow marine deposits, upper Permian arc volcanic rocks and Mesozoic terrigenous sedimentary rocks.

The central plains consist mainly of Quaternary sands, loam and clay, as most of the northern mountain regions and the coastal region are largely composed of Cretaceous granite, Triassic stones and Jurassic sandstone formations.

Bowl- or saucer-shaped, Cambodia covers in the south-western part of the Indochinese peninsula as its landmass and marine territory is situated entirely within the tropics.

The bowl's bottom represents Cambodia's interior, about 75 percent, consisting of alluvial flood-plains of the Tonlé Sap basin, the lower Mekong River and the Bassac River plain, whose waters feed the large and almost centrally located wetlands. As humans preferably settle in these fertile and easily accessible central lowlands, major transformations and widespread cultivation through wet-rice agriculture have over the centuries shaped the landscape into distinctive regional cultivated lands. Domestic plants, such as sugar palms, Coconut trees and banana groves almost exclusively skirt extensive rice paddies, as natural vegetation is confined to elevated lands and near waterways. The Mekong traverses the north to south-east portions of the country, where the low-lying plains extend into Vietnam and reach the South China Sea at the Mekong Delta region.

Cambodia's low mountain ranges - representing the walls of the bowl - remain as the result of only rather recent substantial infrastructural development and economic exploitation - in particular in remote areas - formidably forested. The country is fringed to the north by the Dangrek Mountains plateau, bordering Thailand and Laos, to the north-east by the Annamite Range, in the south-west by the Cardamom Mountains and in the South by the Elephant Mountains. Highlands to the north-east and to the east merge into the Central Highlands and the Mekong Delta lowlands of Vietnam.

A heavily indented coastline at the Gulf of Thailand of length and 60 offshore islands, that dot the territorial waters and locally merge with tidal mangrove marshes - the environmental basis for a remarkable range of marine and coastal eco-regions.

"Sandy materials cover a large proportion of the landscape of Cambodia, on account of the siliceous sedimentary formations that underlie much of the Kingdom. Mesozoic sandstone dominates most of the basement geology in Cambodia and hence has a dominating influence on the properties of upland soils. Arenosols (sandy soils featuring very weak or no soil development) are mapped on only 1.6% of the land area."

"Sandy surface textures are more prevalent than the deep sandy soils that fit the definition for Arenosols. Sandy textured profiles are common amongst the most prevalent soil groups, including Acrisols and Leptosols. The Acrisols are the most prevalent soil group occupying the lowlands - nearly half of the land area of Cambodia. Low fertility and toxic amounts of aluminium pose limitations to its agricultural use, crops that can be successfully cultivated include rubber tree, oil palm, coffee and sugar cane.
The main subgroups are: Gleyic Acrisols (20.5%, Haplic Acrisols (13.3%), Plinthic Acrisol (8.7%) and Ferric Acrisol (6.3%)."


The vast alluvial and lacustrine interconnected Cambodian flood-plain is a geologically relatively recent depression where the sediments of the Mekong and its tributaries accumulate as waters are subject to frequent course changes. The area covers . The Tonlé Sap lake and - river system occupies the lowest area. The Tonle Sap river is a waterway that branches off the Mekong near Phnom Penh in the north-westerly direction and meets the Tonle Sap lake after around . Its waters' flow reverses direction every year, caused by greatly varying amounts of water carried by the Mekong over the course of a year and the impact of monsoonal rains, that coincides with the river's maximum.

The plains of the Mekong and Tonle Sap basin are confined in the North by the Dangrek and Central Annamite Mountains, and to the South by the Cardamom Mountains and Elephant Mountains. The plains completely surround the Tonle Sap Lake in the western half of the country and wind their way through the middle of the country following the course of the Mekong River. The two basins actually form a single body of water, the whole of which effects about 75% of Cambodia’s land cover.

The Mekong river and its tributaries increase water volumes in spring (May) on the northern hemisphere, mainly caused by melting snows. As the Mekong enters Cambodia (over 95% of its waters have already joined the river) it widens and inundates large areas.

The plain's deepest point - the Tonle Sap - flooded area varies from a low of around with a depth of around 1 meter at the end of the dry season (April) to and a depth of up to 9 meters in October/November. This figure rose to during 2000 when some of the worst flood conditions recorded caused over 800 deaths in Cambodia and Vietnam.

Inflow starts in May/June with maximum rates of flow of around 10,000 m/s by late August and ends in October/November, amplified by precipitation of the annual monsoon. In November the lake reaches its maximum size. The annual monsoon coincides to cease around this time of the year. As the Mekong river begins its minimum around this time of the year and its water level falls deeper than the inundated Tonle Sap lake, Tonle Sap river and surrounding wetlands, waters of the lake's basin now drains via the Tonle Sap river into the Mekong. As a result the Tonle Sap River (length around ) flows 6 months a year from South-East (Mekong) to North-West (lake) and 6 month a year in the opposite direction. The mean annual reverse flow volume in the Tonle Sap is , or about half of the maximum lake volume. A further 10% is estimated to enter the system by overland flow from the Mekong. The Mekong branches off into several arms near Phnom Penh and reaches Vietnamese territory south of Koh Thom and Loek Daek districts of Kandal Province.

This region represents the eastern parts of the original extent of the wet evergreen forests that cover the Cardamom - and Elephant Mountains in South-West Cambodia and along the mountains east of Bangkok in Thailand.

The densely wooded hills receive rainfall of annually on their western slopes (which are subject to the South-West monsoons) but only on their eastern - rain shadow - slopes.

The Cardamom/Krâvanh Mountains
Occupying Koh Kong Province and Kampong Speu Province, running in a north-western to south-eastern direction and rising to more than . The highest mountain of Cambodia, Phnom Aural, at is located in Aoral District in Kampong Speu Province.
The Cardamom Mountains form - including the north-western part of Chanthaburi Province, Thailand, the 'Soi Dao Mountains' - "the Cardamom Mountains Moist Forests Ecoregion", that is considered to be one of the most species-rich and intact natural habitats in the region. The climate, size inaccessibility and seclusion of the mountains have allowed a rich variety of wildlife to thrive. The Cardamom and Elephant Mountains remain to be fully researched and documented.

The Elephant Mountains
Chuŏr Phnum Dâmrei - A north-south-trending range of high hills, an extension of the Cardamom/Krâvanh Mountains, in south-eastern Cambodia, rising to elevations of between 500 and 1,000 meters. Extending north from the Gulf of Thailand, they reach a high point in the Bok Koŭ ridge at Mount Bokor near the sea.

To the south-west of the Southern mountain ranges extends a narrow coastal plain that contains the Kampong Saom Bay area and the Sihanoukville peninsula, facing the Gulf of Thailand.

The Dangrek Mountains
A forested range of hills averaging , dividing Thailand from Cambodia, mainly formed of massive sandstone with slate and silt. A few characteristic basalt hills are located on the northern side of the mountain chain. This east–west-trending range extends from the Mekong River westward for approximately , merging with the highland area near San Kamphaeng, Thailand. Essentially the southern escarpment of the sandstone Khorat Plateau of northeastern Thailand, the Dângrêk range slopes gradually northward to the Mun River in Thailand but falls more abruptly in the south to the Cambodian plain. Its highest point is . The watershed along the escarpment in general terms marks the boundary between Thailand and Cambodia, however there are exceptions. The region is covered in dry evergreen forest, mixed dipterocarp forest, and deciduous dipterocarp forests. Tree species like Pterocarpus macrocarpus, Shorea siamensis and Xylia xylocarpa var. kerrii dominate. Illegal logging are issues on both, the Thai as well as on the Cambodian side, leaving large hill stretches denuded, vulnerable tree species such as Dalbergia cochinchinensis have been affected. Forest fires are common during the dry season.
Annamite Range
Lying to the east of the Mekong River, the long chain of mountains called the Annamite Mountains of Indochina and the lowlands that surround them make up the Greater Annamites ecoregion. Levels of rainfall vary from annually. Mean annual temperatures are about . This eco-region contains some of the last relatively intact moist forests in Indochina. Moisture-laden monsoon winds, that blow in from the Gulf of Tonkin ensure permanent high air humidity. Plants and animals adapted to moist conditions, to seek refuge here and evolve into highly specialized types that are found nowhere else on Earth.

Ethnically diverse
More than 30 ethnic groups of indigenous people live in the Annamites, each with their distinctive and traditional music, language, dress and customs. The natural resources of the Greater Annamites are vital to all of these people.

Tall grasses and deciduous forests cover the ground east of the Mekong River in Mondulkiri, where the transitional plains merge with the eastern highlands at altitudes from . The landscape has suffered from rubber farming, logging and particularly mining, although sizable areas of pristine jungle survive, which are home to rare and endemic wildlife.

Cambodia's coastal area covers , distributed among four provinces: Sihanoukville province, Kampot province, Koh Kong province, and Kep province. The total length of the Cambodian coastal area has been disputed. The most widely accepted length is , a 1997 survey by the DANIDA organization announced a length at , and in 1973 the "Oil Authority" found the coast to be long. The Food and Agriculture Organization claims a length of in one of its studies.

The southern mountain ranges drain to the south and west towards the shallow sea. Sediments on the continental shelf are the basis for extensive mangroves marshes, in particular in the Koh Kong province and the Ream National Park.

Cambodia’s islands fall under administration of the 4 coastal provinces. "There are 60 islands in Cambodia's coastal waters. They include 23 in Koh Kong province, 2 in Kampot province, 22 in Sihanoukville and 13 in Kep city.[sic]" Most islands are, apart from the two small groups of the outer islands, in relative proximity to the coast. The islands and the coastal region of Koh Kong Province are mainly composed of upper Jurassic and lower Cretaceous sandstomne massives. The north-westernmost islands near and around the Kaoh Pao river delta (Prek Kaoh Pao) area are to a great extent sediments of estuaries and rivers, very flat and engulfed in contiguous mangrove marshes.

Cambodia's climate, like that of much of the rest of mainland Southeast Asia is dominated by monsoons, which are known as tropical wet and dry because of the distinctly marked seasonal differences. The monsoonal air-flows are caused by annual alternating high pressure and low pressure over the Central Asian landmass. In summer, moisture-laden air—the southwest monsoon—is drawn landward from the Indian Ocean. The flow is reversed during the winter, and the northeast monsoon sends back dry air. The southwest monsoon brings the rainy season from mid-May to mid-September or to early October, and the northeast monsoon flow of drier and cooler air lasts from early November to March. Temperatures are fairly uniform throughout the Tonlé Sap Basin area, with only small variations from the average annual mean of around .

The maximum mean is about ; the minimum mean, about . Maximum temperatures of higher than , however, are common and, just before the start of the rainy season, they may rise to more than . Minimum night temperatures sporadically fall below . in January, the coldest month. May is the warmest month - although strongly influenced by the beginning of the wet season, as the area constitutes the easternmost fringe of the south-west monsoon. Tropical cyclones only rarely cause damage in Cambodia.

The total annual rainfall average is between , and the heaviest amounts fall in the southeast. Rainfall from April to September in the Tonlé Sap Basin-Mekong Lowlands area averages annually, but the amount varies considerably from year to year. Rainfall around the basin increases with elevation. It is heaviest in the mountains along the coast in the southwest, which receive from to more than of precipitation annually as the southwest monsoon reaches the coast. This area of greatest rainfall, however, drains mostly to the sea; only a small quantity goes into the rivers flowing into the basin. Relative humidity is high throughout the entire year; usually exceeding 90%. During the dry season daytime humidity rates average around 50 percent or slightly lower, climbing to about 90% during the rainy season.

The Mekong River and its tributaries comprise one of the largest river systems
in the world. The central Tonle Sap, the "Great Lake" has several input rivers, the most important being the Tonle Sap River, which contributes 62% of the total water supply during the rainy season. Direct rainfall on the lake and the other rivers in the sub-basin contribute the remaining 38%. Major rivers are the Sen river, Sreng River, Stung Pouthisat River, Sisophon River, Mongkol Borei River, and Sangkae River.

Smaller rivers in the southeast, the Cardamom Mountains and Elephant Range form separate drainage divides. To the east the rivers flow into the Tonle Sap, as in the south-west rivers flow into the Gulf of Thailand. Toward the southern slopes of the Elephant Mountains, small rivers flow south-eastward on the eastern side of the divide.

The Mekong River flows southward from the Cambodia-Laos border to a point south of Kratié (town), where it turns west for about and then turns southwest towards Phnom Penh. Extensive rapids run north of Kratie city. From Kampong Cham Province the gradient slopes very gently, and inundation of areas along the river occurs at flood stage. From June through November—through breaks in the natural levees that have built up along its course. At Phnom Penh four major water courses meet at a point called the Chattomukh (Four Faces). The Mekong River flows in from the northeast and the Tonle Sap river emanates from the Tonle Sap—flows in from the northwest. They divide into two parallel channels, the Mekong River proper and the Bassac River, and flow independently through the delta areas of Cambodia and Vietnam to the South China Sea.

The flow of water into the Tonle Sap is seasonal. In spring, the flow of the Mekong River, fed by monsoon rains, increases to a point where its outlets through the delta can't handle the enormous volume of water. At this point, the water pushes northward up the Tonle Sap river and empties into the Tonle Sap lake, thereby increasing the size of the lake from about to about at the height of the flooding. After the Mekong's waters crest — when its downstream channels can handle the volume of water — the flow reverses, and water flows out of the engorged lake.

As the level of the Tonle Sap retreats, it deposits a new layer of sediment. The annual flooding, combined with poor drainage immediately around the lake, transforms the surrounding area into marshlands, unusable for agricultural purposes during the dry season. The sediment deposited into the lake during the Mekong's flood stage appears to be greater than the quantity carried away later by the Tonle Sap River. Gradual silting of the lake would seem to be occurring; during low-water level, it is only about deep, while at flood stage it is between deep.

Cambodia has one of the highest levels of forest cover in the region as the interdependence of Cambodia’s geography and hydrology makes it rich in natural resources and biological diversity - among the bio-richest countries in Southeast Asia. The Royal Government of Cambodia estimates Cambodia contains approximately 10.36 million hectares of forest cover, representing approximately 57.07% of Cambodia’s land area (2011). On the contrary, international observers and independent sources provide rather different numbers. Consensus permeates, as most sources agree, that deforestation, loss of seasonal wetlands and habitat destruction - among countless minor factors - correlates with the absence of strict administrative control and indifference in law enforcement - not only in Cambodia but the entire region.

Figures and assessments are numerous as are available sources. as seen in numbers below, which provide a wide range for interpretation. About (1%) of forest cover is planted forest. Overall Cambodia’s forests contain an estimated 464 million metric tonnes of carbon stock in living forest biomass. Approximately 40% of Cambodia’s Forests have some level of protection, while one of the Cambodia Millennium Development Goals targets is to achieve a 60% forest cover by 2015.

According to the "Forestry Administration" statistics, a total of 380,000 hectares of forest were cleared between 2002 and 2005/2006 - a deforestation rate of 0.5% per year. The main cause of deforestation has been determined to be large-scale agricultural expansions.

The Southern Annamites Montane Rain Forests ecoregion of the montane forests of Kontuey Nea, "the dragon's tail" in the remote north-west of Cambodia, where the boundaries of Cambodia, Laos, and Vietnam meet [this is in the northeast, not the northwest?], is remarkably rich in biodiversity. The relatively intact forests occupy a broad topographic range - from lowlands with wet evergreen forests to montane habitats with evergreen hardwood and conifer forests. The complex geological, topographic and climatic ( rainfall and temperature ) facets that characterize the region make forest structure and composition unique and very variable. There is an unusually high number of near-endemic and endemic species among the many species to be found in the area. The entire eco-region has a size of .

The Tonle Sap, also known as the Great Lake in central Cambodia is the largest freshwater lake in Southeast Asia and one of the richest inland fishing grounds in the world. The Lake functions as a natural flood water reservoir for the Mekong system as a whole and therefore is an important source of water for the Mekong Delta during the dry season. The ecosystem has developed as a result of the Mekong’s seasonal flow fluctuations. A belt of freshwater mangroves known as the "flooded forest" surrounds the lake. The floodplains in turn are surrounded by low hills, covered with evergreen seasonal tropical forest with substantial dipterocarp vegetation or deciduous dry forest. The eco-region consists of a mosaic of habitats for a great number of species. The forest gradually yields to bushes and finally grassland with increasing distance from the lake.

Henri Mouhot: "Travels in the Central Parts of Indo-China" 1864

On higher quality soils or at higher elevation, areas of mixed deciduous forest and semi-evergreen forests occur. This variety of vegetation types accounts for the quantity and diversity of species of the Great Lake ecosystem. Interlocking forest, - grassland and marshland patches provide the many facets and refugiae for the abundant local wildlife.

The lake’s flooded forest and the surrounding floodplains are of utmost importance for Cambodia's agriculture as the region represents the cultural heart of Cambodia, the center of the national freshwater fishery industry - the nation's primary protein source.
Threats to the lake include widespread pollution, stress through growth of the local population which is dependent on the lake for subsistence and livelihood, over-harvesting of fish and other aquatic - often endangered - species, habitat destruction and potential changes in the hydrology, such as the construction and operation of dams, that disrupt the lake's natural flood cycle. However, concerns that the lake is rapidly filling with sediment seem - according to studies - to be unfounded at the present time.

Wetlands cover more than 30% of Cambodia. In addition to the Mekong River and the Tonle Sap floodplain there are the Stung Sen River and the coastal Stung Koh Pao - and Stung Kep estuaries of Koh Kong Province and Kep Province. The freshwater wetlands of Cambodia represent one of the most diverse ecosystems worldwide. The area’s extensive wetland habitats are the product of the annual Mekong maximum, the simultaneous wet season and the drainage paths of a number of minor rivers. See also:Geography of Cambodia#Hydrology The numerous and varied wetlands are Cambodia's central and traditional settlement area, the productive environments for rice cultivation, freshwater fisheries, other forms of agriculture and aquaculture and the constantly growing tourism sector. Considering the eco-region's importance, a variety of plans for local wetland management consolidation exist with varying degrees of completion.

The Cambodian coastline consists of of over 30 species of mangroves - among the most biologically diverse wetlands on earth. The most pristine mangrove forests are found in Koh Kong Province. In addition to mangroves, sea-grass beds extend throughout the coastal areas, especially in Kampot Province, the Sihanoukville Bay Delta and the Kep municipal waters. The meadows are highly productive, but few animals feed directly on the grasses. Those that do tend to be vertebrates such as sea turtles, dabbling ducks and geese.

"With their roots deep in mud, jagged and gnarled mangrove trees are able to grow in the brackish wetlands between land and sea where other plant life cannot survive. The trees offer refuge and nursery grounds for fish, crabs, shrimp, and mollusks. They are nesting - and migratory sites for hundreds of bird species. They also provide homes for monkeys, lizards, sea turtles, and many other animals as well as countless insects."

"Until relatively recently, the mangroves of Koh Kong, Cambodia have remained relatively intact. This is partly because of the region’s location — it is an isolated, inaccessible place — and because decades of war and conflict perversely protected the forests from over-exploitation. Local people, however, tended to use the forest's sustainability, for food, fuel, medicine, building materials, and other basic needs."

Cambodia is home to a wide array of wildlife. There are 212 mammal species, 536 bird species, 176 reptile species (including 89 subspecies), 850 freshwater fish species (Tonlé Sap Lake area), and 435 marine fish species.
Many of the country's species are recognized by the IUCN or World Conservation Union as threatened, endangered, or critically endangered due to deforestation and habitat destruction, poaching, illegal wildlife trade, farming, fishing, and unauthorized forestry concessions. Intensive poaching may have already driven Cambodia's national animal, the Kouprey, to extinction. Wild tigers, Eld's deer, wild water buffaloes and hog deer are at critically low numbers.

"The 1993 Royal Decree on the Protection of Natural Areas recognized 23 protected areas, which at the time covered more than 18% of the country’s total land area."


Cambodia borders Vietnam over a length of , Thailand over a length of and Laos over a length of , with in total and an additional of coastline. The capital ("reach thani") and provinces ("khaet") of Cambodia are first-level administrative divisions. Cambodia is divided into 25 provinces including the capital.

Municipalities and districts are the second-level administrative divisions of Cambodia. The provinces are subdivided into 159 districts and 26 municipalities. The districts and municipalities in turn are further divided into communes ("khum") and quarters ("sangkat").

Cambodia, Laos and Vietnam have experienced major changes in land use and land cover over the last two decades. The emergence from cold war rivalries and recent major economic reforms result in a shift from subsistence agrarian modes of production to market-based agricultural production and industrialized economies, which are heavily integrated into regional and global trade systems.

Cambodia's boundaries were for the most part based upon those recognized by France and by neighboring countries during the colonial period. The boundary with Thailand runs along the watershed of the Dangrek Mountains, although only in its northern sector. The border with Laos and the border with Vietnam result from French administrative decisions and do not follow major natural features. Border disputes have broken out in the past and do persist between Cambodia and Thailand as well as between Cambodia and Vietnam.

Area:
<br>"total:"

<br>"land:"

<br>"water:"
Maritime claims:
<br>"territorial sea:"

<br>"contiguous zone:"

<br>"exclusive economic zone:"

<br>"continental shelf:"
Elevation extremes:
<br>"lowest point:"
Gulf of Thailand 
<br>"highest point:"
Phnum Aoral 

Border disputes


Lakes

In late 1969, the Cambodian government granted a permit to a French company to explore for petroleum in the Gulf of Thailand. By 1972 none had been located, and exploration ceased when the Khmer Republic (see Appendix B) fell in 1975. Subsequent oil and gas discoveries in the Gulf of Thailand and in the South China Sea, however, could spark renewed interest in Cambodia's offshore area, especially because the country is on the same continental shelf as its Southeast Asian oil-producing neighbors.


Total renewable water resources:


Freshwater withdrawal (domestic/industrial/agricultural):



Issues


A nascent environmental movement has been noticed by NGO's - and it is gaining strength, as the example of local resistance against the building of a Chinese hydro-electric dam in the Areng Valley shows.

Consequences

Cambodia is party to the following treaties:


Signed, but not ratified:






</doc>
<doc id="5430" url="https://en.wikipedia.org/wiki?curid=5430" title="Demographics of Cambodia">
Demographics of Cambodia

This article is about the demographic features of the population of Cambodia, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

Between 1874 and 1921, the total population of Cambodia increased from about 946,000 to 2.4 million. By 1950, it had increased to between 3,710,107 and 4,073,967, and in 1962 it had reached 5.7 million. From the 1960s until 1975, the population of Cambodia increased by about 2.2% yearly, the lowest increase in Southeast Asia. By 1975 when the Khmer Rouge took power, it was estimated at 7.3 million. Of this total an estimated one to two million reportedly died between 1975 and 1978. In 1981, the PRK gave the official population figure as nearly 6.7 million, although approximately 6.3 million to 6.4 million is probably more accurate. The average annual rate of population growth from 1978 to 1985 was 2.3% (see table 2, Appendix A). A post-Khmer Rouge baby boom pushed the population above 10 million, although growth has slowed in recent years.

In 1959, about 45% of the population was under 15 years of age. By 1962, this had increased slightly to 46%. In 1962, an estimated 52% of the population was between 15 and 64 years of age, while 2% were older than 65. The percentage of males and females in the three groups was almost the same.

Structure of the population (01.07.2013) (Estimates) (Excluding foreign diplomatic personnel and their dependants. Data based on the 2008 Population Census) :

Births and deaths
The total fertility rate in Cambodia was 3.0 children per woman in 2010. The fertility rate was 4.0 children in 2000. Women in urban areas have 2.2 children on average, compared with 3.3 children per woman in rural areas. Fertility is highest in Mondol Kiri and Rattanak Kiri Provinces, where women have an average of 4.5 children, and lowest in Phnom Penh where women have an average of 2.0 children.

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Total fertility rate and other related statistics by province, as of 2014:

Childhood mortality rates are decreasing in Cambodia. Currently, the infant mortality rate is 45 deaths per 1,000 live births for the five-year period before the survey compared with 66 deaths reported in the 2005 CDHS and 95 in the 2000 CDHS. Under-five mortality rates have also decreased from 124 deaths per 1,000 live births in 2000, 83 deaths in 2005 to 54 deaths per 1,000 in 2010.

Childhood mortality decreases markedly with mother’s education and wealth. Infant mortality, for example, is twice as high among children whose mothers have no schooling compared to those with secondary or higher education (72 versus 31). The association with wealth is even stronger. There are 77 deaths per 1,000 live births among infants from the poorest households compared to only 23 deaths per 1,000 live births among infants from the richest households.

Mortality rates are much higher in rural than urban areas. Infant mortality, for example, is 64 deaths per 1,000 live births in rural areas compared to only 22 in urban areas.
Mortality also differs by province. Infant mortality ranges from only 13 deaths per 1,000 live births in Phnom Penh to 78 deaths per 1,000 live births in Kampong Chhnang and Svay Rieng.

In 1959, life expectancy at birth was 44.2 years for males and 43.3 years for females. By 1970, life expectancy had increased by about 2.5 years since 1945. The greater longevity for females apparently reflected improved health practices during maternity and childbirth.
Source: "UN World Population Prospects"

The largest of the ethnic groups in Cambodia are the Khmer, who comprise approximately 90% of the total population and primarily inhabit the lowland Mekong subregion and the central plains.

The Khmer historically have lived near the lower Mekong River in a contiguous arc that runs from the southern Khorat Plateau where modern-day Thailand, Laos and Cambodia meet in the northeast, stretching southwest through the lands surrounding Tonle Sap lake to the Cardamom Mountains, then continues back southeast to the mouth of the Mekong River in southeastern Vietnam.

Ethnic groups in Cambodia other than the politically and socially dominant Khmer are classified as either "indigenous ethnic minorities" or "non-indigenous ethnic minorities". The indigenous ethnic minorities, more commonly collectively referred to as the Khmer Loeu ("upland Khmer"), constitute the majority in the remote mountainous provinces of Ratanakiri, Mondulkiri and Stung Treng and are present in substantial numbers in Kratie Province.

Approximately 17-21 separate ethnic groups, most of whom speak Austroasiatic languages related to Khmer, are included in the Khmer Loeu designation, including the Kuy and Tampuan people. These peoples are considered by the Khmer to be the aboriginal inhabitants of the land. Two of these highland groups, the Rade and the Jarai, are Chamic peoples who speak Austronesian languages descended from ancient Cham. These indigenous ethnic minorities haven't integrated into Khmer culture and follow their traditional animist beliefs.

The non-indigenous ethnic minorities include immigrants and their descendants who live among the Khmer and have adopted, at least nominally, Khmer culture and language. The three groups most often included are the Chinese Cambodians, Vietnamese and Cham peoples. The Chinese have immigrated to Cambodia from different regions of China throughout Cambodia's history, integrating into Cambodian society and today Chinese Cambodians or Cambodians of mixed Sino-Khmer ancestry dominate the business community, politics and the media. The Cham are descendants of refugees from the various wars of the historical kingdom of Champa. The Cham live amongst the Khmer in the central plains but in contrast to the Khmer who are Theravada Buddhists, the vast majority of Cham follow Islam.

There are also small numbers of other minority groups. Tai peoples in Cambodia include the Lao along the Mekong at the northeast border, Thai (urban and rural), and the culturally Burmese Kola, who have visibly influenced the culture of Pailin Province. Even smaller numbers of recent Hmong immigrants reside along the Lao border and various Burmese peoples have immigrated to the capital, Phnom Penh.

Khmer 90%, Vietnamese 5%, Chinese 1%, other 4%.


The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.


Adult prevalence rate

People living with HIV/AIDS

Deaths

Countries with notable populations of Cambodians are:


</doc>
<doc id="5431" url="https://en.wikipedia.org/wiki?curid=5431" title="Politics of Cambodia">
Politics of Cambodia

The politics of Cambodia are defined within the framework of a constitutional monarchy, in which the King serves as the head of state, and the prime minister is the head of government. The collapse of communism set in motion events that led to the withdrawal of the Vietnamese armed forces, which had established their presence in the country since the fall of the Khmer Rouge. The 1993 constitution, which is currently in force, was promulgated as a result of the 1991 Paris Peace Agreements, followed by elections organized under the aegis of the United Nations Transitional Authority in Cambodia. The constitution proclaims a liberal, multiparty democracy in which powers are devolved to the executive, the judiciary and the legislature. Furthermore, the governing charter declares Cambodia to be an "independent, sovereign, peaceful, permanently neutral and non-aligned State."

Executive power is exercised by the Royal Government, on behalf of and with the consent of the monarch. The government is constituted of the Council of Ministers, headed by the prime minister. The prime minister is aided in his functions by members of the Council such as deputy prime ministers, senior ministers and other ministers. Legislative power is vested in a bicameral legislature composed of the National Assembly, which has the power to vote on draft law, and the Senate, that has the power of review. Upon passage of legislation through the two chambers, the draft law is presented to the monarch for signing and promulgation. The judiciary is tasked with the protection of rights and liberties of the citizens, and with being an impartial arbiter of disputes. The Supreme Court is the highest court of the country and takes appeals from lower courts on questions of law. A separate body called the Constitutional Council was established to provide interpretations of the constitution and the laws, and also to resolve disputes related to election of members of the legislature.

The Cambodian People's Party has dominated the political landscape since the 1997 armed clashes in Phnom Penh. Other prominent political parties include the royalist FUNCINPEC and the erstwhile Cambodia National Rescue Party that was dissolved by the Supreme Court in 2017. Comparative political scientists Steven Levitsky and Lucan Way have described Cambodia as a "competitive authoritarian regime", a hybrid regime type with important characteristics of both democracy and authoritarianism.

Cambodia is a constitutional monarchy with a unitary structure and a parliamentary form of government. The constitution, which prescribes the governing framework, was promulgated in September 1993 by the Constituent Assembly that resulted from the 1993 general election conducted under the auspices of the United Nations Transitional Authority in Cambodia (UNTAC). The assembly adopted the basic principles and measures mandated under the Paris Peace Agreements into the text of the constitution. Assimilated into the governing charter, these provisions place the constitution as the supreme law of the land; declare Cambodia's status as a sovereign, independent and neutral state; enshrine a liberal, multi-party democracy with fair and periodic elections; guarantee respect for human rights; and provide for an independent judiciary. The brutality of the Democratic Kampuchea regime had especially necessitated the inclusion of provisions concerning human rights in order to prevent a return to the policies and practices of the past. These criteria had been drawn from the Namibian constitution drafting process that took place in 1982. German constitutional law scholar, , characterized these benchmarks as the "necessary nucleus of a modern constitutional state." The constitution further sanctifies the status of international law in the issue of human rights by binding Cambodia to "respect" the provisions of human rights treaties adopted by the UN. The 1993 constitution has been amended nine times since its passage – in 1994, 1999, 2001, 2005, 2006, 2008, 2014 and 2018.

The powers are devolved to three branches of the state: the legislature, the executive and the judiciary, in recognition of the doctrine of separation of powers. Political sovereignty rests with the Cambodian people, who exercise their power through the three arms of the state. The Royal Government, which wields executive power, is directly responsible to the National Assembly. The judiciary, which is an independent power, is tasked with the protection of citizens' rights and liberties. Buddhism is proclaimed as the state religion.

The legal system of Cambodia is civil law and has been strongly influenced by the legal heritage of France as a consequence of colonial rule. The Soviet-Vietnamese system dominated the country from 1981 until 1989, and Sri Lankan jurist Basil Fernando argues that its elements are present in the current system as well. The role of customary law, based on Buddhist beliefs and unwritten law drawn from the Angkorean period, is also prevalent.

The constitution contains a commitment to the "market economy system", which along with accompanying provisions effects a fundamental change in the role of the state from the past. Security of private property and the right to sell and exchange freely, necessary conditions for the functioning of the market economy, are provided for. The state's powers of expropriation are limited to the extent they serve public interest, to be exercised only when "fair and just" compensation is made in advance. Operating under the slogan ""Le Cambodge s'aide lui-même"" or "Cambodia will help itself", one of the earliest undertakings of the Royal Government was to implement programs to ensure the economic rehabilitation of Cambodia and its integration in the regional and global economies. On 10 March 1994, the Royal Government declared an "irreversible and irrevocable" move away from a centrally-planned economy towards a market-oriented economy.

Cambodia is a constitutional monarchy, i.e. the King reigns but does not rule, in similar fashion to Queen Elizabeth II of the United Kingdom. The King is officially the Head of State and is the symbol of unity and "perpetuity" of the nation, as defined by Cambodia's constitution.

From September 24, 1993 through October 7, 2004, Norodom Sihanouk reigned as King, after having previously served in a number of offices (including King) since 1941. Under the Constitution, the King has no political power, but as Norodom Sihanouk was revered in the country, his word often carried much influence in the government. While such views are not prevalent in Cambodia, his word was respected by his subjects. The King, often irritated over the conflicts in his government, several times threatened to abdicate unless the political factions in the government got along. This put pressure on the government to solve their differences. This influence of the King was often used to help mediate differences in government.

After the abdication of King Norodom Sihanouk in 2004, he was succeeded by his son Norodom Sihamoni. While the retired King was highly revered in his country for dedicating his lifetime to Cambodia, the current King has spent most of his life abroad in France. Thus, it remains to be seen whether the new King's views will be as highly respected as his father's.

Although in the Khmer language there are many words meaning "king", the word officially used in Khmer (as found in the 1993 Cambodian Constitution) is "preahmâhaksat" (Khmer regular script: ព្រះមហាក្សត្រ), which literally means: "preah"- ("excellent", cognate of the Pali word vara) -"mâha"- (from Sanskrit, meaning "great", cognate with "maha-" in maharaja) -"ksat" ("warrior, ruler", cognate of the Sanskrit word kṣatrá).

On the occasion of King Norodom Sihanouk's retirement in September 2004, the Cambodian National Assembly coined a new word for the retired king: "preahmâhaviraksat" (Khmer regular script: ព្រះមហាវីរក្សត្រ), where "vira" comes from Sanskrit "vīra", meaning "brave or eminent man, hero, chief", cognate of Latin "vir", "viris", English "virile". "Preahmâhaviraksat" is translated in English as "King-Father" (), although the word "father" does not appear in the Khmer noun.

As "preahmâhaviraksat", Norodom Sihanouk retained many of the prerogatives he formerly held as "preahmâhaksat" and was a highly respected and listened-to figure. Thus, in effect, Cambodia could be described as a country with two Kings during Sihanouk's lifetime: the one who was the Head of State, the "preahmâhaksat" Norodom Sihamoni, and the one who was not the Head of State, the "preahmâhaviraksat" Norodom Sihanouk.

Sihanouk died of a pulmonary infarction on October 15, 2012.

Unlike most monarchies, Cambodia's monarchy is not necessarily hereditary and the King is not allowed to select his own heir. Instead, a new King is chosen by a Royal Council of the Throne, consisting of the president of the National Assembly, the Prime Minister, the President of the Senate, the First and Second Vice Presidents of the Senate, the Chiefs of the orders of Mohanikay and Thammayut, and the First and Second Vice-President of the Assembly. The Royal Council meets within a week of the King's death or abdication and selects a new King from a pool of candidates with royal blood.

It has been suggested that Cambodia's ability to peacefully appoint a new King shows that Cambodia's government has stabilized incredibly from the situation the country was in during the 1970s (see History of Cambodia).

The Prime Minister of Cambodia is a representative from the ruling party of the National Assembly. He or she is appointed by the King on the recommendation of the President and Vice Presidents of the National Assembly. In order for a person to become Prime Minister, he or she must first be given a vote of confidence by the National Assembly.

The Prime Minister is officially the Head of Government in Cambodia. Upon entry into office, he or she appoints a Council of Ministers who are responsible to the Prime Minister. Officially, the Prime Minister's duties include chairing meetings of the Council of Ministers (Cambodia's version of a Cabinet) and appointing and leading a government. The Prime Minister and his government make up Cambodia's executive branch of government.

The current Prime Minister is Cambodian People's Party (CPP) member Hun Sen. He has held this position since the criticized 1998 election, one year after the CPP staged a bloody coup in Phnom Penh to overthrow elected Prime Minister Prince Norodom Ranariddh, president of the FUNCINPEC party.

The legislative branch of the Cambodian government is made up of a bicameral parliament.


The official duty of the Parliament is to legislate and make laws. Bills passed by the Parliament are given to the King who gives the proposed bills royal assent. The King does not have veto power over bills passed by the National Assembly and thus, cannot withhold royal assent. The National Assembly also has the power to dismiss the Prime Minister and his government by a two-thirds vote of no confidence.

The upper house of the Cambodian legislature is called the "Senate". It consists of sixty-one members. Two of these members are appointed by the King, two are elected by the lower house of the government, and the remaining fifty-seven are elected popularly by electors from provincial and local governments, in a similar fashion to the Senate of France. Members in this house serve six-year terms.

Prior to 2006, elections had last been held for the Senate in 1999. New elections were supposed to have occurred in 2004, but these elections were initially postponed. On January 22, 2006, 11,352 possible voters went to the poll and chose their candidates. This election was criticized by local monitoring non-governmental organizations as being undemocratic.

, the Cambodian People's Party holds forty-three seats in the Senate, constituting a significant majority. The two other major parties holding seats in the Senate are the Funcinpec party (holding twelve seats) and the Sam Rainsy Party (holding two seats).

The lower house of the legislature is called the "National Assembly". It is made up of 125 members, elected by popular vote to serve a five-year term. Elections were last held for the National Assembly in July 2013.

In order to vote in legislative elections, one must be at least eighteen years of age. However, in order to be elected to the Legislature, one must be at least twenty-five years of age.

The National Assembly is led by a President and two Vice Presidents who are selected by Assembly members prior to each session.

, the Cambodian People's Party holds all 125 seats in the National Assembly.

The judicial branch is independent from the rest of the government, as specified by the Cambodian Constitution. The highest court of judicial branch is the Supreme Council of the Magistracy. Other, lower courts also exist. Until 1997, Cambodia did not have a judicial branch of government despite the nation's Constitution requiring one.

The main duties of the judiciary are to prosecute criminals, settle lawsuits, and, most importantly, protect the freedoms and rights of Cambodian citizens. However, in reality, the judicial branch in Cambodia is highly corrupt and often serves as a tool of the executive branch to silence civil society and its leaders. There are currently 17 justices on the Supreme Council.

ACCT, AsDB, ASEAN, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFAD, IFC, IFRCS, ILO, IMF, IMO, Intelsat (nonsignatory user), International Monetary Fund, Interpol, IOC, ISO (subscriber), ITU, NAM, OPCW, PCA, UN, UNCTAD, UNESCO, UNIDO, UPU, WB, WFTU, WHO, WIPO, WMO, WTO, WToO, WTrO (applicant)

Below the central government are 24 provincial and municipal administration. (In rural areas, first-level administrative divisions are called provinces; in urban areas, they are called municipalities.) The administrations are a part of the Ministry of the Interior and their members are appointed by the central government. Provincial and municipal administrations participate in the creation of nation budget; they also issue land titles and license businesses.

Since 2002, commune-level governments (commune councils) have been composed of members directly elected by commune residents every five years.

In practice, the allocation of responsibilities between various levels of government is uncertain. This uncertainty has created additional opportunities for corruption and increased costs for investors.






</doc>
<doc id="5432" url="https://en.wikipedia.org/wiki?curid=5432" title="Economy of Cambodia">
Economy of Cambodia

The economy of Cambodia currently follows an open market system (market economy) and has seen rapid economic progress in the last decade. Cambodia had a GDP of $24.57 billion in 2018. Per capita income, although rapidly increasing, is low compared with most neighboring countries. Cambodia's two largest industries are textiles and tourism, while agricultural activities remain the main source of income for many Cambodians living in rural areas. The service sector is heavily concentrated on trading activities and catering-related services. Recently, Cambodia has reported that oil and natural gas reserves have been found off-shore.

In 1995, with a GDP of $2.92 billion the government transformed the country's economic system from a planned economy to its present market-driven system. Following those changes, growth was estimated at a value of 7% while inflation dropped from 26% in 1994 to only 6% in 1995. Imports increased due to the influx of foreign aid, and exports, particularly from the country's garment industry, also increased. Although there was a constant economic growth, this growth translated to only about
0.71% for the ASEAN economy in 2016, compared with her neighbor Indonesia, which contributed 37.62%. 

After four years of improving economic performance, Cambodia's economy slowed in 1997–1998 due to the regional economic crisis, civil unrest, and political infighting. Foreign investments declined during this period. Also, in 1998 the main harvest was hit by drought. But in 1999, the first full year of relative peace in 30 years, progress was made on economic reforms and growth resumed at 4%.

Currently, Cambodia's foreign policy focuses on establishing friendly borders with its neighbors (such as Thailand and Vietnam), as well as integrating itself into regional (ASEAN) and global (WTO) trading systems. Some of the obstacles faced by this emerging economy are the need for a better education system and the lack of a skilled workforce; particularly in the poverty-ridden countryside, which struggles with inadequate basic infrastructure. Nonetheless, Cambodia continues to attract investors because of its low wages, plentiful labor, proximity to Asian raw materials, and favorable tax treatment.

Following its independence from France in 1953, the Cambodian state has undergone five periods of political, social, and economic transformation:

In 1989, the State of Cambodia implemented reform policies that transformed the Cambodian economic system from a command economy to an open market one. In line with the economic reformation, private property rights were introduced and state-owned enterprises were privatized. Cambodia also focused on integrating itself into regional and international economic blocs, such as the Association of South East Asian Nations and the World Trade Organization respectively. These policies triggered a growth in the economy, with its national GDP growing at an average of 6.1% before a period of domestic unrest and regional economic instability in 1997 (1997 Asian financial crisis). However, conditions improved and since 1999, the Cambodian economy has continued to grow at an average pace of approximately 6-8% per annum.

In 2007, Cambodia's gross domestic product grew by an estimated 18.6%. Garment exports rose by almost 8%, while tourist arrivals increased by nearly 35%. With exports decreasing, the 2007 GDP growth was driven largely by consumption and investment. Foreign direct investment (FDI) inflows reached US$600 million (7 percent of GDP), slightly more than what the country received in official aid. Domestic investment, driven largely by the private sector, accounted for 23.4 percent of GDP. Export growth, especially to the US, began to slow in late 2007 accompanied by stiffer competition from Vietnam and emerging risks (a slowdown in the US economy and lifting of safeguards on China's exports). US companies were the fifth largest investors in Cambodia, with more than $1.2 billion in investments over the period 1997-2007.

Cambodia was severely damaged by the financial crisis of 2007–2008, and its main economic sector, the garment industry, suffered a 23% drop in exports to the United States and Europe. As a result, 60,000 workers were laid off. However, in the last quarter of 2009 and early 2010, conditions were beginning to improve and the Cambodian economy began to recover. Cambodian exports to the US for the first 11 months of 2012 reached $2.49 billion, a 1 per cent increase year-on-year. Its imports of US goods grew 26 per cent for that period, reaching $213 million. Another factor underscoring the potential of the Cambodian economy is the recent halving of its poverty rate. The poverty rate is 20.5 per cent, meaning that approximately 2.8 million people live below the poverty line.

The table below represents the fluctuations in Cambodia's economy over the period from 2004–2011 (2012 data is not yet available). 

The garment industry represents the largest portion of Cambodia's manufacturing sector, accounting for 80% of the country's exports. In 2012, the exports grew to $4.61 billion up 8% over 2011. In the first half of 2013, the garment industry reported exports worth $1.56 billion. The sector employs 335,400 workers, of which 91% are female.

The sector operates largely on the final phase of garment production, that is turning yarns and fabrics into garments, as the country lacks a strong textile manufacturing base. In 2005, there were fears that the end of the Multi Fibre Arrangement would threaten Cambodia's garment industry; exposing it to stiff competition with China's strong manufacturing capabilities. On the contrary, Cambodia's garment industry at present continues to grow rapidly. This is can be attributed to the country's open economic policy which has drawn in large amounts of foreign investment into this sector of the economy.

Garment Factories by Ownership Nationality in 2010:

note: In 2010, 236 garment export-oriented factories were operating and registered with GMAC,
with 93% being foreign direct investment (FDI).

As seen in the table above, Cambodia's garment industry is characterized by a small percentage of local ownership. This is a reflection of the deficiency of skilled workers in the country as well as the limited leverage and autonomy Cambodian factories have in strategic decisions. Another characteristic of the industry is the country's competitive advantage as the only country where garment factories are monitored and reported according to national and international standards.

This has allowed Cambodia to secure its share of quotas for exports to the US through the US-Cambodia Trade Agreement on Textiles and Apparel (1999–2004), which linked market access to labor standards. However, the Cambodian garment industry remains vulnerable to global competition due to a lack of adequate infrastructure, labor unrest, the absence of a domestic textile industry, and almost complete dependence on imported textile material.

The Garment Manufacturers Association in Cambodia (GMAC) is establishing a specialized training institute to train garment workers. The institute is in Phnom Penh Special Economic Zone and will be completed by late 2016. It aims to train 1,600 garment workers in the first three years and 240 university students each year as part of a separate program.

Agriculture is the traditional mainstay of the Cambodian economy. Agriculture accounted for 90 percent of GDP in 1985 and employed approximately 80 percent of the work force. Rice is the principle commodity.

Major secondary crops include maize, cassava, sweet potatoes, groundnuts, soybeans, sesame seeds, dry beans, and rubber. The principal commercial crop is rubber. In the 1980s it was an important primary commodity, second only to rice, and one of the country's few sources of foreign exchange.

In the 1960s, Cambodia was a prominent tourist destination in the Southeast Asian region. Due to protracted periods of civil war, insurgencies, and especially the genocidal regime of the Khmer Rouge (see Khmer Rouge Genocide), Cambodia's tourism industry was reduced to being virtually non-existent. Since the late 1990s, tourism is fast becoming Cambodia's second largest industry, just behind the garment manufacturing. In 2006, Cambodia's tourism sector generated a revenue of US$1.594 billion, which made up approximately 16% of the country's GDP.
Cultural heritage tourism is especially popular in the country, with many foreign tourists visiting the ancient Hindu temple of Angkor Wat located in the Siem Reap province. Other popular tourist attractions include the Royal Palace, Phnom Penh, as well as ecotourism spots such as Tonlé Sap Lake and the Mekong River.

The tourism industry in Cambodia has been perpetuated by the development of important transportation infrastructure; in particular Cambodia's two international airports in Phnom Penh and Siem Reap respectively. To the Cambodian economy, tourism has been a means for accumulation of foreign currency earnings and employment for the Cambodian workforce, with about 250,000 jobs generated in 2006. Meanwhile, challenges to the industry include a leakage of revenue to foreign markets due to a dependence on foreign goods as well as the prevalence of the Child sex tourism industry.

The gambling industry of Cambodia supports its tourism industry, which is mostly concentrated around the Siem Reap province. The introduction of casino on border cities and towns created an industry that has thrived and contributed to the generation of employment and a steady stream of revenue for the government. However, the issue of corruption in relation to the government bureaucratic process involved in the gambling sector has been raised. It has likewise spur growth in different parts of the country at border crossing towns like Poipet, Bavet and Koh Kong. The growth of the gambling industry in Cambodia is due to its proximity to Thailand where gambling is forbidden.

The increase in tourist arrivals has led to growing demand for hotels and other forms of accommodation surrounding tourist hotspots. Siem Reap in particular has seen a construction boom in recent years. The capital Phnom Penh has also witnessed a growth in the construction and real estate sector. Recently, planned projects that have been on the pipeline for several years have been shelved temporarily due to a reduction in foreign investment.
From 2009, the Cambodian government has allowed foreigners to own condominiums. This has helped in attracting real estate investors from Thailand, Malaysia, Singapore and other countries.

The construction sector attracted investment of $2.1 billion in 2012 which is a 72 per cent rise compared with 2011. Construction licenses issued stood at 1,694 projects in 2012, which was 20% lower than 2011 but they were higher in value.

Oil seeps were discovered in Cambodia as early as the 1950s by Russian and Chinese geologists. Development of the industry was delayed, however, by the Vietnam and Cambodian Civil Wars and the political uncertainty that followed. Further discoveries of oil and natural gas deposits offshore in the early 2000s led to renewed domestic and international interest in Cambodia's production possibilities. As of 2013, the US company Chevron, Japanese JOGMEC and other international companies maintained production sites both on shore and off. Chevron alone had invested over US$160 million and drilled 18 wells.

Sok Khavan, acting director general of the Cambodian National Petroleum Authority, estimated that once the contracts are finalized and legal issues resolved, the Cambodian government will receive approximately 70% of the revenues, contributing to an economy in which the GDP is projected to increase five-fold by 2030. In addition, there are 10,000 square miles offshore in the Gulf of Thailand that holds potential reserves of 12-14 trillion cubic feet of natural gas and an unspecified amount of oil. The rights to this territory are currently a subject of dispute between Cambodia and Thailand, further delaying any possible production developments. In early 2013 it was reported that the two countries were close to a deal that would allow joint production to begin.

Cambodia's emerging democracy has received strong international support. Under the mandate of the United Nations Transitional Authority in Cambodia (UNTAC), $1.72 billion (1.72 G$) was spent in an effort to bring basic security, stability and democratic rule to the country. Various news and media reports suggest that since 1993 the country has been the recipient of some US$10 billion in foreign aid.

With regards to economic assistance, official donors had pledged $880 million at the Ministerial Conference on the Rehabilitation of Cambodia (MCRRC) in Tokyo in June 1992. In addition to that figure, $119 million was pledged in September 1993 at the International Committee on the Reconstruction of Cambodia (ICORC) meeting in Paris, and $643 million at the March 1994 ICORC meeting in Tokyo.

Cambodia experienced a shortfall in foreign aid in the year 2005 due to the government's failure to pass anti-corruption laws, opening up a single import/export window, increasing its spending on education, and complying with policies of good governance. In response, the government adopted the National Strategic Development Plan for 2006–10 (also known as the “Third Five-Year Plan”). The plan focused on three major areas:

There are no significant barriers to bank entry. At the end of 2013, there stood 35 commercial banks of which most have majority foreign ownership. Since 2011 new banks
with offshore funding have begun to enter the market.

Although Cambodia exports mainly garments and products from agriculture and fisheries, it is striving to diversify the economy. There is some evidence of expansion in value-added exports from a low starting point, largely thanks to the manufacture of electrical goods and telecommunications by foreign multinationals implanted in the country. Between 2008 and 2013, high-tech exports climbed from just US$3.8million to US$76.5 million.

It will be challenging for Cambodia to enhance the technological capacity of the many small and medium-sized enterprises (SMEs) active in agriculture, engineering and the natural sciences. Whereas the large foreign firms in Cambodia that are the main source of value-added exports tend to specialize in electrical machinery and telecommunications, the principal task for science and technology policy will be to facilitate spillovers in terms of skills and innovation capability from these large operators towards smaller firms and across other sectors.

There is little evidence that the Law on Patents, Utility Model Certificates and Industrial Designs (2006) has been of practical use, thus far, to any but the larger foreign firms operating in Cambodia. By 2012, 27 patent applications had been filed, all by foreigners. Of the 42 applications for industrial design received up to 2012, 40 had been filed by foreigners. Nevertheless, the law has no doubt encouraged foreign firms to introduce technological improvements to their on-shore production systems, which can only be beneficial.

 




</doc>
<doc id="5433" url="https://en.wikipedia.org/wiki?curid=5433" title="Telecommunications in Cambodia">
Telecommunications in Cambodia

Telecommunications in Cambodia include telephone, radio, television, and Internet services, which are regulated by the Ministry of Posts and Telecommunications. Transport and posts were restored throughout most of the country in the early 1980s during the People's Republic of Kampuchea regime after being disrupted under the Khmer Rouge.

In January 1987, the Soviet-aided Intersputnik space communications station began operation in Phnom Penh and established two-way telecommunication links between the Cambodian capital and the cities of Moscow, Hanoi, Vientiane and Paris. The completion of the earth satellite station restored the telephone and telex links among Phnom Penh, Hanoi, and other countries for the first time since 1975. Although telecommunications services were initially limited to the government, these advances in communications helped break down the country's isolation, both internally and internationally.

Today, with the availability of mobile phones, communications are open to all, though the country's Prime Minister Hun Sen decreed that 3G mobile phones would not be allowed to support video calling.

Smart Axiata, a leading telecommunications company, in 2019 conducted a live trial of its 5G network with support from China's Huawei. The company said it expects to begin rolling out 5G services in Cambodia by the end of 2019.

The government state communications corporation is Telecom Cambodia, founded in 2006 as an expansion of the telecom operating department of the Ministry of Posts and Telecommunications. 

Fixed line service in Phnom Penh and other provincial cities is available. Mobile-phone systems are widely used in urban areas to bypass deficiencies in the fixed-line network. Mobile phone coverage is rapidly expanding in rural areas. Mobile-cellular usage, aided by increasing competition among service providers, is increasing.

International calling access is adequate, but expensive. Fixed line and mobile service is available to all countries from Phnom Penh and major provincial cities.

In 2009 Cambodian broadcasters were a mixture of state-owned, joint public-private, and privately owned companies.

In 2009 there were roughly 50 radio broadcast stations - 1 state-owned broadcaster with multiple stations and a large mixture of public and private broadcasters. Several international broadcasters are also available.


There are radio stations in each of the following provinces: Banteay Meanchey, Battambang, Kampong Cham, Kampong Thom, Kampot, Kandal, Pailin, Preah Vihear, Siem Reap, Sihanoukville and Svay Rieng.

In 2009 there were 9 TV broadcast stations with most operating on multiple channels, including 1 state-operated station broadcasting from multiple locations, 6 stations either jointly operated or privately owned with some broadcasting from several locations, and 2 TV relay stations - one relaying a French TV station and the other relaying a Vietnamese TV station. Multi-channel cable and satellite systems are also available.



 the number of internet users in Cambodia rose to 15.8 million, about 98.5% of the population. According to the Telecommunications Regulator of Cambodia (TRC), the number of registered SIM cards rose by 9.4 percent during the first half of the year, reaching 20.8 million. The SIM card market is saturated, with Cambodia now having more active SIM cards than people. According to TRC, there are six telecommunications firms in the country: Cellcard, Smart Axiata, Metfone, Seatel, Cootel, and qb. Three companies, Metfone, Cellcard, and Smart, account for 90% of users. TRC noted that, as of February 2019, Facebook had seven million users in Cambodia.

List of Internet service providers


In its "Freedom on the Net 2013" report, Freedom House gives Cambodia a "Freedom on the Net Status" of "partly free". 

Compared to traditional media in Cambodia, new media, including online news, social networks and personal blogs, enjoy more freedom and independence from government censorship and restrictions. However, the government does proactively block blogs and websites, either on moral grounds, or for hosting content deemed critical of the government. The government restricts access to sexually explicit content, but does not systematically censor online political discourse. Since 2011 three blogs hosted overseas have been blocked for perceived antigovernment content. In 2012, government ministries threatened to shutter internet cafes too near schools—citing moral concerns—and instituted surveillance of cafe premises and cell phone subscribers as a security measure.

Early in 2011, very likely at the urging of the Ministry of Posts and Telecommunications, all Cambodian ISPs blocked the hosting service Blogspot, apparently in reaction to a December 2010 post on KI-Media, a blog run by Cambodians from both inside and outside the country. The site, which is often critical of the administration, described the prime minister and other officials as "traitors" after opposition leader Sam Rainsy alleged they had sold land to Vietnam at a contested national border. All ISPs but one subsequently restored service to the sites following customer complaints. In February 2011, however, multiple ISPs reinstated blocks on individual Blogspot sites, including KI-Media, Khmerization—another critical citizen journalist blog—and a blog by the Khmer political cartoonist Sacrava. 

There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms without appropriate legal authority. During 2012 NGOs expressed concern about potential online restrictions. In February and November, the government published two circulars, which, if implemented fully, would require Internet cafes to install surveillance cameras and restrict operations within major urban centers. Activists also reported concern about a draft “cybercrimes” law, noting that it could be used to restrict online freedoms. The government maintained it would only regulate criminal activity.

The constitution provides for freedom of speech and press; however, these rights were not always respected in practice. The 1995 press law prohibits prepublication censorship or imprisonment for expressing opinions; however, the government uses the penal code to prosecute citizens on defamation, disinformation, and incitement charges. The penal code does not prescribe imprisonment for defamation, but does for incitement or spreading disinformation, which carry prison sentences of up to three years. Judges also can order fines, which may lead to jail time if not paid. The constitution requires that free speech not adversely affect public security. 

The constitution declares that the king is “inviolable,” and a Ministry of Interior directive conforming to the defamation law reiterates these limits and prohibits publishers and editors from disseminating stories that insult or defame government leaders and institutions. The continued criminalization of defamation and disinformation and a broad interpretation of criminal incitement constrains freedom of expression. 

The law provides for the privacy of residence and correspondence and prohibits illegal searches; however, NGOs report that police routinely conduct searches and seizures without warrants. 

Corruption remains pervasive and governmental human rights bodies are generally ineffective. A weak judiciary that sometimes fails to provide due process or fair trial procedures is a serious problem. The courts lack human and financial resources and, as a result, are not truly independent and are subject to corruption and political influence.




</doc>
<doc id="5434" url="https://en.wikipedia.org/wiki?curid=5434" title="Transport in Cambodia">
Transport in Cambodia

The system of Transport in Cambodia, rudimentary at the best of times, was severely damaged in the chaos that engulfed the nation in the latter half of the 20th century. The country's weak transport infrastructure hindered emergency relief efforts, exacerbating the logistical issues of procurement of supplies in general and their distribution. Cambodia received Soviet technical assistance and equipment to support the maintenance of the transportation network.


Of the current total roadway network, only about 50% of the roads and highways are hard surfaced, all-weather, and in good condition. About 50% of the roads were constructed of crushed stone, gravel, or compacted earth. Secondary roads are of unimproved earth or were little more than tracks. In 1981 Cambodia opened a newly repaired section of National Route 1 which runs southeast from Phnom Penh to the Vietnamese border. The road, which suffered damage during the war years, was restored most probably by Vietnamese army engineers.

In the late-1980s, Cambodia's road network was both underutilized and unable to meet even the modest demands placed upon it by an preindustrial agrarian society. Commercial vehicles, such as trucks and buses, were insufficient in number and lacked the spare parts necessary to keep them running. Road construction and maintenance were ignored by a financially hard-pressed governments, while insurgents regularly destroyed bridges and rendered some routes unsafe for travel.

Cambodia is upgrading the main highways to international standards and most are vastly improved from 2006. Most main roads are now paved. And now road construction is on going from the Thailand border at Poipet to Siem Reap (Angkor Wat).

Chart of 01/2014

Motorcycles are by far the most common transport medium in Cambodia. "Cyclo" (as hand-me-down French) or cycle rickshaws were popular in 1990s but are increasingly replaced by "remorques" (carriages attached to motorcycles) and rickshaws imported from India. Cyclos are unique to Cambodia in that the cyclist sits behind the passenger(s) seat, as opposed to cycle rickshaws in neighbouring countries where the cyclist is at the front and "pulls" the carriage. With 78% mobile phone penetration rate, ride-hailing apps have become popular in recent years. The first locally owned ride-hailing app, ExNet taxi app, was launched in 2016, after which another locally developed PassApp taxi was also introduced. The ExNet and PassApp use the same technology and architect for their application, except that ExNet is a taxi-based ride-hailing service while PassApp is more of rickshaw-based one. Uber and Grab joined the market in 2017. The entry and later merger of Uber and Grab did not negatively affect the local apps as the locals have the first-mover advantage and could secure a large number of patrons. As of today, PassApp is seen as an able competitor for the Singapore-based Grab in the Cambodian transport market.

Aside from the private-hire vehicles and ride-hailing service, public transport is also available but only in the capital. Phnom Penh city bus service started in 2015 with only three routes under the assistance of JICA. Today, Phnom Penh City Bus operates 13 routes.

Two rail lines exist, both originating in Phnom Penh and totaling about 612 kilometers of 1,000 mm (3 ft 3 3⁄8 in) metre gauge single track. 
The first line or the northern line, built by The French colonial government, running from Phnom Penh to Poipet on the Thai border, between 1930 and 1940, with Phnom Penh Railway Station opening in 1932. 
The final connection with Thailand was completed by Royal State Railways in 1942. However, the service from Bangkok to Battambang was suspended when the French Indochinese Government resumed sovereignty over Battambang and the Sisophon area from Thailand on 17 December 1946, as Thailand was seen as a supporter of Khmer Issarak, the anti-French, Khmer nationalist political movement. 
A third line is planned to connect Phnom Penh with Vietnam, the last missing link of the planned rail corridor between Singapore and the city of Kunming, China. A new north-south line is also planned. The lines from Phnom Penh to Sisophon and from Sisophon to Poipet have been rehabilitated; starting with Poipet to Srey Sisophon in April 2018 and Sisophon to Phnom Penh in early July (2018). The active part, the southern line, of the network is the Phnom Penh-Sihanoukville line, with stops at Takeo and Kampot. The first line to be re-opened in Cambodia was the 118 km long route from Phnom Penh to Touk Meas and the complete southern line in May 2011 (or Toll Royal Railway). Toll Royal Railway (Cambodia) had been given a 30-year concession from The Royal Government of Cambodia to operate Cambodia's railway network.

Phnom Penh - Pursat - Moung Ruessei - Battambang - Sisophon - Poipet

Phnom Penh - Takeo - Touk Meas - Damnak Chang'aeur - Veal Renh - Sihanoukville

The nation's extensive inland waterways were important historically in domestic trade. The Mekong and the Tonlé Sap Rivers, their numerous tributaries, and the Tonlé Sap provided avenues of considerable length, including 3,700 kilometers navigable all year by craft drawing 0.6 meters and another 282 kilometers navigable to craft drawing 1.8 meters. In some areas, especially west of the Mekong River and north of the Tonle Sap River, the villages were completely dependent on waterways for communications. Launches, junks, or barges transport passengers, rice, and other food in the absence of roads and railways.

According to the Ministry of Communications, Transport, and Post, Cambodia's main ferry services crossing the Bassac River and the middle Mekong River were restored in 1985. The major Mekong River navigation routes also were cleared for traffic. Seaplane service to all waterways and islands in now offered by Aero Cambodia Airline.

Cambodia has two major ports, Phnom Penh Port and Sihanoukville Port, also known as "Kampong Som", and five minor ports. Phnom Penh, at the junction of the Bassac, the Mekong, and the Tonle Sap Rivers, is the only river port capable of receiving 8,000-ton ships during the wet season and 5,000-ton ships during the dry season. In 2018 the port received 205,000 TEUs totalling 2.9 million tonnes. Its 2018 profits were US$7.35 million, up 36% from 2017.

Sihanoukville port reopened in late-1979. It had been built in 1960 with French assistance. In 1980 some 180 Soviet dockworkers, having brought with them forklifts and trucks, were reportedly working at "Kampong Som" as longshoremen or as instructors of unskilled Cambodian port workers. By 1984 approximately 1,500 Cambodian port workers were handling 2,500 tons of cargo per day. According to official statistics, Sihanoukville had handled only 769,500 tons in the four prior years (1979 to 1983), a level that contrasted sharply with the port's peacetime capacity of about one million tons of cargo per year.


The country possesses twenty-six airfields, of which only thirteen were usable in the mid-1980s. Eight airfields had permanent-surface runways. Phnom Penh International Airport in Phnom Penh is the largest airport; it also serves as the main base for the renascent Cambodian Air Force.

Cambodia's second largest airport is Angkor International Airport in the tourist city of Siem Reap. Tourist traffic into Angkor International Airport saw passenger numbers overtake those of Phnom Penh in 2006, the airport now being the country's busiest.

Cambodia also opened a new Soviet-built airfield at Ream, Sihanoukville International Airport in late-1983, which never saw commercial air traffic until January 2007. There are additional airports in Battambang and Stung Treng.

The new national airline Cambodia Angkor Air was launched in 2009, with an investment from Vietnam Airlines. Aero Cambodia Airline started business in 2011 offering flights to all airports and waterways with seaplanes.







</doc>
