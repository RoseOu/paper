<doc id="5253" url="https://en.wikipedia.org/wiki?curid=5253" title="Constitution">
Constitution

A constitution is an aggregate of fundamental principles or established precedents that constitute the legal basis of a polity, organisation or other type of entity, and commonly determine how that entity is to be governed.

When these principles are written down into a single document or set of legal documents, those documents may be said to embody a "written constitution"; if they are written down in a single comprehensive document, it is said to embody a "codified constitution". Some constitutions (such as that of the United Kingdom) are uncodified, but written in numerous fundamental Acts of a legislature, court cases or treaties.

Constitutions concern different levels of organizations, from sovereign countries to companies and unincorporated associations. A treaty which establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights.

The Constitution of India is the longest written constitution of any country in the world, containing 444 articles in 22 parts, 12 schedules and 124 amendments, with 146,385 words in its English-language version. The Constitution of Monaco is the shortest written constitution, containing 10 chapters with 97 articles, and a total of 3,814 words.

The term "constitution" comes through French from the Latin word "constitutio", used for regulations and orders, such as the imperial enactments ("constitutiones principis": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an "apostolic constitution".

Generally, every modern written constitution confers specific powers on an organization or institutional entity, established upon the primary condition that it abides by the constitution's limitations. According to Scott Gordon, a political organization is constitutional to the extent that it "contain[s] institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority".

Activities of officials within an organization or polity that fall within the constitutional or statutory authority of those officials are termed "within power" (or, in Latin, "intra vires"); if they do not, they are termed "beyond power" (or, in Latin, "ultra vires"). For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities, these activities are considered to be "ultra vires" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial parliament in a federal state trying to legislate in an area that the constitution allocates exclusively to the federal parliament, such as ratifying a treaty. Action that appears to be beyond power may be judicially reviewed and, if found to be beyond power, must cease. Legislation that is found to be beyond power will be "invalid" and of no force; this applies to primary legislation, requiring constitutional authorization, and secondary legislation, ordinarily requiring statutory authorization. In this context, "within power", "intra vires", "authorized" and "valid" have the same meaning; as do "beyond power", "ultra vires", "not authorized" and "invalid".

In most but not all modern states the constitution has supremacy over ordinary statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is "null and void", and the nullification is "ab initio", that is, from inception, not from the date of the finding. It was never "law", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but that the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only that application may be ruled unconstitutional. Historically, the remedies for such violations have been petitions for common law writs, such as "quo warranto".

Excavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash "ca" 2300 BC. Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.

After that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur ("ca" 2050 BC). Some of the better-known ancient law codes include the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code and Mosaic law.

In 621 BC, a scribe named Draco codified the cruel oral laws of the city-state of Athens; this code prescribed the death penalty for many offences (nowadays very severe rules are often called "Draconian"). In 594 BC, Solon, the ruler of Athens, created the new "Solonian Constitution". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than on birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.
Aristotle ("ca" 350 BC) was the first to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was "the arrangement of the offices in a state". In his works "Constitution of Athens", "Politics", and "Nicomachean Ethics" he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system, including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.

The Romans first codified their constitution in 450 BC as the "Twelve Tables". They operated under a series of laws that were added from time to time, but Roman law was not reorganised into a single code until the "Codex Theodosianus" (AD 438); later, in the Eastern Empire, the "Codex repetitæ prælectionis" (534) was highly influential throughout Europe. This was followed in the east by the "Ecloga" of Leo III the Isaurian (740) and the "Basilica" of Basil I (878).

The "Edicts of Ashoka" established constitutional principles for the 3rd century BC Maurya king's rule in Ancient India. For constitutional principles almost lost to antiquity, see the code of Manu.

Many of the Germanic peoples that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic "Code of Euric" (471). This was followed by the "Lex Burgundionum", applying separate codes for Germans and for Romans; the "Pactus Alamannorum"; and the Salic Law of the Franks, all written soon after 500. In 506, the "Breviarum" or ""Lex Romana"" of Alaric II, king of the Visigoths, adopted and consolidated the "Codex Theodosianus" together with assorted earlier Roman laws. Systems that appeared somewhat later include the "Edictum Rothari" of the Lombards (643), the "Lex Visigothorum" (654), the "Lex Alamannorum" (730) and the "Lex Frisionum" ("ca" 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of Æthelberht of Kent (602). In ca. 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the "Doom book" code of laws for England.

Japan's "Seventeen-article constitution" written in 604, reportedly by Prince Shōtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than on institutions of government "per se", and remains a notable early attempt at a government constitution.

The Constitution of Medina (, Ṣaḥīfat al-Madīna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad after his flight (hijra) to Yathrib where he became political leader. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community – the Ummah.
The precise dating of the Constitution of Medina remains debated but generally scholars agree it was written shortly after the Hijra (622).

In Wales, the "Cyfraith Hywel" was codified by Hywel Dda ca. 942–950.

The "Pravda Yaroslava", originally combined by Yaroslav the Wise the Grand Prince of Kyiv, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the "Ruska Pravda", that became the law for all of Kievan Rus. It survived only in later editions of the 15th century.

In England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign "Magna Carta" in 1215. The most important single article of the "Magna Carta", related to ""habeas corpus"", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim – there must be due process of law first. This article, Article 39, of the "Magna Carta" read:

"No free man shall be arrested, or imprisoned, or deprived of his property, or outlawed, or exiled, or in any way destroyed, nor shall we go against him or send against him, unless by legal judgement of his peers, or by the law of the land."

This provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility, but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.

The Nomocanon of Saint Sava () was the first Serbian constitution from 1219. This legal act was well developed. St. Sava's Nomocanon was the compilation of Civil law, based on Roman Law and Canon law, based on Ecumenical Councils and its basic purpose was to organize functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while being at Mount Athos, using "The Nomocanon in Fourteen Titles", "Synopsis of Stefan the Efesian", "Nomocanon of John Scholasticus", Ecumenical Councils' documents, which he modified with the canonical commentaries of Aristinos and Joannes Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, translation of Prohiron and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was completely new compilation of civil and canonical regulations, taken from the Byzantine sources, but completed and reformed by St. Sava to function properly in Serbia. Beside decrees that organized the life of church, there are various norms regarding civil life, most of them were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.

Stefan Dušan, Emperor of Serbs and Greeks, enacted Dušan's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting is notable with the articles 171 and 172 of Dušan's Code, which regulated the juridical independence. They were taken from the Byzantine code Basilika (book VII, 1, 16–17).

In 1222, Hungarian King Andrew II issued the Golden Bull of 1222.

Between 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the "Sachsenspiegel", which became the supreme law used in parts of Germany as late as 1900.

In 1998, S. Kouyaté reconstructed from oral tradition what he claims is a 14th-century charter of the Mali Empire, called the "Kouroukan Fouga".

Around 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the "Fetha Negest" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law, and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The "Fetha Negest" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.
In the Principality of Catalonia, the Catalan constitutions were promulgated by the Court from 1283 (or even two centuries before, if we consider the Usatges of Barcelona as part of the compilation of Constitutions) until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made formally as a royal initiative, but required for its approval or repeal the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws had, as the other modern constitutions, preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.

The Golden Bull of 1356 was a decree issued by a "Reichstag" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.

In China, the Hongwu Emperor created and refined a document he called "Ancestral Injunctions" (first published in 1375, revised twice more before his death in 1398). These rules served in a very real sense as a constitution for the Ming Dynasty for the next 250 years.

The oldest written document still governing a sovereign nation today is that of San Marino. The "Leges Statutae Republicae Sancti Marini" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers and the powers assigned to them. The remaining books cover criminal and civil law, judicial procedures and remedies. Written in 1600, the document was based upon the "Statuti Comunali" (Town Statute) of 1300, itself influenced by the "Codex Justinianus", and it remains in force today.

In 1392 the "Carta de Logu" was legal code of the Giudicato of Arborea promulgated by the "giudicessa" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.

The "Gayanashagowa", the oral constitution of the Iroquois nation also known as the Great Law of Peace, established a system of governance in which sachems (tribal chiefs) of the members of the Iroquois League made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single tribe. The position of sachem descended through families, and were allocated by senior female relatives.

Historians including Donald Grinde, Bruce Johansen and others believe that the Iroquois constitution provided inspiration for the United States Constitution and in 1988 was recognised by a resolution in Congress. The thesis is not considered credible by some scholars. Stanford University historian Jack N. Rakove stated that "The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois" and stated that there are ample European precedents to the democratic institutions of the United States. Francis Jennings noted that the statement made by Benjamin Franklin frequently quoted by proponents of the thesis does not support this idea as it is advocating for a union against these "ignorant savages" and called the idea "absurd". Bruce Johansen contends Jennings, Tooker etc. have "humorlessly missed the ironic nature of Franklin's statement" and persist in "ignoring the relevant sources". Anthropologist Dean Snow stated that though Franklin's Albany Plan may have drawn some inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from this source and argues that "...such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception."

In 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution, and is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, "the Constitution State".

The English Protectorate that was set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short-lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell after Parliament consistently failed to govern effectively. Most of the concepts and ideas embedded into modern constitutional theory, especially bicameralism, separation of powers, the written constitution, and judicial review, can be traced back to the experiments of that period.

Drafted by Major-General John Lambert in 1653, the "Instrument of Government" included elements incorporated from an earlier document "Heads of Proposals", which had been agreed to by the Army Council in 1647, as a set of propositions intended to be a basis for a constitutional settlement after King Charles I was defeated in the First English Civil War. Charles had rejected the propositions, but before the start of the Second Civil War, the Grandees of the New Model Army had presented the "Heads of Proposals" as their alternative to the more radical Agreement of the People presented by the Agitators and their civilian supporters at the Putney Debates.

On January 4, 1649 the Rump Parliament declared "that the people are, under God, the original of all just power; that the Commons of England, being chosen by and representing the people, have the supreme power in this nation".

The "Instrument of Government" was adopted by Parliament on December 15, 1653 and Oliver Cromwell was installed as Lord Protector on the following day. The constitution set up a state council consisting of 21 members while executive authority was vested in the office of "Lord Protector of the Commonwealth"; this position was designated as a non-hereditary life appointment. It also required the calling of triennial Parliaments, with each sitting for at least five months.

The "Instrument of Government" was replaced in May 1657 by England's second, and last, codified constitution, the Humble Petition and Advice, proposed by Sir Christopher Packe. The Petition offered hereditary monarchy to Oliver Cromwell, asserted Parliament's control over issuing new taxation, provided an independent council to advise the king and safeguarded "Triennial" meetings of Parliament. A modified version of the Humble Petition with the clause on kingship removed was ratified on 25 May. This finally met its demise in conjunction with the death of Cromwell and the Restoration of the monarchy.

Other examples of European constitutions of this era were the Corsican Constitution of 1755 and the Swedish Constitution of 1772.

All of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.

What is sometimes called the "enlightened constitution" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).

"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host" was written in 1710 by Pylyp Orlyk, "hetman" of the Zaporozhian Host. It was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's "Spirit of the Laws". This Constitution also limited the executive authority of the "hetman", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.

Corsican Constitutions of 1755 and 1794 were inspired by Jean-Jacques Rousseau. The latter introduced universal suffrage for property owners.

The United States Constitution, ratified June 21, 1788, was influenced by the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.

The Polish–Lithuanian Commonwealth Constitution was passed on May 3, 1791. Its draft was developed by the leading minds of the Enlightenment in Poland such as King Stanislaw August Poniatowski, Stanisław Staszic, Scipione Piattoli, Julian Ursyn Niemcewicz, Ignacy Potocki and Hugo Kołłątaj. It was adopted by the Great Sejm and is considered the first constitution of its kind in Europe and the world's second oldest one after the American Constitution.

Another landmark document was the French Constitution, ratified on September 3, 1791.

On March 19, the Spanish Constitution of 1812 was ratified by a parliament gathered in Cadiz, the only Spanish continental city which was safe from French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South European and Latin American nations like, for example, Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.

In Brazil, the Constitution of 1824 expressed the option for the monarchy as political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, elder son of the king of Portugal. Pedro was crowned in 1822 as first emperor of Brazil. The country was ruled by Constitutional monarchy until 1889, when finally adopted the Republican model.

In Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to another absolute monarchy, Sweden. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones; but maintaining a hereditary monarch limited by the constitution, like the Spanish one.

The first Swiss Federal Constitution was put in force in September 1848 (with official revisions in 1878, 1891, 1949, 1971, 1982 and 1999).

The Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835. The first Serbian constitution (Sretenjski ustav) was adopted at the national assembly in Kragujevac on February 15, 1835.

The Constitution of Canada came into force on July 1, 1867 as the British North America Act, an act of the British Parliament. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Apart from the "Constitution Acts, 1867 to 1982", Canada's constitution also has unwritten elements based in common law and convention.

After tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by "philosopher-kings." Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for government from a legal and historical standpoint.

The Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary "laws of war and peace" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.

A seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.

Along the way several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions is not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.

The later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three "constitutions" involved: The first the "constitution of nature" that includes all of what was called "natural law." The second is the "constitution of society", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a "constitution of government". The second would include such elements as the making of decisions by public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that it is possible for provisions of a written constitution of government to be "unconstitutional" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.

Other writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying "constitutions" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.

Constitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.

Political economy theory regards constitutions as coordination devices that help citizens to prevent rulers from abusing power. If the citizenry can coordinate a response to police government officials in the face of a constitutional fault, then the government have the incentives to honor the rights that the constitution guarantees. An alternative view considers that constitutions are not enforced by the citizens at-large, but rather by the administrative powers of the state. Because rulers cannot themselves implement their policies, they need to rely on a set of organizations (armies, courts, police agencies, tax collectors) to implement it. In this position, they can directly sanction the government by refusing to cooperate, disabling the authority of the rulers. Therefore, constitutions could be characterized by a self-enforcing equilibria between the rulers and powerful administrators.

Most commonly, the term "constitution" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called "Basic Law".

The following are features of democratic constitutions that have been identified by political scientists to exist, in one form or another, in virtually all national constitutions.

A fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.

Most states in the world have codified constitutions.

Codified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have pointed out that high constitutional turnover within a given country may itself be detrimental to separation of powers and the rule of law.

States that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared "ultra vires" by a court, and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, approval in two terms of parliament, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.

Constitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called "unconstitutional constitutional law".

Codified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights. In ethnic nation-states such as Estonia, the mission of the state can be defined as preserving a specific nation, language and culture.

 only two sovereign states, New Zealand and the United Kingdom, have wholly uncodified constitutions. The Basic Laws of Israel have since 1950 been intended to be the basis for a constitution, but as of 2017 it had not been drafted. The various Laws are considered to have precedence over other laws, and give the procedure by which they can be amended, typically by a simple majority of members of the Knesset (parliament).

Uncodified constitutions are the product of an "evolution" of laws and conventions over centuries (such as in the Westminster System that developed in Britain). By contrast to codified constitutions, uncodified constitutions include both written sources – e.g. constitutional statutes enacted by the Parliament – and unwritten sources – constitutional conventions, observation of precedents, royal prerogatives, customs and traditions, such as holding general elections on Thursdays; together these constitute British constitutional law.

Some constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contains constitutional conventions, thus is partially unwritten.

The Constitution of Canada, which evolved from the British North America Acts until severed from nominal British control by the Canada Act 1982 (analogous to the Australia Act 1986), is a similar example. Canada's constitution consists of around 30 different statutes.

The terms "written constitution" and "codified constitution" are often used interchangeably, as are "unwritten constitution" and "uncodified constitution", although this usage is technically inaccurate. A codified constitution is a single document; states that do not have such a document have uncodified, but not entirely unwritten, constitutions, since much of an uncodified constitution is usually written in laws such as the Basic Laws of Israel and the Parliament Acts of the United Kingdom. Uncodified constitutions largely lack protection against amendment by the government of the time. For example, the U.K. Fixed-term Parliaments Act 2011 legislated by simple majority for strictly fixed-term parliaments; until then the ruling party could call a general election at any convenient time up to the maximum term of five years. This change would require a constitutional amendment in most nations.

The presence or lack of entrenchment is a fundamental feature of constitutions. An entrenched constitution cannot be altered in any way by a legislature as part of its normal business concerning ordinary statutory laws, but can only be amended by a different and more onerous procedure. There may be a requirement for a special body to be set up, or the proportion of favourable votes of members of existing legislative bodies may be required to be higher to pass a constitutional amendment than for statutes. The entrenched clauses of a constitution can create different degrees of entrenchment, ranging from simply excluding constitutional amendment from the normal business of a legislature, to making certain amendments either more difficult than normal modifications, or forbidden under any circumstances.

Entrenchment is an inherent feature in most codified constitutions. A codified constitution will incorporate the rules which must be followed for the constitution itself to be changed.

The U.S. constitution is an example of an entrenched constitution, while the U.K. constitution is an example of a constitution that is not entrenched (or codified). In some states the text of the constitution may be changed; in others the original text is not changed, and amendments are passed which add to and may override the original text and earlier amendments.

Procedures for constitutional amendment vary between states. In a nation with a federal system of government the approval of a majority of state or provincial legislatures may be required. Alternatively, a national referendum may be required. Details are to be found in the articles on the constitutions of the various nations and federal states in the world.

In constitutions that are not entrenched, no special procedure is required for modification. Lack of entrenchment is a characteristic of uncodified constitutions; the constitution is not recognised with any higher legal status than ordinary statutes. In the U.K., for example, laws which modify written or unwritten provisions of the constitution are passed on a simple majority in Parliament. No special "constitutional amendment" procedure is required. The principle of parliamentary sovereignty holds that no sovereign parliament may be bound by the acts of its predecessors; and there is no higher authority that can create law which binds Parliament. The sovereign is nominally the head of state with important powers, such as the power to declare war; the uncodified and unwritten constitution removes all these powers in practice.

In practice democratic governments do not use the lack of entrenchment of the constitution to impose the will of the government or abolish all civil rights, as they could in theory do, but the distinction between constitutional and other law is still somewhat arbitrary, usually following historical principles embodied in important past legislation. For example, several British Acts of Parliament such as the Bill of Rights, Human Rights Act and, prior to the creation of Parliament, Magna Carta are regarded as granting fundamental rights and principles which are treated as almost constitutional. Several rights that in another state might be guaranteed by constitution have indeed been abolished or modified by the British parliament in the early 21st century, including the unconditional right to trial by jury, the right to silence without prejudicial inference, permissible detention before a charge is made extended from 24 hours to 42 days, and the right not to be tried twice for the same offence.

The strongest level of entrenchment exists in those constitutions that state that some of their most fundamental principles are absolute, i.e. certain articles may not be amended under any circumstances. An amendment of a constitution that is made consistently with that constitution, except that it violates the absolute non-modifiability, can be called an "unconstitutional constitutional law". Ultimately it is always possible for a constitution to be overthrown by internal or external force, for example, a revolution (perhaps claiming to be justified by the right to revolution) or invasion. In the Constitution of India, the Supreme Court has created the Doctrine of Basic Structure in Kesavananda Bharti's case (1973) stating that the essential features of the Basic structure cannot be amended by the Parliament. The Court has identified judicial review, independence of Judiciary, free and fair election, core of Fundamental Rights as a few of the essential features which are unamendable. However, the Supreme Court did not identify specific provisions which are in the category of absolute entrenchment. A critical analysis of the Doctrine of Basic Structure appears in Professor M. K. Bhandari's book "Basic Structure of Indian Constitution – A Critical Reconsideration".

An example of absolute unmodifiability is found in the German constitution. Articles 1 and 20 protect human dignity, human rights, democracy, rule of law, federal and social state principles, and the people's right of resistance as a last resort against an attempt to abolish the constitutional order. Article 79, Section 3 states that these principles cannot be changed, even according to the methods of amendment defined elsewhere in the document, until a new constitution comes into effect.

Another example is the Constitution of Honduras, which has an article stating that the article itself and certain other articles cannot be changed in any circumstances. Article 374 of the Honduras Constitution asserts this unmodifiability, stating, "It is not possible to reform, in any case, the preceding article, the present article, the constitutional articles referring to the form of government, to the national territory, to the presidential period, the prohibition to serve again as President of the Republic, the citizen who has performed under any title in consequence of which she/he cannot be President of the Republic in the subsequent period." This unmodifiability article played an important role in the 2009 Honduran constitutional crisis.

Constitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.

In a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates that sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.

A federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called "states", "provinces", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).

A confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called "confederal" states are actually federal.

To some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example, the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.

Constitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.

In presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.

In parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a monarchy, it is the monarch who appoints and dismisses ministers, on the advice of the prime minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country, loses a particularly important vote in parliament, such as vote on the budget. When a government loses confidence, it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.

Many constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This provision can be and has been abused to allow a government to suppress dissent without regard for human rights – see the article on state of emergency.

Italian political theorist Giovanni Sartori noted the existence of national constitutions which are a facade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a facade and back again as democratic and autocratic governments succeed each other.

Constitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.

A constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.

Some countries, mainly those with uncodified constitutions, have no such courts at all. For example, the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts.


"Judicial philosophies of constitutional interpretation (note: generally specific to United States constitutional law)"



</doc>
<doc id="5254" url="https://en.wikipedia.org/wiki?curid=5254" title="Common law">
Common law

Common law (also known as judicial precedent or judge-made law) is the body of law derived from judicial decisions of courts and similar tribunals. The defining characteristic of "common law" is that it arises as precedent. In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts, and synthesizes the principles of those past cases as applicable to the current facts. If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (a principle known as "stare decisis"). If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (called a "matter of first impression"), and legislative statutes are either silent or ambiguous on the question, judges have the authority and duty to resolve the issue (one party or the other has to win, and on disagreements of law, judges make that decision). The court states an opinion that gives reasons for the decision, and those reasons agglomerate with past decisions as precedent to bind future judges and litigants. Common law, as the body of law made by judges, stands in contrast to and on equal footing with statutes which are adopted through the legislative process, and regulations which are promulgated by the executive branch (the interactions among these different sources of law are explained later in this article). "Stare decisis", the principle that cases should be decided according to consistent principled rules so that similar facts will yield similar results, lies at the heart of all common law systems.

The common lawso named because it was "common" to all the king's courts across Englandoriginated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. The British Empire spread the English legal system to its colonies, many of which retain the common law system today. These "common law systems" are legal systems that give great weight to judicial precedent, and to the style of reasoning inherited from the English legal system.

The term "common law" has many connotations. The first three set out here are the most-common usages within the legal community. Other connotations from past centuries are sometimes seen and are sometimes heard in everyday speech.

The first definition of "common law" given in "Black's Law Dictionary", 10th edition, 2014, is "The body of law derived from judicial decisions, rather than from statutes or constitutions; [synonym] CASELAW, [contrast] STATUTORY LAW." This usage is given as the first definition in modern legal dictionaries, is characterized as the "most common" usage among legal professionals, and is the usage frequently seen in decisions of courts. In this connotation, "common law" distinguishes the authority that promulgated a law. For example, the law in most Anglo-American jurisdictions includes "statutory law" enacted by a legislature, "regulatory law" (in the U.S.) or “delegated legislation” (in the U.K.) promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law or "case law", "i.e.", decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into
Publication of decisions, and indexing, is essential to the development of common law, and thus governments and private publishers publish law reports. While all decisions in common law jurisdictions are precedent (at varying levels and scope as discussed throughout the article on precedent), some become "leading cases" or "landmark decisions" that are cited especially often.

"Black's Law Dictionary" 10th Ed., definition 2, differentiates "common law" jurisdictions and legal systems from "civil law" or "code" jurisdictions. Common law systems place great weight on court decisions, which are considered "law" with the same force of law as statutes—for nearly a millennium, common law courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean.

By contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act if there is no statute. Civil law judges tend to give less weight to judicial precedent, which means that a civil law judge deciding a given case has more freedom to interpret the text of a statute independently (compared to a common law judge in the same circumstances), and therefore less predictably. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law. The role of providing overarching principles, which in common law jurisdictions is provided in judicial opinions, in civil law jurisdictions is filled by giving greater weight to scholarly literature, as explained below.

Common law systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.

"Black's Law Dictionary" 10th Ed., definition 4, differentiates "common law" (or just "law") from "equity". Before 1873, England had two complementary court systems: courts of "law" which could only award money damages and recognized only the legal owner of property, and courts of "equity" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States. The states of Delaware, Illinois, Mississippi, South Carolina, and Tennessee continue to have divided Courts of Law and Courts of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.

For most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law and equity, though under potentially different procedural law. Nonetheless, the historical distinction between "law" and "equity" remains important today when the case involves issues such as the following:
Courts of equity rely on common law principles of binding precedent.

In addition, there are several historical (but now archaic) uses of the term that, while no longer current, provide background context that assists in understanding the meaning of "common law" today.

In one usage that is now archaic, but that gives insight into the history of the common law, "common law" referred to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied. That usage is obsolete today. It is both underinclusive and overinclusive, as discussed in the section on "misconceptions".

"Common law" as the term is used today in common law countries contrasts with "ius commune". While historically the "ius commune" became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.

The English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.

"Black's Law Dictionary" 10th Ed., definition 3 is "General law common to a country as a whole, as opposed to special law that has only local application." From at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice in "assizes". The term "common law" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.

As used by non-lawyers in popular culture, the term "common law" connotes law based on ancient and unwritten universal custom of the people. The "ancient unwritten universal custom" view was the view among lawyers and judges from the earliest times to the mid-19th century. But for 100 years, lawyers and judges have recognized that the "ancient unwritten universal custom" view does not accord with the facts of the origin and growth of the law, and it is not held within the legal profession today.
Under the modern view, "common law" is not grounded in "custom" or "ancient usage", but rather acquires force of law instantly (without the delay implied by the term "custom" or "ancient") when pronounced by a higher court, because and to the extent the proposition is stated in judicial opinion. From the earliest times through the late 19th century, the dominant theory was that the common law was a pre-existent law or system of rules, a social standard of justice that existed in the habits, customs, and thoughts of the people. Under this older view, the legal profession considered it no part of a judge's duty to make new or change existing law, but only to expound and apply the old. By the early 20th century, largely at the urging of Oliver Wendell Holmes (as discussed throughout this article), this view had fallen into the minority view: Holmes pointed out that the older view worked undesirable and unjust results, and hampered a proper development of the law. In the century since Holmes, the dominant understanding has been that common law "decisions are themselves law, or rather the rules which the courts lay down in making the decisions constitute law". Holmes wrote in a 1917 opinion, "The common law is not a brooding omnipresence in the sky, but the articulate voice of some sovereign or quasi sovereign that can be identified." Among legal professionals (lawyers and judges), the change in understanding occurred in the late 19th and early 20th centuries (as explained later in this article), though lay dictionaries were decades behind in recognizing the change.

The reality of the modern view can be seen in practical operation: under the old "ancient unwritten universal custom" view, (a) jurisdictions could not logically diverge from each other (but nonetheless did), (b) a new decision logically needed to operate retroactively (but did not), and (c) there was no standard to decide which English medieval customs should be "law" and which should not. All three tensions resolve under the modern view: (a) the common law in different jurisdictions may diverge, (b) new decisions need not have retroactive operation, and (c) court decisions are effective immediately as they are issued, not years later, or after they become "custom", and questions of what "custom" might have been at some "ancient" time are simply irrelevant.


In a common law jurisdiction several stages of research and analysis are required to determine "what the law is" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines "what the law is". Then, one applies that law to the facts.

In practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.

Oliver Wendell Holmes, Jr. cautioned that "the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions." Justice Cardozo noted the "common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively", but "[i]ts method is inductive, and it draws its generalizations from particulars".

The common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is totally intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).

One example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, "Winterbottom v. Wright", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The "Winterbottom" court recognized that there would be "absurd and outrageous consequences" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract ("privity") with the negligent party.

A first exception to this rule arose in 1852, in the case of "Thomas v. Winchester", when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put "human life in imminent danger". "Thomas" relied on this reason to create an exception to the "privity" rule. In, 1909, New York held in "Statler v. Ray Mfg. Co." that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn "was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed".

Yet the privity rule survived. In "Cadillac Motor Car Co. v. Johnson", (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was "no question that the wheel was made of dead and 'dozy' wood, quite insufficient for its purposes." The "Cadillac" court was willing to acknowledge that the case law supported exceptions for "an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor". However, held the "Cadillac" court, "one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud,"

Finally, in the famous case of "MacPherson v. Buick Motor Co.", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to "Cadillac" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:

Cardozo's new "rule" exists in no prior case, but is inferrable as a synthesis of the "thing of danger" principle stated in them, merely extending it to "foreseeable danger" even if "the purposes for which it was designed" were not themselves "a source of great danger". "MacPherson" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of "Winterbottom", that "absurd and outrageous consequences" must be avoided, and he does so by drawing a new line in the last sentence quoted above: "There must be knowledge of a danger, not merely possible, but probable." But while adhering to the underlying principle that "some" boundary is necessary, "MacPherson" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.

The example of the evolution of the law of negligence in the preceding paragraphs illustrates two crucial principles: (a) The common law evolves, this evolution is in the hands of judges, and judges have "made law" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.

All law systems rely on written publication of the law, so that it is accessible to all. Common law decisions are published in law reports for use by lawyers, courts and the general public.

After the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law. The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions "in the raw," while private sector publishers often add indexing, editorial analysis, and similar finding aids.

In common law legal systems, the common law is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.

In almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the interstitial common law. To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy.

In common law jurisdictions (in the sense opposed to "civil law"), legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law. As the United States Supreme Court explained in "United States v Texas", 507 U.S. 529 (1993):

For example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document—when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated—for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions will be inherited from pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)

With the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited "ex post facto" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of "United States v. Hudson", which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutional) statute defining the offense and the penalty for it.

Still, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that "The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly."

By contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly—that is, limited to their precise terms—because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some "second order" constitutional law provision ("cf". judicial activism). This principle is applied more strongly in fields of commercial law (contracts and the like) where predictability is of relatively higher value, and less in torts, where courts recognize a greater responsibility to "do justice.".

Where a tort is rooted in common law, all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.

In the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: "The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority..." The first landmark decision on "the judicial power" was "Marbury v. Madison", . Later cases interpreted the "judicial power" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.

The interactions between decisions of different courts is discussed further in the article on precedent.

The United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.

Most of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting "en banc" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.

Other courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit "en banc", and thus the "later" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts—for example, Supreme Court interpretations of the constitution or federal statutes—are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.

In the jurisdictions of England and Wales and of Northern Ireland, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify criminal law decisions of lower courts; it is the final court of appeal for civil law cases in all three of the UK jurisdictions but not for criminal law cases in Scotland. From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.

Canada's federal system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.

The reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, "in most matters it is more important that the applicable rule of law be settled than that it be settled right." This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.

In contrast, in jurisdictions with very weak respect for precedent, fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less "a priori" guidance (unless the written law is very clear and kept updated) and must often leave a bigger "safety margin" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.

This is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York—and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a "choice of law clause" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on.) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.

Likewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.

This is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures—but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example).

The common lawso named because it was "common" to all the king's courts across Englandoriginated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. Prior to the Norman Conquest, much of England's legal business took place in the local folk courts of its various shires and hundreds. A variety of other individual courts also existed across the land: urban boroughs and merchant fairs held their own courts, as did the universities of Oxford and Cambridge, and large landholders also held their own manorial and seigniorial courts as needed. Additionally, the Catholic Church operated its own court system that adjudicated issues of canon law.

The main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).

The doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.

The form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.

The early development of case-law in the thirteenth century has been traced to Bracton's "On the Laws and Customs of England" and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died. The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.
In 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law "common" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system—citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.

Henry II developed the practice of sending judges from his own central court to hear the various disputes throughout the country. His judges would resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as "stare decisis" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name "common law".

Henry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the Archbishop gave rise to a wave of popular outrage against the King. Henry was forced to repeal the disputed laws and to abandon his efforts to hold church members accountable for secular crimes (see also Constitutions of Clarendon).

The English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.

Judge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law—"legislating from the bench" is a traditional and essential function of courts, which was carried over into the U.S. system as an essential component of the "judicial power" specified by Article III of the U.S. Constitution. Justice Oliver Wendell Holmes, Jr. summarized centuries of history in 1917, "judges do and must legislate." There are legitimate debates on how the powers of courts and legislatures should be balanced. However, the view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.

In England, judges have devised a number of rules as to how to deal with precedent decisions.

The term "common law" is often used as a contrast to Roman-derived "civil law", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.

By the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton's "De Legibus et Consuetudinibus Angliae" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's "Institutes". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into "in rem" (typically, actions against a "thing" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and "in personam" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's "Commentaries on the Laws of England", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.

The first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, "Institutes of the Lawes of England" in the 17th century.

The next definitive historical treatise on the common law is "Commentaries on the Laws of England", written by Sir William Blackstone and first published in 1765–1769.

A reception statute is a statutory law adopted when a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence common law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.

Other examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.

Yet, adoption of the common law in the newly-independent nation was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception. Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively-enacted civil law under the control of the political process, rather than the common law developed by judges that—by design—were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to "life, liberty, and the pursuit of happiness" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly-independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later president) John Adams complained that he "suffered very much for the want of books". To bootstrap this most basic need of a common law system—knowable, written law—in 1803, lawyers in Massachusetts donated their books to found a law library. A Jeffersonian newspaper criticized the library, as it would carry forward "all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose."

For several decades after independence, English law still exerted influence over American common law – for example, with "Byrne v Boadle" (1863), which first applied the res ipsa loquitur doctrine.

Well into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, "One cannot be a judge in one's own cause" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.

Reliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, came under critical discussion in the late 19th century, starting in the United States. Oliver Wendell Holmes, Jr. in his famous article, "The Path of the Law", commented, "It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past." Justice Holmes noted that study of maxims might be sufficient for "the man of the present", but "the man of the future is the man of statistics and the master of economics". In an 1880 lecture at Harvard, he wrote:

The life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.

In the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.

Reliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors "should" influence adjudication is the subject of active debate, but it is indisputable that judges "do" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.

As early as the 15th century, it became the practice that litigants who felt they had been cheated by the common law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail.

In England, courts of law (as opposed to equity) were combined with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.

In the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the "civil action". Fed.R.Civ.P. . The distinction survives to the extent that issues that were "common law (as opposed to equity)" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and "equity" issues are decided by a judge.

The states of Delaware, Illinois, Mississippi, South Carolina, and Tennessee continue to have divided Courts of Law and Courts of Chancery, for example, the Delaware Court of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.

For centuries, through to the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: Debt, Detinue, Covenant, Special Assumpsit, General Assumpsit, Trespass, Trover, Replevin, Case (or Trespass on the Case), and Ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific "magic words" encrusted over the centuries. Under the old common law pleading standards, a suit by a "pro se" ("for oneself," without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.

One of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant "a short and plain statement" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.

The main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of Central and South America.

The primary contrast between the two systems is the role of written decisions and precedent.

In common law jurisdictions, nearly every case that presents a "bona fide" disagreement on the law is resolved in a written opinion. The legal reasoning for the decision, known as "ratio decidendi", not only determines the court's judgment between the parties, but also stands as precedent for resolving future disputes. In contrast, civil law decisions typically do not include explanatory opinions, and thus no precedent flows from one decision to the next.
In common law systems, a single decided case is binding common law (connotation 1) to the same extent as statute or regulation, under the principle of "stare decisis". In contrast, in civil law systems, individual decisions have only advisory, not binding effect. In civil law systems, case law only acquires weight when a long series of cases use consistent reasoning, called "jurisprudence constante". Civil law lawyers consult case law to obtain their best prediction of how a court will rule, but comparatively, civil law judges are less bound to follow it.

For that reason, statutes in civil law systems are more comprehensive, detailed, and continuously updated, covering all matters capable of being brought before a court.

Common law systems tend to give more weight to separation of powers between the judicial branch and the executive branch. In contrast, civil law systems are typically more tolerant of allowing individual officials to exercise both powers. One example of this contrast is the difference between the two systems in allocation of responsibility between prosecutor and adjudicator.

Common law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, in civil law systems, criminal proceedings proceed under an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.

The examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.

The proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change his or her evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.

There are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages ("e.g.", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.

The role of the legal academy presents a significant "cultural" difference between common law (connotation 2) and civil law jurisdictions. In both systems, treatises compile decisions and state overarching principles that (in the author's opinion) explain the results of the cases. In neither system are treatises considered "law," but the weight given them is nonetheless quite different.

In common law jurisdictions, lawyers and judges tend to use these treatises as only "finding aids" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. Chief Justice Roberts noted the "great disconnect between the academy and the profession." When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.

In contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role.

The contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.

Examples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. federal courts and most but not all of the states have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But note that in each case, the statute sets the general principles, but the interstitial common law process determines the scope and application of the statute.

An example of convergence from the other direction is shown in the 1982 decision "Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health" (), in which the European Court of Justice held that questions it has already answered need not be resubmitted. This showed how a historically distinctly common law principle is used by a court composed of judges (at that time) of essentially civil law jurisdiction.

The former Soviet Bloc and other socialist countries used a socialist law system, although there is controversy as to whether socialist law ever constituted a separate legal system or not.

Much of the Muslim world uses legal systems based on Sharia (also called Islamic law).

Many churches use a system of canon law. The canon law of the Catholic Church influenced the common law during the medieval period through its preservation of Roman law doctrine such as the presumption of innocence.

The common law constitutes the basis of the legal systems of:

and many other generally English-speaking countries or Commonwealth countries (except the UK's Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the bijuridicial law or civil code of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed Common Law and Civil Law systems.

The remainder of this section discusses jurisdiction-specific variants, arranged chronologically.

Scotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of "precedent" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a "precedent", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a "stare decisis" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same, but in many matters (particularly those of UK-wide interest), they are similar.

Scotland shares the Supreme Court with England, Wales and Northern Ireland for civil cases; the court's decisions are binding on the jurisdiction from which a case arises but only influential on similar cases arising in Scotland. This has had the effect of converging the law in certain areas. For instance, the modern UK law of negligence is based on "Donoghue v Stevenson", a case originating in Paisley, Scotland.

Scotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals. The highest court of appeal in civil cases brought in Scotland is now the Supreme Court of the United Kingdom (before October 2009, final appellate jurisdiction lay with the House of Lords).

The centuries-old authority of the common law courts in England to develop law case by case and to apply statute law—"legislating from the bench"—is a traditional function of courts, which was carried over into the U.S. system as an essential component of the "judicial power" specified by Article III of the U.S. constitution. Justice Oliver Wendell Holmes, Jr. summarized centuries of history in 1917, "judges do and must legislate" (in the federal courts, only interstitially, in state courts, to the full limits of common law adjudicatory authority).

The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. In 1664, the colony of New York had two distinct legal systems: on Manhattan Island and along the Hudson River, sophisticated courts modeled on those of the Netherlands were resolving disputes learnedly in accordance with Dutch customary law. On Long Island, Staten Island, and in Westchester, on the other hand, English courts were administering a crude, untechnical variant of the common law carried from Puritan New England and practiced without the intercession of lawyers. When the English finally regained control of New Netherland they imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. New York began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.

Under Louisiana's codified system, the Louisiana Civil Code, private law—that is, substantive law between private sector parties—is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.

Louisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.

Historically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows. 

The U.S. state of California has a system based on common law, but it has codified the law in the manner of the civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case "Li v. Yellow Cab Co.", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)

The United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining broad statutory language (connotation 1(b) above), but, unlike state courts, do not act as an independent source of common law.

Before 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.

In 1938, the U.S. Supreme Court in "Erie Railroad Co. v. Tompkins" 304 U.S. 64, 78 (1938), overruled earlier precedent, and held "There is no federal general common law," thus confining the federal courts to act only as interstitial interpreters of law originating elsewhere. "E.g.", "Texas Industries v. Radcliff", (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.

Later courts have limited "Erie" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. "See, e.g.", "Clearfield Trust Co. v. United States", (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); "see also" "International News Service v. Associated Press", 248 U.S. 215 (1918) (creating a cause of action for misappropriation of "hot news" that lacks any statutory grounding); "but see National Basketball Association v. Motorola, Inc.", 105 F.3d 841, 843–44, 853 (2d Cir. 1997) (noting continued vitality of "INS" "hot news" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.

Most executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.

For example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).

The law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.

Ancient India represented a distinct tradition of law, and had an historically independent school of legal theory and practice. The "Arthashastra", dating from 400 BCE and the "Manusmriti", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.

When India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.

Post-partition, India retained its common law system. Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.

The exception to this rule is in the state of Goa, annexed in stages in the 1960s through 1980s. In Goa, a Portuguese uniform civil code is in place, in which all religions have a common law regarding marriages, divorces and adoption.

Post-partition, Pakistan retained its common law system.

Post-partition, Bangladesh retained its common law system.

Canada has separate federal and provincial legal systems. The division of jurisdiction between the federal and provincial Parliaments is specified in the Canadian constitution.

Each province and territory is considered a separate jurisdiction with respect to common law matters. As such, only the provincial legislature may enact legislation to amend private law. Each has its own procedural law, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. This is the highest court in provincial jurisdiction, only subject to the Supreme Court of Canada in terms of appeal of their decisions. All but one of the provinces of Canada use a common law system (the exception being Quebec, which uses a French-heritage civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).

Canadian Federal Courts operate under a separate system throughout Canada and deal with narrower subject matter than superior courts in provincial jurisdiction. They hear cases reserved for federal jurisdiction by the Canadian constitution, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate level court in federal jurisdiction and hears cases in multiple cities, and unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.

Criminal law is uniform throughout Canada. It is based on the constitution and federal statutory Criminal Code, as interpreted by the Supreme Court of Canada. The administration of justice and enforcement of the criminal code are the responsibilities of the provinces.

Canadian federal statutes must use the terminology of both the common law and civil law for those matters; this is referred to as legislative bijuralism.

Nicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, USA interventions/occupations during the period from 1909 to 1933, the influence of USA institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of USA culture and institutions.

Israel has a common law legal system. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).

Roman Dutch Common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri-Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French "code civil" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.

Ghana follows the English common-law tradition which was inherited from the British during her colonisation. Consequently, the laws of Ghana are, for the most part, a modified version of imported law that is continuously adapting to changing socio-economic and political realities of the country. The Bond of 1844 marked the period when the people of Ghana (then Gold Coast) ceded their independence to the British and gave the British judicial authority. Later, the Supreme Court Ordinance of 1876 "formally" introduced British law, be it the common law or statutory law, in the Gold Coast. Section 14 of the Ordinance formalised the application of the common-law tradition in the country.

Ghana, after independence, did not do away with the common law system inherited from the British, and today it has been enshrined in the 1992 Constitution of the country. Chapter four of Ghana's Constitution, entitled "The Laws of Ghana", has in Article 11(1) the list of laws applicable in the state. This comprises (a) the Constitution; (b) enactments made by or under the authority of the Parliament established by the Constitution; (c) any Orders, Rules and Regulations made by any person or authority under a power conferred by the Constitution; (d) the existing law; and (e) the common law. Thus, the modern-day Constitution of Ghana, like those before it, embraced the English common law by entrenching it in its provisions. The doctrine of judicial precedence which is based on the principle of "stare decisis" as applied in England and other pure common law countries also applies in Ghana.

South Korea is undergoing a transition of its legal system to common law due to its obligations to open its legal market to overseas law firms, influence of strong feminism in the state system to enable and codify much stronger penalties for sex and gender related offences as it would be possible under a civil law system (related new laws were copied & pasted from US/UK law and strengthened), and overall preference to US/UK systems instead of the previous civil law system that was initially influenced by Japan, as Koreans have a very strong anti-Japanese sentiment due to past brutal colonial rule. South Korea has introduced a US-style jury system different than the lay judge system of Germany and Japan (on which the previous legal system of Korea was based on), emphasis of precedents rather than written law, imposition of harsh and maximum penalties (maximum jail terms twice as long than in Japan and Germany, true life sentences), transition to punitive justice system and planned transfer of investigation powers to police, establishment of US-style appeals court, additive penalties among others.

Edward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament, wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his "Institutes" and "Reports" until the end of the 18th century. His works are still cited by common law courts around the world.

The next definitive historical treatise on the common law is "Commentaries on the Laws of England", written by Sir William Blackstone and first published in 1765–1769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.

While he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Oliver Wendell Holmes, Jr. published a short volume called "The Common Law", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law "is"; rather, Holmes describes the common law "process". Law professor John Chipman Gray's "The Nature and Sources of the Law", an examination and survey of the common law, is also still commonly read in U.S. law schools.

In the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.

Scots "common law" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called "Institutional Texts" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, "Jus Feudale" (1655) and Stair, "The Institutions of the Law of Scotland" (1681).












</doc>
<doc id="5255" url="https://en.wikipedia.org/wiki?curid=5255" title="Civil law">
Civil law

Civil law may refer to:



</doc>
<doc id="5257" url="https://en.wikipedia.org/wiki?curid=5257" title="Court of appeals (disambiguation)">
Court of appeals (disambiguation)

A court of appeals is an appellate court generally.

Court of Appeals may refer to:




</doc>
<doc id="5259" url="https://en.wikipedia.org/wiki?curid=5259" title="Common descent">
Common descent

Common descent is a concept in evolutionary biology according to which various organisms share a common ancestor. Its broadest application, the universal common ancestry (UCA) of all life on Earth, is a central assumption of modern evolutionary theory.

Common descent is an effect of speciation, in which multiple species derive from a single ancestral population. The more recent the ancestral population two species have in common, the more closely are they related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about the monophyly (single ancestry) of life. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.

Universal common descent through an evolutionary process was first proposed by the British naturalist Charles Darwin in the concluding sentence of his 1859 book "On the Origin of Species":

In the 1740s, the French mathematician Pierre Louis Maupertuis made the first known suggestion that all organisms had a common ancestor, and had diverged through random variation and natural selection. In "Essai de cosmologie" (1750), Maupertuis noted:

May we not say that, in the fortuitous combination of the productions of Nature, since only those creatures "could" survive in whose organizations a certain degree of adaptation was present, there is nothing extraordinary in the fact that such adaptation is actually found in all these species which now exist? Chance, one might say, turned out a vast number of individuals; a small proportion of these were organized in such a manner that the animals' organs could satisfy their needs. A much greater number showed neither adaptation nor order; these last have all perished... Thus the species which we see today are but a small part of all those that a blind destiny has produced.

In 1790, the philosopher Immanuel Kant wrote in "Kritik der Urteilskraft" ("Critique of Judgement") that the similarity of animal forms implies a common original type, and thus a common parent.

In 1794, Charles Darwin's grandfather, Erasmus Darwin asked:
[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?

Charles Darwin's views about common descent, as expressed in "On the Origin of Species", were that it was probable that there was only one progenitor for all life forms:

Therefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.

But he precedes that remark by, "Analogy would lead me one step further, namely, to the belief that all animals and plants have descended from some one prototype. But analogy may be a deceitful guide." And in the subsequent edition, he asserts rather, "We do not know all the possible transitional gradations between the simplest and the most perfect organs; it cannot be pretended that we know all the varied means of Distribution during the long lapse of years, or that we know how imperfect the Geological Record is. Grave as these several difficulties are, in my judgment they do not overthrow the theory of descent from a few created forms with subsequent modification". 

Common descent was widely accepted amongst the scientific community after Darwin's publication. In 1907, Vernon Kellogg commented that "practically no naturalists of position and recognized attainment doubt the theory of descent."

In 2008, biologist T. Ryan Gregory noted that:

No reliable observation has ever been found to contradict the general notion of common descent. It should come as no surprise, then, that the scientific community at large has accepted evolutionary descent as a historical reality since Darwin’s time and considers it among the most reliably established and fundamentally important facts in all of science.

All known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources. Analysis of small sequence differences in widely shared substances such as cytochrome c further supports universal common descent. Some 23 proteins are found in all organisms, serving as enzymes carrying out core functions like DNA replication. The fact that only one such set of enzymes exists is convincing evidence of a single ancestry. 6,331 genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.

The genetic code (the "translation table" according to which DNA information is translated into amino acids, and hence proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of universal common descent.

The way that codons (DNA triplets) are mapped to amino acids seems to be strongly optimised. Richard Egel argues that in particular the hydrophobic (non-polar) side-chains are well organised, suggesting that these enabled the earliest organisms to create peptides with water-repelling regions able to support the essential electron exchange (redox) reactions for energy transfer.

Similarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for universal common descent. Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code redundantly for the same amino acid. Since many species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for their sharing a recent common ancestor. Had the amino acid sequences come from different ancestors, they would have been coded for by any of the redundant codons, and since the correct amino acids would already have been in place, natural selection would not have driven any change in the codons, however much time was available. Genetic drift could change the codons, but it would be extremely unlikely to make all the redundant codons in a whole sequence match exactly across multiple lineages. Similarly, shared nucleotide sequences, especially where these are apparently neutral such as the positioning of introns and pseudogenes, provide strong evidence of common ancestry.

Biologists often point to the universality of many aspects of cellular life as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry - rather than through universal common descent - and therefore resulted in convergent evolution. In contrast, there is evidence for homology of the central subunits of Transmembrane ATPases throughout all living organisms, especially how the rotating elements are bound to the membrane. This supports the assumption of a LUCA as a cellular organism, although primordial membranes may have been semipermeable and evolved later to the membranes of modern bacteria, and on a second path to those of modern archaea also.

Another important piece of evidence is from detailed phylogenetic trees (i.e., "genealogic trees" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave "strong quantitative support, by a formal test, for the unity of life."

Traditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.

Theobald noted that substantial horizontal gene transfer could have occurred during early evolution. Bacteria today remain capable of gene exchange between distantly-related lineages. This weakens the basic assumption of phylogenetic analysis, that similarity of genomes implies common ancestry, because sufficient gene exchange would allow lineages to share much of their genome whether or not they shared an ancestor (monophyly). This has led to questions about the single ancestry of life. However, biologists consider it very unlikely that completely unrelated proto-organisms could have exchanged genes, as their different coding mechanisms would have resulted only in garble rather than functioning systems. Later, however, many organisms all derived from a single ancestor could readily have shared genes that all worked in the same way, and it appears that they have.

If early organisms had been driven by the same environmental conditions to evolve similar biochemistry convergently, they might independently have acquired similar genetic sequences. Theobald's "formal test" was accordingly criticised by Takahiro Yonezawa and colleagues for not including consideration of convergence. They argued that Theobald's test was insufficient to distinguish between the competing hypotheses. Theobald has defended his method against this claim, arguing that his tests distinguish between phylogenetic structure and mere sequence similarity. Therefore, Theobald argued, his results show that "real universally conserved proteins are homologous."





</doc>
<doc id="5261" url="https://en.wikipedia.org/wiki?curid=5261" title="Celtic music">
Celtic music

Celtic music is a broad grouping of music genres that evolved out of the folk music traditions of the Celtic people of Western Europe. It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from "trad" (traditional) music to a wide range of hybrids.

"Celtic music" means two things mainly. First, it is the music of the people that identify themselves as Celts. Secondly, it refers to whatever qualities may be unique to the music of the Celtic nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic music genres have a lot in common.

These following melodic practices may be used widely across the different variants of Celtic Music:


These two latter usage patterns may simply be remnants of formerly widespread melodic practices.

Often, the term "Celtic music" is applied to the music of Ireland and Scotland because both lands have produced well-known distinctive styles which actually have genuine commonality and clear mutual influences. The definition is further complicated by the fact that Irish independence has allowed Ireland to promote 'Celtic' music as a specifically Irish product. However, these are modern geographical references to a people who share a common Celtic ancestry and consequently, a common musical heritage.

These styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galicia, Cantabria and Asturias (Spain) and Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.

In "Celtic Music: A Complete Guide", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate "mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music."

There is also tremendous variation between "Celtic" regions. Ireland, Scotland, Brittany, Wales, and Cornwall have living traditions of language and music, and there has been a recent major revival of interest in Celtic heritage in the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic "Gallaic language" used into Roman times. Most of the Iberian Peninsula had a similar Celtic language in pre-Roman times. A Brythonic language was used in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon invasions via Brittany. The Romance language currently spoken in Galicia, Galician (Galego) is closely related to the Portuguese language used mainly in Brazil and Portugal. Galician music is claimed to be "Celtic". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).

Breton artist Alan Stivell was one of the earliest musicians to use the word "Celtic" and "Keltia" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.

There are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:

The modern Celtic music scene involves a large number of music festivals, as it has traditionally. Some of the most prominent festivals focused solely on music include:


The oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated English, Scottish, Irish, Welsh, German, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.

In the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagadoù (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.

In the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Enya, Donna Taggart, Altan, Capercaillie, The Corrs, Loreena McKennitt, Anúna, Riverdance and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.

Later, beginning in 1982 with The Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggae, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, The Young Dubliners, The Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.

Today there are Celtic-influenced subgenres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.

Outside of America, the first deliberate attempts to create a "Pan-Celtic music" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring "Hen Wlad Fy Nhadau" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became ""Bro goz va zadoù"" ("Old land of my fathers") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. 
Probably the most successful all inclusive Celtic music composition in recent years is Shaun Daveys composition 'The Pilgrim'. This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.

Modern music may also be termed "Celtic" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band "Skara Brae" recorded its only LP (simply called "Skara Brae"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded "A Prince Among Islands", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.

Several contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.).




</doc>
<doc id="5267" url="https://en.wikipedia.org/wiki?curid=5267" title="Constellation">
Constellation

A constellation is an area on the celestial sphere in which a group of stars forms an imaginary outline or pattern, typically representing an animal, mythological person or creature, a god, or an inanimate object.

The origins of the earliest constellations likely go back to prehistory. People used them to relate stories of their beliefs, experiences, creation, or mythology. Different cultures and countries adopted their own constellations, some of which lasted into the early 20th century before today's constellations were internationally recognized. The recognition of constellations has changed significantly over time. Many have changed in size or shape. Some became popular, only to drop into obscurity. Others were limited to a single culture or nation.

The 48 traditional Western constellations are Greek. They are given in Aratus' work "Phenomena" and Ptolemy's "Almagest", though their origin probably predates these works by several centuries. Constellations in the far southern sky were added from the 15th century until the mid-18th century when European explorers began traveling to the Southern Hemisphere. Twelve ancient constellations belong to the zodiac (straddling the ecliptic, which the Sun, Moon, and planets all traverse). The origins of the zodiac remain historically uncertain; its astrological divisions became prominent c. 400 BC in Babylonian or Chaldean astronomy.

In 1922, the International Astronomical Union (IAU) formally accepted the modern list of 88 constellations, and in 1928 adopted official constellation boundaries that together cover the entire celestial sphere. Any given point in a celestial coordinate system lies in one of the modern constellations. Some astronomical naming systems include the constellation where a given celestial object is found to convey its approximate location in the sky. The Flamsteed designation of a star, for example, consists of a number and the genitive form of the constellation name.

Other star patterns or groups called asterisms are not constellations per se, but are used by observers to navigate the night sky. Asterisms may be several stars within a constellation, or they may share stars with more than one constellation. Examples of asterisms include the Pleiades and Hyades within the constellation Taurus and the False Cross split between the southern constellations Carina and Vela, or Venus' Mirror in the constellation of Orion.

The word "constellation" comes from the Late Latin term , which can be translated as "set of stars"; it came into use in English during the 14th century. The Ancient Greek word for constellation is ἄστρον. These terms generally referred to a recognisable pattern of stars whose appearance is associated with mythological characters or creatures, earthbound animals, or objects. A more modern astronomical sense of the term "constellation" denotes one of the 88 IAU designated constellations recognized today.
Colloquial usage does not draw a sharp distinction between "constellations" and smaller "asterisms" (pattern of stars), yet the modern accepted astronomical constellations employ such a distinction. E.g., the Pleiades and the Hyades are both asterisms, and each lies within the boundaries of the constellation of Taurus. Another example is the northern asterism popularly known as the Big Dipper (US) or the Plough (UK), composed of the seven brightest stars within the area of the IAU-defined constellation of Ursa Major. The southern False Cross asterism includes portions of the constellations Carina and Vela and the Summer Triangle is composed of the brightest stars in the constellations Lyra, Aquila and Cygnus.

A constellation (or star), viewed from a particular latitude on Earth, that never sets below the horizon is termed circumpolar. From the North Pole or South Pole, all constellations south or north of the celestial equator are circumpolar. Depending on the definition, equatorial constellations may include those that lie between declinations 45° north and 45° south, or those that pass through the declination range of the ecliptic or zodiac ranging between 23½° north, the celestial equator, and 23½° south.

Although stars in constellations appear near each other in the sky, they usually lie at a variety of distances away from the Earth. Since stars have their own independent motions, all constellations will change slowly over time. After tens to hundreds of thousands of years, familiar outlines will generally become unrecognizable. Astronomers can predict the past or future constellation outlines by measuring individual stars' common proper motions or cpm by accurate astrometry and their radial velocities by astronomical spectroscopy.

It has been suggested that the 17,000 year old cave paintings in Lascaux Southern France depict star constellations such as Taurus, Orion's Belt and the Pleiades. However this view is not yet generally accepted among scientists.

Inscribed stones and clay writing tablets from Mesopotamia (in modern Iraq) dating to 3000 BC provide the earliest generally accepted evidence for humankind's identification of constellations. It seems that the bulk of the Mesopotamian constellations were created within a relatively short interval from around 1300 to 1000 BC. Mesopotamian constellations appeared later in many of the classical Greek constellations.

The oldest Babylonian catalogues of stars and constellations date back to the beginning of the Middle Bronze Age, most notably the "Three Stars Each" texts and the "MUL.APIN", an expanded and revised version based on more accurate observation from around 1000 BC. However, the numerous Sumerian names in these catalogues suggest that they built on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.

The classical Zodiac is a revision of Neo-Babylonian constellations from the 6th century BC. The Greeks adopted the Babylonian constellations in the 4th century BC. Twenty Ptolemaic constellations are from the Ancient Near East. Another ten have the same stars but different names.

Biblical scholar E. W. Bullinger interpreted some of the creatures mentioned in the books of Ezekiel and Revelation as the middle signs of the four quarters of the Zodiac, with the Lion as Leo, the Bull as Taurus, the Man representing Aquarius, and the Eagle standing in for Scorpio. The biblical Book of Job also makes reference to a number of constellations, including "bier", "fool" and "heap" (Job 9:9, 38:31-32), rendered as "Arcturus, Orion and Pleiades" by the KJV, but "‘Ayish" "the bier" actually corresponding to Ursa Major. The term "Mazzaroth" , translated as "a garland of crowns", is a "hapax legomenon" in Job 38:32, and it might refer to the zodiacal constellations.

There is only limited information on ancient Greek constellations, with some fragmentary evidence being found in the "Works and Days" of the Greek poet Hesiod, who mentioned the "heavenly bodies". Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC. The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC. The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus. The basis of Western astronomy as taught during Late Antiquity and until the Early Modern period is the "Almagest" by Ptolemy, written in the 2nd century.

In the Ptolemaic Kingdom, native Egyptian tradition of anthropomorphic figures represented the planets, stars, and various constellations. Some of these were combined with Greek and Babylonian astronomical systems culminating in the Zodiac of Dendera; it remains unclear when this occurred, but most were placed during the Roman period between 2nd to 4th centuries AD. The oldest known depiction of the zodiac showing all the now familiar constellations, along with some original Egyptian constellations, decans, and planets. Ptolemy's "Almagest" remained the standard definition of constellations in the medieval period both in Europe and in Islamic astronomy.

Ancient China had a long tradition of observing celestial phenomena. Nonspecific Chinese star names, later categorized in the twenty-eight mansions, have been found on oracle bones from Anyang, dating back to the middle Shang dynasty. These constellations are some of the most important observations of Chinese sky, attested from the 5th century BC. Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently.

Three schools of classical Chinese astronomy in the Han period are attributed to astronomers of the earlier Warring States period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period). Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida. The oldest extant Chinese star chart dates to that period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song dynasty, and during the Yuan dynasty became increasingly influenced by medieval Islamic astronomy (see Treatise on Astrology of the Kaiyuan Era). As maps were prepared during this period on more scientific lines, they were considered as more reliable.
A well known map from the Song period is the Suzhou Astronomical Chart, which was prepared with carvings of stars on the planisphere of the Chinese sky on a stone plate; it is done accurately based on observations, and it shows the supernova of the year of 1054 in Taurus.

Influenced by European astronomy during the late Ming dynasty, more stars were depicted on the charts but retaining the traditional constellations; new stars observed were incorporated as supplementary stars in old constellations in the southern sky which did not depict any of the traditional stars recorded by ancient Chinese astronomers. Further improvements were made during the later part of the Ming dynasty by Xu Guangqi and Johann Adam Schall von Bell, the German Jesuit and was recorded in Chongzhen Lishu (Calendrical Treatise of Chongzhen period, 1628). Traditional Chinese star maps incorporated 23 new constellations with 125 stars of the southern hemisphere of the sky based on the knowledge of Western star charts; with this improvement, the Chinese Sky was integrated with the World astronomy.

Historically, the origins of the constellations of the northern and southern skies are distinctly different. Most northern constellations date to antiquity, with names based mostly on Classical Greek legends. Evidence of these constellations has survived in the form of star charts, whose oldest representation appears on the statue known as the Farnese Atlas, based perhaps on the star catalogue of the Greek astronomer Hipparchus. Southern constellations are more modern inventions, sometimes as substitutes for ancient constellations (e.g. Argo Navis). Some southern constellations had long names that were shortened to more usable forms; e.g. Musca Australis became simply Musca.

Some of the early constellations were never universally adopted. Stars were often grouped into constellations differently by different observers, and the arbitrary constellation boundaries often led to confusion as to which constellation a celestial object belonged. Before astronomers delineated precise boundaries (starting in the 19th century), constellations generally appeared as ill-defined regions of the sky. Today they now follow officially accepted designated lines of Right Ascension and Declination based on those defined by Benjamin Gould in epoch 1875.0 in his star catalogue "Uranometria Argentina".

The 1603 star atlas "Uranometria" of Johann Bayer assigned stars to individual constellations and formalized the division by assigning a series of Greek and Latin letters to the stars within each constellation. These are known today as Bayer designations. Subsequent star atlases led to the development of today's accepted modern constellations.

The southern sky, below about −65° declination, was only partially catalogued by ancient Babylonians, Egyptian, Greeks, Chinese, and Persian astronomers of the north. Knowledge that northern and southern star patterns differed goes back to Classical writers, who describe, for example, the African circumnavigation expedition commissioned by Egyptian Pharaoh Necho II in c. 600 BC and those of Hanno the Navigator in c. 500 BC. However, much of this history was lost with the Destruction of the Library of Alexandria.

The history of southern constellations is not straightforward. Different groupings and different names were proposed by various observers, some reflecting national traditions or designed to promote various sponsors. Southern constellations were important from the 14th to 16th centuries, when sailors used the stars for celestial navigation. Italian explorers who recorded new southern constellations include Andrea Corsali, Antonio Pigafetta, and Amerigo Vespucci.

Many of the 88 IAU-recognized constellations in this region first appeared on celestial globes developed in the late 16th century by Petrus Plancius, based mainly on observations of the Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman. These became widely known through Johann Bayer's star atlas "Uranometria" of 1603. more were created in 1763 by the French astronomer Nicolas Louis de Lacaille appearing in his star catalogue, published in 1756.

Several modern proposals have not survived. The French astronomers Pierre Lemonnier and Joseph Lalande, for example, proposed constellations that were once popular but have since been dropped. The northern constellation Quadrans Muralis survived into the 19th century (when its name was attached to the Quadrantid meteor shower), but is now divided between Boötes and Draco.

A general list of 88 constellations was produced for the International Astronomical Union in 1922. It is roughly based on the traditional Greek constellations listed by Ptolemy in his "Almagest" in the 2nd century and Aratus' work "Phenomena", with early modern modifications and additions (most importantly introducing constellations covering the parts of the southern sky unknown to Ptolemy) by Petrus Plancius (1592, 1597/98 and 1613), Johannes Hevelius (1690) and Nicolas Louis de Lacaille (1763), who named fourteen constellations and renamed a fifteenth one. De Lacaille studied the stars of the southern hemisphere from 1750 until 1754 from Cape of Good Hope, when he was said to have observed more than 10,000 stars using a refracting telescope.

In 1922, Henry Norris Russell produced a general list of 88 constellations and some useful abbreviations for them. However, these constellations did not have clear borders between them. In 1928, the International Astronomical Union (IAU) formally accepted 88 modern constellations, with contiguous boundaries along vertical and horizontal lines of right ascension and declination developed by Eugene Delporte that, together, cover the entire celestial sphere; this list was finally published in 1930. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo or Scorpius. The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields. Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.

The boundaries developed by Delporte used data that originated back to epoch B1875.0, which was when Benjamin A. Gould first made his proposal to designate boundaries for the celestial sphere, a suggestion upon which Delporte would base his work. The consequence of this early date is that because of the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.

The Great Rift, a series of dark patches in the Milky Way, is more visible and striking in the southern hemisphere than in the northern. It vividly stands out when conditions are otherwise so dark that the Milky Way's central region casts shadows on the ground. Some cultures have discerned shapes in these patches and have given names to these "dark cloud constellations". Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the "emu in the sky" whose head is formed by the Coalsack, a dark nebula, instead of the stars.



"General & Nonspecialized – Entire Celestial Heavens":

"Northern Celestial Hemisphere & North Circumpolar Region":

"Equatorial, Ecliptic, & Zodiacal Celestial Sky":

"Southern Celestial Hemisphere & South Circumpolar Region":




</doc>
<doc id="5269" url="https://en.wikipedia.org/wiki?curid=5269" title="Character">
Character

Character(s) may refer to:










</doc>
<doc id="5270" url="https://en.wikipedia.org/wiki?curid=5270" title="Car (disambiguation)">
Car (disambiguation)

A car is a wheeled motor vehicle used for transporting passengers.

Car, Cars, CAR or CARs may also refer to:


















</doc>
<doc id="5272" url="https://en.wikipedia.org/wiki?curid=5272" title="Printer (computing)">
Printer (computing)

In computing, a printer is a peripheral device which makes a persistent representation of graphics or text, usually on paper. While most output is human-readable, bar code printers are an example of an expanded use for printers.

The first computer printer designed was a mechanically driven apparatus by Charles Babbage for his difference engine in the 19th century; however, his mechanical printer design was not built until 2000.

The first electronic printer was the EP-101, invented by Japanese company Epson and released in 1968.

The first commercial printers generally used mechanisms from electric typewriters and Teletype machines. The demand for higher speed led to the development of new systems specifically for computer use. In the 1980s there were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high quality line art like blueprints.

The introduction of the low-cost laser printer in 1984 with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter, set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as a laser printer in terms of flexibility, but produced somewhat lower quality output (depending on the paper) from much less expensive mechanisms. Inkjet systems rapidly displaced dot matrix and daisy wheel printers from the market. By the 2000s high-quality printers of this sort had fallen under the $100 price point and became commonplace.

The rapid update of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a "physical backup" is of little benefit today. Even the desire for printed output for "offline reading" while on mass transit or aircraft has been displaced by e-book readers and tablet computers. Today, traditional printers are being used more for special purposes, like printing photographs or artwork, and are no longer a must-have peripheral.

Starting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. These devices are in their earliest stages of development and have not yet become commonplace.

"Personal" printers are primarily designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), 
and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.

"Networked" or "shared" printers are "designed for high-volume, high-speed printing". They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.

A "virtual printer" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer. A virtual printer can be used to create a file which is an image of the data which would be printed, for archival purposes or as input to another program, for example to create a PDF or to transmit to another system or user.

A "barcode printer" is a computer peripheral for printing barcode labels or tags that can be attached to, or printed directly on, physical objects. Barcode printers are commonly used to label cartons before shipment, or to label retail items with UPCs or EANs.

A "3D printer" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material (including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.

The choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies do not work with certain types of physical media, such as carbon paper or transparencies.

A second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.

Cheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.

The following printing technologies are routinely found in modern printers:

A laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.

Another toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.

Inkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.

Solid ink printers, also known as phase-change printers, are a type of thermal transfer printer. They use solid sticks of CMYK-coloured ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. The printhead sprays the ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as colour office printers, and are excellent at printing on transparencies and other non-porous media. Solid ink printers can produce excellent results. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. In addition, this type of printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tek sold the printing business to Xerox in 2001.

A dye-sublimation printer (or dye-sub printer) is a printer which employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper or canvas. The process is usually to lay one colour at a time using a ribbon that has colour panels. Dye-sub printers are intended primarily for high-quality colour applications, including colour photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.

Thermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colours can be achieved with special papers and different temperatures and heating rates for different colours; these coloured sheets are not required in black-and-white output. One example is Zink (a portmanteau of "zero ink").

The following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.

 Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of "fully formed characters", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by "overstriking", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers and line printers. Dot matrix printers remain in common use in businesses where multi-part forms are printed. "An overview of impact printing" contains a detailed description of many of the technologies used.

Several different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known "golf ball" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.

The common teleprinter could easily be interfaced to the computer and became very popular except for those computers manufactured by IBM. Some models used a "typebox" that was positioned, in the X- and Y-axes, by a mechanism and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.

Daisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the "daisy wheel", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as "letter-quality printers" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.

The term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms ("type").

Dot-matrix printers can be broadly divided into two major classes:

Dot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.

In the 1970s and '80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronised longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.

Some dot matrix printers, such as the NEC P6300, can be upgraded to print in colour. This is achieved through the use of a four-colour ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Colour graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, colour graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.

Dot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. It also has security advantages as ink impressed into a paper matrix by force is harder to erase invisibly. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.

Line printers print an entire line of text at a time. Four principal designs exist.


In each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper. Each system could have slight timing issues, which could cause minor misalignment of the resulting printed characters. For drum or typebar printers, this appeared as vertical misalignment, with characters being printed slightly above or below the rest of the line. In chain or bar printers, the misalignment was horizontal, with printed characters being crowded closer together or farther apart. This was much less noticeable to human vision than vertical misalignment, where characters seemed to bounce up and down in the line, so they were considered as higher quality print. 


Line printers are the fastest of all impact printers and are used for bulk printing in large computer centres. A line printer can print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operate with tight tolerances and require regular preventive maintenance (PM) to produce top quality print. They are virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many computer operating systems, which use the abbreviations "lp", "lpr", or "LPT" to refer to printers.

Liquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than ±0.1%. (All laser printers have an accuracy of ±1%.)

Worldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.

Pen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.

A number of other sorts of printers are important for historical reasons, or for special purpose uses


Most printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.

Today, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.

The speed of early printers was measured in units of "characters per minute" (cpm) for character printers, or "lines per minute" (lpm) for line printers. Modern printers are measured in "pages per minute" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially colour images. Speeds in ppm usually apply to A4 paper in most countries in the world, and letter paper size, about 6% shorter, in North America.

The data received by a printer may be:


Some printers can process all four types of data, others not.


Today it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.

A monochrome printer can only produce an image consisting of one colour, usually black. A monochrome printer may also be able to produce various tones of that color, such as a grey-scale. A colour printer can produce images of multiple colours. A photo printer is a colour printer that can produce images that mimic the colour range (gamut) and resolution of prints made from photographic film. Many can be used on a standalone basis without a computer, using a memory card or USB connector.

The page yield is number of pages that can be printed from a toner cartridge or ink cartridge—before the cartridge needs to be refilled or replaced.
The actual number of pages yielded by a specific cartridge depends on a number of factors.

For a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.

In order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).

Retailers often apply the "razor and blades" business model: a company may sell a printer at cost, and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.

Other manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on the ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: "cheap printer – expensive ink" or "expensive printer – cheap ink". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.

Printer steganography is a type of steganography – "hiding data within data" – produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.

More than half of all printers sold at U.S. retail in 2010 were wireless-capable, but nearly three-quarters of consumers who have access to those printers weren't taking advantage of the increased access to print from multiple devices according to the new Wireless Printing Study.


</doc>
<doc id="5278" url="https://en.wikipedia.org/wiki?curid=5278" title="Copyright">
Copyright

Copyright is the exclusive right given to the creator of a creative work to reproduce the work, usually for a limited time. The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. A copyright is subject to limitations based on public interest considerations, such as the fair use doctrine in the United States.
Some jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders. These rights frequently include reproduction, control over derivative works, distribution, public performance, and moral rights such as attribution.

Copyrights can be granted by public law and are in that case considered "territorial rights". This means that copyrights granted by the law of a certain state, do not extend beyond the territory of that specific jurisdiction. Copyrights of this type vary by country; many countries, and sometimes a large group of countries, have made agreements with other countries on procedures applicable when works "cross" national borders or national rights are inconsistent.

Typically, the public law duration of a copyright expires 50 to 100 years after the creator dies, depending on the jurisdiction. Some countries require certain copyright formalities to establishing copyright, others recognize copyright in any completed work, without formal registration.

The concept of copyright developed after the printing press came into use in Europe in the 1400s and 1500s. The printing press made it much cheaper to produce works, but as there was initially no copyright law, anyone could buy or rent a press and print any text. Popular new works were immediately re-set and re-published by competitors, so printers needed a constant stream of new material. Fees paid to authors for new works were high, and significantly supplemented the incomes of many academics. 

Printing brought profound social changes. The rise in literacy across Europe led to a dramatic increase in the demand for reading matter. Prices of reprints were low, so publications could be bought by poorer people, creating a mass audience. In German-speaking areas, most publications were academic papers, and most were scientific and technical publications, often autodidactic practical instruction manuals on topics such as dike construction. After copyright law became established (in 1710 in England and Scotland, and in the 1840s in German-speaking areas) the low-price mass market vanished, and fewer, more expensive editions were published.

The concept of copyright first developed in England. In reaction to the printing of "scandalous books and pamphlets", the English Parliament passed the Licensing of the Press Act 1662, which required all intended publications to be registered with the government-approved Stationers' Company, giving the Stationers the right to regulate what material could be printed.

The Statute of Anne, enacted in 1710 in England and Scotland provided the first legislation to protect copyrights (but not authors' rights). The Copyright Act of 1814 extended more rights for authors but did not protect British from reprinting in the US. The Berne International Copyright Convention of 1886 finally provided protection for authors among the countries who signed the agreement, although the US did not join the Berne Convention until 1989.

In the US, the Constitution protects the rights of authors and the legislature, Congress, can create national copyright laws but must exercise their power within the scope of the Constitution. Modeled on the Statute of Anne, Congress enacted the Copyright Act of 1790. While the national law protected authors’ published works, authority was granted to the states to protect authors’ unpublished works. These two protections exist today: protection by the state for unpublished work, subsequent protection by federal law for published work.

Congress enacted an updated law in 1909, which was later determined to be flawed and was subsequently replaced by the 1976 Copyright Act. This act expanded the items that were eligible for protection, including literary, music, dramatic, pictorial/sculptural works, motion pictures, sound recordings, and choreographic works. This act also extended the copyright protection to life plus 50 years. One final change was that it “codified a fair use exception to copyright”. With these changes in place, the US was in a better position to join the Berne Convention, extending copyright protections internationally.

Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per se.

Copyright has developed into a concept that has a significant effect on nearly every modern industry, including not just literary work, but also forms of creative work such as sound recordings, films, photographs, software, and architecture.

Often seen as the first real copyright law, the 1709 British Statute of Anne gave the publishers rights for a fixed period, after which the copyright expired.
The act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing ... Books, and other Writings, without the Consent of the Authors ... to their very great Detriment, and too often to the Ruin of them and their Families:". A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.

The Copyright Clause of the United States, Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.

The original length of copyright in the United States was 14 years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.

Copyright law was enacted rather late in German states, and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.

The 1886 Berne Convention first established recognition of copyrights among sovereign nations, rather than merely bilaterally. Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention. As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100 years later with the passage of the Copyright, Designs and Patents Act 1988. Specially, for educational and scientific research purposes, the Berne Convention provides the developing countries issue compulsory licenses for the translation or reproduction of copyrighted works within the limits prescribed by the Convention. This was a special provision that had been added at the time of 1971 revision of the Convention, because of the strong demands of the developing countries. The United States did not sign the Berne Convention until 1989.

The United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as "all rights reserved"), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.

The regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application.

In 1961, the United International Bureaux for the Protection of Intellectual Property signed the Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations. In 1996, this organization was succeeded by the founding of the World Intellectual Property Organization, which launched the 1996 WIPO Performances and Phonograms Treaty and the 2002 WIPO Copyright Treaty, which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The Trans-Pacific Partnership includes intellectual Property Provisions relating to copyright.

Copyright laws are standardized somewhat through these international conventions such as the Berne Convention and Universal Copyright Convention. These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.

The original holder of the copyright may be the employer of the author rather than the author himself if the work is a "work for hire". For example, in English law the Copyright, Designs and Patents Act 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire". Typically, the first owner of a copyright is the person who created the work i.e. the author. But when more than one person creates the work, then a case of joint authorship can be made provided some criteria are met.

Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by jurisdiction, but these can include poems, theses, fictional characters plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs. Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.

Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed. For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's. Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, "Steamboat Willie" is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.

Typically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some "skill, labour, and judgment" that has gone into it. In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.

Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.

In all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights. However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as "prima facie" evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. (In the US, registering after an infringement only enables one to receive actual damages and lost profits.)

A widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man's copyright. It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office says the technique is not a substitute for actual registration. The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original or establish who created the work. 

The Berne Convention allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection. U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration". Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance".

Before 1989, United States law required the use of a copyright notice, consisting of the copyright symbol (©, the letter C inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder. Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (℗, the letter P inside a circle), which indicates a sound recording copyright, with the letter P indicating a "phonorecord". In addition, the phrase "All rights reserved" was once required to assert copyright, but that phrase is now legally obsolete. Almost everything on the Internet has some sort of copyright attached to it. Whether these things are watermarked, signed, or have any other sort of indication of the copyright is a different story however.

In 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976 Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit – using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.

Copyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: Legal aspects of file sharing)

In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.

"...by 1978, the scope was expanded to apply to any 'expression' that has been 'fixed' in any medium, this protection granted automatically whether the maker wants it or not, no registration required."

For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.

Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available. Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect. In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.

According to World Intellectual Property Organisation, copyright protects two types of rights. Economic rights allow right owners to derive financial reward from the use of their works by others. Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work. The author or creator may be the owner of the economic rights or those rights may be transferred to one or more copyright owners. Many countries do not allow the transfer of moral rights. Where Economic rights allow right owners to derive financial reward from the use of their works by others, the Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work.

With any kind of property, its owner may decide how it is to be used, and others can use it lawfully only if they have the owner's permission, often through a license. The owner's use of the property must, however, respect the legally recognised rights and interests of other members of society. So the owner of a copyright-protected work may decide how to use the work, and may prevent others from using it without permission. National laws usually grant copyright owners exclusive rights to allow third parties to use their works, subject to the legally recognised rights and interests of others. Most copyright laws state that authors or other right owners have the right to authorise or prevent certain acts in relation to a work. Right owners can authorise or prohibit:


Moral rights are concerned with the non-economic rights of a creator. They protect the creator's connection with a work as well as the integrity of the work. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. In some EU countries, such as France, moral rights last indefinitely. In the UK, however, moral rights are finite. That is, the right of attribution and the right of integrity last only as long as the work is in copyright. When the copyright term comes to an end, so too do the moral rights in that work. This is just one reason why the moral rights regime within the UK is often regarded as weaker or inferior to the protection of moral rights in continental Europe and elsewhere in the world. The Berne Convention, in Article 6bis, requires its members to grant authors the following rights:


These and other similar rights granted in national laws are generally known as the moral rights of authors. The Berne Convention requires these rights to be independent of authors’ economic rights. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. This means that even where, for example, a film producer or publisher owns the economic rights in a work, in many jurisdictions the individual author continues to have moral rights. Recently, as a part of the debates being held at the U.S. Copyright Office on the question of inclusion of Moral Rights as a part of the framework of the Copyright Law in United States, the Copyright Office concluded that many diverse aspects of the current moral rights patchwork—including copyright law's derivative work right, state moral rights statutes, and contract law—are generally working well and should not be changed. Further, the Office concludes that there is no need for the creation of a blanket moral rights statute at this time. However, there are aspects of the U.S. moral rights patchwork that could be improved to the benefit of individual authors and the copyright system as a whole.

The Copyright Law in the United States, several exclusive rights are granted to the holder of a copyright, as are listed below:


The basic right when a work is protected by copyright is that the holder may determine and decide how and under what conditions the protected work may be used by others. This includes the right to decide to distribute the work for free. This part of copyright is often overseen. The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right.

UK copyright law gives creators both economic rights and moral rights. While ‘copying’ someone else's work without permission may constitute an infringement of their economic rights, that is, the reproduction right or the right of communication to the public, whereas, ‘mutilating’ it might infringe the creator's moral rights. In the UK, moral rights include the right to be identified as the author of the work, which is generally identified as the right of attribution, and the right not to have your work subjected to ‘derogatory treatment’, that is the right of integrity.

Indian copyright law is at parity with the international standards as contained in TRIPS. The Indian Copyright Act, 1957, pursuant to the amendments in 1999, 2002 and 2012, fully reflects the Berne Convention for Protection of Literary and Artistic Works, 1886 and the Universal Copyrights Convention, to which India is a party. India is also a party to the Geneva Convention for the Protection of Rights of Producers of Phonograms and is an active member of the World Intellectual Property Organization (WIPO) and United Nations Educational, Scientific and Cultural Organization (UNESCO). The Indian system provides both the economic and moral rights under different provisions of its Indian Copyright Act of 1957.

Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States and the United Kingdom), copyrights expire at the end of the calendar year in which they would otherwise expire.

The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.

In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain. In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain. Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.

But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.

In 1998, the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.

In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas). However, there are protections available for those areas copyright does not cover – such as trademarks and patents.

There are some exceptions to what copyright will protect. Copyright will not protect:

The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of Baker v. Selden, has since been codified by the Copyright Act of 1976 at 17 U.S.C. § 102(b).

Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD. In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores.

Some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.

In "Kirtsaeng v. John Wiley & Sons, Inc.", in 2013, the United States Supreme Court held in a 6–3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.

In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.

Copyright does not prohibit all copying or replication. In the United States, the fair use doctrine, codified by the Copyright Act of 1976 as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:

In the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation. The concept is sometimes not well defined; however in Canada, private copying for personal use has been expressly permitted by statute since 1999. In "Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)", 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the "Copyright Act 1968" (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice). Under current Australian law, although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to "format shift" that work from one medium to another for personal, private use, or to "time shift" a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.

In the United States the AHRA (Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.

Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. An appellate court has held that fair use is not a defense to engaging in such distribution.

EU copyright laws recognise the right of EU member states to implement some national exceptions to copyright. Examples of those exceptions are:

It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.

A copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another. For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet; however, the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy or distribute the work in a particular region or for a specified period of time.

A transfer or licence may have to meet particular formal requirements in order to be effective, for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under U.S. law. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.

Copyright may also be licensed. Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.

Copyright licenses known as "open" or free licenses seek to grant several rights to licensees, either for a fee or not. "Free" in this context is not as much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the Free Software Definition, the Debian Free Software Guidelines, the Open Source Definition and the Definition of Free Cultural Works. Further refinements to these definitions have resulted in categories such as copyleft and permissive. Common examples of free licences are the GNU General Public License, BSD licenses and some Creative Commons licenses.

Founded in 2001 by James Boyle, Lawrence Lessig, and Hal Abelson, the Creative Commons (CC) is a non-profit organization which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, gratis. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.

Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them are not properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright-holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work. approximately 130 million individuals had received such licenses.

Some sources are critical of particular aspects of the copyright system. This is known as a debate over copynorms. Particularly to the background of uploading content to internet platforms and the digital exchange of original work, there is discussion about the copyright aspects of downloading and streaming, the copyright aspects of hyperlinking and framing.

Concerns are often couched in the language of digital rights, digital freedom, database rights, open data or censorship. Discussions include "Free Culture", a 2004 book by Lawrence Lessig. Lessig coined the term permission culture to describe a worst-case system. "Good Copy Bad Copy" (documentary) and , discuss copyright. Some suggest an alternative compensation system. In Europe consumers are acting up against the raising costs of music, film and books, and as a result Pirate Parties have been created. Some groups reject copyright altogether, taking an anti-copyright stance. The perceived inability to enforce copyright online leads some to advocate ignoring legal statutes when on the web.

Copyright, like other intellectual property rights, is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be used or exploited by anyone without obtaining permission, and normally without payment. However, in paying public domain regimes the user may still have to pay royalties to the state or to an authors' association. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright. Public domain works should not be confused with works that are publicly available. Works posted in the internet, for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.



</doc>
<doc id="5282" url="https://en.wikipedia.org/wiki?curid=5282" title="Catalan language">
Catalan language

Catalan (; autonym: ; ) is a Western Romance language derived from Vulgar Latin and named after the medieval Principality of Catalonia, in northeastern modern Spain. It is the only official language of Andorra, and a co-official language of the Spanish autonomous communities of Catalonia, the Balearic Islands and Valencia (where the language is known as Valencian). It also has semi-official status in the Italian comune of Alghero. It is also spoken in the eastern strip of Aragon, in some villages of the region of Murcia called Carche and in the Pyrénées-Orientales department of France. These territories are often called or "Catalan Countries".

Catalan evolved from Vulgar Latin in the Middle Ages around the eastern Pyrenees. 19th-century Spain saw a Catalan literary revival, culminating in the early 1900s.

The word "Catalan" is derived from the territorial name of Catalonia, itself of disputed etymology. The main theory suggests that (Latin "Gathia Launia") derives from the name "Gothia" or "Gauthia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence "Gothland" > "Gothlandia" > "Gothalania" > "Catalonia" theoretically derived.

In English, the term referring to a person first appears in the mid 14th century as "Catelaner", followed in the 15th century as "Catellain" (from French). It is attested a language name since at least 1652. The word "Catalan" can be pronounced in English as , or .

The endonym is pronounced in the Eastern Catalan dialects, and in the Western dialects. In the Valencian Community, the term is frequently used instead. The names "Catalan" and "Valencian" are two names for the same language. See also status of Valencian below.

By the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988.

In the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries.

During the 11th and 12th centuries the Catalan rulers expanded up to north of the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century.

In the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232–1315), the Four Great Chronicles (13th–14th centuries), and the Valencian school of poetry culminating in Ausiàs March (1397–1459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms "one of the 'great languages' of medieval Europe".

Martorell's outstanding novel of chivalry "Tirant lo Blanc" (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan.

With the union of the crowns of Castille and Aragon (1479), the use of Spanish gradually became more prestigious and marked the start of the decline of the Catalan. Starting in the 16th century, Catalan literature came under the influence of Spanish, and the urban and literary classes became bilingual.

With the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region.

Shortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the regional languages of France, such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque.

Following the French capture of Algeria (1833), that region saw several waves of Catalan-speaking settlers. People from the Spanish Alacant province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Menorca. Their speech was known as "patuet". By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as "Pieds-Noirs") or Alacant.

Nowadays, France recognizes only French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyrénées-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education.

The decline of Catalan continued in the 16th and 17th centuries. The defeat of the pro-Habsburg coalition in the War of Spanish Succession (1714) initiated a series of laws which, among other centralizing measures, imposed the use of Spanish in legal documentation all over Spain.

In parallel, however, the 19th century saw a Catalan literary revival (), which has continued up to the present day. This period starts with Aribau's "Ode to the Homeland" (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimerà (drama).

In the 19th century, the region of Carche, in the province of Murcia was repopulated with Catalan speakers from the Land of Valencia. The Second Spanish Republic (1931–1939) saw a brief period of tolerance, with most restrictions against Catalan being lifted. Despite orthographic standardization in 1913 and the official status of the language during the Second Spanish Republic (1931–39) the Francoist dictatorship banned the use of Catalan in schools and in the public administration between 1939 and 1975.

Since the Spanish transition to democracy (1975–1982), Catalan has been institutionalized as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is an unparalleled large bilingual European non-state linguistic community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations – if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or more recently arrived immigrant students. There is also some intergenerational shift towards Catalan.

According to the Statistical Institute of Catalonia, in 2013 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language: 7% of the population self-identifies with both Catalan and Spanish equally, 36.4% with Catalan and 47.5% only Spanish. In 2003 the same studies concluded no language preference for self-identification within the population above 15 years old: 5% self-identified with both languages, 44.3% with Catalan and 47.5 with Spanish. In order to promote use of Catalan, the Generalitat de Catalunya (Catalonia's official Autonomous government) spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories, with entities such as Consorci per a la Normalització lingüística.

In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several policies favouring Catalan have been enforced, like Catalan medium education.

On the other hand, there are several language shift processes currently taking place. In the Northern Catalonia area of France, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association promotes a network of community-run schools engaged in Catalan language immersion programs.

In Alicante province, Catalan is being replaced by Spanish and in Alghero by Italian. There is also well ingrained diglossia in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands.

The ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ramón Menéndez Pidal.

According to Pèire Bèc, its specific classification is as follows:

Catalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term "Occitan language" (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages.

Catalan shares many traits with the other neighboring Romance languages (Italian, Sardinian, Occitan, French, Spanish and Portuguese among others). However, despite being spoken mostly on the Iberian Peninsula, Catalan has marked differences with the Iberian Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; showing instead its closest affinity with languages native to France and northern Italy, particularly Occitan and to a lesser extent Gallo-Romance (Franco-Provençal, French, Gallo-Italian).

According to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese and Spanish; 76% with Ladin; 75% with Sardinian; and 73% with Romanian.

During much of its history, and especially during the Francoist dictatorship (1939–1975), the Catalan language was ridiculed as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in features closer to Occitan (and French).

There is evidence that, at least from the 2nd century , the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation arose generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish , Asturian and Portuguese vs. Catalan , Occitan "to boil") and innovatory regionalisms (Sp , Ast vs. Cat , Oc "bullock"), while Catalan has a shared history with the Western Romance innovative core, especially Occitan.

Like all Romance languages, Catalan has a handful of native words which are rare or only found in Catalan. These include:

The Gothic superstrate produced different outcomes in Spanish and Catalan. For example, Catalan "mud" and "to roast", of Germanic origin, contrast with Spanish and , of Latin origin; whereas Catalan "spinning wheel" and "temple", of Latin origin, contrast with Spanish and , of Germanic origin.

The same happens with Arabic loanwords. Thus, Catalan "large earthenware jar" and "tile", of Arabic origin, contrast with Spanish and , of Latin origin; whereas Catalan "oil" and "olive", of Latin origin, contrast with Spanish and . However, the Arabic element in Spanish is generally much more prevalent.

Situated between two large linguistic blocks (Iberian Romance and Gallo-Romance), Catalan has many unique lexical choices, such as "to miss somebody", "to calm somebody down", and "reject".

Traditionally Catalan-speaking territories are sometimes called the (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions.

The number of people known to be fluent in Catalan varies depending on the sources used. A 2004 study did not count the total number of speakers, but estimated a total of 9–9.5 million by matching the percentage of speakers to the population of each area where Catalan is spoken. The web site of the Generalitat de Catalunya estimated that as of 2004 there were 9,118,882 speakers of Catalan. These figures only reflect potential speakers; today it is the native language of only 35.6% of the Catalan population. According to "Ethnologue", Catalan had four million native speakers and five million second-language speakers in 2012.

According to a 2011 study the total number of Catalan speakers is over 9.8 million, with 5.9 million residing in Catalonia. More than half of them speak Catalan as a second language, with native speakers being about 4.4 million of those (more than 2.8 in Catalonia). Very few Catalan monoglots exist; basically, virtually all of the Catalan speakers in Spain are bilingual speakers of Catalan and Spanish, with a sizable population of Spanish-only speakers of immigrant origin (typically born outside Catalonia or with both parents born outside Catalonia) existing in the major Catalan urban areas as well. In Roussillon, only a minority of French Catalans speak Catalan nowadays, with French being the majority language for the inhabitants after a continued process of language shift. According to a 2019 survey by the Catalan government, 31.5% of the inhabitants of Catalonia have Catalan as first language at home whereas 52.7% have Spanish, 2.8% both Catalan and Spanish and 10.8% other languages.

Spanish is the most spoken language in Barcelona (according to the linguistic census held by the Government of Catalonia in 2013) and it is understood almost universally. According to this census of 2013 Catalan is also very commonly spoken in the city of 1,501,262: it is understood by 95% of the population, while 72.3% can speak it over the age of 2 (1,137,816), 79% can read it (1,246.555), and 53% can write it (835,080). The percentage in Barcelona who can speak is 72.3% is less than overall percentage of persons in Catalonia (7.5 million inhabitants) who can speak Catalan, 81.2% over the age of 15 (according to the graph below in this Wikipedia article 'Level of Knowledge'). Knowledge of Catalan has increased significantly in recent decades thanks to a language immersion educational system. The most important social characteristic of the Catalan language is that all the areas where it is spoken are bilingual in practice: together with the French language in Roussillon, with Italian in Alghero, with Spanish and French in Andorra and with Spanish in the rest of the territories.

(% of the population 15 years old and older).

(% of the population 15 years old and older).

Catalan phonology varies by dialect. Notable features include:

In contrast to other Romance languages, Catalan has many monosyllabic words, and these may end in a wide variety of consonants, including some consonant clusters. Additionally, Catalan has final obstruent devoicing, which gives rise to an abundance of such couplets as "(male friend") vs. ("female friend").

Central Catalan pronunciation is considered to be standard for the language. The descriptions below are mostly representative of this variety. For the differences in pronunciation between the different dialects, see the section on pronunciation of dialects in this article.

Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic also has instances of stressed . Dialects differ in the different degrees of vowel reduction, and the incidence of the pair .

In Central Catalan, unstressed vowels reduce to three: ; ; remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article).

The consonant system of Catalan is rather conservative.

Catalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyse the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact).


The dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Alguerese dialect.

Catalan is split in two major dialectal blocks: Eastern Catalan, and Western Catalan. The main difference lies in the treatment of unstressed and ; which have merged to in Eastern dialects, but which remain distinct as and in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary.

Western Catalan comprises the two dialects of Northwestern Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Alguerese. Each dialect can be further subdivided in several subdialects. The terms "Catalan" and "Valencian" (respectively used in Catalonia and the Valencian Community) are two varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in the Valencian Community.

Central Catalan is considered the standard pronunciation of the language and has the highest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona.

Catalan has an inflectional grammar. Nouns have two genders (masculine, feminine), and two numbers (singular, plural). Pronouns additionally can have a neuter gender, and some are also inflected for case and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages.

Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed . Dialects differ in the different degrees of vowel reduction, and the incidence of the pair .

In Eastern Catalan (except Majorcan), unstressed vowels reduce to three: ; ; remains distinct. There are a few instances of unreduced , in some words. Alguerese has lowered to .

In Majorcan, unstressed vowels reduce to four: follow the Eastern Catalan reduction pattern; however reduce to , with remaining distinct, as in Western Catalan.

In Western Catalan, unstressed vowels reduce to five: ; ; remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases.

Central, Western, and Balearic differ in the lexical incidence of stressed and . Usually, words with in Central Catalan correspond to in Balearic and in Western Catalan. Words with in Balearic almost always have in Central and Western Catalan as well. As a result, Central Catalan has a much higher incidence of .

Western Catalan: In verbs, the ending for 1st-person present indicative is in verbs of the 1st conjugation and -∅ in verbs of the 2nd and 3rd conjugations in most of the Valencian Community, or in all verb conjugations in the Northern Valencian Community and Western Catalonia.E.g. , , (Valencian); , , (Northwestern Catalan).

Eastern Catalan: In verbs, the ending for 1st-person present indicative is , , or -∅ in all conjugations. E.g. (Central), (Balearic), and (Northern), all meaning ('I speak').
Western Catalan: In verbs, the inchoative endings are /, , , .

Eastern Catalan: In verbs, the inchoative endings are , , , .

Western Catalan: In nouns and adjectives, maintenance of of medieval plurals in proparoxytone words.E.g. 'men', 'youth'.

Eastern Catalan: In nouns and adjectives, loss of of medieval plurals in proparoxytone words.E.g. 'men', 'youth'.

Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.

Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of the Valencian Community and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.

The most notable difference between both standards is some tonic accentuation, for instance: (IEC) – (AVL). Nevertheless, AVL's standard keeps the grave accent , while pronouncing it as rather than , in some words like: ('what'), or . Other divergences include the use of (AVL) in some words instead of like in / ('almond'), / ('back'), the use of elided demonstratives ( 'this', 'that') in the same level as reinforced ones () or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in at the same level as or the priority use of morpheme in 1st person singular in present indicative ( verbs): instead of ('I buy').

In the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing as much as ('we sing') but the University says that the priority form in the Balearic Islands must be in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: ('I buy'), ('I fear'), ('I sleep').

In Alghero, the IEC has adapted its standard to the Alguerese dialect. In this standard one can find, among other features: the definite article instead of , special possessive pronouns and determinants ('mine'), ('his/her'), ('yours'), and so on, the use of in the imperfect tense in all conjugations: , , ; the use of many archaic words, usual words in Alguerese: instead of ('less'), instead of ('someone'), instead of ('which'), and so on; and the adaptation of weak pronouns.

In 2011, the Aragonese government passed a decree approving the statutes of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon) as originally provided for by Law 10/2009. The new entity, designated as , shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja.

Valencian is classified as a Western dialect, along with the northwestern varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). The various forms of Catalan and Valencian are mutually intelligible (ranging from 90% to 95%)

Linguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language ("Acadèmia Valenciana de la Llengua," AVL) declares the linguistic unity between Valencian and Catalan varieties.

The AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castelló ("Normes de Castelló"). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture ("Acadèmia de Cultura Valenciana", RACV), which uses for Valencian an independent standard.

Despite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture ("Acadèmia de Cultura Valenciana", RACV), which uses for Valencian a standard independent from Catalan.

This clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical.

Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.

Literary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there has been a tendency towards favoring words of Northern dialects to the detriment of others, 

Like other languages, Catalan has a large list of loanwords from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. In the 14th and 15th centuries Catalan had a far greater number of Greco-Latin loanwords than other Romance languages, as is attested for example in Roís de Corella's writings. The incorporation of learned, or "bookish" words from its own ancestor language, Latin, into Catalan is arguably another form of lexical borrowing through the influence of written language and the liturgical language of the Church. Throughout the Middle Ages and into the early modern period, most literate Catalan speakers were also literate in Latin; and thus they easily adopted Latin words into their writing—and eventually speech—in Catalan.

The process of morphological derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example ("electrical") vs. . Prefixes are usually appended to verbs, as in ("foresee").

There is greater regularity in the process of word-compounding, where one can find compounded words formed much like those in English.

Catalan uses the Latin script, with some added symbols and digraphs. The Catalan orthography is systematic and largely phonologically based. Standardization of Catalan was among the topics discussed during the First International Congress of the Catalan Language, held in Barcelona October 1906. Subsequently, the Philological Section of the Institut d'Estudis Catalans (IEC, founded in 1911) published the "Normes ortogràfiques" in 1913 under the direction of Antoni Maria Alcover and Pompeu Fabra. In 1932, Valencian writers and intellectuals gathered in Castelló de la Plana to make a formal adoption of the so-called "Normes de Castelló", a set of guidelines following Pompeu Fabra's Catalan language norms.

The grammar of Catalan is similar to other Romance languages. Features include:

In gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix . Thus, the alternance of /, has been replaced by "ø"/. There are only a few exceptions, like / ("scarce"). Many not completely predictable morphological alternations may occur, such as:

Catalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has / ("boy"/"girl") and / ("cock"/"hen"), whereas French has / and /.

There is a tendency to abandon traditionally gender-invariable adjectives in favour of marked ones, something prevalent in Occitan and French. Thus, one can find / ("boiling") in contrast with traditional /.

As in the other Western Romance languages, the main plural expression is the suffix , which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely.
The most important one is the addition of before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: / ("the pulse"/"the pulses") vs. / ("the dust"/"the dusts").

The inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like ("of + the [plural]"), but not as many as Italian (which has , , , etc.).

Central Catalan has abandoned almost completely unstressed possessives (, etc.) in favour of constructions of article + stressed forms (, etc.), a feature shared with Italian.

The morphology of Catalan personal pronouns is complex, specially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian). Features include the gender-neutral and the great degree of freedom when combining different unstressed pronouns (65 combinations).

Catalan pronouns exhibit T–V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality.

This flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have ("they recommended me to him"), whereas in French one must say , and Spanish . This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish).

Like all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs.

The Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of ("to go") + infinitive.

Catalan verbs are traditionally divided into three conjugations, with vowel themes , , , the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of , with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding.

The grammar of Catalan follows the general pattern of Western Romance languages. The primary word order is subject–verb–object. However, word order is very flexible. Commonly, verb-subject constructions are used to achieve a semantic effect. The sentence "The train has arrived" could be translated as or . Both sentences mean "the train has arrived", but the former puts a focus on the train, while the latter puts a focus on the arrival. This subtle distinction is described as "what you might say while waiting in the station" versus "what you might say on the train."

In Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction "i" ("and").

Selected text from Manuel de Pedrolo's 1970 novel ("A love affair outside the city").

Institutions

About the Catalan language

Monolingual dictionaries

Bilingual and multilingual dictionaries

Automated translation systems

Phrasebooks

Learning resources

Catalan-language online encyclopedia


</doc>
<doc id="5285" url="https://en.wikipedia.org/wiki?curid=5285" title="STS-51-F">
STS-51-F

STS-51-F (also known as Spacelab 2) was the 19th flight of NASA's Space Shuttle program and the eighth flight of Space Shuttle "Challenger". It launched from Kennedy Space Center, Florida, on July 29, 1985, and landed just under eight days later on August 6.

While STS-51-F's primary payload was the Spacelab 2 laboratory module, the payload that received the most publicity was the Carbonated Beverage Dispenser Evaluation, which was an experiment in which both Coca-Cola and Pepsi tried to make their carbonated drinks available to astronauts. A helium-cooled Infrared telescope (IRT) was also flown on this mission, and while it did have some problems, it observed 60% of the galactic plane in infrared light.

During launch "Challenger" experienced multiple sensor failings in its RS-25 engines and had to perform an "Abort to Orbit" (ATO) emergency procedure. It is the only Shuttle mission to have carried out an abort after launching. As a result of the ATO, the mission was carried out at a slightly lower orbital altitude.

As with previous Spacelab missions, the crew was divided between two 12-hour shifts. Acton, Bridges and Henize made up the "Red Team" while Bartoe, England and Musgrave comprised the "Blue Team"; commander Fullerton could take either shift when needed. "Challenger" carried two EMUs in the event of an emergency spacewalk, which would have been performed by England and Musgrave.

STS-51-F's first launch attempt on July 12, 1985 was halted with the countdown at T−3 seconds after main engine ignition, when a malfunction of the number two RS-25 coolant valve caused the shutdown of all three main engines. "Challenger" launched successfully on its second attempt on July 29, 1985, at 17:00 EDT, after a delay of 1 hour 37 minutes due to a problem with the table maintenance block update uplink.

At 3 minutes 31 seconds into the ascent, one of the central engine's two high-pressure fuel turbopump turbine discharge temperature sensors failed. 2 minutes and 12 seconds later, the second sensor failed, causing the shutdown of the central engine. This was the only in-flight main-engine failure of the Space Shuttle program. Approximately 8 minutes into the flight, one of the same temperature sensors in the right engine failed, and the remaining right-engine temperature sensor displayed readings near the redline for engine shutdown. Booster Systems Engineer Jenny M. Howard acted quickly to command the crew to inhibit any further automatic RS-25 shutdowns based on readings from the remaining sensors, preventing the potential shutdown of a second engine and a possible abort mode that may have resulted in the loss of the vehicle and crew.

The failed RS-25 resulted in an Abort to Orbit (ATO) trajectory, whereby the shuttle achieved a lower-than-planned orbital altitude.

STS-51-F's primary payload was the laboratory module Spacelab 2. A special part of the modular Spacelab system, the "igloo", which was located at head of a three-pallet train, provided on-site support to instruments mounted on pallets. The main mission objective was to verify performance of Spacelab systems, determine the interface capability of the orbiter, and measure the environment created by the spacecraft. Experiments covered life sciences, plasma physics, astronomy, high-energy astrophysics, solar physics, atmospheric physics and technology research. Despite mission replanning necessitated by "Challenger"<nowiki>'</nowiki>s abort to orbit trajectory, the Spacelab mission was declared a success.

The flight marked the first time the ESA Instrument Pointing System (IPS) was tested in orbit. This unique pointing instrument was designed with an accuracy of one arcsecond. Initially, some problems were experienced when it was commanded to track the Sun, but a series of software fixes were made and the problem was corrected. In addition, Tony England became the second amateur radio operator to transmit from space during the mission.

The Spacelab Infrared Telescope (IRT) was also flown on the mission. The IRT was a 15.2 cm aperture helium-cooled infrared telescope, observing light between wavelengths of 1.7 to 118 μm. The experiment experienced some problems, it was thought heat emissions from the Shuttle corrupting long-wavelength data, but it still returned useful astronomical data. Another problem was that a piece of mylar insulation broke loose and floated in the line-of-sight of the telescope. IRT collected infrared data on 60% of the galactic plane. (see also List of largest infrared telescopes) A later space mission that experienced a stray light problem from debris was Gaia astrometry spacecraft launch in 2013 by the ESAthe source of the stray light was later identified as the fibers of the sunshield, protruding beyond the edges of the shield.

The Plasma Diagnostics Package (PDP), which had been previously flown on STS-3, made its return on the mission, and was part of a set of plasma physics experiments designed to study the Earth's ionosphere. During the third day of the mission, it was grappled out of the payload bay by the Remote Manipulator System and released for six hours. During this time, "Challenger" maneuvered around the PDP as part of a targeted proximity operations exercise. The PDP was successfully grappled by the RMS and returned to the payload bay at the beginning of the fourth day of the mission.

In a heavily publicized marketing experiment, astronauts aboard STS-51-F drank carbonated beverages from specially-designed cans provided by competitors Coca-Cola and Pepsi. Post-flight, the astronauts revealed that they preferred Tang, in part because it could be mixed on-orbit with existing chilled-water supplies, whereas there was no dedicated refrigeration equipment on board to chill the cans, which also fizzed excessively in microgravity.

In an experiment during the mission, thruster rockets were fired at a point over Tasmania and also above Boston to create two "holes" – plasma depletion regions – in the ionosphere. A worldwide group of geophysicists collaborated with the observations made from Spacelab 2.

"Challenger" landed at Edwards Air Force Base, California, on August 6, 1985, at 12:45:26 p.m. PDT. Its rollout distance was . The mission had been extended by 17 orbits for additional payload activities due to the Abort to Orbit. The orbiter arrived back at Kennedy Space Center on August 11, 1985.

The mission insignia was designed by Houston artist Skip Bradley. is depicted ascending toward the heavens in search of new knowledge in the field of solar and stellar astronomy, with its Spacelab 2 payload. The constellations Leo and Orion are shown in the positions they were in relative to the Sun during the flight. The nineteen stars indicate that the mission is the 19th shuttle flight.

C. Gordon Fullerton died on August 21, 2013, aged 76.

Karl Gordon Henize died October 5, 1993 on an expedition to Mount Everest studying the effects of radiation from space. (age 66)

One of the purposes of the mission was to test how suitable the Shuttle was for conducting infrared observations, and the IRT was operated on this mission. However, the orbiter was found to have some draw-backs for infrared astronomy, and this led to later infrared telescopes being free-flying from the Shuttle orbiter.




</doc>
<doc id="5288" url="https://en.wikipedia.org/wiki?curid=5288" title="Classical period (music)">
Classical period (music)

The Classical period was an era of classical music between roughly 1730 and 1820.

The Classical period falls between the Baroque and the Romantic periods. Classical music has a lighter, clearer texture than Baroque music and is less complex. It is mainly homophonic, using a clear melody line over a subordinate chordal accompaniment, but counterpoint was by no means forgotten, especially later in the period. It also makes use of "style galant" which emphasized light elegance in place of the Baroque's dignified seriousness and impressive grandeur. Variety and contrast within a piece became more pronounced than before and the orchestra increased in size, range, and power.

The harpsichord was replaced as the main keyboard instrument by the piano (or fortepiano). Unlike the harpsichord, which plucks strings with quills, pianos strike the strings with leather-covered hammers when the keys are pressed, which enables the performer to play louder or softer (hence the original name "fortepiano," literally "loud soft") and play with more expression; in contrast, the force with which a performer plays the harpsichord keys does not change the sound. Instrumental music was considered important by Classical period composers. The main kinds of instrumental music were the sonata, trio, string quartet, symphony (performed by an orchestra) and the solo concerto, which featured a virtuoso solo performer playing a solo work for violin, piano, flute, or another instrument, accompanied by an orchestra. Vocal music, such as songs for a singer and piano (notably the work of Schubert), choral works, and opera (a staged dramatic work for singers and orchestra) were also important during this period.

The best-known composers from this period are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Franz Schubert; other notable names include Luigi Boccherini, Muzio Clementi, Antonio Salieri, Leopold Mozart, Johann Christian Bach, Carl Philipp Emanuel Bach, and Christoph Willibald Gluck. Ludwig van Beethoven is regarded either as a Romantic composer or a Classical period composer who was part of the transition to the Romantic era. Franz Schubert is also a transitional figure, as were Johann Nepomuk Hummel, Luigi Cherubini, Gaspare Spontini, Gioachino Rossini, Carl Maria von Weber and Niccolò Paganini. The period is sometimes referred to as the era of "Viennese Classic" or "Classicism" (), since Gluck, Mozart, Haydn, Salieri, Schubert, and Beethoven all worked in Vienna.

In the middle of the 18th century, Europe began to move toward a new style in architecture, literature, and the arts, generally known as Classicism. This style sought to emulate the ideals of Classical antiquity, especially those of Classical Greece. Classical music used formality and emphasis on order and hierarchy, and a "clearer", "cleaner" style that used clearer divisions between parts (notably a clear, single melody accompanied by chords), brighter contrasts and "tone colors" (achieved by the use of dynamic changes and modulations to more keys). In contrast with the richly layered music of the Baroque era, Classical music moved towards simplicity rather than complexity. In addition, the typical size of orchestras began to increase, giving orchestras a more powerful sound.

The remarkable development of ideas in "natural philosophy" had already established itself in the public consciousness. In particular, Newton's physics was taken as a paradigm: structures should be well-founded in axioms and be both well-articulated and orderly. This taste for structural clarity began to affect music, which moved away from the layered polyphony of the Baroque period toward a style known as homophony, in which the melody is played over a subordinate harmony. This move meant that chords became a much more prevalent feature of music, even if they interrupted the melodic smoothness of a single part. As a result, the tonal structure of a piece of music became more audible.

The new style was also encouraged by changes in the economic order and social structure. As the 18th century progressed, the nobility became the primary patrons of instrumental music, while public taste increasingly preferred lighter, funny comic operas. This led to changes in the way music was performed, the most crucial of which was the move to standard instrumental groups and the reduction in the importance of the "continuo"—the rhythmic and harmonic groundwork of a piece of music, typically played by a keyboard (harpsichord or organ) and usually accompanied by a varied group of bass instruments, including cello, double bass, bass viol, and theorbo. One way to trace the decline of the continuo and its figured chords is to examine the disappearance of the term "obbligato", meaning a mandatory instrumental part in a work of chamber music. In Baroque compositions, additional instruments could be added to the continuo group according to the group or leader's preference; in Classical compositions, all parts were specifically noted, though not always "notated", so the term "obbligato" became redundant. By 1800, basso continuo was practically extinct, except for the occasional use of a pipe organ continuo part in a religious Mass in the early 1800s.

Economic changes also had the effect of altering the balance of availability and quality of musicians. While in the late Baroque, a major composer would have the entire musical resources of a town to draw on, the musical forces available at an aristocratic hunting lodge or small court were smaller and more fixed in their level of ability. This was a spur to having simpler parts for ensemble musicians to play, and in the case of a resident virtuoso group, a spur to writing spectacular, idiomatic parts for certain instruments, as in the case of the Mannheim orchestra, or virtuoso solo parts for particularly skilled violinists or flautists. In addition, the appetite by audiences for a continual supply of new music carried over from the Baroque. This meant that works had to be performable with, at best, one or two rehearsals. Even after 1790 Mozart writes about "the rehearsal", with the implication that his concerts would have only one rehearsal.

Since there was a greater emphasis on a single melodic line, there was greater emphasis on notating that line for dynamics and phrasing. This contrasts with the Baroque era, when melodies were typically written with no dynamics, phrasing marks or ornaments, as it was assumed that the performer would improvise these elements on the spot. In the Classical era, it became more common for composers to indicate where they wanted performers to play ornaments such as trills or turns. The simplification of texture made such instrumental detail more important, and also made the use of characteristic rhythms, such as attention-getting opening fanfares, the funeral march rhythm, or the minuet genre, more important in establishing and unifying the tone of a single movement.

The Classical period also saw the gradual development of sonata form, a set of structural principles for music that reconciled the Classical preference for melodic material with harmonic development, which could be applied across musical genres. The sonata itself continued to be the principal form for solo and chamber music, while later in the Classical period the string quartet became a prominent genre. The symphony form for orchestra was created in this period (this is popularly attributed to Joseph Haydn). The "concerto grosso" (a concerto for more than one musician), a very popular form in the Baroque era, began to be replaced by the "solo concerto", featuring only one soloist. Composers began to place more importance on the particular soloist's ability to show off virtuoso skills, with challenging, fast scale and arpeggio runs. Nonetheless, some "concerti grossi" remained, the most famous of which being Mozart's Sinfonia Concertante for Violin and Viola in E flat Major.

In the classical period, the theme is made up of phrases with contrasting melodic figures and rhythms. These phrases are relatively brief, typically four bars in length, and can occasionally seem sparse or terse. The texture is mainly homophonic, with a clear melody above a subordinate chordal accompaniment, for instance an Alberti bass. This contrasts with the practice in Baroque music, where a piece or movement would typically have only one musical subject, which would then be worked out in a number of voices according to the principles of counterpoint, while maintaining a consistent rhythm or metre throughout. As a result, Classical music tends to have a lighter, clearer texture than the Baroque. The classical style draws on the "style galant", a musical style which emphasised light elegance in place of the Baroque's dignified seriousness and impressive grandeur.

Structurally, Classical music generally has a clear musical form, with a well-defined contrast between tonic and dominant, introduced by clear cadences. Dynamics are used to highlight the structural characteristics of the piece. In particular, sonata form and its variants were developed during the early classical period and was frequently used. The Classical approach to structure again contrasts with the Baroque, where a composition would normally move between tonic and dominant and back again, but through a continual progress of chord changes and without a sense of "arrival" at the new key. While counterpoint was less emphasised in the classical period, it was by no means forgotten, especially later in the period, and composers still used counterpoint in "serious" works such as symphonies and string quartets, as well as religious pieces, such as Masses.

The classical musical style was supported by technical developments in instruments. The widespread adoption of equal temperament made classical musical structure possible, by ensuring that cadences in all keys sounded similar. The fortepiano and then the pianoforte replaced the harpsichord, enabling more dynamic contrast and more sustained melodies. Over the Classical period, keyboard instruments became richer, more sonorous and more powerful.

The orchestra increased in size and range, and became more standardised. The harpsichord or pipe organ basso continuo role in orchestra fell out of use between 1750 and 1775, leaving the string section woodwinds became a self-contained section, consisting of clarinets, oboes, flutes and bassoons.

While vocal music such as comic opera was popular, great importance was given to instrumental music. The main kinds of instrumental music were the sonata, trio, string quartet, symphony, concerto (usually for a virtuoso solo instrument accompanied by orchestra), and light pieces such as serenades and divertimentos. Sonata form developed and became the most important form. It was used to build up the first movement of most large-scale works in symphonies and string quartets. Sonata form was also used in other movements and in single, standalone pieces such as overtures.

In his book "The Classical Style", author and pianist Charles Rosen claims that from 1755 to 1775, composers groped for a new style that was more effectively dramatic. In the Baroque, dramatic expression was limited to the representation of individual sentiments (what Rosen terms "dramatic sentiment"). For example, in Handel's oratorio "Jephtha", the composer renders four different emotions, one for each character, in the quartet "O, spare your daughter". Eventually this depiction of individual emotions came to be seen as simplistic and unrealistic; composers sought to portray multiple emotions, simultaneously or progressively, within a single character or movement ("dramatic action"). Thus in the finale of act 2 of Mozart's "Die Entführung aus dem Serail", the lovers move "from joy through suspicion and outrage to final reconciliation." 

Musically speaking, this "dramatic action" required more musical variety. Where Baroque music was characterized by seamless flow within individual movements and largely uniform textures, composers after the Baroque sought to interrupt this flow with changes in texture, dynamic, harmony, or tempo. Among the stylistic developments which followed the Baroque, the most dramatic came to be called "Empfindsamkeit", (roughly "sensitive style"), and its most well-known practitioner was Carl Philipp Emmanuel Bach. Composers of this style employed the above-discussed interruptions in the most abrupt manner, and the music can sound illogical at times. The Italian composer Domenico Scarlatti took these developments further. His more than five hundred single-movement keyboard sonatas also contain abrupt changes of texture, but these changes are organized into periods, balanced phrases that became a hallmark of the classical style. However, Scarlatti's changes in texture still sound sudden and unprepared. The outstanding achievement of the great classical composers (Haydn, Mozart and Beethoven) was their ability to make these dramatic surprises sound logically motivated, so that "the expressive and the elegant could join hands."

At first the new style took over Baroque forms—the ternary "da capo aria" and the "sinfonia" and "concerto"—but composed with simpler parts, more notated ornamentation, rather than the improvised ornaments that were common in the Baroque era, and more emphatic division of pieces into sections. However, over time, the new aesthetic caused radical changes in how pieces were put together, and the basic formal layouts changed. Composers from this period sought dramatic effects, striking melodies, and clearer textures. One of the big textural changes was a shift away from the complex, dense polyphonic style of the Baroque, in which multiple interweaving melodic lines were played simultaneously, and towards homophony, a lighter texture which uses a clear single melody line accompanied by chords.

Baroque music generally uses many harmonic fantasies and polyphonic sections that focus less on the structure of the musical piece, and there was less emphasis on clear musical phrases. In the classical period, the harmonies became simpler. However, the structure of the piece, the phrases and small melodic or rhythmic motives, became much more important than in the Baroque period.

Another important break with the past was the radical overhaul of opera by Christoph Willibald Gluck, who cut away a great deal of the layering and improvisational ornaments and focused on the points of modulation and transition. By making these moments where the harmony changes more of a focus, he enabled powerful dramatic shifts in the emotional color of the music. To highlight these transitions, he used changes in instrumentation (orchestration), melody, and mode. Among the most successful composers of his time, Gluck spawned many emulators, one of whom was Antonio Salieri. Their emphasis on accessibility brought huge successes in opera, and in other vocal music such as songs, oratorios, and choruses. These were considered the most important kinds of music for performance and hence enjoyed greatest public success.

The phase between the Baroque and the rise of the Classical (around 1730), was home to various competing musical styles. The diversity of artistic paths are represented in the sons of Johann Sebastian Bach: Wilhelm Friedemann Bach, who continued the Baroque tradition in a personal way; Johann Christian Bach, who simplified textures of the Baroque and most clearly influenced Mozart; and Carl Philipp Emmanuel Bach, who composed passionate and sometimes violently eccentric music of the "Empfindsamkeit" movement. Musical culture was caught at a crossroads: the masters of the older style had the technique, but the public hungered for the new. This is one of the reasons C. P. E. Bach was held in such high regard: he understood the older forms quite well and knew how to present them in new garb, with an enhanced variety of form.

By the late 1750s there were flourishing centers of the new style in Italy, Vienna, Mannheim, and Paris; dozens of symphonies were composed and there were bands of players associated with musical theatres. Opera or other vocal music accompanied by orchestra was the feature of most musical events, with concertos and symphonies (arising from the overture) serving as instrumental interludes and introductions for operas and church services. Over the course of the Classical period, symphonies and concertos developed and were presented independently of vocal music.

The "normal" orchestra ensemble—a body of strings supplemented by winds—and movements of particular rhythmic character were established by the late 1750s in Vienna. However, the length and weight of pieces was still set with some Baroque characteristics: individual movements still focused on one "affect" (musical mood) or had only one sharply contrasting middle section, and their length was not significantly greater than Baroque movements. There was not yet a clearly enunciated theory of how to compose in the new style. It was a moment ripe for a breakthrough. 

The first great master of the style was the composer Joseph Haydn. In the late 1750s he began composing symphonies, and by 1761 he had composed a triptych ("Morning", "Noon", and "Evening") solidly in the contemporary mode. As a vice-Kapellmeister and later Kapellmeister, his output expanded: he composed over forty symphonies in the 1760s alone. And while his fame grew, as his orchestra was expanded and his compositions were copied and disseminated, his voice was only one among many.

While some scholars suggest that Haydn was overshadowed by Mozart and Beethoven, it would be difficult to overstate Haydn's centrality to the new style, and therefore to the future of Western art music as a whole. At the time, before the pre-eminence of Mozart or Beethoven, and with Johann Sebastian Bach known primarily to connoisseurs of keyboard music, Haydn reached a place in music that set him above all other composers except perhaps the Baroque era's George Frideric Handel. Haydn took existing ideas, and radically altered how they functioned—earning him the titles "father of the symphony" and "father of the string quartet".

One of the forces that worked as an impetus for his pressing forward was the first stirring of what would later be called Romanticism—the "Sturm und Drang", or "storm and stress" phase in the arts, a short period where obvious and dramatic emotionalism was a stylistic preference. Haydn accordingly wanted more dramatic contrast and more emotionally appealing melodies, with sharpened character and individuality in his pieces. This period faded away in music and literature: however, it influenced what came afterward and would eventually be a component of aesthetic taste in later decades.

The "Farewell Symphony", No. 45 in F Minor, exemplifies Haydn's integration of the differing demands of the new style, with surprising sharp turns and a long slow adagio to end the work. In 1772, Haydn completed his Opus 20 set of six string quartets, in which he deployed the polyphonic techniques he had gathered from the previous Baroque era to provide structural coherence capable of holding together his melodic ideas. For some, this marks the beginning of the "mature" Classical style, in which the period of reaction against late Baroque complexity yielded to a period of integration Baroque and Classical elements.

Haydn, having worked for over a decade as the music director for a prince, had far more resources and scope for composing than most other composers. His position also gave him the ability to shape the forces that would play his music, as he could select skilled musicians. This opportunity was not wasted, as Haydn, beginning quite early on his career, sought to press forward the technique of building and developing ideas in his music. His next important breakthrough was in the Opus 33 string quartets (1781), in which the melodic and the harmonic roles segue among the instruments: it is often momentarily unclear what is melody and what is harmony. This changes the way the ensemble works its way between dramatic moments of transition and climactic sections: the music flows smoothly and without obvious interruption. He then took this integrated style and began applying it to orchestral and vocal music.

Haydn's gift to music was a way of composing, a way of structuring works, which was at the same time in accord with the governing aesthetic of the new style. However, a younger contemporary, Wolfgang Amadeus Mozart, brought his genius to Haydn's ideas and applied them to two of the major genres of the day: opera, and the virtuoso concerto. Whereas Haydn spent much of his working life as a court composer, Mozart wanted public success in the concert life of cities, playing for the general public. This meant he needed to write operas and write and perform virtuoso pieces. Haydn was not a virtuoso at the international touring level; nor was he seeking to create operatic works that could play for many nights in front of a large audience. Mozart wanted to achieve both. Moreover, Mozart also had a taste for more chromatic chords (and greater contrasts in harmonic language generally), a greater love for creating a welter of melodies in a single work, and a more Italianate sensibility in music as a whole. He found, in Haydn's music and later in his study of the polyphony of J.S. Bach, the means to discipline and enrich his artistic gifts.

Mozart rapidly came to the attention of Haydn, who hailed the new composer, studied his works, and considered the younger man his only true peer in music. In Mozart, Haydn found a greater range of instrumentation, dramatic effect and melodic resource. The learning relationship moved in both directions. Mozart also had a great respect for the older, more experienced composer, and sought to learn from him.

Mozart's arrival in Vienna in 1780 brought an acceleration in the development of the Classical style. There, Mozart absorbed the fusion of Italianate brilliance and Germanic cohesiveness that had been brewing for the previous 20 years. His own taste for flashy brilliances, rhythmically complex melodies and figures, long cantilena melodies, and virtuoso flourishes was merged with an appreciation for formal coherence and internal connectedness. It is at this point that war and economic inflation halted a trend to larger orchestras and forced the disbanding or reduction of many theater orchestras. This pressed the Classical style inwards: toward seeking greater ensemble and technical challenges—for example, scattering the melody across woodwinds, or using a melody harmonized in thirds. This process placed a premium on small ensemble music, called chamber music. It also led to a trend for more public performance, giving a further boost to the string quartet and other small ensemble groupings.

It was during this decade that public taste began, increasingly, to recognize that Haydn and Mozart had reached a high standard of composition. By the time Mozart arrived at age 25, in 1781, the dominant styles of Vienna were recognizably connected to the emergence in the 1750s of the early Classical style. By the end of the 1780s, changes in performance practice, the relative standing of instrumental and vocal music, technical demands on musicians, and stylistic unity had become established in the composers who imitated Mozart and Haydn. During this decade Mozart composed his most famous operas, his six late symphonies that helped to redefine the genre, and a string of piano concerti that still stand at the pinnacle of these forms.

One composer who was influential in spreading the more serious style that Mozart and Haydn had formed is Muzio Clementi, a gifted virtuoso pianist who tied with Mozart in a musical "duel" before the emperor in which they each improvised on the piano and performed their compositions. Clementi's sonatas for the piano circulated widely, and he became the most successful composer in London during the 1780s. Also in London at this time was Jan Ladislav Dussek, who, like Clementi, encouraged piano makers to extend the range and other features of their instruments, and then fully exploited the newly opened up possibilities. The importance of London in the Classical period is often overlooked, but it served as the home to the Broadwood's factory for piano manufacturing and as the base for composers who, while less notable than the "Vienna School", had a decisive influence on what came later. They were composers of many fine works, notable in their own right. London's taste for virtuosity may well have encouraged the complex passage work and extended statements on tonic and dominant.

When Haydn and Mozart began composing, symphonies were played as single movements—before, between, or as interludes within other works—and many of them lasted only ten or twelve minutes; instrumental groups had varying standards of playing, and the continuo was a central part of music-making.

In the intervening years, the social world of music had seen dramatic changes. International publication and touring had grown explosively, and concert societies formed. Notation became more specific, more descriptive—and schematics for works had been simplified (yet became more varied in their exact working out). In 1790, just before Mozart's death, with his reputation spreading rapidly, Haydn was poised for a series of successes, notably his late oratorios and "London" symphonies. Composers in Paris, Rome, and all over Germany turned to Haydn and Mozart for their ideas on form.
In the 1790s, a new generation of composers, born around 1770, emerged. While they had grown up with the earlier styles, they heard in the recent works of Haydn and Mozart a vehicle for greater expression. In 1788 Luigi Cherubini settled in Paris and in 1791 composed "Lodoiska", an opera that raised him to fame. Its style is clearly reflective of the mature Haydn and Mozart, and its instrumentation gave it a weight that had not yet been felt in the grand opera. His contemporary Étienne Méhul extended instrumental effects with his 1790 opera "Euphrosine et Coradin", from which followed a series of successes. The final push towards change came from Gaspare Spontini, who was deeply admired by future romantic composers such as Weber, Berlioz and Wagner. The innovative harmonic language of his operas, their refined instrumentation and their "enchained" closed numbers (a structural pattern which was later adopted by Weber in Euryanthe and from him handed down, through Marschner, to Wagner), formed the basis from which French and German romantic opera had its beginnings. 

The most fateful of the new generation was Ludwig van Beethoven, who launched his numbered works in 1794 with a set of three piano trios, which remain in the repertoire. Somewhat younger than the others, though equally accomplished because of his youthful study under Mozart and his native virtuosity, was Johann Nepomuk Hummel. Hummel studied under Haydn as well; he was a friend to Beethoven and Franz Schubert. He concentrated more on the piano than any other instrument, and his time in London in 1791 and 1792 generated the composition and publication in 1793 of three piano sonatas, opus 2, which idiomatically used Mozart's techniques of avoiding the expected cadence, and Clementi's sometimes modally uncertain virtuoso figuration. Taken together, these composers can be seen as the vanguard of a broad change in style and the center of music. They studied one another's works, copied one another's gestures in music, and on occasion behaved like quarrelsome rivals.

The crucial differences with the previous wave can be seen in the downward shift in melodies, increasing durations of movements, the acceptance of Mozart and Haydn as paradigmatic, the greater use of keyboard resources, the shift from "vocal" writing to "pianistic" writing, the growing pull of the minor and of modal ambiguity, and the increasing importance of varying accompanying figures to bring "texture" forward as an element in music. In short, the late Classical was seeking music that was internally more complex. The growth of concert societies and amateur orchestras, marking the importance of music as part of middle-class life, contributed to a booming market for pianos, piano music, and virtuosi to serve as exemplars. Hummel, Beethoven, and Clementi were all renowned for their improvising.
The direct influence of the Baroque continued to fade: the figured bass grew less prominent as a means of holding performance together, the performance practices of the mid-18th century continued to die out. However, at the same time, complete editions of Baroque masters began to become available, and the influence of Baroque style continued to grow, particularly in the ever more expansive use of brass. Another feature of the period is the growing number of performances where the composer was not present. This led to increased detail and specificity in notation; for example, there were fewer "optional" parts that stood separately from the main score.

The force of these shifts became apparent with Beethoven's 3rd Symphony, given the name "Eroica", which is Italian for "heroic", by the composer. As with Stravinsky's "The Rite of Spring", it may not have been the first in all of its innovations, but its aggressive use of every part of the Classical style set it apart from its contemporary works: in length, ambition, and harmonic resources as well.

The First Viennese School is a name mostly used to refer to three composers of the Classical period in late-18th-century Vienna: Haydn, Mozart, and Beethoven. Franz Schubert is occasionally added to the list.

In German-speaking countries, the term "Wiener Klassik" (lit. "Viennese classical era/art") is used. That term is often more broadly applied to the Classical era in music as a whole, as a means to distinguish it from other periods that are colloquially referred to as "classical", namely Baroque and Romantic music.

The term "Viennese School" was first used by Austrian musicologist Raphael Georg Kiesewetter in 1834, although he only counted Haydn and Mozart as members of the school. Other writers followed suit, and eventually Beethoven was added to the list. The designation "first" is added today to avoid confusion with the Second Viennese School.

Whilst, Schubert apart, these composers certainly knew each other (with Haydn and Mozart even being occasional chamber-music partners), there is no sense in which they were engaged in a collaborative effort in the sense that one would associate with 20th-century schools such as the Second Viennese School, or Les Six. Nor is there any significant sense in which one composer was "schooled" by another (in the way that Berg and Webern were taught by Schoenberg), though it is true that Beethoven for a time received lessons from Haydn.

Attempts to extend the First Viennese School to include such later figures as Anton Bruckner, Johannes Brahms, and Gustav Mahler are merely journalistic, and never encountered in academic musicology.

Musical eras and their prevalent styles, forms and instruments seldom disappear at once; instead, features are replaced over time, until the old approach is simply felt as "old-fashioned". The Classical style did not "die" suddenly; rather, it gradually got phased out under the weight of changes. To give just one example, while it is generally stated that the Classical era stopped using the harpsichord in orchestras, this did not happen all of a sudden at the start of the Classical era in 1750. Rather, orchestras slowly stopped using the harpsichord to play basso continuo until the practice was discontinued by the end of the 1700s.

One crucial change was the shift towards harmonies centering on "flatward" keys: shifts in the subdominant direction . In the Classical style, major key was far more common than minor, chromaticism being moderated through the use of "sharpward" modulation (e.g., a piece in C major modulating to G major, D major, or A major, all of which are keys with more sharps). As well, sections in the minor mode were often used for contrast. Beginning with Mozart and Clementi, there began a creeping colonization of the subdominant region (the ii or IV chord, which in the key of C major would be the keys of d minor or F major). With Schubert, subdominant modulations flourished after being introduced in contexts in which earlier composers would have confined themselves to dominant shifts (modulations to the dominant chord, e.g., in the key of C major, modulating to G major). This introduced darker colors to music, strengthened the minor mode, and made structure harder to maintain. Beethoven contributed to this by his increasing use of the fourth as a consonance, and modal ambiguity—for example, the opening of the Symphony No. 9 in D minor.

Franz Schubert, Carl Maria von Weber, and John Field are among the most prominent in this generation of "Proto-Romantics", along with the young Felix Mendelssohn. Their sense of form was strongly influenced by the Classical style. While they were not yet "learned" composers (imitating rules which were codified by others), they directly responded to works by Beethoven, Mozart, Clementi, and others, as they encountered them. The instrumental forces at their disposal in orchestras were also quite "Classical" in number and variety, permitting similarity with Classical works.

However, the forces destined to end the hold of the Classical style gathered strength in the works of many of the above composers, particularly Beethoven. The most commonly cited one is harmonic innovation. Also important is the increasing focus on having a continuous and rhythmically uniform accompanying figuration: Beethoven's Moonlight Sonata was the model for hundreds of later pieces—where the shifting movement of a rhythmic figure provides much of the drama and interest of the work, while a melody drifts above it. Greater knowledge of works, greater instrumental expertise, increasing variety of instruments, the growth of concert societies, and the unstoppable domination of the increasingly more powerful piano (which was given a bolder, louder tone by technological developments such as the use of steel strings, heavy cast-iron frames and sympathetically vibrating strings) all created a huge audience for sophisticated music. All of these trends contributed to the shift to the "Romantic" style.

Drawing the line between these two styles is very difficult: some sections of Mozart's later works, taken alone, are indistinguishable in harmony and orchestration from music written 80 years later—and some composers continued to write in normative Classical styles into the early 20th century. Even before Beethoven's death, composers such as Louis Spohr were self-described Romantics, incorporating, for example, more extravagant chromaticism in their works (e.g., using chromatic harmonies in a piece's chord progression). Conversely, works such as Schubert's Symphony No. 5, written during the chronological dawn of the Romantic era, exhibit a deliberately anachronistic artistic paradigm, harking back to the compositional style of several decades before.

However, Vienna's fall as the most important musical center for orchestral composition during the late 1820s, precipitated by the deaths of Beethoven and Schubert, marked the Classical style's final eclipse—and the end of its continuous organic development of one composer learning in close proximity to others. Franz Liszt and Frédéric Chopin visited Vienna when they were young, but they then moved on to other cities. Composers such as Carl Czerny, while deeply influenced by Beethoven, also searched for new ideas and new forms to contain the larger world of musical expression and performance in which they lived.

Renewed interest in the formal balance and restraint of 18th century classical music led in the early 20th century to the development of so-called Neoclassical style, which numbered Stravinsky and Prokofiev among its proponents, at least at certain times in their careers.

The Baroque guitar, with four or five sets of double strings or "courses" and elaborately decorated soundhole, was a very different instrument from the early classical guitar which more closely resembles the modern instrument with the standard six strings. Judging by the number of instructional manuals published for the instrument – over three hundred texts were published by over two hundred authors between 1760 and 1860 – the classical period marked a golden age for guitar.

In the Baroque era, there was more variety in the bowed stringed instruments used in ensembles, with instruments such as the viola d'amore and a range of fretted viols being used, ranging from small viols to large bass viols. In the Classical period, the string section of the orchestra was standardized as just four instruments:


In the Baroque era, the double bass players were not usually given a separate part; instead, they typically played the same basso continuo bassline that the cellos and other low-pitched instruments (e.g., theorbo, serpent wind instrument, viols), albeit an octave below the cellos, because the double bass is a transposing instrument that sounds one octave lower than it is written. In the Classical era, some composers continued to write only one bass part for their symphony, labeled "bassi"; this bass part was played by cellists and double bassists. During the Classical era, some composers began to give the double basses their own part.










</doc>
<doc id="5295" url="https://en.wikipedia.org/wiki?curid=5295" title="Character encoding">
Character encoding

Character encoding is used to represent a repertoire of characters by some kind of encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in computation, data storage, and transmission of textual data. "Character set", "character map", "codeset" and "code page" are related, but not identical, terms.

Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent most of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.

The history of character codes illustrates the evolving need for machine-mediated character-based symbolic information over a distance, using once-novel electrical means. The earliest codes were based upon manual and hand-written encoding and cyphering systems, such as Bacon's cipher, Braille, International maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). With the adoption of electrical and electro-mechanical techniques these earliest codes were adapted to the new capabilities and limitations of the early machines. The earliest well-known electrically-transmitted character code, Morse code, introduced in the 1840s, used a system of four "symbols" (short signal, long signal, short space, long space) to generate codes of variable length. Though most commercial use of Morse code was via machinery, it was also used as a manual code, generatable by hand on a telegraph key and decipherable by ear, and persists in amateur radio use. Most codes are of fixed per-character length or variable-length sequences of fixed-length codes (e.g. Unicode).
Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode. Unicode, a well defined and extensible encoding system, has supplanted most earlier character encodings, but the path of code development to the present is fairly well known.

The Baudot code, a five-bit encoding, was created by Émile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930. The name "baudot" has been erroneously applied to ITA2 and its many variants. ITA2 suffered from many shortcomings and was often "improved" by many equipment manufacturers, sometimes creating compatibility issues. In 1959 the U.S. military defined its Fieldata code, a six-or seven-bit code, introduced by the U.S. Army Signal Corps. While Fieldata addressed many of the then-modern issues (e.g. letter and digit codes arranged for machine collation), Fieldata fell short of its goals and was short-lived. In 1963 the first ASCII (American Standard Code for Information Interchange) code was released (X3.4-1963) by the ASCII committee (which contained at least one member of the Fieldata committee, W. F. Leubbert) which addressed most of the shortcomings of Fieldata, using a simpler code. Many of the changes were subtle, such as collatable character sets within certain numeric ranges. ASCII63 was a success, widely adopted by industry, and with the follow-up issue of the 1967 ASCII code (which added lower-case letters and fixed some "control code" issues) ASCII67 was adopted fairly widely. ASCII67's American-centric nature was somewhat addressed in the European ECMA-6 standard, which persists today as the base encoding for the UNICODE extended encoding strings.
Somewhat historically isolated, IBM's Binary Coded Decimal (BCD) was a six-bit encoding scheme used by IBM in as early as 1959 in its 1401 and 1620 computers, and in its 7000 Series (for example, 704, 7040, 709 and 7090 computers), as well as in associated peripherals. BCD extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. It was the precursor to EBCDIC. For the most part, IBMs codes were used primarily with IBM equipment, which was more or less a closed ecosystem, and did not see much adoption outside of IBM "circles". IBM's Extended Binary Coded Decimal Interchange Code (usually abbreviated as EBCDIC) is an eight-bit encoding scheme developed in 1963.

The limitations of such sets soon became apparent, and a number of "ad hoc" methods were developed to extend them. The need to support more writing systems for different languages, including the CJK family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous "ad hoc" approaches.

In trying to develop universally interchangeable character encodings, researchers in the 1980s faced the dilemma that on the one hand, it seemed necessary to add more bits to accommodate additional characters, but on the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users).

The compromise solution that was eventually found and developed into Unicode was to break the assumption (dating back to telegraph codes) that each character should always directly correspond to a particular sequence of bits. Instead, characters would first be mapped to a universal intermediate representation in the form of abstract numbers called code points. Code points would then be represented in a variety of ways and with various default numbers of bits per character (code units) depending on context. To encode code points higher than the length of the code unit, such as above 256 for 8-bit units, the solution was to implement variable-width encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.




The character repertoire is an abstract set of more than one million characters found in a wide variety of scripts including Latin, Cyrillic, Chinese, Korean, Japanese, Hebrew, and Aramaic.

Other symbols such as musical notation are also included in the character repertoire. Both the Unicode and GB18030 standards have a character repertoire. As new characters are added to one standard, the other standard also adds those characters, to maintain parity.

The code unit size is equivalent to the bit measurement for the particular encoding:

"Example of a code unit:" Consider a string of the letters "abc" followed by (represented with 1 char32_t, 2 char16_t or 4 char8_t). That string contains:

The convention to refer to a character in Unicode is to start with 'U+' followed by the codepoint value in hexadecimal. The range of valid code points for the Unicode standard is U+0000 to U+10FFFF, inclusive, divided in 17 planes, identified by the numbers 0 to 16. Characters in the range U+0000 to U+FFFF are in plane 0, called the Basic Multilingual Plane (BMP). This plane contains most commonly-used characters. Characters in the range U+10000 to U+10FFFF in the other planes are called supplementary characters.

The following table shows examples of code point values:
A code point is represented by a sequence of code units. The mapping is defined by the encoding. Thus, the number of code units required to represent a code point depends on the encoding:

Unicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a modern, unified character encoding. Rather than mapping characters directly to octets (bytes), they separately define what characters are available, corresponding natural numbers (code points), how those numbers are encoded as a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets. The purpose of this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model correctly requires more precise terms than "character set" and "character encoding." The terms used in the modern model follow:

A character repertoire is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the Windows code pages). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into basic information units. The basic variants of the Latin, Greek and Cyrillic alphabets can be broken down into letters, digits, punctuation, and a few "special characters" such as the space, which can all be arranged in simple linear sequences that are displayed in the same order they are read. But even with these alphabets, diacritics pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and glyphs that are joined together in different ways for different situations.

A coded character set (CCS) is a function that maps characters to "code points" (each code point represents one character). For example, in a given repertoire, the capital letter "A" in the Latin alphabet might be represented by the code point 65, the character "B" to 66, and so on. Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points.

A character encoding form (CEF) is the mapping of code points to "code units" to facilitate storage in a system that represents numbers as bit sequences of fixed length (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units can only directly represent code points 0 to 65,535 in each unit, but larger code points (say, 65,536 to 1.4 million) could be represented by using multiple 16-bit units. This correspondence is defined by a CEF.

Next, a character encoding scheme (CES) is the mapping of code units to a sequence of octets to facilitate storage on an octet-based file system or transmission over an octet-based network. Simple character encoding schemes include UTF-8, UTF-16BE, UTF-32BE, UTF-16LE or UTF-32LE; compound character encoding schemes, such as UTF-16, UTF-32 and ISO/IEC 2022, switch between several simple schemes by using byte order marks or escape sequences; compressing schemes try to minimise the number of bytes used per code unit (such as SCSU, BOCU, and Punycode).

Although UTF-32BE is a simpler CES, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-width ASCII and maps Unicode code points to variable-width sequences of octets, or UTF-16BE, which is backward compatible with fixed-width UCS-2BE and maps Unicode code points to variable-width sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.

Finally, there may be a higher level protocol which supplies additional information to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.

The Unicode model uses the term character map for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.

Historically, the terms "character encoding", "character map", "character set" and "code page" were synonymous in computer science, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units – usually with a single character per code unit. But now the terms have related but distinct meanings, due to efforts by standards bodies to use precise terminology when writing about and unifying many different encoding systems. Regardless, the terms are still used interchangeably, with "character set" being nearly ubiquitous.

A "code page" usually means a byte-oriented encoding, but with regard to some suite of encodings (covering different scripts), where many characters share the same codes in most or all those code pages. Well-known code page suites are "Windows" (based on Windows-1252) and "IBM"/"DOS" (based on code page 437), see Windows code page for details. Most, but not all, encodings referred to as code pages are single-byte encodings (but see octet on byte size.)

IBM's Character Data Representation Architecture (CDRA) designates with coded character set identifiers (CCSIDs) and each of which is variously called a "charset", "character set", "code page", or "CHARMAP".

The term "code page" does not occur in Unix or Linux where "charmap" is preferred, usually in the larger context of locales.

Contrasted to CCS above, a "character encoding" is a map from abstract characters to code words. A "character set" in HTTP (and MIME) parlance is the same as a character encoding (but not the same as CCS).

"Legacy encoding" is a term sometimes used to characterize old character encodings, but with an ambiguity of sense. Most of its use is in the context of Unicodification, where it refers to encodings that fail to cover all Unicode code points, or, more generally, using a somewhat different character repertoire: several code points representing one Unicode character, or versa (see e.g. code page 437). Some sources refer to an encoding as "legacy" only because it preceded Unicode. All Windows code pages are usually referred to as legacy, both because they antedate Unicode and because they are unable to represent all 2 possible Unicode code points.

As a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes as a form of data transcoding. Some of these are cited below.

Cross-platform:
Unix-like: 

Windows:




</doc>
<doc id="5298" url="https://en.wikipedia.org/wiki?curid=5298" title="Control character">
Control character

In computing and telecommunication, a control character or non-printing character (NPC) is a code point (a number) in a character set, that does not represent a written symbol. They are used as in-band signaling to cause effects other than the addition of a symbol to the text. All other characters are mainly printing, printable, or graphic characters, except perhaps for the "space" character (see ASCII printable characters).

All entries in the ASCII table below code 32 (technically the C0 control code set) are of this kind, including CR and LF used to separate lines of text. The code 127 (DEL) is also a control character. Extended ASCII sets defined by ISO 8859 added the codes 128 through 159 as control characters, this was primarily done so that if the high bit was stripped it would not change a printing character to a C0 control code, but there have been some assignments here, in particular NEL. This second set is called the C1 set.

These 65 control codes were carried over to Unicode. Unicode added more characters that could be considered controls, but it makes a distinction between these "Formatting characters" (such as the Zero-width non-joiner), and the 65 Control characters.

The Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes as well as additional codes which are mostly used to control IBM peripherals.

Procedural signs in Morse code are a form of control character.

A form of control characters were introduced in the 1870 Baudot code: NUL and DEL.
The 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.

The bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.

Control characters have also been called "format effectors".

The control characters in ASCII still in common use include:

Control characters may be described as doing something when the user inputs them, such as code 3 (End-of-Text character, ETX, codice_32) to interrupt the running process, or code 4 (End of transmission, EOT, codice_33), used to end text input or to exit a Unix shell. These uses usually have little to do with their use when they are in text being output, and on modern systems usually do not involve the transmission of the code number at all (instead the program gets the fact that the user is holding down the Ctrl key and pushing the key marked with a 'C').

There were quite a few control characters defined (33 in ASCII, and the ECMA-48 standard adds 32 more). This was because early terminals had very primitive mechanical or electrical controls that made any kind of state-remembering API quite expensive to implement, thus a different code for each and every function looked like a requirement. It quickly became possible and inexpensive to interpret sequences of codes to perform a function, and device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 27 (escape), followed by a series of characters called a "control sequence" or "escape sequence". The mechanism was invented by Bob Bemer, the father of ASCII. For example, the sequence of code 27, followed by the printable characters <nowiki>"[2;10H"</nowiki>, would cause a DEC VT-102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64. But the number of non-standard variations in use is large, especially among printers, where technology has advanced far faster than any standards body can possibly keep up with.

In Unicode, "Control-characters" are U+0000—U+001F (C0 controls), U+007F (delete), and U+0080—U+009F (C1 controls). Their General Category is "Cc". Formatting codes are distinct, in General Category "Cf". The Cc control characters have no Name in Unicode, but are given labels such as "<control-001A>" instead.

There are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:

ASCII-based keyboards have a key labelled "Control", "Ctrl", or (rarely) "Cntl" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 64 from ASCII code value in decimal of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 31, forcing bits 6 and 7 to zero. For example, pressing "control" and the letter "g" or "G" (code 107 in octal or 71 in base 10, which is 01000111 in binary, produces the code 7 (Bell, 7 in base 10, or 00000111 in binary). The NULL character (code 0) is represented by Ctrl-@, "@" being the code immediately before "A" in the ASCII character set. For convenience, a lot of terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. This approach is not able to represent the DEL character because of its value (code 127), but Ctrl-? is often used for this character, as subtracting 64 from a '?' gives −1, which if masked to 7 bits is 127.

When the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys. In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter. The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) vary between systems. Some will produce the same character code as if the control key were not held down. Other systems translate these keys into control characters when the control key is held down. The interpretation of the control key with non-ASCII ("foreign") keys also varies between systems.

Control characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64. Control characters generated using letter keys are thus displayed with the upper-case form of the letter. For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.

Keyboards also typically have a few single keys which produce control character codes. For example, the key labelled "Backspace" typically produces code 8, "Tab" code 9, "Enter" or "Return" code 13 (though some keyboards might produce code 10 for "Enter").

Many keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions. The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. "Dumb" computer terminals typically use control sequences. Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods. Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.

The control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.

Printing control characters were first used to control the physical mechanism of printers, the earliest output device. An early implementation of this idea was the out-of-band ASA carriage control characters. Later, control characters were integrated into the stream of data to be printed.
The carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).
The line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (which would be the leftmost position for left-to-right scripts, such as the alphabets used for Western languages, and the rightmost position for right-to-left scripts such as the Hebrew and Arabic alphabets). 
The vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading. 
The form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line. 
The backspace character (BS) moves the printing position one character space backwards. On printers, this is most often used so the printer can overprint characters to make other, not normally available, characters. On terminals and other electronic output devices, there are often software (or hardware) configuration choices which will allow a destruct backspace (i.e., a BS, SP, BS sequence) which erases, or a non-destructive one which does not. 
The shift in and shift out characters (SO and SI) selected alternate character sets, fonts, underlining or other printing modes. Escape sequences were often used to do the same thing.

With the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.

The separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.
End of medium (EM) warns that the tape (or other recording medium) is ending.
While many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.

The transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.

The start of heading (SOH) character was to mark a non-data section of a data stream—the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.

The escape character (ESC) was intended to "quote" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today.

The substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, "end of text" or "end of file" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.

The cancel character (CAN) signalled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again. The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element.

When a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).

The device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver is temporarily unable to accept any more data. Digital Equipment Corporation invented a convention which used 19 (the device control 3 character (DC3), also known as control-S, or XOFF) to "S"top transmission, and 17 (the device control 1 character (DC1), a.k.a. control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however.

The data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX. For example a packet may be structured in the following way (DLE) <STX> <PAYLOAD> (DLE) <ETX>.

Code 7 (BEL) is intended to cause an audible signal in the receiving terminal.

Many of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, "synchronous idle" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word— this is a feature of "asynchronous" communication. "Synchronous" communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)

Code 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes. It is convenient to treat this as a "fill" character with no meaning otherwise. Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing it is often used for padding in fixed length records and more commonly, to mark the end of a string.

Code 127 (DEL, a.k.a. "rubout") is likewise a special case. Its 7-bit code is "all-bits-on" in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfrów. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. Some systems (such as the original Apples) converted it to a backspace. But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black "box" character useful for erasing text by overprinting with ink).

Non-erasable Programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.

Many file systems do not allow control characters in the filenames, as they may have reserved functions.




</doc>
<doc id="5299" url="https://en.wikipedia.org/wiki?curid=5299" title="Carbon">
Carbon

Carbon (from "coal") is a chemical element with the symbol C and atomic number 6. It is nonmetallic and tetravalent—making four electrons available to form covalent chemical bonds. It belongs to group 14 of the periodic table. Three isotopes occur naturally, C and C being stable, while C is a radionuclide, decaying with a half-life of about 5,730 years. Carbon is one of the few elements known since antiquity.

Carbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.

The atoms of carbon can bond together in diverse ways, resulting in various allotropes of carbon. The best known allotropes are graphite, diamond, and buckminsterfullerene. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb "γράφειν" which means "to write"), while diamond is the hardest naturally occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form at standard temperature and pressure. They are chemically resistant and require high temperature to react even with oxygen.

The most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions. For this reason, carbon has often been referred to as the "king of the elements".

The allotropes of carbon include graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms, including other carbon atoms, and is capable of forming multiple stable covalent bonds with suitable multivalent atoms. Carbon is known to form almost ten million compounds, a large majority of all chemical compounds. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point, as its triple point is at and , so it sublimes at about . Graphite is much more reactive than diamond at standard conditions, despite being more thermodynamically stable, as its delocalised pi system is much more vulnerable to attack. For example, graphite can be oxidised by hot concentrated nitric acid at standard conditions to mellitic acid, C(COH), which preserves the hexagonal units of graphite while breaking up the larger structure.

Carbon sublimes in a carbon arc, which has a temperature of about 5800 K (5,530 °C or 9,980 °F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest-melting-point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper, which are weaker reducing agents at room temperature.

Carbon is the sixth element, with a ground-state electron configuration of 1s2s2p, of which the four outer electrons are valence electrons. Its first four ionisation energies, 1086.5, 2352.6, 4620.5 and 6222.7 kJ/mol, are much higher than those of the heavier group-14 elements. The electronegativity of carbon is 2.5, significantly higher than the heavier group-14 elements (1.8–1.9), but close to most of the nearby nonmetals, as well as some of the second- and third-row transition metals. Carbon's covalent radii are normally taken as 77.2 pm (C−C), 66.7 pm (C=C) and 60.3 pm (C≡C), although these may vary depending on coordination number and what the carbon is bonded to. In general, covalent radius decreases with lower coordination number and higher bond order.

Carbon compounds form the basis of all known life on Earth, and the carbon–nitrogen cycle provides some of the energy produced by the Sun and other stars. Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:

Carbon monoxide can be recycled to smelt even more iron:

with sulfur to form carbon disulfide and with steam in the coal-gas reaction:
Carbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.

The system of carbon allotropes spans a range of extremes:
Atomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with diverse molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite, glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).

Graphene is a two-dimensional sheet of carbon with the atoms arranged in a hexagonal lattice. As of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes. If successful, graphene could be used in the construction of a space elevator. It could also be used to safely store hydrogen for use in a hydrogen based engine in cars.
The amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot) and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a π-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.
At very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that ""diamonds are forever"", they are thermodynamically unstable (Δ"G"°(diamond, 298 K) = 2.9 kJ/mol) under normal conditions (298 K, 10 Pa) and transform into graphite. Due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. The bottom left corner of the phase diagram for carbon has not been scrutinized experimentally. However, a recent computational study employing density functional theory methods reached the conclusion that as and , diamond becomes "more stable" than graphite by approximately 1.1 kJ/mol. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.

Fullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of flat hexagonal cells only, some of the cells of which fullerenes are formed may be pentagons, nonplanar hexagons, or even heptagons of carbon atoms. The sheets are thus warped into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names "fullerene" and "buckyball" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C buckminsterfullerene). Carbon nanotubes (buckytubes) are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid buckytube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.

Of the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2 kg/m. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure −(C:::C)−. Carbon in this modification is linear with "sp" orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is 40 times that of the hardest known material – diamond.

In 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high energy low duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagnetism, fluorescence, and a hardness superior to diamonds.

In the vapor phase, some of the carbon is in the form of dicarbon (). When excited, this gas glows green.

Carbon is the fourth most abundant chemical element in the observable universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the solar system was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.

In 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed "a couple of billion years" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.

It has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is , this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).

In combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 900 gigatonnes of carbon — each ppm corresponds to 2.13 Gt) and dissolved in all water bodies (approximately 36,000 gigatonnes of carbon). Carbon in the biosphere has been estimated at 550 gigatonnes but with a large uncertainty, due mostly to a huge uncertainty in the amount of terrestrial deep subsurface bacteria. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal "reserves" (not "resources") amount to around 900 gigatonnes with perhaps 18,000 Gt of resources. Oil reserves are around 150 gigatonnes. Proven sources of natural gas are about (containing about 105 gigatonnes of carbon), but studies estimate another of "unconventional" deposits such as shale gas, representing about 540 gigatonnes of carbon.

Carbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500 Gt, or 3,000 Gt.

In the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2,000 Gt.

Carbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble and so on). Coal is very rich in carbon (anthracite contains 92–98%) and is the largest commercial source of mineral carbon, accounting for 4,000 gigatonnes or 80% of fossil fuel.

As for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic "necks", or "pipes". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.

Carbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9–15 km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton. As such, of atmospheric carbon dioxide contains carbon-14.

Carbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in our solar system. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.

Isotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (C) forms 98.93% of the carbon on Earth, while carbon-13 (C) forms the remaining 1.07%. The concentration of C is further increased in biological materials because biochemical reactions discriminate against C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in nuclear magnetic resonance (NMR) experiments is done with the isotope C.

Carbon-14 (C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of 1 part per trillion (0.0000000001%) or more, mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158 MeV β emission. Because of its relatively short half-life of 5730 years, C is virtually absent in ancient rocks. The amount of C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000 years.

There are 15 known isotopes of carbon and the shortest-lived of these is C which decays through proton emission and alpha decay and has a half-life of 1.98739x10 s. The exotic C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.

Formation of the carbon atomic nucleus occurs within a giant or supergiant star through the triple-alpha process. This requires a nearly simultaneous collision of three alpha particles (helium nuclei), as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. The triple-alpha process happens in conditions of temperatures over 100 megakelvins and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.

According to current physical cosmology theory, carbon is formed in the interiors of stars on the horizontal branch. When massive stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of the next-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it.

The CNO cycle is an additional hydrogen fusion mechanism that powers stars, wherein carbon operates as a catalyst.

Rotational transitions of various isotopic forms of carbon monoxide (for example, CO, CO, and CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.

Under terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.

Carbon can form very long chains of interconnecting carbon–carbon bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon than do not. A similar claim can be made for hydrogen because most organic compounds contain hydrogen chemically bonded to carbon or another common element like oxygen or nitrogen.

The simplest form of an organic molecule is the hydrocarbon—a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other atoms, known as heteroatoms. Common heteroatoms that appear in organic compounds include oxygen, nitrogen, sulfur, phosphorus, and the nonradioactive halogens, as well as the metals lithium and magnesium. Organic compounds containing bonds to metal are known as organometallic compounds ("see below"). Certain groupings of atoms, often including heteroatoms, recur in large numbers of organic compounds. These collections, known as "functional groups", confer common reactivity patterns and allow for the systematic study and categorization of organic compounds. Chain length, shape and functional groups all affect the properties of organic molecules.

In most stable compounds of carbon (and nearly all stable "organic" compounds), carbon obeys the octet rule and is "tetravalent", meaning that a carbon atom forms a total of four covalent bonds (which may include double and triple bonds). Exceptions include a small number of stabilized "carbocations" (three bonds, positive charge), "radicals" (three bonds, neutral), "carbanions" (three bonds, negative charge) and "carbenes" (two bonds, neutral), although these species are much more likely to be encountered as unstable, reactive intermediates.

Carbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.

When combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, and aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells.

Commonly carbon-containing compounds which are associated with minerals or which do not contain bonds to the other carbon atoms, halogens, or hydrogen, are treated separately from classical organic compounds; the definition is not rigid, and the classification of some compounds can vary from author to author (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide () is similar. Nevertheless, due to its physical properties and its association with organic synthesis, carbon disulfide is sometimes classified as an "organic" solvent.

The other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)), similar to diatomic halides. Likewise, the heavier analog of cyanide, cyaphide (CP), is also considered inorganic, though most simple derivatives are highly unstable. Other uncommon oxides are carbon suboxide (), the unstable dicarbon monoxide (CO), carbon trioxide (CO), cyclopentanepentone (CO), cyclohexanehexone (CO), and mellitic anhydride (CO). However, mellitic anhydride is the triple acyl anhydride of mellitic acid; moreover, it contains a benzene ring. Thus, many chemists consider it to be organic.

With reactive metals, such as tungsten, carbon forms either carbides (C) or acetylides () to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond. Nevertheless, even the most polar and salt-like of carbides are not completely ionic compounds.

Organometallic compounds by definition contain at least one carbon-metal covalent bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), η-alkene compounds (for example, Zeise's salt), and η-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls and metal cyanides exist (for example, tetracarbonylnickel and potassium ferricyanide); some workers consider metal carbonyl and cyanide complexes without other carbon ligands to be purely inorganic, and not organometallic. However, most organometallic chemists consider metal complexes with any carbon ligand, even 'inorganic carbon' (e.g., carbonyls, cyanides, and certain types of carbides and acetylides) to be organometallic in nature. Metal complexes containing organic ligands without a carbon-metal covalent bond (e.g., metal carboxylates) are termed "metalorganic" compounds.

While carbon is understood to strongly prefer formation of four covalent bonds, other exotic bonding schemes are also known. Carboranes are highly stable dodecahedral derivatives of the [BH] unit, with one BH replaced with a CH. Thus, the carbon is bonded to five boron atoms and one hydrogen atom. The cation [(PhPAu)C] contains an octahedral carbon bound to six phosphine-gold fragments. This phenomenon has been attributed to the aurophilicity of the gold ligands, which provide additional stabilization of an otherwise labile species. In nature, the iron-molybdenum cofactor (FeMoco) responsible for microbial nitrogen fixation likewise has an octahedral carbon center (formally a carbide, C(-IV)) bonded to six iron atoms. In 2016, it was confirmed that, in line with earlier theoretical predictions, the hexamethylbenzene dication contains a carbon atom with six bonds. More specifically, the dication could be described structurally by the formulation [MeC(η-CMe)], making it an "organic metallocene" in which a MeC fragment is bonded to a η-CMe fragment through all five of the carbons of the ring.

It is important to note that in the cases above, each of the bonds to carbon contain less than two formal electron pairs. Thus, the formal electron count of these species does not exceed an octet. This makes them hypercoordinate but not hypervalent. Even in cases of alleged 10-C-5 species (that is, a carbon with five ligands and a formal electron count of ten), as reported by Akiba and co-workers, electronic structure calculations conclude that the electron population around carbon is still less than eight, as is true for other compounds featuring four-electron three-center bonding.

The English name "carbon" comes from the Latin "carbo" for coal and charcoal, whence also comes the French "charbon", meaning charcoal. In German, Dutch and Danish, the names for carbon are "Kohlenstoff", "koolstof" and "kulstof" respectively, all literally meaning coal-substance.

Carbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500 BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.

In 1722, René Antoine Ferchault de Réaumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram.
In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave "aerial acid" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name "carbone" (Latin "carbonum") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.

A new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers – Robert Curl, Harold Kroto and Richard Smalley – received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms lead to the discovery of further exotic allotropes, including glassy carbon, and the realization that "amorphous carbon" is not strictly amorphous.

Commercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica and feldspars in schists, gneisses and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made simply by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.

There are three types of natural graphite—amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry "amorphous" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.

According to the USGS, world production of natural graphite was 1.1 million tonnes in 2010, to which China contributed 800,000 t, India 130,000 t, Brazil 76,000 t, North Korea 30,000 t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998 million was produced in 2009.

The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).

Only a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.

Historically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.

Diamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the diamond fields in South Africa. Production has increased over time and now an accumulated total of 4.5 billion carats have been mined since that date. About 20% of that amount has been mined in the last 5 years alone, and during the last ten years 9 new mines have started production while 4 more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.

In the United States, diamonds have been found in Arkansas, Colorado and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.

Today, most commercially viable diamond deposits are in Russia, Botswana, Australia and the Democratic Republic of Congo. In 2005, Russia produced almost one-fifth of the global diamond output, reports the British Geological Survey. Australia has the richest diamantiferous pipe with production reaching peak levels of per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada, Siberia (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe), Brazil, and in Northern and Western Australia.

Carbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.

The uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors and as a neutron moderator in nuclear reactors.

Charcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.

Carbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.

Carbon black is used as the black pigment in printing ink, artist's oil paint and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron and titanium, are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.
The diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets function dramatically differently.

Unlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.

Industrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20 tonnes annually) are unsuitable for use as gemstones are relegated for industrial use (known as "bort)". synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3 billion carats (600 tonnes) of synthetic diamond is produced annually.

The dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.

Pure carbon has extremely low toxicity to humans and can be handled and even ingested safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and Ötzi the Iceman was found to have carbon tattoos that survived during his life and for 5200 years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can be harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.

Carbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to "Drosophila".

Carbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.

In nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250 °C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.

The great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant "Ricinus communis", cyanide (CN), and carbon monoxide; and such essentials to life as glucose and protein.




</doc>
<doc id="5300" url="https://en.wikipedia.org/wiki?curid=5300" title="Computer data storage">
Computer data storage

Computer data storage, often called storage, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.

The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) are referred to as "memory", while slower persistent technologies are referred to as "storage".

Even the very first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.

Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.

A modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 1 or 0. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate "the binary representation of the piece of information", or simply data. For example, the complete works of Shakespeare, about 1250 pages in print, can be stored in about five megabytes (40 million bits) with one byte per character.

Data are encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g., character encodings like ASCII, image encodings like JPEG, video encodings like MPEG-4).

By adding bits to each encoded unit, redundancy allows the computer to both detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or "physical bit fatigue", loss of the physical bit in storage of its ability to maintain a distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g., due to random radiation) is typically corrected upon detection. A bit, or a group of malfunctioning physical bits (not always the specific defective bit is known; group definition depends on specific storage device) is typically automatically fenced-out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.

Data compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string ("compress") and reconstruct the original string ("decompress") when needed. This utilizes substantially less storage (tens of percents) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not.

For security reasons certain types of data (e.g., credit-card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots.

Generally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary and off-line storage is also guided by cost per bit.

In contemporary usage, "memory" is usually semiconductor storage read-write random-access memory, typically DRAM (dynamic RAM) or other forms of fast but temporary storage. "Storage" consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down).

Historically, "memory" has been called "core memory", "main memory", "real storage" or "internal memory". Meanwhile, non-volatile storage devices have been referred to as "secondary storage", "external memory" or "auxiliary/peripheral storage".

"Primary storage" (also known as "main memory", "internal memory" or "prime memory"), often referred to simply as "memory", is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.

Historically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive.

This led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. (The particular types of RAM used for primary storage are also volatile, i.e. they lose the information when not powered).

As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM:


Main memory is directly or indirectly connected to the central processing unit via a "memory bus". It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks.

As the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile "secondary" storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of "random access").

Many types of "ROM" are not literally "read only", as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly.

Recently, "primary storage" and "secondary storage" in some uses refer to what was historically called, respectively, "secondary storage" and "tertiary storage".

"Secondary storage" (also known as "external memory" or "auxiliary storage"), differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.

In modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as secondary storage. The access time per byte for HDDs or SSDs is typically measured in milliseconds (one thousandth seconds), while the access time per byte for primary storage is measured in nanoseconds (one billionth seconds). Thus, secondary storage is significantly slower than primary storage. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. Other examples of secondary storage technologies include USB flash drives, floppy disks, magnetic tape, paper tape, punched cards, and RAM disks.

Once the disk read/write head on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based upon sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel in order to increase the bandwidth between primary and secondary memory.

Secondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, while also providing metadata describing the owner of a certain file, the access time, the access permissions, and other information.

Most computer operating systems use the concept of virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to a swap file or page file on secondary storage, retrieving them later when needed. If a lot of pages are moved to slower secondary storage, the system performance is degraded.

"Tertiary storage" or "tertiary memory" is a level below secondary storage. Typically, it involves a robotic mechanism which will "mount" (insert) and "dismount" removable mass storage media into a storage device according to the system's demands; such data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes.

When a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.

Tertiary storage is also known as "nearline storage" because it is "near to online". The formal distinction between online, nearline, and offline storage is:


For example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage.

"Off-line storage" is a computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.

Off-line storage is used to transfer information, since the detached medium can be easily physically transported. Additionally, in case a disaster, for example a fire, destroys the original data, a medium in a remote location will probably be unaffected, enabling disaster recovery. Off-line storage increases general information security, since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.

In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are most popular, and to much lesser extent removable hard disk drives. In enterprise uses, magnetic tape is predominant. Older examples are floppy disks, Zip disks, or punched cards.

Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.

Non-volatile memory retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information. Volatile memory requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory.

Dynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed, otherwise it would vanish. Static random-access memory is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost.

An uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes.





Utilities such as hdparm and sar can be used to measure IO performance in Linux.


Full disk encryption, volume and virtual disk encryption, andor file/folder encryption is readily available for most storage devices.

Hardware memory encryption is available in Intel Architecture, supporting Total Memory Encryption (TME) and page granular memory encryption with multiple keys (MKTME). and in SPARC M7 generation since October 2015.

, the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies, such as all-flash arrays (AFAs) are proposed for development.

Semiconductor memory uses semiconductor-based integrated circuit (IC) chips to store information. Data is typically stored in metal–oxide–semiconductor (MOS) memory cells. A semiconductor memory chip may contain millions of memory cells, consisting of tiny MOS field-effect transistors (MOSFETs) and/or MOS capacitors. Both "volatile" and "non-volatile" forms of semiconductor memory exist, the former using standard MOSFETs and the latter using floating-gate MOSFETs.

In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor random-access memory (RAM), particularly dynamic random-access memory (DRAM). Since the turn of the century, a type of non-volatile floating-gate semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them.

As early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.

Magnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is "non-volatile". The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms:


In early computers, magnetic storage was also used as:


Optical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is "non-volatile". The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are currently in common use:


Magneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is "non-volatile", "sequential access", slow write, fast read storage used for tertiary and off-line storage.

3D optical data storage has also been proposed.

Light induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage.

Paper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole.
A few technologies allow people to make marks on paper that are easily read by machine—these are widely used for tabulating votes and grading standardized tests. Barcodes made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it.









While a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices:


Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in a same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle also recovery from disasters (see disaster recovery above).

A secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.


Large quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. The smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers.

Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.

Robotic storage is used for backups, and for high-capacity archives in imaging, medical, and video industries. Hierarchical storage management is a most known archiving strategy of automatically "migrating" long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are "retrieved" back to disk.






</doc>
<doc id="5302" url="https://en.wikipedia.org/wiki?curid=5302" title="Conditional">
Conditional

Conditional may refer to:






</doc>
<doc id="5304" url="https://en.wikipedia.org/wiki?curid=5304" title="Cone (disambiguation)">
Cone (disambiguation)

A cone is a basic geometrical shape.

Cone may also refer to:















</doc>
<doc id="5306" url="https://en.wikipedia.org/wiki?curid=5306" title="Chemical equilibrium">
Chemical equilibrium

In a chemical reaction, chemical equilibrium is the state in which both reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system. Usually, this state results when the forward reaction proceeds at the same rate as the reverse reaction. The reaction rates of the forward and backward reactions are generally not zero, but equal. Thus, there are no net changes in the concentrations of the reactant(s) and product(s). Such a state is known as dynamic equilibrium.

The concept of chemical equilibrium was developed after Berthollet (1803) found that some chemical reactions are reversible. For any reaction mixture to exist at equilibrium, the rates of the forward and backward (reverse) reactions are equal. In the following chemical equation with arrows pointing both ways to indicate equilibrium, A and B are reactant chemical species, S and T are product species, and "α", "β", "σ", and "τ" are the stoichiometric coefficients of the respective reactants and products:

The equilibrium concentration position of a reaction is said to lie "far to the right" if, at equilibrium, nearly all the reactants are consumed. Conversely the equilibrium position is said to be "far to the left" if hardly any product is formed from the reactants.

Guldberg and Waage (1865), building on Berthollet's ideas, proposed the law of mass action:

where A, B, S and T are active masses and "k" and "k" are rate constants. Since at equilibrium forward and backward rates are equal:

and the ratio of the rate constants is also a constant, now known as an equilibrium constant.

By convention the products form the numerator.
However, the law of mass action is valid only for concerted one-step reactions that proceed through a single transition state and is not valid in general because rate equations do not, in general, follow the stoichiometry of the reaction as Guldberg and Waage had proposed (see, for example, nucleophilic aliphatic substitution by S1 or reaction of hydrogen and bromine to form hydrogen bromide). Equality of forward and backward reaction rates, however, is a necessary condition for chemical equilibrium, though it is not sufficient to explain why equilibrium occurs.

Despite the failure of this derivation, the equilibrium constant for a reaction is indeed a constant, independent of the activities of the various species involved, though it does depend on temperature as observed by the van 't Hoff equation. Adding a catalyst will affect both the forward reaction and the reverse reaction in the same way and will not have an effect on the equilibrium constant. The catalyst will speed up both reactions thereby increasing the speed at which equilibrium is reached.

Although the macroscopic equilibrium concentrations are constant in time, reactions do occur at the molecular level. For example, in the case of acetic acid dissolved in water and forming acetate and hydronium ions,
a proton may hop from one molecule of acetic acid on to a water molecule and then on to an acetate anion to form another molecule of acetic acid and leaving the number of acetic acid molecules unchanged. This is an example of dynamic equilibrium. Equilibria, like the rest of thermodynamics, are statistical phenomena, averages of microscopic behavior.

Le Châtelier's principle (1884) predicts the behavior of an equilibrium system when changes to its reaction conditions occur. "If a dynamic equilibrium is disturbed by changing the conditions, the position of equilibrium moves to partially reverse the change". For example, adding more S from the outside will cause an excess of products, and the system will try to counteract this by increasing the reverse reaction and pushing the equilibrium point backward (though the equilibrium constant will stay the same).

If mineral acid is added to the acetic acid mixture, increasing the concentration of hydronium ion, the amount of dissociation must decrease as the reaction is driven to the left in accordance with this principle. This can also be deduced from the equilibrium constant expression for the reaction:
If {HO} increases {CHCOH} must increase and must decrease. The HO is left out, as it is the solvent and its concentration remains high and nearly constant.

A quantitative version is given by the reaction quotient.

J. W. Gibbs suggested in 1873 that equilibrium is attained when the Gibbs free energy of the system is at its minimum value (assuming the reaction is carried out at constant temperature and pressure). What this means is that the derivative of the Gibbs energy with respect to reaction coordinate (a measure of the extent of reaction that has occurred, ranging from zero for all reactants to a maximum for all products) vanishes, signaling a stationary point. This derivative is called the reaction Gibbs energy (or energy change) and corresponds to the difference between the chemical potentials of reactants and products at the composition of the reaction mixture. This criterion is both necessary and sufficient. If a mixture is not at equilibrium, the liberation of the excess Gibbs energy (or Helmholtz energy at constant volume reactions) is the "driving force" for the composition of the mixture to change until equilibrium is reached. The equilibrium constant can be related to the standard Gibbs free energy change for the reaction by the equation

where R is the universal gas constant and T the temperature.

When the reactants are dissolved in a medium of high ionic strength the quotient of activity coefficients may be taken to be constant. In that case the concentration quotient, "K",
where [A] is the concentration of A, etc., is independent of the analytical concentration of the reactants. For this reason, equilibrium constants for solutions are usually determined in media of high ionic strength. "K" varies with ionic strength, temperature and pressure (or volume). Likewise "K" for gases depends on partial pressure. These constants are easier to measure and encountered in high-school chemistry courses.

At constant temperature and pressure, one must consider the Gibbs free energy, "G", while at constant temperature and volume, one must consider the Helmholtz free energy, "A", for the reaction; and at constant internal energy and volume, one must consider the entropy, "S", for the reaction.

The constant volume case is important in geochemistry and atmospheric chemistry where pressure variations are significant. Note that, if reactants and products were in standard state (completely pure), then there would be no reversibility and no equilibrium. Indeed, they would necessarily occupy disjoint volumes of space. The mixing of the products and reactants contributes a large entropy (known as entropy of mixing) to states containing equal mixture of products and reactants. The standard Gibbs energy change, together with the Gibbs energy of mixing, determine the equilibrium state.

In this article only the constant pressure case is considered. The relation between the Gibbs free energy and the equilibrium constant can be found by considering chemical potentials.

At constant temperature and pressure, the Gibbs free energy, "G", for the reaction depends only on the extent of reaction: "ξ" (Greek letter xi), and can only decrease according to the second law of thermodynamics. It means that the derivative of "G" with "ξ" must be negative if the reaction happens; at the equilibrium the derivative being equal to zero.

In order to meet the thermodynamic condition for equilibrium, the Gibbs energy must be stationary, meaning that the derivative of "G" with respect to the extent of reaction: "ξ", must be zero. It can be shown that in this case, the sum of chemical potentials of the products is equal to the sum of those corresponding to the reactants. Therefore, the sum of the Gibbs energies of the reactants must be the equal to the sum of the Gibbs energies of the products.

where "μ" is in this case a partial molar Gibbs energy, a chemical potential. The chemical potential of a reagent A is a function of the activity, {A} of that reagent.

(where "μ" is the standard chemical potential).

The definition of the Gibbs energy equation interacts with the fundamental thermodynamic relation to produce

Inserting "dN" = "ν dξ" into the above equation gives a Stoichiometric coefficient (formula_11) and a differential that denotes the reaction occurring once ("dξ"). At constant pressure and temperature the above equations can be written as

This results in:

By substituting the chemical potentials:

the relationship becomes:

which is the standard Gibbs energy change for the reaction that can be calculated using thermodynamical tables.
The reaction quotient is defined as:

Therefore,

At equilibrium:

leading to:

and

Obtaining the value of the standard Gibbs energy change, allows the calculation of the equilibrium constant.

For a reactional system at equilibrium: "Q" = "K"; "ξ" = "ξ".



Note that activities and equilibrium constants are dimensionless numbers.

The expression for the equilibrium constant can be rewritten as the product of a concentration quotient, "K" and an activity coefficient quotient, "Γ".

[A] is the concentration of reagent A, etc. It is possible in principle to obtain values of the activity coefficients, γ. For solutions, equations such as the Debye–Hückel equation or extensions such as Davies equation Specific ion interaction theory or Pitzer equations may be used. However this is not always possible. It is common practice to assume that "Γ" is a constant, and to use the concentration quotient in place of the thermodynamic equilibrium constant. It is also general practice to use the term "equilibrium constant" instead of the more accurate "concentration quotient". This practice will be followed here.

For reactions in the gas phase partial pressure is used in place of concentration and fugacity coefficient in place of activity coefficient. In the real world, for example, when making ammonia in industry, fugacity coefficients must be taken into account. Fugacity, "f", is the product of partial pressure and fugacity coefficient. The chemical potential of a species in the gas phase is given by
so the general expression defining an equilibrium constant is valid for both solution and gas phases.

In aqueous solution, equilibrium constants are usually determined in the presence of an "inert" electrolyte such as sodium nitrate NaNO or potassium perchlorate KClO. The ionic strength of a solution is given by
where "c" and "z" stand for the concentration and ionic charge of ion type "i", and the sum is taken over all the "N" types of charged species in solution. When the concentration of dissolved salt is much higher than the analytical concentrations of the reagents, the ions originating from the dissolved salt determine the ionic strength, and the ionic strength is effectively constant. Since activity coefficients depend on ionic strength the activity coefficients of the species are effectively independent of concentration. Thus, the assumption that "Γ" is constant is justified. The concentration quotient is a simple multiple of the equilibrium constant.
However, "K" will vary with ionic strength. If it is measured at a series of different ionic strengths the value can be extrapolated to zero ionic strength. The concentration quotient obtained in this manner is known, paradoxically, as a thermodynamic equilibrium constant.

To use a published value of an equilibrium constant in conditions of ionic strength different from the conditions used in its determination, the value should be adjusted.

A mixture may appear to have no tendency to change, though it is not at equilibrium. For example, a mixture of SO and O is metastable as there is a kinetic barrier to formation of the product, SO.

The barrier can be overcome when a catalyst is also present in the mixture as in the contact process, but the catalyst does not affect the equilibrium concentrations.

Likewise, the formation of bicarbonate from carbon dioxide and water is very slow under normal conditions
but almost instantaneous in the presence of the catalytic enzyme carbonic anhydrase.

When pure substances (liquids or solids) are involved in equilibria their activities do not appear in the equilibrium constant because their numerical values are considered one.

Applying the general formula for an equilibrium constant to the specific case of a dilute solution of acetic acid in water one obtains

For all but very concentrated solutions, the water can be considered a "pure" liquid, and therefore it has an activity of one. The equilibrium constant expression is therefore usually written as

A particular case is the self-ionization of water itself

Because water is the solvent, and has an activity of one, the self-ionization constant of water is defined as

It is perfectly legitimate to write [H] for the hydronium ion concentration, since the state of solvation of the proton is constant (in dilute solutions) and so does not affect the equilibrium concentrations. "K" varies with variation in ionic strength and/or temperature.

The concentrations of H and OH are not independent quantities. Most commonly [OH] is replaced by "K"[H] in equilibrium constant expressions which would otherwise include hydroxide ion.

Solids also do not appear in the equilibrium constant expression, if they are considered to be pure and thus their activities taken to be one. An example is the Boudouard reaction:

for which the equation (without solid carbon) is written as:

Consider the case of a dibasic acid HA. When dissolved in water, the mixture will contain HA, HA and A. This equilibrium can be split into two steps in each of which one proton is liberated.
"K" and" K" are examples of "stepwise" equilibrium constants. The "overall" equilibrium constant, "β", is product of the stepwise constants.
Note that these constants are dissociation constants because the products on the right hand side of the equilibrium expression are dissociation products. In many systems, it is preferable to use association constants.
"β" and "β" are examples of association constants. Clearly and ; and 
For multiple equilibrium systems, also see: theory of Response reactions.

The effect of changing temperature on an equilibrium constant is given by the van 't Hoff equation
Thus, for exothermic reactions (Δ"H" is negative), "K" decreases with an increase in temperature, but, for endothermic reactions, (ΔH is positive) "K" increases with an increase temperature. An alternative formulation is
At first sight this appears to offer a means of obtaining the standard molar enthalpy of the reaction by studying the variation of "K" with temperature. In practice, however, the method is unreliable because error propagation almost always gives very large errors on the values calculated in this way.

The effect of electric field on equilibrium has been studied by Manfred Eigen among others.

Equilibrium can be broadly classified as heterogeneous and homogeneous equilibrium. Homogeneous equilibrium consists of reactants and products belonging in the same phase whereas heterogeneous equilibrium comes into play for reactants and products in different phases. 

In these applications, terms such as stability constant, formation constant, binding constant, affinity constant, association/dissociation constant are used. In biochemistry, it is common to give units for binding constants, which serve to define the concentration units used when the constant's value was determined.

When the only equilibrium is that of the formation of a 1:1 adduct as the composition of a mixture, there are many ways that the composition of a mixture can be calculated. For example, see ICE table for a traditional method of calculating the pH of a solution of a weak acid.

There are three approaches to the general calculation of the composition of a mixture at equilibrium.


In general, the calculations are rather complicated or complex. For instance, in the case of a dibasic acid, HA dissolved in water the two reactants can be specified as the conjugate base, A, and the proton, H. The following equations of mass-balance could apply equally well to a base such as 1,2-diaminoethane, in which case the base itself is designated as the reactant A:

With T the total concentration of species A. Note that it is customary to omit the ionic charges when writing and using these equations.

When the equilibrium constants are known and the total concentrations are specified there are two equations in two unknown "free concentrations" [A] and [H]. This follows from the fact that [HA] = "β"[A][H], [HA] = "β"[A][H] and [OH] = "K"[H]

so the concentrations of the "complexes" are calculated from the free concentrations and the equilibrium constants.
General expressions applicable to all systems with two reagents, A and B would be

It is easy to see how this can be extended to three or more reagents.

The composition of solutions containing reactants A and H is easy to calculate as a function of p[H]. When [H] is known, the free concentration [A] is calculated from the mass-balance equation in A.

The diagram alongside, shows an example of the hydrolysis of the aluminium Lewis acid Al shows the species concentrations for a 5 × 10 M solution of an aluminium salt as a function of pH. Each concentration is shown as a percentage of the total aluminium.

The diagram above illustrates the point that a precipitate that is not one of the main species in the solution equilibrium may be formed. At pH just below 5.5 the main species present in a 5 μM solution of Al are aluminium hydroxides Al(OH), and , but on raising the pH Al(OH) precipitates from the solution. This occurs because Al(OH) has a very large lattice energy. As the pH rises more and more Al(OH) comes out of solution. This is an example of Le Châtelier's principle in action: Increasing the concentration of the hydroxide ion causes more aluminium hydroxide to precipitate, which removes hydroxide from the solution. When the hydroxide concentration becomes sufficiently high the soluble aluminate, , is formed.

Another common instance where precipitation occurs is when a metal cation interacts with an anionic ligand to form an electrically neutral complex. If the complex is hydrophobic, it will precipitate out of water. This occurs with the nickel ion Ni and dimethylglyoxime, (dmgH): in this case the lattice energy of the solid is not particularly large, but it greatly exceeds the energy of solvation of the molecule Ni(dmgH).

At equilibrium, at a specified temperature and pressure, and with no external forces, the Gibbs free energy "G" is at a minimum:

where μ is the chemical potential of molecular species "j", and "N" is the amount of molecular species "j". It may be expressed in terms of thermodynamic activity as:

where formula_51 is the chemical potential in the standard state, "R" is the gas constant "T" is the absolute temperature, and "A" is the activity.

For a closed system, no particles may enter or leave, although they may combine in various ways. The total number of atoms of each element will remain constant. This means that the minimization above must be subjected to the constraints:

where "a" is the number of atoms of element "i" in molecule "j" and "b" is the total number of atoms of element "i", which is a constant, since the system is closed. If there are a total of "k" types of atoms in the system, then there will be "k" such equations. If ions are involved, an additional row is added to the a matrix specifying the respective charge on each molecule which will sum to zero.

This is a standard problem in optimisation, known as constrained minimisation. The most common method of solving it is using the method of Lagrange multipliers (although other methods may be used).

Define:

where the "λ" are the Lagrange multipliers, one for each element. This allows each of the "N" and "λ" to be treated independently, and it can be shown using the tools of multivariate calculus that the equilibrium condition is given by

(For proof see Lagrange multipliers.) This is a set of ("m" + "k") equations in ("m" + "k") unknowns (the "N" and the "λ") and may, therefore, be solved for the equilibrium concentrations "N" as long as the chemical activities are known as functions of the concentrations at the given temperature and pressure. (In the ideal case, activities are proportional to concentrations.) (See Thermodynamic databases for pure substances.) Note that the second equation is just the initial constraints for minimization.

This method of calculating equilibrium chemical concentrations is useful for systems with a large number of different molecules. The use of "k" atomic element conservation equations for the mass constraint is straightforward, and replaces the use of the stoichiometric coefficient equations. The results are consistent with those specified by chemical equations. For example, if equilibrium is specified by a single chemical equation:,

where ν is the stochiometric coefficient for the "j" th molecule (negative for reactants, positive for products) and "R" is the symbol for the "j" th molecule, a properly balanced equation will obey:

Multiplying the first equilibrium condition by ν and using the above equation yields:

As above, defining ΔG

where "K" is the equilibrium constant, and ΔG will be zero at equilibrium.

Analogous procedures exist for the minimization of other thermodynamic potentials.



</doc>
<doc id="5308" url="https://en.wikipedia.org/wiki?curid=5308" title="Combination">
Combination

In mathematics, a combination is a selection of items from a collection, such that (unlike permutations) the order of selection does not matter. For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange.
More formally, a "k"-combination of a set "S" is a subset of "k" distinct elements of "S". If the set has "n" elements, the number of "k"-combinations is equal to the binomial coefficient

which can be written using factorials as formula_2 whenever formula_3, and which is zero when formula_4. The set of all "k"-combinations of a set "S" is often denoted by formula_5.

Combinations refer to the combination of "n" things taken "k" at a time without repetition. To refer to combinations in which repetition is allowed, the terms "k"-selection, "k"-multiset, or "k"-combination with repetition are often used. If, in the above example, it were possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.

Although the set of three fruits was small enough to write a complete list of combinations, with large sets this becomes impractical. For example, a poker hand can be described as a 5-combination ("k" = 5) of cards from a 52 card deck ("n" = 52). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is 1 / 2,598,960.

The number of "k"-combinations from a given set "S" of "n" elements is often denoted in elementary combinatorics texts by formula_6, or by a variation such as formula_7, formula_8, formula_9, formula_10 or even formula_11 (the latter form was standard in French, Romanian, Russian, Chinese and Polish texts). The same number however occurs in many other mathematical contexts, where it is denoted by formula_12 (often read as ""n" choose "k""); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. One can define formula_12 for all natural numbers "k" at once by the relation

from which it is clear that

and further, 

To see that these coefficients count "k"-combinations from "S", one can first consider a collection of "n" distinct variables "X" labeled by the elements "s" of "S", and expand the product over all elements of "S":

it has 2 distinct terms corresponding to all the subsets of "S", each subset giving the product of the corresponding variables "X". Now setting all of the "X" equal to the unlabeled variable "X", so that the product becomes , the term for each "k"-combination from "S" becomes "X", so that the coefficient of that power in the result equals the number of such "k"-combinations.

Binomial coefficients can be computed explicitly in various ways. To get all of them for the expansions up to , one can use (in addition to the basic cases already given) the recursion relation

for 0 < "k" < "n", which follows from =; this leads to the construction of Pascal's triangle.

For determining an individual binomial coefficient, it is more practical to use the formula

The numerator gives the number of "k"-permutations of "n", i.e., of sequences of "k" distinct elements of "S", while the denominator gives the number of such "k"-permutations that give the same "k"-combination when the order is ignored.

When "k" exceeds "n"/2, the above formula contains factors common to the numerator and the denominator, and canceling them out gives the relation

for 0 ≤ "k" ≤ "n". This expresses a symmetry that is evident from the binomial formula, and can also be understood in terms of "k"-combinations by taking the complement of such a combination, which is an -combination.

Finally there is a formula which exhibits this symmetry directly, and has the merit of being easy to remember:

where "n"<nowiki>!</nowiki> denotes the factorial of "n". It is obtained from the previous formula by multiplying denominator and numerator by !, so it is certainly inferior as a method of computation to that formula.

The last formula can be understood directly, by considering the "n"<nowiki>!</nowiki> permutations of all the elements of "S". Each such permutation gives a "k"-combination by selecting its first "k" elements. There are many duplicate selections: any combined permutation of the first "k" elements among each other, and of the final ("n" − "k") elements among each other produces the same combination; this explains the division in the formula.

From the above formulas follow relations between adjacent numbers in Pascal's triangle in all three directions:

Together with the basic cases formula_23, these allow successive computation of respectively all numbers of combinations from the same set (a row in Pascal's triangle), of "k"-combinations of sets of growing sizes, and of combinations with a complement of fixed size .

As a specific example, one can compute the number of five-card hands possible from a standard fifty-two card deck as:

Alternatively one may use the formula in terms of factorials and cancel the factors in the numerator against parts of the factors in the denominator, after which only multiplication of the remaining factors is required:

Another alternative computation, equivalent to the first, is based on writing

which gives

When evaluated in the following order, , this can be computed using only integer arithmetic. The reason is that when each division occurs, the intermediate result that is produced is itself a binomial coefficient, so no remainders ever occur.

Using the symmetric formula in terms of factorials without performing simplifications gives a rather extensive calculation:

One can enumerate all "k"-combinations of a given set "S" of "n" elements in some fixed order, which establishes a bijection from an interval of formula_12 integers with the set of those "k"-combinations. Assuming "S" is itself ordered, for instance "S" = { 1, 2, …, "n" }, there are two natural possibilities for ordering its "k"-combinations: by comparing their smallest elements first (as in the illustrations above) or by comparing their largest elements first. The latter option has the advantage that adding a new largest element to "S" will not change the initial part of the enumeration, but just add the new "k"-combinations of the larger set after the previous ones. Repeating this process, the enumeration can be extended indefinitely with "k"-combinations of ever larger sets. If moreover the intervals of the integers are taken to start at 0, then the "k"-combination at a given place "i" in the enumeration can be computed easily from "i", and the bijection so obtained is known as the combinatorial number system. It is also known as "rank"/"ranking" and "unranking" in computational mathematics.

There are many ways to enumerate "k" combinations. One way is to visit all the binary numbers less than 2. Choose those numbers having "k" nonzero bits, although this is very inefficient even for small "n" (e.g. "n" = 20 would require visiting about one million numbers while the maximum number of allowed "k" combinations is about 186 thousand for "k" = 10). The positions of these 1 bits in such a number is a specific "k"-combination of the set { 1, …, "n" }. Another simple, faster way is to track "k" index numbers of the elements selected, starting with {0 .. "k"−1} (zero-based) or {1 .. "k"} (one-based) as the first allowed "k"-combination and then repeatedly moving to the next allowed "k"-combination by incrementing the last index number if it is lower than "n"-1 (zero-based) or "n" (one-based) or the last index number "x" that is less than the index number following it minus one if such an index exists and resetting the index numbers after "x" to {"x"+1, "x"+2, …}.

A "k"-combination with repetitions, or "k"-multicombination, or multisubset of size "k" from a set "S" is given by a sequence of "k" not necessarily distinct elements of "S", where order is not taken into account: two sequences define the same multiset if one can be obtained from the other by permuting the terms. In other words, the number of ways to sample "k" elements from a set of "n" elements allowing for duplicates (i.e., with replacement) but disregarding different orderings (e.g. {2,1,2} = {1,2,2}). Associate an index to each element of "S" and think of the elements of "S" as "types" of objects, then we can let formula_30 denote the number of elements of type "i" in a multisubset. The number of multisubsets of size "k" is then the number of nonnegative integer solutions of the Diophantine equation:
If "S" has "n" elements, the number of such "k"-multisubsets is denoted by,

a notation that is analogous to the binomial coefficient which counts "k"-subsets. This expression, "n" multichoose "k", can also be given in terms of binomial coefficients:

This relationship can be easily proved using a representation known as stars and bars. 

A solution of the above Diophantine equation can be represented by formula_34 "stars", a separator (a "bar"), then formula_35 more stars, another separator, and so on. The total number of stars in this representation is "k" and the number of bars is "n" - 1 (since no separator is needed at the very end). Thus, a string of "k" + "n" - 1 symbols (stars and bars) corresponds to a solution if there are "k" stars in the string. Any solution can be represented by choosing "k" out of positions to place stars and filling the remaining positions with bars. For example, the solution formula_36 of the equation formula_37 can be represented by
formula_38.
The number of such strings is the number of ways to place 10 stars in 13 positions, formula_39 which is the number of 10-multisubsets of a set with 4 elements.

As with binomial coefficients, there are several relationships between these multichoose expressions. For example, for formula_40,

This identity follows from interchanging the stars and bars in the above representation.

For example, if you have four types of donuts ("n" = 4) on a menu to choose from and you want three donuts ("k" = 3), the number of ways to choose the donuts with repetition can be calculated as

This result can be verified by listing all the 3-multisubsets of the set "S" = {1,2,3,4}. This is displayed in the following table. The second column shows the nonnegative integer solutions formula_43 of the equation formula_44 and the last column gives the stars and bars representation of the solutions.

The number of "k"-combinations for all "k" is the number of subsets of a set of "n" elements. There are several ways to see that this number is 2. In terms of combinations, formula_45, which is the sum of the "n"th row (counting from 0) of the binomial coefficients in Pascal's triangle. These combinations (subsets) are enumerated by the 1 digits of the set of base 2 numbers counting from 0 to 2  −  1, where each digit position is an item from the set of "n".

Given 3 cards numbered 1 to 3, there are 8 distinct combinations (subsets), including the empty set:

Representing these subsets (in the same order) as base 2 numerals:

There are various algorithms to pick out a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a "k"-combination efficiently from a population of size "n" is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of formula_47. (see reservoir sampling).




</doc>
<doc id="5309" url="https://en.wikipedia.org/wiki?curid=5309" title="Software">
Software

Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.

At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or is interrupted by the operating system. , most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.

The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.

An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli Numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.

The first theory about software—prior to the creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay "On Computable Numbers, with an Application to the Entscheidungsproblem" (decision problem).

This eventually led to the creation of the academic fields of computer science and software engineering; Both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering and development of software.

However, prior to 1946, software was not yet the programs stored in the memory of stored-program digital computers, as we now understand it. The first electronic computing devices were instead rewired in order to "reprogram" them.

In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper "The Teaching of Concrete Mathematics" contained the earliest known usage of the term "software" found in a search of JSTOR's electronic archives, predating the OED's citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term "software" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.

On virtually all computer platforms, software can be grouped into a few broad categories.

Based on the goal, computer software can be divided into:



Programming tools are also software in the form of programs or applications that software developers (also known as
"programmers, coders, hackers" or "software engineers") use to create, debug, maintain (i.e. improve or fix), or otherwise support software.

Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.

Users often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.


Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to "execute" the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.

Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using "pointers" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.

Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.

Many bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that "every program has at least one more bug" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together.

The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.

Proprietary software can be divided into two types:


Open-source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software.

Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a "detailed idea (e.g. an algorithm) on how to implement" a piece of software, or a component of a piece of software. Ideas for useful things that software could "do", and user "requirements", are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since "all" useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.

Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over "any" programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.

Design and implementation of software varies depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the latter has much more basic functionality.

Software is usually designed and created (aka coded/written/programmed) in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software (if applicable). As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like "Form1.Close()" and "Form1.Show()" to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.

Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.

Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.

A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as "coder" and "hacker"although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.

A great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the co-founder of Microsoft was the richest person in the world in 2009, largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products - both market leaders in their respective product categories.

Non-profit software organizations include the Free Software Foundation, GNU Project and the Mozilla Foundation. Software standard organizations like the W3C, IETF develop recommended software standards such as XML, HTTP and HTML, so that software can interoperate through these standards.

Other well-known large software companies include Google, IBM, TCS, Infosys, Wipro, HCL Technologies, Oracle, Novell, SAP, Symantec, Adobe Systems, Sidetrade and Corel, while small companies often provide innovation.


</doc>
<doc id="5311" url="https://en.wikipedia.org/wiki?curid=5311" title="Computer programming">
Computer programming

Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result. Programming involves tasks such as; analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

Tasks accompanying and related to programming include; testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term "software development" is used for this larger process with the term "programming", "implementation", or "coding" reserved for the actual writing of code. "Software engineering" combines engineering techniques with software development practices. "Reverse engineering" is the opposite process. A "hacker" is any skilled computer expert that uses their technical knowledge to overcome a problem, but it can also mean a "security hacker" in common language.

Programmable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the "Book of Ingenious Devices". In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the "program" - a series of pasteboard cards with holes punched in them.

Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in "A Manuscript On Deciphering Cryptographic Messages". He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.

The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.
In the 1880's Herman Hollerith invented the concept of storing "data" in machine-readable form. Later a control panel (plugboard) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940's, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way; as were the first electronic computers. However, with the concept of the stored-program computers introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.

Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format, (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.
High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957 and many other languages were soon developed – in particular, COBOL aimed at commercial data processing, and Lisp for computer research.

Programs were mostly still entered using punched cards or paper tape. See computer programming in the punch card era. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors (programs themselves) were developed that allowed changes and corrections to be made much more easily than with punched cards.

Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.

Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.

Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:

The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.

Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.

The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into "orders" using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.

"Programming a Computer for Playing Chess" was a 1950 paper that evaluated a "minimax" algorithm that is part of the history of algorithmic complexity; a course on IBM's Deep Blue (chess computer) is part of the computer science curriculum at Stanford University.

The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.

Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.

A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).

Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.

It is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).

Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).

Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.

After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when passing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be done manually, using a divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.

Debugging is often done with IDEs like Eclipse, Visual Studio, Xcode, Kdevelop, NetBeans and . Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.

Different programming languages support different styles of programming (called "programming paradigms"). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.

Allen Downey, in his book,""How To Think Like A Computer Scientist"", writes:

Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.

Computer programmers are those who write computer software. Their jobs usually involve:





</doc>
<doc id="5312" url="https://en.wikipedia.org/wiki?curid=5312" title="The Consolation of Philosophy">
The Consolation of Philosophy

The Consolation of Philosophy () is a philosophical work by the Roman statesman Boethius, written around the year 524. It has been described as the single most important and influential work in the West on Medieval and early Renaissance Christianity, as well as the last great Western work of the Classical Period.

"The Consolation of Philosophy" was written in AD 523 during a one-year imprisonment Boethius served while awaiting trial – and eventual execution – for the alleged crime of treason under the Ostrogothic King Theodoric the Great. Boethius was at the very heights of power in Rome, holding the prestigious office of "magister officiorum", and was brought down by treachery. This experience inspired the text, which reflects on how evil can exist in a world governed by God (the problem of theodicy), and how happiness is still attainable amidst fickle fortune, while also considering the nature of happiness and God. It has been described as "by far the most interesting example of prison literature the world has ever seen."

Boethius writes the book as a conversation between himself and Lady Philosophy. Lady Philosophy consoles Boethius by discussing the transitory nature of fame and wealth ("no man can ever truly be secure until he has been forsaken by Fortune"), and the ultimate superiority of things of the mind, which she calls the "one true good". She contends that happiness comes from within, and that virtue is all that one truly has, because it is not imperilled by the vicissitudes of fortune.

Boethius engages questions such as the nature of predestination and free will, why evil men often prosper and good men fall into ruin, human nature, virtue, and justice. He speaks about the nature of free will and determinism when he asks if God knows and sees all, or does man have free will. On human nature, Boethius says that humans are essentially good and only when they give in to “wickedness” do they “sink to the level of being an animal.” On justice, he says criminals are not to be abused, rather treated with sympathy and respect, using the analogy of doctor and patient to illustrate the ideal relationship between prosecutor and criminal.

In the "Consolation", Boethius answered religious questions without reference to Christianity, relying solely on natural philosophy and the Classical Greek tradition. He believed in the correspondence between faith and reason. The truths found in Christianity would be no different from the truths found in philosophy. In the words of Henry Chadwick, "If the "Consolation" contains nothing distinctively Christian, it is also relevant that it contains nothing specifically pagan either...[it] is a work written by a Platonist who is also a Christian."

Boethius repeats the Macrobius' model of the Earth in the center of a spherical cosmos.

From the Carolingian epoch to the end of the Middle Ages and beyond it was one of the most popular and influential philosophical works, read by statesmen, poets, and historians, as well as of philosophers and theologians. It is through Boethius that much of the thought of the Classical period was made available to the Western Medieval world. It has often been said Boethius was the “last of the Romans and the first of the Scholastics”.

The philosophical message of the book fits well with the religious piety of the Middle Ages. Readers were encouraged not to seek worldly goods such as money and power, but to seek internalized virtues. Evil had a purpose, to provide a lesson to help change for good; while suffering from evil was seen as virtuous. Because God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness. The Middle Ages, with their vivid sense of an overruling fate, found in Boethius an interpretation of life closely akin to the spirit of Christianity. "The Consolation of Philosophy" stands, by its note of fatalism and its affinities with the Christian doctrine of humility, midway between the pagan philosophy of Seneca the Younger and the later Christian philosophy of consolation represented by Thomas à Kempis.

The book is heavily influenced by Plato and his dialogues (as was Boethius himself). Its popularity can in part be explained by its Neoplatonic and Christian ethical messages, although current scholarly research is still far from clear exactly why and how the work became so vastly popular in the Middle Ages.

Translations into the vernacular were done by famous notables, including King Alfred (Old English), Jean de Meun (Old French), Geoffrey Chaucer (Middle English), Queen Elizabeth I (Early Modern English), and Notker Labeo (Old High German).

Found within the "Consolation" are themes that have echoed throughout the Western canon: the female figure of wisdom that informs Dante, the ascent through the layered universe that is shared with Milton, the reconciliation of opposing forces that find their way into Chaucer in The Knight's Tale, and the Wheel of Fortune so popular throughout the Middle Ages.

Citations from it occur frequently in Dante's "Divina Commedia". Of Boethius, Dante remarked "“The blessed soul who exposes the deceptive world to anyone who gives ear to him.”

Boethian influence can be found nearly everywhere in Geoffrey Chaucer's poetry, e.g. in "Troilus and Criseyde", "The Knight's Tale", "The Clerk's Tale", "The Franklin's Tale", "The Parson's Tale" and "The Tale of Melibee", in the character of Lady Nature in "The Parliament of Fowls" and some of the shorter poems, such as "Truth", "The Former Age" and "Lak of Stedfastnesse". Chaucer translated the work in his "Boece".

The Italian composer Luigi Dallapiccola used some of the text in his choral work "Canti di prigionia" (1938). The Australian composer Peter Sculthorpe quoted parts of it in his opera or music theatre work "Rites of Passage" (1972–73), which was commissioned for the opening of the Sydney Opera House but was not ready in time.

Tom Shippey in "The Road to Middle-earth" says how “Boethian” much of the treatment of evil is in Tolkien's "The Lord of the Rings". Shippey says that Tolkien knew well the translation of Boethius that was made by King Alfred and he quotes some “Boethian” remarks from Frodo, Treebeard and Elrond.

Boethius and "Consolatio Philosophiae" are cited frequently by the main character Ignatius J. Reilly in the Pulitzer Prize-winning "A Confederacy of Dunces" (1980).

It is a prosimetrical text, meaning that it is written in alternating sections of prose and metered verse. In the course of the text, Boethius displays a virtuosic command of the forms of Latin poetry. It is classified as a Menippean satire, a fusion of allegorical tale, platonic dialogue, and lyrical poetry.

In the 20th century there were close to four hundred manuscripts still surviving, a testament to its popularity.

Hundreds of Latin songs were recorded in neumes from the ninth century through to the thirteenth century, including settings of the poetic passages from Boethius's "The Consolation of Philosophy". The music of this song repertory had long been considered irretrievably lost because the notational signs indicated only melodic outlines, relying on now-lapsed oral traditions to fill in the missing details. However, research conducted by Dr Sam Barrett at the University of Cambridge, extended in collaboration with medieval music ensemble Sequentia, has shown that principles of musical setting for this period can be identified, providing crucial information to enable modern realisations. Sequentia performed the world premiere of the reconstructed songs from Boethius's "The Consolation of Philosophy" at Pembroke College, Cambridge, in April 2016, bringing to life music not heard in over 1,000 years; a number of the songs were subsequently recorded on the CD "Boethius: Songs of Consolation. Metra from 11th-Century Canterbury" (Glossa, 2018). A website launched by the University of Cambridge in 2018 provides further details of the reconstruction process, bringing together manuscripts, reconstructions, and video resources.





</doc>
<doc id="5313" url="https://en.wikipedia.org/wiki?curid=5313" title="Crouching Tiger, Hidden Dragon">
Crouching Tiger, Hidden Dragon

Crouching Tiger, Hidden Dragon () is a 2000 "wuxia" film directed by Ang Lee and written by Wang Hui-ling, James Schamus and Tsai Kuo Jung, based on the Chinese novel by Wang Dulu. The film features an international cast of Chinese actors, including Chow Yun-fat, Michelle Yeoh, Zhang Ziyi and Chang Chen.

A multinational venture, the film was made on a US$17 million budget, and was produced by Asian Union Film & Entertainment, China Film Co-Productions Corporation, Columbia Pictures Film Production Asia, Edko Films, Good Machine International, and Zoom Hunt Productions. With dialogue in Mandarin, subtitled for various markets, "Crouching Tiger, Hidden Dragon" became a surprise international success, grossing $213.5 million worldwide. It grossed US$128 million in the United States, becoming the highest-grossing foreign-language film produced overseas in American history.

"Crouching Tiger, Hidden Dragon" has won over 40 awards, and was nominated for 10 Academy Awards, including Best Picture, and won Best Foreign Language Film (Taiwan), Best Art Direction, Best Original Score and Best Cinematography, receiving the most nominations ever for a non-English language film at the time, until 2018's "Roma" tied this record. The film also won four BAFTAs and two Golden Globe Awards, one for Best Foreign Film. Along with its awards success, "Crouching Tiger" continues to be hailed as one of the greatest and most influential films. The film has been praised for its story, direction, and cinematography, and for its martial arts sequences.

In the 18th century Qing dynasty China, Li Mu Bai is an accomplished Wudang swordsman and Yu Shu Lien heads a private security company. Yu Shu Lien and Li Mu Bai have feelings for each other, but because Shu Lien had been engaged to Mu Bai's close friend, Meng Sizhao, before his death, Shu Lien and Mu Bai feel bound by loyalty to Meng Sizhao and have not acted on their feelings for one another. Mu Bai, choosing to retire, asks Shu Lien to give his sword "Green Destiny" to their benefactor Sir Te in Beijing. Long ago, Mu Bai's teacher was killed by Jade Fox, a woman who sought to learn Wudang skills. While at Sir Te's place, Shu Lien makes the acquaintance of Jen Yu, who is the daughter of rich and powerful Governor Yu and is about to get married.

One evening, a masked thief sneaks into Sir Te's estate and steals the Green Destiny. Sir Te's servant Master Bo and Shu Lien trace the theft to Governor Yu's compound, where Jade Fox had been posing as Jen's governess for many years. Soon after, Mu Bai arrives in Beijing and discusses the theft with Shu Lien. Master Bo makes the acquaintance of Inspector Tsai, a police investigator from the provinces, and his daughter May, who have come to Beijing in pursuit of Fox. Fox challenges the pair and Master Bo to a showdown that night. Following a protracted battle, the group is on the verge of defeat when Mu Bai arrives and outmaneuvers Fox. Before Mu Bai can kill Fox, the masked thief reappears and helps Fox. Fox kills Tsai before fleeing with the thief (who is revealed to be Jen). After seeing Jen fight Mu Bai, Fox realizes Jen had been secretly studying the Wudang manual and had surpassed her in combat skills.

At night, a desert bandit named Lo breaks into Jen's bedroom and asks her to leave with him. A flashback reveals that in the past, when Governor Yu and his family were traveling in the western deserts, Lo and his bandits had raided Jen's caravan and Lo had stolen her comb. She pursued him to his desert cave to get her comb back. However, the pair soon fell passionately in love. Lo eventually convinced Jen to return to her family, though not before telling her a legend of a man who jumped off a cliff to make his wishes come true. Because the man's heart was pure, he did not die. Lo came to Beijing to persuade Jen not to go through with her arranged marriage. However, Jen refuses to leave with him. Later, Lo interrupts Jen's wedding procession, begging her to leave with him. Nearby, Shu Lien and Mu Bai convince Lo to wait for Jen at Mount Wudang, where he will be safe from Jen's family, who are furious with him. Jen runs away from her husband on their wedding night before the marriage could be consummated. Disguised in male clothing, she is accosted at an inn by a large group of warriors; armed with the Green Destiny and her own superior combat skills, she emerges victorious.

Jen visits Shu Lien, who tells her that Lo is waiting for her at Mount Wudang. After an angry exchange, the two women engage in a duel. Shu Lien is the superior fighter, but Jen wields the Green Destiny: the sword destroys each weapon that Shu Lien wields, until Shu Lien finally manages to defeat Jen with a broken sword. When Shu Lien shows mercy, Jen wounds Shu Lien in the arm. Mu Bai arrives and pursues Jen into a bamboo forest. Mu Bai confronts Jen and offers to take her as his student. She arrogantly promises to accept him as her teacher if he can take Green Destiny from her in three moves. Mu Bai is able to take the sword in only one move, but Jen goes back on her word to accept him as teacher. Mu Bai throws the sword over a waterfall, Jen dives after it, and is then rescued by Fox. Fox puts Jen into a drugged sleep and places her in a cavern; Mu Bai and Shu Lien discover her there. Fox suddenly reappears and attacks the others with poisoned darts. Mu Bai blocks the needles with his sword and avenges his master's death by mortally wounding Fox, only to realize that one of the darts hit him in the neck. Fox dies, confessing that her goal had been to kill Jen because Jen had hidden the secrets of Wudang's best fighting techniques from her.

As Jen leaves to prepare an antidote for the poisoned dart, Mu Bai prepares to die. With his last breaths, he finally confesses his love for Shu Lien. He dies in her arms as Jen returns, too late to save him. The Green Destiny is returned to Sir Te. Jen later goes to Mount Wudang and spends one last night with Lo. The next morning, Lo finds Jen standing on a bridge overlooking the edge of the mountain. In an echo of the legend that they spoke about in the desert, she asks him to make a wish. He wishes for them to be together again, back in the desert, and Jen jumps off the bridge.

The name "Crouching Tiger Hidden Dragon" is a literal translation of the Chinese idiom "臥虎藏龍" which describes a place or situation that is full of unnoticed masters. It is from a poem of the ancient Chinese poet Yu Xin's (513–581) that reads "暗石疑藏虎，盤根似臥龍", which means "behind the rock in the dark probably hides a tiger, and the coiling giant root resembles a crouching dragon." The last character in Xiaohu and Jiaolong's names mean "Tiger" and "Dragon", respectively.

Some prominent martial arts styles traditionally were held to have been originated by women, e.g. "History of Wing Chun" credits a Buddhist nun Ng Mui teaching Yim Wing-Chun the style. The film's title reference to masters one does not notice necessarily includes mostly women, and suggests the advantage of a female bodyguard. Traditional weapons taught in Wing Chun are exactly those easily concealed (butterfly knives) or similar to those taught in Japanese Aikido primarily to women (long pole). The style emphasizes centerline blows to vital areas, as opposed to maiming strikes against limbs, thus seems intended for death blows, knockouts, and tactics generally used by physically weaker opponents seeking to win in a single strike rather than engage in an extended confrontation. Again, the title suggests powerful creatures that strike by surprise.

A teacher's desire to have a worthy student, the obligations between a student and a master, and tensions in these relationships are central to the characters' motives, conflicts between the characters, and the unfolding of the film's plot. Li Mu Bai is burdened with the responsibility for avenging his master's death, and turns his back on retirement to live up to this obligation. His fascination with the prospect of having Jen as a disciple also motivates his behavior, and that of Jade Fox.

Regarding conflicts in the student-teacher relationship, the potential for exploitation created by the subordinate position of the student and the tensions that exist when a student surpasses or resists a teacher are explored. Jen hides her mastery of martial arts from her teacher, Jade Fox, which leads both to their parting of ways and to Jade Fox's attempt on Jen's life. At the same time, Jade Fox tried to learn Wudang martial arts from Li Mu Bai's master but was refused, even though she tried convincing him by sleeping with him.

Poison is also a significant theme in the film. In the world of martial arts, poison is considered the act of one who is too cowardly and dishonorable to fight; and indeed, the only character who explicitly fits these characteristics is Jade Fox. The poison is a weapon of her bitterness, and quest for vengeance: she poisons the master of Wudang, attempts to poison Jen, and succeeds in killing Mu Bai using a poisoned needle.

However, the poison is not only of the physical sort: Jade Fox's tutelage of Jen has left Jen spiritually poisoned, which can be seen in the lying, stealing, and betrayal Jen commits. However, Jen is never seen to use poison herself, even though she was trained by Jade Fox. This indicates that she may yet be reformed and integrated into society. In further play on this theme by the director, Jade Fox, as she dies, refers to the poison from a young child, "the deceit of an eight-year-old girl," obviously referring to what she considers her own spiritual poisoning by her young apprentice Jen. Li Mu Bai himself warns that without guidance, Jen could become a "poison dragon".

The film was originally written as a novel series by Wang Dulu starting in the late 1930s. The film is loosely adapted from the storyline of the fourth book in the series, "Crouching Tiger, Hidden Dragon".

Although its Academy Award was presented to Taiwan, "Crouching Tiger, Hidden Dragon" was in fact an international co-production between companies in four regions: the Chinese company China Film Co-Production Corporation; the American companies Columbia Pictures Film Production Asia, Sony Pictures Classics, and Good Machine; the Hong Kong company EDKO Film; and the Taiwanese Zoom Hunt International Productions Company, Ltd; as well as the unspecified United China Vision, and Asia Union Film and Entertainment Ltd., created solely for this film.

The film was made in Beijing, with location shooting in the Anhui, Hebei, Jiangsu, and Xinjiang provinces of China. The first phase of shooting was in the Gobi Desert where it consistently rained. Director Ang Lee noted, "I didn't take one break in eight months, not even for half a day. I was miserable -- I just didn't have the extra energy to be happy. Near the end, I could hardly breathe. I thought I was about to have a stroke." The stunt work was mostly performed by the actors themselves and Ang Lee stated in an interview that computers were used "only to remove the safety wires that held the actors." "Most of the time you can see their faces," he added, "That's really them in the trees."

Another compounding issue was the difference between accents of the four lead actors: Chow Yun-fat is from Hong Kong and speaks Cantonese natively; Michelle Yeoh is from Malaysia and grew up speaking English and Malay so she learned the Mandarin lines phonetically; Chang Chen is from Taiwan and he speaks Mandarin in a Taiwanese accent. Only Zhang Ziyi spoke with a native Mandarin accent that Ang Lee wanted. Chow Yun Fat said, on "the first day [of shooting], I had to do 28 takes just because of the language. That's never happened before in my life."

Because the film specifically targeted Western audiences rather than the domestic audiences who were already used to Wuxia films, English subtitles were needed. Ang Lee, who was educated in the West, personally edited the subtitles to ensure they were satisfactory for Western audiences.

The score was composed by Tan Dun, originally performed by Shanghai Symphony Orchestra, Shanghai National Orchestra, and Shanghai Percussion Ensemble. It also features many solo passages for cello played by Yo-Yo Ma. The "last track" ("A Love Before Time") features Coco Lee, who later performed it at the Academy Awards. The music for the entire film was produced in two weeks.

The film was adapted into a video game, a comics series, and a 34-episode Taiwanese television series based on the original novel. The latter was released in 2004 as "New Crouching Tiger, Hidden Dragon" for US and Canadian release.

The film was released on VHS and DVD on June 5, 2001 by Columbia TriStar Home Entertainment.

"Crouching Tiger, Hidden Dragon" was very well received in the Western world, receiving numerous awards. The review aggregator Rotten Tomatoes reported that 97% of critics gave the film positive reviews, based on 153 reviews with an average rating of 8.6/10. The website's critical consensus states: "The movie that catapulted Ang Lee into the ranks of upper echelon Hollywood filmmakers, "Crouching Tiger, Hidden Dragon" features a deft mix of amazing martial arts battles, beautiful scenery, and tasteful drama." Metacritic reported the film had an average score of 93 out of 100, based on 31 reviews, indicating "universal acclaim".

Some Chinese-speaking viewers were bothered by the accents of the leading actors. Neither Chow (a native Cantonese speaker) nor Yeoh (who was born and raised in Malaysia) spoke Mandarin as a mother tongue. All four main actors spoke with different accents: Chow speaks with a Cantonese accent; Yeoh with a Malaysian accent; Chang Chen a Taiwanese accent; and Zhang Ziyi a Beijing accent. Yeoh responded to this complaint in a December 28, 2000, interview with "Cinescape". She argued, "My character lived outside of Beijing, and so I didn't have to do the Beijing accent." When the interviewer, Craig Reid, remarked, "My mother-in-law has this strange Sichuan-Mandarin accent that's hard for me to understand.", Yeoh responded: "Yes, provinces all have their very own strong accents. When we first started the movie, Cheng Pei Pei was going to have her accent, and Chang Zhen was going to have his accent, and this person would have that accent. And in the end nobody could understand what they were saying. Forget about us, even the crew from Beijing thought this was all weird."

The film led to a boost in popularity of Chinese wuxia films in the western world, where they were previously little known, and led to films such as "House of Flying Daggers" and "Hero" marketed towards Western audiences. The film also provided the breakthrough role for Zhang Ziyi's career, who noted:

The character of Lo, or "Dark Cloud" the desert bandit, influenced the development of the protagonist of the "Prince of Persia" series of video games.

The film is ranked at number 497 on "Empire"'s 2008 list of the 500 greatest movies of all time and at number 66 in the magazine's 100 Best Films of World Cinema, published in 2010.

In 2010, the Independent Film & Television Alliance selected the film as one of the 30 Most Significant Independent Films of the last 30 years.

In 2016, it was voted the 35th-best film of the 21st century as picked by 177 film critics from around the world.

"Film Journal" noted that "Crouching Tiger, Hidden Dragon" "pulled off the rare trifecta of critical acclaim, boffo box-office and gestalt shift", in reference to its ground-breaking success for a subtitled film in the American market.

In 2019, "The Guardian" ranked the film 51st in its 100 best films of the 21st century list.

Wu and Chan (2007) look at "Crouching Tiger, Hidden Dragon" as somewhat of an example of "counter-flow", a film that has challenged Hollywood's grip on the film market. They argue that as a product of globalization, the movie did not demonstrate a one-way flow based on Western ideology, but was multidirectional with the ability of local resources to influence the West and gain capital. Despite its international success and perceived ability to change the flow from East to West, however, there were still instances of Western adaptation for the movie, such as putting more emphasis on female characters to better execute a balance between gender roles in the East and West. The script of the film was written between Taiwan and Hollywood and in translating the film to English, many cultural references were lost, which made maintaining the cultural authenticity of the film while still reaching out to the West very difficult. The thematic conflict throughout the movie between societal roles and personal desires attribute to the international reception of the film, which resonates with both the Eastern and Western audiences. Additionally, international networks were used in the production and promotion of the film, which were needed to achieve its global distribution. Additional marketing strategies were needed for the film to attract the Western audience, who were unfamiliar with the cultural products of the East.

The film premiered in cinemas on December 8, 2000, in limited release within the US. During its opening weekend, the film opened in 15th place, grossing $663,205 in business, showing at 16 locations. On January 12, 2001, "Crouching Tiger, Hidden Dragon" premiered in cinemas in wide release throughout the US grossing $8,647,295 in business, ranking in sixth place. The film "Save the Last Dance" came in first place during that weekend, grossing $23,444,930. The film's revenue dropped by almost 30% in its second week of release, earning $6,080,357. For that particular weekend, the film fell to eighth place screening in 837 theaters. "Save the Last Dance" remained unchanged in first place, grossing $15,366,047 in box-office revenue. During its final week in release, "Crouching Tiger, Hidden Dragon" opened in a distant 50th place with $37,233 in revenue. The film went on to top out domestically at $128,078,872 in total ticket sales through a 31-week theatrical run. Internationally, the film took in an additional $85,446,864 in box-office business for a combined worldwide total of $213,525,736. For 2000 as a whole, the film cumulatively ranked at a worldwide box-office performance position of 19.

Gathering widespread critical acclaim at the Toronto and New York film festivals, the film also became a favorite when Academy Awards nominations were announced in 2001. The film was, however, screened out of competition at the 2000 Cannes Film Festival. The film received ten Academy Award nominations, which was the highest ever for a non-English language film, up until it was tied by "Roma" (2018).

A Direct to TV sequel to the film, "", was released in 2016. It was directed by Yuen Woo-ping, who was the action choreographer for the first film. It is a co-production between Pegasus Media, China Film Group Corporation, and the Weinstein Company. Unlike the original film, the sequel was filmed in English for international release and dubbed to Mandarin for Chinese releases.

"Sword of Destiny" is based on the book "Iron Knight, Silver Vase", the next (and last) novel in the Crane-Iron Pentalogy. It features a mostly new cast, headed by Donnie Yen. Michelle Yeoh reprised her role from the original. Zhang Ziyi was also approached to appear in "Sword of Destiny" but refused, stating that she would only appear in a sequel if Ang Lee were directing it.

In the United States, the sequel was for the most part not shown in theaters, instead being distributed via the video streaming service Netflix.

The theme of Janet Jackson's song "China Love" was related to the film by MTV News, in which Jackson sings of the daughter of an emperor in love with a warrior, unable to sustain relations when forced to marry into royalty.

The names of the pterosaur genus "Kryptodrakon" and the ceratopsian genus "Yinlong" (both meaning "hidden dragon" in Greek and Mandarin respectively) allude to the film.




</doc>
<doc id="5314" url="https://en.wikipedia.org/wiki?curid=5314" title="Charlemagne">
Charlemagne

Charlemagne (; ) or Charles the Great (2 April 748 – 28 January 814), numbered Charles I, was King of the Franks from 768, King of the Lombards from 774, and Emperor of the Romans from 800. During the Early Middle Ages, he united the majority of western and central Europe. He was the first recognised emperor to rule from western Europe since the fall of the Western Roman Empire three centuries earlier. The expanded Frankish state that Charlemagne founded is called the Carolingian Empire. He was later canonised by Antipope Paschal III.

Charlemagne was the eldest son of Pepin the Short and Bertrada of Laon, born before their canonical marriage. He became king in 768 following his father's death, initially as co-ruler with his brother Carloman I. Carloman's sudden death in December 771 under unexplained circumstances left Charlemagne the sole ruler of the Frankish Kingdom. He continued his father's policy towards the papacy and became its protector, removing the Lombards from power in northern Italy and leading an incursion into Muslim Spain. He campaigned against the Saxons to his east, Christianising them upon penalty of death and leading to events such as the Massacre of Verden. He reached the height of his power in 800 when he was crowned "Emperor of the Romans" by Pope Leo III on Christmas Day at Old St. Peter's Basilica in Rome.

Charlemagne has been called the "Father of Europe" ("Pater Europae"), as he united most of Western Europe for the first time since the classical era of the Roman Empire and united parts of Europe that had never been under Frankish or Roman rule. His rule spurred the Carolingian Renaissance, a period of energetic cultural and intellectual activity within the Western Church. The Eastern Orthodox Church viewed Charlemagne less favorably due to his support of the filioque and the Pope's having preferred him as Emperor over the Byzantine Empire's first female pretender Irene of Athens. These and other disputes led to the eventual later split of Rome and Constantinople in the Great Schism of 1054.

Charlemagne died in 814 and was laid to rest in Aachen Cathedral in his imperial capital city of Aachen. He married at least four times and had three legitimate sons who lived to adulthood, but only the youngest of them, Louis the Pious, survived to succeed him.

By the 6th century, the western Germanic tribe of the Franks had been Christianised, due in considerable measure to the Catholic conversion of Clovis I. Francia, ruled by the Merovingians, was the most powerful of the kingdoms that succeeded the Western Roman Empire. Following the Battle of Tertry, the Merovingians declined into powerlessness, for which they have been dubbed the "rois fainéants" ("do-nothing kings"). Almost all government powers were exercised by their chief officer, the mayor of the palace.

In 687, Pepin of Herstal, mayor of the palace of Austrasia, ended the strife between various kings and their mayors with his victory at Tertry. He became the sole governor of the entire Frankish kingdom. Pepin was the grandson of two important figures of the Austrasian Kingdom: Saint Arnulf of Metz and Pepin of Landen. Pepin of Herstal was eventually succeeded by his son Charles, later known as Charles Martel (Charles the Hammer).

After 737, Charles governed the Franks in lieu of a king and declined to call himself "king". Charles was succeeded in 741 by his sons Carloman and Pepin the Short, the father of Charlemagne. In 743, the brothers placed Childeric III on the throne to curb separatism in the periphery. He was the last Merovingian king. Carloman resigned office in 746, preferring to enter the church as a monk. Pepin brought the question of the kingship before Pope Zachary, asking whether it was logical for a king to have no royal power. The pope handed down his decision in 749, decreeing that it was better for Pepin to be called king, as he had the powers of high office as Mayor, so as not to confuse the hierarchy. He, therefore, ordered him to become the "true king".<ref name="France/Pippin"></ref>

In 750, Pepin was elected by an assembly of the Franks, anointed by the archbishop, and then raised to the office of king. The Pope branded Childeric III as "the false king" and ordered him into a monastery. The Merovingian dynasty was thereby replaced by the Carolingian dynasty, named after Charles Martel. In 753, Pope Stephen II fled from Italy to Francia, appealing to Pepin for assistance for the rights of St. Peter. He was supported in this appeal by Carloman, Charles' brother. In return, the pope could provide only legitimacy. He did this by again anointing and confirming Pepin, this time adding his young sons Carolus (Charlemagne) and Carloman to the royal patrimony. They thereby became heirs to the realm that already covered most of western Europe. In 754, Pepin accepted the Pope's invitation to visit Italy on behalf of St. Peter's rights, dealing successfully with the Lombards.

Under the Carolingians, the Frankish kingdom spread to encompass an area including most of Western Europe; the east-west division of the kingdom formed the basis for modern France and Germany. Orman portrays the Treaty of Verdun (843) between the warring grandsons of Charlemagne as the foundation event of an independent France under its first king Charles the Bald; an independent Germany under its first king Louis the German; and an independent intermediate state stretching from the Low Countries along the borderlands to south of Rome under Lothair I, who retained the title of emperor and the capitals Aachen and Rome without the jurisdiction. The middle kingdom had broken up by 890 and partly absorbed into the Western kingdom (later France) and the Eastern kingdom (Germany) and the rest developing into smaller "buffer" nations that exist between France and Germany to this day, namely the Benelux and Switzerland. 

The most likely date of Charlemagne's birth is reconstructed from several sources. The date of 742—calculated from Einhard's date of death of January 814 at age 72—predates the marriage of his parents in 744. The year given in the "Annales Petaviani", 747, would be more likely, except that it contradicts Einhard and a few other sources in making Charlemagne sixty-seven years old at his death. The month and day of 2 April are based on a calendar from Lorsch Abbey.

In 747, Easter fell on 2 April, a coincidence that likely would have been remarked upon by chroniclers but was not. If Easter was being used as the beginning of the calendar year, then 2 April 747 could have been, by modern reckoning, April 748 (not on Easter). The date favoured by the preponderance of evidence is 2 April 742, based on Charlemagne's age at the time of his death. This date supports the concept that Charlemagne was technically an illegitimate child, although that is not mentioned by Einhardin either since he was born out of wedlock; Pepin and Bertrada were bound by a private contract or Friedelehe at the time of his birth, but did not marry until 744.

Charlemagne's exact birthplace is unknown, although historians have suggested Aachen in modern-day Germany, and Liège (Herstal) in present-day Belgium as possible locations. Aachen and Liège are close to the region whence the Merovingian and Carolingian families originated. Other cities have been suggested, including Düren, Gauting, Mürlenbach, Quierzy, and Prüm. No definitive evidence resolves the question.

Charlemagne was the eldest child of Pepin the Short (714 – 24 September 768, reigned from 751) and his wife Bertrada of Laon (720 – 12 July 783), daughter of Caribert of Laon. Many historians consider Charlemagne (Charles) to have been illegitimate, although some state that this is arguable, because Pepin did not marry Bertrada until 744, which was after Charles' birth; this status did not exclude him from the succession.

Records name only Carloman, Gisela, and three short-lived children named Pepin, Chrothais and Adelais as his younger siblings.

The most powerful officers of the Frankish people, the Mayor of the Palace ("Maior Domus") and one or more kings ("rex", "reges"), were appointed by the election of the people. Elections were not periodic, but were held as required to elect officers "ad quos summa imperii pertinebat", "to whom the highest matters of state pertained". Evidently, interim decisions could be made by the Pope, which ultimately needed to be ratified using an assembly of the people that met annually.

Before he was elected king in 751, Pepin was initially a mayor, a high office he held "as though hereditary" ("velut hereditario fungebatur"). Einhard explains that "the honour" was usually "given by the people" to the distinguished, but Pepin the Great and his brother Carloman the Wise received it as though hereditary, as had their father, Charles Martel. There was, however, a certain ambiguity about quasi-inheritance. The office was treated as joint property: one Mayorship held by two brothers jointly. Each, however, had his own geographic jurisdiction. When Carloman decided to resign, becoming ultimately a Benedictine at Monte Cassino, the question of the disposition of his quasi-share was settled by the pope. He converted the mayorship into a kingship and awarded the joint property to Pepin, who gained the right to pass it on by inheritance.

This decision was not accepted by all family members. Carloman had consented to the temporary tenancy of his own share, which he intended to pass on to his son, Drogo, when the inheritance should be settled at someone's death. By the Pope's decision, in which Pepin had a hand, Drogo was to be disqualified as an heir in favour of his cousin Charles. He took up arms in opposition to the decision and was joined by Grifo, a half-brother of Pepin and Carloman, who had been given a share by Charles Martel, but was stripped of it and held under loose arrest by his half-brothers after an attempt to seize their shares by military action. Grifo perished in combat in the Battle of Saint-Jean-de-Maurienne while Drogo was hunted down and taken into custody.

On the death of Pepin, 24 September 768, the kingship passed jointly to his sons, "with divine assent" ("divino nutu"). According to the "Life", Pepin died in Paris. The Franks "in general assembly" ("generali conventu") gave them both the rank of a king ("reges") but "partitioned the whole body of the kingdom equally" ("totum regni corpus ex aequo partirentur"). The "annals" tell a slightly different version, with the king dying at St-Denis, near Paris. The two "lords" ("domni") were "elevated to kingship" ("elevati sunt in regnum"), Charles on 9 October in Noyon, Carloman on an unspecified date in Soissons. If born in 742, Charles was 26 years old, but he had been campaigning at his father's right hand for several years, which may help to account for his military skill. Carloman was 17.

The language, in either case, suggests that there were not two inheritances, which would have created distinct kings ruling over distinct kingdoms, but a single joint inheritance and a joint kingship tenanted by two equal kings, Charles and his brother Carloman. As before, distinct jurisdictions were awarded. Charles received Pepin's original share as Mayor: the outer parts of the kingdom bordering on the sea, namely Neustria, western Aquitaine, and the northern parts of Austrasia; while Carloman was awarded his uncle's former share, the inner parts: southern Austrasia, Septimania, eastern Aquitaine, Burgundy, Provence, and Swabia, lands bordering Italy. The question of whether these jurisdictions were joint shares reverting to the other brother if one brother died or were inherited property passed on to the descendants of the brother who died was never definitely settled. It came up repeatedly over the succeeding decades until the grandsons of Charlemagne created distinct sovereign kingdoms.

Aquitaine under Rome had been in southern Gaul, Romanised and speaking a Romance language. Similarly, Hispania had been populated by peoples who spoke various languages, including Celtic, but the area was now populated primarily by Romance language speakers. Between Aquitaine and Hispania were the Euskaldunak, Latinised to Vascones, or Basques, living in Basque country, Vasconia, which extended, according to the distributions of place names attributable to the Basques, most densely in the western Pyrenees but also as far south as the upper Ebro River in Spain and as far north as the Garonne River in France. The French name, Gascony, derives from Vasconia. The Romans were never able to entirely subject Vasconia. The parts they did, in which they placed the region's first cities, were sources of legions in the Roman army valued for their fighting abilities. The border with Aquitaine was Toulouse.

At about 660, the Duchy of Vasconia united with the Duchy of Aquitaine to form a single realm under Felix of Aquitaine, governing from Toulouse. This was a joint kingship with a Basque Duke, Lupus I. "Lupus" is the Latin translation of Basque Otsoa, "wolf". At Felix's death in 670 the joint property of the kingship reverted entirely to Lupus. As the Basques had no law of joint inheritance but practised primogeniture, Lupus in effect founded a hereditary dynasty of Basque rulers of an expanded Aquitaine.

The Latin chronicles of the end of Visigothic Hispania omit many details, such as identification of characters, filling in the gaps and reconciliation of numerous contradictions. Muslim sources, however, present a more coherent view, such as in the "Ta'rikh iftitah al-Andalus" ("History of the Conquest of al-Andalus") by Ibn al-Qūṭiyya ("the son of the Gothic woman", referring to the granddaughter of Wittiza, the last Visigothic king of a united Hispania, who married a Moor). Ibn al-Qūṭiyya, who had another, much longer name, must have been relying to some degree on family oral tradition.

According to Ibn al-Qūṭiyya Wittiza, the last Visigothic king of a united Hispania died before his three sons, Almund, Romulo, and Ardabast reached maturity. Their mother was queen regent at Toledo, but Roderic, army chief of staff, staged a rebellion, capturing Córdoba. He chose to impose a joint rule over distinct jurisdictions on the true heirs. Evidence of a division of some sort can be found in the distribution of coins imprinted with the name of each king and in the king lists. Wittiza was succeeded by Roderic, who reigned for seven and a half years, followed by Achila (Aquila), who reigned three and a half years. If the reigns of both terminated with the incursion of the Saracens, then Roderic appears to have reigned a few years before the majority of Achila. The latter's kingdom is securely placed to the northeast, while Roderic seems to have taken the rest, notably modern Portugal.

The Saracens crossed the mountains to claim Ardo's Septimania, only to encounter the Basque dynasty of Aquitaine, always the allies of the Goths. Odo the Great of Aquitaine was at first victorious at the Battle of Toulouse in 721. Saracen troops gradually massed in Septimania and in 732 an army under Emir Abdul Rahman Al Ghafiqi advanced into Vasconia, and Odo was defeated at the Battle of the River Garonne. They took Bordeaux and were advancing towards Tours when Odo, powerless to stop them, appealed to his arch-enemy, Charles Martel, mayor of the Franks. In one of the first of the lightning marches for which the Carolingian kings became famous, Charles and his army appeared in the path of the Saracens between Tours and Poitiers, and in the Battle of Tours decisively defeated and killed al-Ghafiqi. The Moors returned twice more, each time suffering defeat at Charles' hands—at the River Berre near Narbonne in 737 and in the Dauphiné in 740. Odo's price for salvation from the Saracens was incorporation into the Frankish kingdom, a decision that was repugnant to him and also to his heirs.

After the death of his father, Hunald I allied himself with free Lombardy. However, Odo had ambiguously left the kingdom jointly to his two sons, Hunald and Hatto. The latter, loyal to Francia, now went to war with his brother over full possession. Victorious, Hunald blinded and imprisoned his brother, only to be so stricken by conscience that he resigned and entered the church as a monk to do penance. The story is told in Annales Mettenses priores. His son Waifer took an early inheritance, becoming duke of Aquitaine and ratified the alliance with Lombardy. Waifer decided to honour it, repeating his father's decision, which he justified by arguing that any agreements with Charles Martel became invalid on Martel's death. Since Aquitaine was now Pepin's inheritance because of the earlier assistance that was given by Charles Martel, according to some the latter and his son, the young Charles, hunted down Waifer, who could only conduct a guerrilla war, and executed him.

Among the contingents of the Frankish army were Bavarians under Tassilo III, Duke of Bavaria, an Agilofing, the hereditary Bavarian ducal family. Grifo had installed himself as Duke of Bavaria, but Pepin replaced him with a member of the ducal family yet a child, Tassilo, whose protector he had become after the death of his father. The loyalty of the Agilolfings was perpetually in question, but Pepin exacted numerous oaths of loyalty from Tassilo. However, the latter had married Liutperga, a daughter of Desiderius, king of Lombardy. At a critical point in the campaign, Tassilo left the field with all his Bavarians. Out of reach of Pepin, he repudiated all loyalty to Francia. Pepin had no chance to respond as he grew ill and died within a few weeks after Waifer's execution.

The first event of the brothers' reign was the uprising of the Aquitainians and Gascons, in 769, in that territory split between the two kings. One year earlier, Pepin had finally defeated Waifer, Duke of Aquitaine, after waging a destructive, ten-year war against Aquitaine. Now, Hunald II led the Aquitainians as far north as Angoulême. Charles met Carloman, but Carloman refused to participate and returned to Burgundy. Charles went to war, leading an army to Bordeaux, where he set up a fort at Fronsac. Hunald was forced to flee to the court of Duke Lupus II of Gascony. Lupus, fearing Charles, turned Hunald over in exchange for peace, and Hunald was put in a monastery. Gascon lords also surrendered, and Aquitaine and Gascony were finally fully subdued by the Franks.

The brothers maintained lukewarm relations with the assistance of their mother Bertrada, but in 770 Charles signed a treaty with Duke Tassilo III of Bavaria and married a Lombard Princess (commonly known today as Desiderata), the daughter of King Desiderius, to surround Carloman with his own allies. Though Pope Stephen III first opposed the marriage with the Lombard princess, he found little to fear from a Frankish-Lombard alliance.

Less than a year after his marriage, Charlemagne repudiated Desiderata and married a 13-year-old Swabian named Hildegard. The repudiated Desiderata returned to her father's court at Pavia. Her father's wrath was now aroused, and he would have gladly allied with Carloman to defeat Charles. Before any open hostilities could be declared, however, Carloman died on 5 December 771, apparently of natural causes. Carloman's widow Gerberga fled to Desiderius' court with her sons for protection.

At his succession in 772, Pope Adrian I demanded the return of certain cities in the former exarchate of Ravenna in accordance with a promise at the succession of Desiderius. Instead, Desiderius took over certain papal cities and invaded the Pentapolis, heading for Rome. Adrian sent ambassadors to Charlemagne in autumn requesting he enforce the policies of his father, Pepin. Desiderius sent his own ambassadors denying the pope's charges. The ambassadors met at Thionville, and Charlemagne upheld the pope's side. Charlemagne demanded what the pope had requested, but Desiderius swore never to comply. Charlemagne and his uncle Bernard crossed the Alps in 773 and chased the Lombards back to Pavia, which they then besieged. Charlemagne temporarily left the siege to deal with Adelchis, son of Desiderius, who was raising an army at Verona. The young prince was chased to the Adriatic littoral and fled to Constantinople to plead for assistance from Constantine V, who was waging war with Bulgaria.

The siege lasted until the spring of 774 when Charlemagne visited the pope in Rome. There he confirmed his father's grants of land, with some later chronicles falsely claiming that he also expanded them, granting Tuscany, Emilia, Venice and Corsica. The pope granted him the title "patrician". He then returned to Pavia, where the Lombards were on the verge of surrendering. In return for their lives, the Lombards surrendered and opened the gates in early summer. Desiderius was sent to the abbey of Corbie, and his son Adelchis died in Constantinople, a patrician. Charles, unusually, had himself crowned with the Iron Crown and made the magnates of Lombardy pay homage to him at Pavia. Only Duke Arechis II of Benevento refused to submit and proclaimed independence. Charlemagne was then master of Italy as king of the Lombards. He left Italy with a garrison in Pavia and a few Frankish counts in place the same year.

Instability continued in Italy. In 776, Dukes Hrodgaud of Friuli and Hildeprand of Spoleto rebelled. Charlemagne rushed back from Saxony and defeated the Duke of Friuli in battle; the Duke was slain. The Duke of Spoleto signed a treaty. Their co-conspirator, Arechis, was not subdued, and Adelchis, their candidate in Byzantium, never left that city. Northern Italy was now faithfully his.

In 787, Charlemagne directed his attention towards the Duchy of Benevento, where Arechis II was reigning independently with the self-given title of Princeps. Charlemagne's siege of Salerno forced Arechis into submission. However, after Arechis II's death in 787, his son Grimoald III proclaimed the Duchy of Benevento newly independent. Grimoald was attacked many times by Charles' or his sons' armies, without achieving a definitive victory. Charlemagne lost interest and never again returned to Southern Italy where Grimoald was able to keep the Duchy free from Frankish suzerainty.

During the first peace of any substantial length (780–782), Charles began to appoint his sons to positions of authority. In 781, during a visit to Rome, he made his two youngest sons kings, crowned by the Pope. The elder of these two, Carloman, was made the king of Italy, taking the Iron Crown that his father had first worn in 774, and in the same ceremony was renamed "Pepin" (not to be confused with Charlemagne's eldest, possibly illegitimate son, Pepin the Hunchback). The younger of the two, Louis, became King of Aquitaine. Charlemagne ordered Pepin and Louis to be raised in the customs of their kingdoms, and he gave their regents some control of their subkingdoms, but kept the real power, though he intended his sons to inherit their realms. He did not tolerate insubordination in his sons: in 792, he banished Pepin the Hunchback to the monastery of Prüm, because the young man had joined a rebellion against him.

Charles was determined to have his children educated, including his daughters, as his parents had instilled the importance of learning in him at an early age. His children were also taught skills in accord with their aristocratic status, which included training in riding and weaponry for his sons, and embroidery, spinning and weaving for his daughters.

The sons fought many wars on behalf of their father. Charles was mostly preoccupied with the Bretons, whose border he shared and who insurrected on at least two occasions and were easily put down. He also fought the Saxons on multiple occasions. In 805 and 806, he was sent into the Böhmerwald (modern Bohemia) to deal with the Slavs living there (Bohemian tribes, ancestors of the modern Czechs). He subjected them to Frankish authority and devastated the valley of the Elbe, forcing tribute from them. Pippin had to hold the Avar and Beneventan borders and fought the Slavs to his north. He was uniquely poised to fight the Byzantine Empire when that conflict arose after Charlemagne's imperial coronation and a Venetian rebellion. Finally, Louis was in charge of the Spanish March and fought the Duke of Benevento in southern Italy on at least one occasion. He took Barcelona in a great siege in 797.

Charlemagne kept his daughters at home with him and refused to allow them to contract sacramental marriages (though he originally condoned an engagement between his eldest daughter Rotrude and Constantine VI of Byzantium, this engagement was annulled when Rotrude was 11). Charlemagne's opposition to his daughters' marriages may possibly have intended to prevent the creation of cadet branches of the family to challenge the main line, as had been the case with Tassilo of Bavaria. However, he tolerated their extramarital relationships, even rewarding their common-law husbands and treasuring the illegitimate grandchildren they produced for him. He also, apparently, refused to believe stories of their wild behaviour. After his death the surviving daughters were banished from the court by their brother, the pious Louis, to take up residence in the convents they had been bequeathed by their father. At least one of them, Bertha, had a recognised relationship, if not a marriage, with Angilbert, a member of Charlemagne's court circle.

The destructive war led by Pepin in Aquitaine, although brought to a satisfactory conclusion for the Franks, proved the Frankish power structure south of the Loire was feeble and unreliable. After the defeat and death of Waiofar in 768, while Aquitaine submitted again to the Carolingian dynasty, a new rebellion broke out in 769 led by Hunald II, a possible son of Waifer. He took refuge with the ally Duke Lupus II of Gascony, but probably out of fear of Charlemagne's reprisal, Lupus handed him over to the new King of the Franks to whom he pledged loyalty, which seemed to confirm the peace in the Basque area south of the Garonne.

Wary of new Basque uprisings, Charlemagne seems to have tried to contain Duke Lupus's power by appointing Seguin as the Count of Bordeaux (778) and other counts of Frankish background in bordering areas (, County of Fézensac). The Basque Duke, in turn, seems to have contributed decisively or schemed the Battle of Roncevaux Pass (referred to as "Basque treachery"). The defeat of Charlemagne's army in Roncevaux (778) confirmed his determination to rule directly by establishing the Kingdom of Aquitaine (ruled by Louis the Pious) based on a power base of Frankish officials, distributing lands among colonisers and allocating lands to the Church, which he took as an ally. A Christianisation programme was put in place across the high Pyrenees (778).
The new political arrangement for Vasconia did not sit well with local lords. As of 788 Adalric was fighting and capturing Chorson, Carolingian Count of Toulouse. He was eventually released, but Charlemagne, enraged at the compromise, decided to depose him and appointed his trustee William of Gellone. William, in turn, fought the Basques and defeated them after banishing Adalric (790).

From 781 (Pallars, Ribagorça) to 806 (Pamplona under Frankish influence), taking the County of Toulouse for a power base, Charlemagne asserted Frankish authority over the Pyrenees by subduing the south-western marches of Toulouse (790) and establishing vassal counties on the southern Pyrenees that were to make up the Marca Hispanica. As of 794, a Frankish vassal, the Basque lord Belasko ("al-Galashki", 'the Gaul') ruled Álava, but Pamplona remained under Cordovan and local control up to 806. Belasko and the counties in the Marca Hispánica provided the necessary base to attack the Andalusians (an expedition led by William Count of Toulouse and Louis the Pious to capture Barcelona in 801). Events in the Duchy of Vasconia (rebellion in Pamplona, count overthrown in Aragon, Duke Seguin of Bordeaux deposed, uprising of the Basque lords, etc.) were to prove it ephemeral upon Charlemagne's death.

According to the Muslim historian Ibn al-Athir, the Diet of Paderborn had received the representatives of the Muslim rulers of Zaragoza, Girona, Barcelona and Huesca. Their masters had been cornered in the Iberian peninsula by Abd ar-Rahman I, the Umayyad emir of Cordova. These "Saracen" (Moorish and Muladi) rulers offered their homage to the king of the Franks in return for military support. Seeing an opportunity to extend Christendom and his own power and believing the Saxons to be a fully conquered nation, Charlemagne agreed to go to Spain.

In 778, he led the Neustrian army across the Western Pyrenees, while the Austrasians, Lombards, and Burgundians passed over the Eastern Pyrenees. The armies met at Saragossa and Charlemagne received the homage of the Muslim rulers, Sulayman al-Arabi and Kasmin ibn Yusuf, but the city did not fall for him. Indeed, Charlemagne faced the toughest battle of his career. The Muslims forced him to retreat. He decided to go home since he could not trust the Basques, whom he had subdued by conquering Pamplona. He turned to leave Iberia, but as he was passing through the Pass of Roncesvalles one of the most famous events of his reign occurred. The Basques attacked and destroyed his rearguard and baggage train. The Battle of Roncevaux Pass, though less a battle than a skirmish, left many famous dead, including the seneschal Eggihard, the count of the palace Anselm, and the warden of the Breton March, Roland, inspiring the subsequent creation of the Song of Roland ("La Chanson de Roland").

The conquest of Italy brought Charlemagne in contact with the Saracens who, at the time, controlled the Mediterranean. Charlemagne's eldest son, Pepin the Hunchback, was much occupied with Saracens in Italy. Charlemagne conquered Corsica and Sardinia at an unknown date and in 799 the Balearic Islands. The islands were often attacked by Saracen pirates, but the counts of Genoa and Tuscany (Boniface) controlled them with large fleets until the end of Charlemagne's reign. Charlemagne even had contact with the caliphal court in Baghdad. In 797 (or possibly 801), the caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian elephant named Abul-Abbas and a clock.

In Hispania, the struggle against the Moors continued unabated throughout the latter half of his reign. Louis was in charge of the Spanish border. In 785, his men captured Girona permanently and extended Frankish control into the Catalan littoral for the duration of Charlemagne's reign (the area remained nominally Frankish until the Treaty of Corbeil in 1258). The Muslim chiefs in the northeast of Islamic Spain were constantly rebelling against Cordovan authority, and they often turned to the Franks for help. The Frankish border was slowly extended until 795, when Girona, Cardona, Ausona and Urgell were united into the new Spanish March, within the old duchy of Septimania.

In 797, Barcelona, the greatest city of the region, fell to the Franks when Zeid, its governor, rebelled against Cordova and, failing, handed it to them. The Umayyad authority recaptured it in 799. However, Louis of Aquitaine marched the entire army of his kingdom over the Pyrenees and besieged it for two years, wintering there from 800 to 801, when it capitulated. The Franks continued to press forward against the emir. They took Tarragona in 809 and Tortosa in 811. The last conquest brought them to the mouth of the Ebro and gave them raiding access to Valencia, prompting the Emir al-Hakam I to recognise their conquests in 813.

Charlemagne was engaged in almost constant warfare throughout his reign, often at the head of his elite "scara" bodyguard squadrons. In the Saxon Wars, spanning thirty years and eighteen battles, he conquered Saxonia and proceeded to convert it to Christianity.

The Germanic Saxons were divided into four subgroups in four regions. Nearest to Austrasia was Westphalia and furthest away was Eastphalia. Between them was Engria and north of these three, at the base of the Jutland peninsula, was Nordalbingia.

In his first campaign, in 773, Charlemagne forced the Engrians to submit and cut down an Irminsul pillar near Paderborn. The campaign was cut short by his first expedition to Italy. He returned in 775, marching through Westphalia and conquering the Saxon fort at Sigiburg. He then crossed Engria, where he defeated the Saxons again. Finally, in Eastphalia, he defeated a Saxon force, and its leader Hessi converted to Christianity. Charlemagne returned through Westphalia, leaving encampments at Sigiburg and Eresburg, which had been important Saxon bastions. He then controlled Saxony with the exception of Nordalbingia, but Saxon resistance had not ended.

Following his subjugation of the Dukes of Friuli and Spoleto, Charlemagne returned rapidly to Saxony in 776, where a rebellion had destroyed his fortress at Eresburg. The Saxons were once again defeated, but their main leader, Widukind, escaped to Denmark, his wife's home. Charlemagne built a new camp at Karlstadt. In 777, he called a national diet at Paderborn to integrate Saxony fully into the Frankish kingdom. Many Saxons were baptised as Christians.

In the summer of 779, he again invaded Saxony and reconquered Eastphalia, Engria and Westphalia. At a diet near Lippe, he divided the land into missionary districts and himself assisted in several mass baptisms (780). He then returned to Italy and, for the first time, the Saxons did not immediately revolt. Saxony was peaceful from 780 to 782.

He returned to Saxony in 782 and instituted a code of law and appointed counts, both Saxon and Frank. The laws were draconian on religious issues; for example, the "Capitulatio de partibus Saxoniae" prescribed death to Saxon pagans who refused to convert to Christianity. This led to renewed conflict. That year, in autumn, Widukind returned and led a new revolt. In response, at Verden in Lower Saxony, Charlemagne is recorded as having ordered the execution of 4,500 Saxon prisoners by beheading, known as the Massacre of Verden ("Verdener Blutgericht"). The killings triggered three years of renewed bloody warfare. During this war, the East Frisians between the Lauwers and the Weser joined the Saxons in revolt and were finally subdued. The war ended with Widukind accepting baptism. The Frisians afterwards asked for missionaries to be sent to them and a bishop of their own nation, Ludger, was sent. Charlemagne also promulgated a law code, the "Lex Frisonum", as he did for most subject peoples.

Thereafter, the Saxons maintained the peace for seven years, but in 792 Westphalia again rebelled. The Eastphalians and Nordalbingians joined them in 793, but the insurrection was unpopular and was put down by 794. An Engrian rebellion followed in 796, but the presence of Charlemagne, Christian Saxons and Slavs quickly crushed it. The last insurrection occurred in 804, more than thirty years after Charlemagne's first campaign against them, but also failed. According to Einhard:
By 774, Charlemagne had invaded the Kingdom of Lombardy, and he later annexed the Lombardian territories and assumed its crown, placing the Papal States under Frankish protection. The Duchy of Spoleto south of Rome was acquired in 774, while in the central western parts of Europe, the Duchy of Bavaria was absorbed and the Bavarian policy continued of establishing tributary marches, (borders protected in return for tribute or taxes) among the Slavic Serbs and Czechs. The remaining power confronting the Franks in the east were the Avars. However, Charlemagne acquired other Slavic areas, including Bohemia, Moravia, Austria and Croatia.

In 789, Charlemagne turned to Bavaria. He claimed that Tassilo III, Duke of Bavaria was an unfit ruler, due to his oath-breaking. The charges were exaggerated, but Tassilo was deposed anyway and put in the monastery of Jumièges. In 794, Tassilo was made to renounce any claim to Bavaria for himself and his family (the Agilolfings) at the synod of Frankfurt; he formally handed over to the king all of the rights he had held. Bavaria was subdivided into Frankish counties, as had been done with Saxony.

In 788, the Avars, an Asian nomadic group that had settled down in what is today Hungary (Einhard called them Huns), invaded Friuli and Bavaria. Charlemagne was preoccupied with other matters until 790 when he marched down the Danube and ravaged Avar territory to the Győr. A Lombard army under Pippin then marched into the Drava valley and ravaged Pannonia. The campaigns ended when the Saxons revolted again in 792.

For the next two years, Charlemagne was occupied, along with the Slavs, against the Saxons. Pippin and Duke Eric of Friuli continued, however, to assault the Avars' ring-shaped strongholds. The great Ring of the Avars, their capital fortress, was taken twice. The booty was sent to Charlemagne at his capital, Aachen, and redistributed to his followers and to foreign rulers, including King Offa of Mercia. Soon the Avar tuduns had lost the will to fight and travelled to Aachen to become vassals to Charlemagne and to become Christians. Charlemagne accepted their surrender and sent one native chief, baptised Abraham, back to Avaria with the ancient title of khagan. Abraham kept his people in line, but in 800, the Bulgarians under Khan Krum attacked the remains of the Avar state.

In 803, Charlemagne sent a Bavarian army into Pannonia, defeating and bringing an end to the Avar confederation.

In November of the same year, Charlemagne went to Regensburg where the Avar leaders acknowledged him as their ruler. In 805, the Avar khagan, who had already been baptised, went to Aachen to ask permission to settle with his people south-eastward from Vienna. The Transdanubian territories became integral parts of the Frankish realm, which was abolished by the Magyars in 899–900.

In 789, in recognition of his new pagan neighbours, the Slavs, Charlemagne marched an Austrasian-Saxon army across the Elbe into Obotrite territory. The Slavs ultimately submitted, led by their leader Witzin. Charlemagne then accepted the surrender of the Veleti under Dragovit and demanded many hostages. He also demanded permission to send missionaries into this pagan region unmolested. The army marched to the Baltic before turning around and marching to the Rhine, winning much booty with no harassment. The tributary Slavs became loyal allies. In 795, when the Saxons broke the peace, the Abotrites and Veleti rebelled with their new ruler against the Saxons. Witzin died in battle and Charlemagne avenged him by harrying the Eastphalians on the Elbe. Thrasuco, his successor, led his men to conquest over the Nordalbingians and handed their leaders over to Charlemagne, who honoured him. The Abotrites remained loyal until Charles' death and fought later against the Danes.

When Charlemagne incorporated much of Central Europe, he brought the Frankish state face to face with the Avars and Slavs in the southeast. The most southeast Frankish neighbours were Croats, who settled in Pannonian Croatia and Dalmatian Croatia. While fighting the Avars, the Franks had called for their support. During the 790s, he won a major victory over them in 796. Pannonian Croat Duke Vojnomir of Pannonian Croatia aided Charlemagne, and the Franks made themselves overlords over the Croats of northern Dalmatia, Slavonia and Pannonia.

The Frankish commander Eric of Friuli wanted to extend his dominion by conquering the Littoral Croat Duchy. During that time, Dalmatian Croatia was ruled by Duke Višeslav of Croatia. In the Battle of Trsat, the forces of Eric fled their positions and were routed by the forces of Višeslav. Eric was among those killed which was a great blow for the Carolingian Empire.

Charlemagne also directed his attention to the Slavs to the west of the Avar khaganate: the Carantanians and Carniolans. These people were subdued by the Lombards and Bavarii and made tributaries, but were never fully incorporated into the Frankish state.

In 799, Pope Leo III had been assaulted by some of the Romans, who tried to put out his eyes and tear out his tongue. Leo escaped and fled to Charlemagne at Paderborn. Charlemagne, advised by scholar Alcuin, travelled to Rome, in November 800 and held a synod. On 23 December, Leo swore an oath of innocence to Charlemagne. His position having thereby been weakened, the Pope sought to restore his status. Two days later, at Mass, on Christmas Day (25 December), when Charlemagne knelt at the altar to pray, the Pope crowned him "Imperator Romanorum" ("Emperor of the Romans") in Saint Peter's Basilica. In so doing, the Pope rejected the legitimacy of Empress Irene of Constantinople:

Charlemagne's coronation as Emperor, though intended to represent the continuation of the unbroken line of Emperors from Augustus to Constantine VI, had the effect of setting up two separate (and often opposing) Empires and two separate claims to imperial authority. It led to war in 802, and for centuries to come, the Emperors of both West and East would make competing claims of sovereignty over the whole.

Einhard says that Charlemagne was ignorant of the Pope's intent and did not want any such coronation:
A number of modern scholars, however, suggest that Charlemagne was indeed aware of the coronation; certainly, he cannot have missed the bejewelled crown waiting on the altar when he came to pray – something even contemporary sources support.

Historians have debated for centuries whether Charlemagne was aware before the coronation of the Pope's intention to crown him Emperor (Charlemagne declared that he would not have entered Saint Peter's had he known, according to chapter twenty-eight of Einhard's "Vita Karoli Magni"), but that debate obscured the more significant question of "why" the Pope granted the title and why Charlemagne accepted it.

Collins points out "[t]hat the motivation behind the acceptance of the imperial title was a romantic and antiquarian interest in reviving the Roman empire is highly unlikely." For one thing, such romance would not have appealed either to Franks or Roman Catholics at the turn of the ninth century, both of whom viewed the Classical heritage of the Roman Empire with distrust. The Franks took pride in having "fought against and thrown from their shoulders the heavy yoke of the Romans" and "from the knowledge gained in baptism, clothed in gold and precious stones the bodies of the holy martyrs whom the Romans had killed by fire, by the sword and by wild animals", as Pepin III described it in a law of 763 or 764.

Furthermore, the new title—carrying with it the risk that the new emperor would "make drastic changes to the traditional styles and procedures of government" or "concentrate his attentions on Italy or on Mediterranean concerns more generally"—risked alienating the Frankish leadership.

For both the Pope and Charlemagne, the Roman Empire remained a significant power in European politics at this time. The Byzantine Empire, based in Constantinople, continued to hold a substantial portion of Italy, with borders not far south of Rome. Charles' sitting in judgment of the Pope could be seen as usurping the prerogatives of the Emperor in Constantinople:

For the Pope, then, there was "no living Emperor at that time" though Henri Pirenne disputes this saying that the coronation "was not in any sense explained by the fact that at this moment a woman was reigning in Constantinople". Nonetheless, the Pope took the extraordinary step of creating one. The papacy had since 727 been in conflict with Irene's predecessors in Constantinople over a number of issues, chiefly the continued Byzantine adherence to the doctrine of iconoclasm, the destruction of Christian images; while from 750, the secular power of the Byzantine Empire in central Italy had been nullified.

By bestowing the Imperial crown upon Charlemagne, the Pope arrogated to himself "the right to appoint ... the Emperor of the Romans, ... establishing the imperial crown as his own personal gift but simultaneously granting himself implicit superiority over the Emperor whom he had created." And "because the Byzantines had proved so unsatisfactory from every point of view—political, military and doctrinal—he would select a westerner: the one man who by his wisdom and statesmanship and the vastness of his dominions ... stood out head and shoulders above his contemporaries."

With Charlemagne's coronation, therefore, "the Roman Empire remained, so far as either of them [Charlemagne and Leo] were concerned, one and indivisible, with Charles as its Emperor", though there can have been "little doubt that the coronation, with all that it implied, would be furiously contested in Constantinople".

Alcuin writes hopefully in his letters of an "Imperium Christianum" ("Christian Empire"), wherein, "just as the inhabitants of the [Roman Empire] had been united by a common Roman citizenship", presumably this new empire would be united by a common Christian faith. This writes the view of Pirenne when he says "Charles was the Emperor of the "ecclesia" as the Pope conceived it, of the Roman Church, regarded as the universal Church". The "Imperium Christianum" was further supported at a number of synods all across Europe by Paulinus of Aquileia.

What is known, from the Byzantine chronicler Theophanes, is that Charlemagne's reaction to his coronation was to take the initial steps towards securing the Constantinopolitan throne by sending envoys of marriage to Irene, and that Irene reacted somewhat favourably to them.

It is important to distinguish between the universalist and localist conceptions of the empire, which remain controversial among historians. According to the former, the empire was a universal monarchy, a "commonwealth of the whole world, whose sublime unity transcended every minor distinction"; and the emperor "was entitled to the obedience of Christendom". According to the latter, the emperor had no ambition for universal dominion; his realm was limited in the same way as that of every other ruler, and when he made more far-reaching claims his object was normally to ward off the attacks either of the Pope or of the Byzantine emperor. According to this view, also, the origin of the empire is to be explained by specific local circumstances rather than by overarching theories.

According to Ohnsorge, for a long time, it had been the custom of Byzantium to designate the German princes as spiritual "sons" of the Romans. What might have been acceptable in the fifth century had become provoking and insulting to the Franks in the eighth century. Charles came to believe that the Roman emperor, who claimed to head the world hierarchy of states, was, in reality, no greater than Charles himself, a king as other kings, since beginning in 629 he had entitled himself "Basileus" (translated literally as "king"). Ohnsorge finds it significant that the chief wax seal of Charles, which bore only the inscription: "Christe, protege Carolum regem Francorum [Christ, protect Charles, king of the Franks], was used from 772 to 813, even during the imperial period and was not replaced by a special imperial seal; indicating that Charles felt himself to be just the king of the Franks. Finally, Ohnsorge points out that in the spring of 813 at Aachen Charles crowned his only surviving son, Louis, as the emperor without recourse to Rome with only the acclamation of his Franks. The form in which this acclamation was offered was Frankish-Christian rather than Roman. This implies both independence from Rome and a Frankish (non-Roman) understanding of empire.

Charlemagne used these circumstances to claim that he was the "renewer of the Roman Empire", which had declined under the Byzantines. In his official charters, Charles preferred the style "Karolus serenissimus Augustus a Deo coronatus magnus pacificus imperator Romanum gubernans imperium" ("Charles, most serene Augustus crowned by God, the great, peaceful emperor ruling the Roman empire") to the more direct "Imperator Romanorum" ("Emperor of the Romans").

The title of Emperor remained in the Carolingian family for years to come, but divisions of territory and in-fighting over supremacy of the Frankish state weakened its significance. The papacy itself never forgot the title nor abandoned the right to bestow it. When the family of Charles ceased to produce worthy heirs, the Pope gladly crowned whichever Italian magnate could best protect him from his local enemies. The empire would remain in continuous existence for over a millennium, as the Holy Roman Empire, a true imperial successor to Charles.

The iconoclasm of the Byzantine Isaurian Dynasty was endorsed by the Franks. The Second Council of Nicaea reintroduced the veneration of icons under Empress Irene. The council was not recognised by Charlemagne since no Frankish emissaries had been invited, even though Charlemagne ruled more than three provinces of the classical Roman empire and was considered equal in rank to the Byzantine emperor. And while the Pope supported the reintroduction of the iconic veneration, he politically digressed from Byzantium. He certainly desired to increase the influence of the papacy, to honour his saviour Charlemagne, and to solve the constitutional issues then most troubling to European jurists in an era when Rome was not in the hands of an emperor. Thus, Charlemagne's assumption of the imperial title was not a usurpation in the eyes of the Franks or Italians. It was, however, seen as such in Byzantium, where it was protested by Irene and her successor Nikephoros I—neither of whom had any great effect in enforcing their protests.

The East Romans, however, still held several territories in Italy: Venice (what was left of the Exarchate of Ravenna), Reggio (in Calabria), Otranto (in Apulia), and Naples (the "Ducatus Neapolitanus"). These regions remained outside of Frankish hands until 804, when the Venetians, torn by infighting, transferred their allegiance to the Iron Crown of Pippin, Charles' son. The "Pax Nicephori" ended. Nicephorus ravaged the coasts with a fleet, initiating the only instance of war between the Byzantines and the Franks. The conflict lasted until 810 when the pro-Byzantine party in Venice gave their city back to the Byzantine Emperor, and the two emperors of Europe made peace: Charlemagne received the Istrian peninsula and in 812 the emperor Michael I Rangabe recognised his status as Emperor, although not necessarily as "Emperor of the Romans".

After the conquest of Nordalbingia, the Frankish frontier was brought into contact with Scandinavia. The pagan Danes, "a race almost unknown to his ancestors, but destined to be only too well known to his sons" as Charles Oman described them, inhabiting the Jutland peninsula, had heard many stories from Widukind and his allies who had taken refuge with them about the dangers of the Franks and the fury which their Christian king could direct against pagan neighbours.

In 808, the king of the Danes, Godfred, expanded the vast Danevirke across the isthmus of Schleswig. This defence, last employed in the Danish-Prussian War of 1864, was at its beginning a long earthenwork rampart. The Danevirke protected Danish land and gave Godfred the opportunity to harass Frisia and Flanders with pirate raids. He also subdued the Frank-allied Veleti and fought the Abotrites.

Godfred invaded Frisia, joked of visiting Aachen, but was murdered before he could do any more, either by a Frankish assassin or by one of his own men. Godfred was succeeded by his nephew Hemming, who concluded the Treaty of Heiligen with Charlemagne in late 811.

In 813, Charlemagne called Louis the Pious, king of Aquitaine, his only surviving legitimate son, to his court. There Charlemagne crowned his son as co-emperor and sent him back to Aquitaine. He then spent the autumn hunting before returning to Aachen on 1 November. In January, he fell ill with pleurisy. In deep depression (mostly because many of his plans were not yet realised), he took to his bed on 21 January and as Einhard tells it:

He was buried that same day, in Aachen Cathedral, although the cold weather and the nature of his illness made such a hurried burial unnecessary. The earliest surviving "planctus", the "Planctus de obitu Karoli", was composed by a monk of Bobbio, which he had patronised. A later story, told by Otho of Lomello, Count of the Palace at Aachen in the time of Emperor Otto III, would claim that he and Otto had discovered Charlemagne's tomb: Charlemagne, they claimed, was seated upon a throne, wearing a crown and holding a sceptre, his flesh almost entirely incorrupt. In 1165, Emperor Frederick I re-opened the tomb again and placed the emperor in a sarcophagus beneath the floor of the cathedral. In 1215 Emperor Frederick II re-interred him in a casket made of gold and silver.

Charlemagne's death emotionally affected many of his subjects, particularly those of the literary clique who had surrounded him at Aachen. An anonymous monk of Bobbio lamented:

Louis succeeded him as Charles had intended. He left a testament allocating his assets in 811 that was not updated prior to his death. He left most of his wealth to the Church, to be used for charity. His empire lasted only another generation in its entirety; its division, according to custom, between Louis's own sons after their father's death laid the foundation for the modern states of Germany and France.

The Carolingian king exercised the "bannum", the right to rule and command. Under the Franks, it was a royal prerogative but could be delegated. He had supreme jurisdiction in judicial matters, made legislation, led the army, and protected both the Church and the poor. His administration was an attempt to organise the kingdom, church and nobility around him. As an administrator, Charlemagne stands out for his many reforms: monetary, governmental, military, cultural and ecclesiastical. He is the main protagonist of the "Carolingian Renaissance".

Charlemagne's success rested primarily on novel siege technologies and excellent logistics rather than the long-claimed "cavalry revolution" led by Charles Martel in 730s. However, the stirrup, which made the "shock cavalry" lance charge possible, was not introduced to the Frankish kingdom until the late eighth century.

Horses were used extensively by the Frankish military because they provided a quick, long-distance method of transporting troops, which was critical to building and maintaining the large empire.

Charlemagne had an important role in determining Europe's immediate economic future. Pursuing his father's reforms, Charlemagne abolished the monetary system based on the gold . Instead, he and the Anglo-Saxon King Offa of Mercia took up Pippin's system for pragmatic reasons, notably a shortage of the metal.

The gold shortage was a direct consequence of the conclusion of peace with Byzantium, which resulted in ceding Venice and Sicily to the East and losing their trade routes to Africa. The resulting standardisation economically harmonised and unified the complex array of currencies that had been in use at the commencement of his reign, thus simplifying trade and commerce.
Charlemagne established a new standard, the (from the Latin , the modern pound), which was based upon a pound of silver—a unit of both money and weight—worth 20 sous (from the Latin [which was primarily an accounting device and never actually minted], the modern shilling) or 240 (from the Latin , the modern penny). During this period, the and the were counting units; only the was a coin of the realm.

Charlemagne instituted principles for accounting practice by means of the Capitulare de villis of 802, which laid down strict rules for the way in which incomes and expenses were to be recorded.

Charlemagne applied this system to much of the European continent, and Offa's standard was voluntarily adopted by much of England. After Charlemagne's death, continental coinage degraded, and most of Europe resorted to using the continued high-quality English coin until about 1100.

Early in Charlemagne's rule he tacitly allowed Jews to monopolise money lending. At the time, lending of money for interest was proscribed in 814 because it violated Church law. Charlemagne introduced the "Capitulary for the Jews", a prohibition on Jews engaging in money-lending due to the religious convictions of the majority of his constituents, in essence banning it across the board, a reversal of his earlier recorded general policy. In addition to this broad change, Charlemagne also performed a significant number of microeconomic reforms, such as direct control of prices and levies on certain goods and commodities.

His "Capitulary for the Jews", however, was not representative of his overall economic relationship or attitude towards the Frankish Jews, and certainly not his earlier relationship with them, which evolved over his life. His personal physician, for example, was Jewish, and he employed one Jew, Isaac, who was his personal representative to the Muslim caliphate of Baghdad. Letters have been credited to him that invited Jews to settle in his kingdom.

Part of Charlemagne's success as a warrior, an administrator and ruler can be traced to his admiration for learning and education. His reign is often referred to as the Carolingian Renaissance because of the flowering of scholarship, literature, art and architecture that characterise it. Charlemagne came into contact with the culture and learning of other countries (especially Moorish Spain, Anglo-Saxon England, and Lombard Italy) due to his vast conquests. He greatly increased the provision of monastic schools and scriptoria (centres for book-copying) in Francia.

Charlemagne was a lover of books, sometimes having them read to him during meals. He was thought to enjoy the works of Augustine of Hippo. His court played a key role in producing books that taught elementary Latin and different aspects of the church. It also played a part in creating a royal library that contained in-depth works on language and Christian faith.

Charlemagne encouraged clerics to translate Christian creeds and prayers into their respective vernaculars as well to teach grammar and music. Due to the increased interest of intellectual pursuits and the urging of their king, the monks accomplished so much copying that almost every manuscript from that time was preserved. At the same time, at the urging of their king, scholars were producing more secular books on many subjects, including history, poetry, art, music, law, theology, etc. Due to the increased number of titles, private libraries flourished. These were mainly supported by aristocrats and churchmen who could afford to sustain them. At Charlemagne's court, a library was founded and a number of copies of books were produced, to be distributed by Charlemagne. Book production was completed slowly by hand and took place mainly in large monastic libraries. Books were so in demand during Charlemagne's time that these libraries lent out some books, but only if that borrower offered valuable collateral in return.
Most of the surviving works of classical Latin were copied and preserved by Carolingian scholars. Indeed, the earliest manuscripts available for many ancient texts are Carolingian. It is almost certain that a text which survived to the Carolingian age survives still.

The pan-European nature of Charlemagne's influence is indicated by the origins of many of the men who worked for him: Alcuin, an Anglo-Saxon from York; Theodulf, a Visigoth, probably from Septimania; Paul the Deacon, Lombard; Italians Peter of Pisa and Paulinus of Aquileia; and Franks Angilbert, Angilram, Einhard and Waldo of Reichenau.

Charlemagne promoted the liberal arts at court, ordering that his children and grandchildren be well-educated, and even studying himself (in a time when even leaders who promoted education did not take time to learn themselves) under the tutelage of Peter of Pisa, from whom he learned grammar; Alcuin, with whom he studied rhetoric, dialectic (logic), and astronomy (he was particularly interested in the movements of the stars); and Einhard, who tutored him in arithmetic.

His great scholarly failure, as Einhard relates, was his inability to write: when in his old age he attempted to learn—practising the formation of letters in his bed during his free time on books and wax tablets he hid under his pillow—"his effort came too late in life and achieved little success", and his ability to read—which Einhard is silent about, and which no contemporary source supports—has also been called into question.

In 800, Charlemagne enlarged the hostel at the Muristan in Jerusalem and added a library to it. He certainly had not been personally in Jerusalem.

Charlemagne expanded the reform Church's programme unlike his father, Pippin, and uncle, Carloman. The deepening of the spiritual life was later to be seen as central to public policy and royal governance. His reform focused on strengthening the church's power structure, improving clergy's skill and moral quality, standardising liturgical practices, improvements on the basic tenets of the faith and the rooting out of paganism. His authority extended over church and state. He could discipline clerics, control ecclesiastical property and define orthodox doctrine. Despite the harsh legislation and sudden change, he had developed support from clergy who approved his desire to deepen the piety and morals of his subjects.

In 809–810, Charlemagne called a church council in Aachen, which confirmed the unanimous belief in the West that the Holy Spirit proceeds from the Father and the Son ("ex Patre Filioque") and sanctioned inclusion in the Nicene Creed of the phrase "Filioque" (and the Son). For this Charlemagne sought the approval of Pope Leo III. The Pope, while affirming the doctrine and approving its use in teaching, opposed its inclusion in the text of the Creed as adopted in the 381 First Council of Constantinople. This spoke of the procession of the Holy Spirit from the Father, without adding phrases such as "and the Son", "through the Son", or "alone". Stressing his opposition, the Pope had the original text inscribed in Greek and Latin on two heavy shields that were displayed in Saint Peter's Basilica.

During Charles' reign, the Roman half uncial script and its cursive version, which had given rise to various continental minuscule scripts, were combined with features from the insular scripts in use in Irish and English monasteries. Carolingian minuscule was created partly under the patronage of Charlemagne. Alcuin, who ran the palace school and scriptorium at Aachen, was probably a chief influence.

The revolutionary character of the Carolingian reform, however, can be over-emphasised; efforts at taming Merovingian and Germanic influence had been underway before Alcuin arrived at Aachen. The new minuscule was disseminated first from Aachen and later from the influential scriptorium at Tours, where Alcuin retired as an abbot.

Charlemagne engaged in many reforms of Frankish governance while continuing many traditional practices, such as the division of the kingdom among sons.

In 806, Charlemagne first made provision for the traditional division of the empire on his death. For Charles the Younger he designated Austrasia and Neustria, Saxony, Burgundy and Thuringia. To Pippin, he gave Italy, Bavaria, and Swabia. Louis received Aquitaine, the Spanish March and Provence. The imperial title was not mentioned, which led to the suggestion that, at that particular time, Charlemagne regarded the title as an honorary achievement that held no hereditary significance.

Pepin died in 810 and Charles in 811. Charlemagne then reconsidered the matter, and in 813, crowned his youngest son, Louis, co-emperor and co-King of the Franks, granting him a half-share of the empire and the rest upon Charlemagne's own death. The only part of the Empire that Louis was not promised was Italy, which Charlemagne specifically bestowed upon Pippin's illegitimate son Bernard.

Einhard tells in his twenty-fourth chapter: Charlemagne threw grand banquets and feasts for special occasions such as religious holidays and four of his weddings. When he was not working, he loved Christian books, horseback riding, swimming, bathing in natural hot springs with his friends and family, and hunting. Franks were well known for horsemanship and hunting skills. Charles was a light sleeper and would stay in his bed chambers for entire days at a time due to restless nights. During these days, he would not get out of bed when a quarrel occurred in his kingdom, instead summoning all members of the situation into his bedroom to be given orders. Einhard tells again in the twenty-fourth chapter: "In summer after the midday meal, he would eat some fruit, drain a single cup, put off his clothes and shoes, just as he did for the night, and rest for two or three hours. He was in the habit of awaking and rising from bed four or five times during the night."

Charlemagne probably spoke a Rhenish Franconian dialect.

He also spoke Latin and had at least some understanding of Greek, according to Einhard ("Grecam vero melius intellegere quam pronuntiare poterat", "he could understand Greek better than he could speak it").

The largely fictional account of Charlemagne's Iberian campaigns by Pseudo-Turpin, written some three centuries after his death, gave rise to the legend that the king also spoke Arabic.

Charlemagne's personal appearance is known from a good description by Einhard after his death in the biography "Vita Karoli Magni". Einhard states:

The physical portrait provided by Einhard is confirmed by contemporary depictions such as coins and his bronze statuette kept in the Louvre. In 1861, Charlemagne's tomb was opened by scientists who reconstructed his skeleton and estimated it to be measured . An estimate of his height from an X-ray and CT scan of his tibia performed in 2010 is . This puts him in the 99th percentile of height for his period, given that average male height of his time was . The width of the bone suggested he was gracile in body build.

Charlemagne wore the traditional costume of the Frankish people, described by Einhard thus:

He wore a blue cloak and always carried a sword typically of a golden or silver hilt. He wore intricately jeweled swords to banquets or ambassadorial receptions. Nevertheless:
On great feast days, he wore embroidery and jewels on his clothing and shoes. He had a golden buckle for his cloak on such occasions and would appear with his great diadem, but he despised such apparel according to Einhard, and usually dressed like the common people.

Charlemagne had residences across his kingdom, including numerous private estates that were governed in accordance with the Capitulare de villis. A 9th-century document detailing the inventory of an estate at Asnapium listed amounts of livestock, plants and vegetables and kitchenware including cauldrons, drinking cups, brass kettles and firewood. The manor contained seventeen houses built inside the courtyard for nobles and family members and was separated from its supporting villas.

Charlemagne had eighteen children with eight of his ten known wives or concubines. Nonetheless, he had only four legitimate grandsons, the four sons of his fourth son, Louis. In addition, he had a grandson (Bernard of Italy, the only son of his third son, Pepin of Italy), who was illegitimate but included in the line of inheritance. Among his descendants are several royal dynasties, including the Habsburg, Capetian and Plantagenet dynasties. By consequence, most if not all established European noble families ever since can genealogically trace their background to Charlemagne.

He was named "Charles" in French and English, "Carolus" in Latin, after his grandfather, Charles Martel. Later Old French historians dubbed him "Charles le Magne" (Charles the Great), becoming Charlemagne in English after the Norman conquest of England. The epithet Carolus Magnus was widely used, leading to numerous translations into many languages of Europe.

Charles' achievements gave a new meaning to his name. In many languages of Europe, the very word for "king" derives from his name; e.g., , , , , , , , , , , , , . This development parallels that of the name of the Caesars in the original Roman Empire, which became "kaiser" and "tsar" (or "czar"), among others.

Charlemagne was revered as a saint in the Holy Roman Empire and some other locations after the twelfth century. The Apostolic See did not recognise his invalid canonisation by Antipope Paschal III, done to gain the favour of Frederick Barbarossa in 1165. The Apostolic See annulled all of Paschal's ordinances at the Third Lateran Council in 1179. He is not enumerated among the 28 saints named "Charles" in the "Roman Martyrology". His beatification has been acknowledged as "cultus confirmed" and is celebrated on 28 January.

Charlemagne had a sustained impact on European culture. The author of the "Visio Karoli Magni" written around 865 uses facts gathered apparently from Einhard and his own observations on the decline of Charlemagne's family after the dissensions war (840–43) as the basis for a visionary tale of Charles' meeting with a prophetic spectre in a dream.

Charlemagne was a model knight as one of the Nine Worthies who enjoyed an important legacy in European culture. One of the great medieval literary cycles, the Charlemagne cycle or the "Matter of France", centres on his deeds—the Emperor with the Flowing Beard of "Roland" fame—and his historical commander of the border with Brittany, Roland, and the 12 paladins. These are analogous to, and inspired the myth of, the Knights of the Round Table of King Arthur's court. Their tales constitute the first "chansons de geste".

In the 12th century, Geoffrey of Monmouth based his stories of Arthur largely on stories of Charlemagne. During the Hundred Years' War in the 14th century, there was considerable cultural conflict in England, where the Norman rulers were aware of their French roots and identified with Charlemagne, Anglo-Saxon natives felt more affinity for Arthur, whose own legends were relatively primitive. Therefore, storytellers in England adapted legends of Charlemagne and his 12 Peers to the Arthurian tales.

In the "Divine Comedy", the spirit of Charlemagne appears to Dante in the , among the other "warriors of the faith".

Charlemagne appears in "Adelchi", the second tragedy by Italian writer Alessandro Manzoni, first published in 1822.

In 1867, an equestrian statue of Charlemagne was made by Louis Jehotte and was inaugurated in 1868 on the Boulevard d'Avroy in Liège. In the niches of the neo-roman pedestal are six statues of Charlemagne's ancestors (Sainte Begge, Pépin de Herstal, Charles Martel, Bertrude, Pépin de Landen and Pépin le Bref).

The North Wall Frieze in the courtroom of the Supreme Court of the United States depicts Charlemagne as a legal reformer.

The city of Aachen has, since 1949, awarded an international prize (called the "Karlspreis der Stadt Aachen") in honour of Charlemagne. It is awarded annually to "personages of merit who have promoted the idea of western unity by their political, economic and literary endeavours." Winners of the prize include Richard von Coudenhove-Kalergi, the founder of the pan-European movement, Alcide De Gasperi, and Winston Churchill.

In its national anthem, "El Gran Carlemany", the nation of Andorra credits Charlemagne with its independence.

In 1964, young French singer France Gall released the hit song "Sacré Charlemagne" in which the lyrics blame the great king for imposing the burden of compulsory education on French children.

Charlemagne is quoted by Dr Henry Jones, Sr. in "Indiana Jones and the Last Crusade". After using his umbrella to induce a flock of seagulls to smash through the glass cockpit of a pursuing German fighter plane, Henry Jones remarks, "I suddenly remembered my Charlemagne: 'Let my armies be the rocks and the trees and the birds in the sky. Despite the quote's popularity since the movie, there is no evidence that Charlemagne actually said this.

"The Economist" features a weekly column entitled "Charlemagne", focusing generally on European affairs and, more usually and specifically, on the European Union and its politics.

Actor and singer Christopher Lee's symphonic metal concept album "" and its heavy metal follow-up "" feature the events of Charlemagne's life.

A 2010 episode of "QI" discussed the mathematics completed by Mark Humphrys that calculated that all modern Europeans are highly likely to share Charlemagne as a common ancestor (see most recent common ancestor).

In April 2014, on the occasion of the 1200th anniversary of Charlemagne's death, public art "Mein Karl" by Ottmar Hörl at Katschhof place was installed between city hall and the Aachen cathedral, displaying 500 Charlemagne statues.

The expansion pack "Age of Charlemagne" for the 2015 strategy game "" features Charlemagne as the faction leader for his half of the Carolingian Empire.



</doc>
<doc id="5315" url="https://en.wikipedia.org/wiki?curid=5315" title="Character encodings in HTML">
Character encodings in HTML

HTML (Hypertext Markup Language) has been in use since 1991, but HTML 4.0 (December 1997) was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII, two goals are worth considering: the information's integrity, and universal browser display.

There are several ways to specify which character encoding is used in the document. First, the web server can include the character encoding or "codice_1" in the Hypertext Transfer Protocol (HTTP) codice_2 header, which would typically look like this:
This method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module codice_3.

For HTML it is possible to include this information inside the codice_4 element near the top of the document:

HTML5 also allows the following syntax to mean exactly the same:

XHTML documents have a third option: to express the character encoding via XML declaration, as follows:
As the character encoding cannot be known until this declaration is parsed, there can be a problem knowing which encoding is used for the declaration itself. The main principle is that the declaration shall be encoded in pure ASCII, and therefore (if the declaration is inside the file) the encoding needs to be an ASCII extension. In order to allow encodings not backwards compatible with ASCII, browsers must be able to parse declarations in such encodings. Examples of such encodings are UTF-16BE and UTF-16LE.

As of HTML5 the recommended charset is UTF-8. An "encoding sniffing algorithm" is defined in the specification to determine the character encoding of the document based on multiple sources of input, including:

For ASCII-compatible character encodings the consequence of choosing incorrectly is that characters outside the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly—in some cases, always—require characters outside that range. In CJK environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override "incorrect" charset label manually as well.

It is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.

Successful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.

In addition to native character encodings, characters can also be encoded as "character references", which can be "numeric character references" (decimal or hexadecimal) or "character entity references". Character entity references are also sometimes referred to as "named entities", or "HTML entities" for HTML. HTML's usage of character references derives from SGML.

A "numeric character reference" in HTML refers to a character by its Universal Character Set/Unicode "code point", and uses the format

or

where "nnnn" is the code point in decimal form, and "hhhh" is the code point in hexadecimal form. The "x" must be lowercase in XML documents. The "nnnn" or "hhhh" may be any number of digits and may include leading zeros. The "hhhh" may mix uppercase and lowercase, though uppercase is the usual style.

Not all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.

For codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.

Character entity references can also have the format codice_7 where "name" is a case-sensitive alphanumeric string. For example, "λ" can also be encoded as codice_8 in an HTML document. The character entity references codice_9, codice_10, codice_11 and codice_12 are predefined in HTML and SGML, because codice_13, codice_14, codice_15 and codice_16 are already used to delimit markup. This notably does not include XML's codice_17 (') entity. For a list of all named HTML character entity references (about 250), see List of XML and HTML character entity references.

Unnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.

Unlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:


All other character entity references have to be defined before they can be used. For example, use of codice_23 (which gives é, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the codice_24 in hexadecimal numeric references be in lowercase: for example codice_25 rather than codice_26. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities.




</doc>
<doc id="5320" url="https://en.wikipedia.org/wiki?curid=5320" title="Carbon nanotube">
Carbon nanotube

Carbon nanotubes (CNTs) are tubes made of carbon with diameters typically measured in nanometers.

Carbon nanotubes often refers to single-wall carbon nanotubes (SWCNTs) with diameters in the range of a nanometer. They were discovered independently by Iijima and and Bethune et al. in carbon arc chambers similar to those used to produce fullerenes. Single-wall carbon nanotubes are one of the allotropes of carbon, intermediate between fullerene cages and flat graphene.

Although not made this way, single-wall carbon nanotubes can be thought of as cutouts from a two-dimensional hexagonal lattice of carbon atoms rolled up along one of the Bravais lattice vectors of the hexagonal lattice to form a hollow cylinder. In this construction, periodic boundary conditions are imposed over the length of this roll up vector to yield a lattice with helical symmetry of seamlessly bonded carbon atoms on the cylinder surface.

Carbon nanotubes also often refer to multi-wall carbon nanotubes (MWCNTs) consisting of nested single-wall carbon nanotubes. If not identical, these tubes are very similar to Oberlin, Endo and Koyama's long straight and parallel carbon layers cylindrically rolled around a hollow tube. Multi-wall carbon nanotubes are also sometimes used to refer to double- and triple-wall carbon nanotubes.

Carbon nanotubes can also refer to tubes with an undetermined carbon-wall structure and diameters less than 100 nanometers. Such tubes were discovered by Radushkevich and Lukyanovich. While nanotubes of other compositions exist, most research has been focused on the carbon ones. Therefore, the "carbon" qualifier is often left implicit in the acronyms, and the names are abbreviated NT, SWNT, and MWNT.

Carbon nanotubes can exhibit remarkable electrical conductivity. They also have exceptional and thermal conductivity, because of their nanostructure and strength of the bonds between carbon atoms. In addition, they can be chemically modified. These properties are expected to be valuable in many areas of technology, such as electronics, optics, composite materials (replacing or complementing carbon fibers), nanotechnology, and other applications of materials science.

Individual carbon nanotubes naturally align themselves into "ropes" held together by relatively weak van der Waals forces. The length of a carbon nanotube produced by common production methods is often not reported, but is much larger than its diameter. Although rare, nanotubes half a meter long have been created, with a length-to-diameter ratio of more than 100,000,000:1. For many purposes, the length of carbon nanotubes can be assumed to be infinite.

Rolling up a hexagonal lattice along different directions to form different single-wall carbon nanotubes shows that all of these tubes have helical and translational symmetry along the tube axis and many also have nontrivial rotational symmetry about this axis. In addition, most are chiral, meaning the tube and its mirror image cannot be superimposed. This construction also allows single-wall carbon nanotubes to be labeled by a pair of small integers.

A special group of achiral single-wall carbon nanotubes are metallic, but all the rest are either small or moderate band gap semiconductors. These electrical properties, however, do not depend of whether the tube is rolled up above or below the graphene plane and hence are the same for a tube and its mirror image.

The structure of an ideal (infinitely long) single-walled carbon nanotube is that of a regular hexagonal lattice drawn on an infinite cylindrical surface, whose vertices are the positions of the carbon atoms. Since the length of the carbon-carbon bonds is fairly fixed, there are constraints on the diameter of the cylinder and the arrangement of the atoms on it.

In the study of nanotubes, one defines a "zigzag" path on a graphene-like lattice as a path that turns 60 degrees, alternating left and right, after stepping through each bond. It is also conventional to define an "armchair" path as one that makes two left turns of 60 degrees followed by two right turns every four steps.

On some carbon nanotubes, there is a closed zigzag path that goes around the tube. One says that the tube is of the zigzag type or configuration, or simply is a zigzag nanotube. If the tube is instead encircled by a closed armchair path, it is said to be of the armchair type, or an armchair nanotube.

An infinite nanotube that is of the zigzag (or armchair) type consists entirely of closed zigzag (or armchair) paths, connected to each other.

The zigzag and armchair configurations are not the only structures that a single-walled nanotube can have. To describe the structure of a general infinitely long tube, one should imagine it being sliced open by a cut parallel to its axis, that goes through some atom "A", and then unrolled flat on the plane, so that its atoms and bonds coincide with those of an imaginary graphene sheet—more precisely, with an infinitely long strip of that sheet.

The two halves of the atom "A" will end up on opposite edges of the strip, over two atoms "A1" and "A2" of the graphene. The line from "A1" to "A2" will correspond to the circumference of the cylinder that went through the atom "A", and will be perpendicular to the edges of the strip.

In the graphene lattice, the atoms can be split into two classes, depending on the directions of their three bonds. Half the atoms have their three bonds directed the same way, and half have their three bonds rotated 180 degrees relative to the first half. The atoms "A1" and "A2", which correspond to the same atom "A" on the cylinder, must be in the same class.

It follows that the circumference of the tube and the angle of the strip are not arbitrary, because they are constrained to the lengths and directions of the lines that connect pairs of graphene atoms in the same class.
Let u and v be two linearly independent vectors that connect the graphene atom "A1" to two of its nearest atoms with the same bond directions. That is, if one numbers consecutive carbons around a graphene cell with C1 to C6, then u can be the vector from C1 to C3, and v be the vector from C1 to C5. Then, for any other atom "A2" with same class as "A1", the vector from "A1" to "A2" can be written as a linear combination "n" u + "m" v, where "n" and "m" are integers. And, conversely, each pair of integers ("n","m") defines a possible position for "A2".

Given "n" and "m", one can reverse this theoretical operation by drawing the vector w on the graphene lattice, cutting a strip of the latter along lines perpendicular to w through its endpoints "A1" and "A2", and rolling the strip into a cylinder so as to bring those two points together. If this construction is applied to a pair ("k",0), the result is a zigzag nanotube, with closed zigzag paths of 2"k" atoms. If it is applied to a pair ("k","k"), one obtains an armchair tube, with closed armchair paths of 4"k" atoms.

Moreover, the structure of the nanotube is not changed if the strip is rotated by 60 degrees clockwise around "A1" before applying the hypothetical reconstruction above. Such a rotation changes the corresponding pair ("n","m") to the pair (−2"m","n"+"m").

It follows that many possible positions of "A2" relative to "A1" — that is, many pairs ("n","m") — correspond to the same arrangement of atoms on the nanotube. That is the case, for example, of the six pairs (1,2), (−2,3), (−3,1), (−1,−2), (2,−3), and (3,−1). In particular, the pairs ("k",0) and (0,"k") describe the same nanotube geometry.

These redundancies can be avoided by considering only pairs ("n","m") such that "n" > 0 and "m" ≥ 0; that is, where the direction of the vector w lies between those of u (inclusive) and v (exclusive). It can be verified that every nanotube has exactly one pair ("n","m") that satisfies those conditions, which is called the tube's type. Conversely, for every type there is a hypothetical nanotube. In fact, two nanotubes have the same type if and only if one can be conceptually rotated and translated so as to match the other exactly.

Instead of the type ("n","m"), the structure of a carbon nanotube can be specified by giving the length of the vector w (that is, the circumference of the nanotube), and the angle "α" between the directions of u and w, which may range from 0 (inclusive) to 60 degrees clockwise (exclusive). If the diagram is drawn with u horizontal, the latter is the tilt of the strip away from the vertical.

Here are some unrolled nanotube diagrams:
A nanotube is chiral if it has type ("n","m"), with "m" > 0 and "m" ≠ "n"; then its enantiomer (mirror image) has type ("m","n"), which is different from ("n","m"). This operation corresponds to mirroring the unrolled strip about the line "L" through "A1" that makes an angle of 30 degrees clockwise from the direction of the u vector (that is, with the direction of the vector u+v). The only types of nanotubes that are achiral are the ("k",0) "zigzag" tubes and the ("k","k") "armchair" tubes.

If two enantiomers are to be considered the same structure, then one may consider only types ("n","m") with 0 ≤ "m" ≤ "n" and "n" > 0. Then the angle "α" between u and w, which may range from 0 to 30 degrees (inclusive both), is called the "chiral angle" of the nanotube.

From "n" and "m" one can also compute the circumference "c", which is the length of the vector w, which turns out to be

in picometres. The diameter formula_2 of the tube is then formula_3, that is

also in picometres. (These formulas are only approximate, especially for small "n" and "m" where the bonds are strained; and they do not take into account the thickness of the wall.)

The tilt angle "α" between u and w and the circumference "c" are related to the type indices "n" and "m" by
where arg("x","y") is the clockwise angle between the "X"-axis and the vector ("x","y"); a function that is available in many programming languages as codice_1("y","x"). Conversely, given "c" and "α", one can get the type ("n","m") by the formulas
which must evaluate to integers.

If "n" and "m" are too small, the structure described by the pair ("n","m") will describe a molecule that cannot be reasonably called a "tube", and may not even be stable. For example, the structure theoretically described by the pair (1,0) (the limiting "zigzag" type) would be just a chain of carbons. That is a real molecule, the carbyne; which has some characteristics of nanotubes (such as orbital hybridization, high tensile strength, etc.) — but has no hollow space, and may not be obtainable as a condensed phase. The pair (2,0) would theoretically yield a chain of fused 4-cycles; and (1,1), the limiting "armchair" structure, would yield a chain of bi-connected 4-rings. These structures may not be realizable.

The thinnest carbon nanotube proper is the armchair structure with type (2,2), which has a diameter of 0.3 nm. This nanotube was grown inside a multi-walled carbon nanotube. Assigning of the carbon nanotube type was done by a combination of high-resolution transmission electron microscopy (HRTEM), Raman spectroscopy, and density functional theory (DFT) calculations.

The thinnest "freestanding" single-walled carbon nanotube is about 0.43 nm in diameter. Researchers suggested that it can be either (5,1) or (4,2) SWCNT, but the exact type of the carbon nanotube remains questionable. (3,3), (4,3), and (5,1) carbon nanotubes (all about 0.4 nm in diameter) were unambiguously identified using aberration-corrected high-resolution transmission electron microscopy inside double-walled CNTs.

Here are some tube types that are "degenerate" for being too narrow:
The observation of the "longest" carbon nanotubes grown so far, around 1/2 meter (550 mm long), was reported in 2013. These nanotubes were grown on silicon substrates using an improved chemical vapor deposition (CVD) method and represent electrically uniform arrays of single-walled carbon nanotubes.

The "shortest" carbon nanotube can be considered to be the organic compound cycloparaphenylene, which was synthesized in 2008.

The "highest density" of CNTs was achieved in 2013, grown on a conductive titanium-coated copper surface that was coated with co-catalysts cobalt and molybdenum at lower than typical temperatures of 450 °C. The tubes averaged a height of 380 nm and a mass density of 1.6 g cm. The material showed ohmic conductivity (lowest resistance ∼22 kΩ).

There is no consensus on some terms describing carbon nanotubes in scientific literature: both "-wall" and "-walled" are being used in combination with "single", "double", "triple", or "multi", and the letter C is often omitted in the abbreviation, for example, multi-walled carbon nanotube (MWNT). International Standards Organization uses single-wall or multi-wall in its documents.

Multi-walled nanotubes (MWNTs) consist of multiple rolled layers (concentric tubes) of graphene. There are two models that can be used to describe the structures of multi-walled nanotubes. In the "Russian Doll" model, sheets of graphite are arranged in concentric cylinders, e.g., a (0,8) single-walled nanotube (SWNT) within a larger (0,17) single-walled nanotube. In the "Parchment" model, a single sheet of graphite is rolled in around itself, resembling a scroll of parchment or a rolled newspaper. The interlayer distance in multi-walled nanotubes is close to the distance between graphene layers in graphite, approximately 3.4 Å. The Russian Doll structure is observed more commonly. Its individual shells can be described as SWNTs, which can be metallic or semiconducting. Because of statistical probability and restrictions on the relative diameters of the individual tubes, one of the shells, and thus the whole MWNT, is usually a zero-gap metal.

Double-walled carbon nanotubes (DWNTs) form a special class of nanotubes because their morphology and properties are similar to those of SWNTs but they are more resistant to chemicals. This is especially important when it is necessary to graft chemical functions to the surface of the nanotubes (functionalization) to add properties to the CNT. Covalent functionalization of SWNTs will break some C=C double bonds, leaving "holes" in the structure on the nanotube and thus modifying both its mechanical and electrical properties. In the case of DWNTs, only the outer wall is modified. DWNT synthesis on the gram-scale by the CCVD technique was first proposed in 2003 from the selective reduction of oxide solutions in methane and hydrogen.

The telescopic motion ability of inner shells and their unique mechanical properties will permit the use of multi-walled nanotubes as the main movable arms in upcoming nanomechanical devices. The retraction force that occurs to telescopic motion is caused by the Lennard-Jones interaction between shells, and its value is about 1.5 nN.

 Junctions between two or more nanotubes have been widely discussed theoretically. Such junctions are quite frequently observed in samples prepared by arc discharge as well as by chemical vapor deposition. The electronic properties of such junctions were first considered theoretically by Lambin et al., who pointed out that a connection between a metallic tube and a semiconducting one would represent a nanoscale heterojunction. Such a junction could therefore form a component of a nanotube-based electronic circuit. The adjacent image shows a junction between two multiwalled nanotubes.
Junctions between nanotubes and graphene have been considered theoretically and studied experimentally. Nanotube-graphene junctions form the basis of pillared graphene, in which parallel graphene sheets are separated by short nanotubes. Pillared graphene represents a class of three-dimensional carbon nanotube architectures.

Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>100 nm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical-initiated thermal crosslinking method to fabricate macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano-structured pores, and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices, implants, and sensors.

Carbon nanobuds are a newly created material combining two previously discovered allotropes of carbon: carbon nanotubes and fullerenes. In this new material, fullerene-like "buds" are covalently bonded to the outer sidewalls of the underlying carbon nanotube. This hybrid material has useful properties of both fullerenes and carbon nanotubes. In particular, they have been found to be exceptionally good field emitters. In composite materials, the attached fullerene molecules may function as molecular anchors preventing slipping of the nanotubes, thus improving the composite's mechanical properties.

A carbon peapod is a novel hybrid carbon material which traps fullerene inside a carbon nanotube. It can possess interesting magnetic properties with heating and irradiation. It can also be applied as an oscillator during theoretical investigations and predictions.

In theory, a nanotorus is a carbon nanotube bent into a torus (doughnut shape). Nanotori are predicted to have many unique properties, such as magnetic moments 1000 times larger than that previously expected for certain specific radii. Properties such as magnetic moment, thermal stability, etc. vary widely depending on the radius of the torus and the radius of the tube.

Graphenated carbon nanotubes are a relatively new hybrid that combines graphitic foliates grown along the sidewalls of multiwalled or bamboo style CNTs. The foliate density can vary as a function of deposition conditions (e.g., temperature and time) with their structure ranging from a few layers of graphene (< 10) to thicker, more graphite-like. The fundamental advantage of an integrated graphene-CNT structure is the high surface area three-dimensional framework of the CNTs coupled with the high edge density of graphene. Depositing a high density of graphene foliates along the length of aligned CNTs can significantly increase the total charge capacity per unit of nominal area as compared to other carbon nanostructures.

Cup-stacked carbon nanotubes (CSCNTs) differ from other quasi-1D carbon structures, which normally behave as quasi-metallic conductors of electrons. CSCNTs exhibit semiconducting behavior because of the stacking microstructure of graphene layers.

Many properties of single-walled carbon nanotubes depend significantly on the ("n","m") type, and this dependence is non-monotonic (see Kataura plot). In particular, the band gap can vary from zero to about 2 eV and the electrical conductivity can show metallic or semiconducting behavior.

Carbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus. This strength results from the covalent sp bonds formed between the individual carbon atoms. In 2000, a multiwalled carbon nanotube was tested to have a tensile strength of . (For illustration, this translates into the ability to endure tension of a weight equivalent to on a cable with cross-section of ). Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to ≈, which is in agreement with quantum/atomistic models. Because carbon nanotubes have a low density for a solid of 1.3 to 1.4 g/cm, its specific strength of up to 48,000 kN·m·kg is the best of known materials, compared to high-carbon steel's 154 kN·m·kg.

Although the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes lead to significant reduction in the effective strength of multiwalled carbon nanotubes and carbon nanotube bundles down to only a few GPa. This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to ≈60 GPa for multiwalled carbon nanotubes and ≈17 GPa for double-walled carbon nanotube bundles. CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.

On the other hand, there was evidence that in the radial direction they are rather soft. The first transmission electron microscope observation of radial elasticity suggested that even van der Waals forces can deform two adjacent nanotubes. Later, nanoindentations with an atomic force microscope were performed by several groups to quantitatively measure radial elasticity of multiwalled carbon nanotubes and tapping/contact mode atomic force microscopy was also performed on single-walled carbon nanotubes. Young's modulus of on the order of several GPa showed that CNTs are in fact very soft in the radial direction.

Unlike graphene, which is a two-dimensional semimetal, carbon nanotubes are either metallic or semiconducting along the tubular axis. For a given ("n","m") nanotube, if "n" = "m", the nanotube is metallic; if "n" − "m" is a multiple of 3 and n ≠ m and nm ≠ 0, then the nanotube is quasi-metallic with a very small band gap, otherwise the nanotube is a moderate semiconductor.
Thus, all armchair ("n" = "m") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.
Carbon nanotubes are not semimetallic because the degenerate point (the point where the π [bonding] band meets the π* [anti-bonding] band, at which the energy goes to zero) is slightly shifted away from the "K" point in the Brillouin zone because of the curvature of the tube surface, causing hybridization between the σ* and π* anti-bonding bands, modifying the band dispersion.

The rule regarding metallic versus semiconductor behavior has exceptions because curvature effects in small-diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic). In theory, metallic nanotubes can carry an electric current density of 4 × 10 A/cm, which is more than 1,000 times greater than those of metals such as copper, where for copper interconnects, current densities are limited by electromigration. Carbon nanotubes are thus being explored as interconnects and conductivity-enhancing components in composite materials, and many groups are attempting to commercialize highly conducting electrical wire assembled from individual carbon nanotubes. There are significant challenges to be overcome however, such as undesired current saturation under voltage, and the much more resistive nanotube-to-nanotube junctions and impurities, all of which lower the electrical conductivity of the macroscopic nanotube wires by orders of magnitude, as compared to the conductivity of the individual nanotubes.

Because of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2"G", where "G" = 2"e"/"h" is the conductance of a single ballistic quantum channel.

Because of the role of the π-electron system in determining the electronic properties of graphene, doping in carbon nanotubes differs from that of bulk crystalline semiconductors from the same group of the periodic table (e.g., silicon). Graphitic substitution of carbon atoms in the nanotube wall by boron or nitrogen dopants leads to p-type and n-type behavior, respectively, as would be expected in silicon. However, some non-substitutional (intercalated or adsorbed) dopants introduced into a carbon nanotube, such as alkali metals and electron-rich metallocenes, result in n-type conduction because they donate electrons to the π-electron system of the nanotube. By contrast, π-electron acceptors such as FeCl or electron-deficient metallocenes function as p-type dopants because they draw π-electrons away from the top of the valence band.

Intrinsic superconductivity has been reported, although other experiments found no evidence of this, leaving the claim a subject of debate.

Carbon nanotubes have useful absorption, photoluminescence (fluorescence), and Raman spectroscopy properties. Spectroscopic methods offer the possibility of quick and non-destructive characterization of relatively large amounts of carbon nanotubes. There is a strong demand for such characterization from the industrial point of view: numerous parameters of nanotube synthesis can be changed, intentionally or unintentionally, to alter the nanotube quality. As shown below, optical absorption, photoluminescence, and Raman spectroscopies allow quick and reliable characterization of this "nanotube quality" in terms of non-tubular carbon content, structure (chirality) of the produced nanotubes, and structural defects. These features determine nearly any other properties such as optical, mechanical, and electrical properties.

Carbon nanotubes are unique "one-dimensional systems" which can be envisioned as rolled single sheets of graphite (or more precisely graphene). This rolling can be done at different angles and curvatures resulting in different nanotube properties. The diameter typically varies in the range 0.4–40 nm (i.e., "only" ~100 times), but the length can vary ~100,000,000,000 times, from 0.14 nm to 55.5 cm. The nanotube aspect ratio, or the length-to-diameter ratio, can be as high as 132,000,000:1, which is unequalled by any other material. Consequently, all the properties of the carbon nanotubes relative to those of typical semiconductors are extremely anisotropic (directionally dependent) and tunable.

Whereas mechanical, electrical, and electrochemical (supercapacitor) properties of the carbon nanotubes are well established and have immediate applications, the practical use of optical properties is yet unclear. The aforementioned tunability of properties is potentially useful in optics and photonics. In particular, light-emitting diodes (LEDs) and photo-detectors based on a single nanotube have been produced in the lab. Their unique feature is not the efficiency, which is yet relatively low, but the narrow selectivity in the wavelength of emission and detection of light and the possibility of its fine tuning through the nanotube structure. In addition, bolometer and optoelectronic memory devices have been realised on ensembles of single-walled carbon nanotubes.

Crystallographic defects also affect the tube's electrical properties. A common result is lowered conductivity through the defective region of the tube. A defect in armchair-type tubes (which can conduct electricity) can cause the surrounding region to become semiconducting, and single monatomic vacancies induce magnetic properties.

All nanotubes are expected to be very good thermal conductors along the tube, exhibiting a property known as "ballistic conduction", but good insulators lateral to the tube axis. Measurements show that an individual SWNT has a room-temperature thermal conductivity along its axis of about 3500 W·m·K; compare this to copper, a metal well known for its good thermal conductivity, which transmits 385 W·m·K. An individual SWNT has a room-temperature thermal conductivity across its axis (in the radial direction) of about 1.52 W·m·K, which is about as thermally conductive as soil. Macroscopic assemblies of nanotubes such as films or fibres have reached up to 1500 W·m·K so far. Networks composed of nanotubes demonstrate different values of thermal conductivity, from the level of thermal insulation with the thermal conductivity of 0.1 W·m·K to such high values. That is dependent on the amount of contribution to the thermal resistance of the system caused by the presence of impurities, misalignments and other factors. The temperature stability of carbon nanotubes is estimated to be up to 2800 °C in vacuum and about 750 °C in air.

Crystallographic defects strongly affect the tube's thermal properties. Such defects lead to phonon scattering, which in turn increases the relaxation rate of the phonons. This reduces the mean free path and reduces the thermal conductivity of nanotube structures. Phonon transport simulations indicate that substitutional defects such as nitrogen or boron will primarily lead to scattering of high-frequency optical phonons. However, larger-scale defects such as Stone Wales defects cause phonon scattering over a wide range of frequencies, leading to a greater reduction in thermal conductivity.

Techniques have been developed to produce nanotubes in sizable quantities, including arc discharge, laser ablation, chemical vapor deposition (CVD) and high-pressure carbon monoxide disproportionation (HiPCO). Among these arc discharge, laser ablation, chemical vapor deposition (CVD) are batch by batch process and HiPCO is gas phase continuous process. Most of these processes take place in a vacuum or with process gases. The CVD growth method is popular, as it yields high quantity and has a degree of control over diameter, length and morphology. Using particulate catalysts, large quantities of nanotubes can be synthesized by these methods, but achieving the repeatability becomes a major problem with CVD growth. The HiPCO process advances in catalysis and continuous growth are making CNTs more commercially viable. The HiPCO process helps in producing high purity single walled carbon nanotubes in higher quantity. The HiPCO reactor operates at high temperature 900-1100 °C and high pressure ~30-50 bar. It uses carbon monoxide as the carbon source and Nickel/ iron penta carbonyl as catalyst. These catalyst acts as the nucleation site for the nanotubes to grow.

Vertically aligned carbon nanotube arrays are also grown by thermal chemical vapor deposition. A substrate (quartz, silicon, stainless steel, etc.) is coated with a catalytic metal (Fe, Co, Ni) layer. Typically that layer is iron, and is deposited via sputtering to a thickness of 1–5 nm. A 10–50 nm underlayer of alumina is often also put down on the substrate first. This imparts controllable wetting and good interfacial properties.
When the substrate is heated to the growth temperature (~700 °C), the continuous iron film breaks up into small islands... each island then nucleates a carbon nanotube. The sputtered thickness controls the island size, and this in turn determines the nanotube diameter. Thinner iron layers drive down the diameter of the islands, and they drive down the diameter of the nanotubes grown. The amount of time that the metal island can sit at the growth temperature is limited, as they are mobile, and can merge into larger (but fewer) islands. Annealing at the growth temperature reduces the site density (number of CNT/mm) while increasing the catalyst diameter.

The as-prepared carbon nanotubes always have impurities such as other forms of carbon (amorphous carbon, fullerene, etc.) and non-carbonaceous impurities (metal used for catalyst). These impurities need to be removed to make use of the carbon nanotubes in applications.

Carbon nanotubes are modelled in a similar manner as traditional composites in which a reinforcement phase is surrounded by a matrix phase. Ideal models such as cylindrical, hexagonal and square models are common. The size of the micromechanics model is highly function of the studied mechanical properties. The concept of representative volume element (RVE) is used to determine the appropriate size and configuration of computer model to replicate the actual behavior of CNT reinforced nanocomposite. Depending on the material property of interest (thermal, electrical, modulus, creep), one RVE might predict the property better than the alternatives. While the implementation of ideal model is comutationally efficient, they do not represent microstructural features observed in scanning electron microscopy of actual nanocomposites. To incorporate realistic modeling, computer models are also generated to incorporate variability such as waviness, orientation and agglomeration of multiwall or single wall carbon nanotubes.

There are many metrology standards and reference materials available for carbon nanotubes.

For single-wall carbon nanotubes, ISO/TS 10868 describes a measurement method for the diameter, purity, and fraction of metallic nanotubes through optical absorption spectroscopy, while ISO/TS 10797 and ISO/TS 10798 establish methods to characterize the morphology and elemental composition of single-wall carbon nanotubes, using transmission electron microscopy and scanning electron microscopy respectively, coupled with energy dispersive X-ray spectrometry analysis.

NIST SRM 2483 is a soot of single-wall carbon nanotubes used as a reference material for elemental analysis, and was characterized using thermogravimetric analysis, prompt gamma activation analysis, induced neutron activation analysis, inductively coupled plasma mass spectroscopy, resonant Raman scattering, UV-visible-near infrared fluorescence spectroscopy and absorption spectroscopy, scanning electron microscopy, and transmission electron microscopy. The Canadian National Research Council also offers a certified reference material SWCNT-1 for elemental analysis using neutron activation analysis and inductively coupled plasma mass spectroscopy. NIST RM 8281 is a mixture of three lengths of single-wall carbon nanotube.

For multiwall carbon nanotubes, ISO/TR 10929 identifies the basic properties and the content of impurities, while ISO/TS 11888 describes morphology using scanning electron microscopy, transmission electron microscopy, viscometry, and light scattering analysis. ISO/TS 10798 is also valid for multiwall carbon nanotubes.

Carbon nanotubes can be functionalized to attain desired properties that can be used in a wide variety of applications. The two main methods of carbon nanotube functionalization are covalent and non-covalent modifications. Because of their apparent hydrophobic nature, carbon nanotubes tend to agglomerate hindering their dispersion in solvents or viscous polymer melts. The resulting nanotube bundles or aggregates reduce the mechanical performance of the final composite. The surface of the carbon nanotubes can be modified to reduce the hydrophobicity and improve interfacial adhesion to a bulk polymer through chemical attachment.

Also surface of carbon nanotubes can be fluorinated or halofluorinated by CVD-method with fluorocarbons, hydro- or halofluorocarbons by heating while in contact of such carbon material with fluoroorganic substance to form partially fluorinated carbons (so called Fluocar materials) with grafted (halo)fluoroalkyl functionality.

A primary obstacle for applications of carbon nanotubes has been their cost. Prices for single-walled nanotubes declined from around $1500 per gram as of 2000 to retail prices of around $50 per gram of as-produced 40–60% by weight SWNTs as of March 2010. As of 2016, the retail price of as-produced 75% by weight SWNTs was $2 per gram. SWNTs are forecast to make a large impact in electronics applications by 2020 according to "The Global Market for Carbon Nanotubes" report.

Current use and application of nanotubes has mostly been limited to the use of bulk nanotubes, which is a mass of rather unorganized fragments of nanotubes. Bulk nanotube materials may never achieve a tensile strength similar to that of individual tubes, but such composites may, nevertheless, yield strengths sufficient for many applications. Bulk carbon nanotubes have already been used as composite fibers in polymers to improve the mechanical, thermal and electrical properties of the bulk product.

Other current applications include:

Current research for modern applications include:

Carbon nanotubes can serve as additives to various structural materials. For instance, nanotubes form a tiny portion of the material(s) in some (primarily carbon fiber) baseball bats, golf clubs, car parts, or damascus steel.

IBM expected carbon nanotube transistors to be used on Integrated Circuits by 2020.

The strength and flexibility of carbon nanotubes makes them of potential use in controlling other nanoscale structures, which suggests they will have an important role in nanotechnology engineering. The highest tensile strength of an individual multi-walled carbon nanotube has been tested to be 63 GPa. Carbon nanotubes were found in Damascus steel from the 17th century, possibly helping to account for the legendary strength of the swords made of it. Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (>1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.

CNTs are potential candidates for future via and wire material in nano-scale VLSI circuits. Eliminating electromigration reliability concerns that plague today's Cu interconnects, isolated (single and multi-wall) CNTs can carry current densities in excess of 1000 MA/cm without electromigration damage.

Single-walled nanotubes are likely candidates for miniaturizing electronics. The most basic building block of these systems is an electric wire, and SWNTs with diameters of an order of a nanometer can be excellent conductors. One useful application of SWNTs is in the development of the first intermolecular field-effect transistors (FET). The first intermolecular logic gate using SWCNT FETs was made in 2001. A logic gate requires both a p-FET and an n-FET. Because SWNTs are p-FETs when exposed to oxygen and n-FETs otherwise, it is possible to expose half of an SWNT to oxygen and protect the other half from it. The resulting SWNT acts as a "not" logic gate with both p- and n-type FETs in the same molecule.
Large quantities of pure CNTs can be made into a freestanding sheet or film by surface-engineered tape-casting (SETC) fabrication technique which is a scalable method to fabricate flexible and foldable sheets with superior properties. Another reported form factor is CNT fiber (a.k.a. filament) by wet spinning. The fiber is either directly spun from the synthesis pot or spun from pre-made dissolved CNTs. Individual fibers can be turned into a yarn. Apart from its strength and flexibility, the main advantage is making an electrically conducting yarn. The electronic properties of individual CNT fibers (i.e. bundle of individual CNT) are governed by the two-dimensional structure of CNTs. The fibers were measured to have a resistivity only one order of magnitude higher than metallic conductors at 300K. By further optimizing the CNTs and CNT fibers, CNT fibers with improved electrical properties could be developed.

CNT-based yarns are suitable for applications in energy and electrochemical water treatment when coated with an ion-exchange membrane. Also, CNT-based yarns could replace copper as a winding material. Pyrhönen et al. (2015) have built a motor using CNT winding.

The National Institute for Occupational Safety and Health (NIOSH) is the leading United States federal agency conducting research and providing guidance on the occupational safety and health implications and applications of nanotechnology. Early scientific studies have indicated that some of these nanoscale particles may pose a greater health risk than the larger bulk form of these materials. In 2013, NIOSH published a Current Intelligence Bulletin detailing the potential hazards and recommended exposure limit for carbon nanotubes and fibers.

As of October 2016, single wall carbon nanotubes have been registered through the European Union's Registration, Evaluation, Authorization and Restriction of Chemicals (REACH) regulations, based on evaluation of the potentially hazardous properties of SWCNT. Based on this registration, SWCNT commercialization is allowed in the EU up to 10 metric tons. Currently, the type of SWCNT registered through REACH is limited to the specific type of single wall carbon nanotubes manufactured by OCSiAl, which submitted the application.

The true identity of the discoverers of carbon nanotubes is a subject of some controversy. A 2006 editorial written by Marc Monthioux and Vladimir Kuznetsov in the journal "Carbon" described the interesting and often-misstated origin of the carbon nanotube. A large percentage of academic and popular literature attributes the discovery of hollow, nanometer-size tubes composed of graphitic carbon to Sumio Iijima of NEC in 1991. He published a paper describing his discovery which initiated a flurry of excitement and could be credited by inspiring the many scientists now studying applications of carbon nanotubes. Though Iijima has been given much of the credit for discovering carbon nanotubes, it turns out that the timeline of carbon nanotubes goes back much further than 1991.

In 1952, L. V. Radushkevich and V. M. Lukyanovich published clear images of 50 nanometer diameter tubes made of carbon in the Soviet "Journal of Physical Chemistry". This discovery was largely unnoticed, as the article was published in Russian, and Western scientists' access to Soviet press was limited during the Cold War. Monthioux and Kuznetsov mentioned in their "Carbon" editorial: 

In 1976, Morinobu Endo of CNRS observed hollow tubes of rolled up graphite sheets synthesised by a chemical vapour-growth technique. The first specimens observed would later come to be known as single-walled carbon nanotubes (SWNTs). Endo, in his early review of vapor-phase-grown carbon fibers (VPCF), also reminded us that he had observed a hollow tube, linearly extended with parallel carbon layer faces near the fiber core. This appears to be the observation of multi-walled carbon nanotubes at the center of the fiber. The mass-produced MWCNTs today are strongly related to the VPGCF developed by Endo. In fact, they call it the "Endo-process", out of respect for his early work and patents.

In 1979, John Abrahamson presented evidence of carbon nanotubes at the 14th Biennial Conference of Carbon at Pennsylvania State University. The conference paper described carbon nanotubes as carbon fibers that were produced on carbon anodes during arc discharge. A characterization of these fibers was given as well as hypotheses for their growth in a nitrogen atmosphere at low pressures.

In 1981, a group of Soviet scientists published the results of chemical and structural characterization of carbon nanoparticles produced by a thermocatalytical disproportionation of carbon monoxide. Using TEM images and XRD patterns, the authors suggested that their "carbon multi-layer tubular crystals" were formed by rolling graphene layers into cylinders. They speculated that by rolling graphene layers into a cylinder, many different arrangements of graphene hexagonal nets are possible. They suggested two possibilities of such arrangements: circular arrangement (armchair nanotube) and a spiral, helical arrangement (chiral tube).

In 1987, Howard G. Tennent of Hyperion Catalysis was issued a U.S. patent for the production of "cylindrical discrete carbon fibrils" with a "constant diameter between about 3.5 and about 70 nanometers..., length 10 times the diameter, and an outer region of multiple essentially continuous layers of ordered carbon atoms and a distinct inner core..."

Iijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991 and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties helped create the initial excitement associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries by Iijima and Ichihashi at NEC and Bethune "et al." at IBM of "single-walled" carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge. The arc discharge technique was well known to produce the famed Buckminster fullerene on a preparative scale, and these results appeared to extend the run of accidental discoveries relating to fullerenes. The discovery of nanotubes remains a contentious issue. Many believe that Iijima's report in 1991 is of particular importance because it brought carbon nanotubes into the awareness of the scientific community as a whole.

"This article incorporates public domain text from National Institute of Environmental Health Sciences (NIEHS) as quoted."



</doc>
<doc id="5321" url="https://en.wikipedia.org/wiki?curid=5321" title="Czech Republic">
Czech Republic

The Czech Republic (; ), also known by its short-form name, Czechia (; ), is a country in Central Europe bordered by Poland to the northeast, Slovakia to the southeast, Austria to the south, and Germany to the west. The Czech Republic is a landlocked country with a hilly landscape that covers an area of with a mostly temperate continental climate and oceanic climate. It is a unitary parliamentary republic, with /1e6 round 1 million inhabitants. Its capital and largest city is Prague, with 1.3 million residents; other major cities are Brno, Ostrava, Olomouc and Pilsen.

The Czech Republic includes the historical territories of Bohemia, Moravia, and Czech Silesia. The Czech state was formed in the late ninth century as the Duchy of Bohemia under the Great Moravian Empire. In 1002, the duchy was formally recognized as an Imperial State of the Holy Roman Empire along with the Kingdom of Germany, the Kingdom of Burgundy, the Kingdom of Italy; and became the Kingdom of Bohemia in 1198, reaching its greatest territorial extent in the 14th century. Prague was the imperial seat in periods between the 14th and 17th century. The Protestant Bohemian Reformation of the 15th century led to the Hussite Wars, the first of many conflicts with the Catholic Church.

Following the Battle of Mohács in 1526, the whole Crown of Bohemia was gradually integrated into the Habsburg Monarchy. The Protestant Bohemian Revolt (1618–20) against the Catholic Habsburgs led to the Thirty Years' War. After the Battle of the White Mountain, the Habsburgs consolidated their rule, eradicated Protestantism, reimposed Catholicism, and adopted a policy of gradual Germanization. This contributed to anti-Habsburg sentiment and resentment of the Catholic Church that continues to this day. With the dissolution of the Holy Roman Empire in 1806, the Bohemian Kingdom became part of the Austrian Empire (1804 to 1867) and the Czech language experienced a revival as a consequence of widespread romantic nationalism. In the 19th century, the Czech lands became the industrial powerhouse of the monarchy and were subsequently the core of the First Czechoslovak Republic, which was formed in 1918 following the collapse of the Austro-Hungarian Empire after World War I.

Czechoslovakia was the only democracy in Central Europe during the interwar period (after the May Coup in Poland, in 1926). However, parts of the country were occupied by Germany in World War II, while the Slovak region became a German puppet state. Czechoslovakia was liberated in 1945 by the Soviet Union and the United States. Most of the German-speaking minority were expelled following the war. The Communist Party of Czechoslovakia won the 1946 elections and after the 1948 "coup d'état" established a one-party communist state under Soviet influence. Increasing dissatisfaction with the regime culminated in 1968 to the reform movement known as the Prague Spring, which ended in a Soviet-led invasion. Czechoslovakia remained occupied until the 1989 Velvet Revolution, which peacefully ended communist rule and reestablished democracy and a market economy. On 1 January 1993, Czechoslovakia peacefully dissolved, with its constituent states becoming the independent states of the Czech Republic and Slovakia.

The Czech Republic is a developed country with an advanced, high income social market economy. It is a welfare state with a European social model, universal health care, and tuition-free university education. It ranks 13th in the UN inequality-adjusted human development and 14th in the World Bank Human Capital Index ahead of countries such as the United States, the United Kingdom and France. It ranks as the eleventh safest and most peaceful country and performs strongly in democratic governance. The Czech Republic joined NATO in 1999 and the European Union (EU) in 2004. It is also a member of the OECD, the United Nations, the OSCE, and the Council of Europe.

The traditional English name "Bohemia" derives from Latin "Boiohaemum", which means "home of the Boii". The current English name comes from the Polish ethnonym associated with the area, which ultimately comes from the Czech word "Čech". The name comes from the Slavic tribe () and, according to legend, their leader Čech, who brought them to Bohemia, to settle on Říp Mountain. The etymology of the word "Čech" can be traced back to the Proto-Slavic root "*čel-", meaning "member of the people; kinsman", thus making it cognate to the Czech word "člověk" (a person).

The country has been traditionally divided into three lands, namely Bohemia ("Čechy") in the west, Moravia ("Morava") in the east, and Czech Silesia ("Slezsko"; the smaller, south-eastern part of historical Silesia, most of which is located within modern Poland) in the northeast. Known as the "lands of the Bohemian Crown" since the 14th century, a number of other names for the country have been used, including "Czech/Bohemian lands", "Bohemian Crown", "Czechia" and the "lands of the Crown of Saint Wenceslas". When the country regained its independence after the dissolution of the Austro-Hungarian empire in 1918, the new name of "Czechoslovakia" was coined to reflect the union of the Czech and Slovak nations within the one country.

After Czechoslovakia dissolved in 1992, the new Czech state lacked a common English short name. The Czech Ministry of Foreign Affairs recommended the English name "Czechia" in 1993, and the Czech government approved "Czechia" as the official short name in 2016.

Archaeologists have found evidence of prehistoric human settlements in the area, dating back to the Paleolithic era. The Venus of Dolní Věstonice, dated to 29,000–25,000 BCE, together with a few others from nearby locations, is the oldest known ceramic artifact in the world.

In the classical era, as a result of the 3rd century BC Celtic migrations, Bohemia became associated with the Boii. The Boii founded a city on the site of modern Prague; some of its ancient ruins are now a tourist attraction. According to a 2000 study by Semino, 35.6% of Czech males have y-chromosome haplogroup R1b, which is common among Celts but rare among Slavs. Some modern Czechs claim that the people are as much descendants of the Boii as they are from the later Slavic invaders (as well as the historical Germanic peoples of Czech lands).

Later in the 1st century, the Germanic tribes of the Marcomanni and Quadi settled there. Their king Maroboduus is the first documented ruler of Bohemia. During the Migration Period around the 5th century, many Germanic tribes moved westwards and southwards out of Central Europe. Most of the names of Czech rivers are Celtic or old Germanic in origin, dating from usage in those years.

Slavs from the Black Sea–Carpathian region settled in the area (their migration was pushed by an invasion of peoples from Siberia and Eastern Europe into their area: Huns, Avars, Bulgars and Magyars). In the sixth century, they moved westwards into Bohemia, Moravia, and some of present-day Austria and Germany.

During the 7th century, the Frankish merchant Samo, supporting the Slavs fighting against nearby settled Avars, became the ruler of the first known Slavic state in Central Europe, Samo's Empire. The principality of Great Moravia, controlled by Moymir dynasty, arose in the 8th century. It reached its zenith in the 9th (during the reign of Svatopluk I of Moravia), holding off the influence of the Franks. Great Moravia was Christianized, with a crucial role being played by the Byzantine mission of Cyril and Methodius. They created the artificial language Old Church Slavonic, the first literary and liturgical language of the Slavs, and the Glagolitic alphabet.

The Duchy of Bohemia emerged in the late 9th century, when it was unified by the Přemyslid dynasty. In 10th century Boleslaus I, Duke of Bohemia conquered Moravia, Silesia and expanded farther to the east. The Duchy of Bohemia, raised to the Kingdom of Bohemia in 1198, was from 1002 until 1806 an Imperial State of the Holy Roman Empire alongside the Kingdom of Germany, the Kingdom of Burgundy, the Kingdom of Italy and numerous other territories such as the Old Swiss Confederacy and various Papal States. The kingdom was a significant regional power during the Middle Ages.

In 1212, King Přemysl Ottokar I (bearing the title "king" from 1198) extracted the Golden Bull of Sicily (a formal edict) from the emperor, confirming Ottokar and his descendants' royal status; the Duchy of Bohemia was raised to a Kingdom. The bull declared that the King of Bohemia would be exempt from all future obligations to the Holy Roman Empire except for participation in imperial councils. German immigrants settled in the Bohemian periphery in the 13th century. Germans populated towns and mining districts and, in some cases, formed German colonies in the interior of Bohemia. In 1235, the Mongols launched an invasion of Europe. After the Battle of Legnica in Poland, the Mongols carried their raids into Moravia, but were defensively defeated at the fortified town of Olomouc. The Mongols subsequently invaded and defeated Hungary.
King Přemysl Otakar II earned the nickname "Iron and Golden King" because of his military power and wealth. He acquired Austria, Styria, Carinthia and Carniola, thus spreading the Bohemian territory to the Adriatic Sea. He met his death at the Battle on the Marchfeld in 1278 in a war with his rival, King Rudolph I of Germany. Ottokar's son Wenceslaus II acquired the Polish crown in 1300 for himself and the Hungarian crown for his son. He built a great empire stretching from the Danube river to the Baltic Sea. In 1306, the last king of Přemyslid line Wenceslaus III was murdered in mysterious circumstances in Olomouc while he was resting. After a series of dynastic wars, the House of Luxembourg gained the Bohemian throne.

The 14th century, in particular, the reign of the Bohemian king Charles IV (1316–1378), who in 1346 became King of the Romans and in 1354 both King of Italy and Holy Roman Emperor, is considered the Golden Age of Czech history. Of particular significance was the founding of Charles University in Prague in 1348, Charles Bridge, Charles Square. Much of Prague Castle and the cathedral of Saint Vitus in Gothic style were completed during his reign. He unified Brandenburg (until 1415), Lusatia (until 1635), and Silesia (until 1742) under the Bohemian crown. The Black Death, which had raged in Europe from 1347 to 1352, decimated the Kingdom of Bohemia in 1380, killing about 10% of the population.
Bohemian Reformation started around 1402 by Jan Hus. Although Hus was named a heretic and burnt in Constance in 1415, his followers (led by warlords Jan Žižka and Prokop the Great) seceded from the Catholic Church and in the Hussite Wars (1419–1434) defeated five crusades organized against them by the Holy Roman Emperor Sigismund. Petr Chelčický continued with the Hussite movement. During the next two centuries, 90% of the population in Bohemian and Moravian lands were considered Hussites. Hussite George of Podebrady was even a king. Hus's thoughts were a major influence on the later Lutheranism. Martin Luther himself said "we are all Hussites, without having been aware of it" and considered himself as Hus's direct successor.
After 1526 Bohemia came increasingly under Habsburg control as the Habsburgs became first the elected and then in 1627 the hereditary rulers of Bohemia. The of the 16th century, the founders of the central European Habsburg Monarchy, were buried in Prague. Between 1583–1611 Prague was the official seat of the Holy Roman Emperor Rudolf II and his court.

The Defenestration of Prague and subsequent revolt against the Habsburgs in 1618 marked the start of the Thirty Years' War, which quickly spread throughout Central Europe. In 1620, the rebellion in Bohemia was crushed at the Battle of White Mountain, and the ties between Bohemia and the Habsburgs' hereditary lands in Austria were strengthened. The leaders of the Bohemian Revolt were executed in 1621. The nobility and the middle class Protestants had to either convert to Catholicism or leave the country.

The following period, from 1620 to the late 18th century, has often been called colloquially the "Dark Age". The population of the Czech lands declined by a third through the expulsion of Czech Protestants as well as due to the war, disease and famine. The Habsburgs prohibited all Christian confessions other than Catholicism. The flowering of Baroque culture shows the ambiguity of this historical period.
Ottoman Turks and Tatars invaded Moravia in 1663. In 1679–1680 the Czech lands faced the devastating Great Plague of Vienna and an uprising of serfs.
The reigns of Maria Theresa of Austria and her son Joseph II, Holy Roman Emperor and co-regent from 1765, were characterized by enlightened absolutism. In 1740, most of Silesia (except the southernmost area) was seized by King Frederick II of Prussia in the Silesian Wars. In 1757 the Prussians invaded Bohemia and after the Battle of Prague (1757) occupied the city. More than one quarter of Prague was destroyed and St. Vitus Cathedral also suffered heavy damage. Frederick was defeated soon after at the Battle of Kolín and had to leave Prague and retreat from Bohemia. In 1770 and 1771 Great Famine killed about one tenth of the Czech population, or 250,000 inhabitants, and radicalized the countryside leading to peasant uprisings. Serfdom was abolished (in two steps) between 1781 and 1848. Several large battles of the Napoleonic Wars – Battle of Austerlitz, Battle of Kulm – took place on the current territory of the Czech Republic. Joseph Radetzky von Radetz, born to a noble Czech family, was a field marshal and chief of the general staff of the Austrian Empire army during these wars.

The end of the Holy Roman Empire in 1806 led to degradation of the political status of the Kingdom of Bohemia. Bohemia lost its position of an electorate of the Holy Roman Empire as well as its own political representation in the Imperial Diet. Bohemian lands became part of the Austrian Empire and later of Austria–Hungary. During the 18th and 19th century the Czech National Revival began its rise, with the purpose to revive Czech language, culture and national identity. The Revolution of 1848 in Prague, striving for liberal reforms and autonomy of the Bohemian Crown within the Austrian Empire, was suppressed.
In 1866 Austria was defeated by Prussia in the Austro-Prussian War (see also Battle of Königgrätz and Peace of Prague). The Austrian Empire needed to redefine itself to maintain unity in the face of nationalism. At first it seemed that some concessions would be made also to Bohemia, but in the end the Emperor Franz Joseph I effected a compromise with Hungary only. The Austro-Hungarian Compromise of 1867 and the never realized coronation of Franz Joseph as King of Bohemia led to a huge disappointment of Czech politicians. The Bohemian Crown lands became part of the so-called Cisleithania (officially "The Kingdoms and Lands represented in the Imperial Council").

Prague pacifist Bertha von Suttner was awarded the Nobel Peace Prize in 1905. In the same year, the Czech Social Democratic and progressive politicians (including Tomáš Garrigue Masaryk) started the fight for universal suffrage. The first elections under universal male suffrage were held in 1907. The last King of Bohemia was Charles I of Austria who ruled in 1916–1918.

An estimated 1.4 million Czech soldiers fought in World War I, of whom some 150,000 died. Although the majority of Czech soldiers fought for the Austro-Hungarian Empire, more than 90,000 Czech volunteers formed the Czechoslovak Legions in France, Italy and Russia, where they fought against the Central Powers and later against Bolshevik troops. In 1918, during the collapse of the Habsburg Empire at the end of World War I, the independent republic of Czechoslovakia, which joined the winning Allied powers, was created, with Tomáš Garrigue Masaryk in the lead. This new country incorporated the Bohemian Crown (Bohemia, Moravia and Silesia) and parts of the Kingdom of Hungary (Slovakia and the Carpathian Ruthenia) with significant German, Hungarian, Polish and Ruthenian speaking minorities. Czechoslovakia concluded a treaty of alliance with Romania and Yugoslavia (the so-called Little Entente) and particularly with France.

The First Czechoslovak Republic comprised only 27% of the population of the former Austria-Hungary, but nearly 80% of the industry, which enabled it to successfully compete with Western industrial states. In 1929 compared to 1913, the gross domestic product increased by 52% and industrial production by 41%. In 1938 Czechoslovakia held 10th place in the world industrial production.

Although the First Czechoslovak Republic was a unitary state, it provided what were at the time rather extensive rights to its minorities and remained the only democracy in this part of Europe in the interwar period. The effects of the Great Depression including high unemployment and massive propaganda from Nazi Germany, however, resulted in discontent and strong support among ethnic Germans for a break from Czechoslovakia.
Adolf Hitler took advantage of this opportunity and using Konrad Henlein's separatist Sudeten German Party, gained the largely German-speaking Sudetenland (and its substantial Maginot Line-like border fortifications) through the 1938 Munich Agreement (signed by Nazi Germany, France, Britain, and Italy). Czechoslovakia was not invited to the conference, and Czechs and Slovaks call the Munich Agreement the Munich Betrayal because France (which had an alliance with Czechoslovakia) and Britain gave up Czechoslovakia instead of facing Hitler, which later proved inevitable.

Despite the mobilization of 1.2 million-strong Czechoslovak army and the Franco-Czech military alliance, Poland annexed the Zaolzie area around Český Těšín; Hungary gained parts of Slovakia and the Subcarpathian Rus as a result of the First Vienna Award in November 1938. The remainders of Slovakia and the Subcarpathian Rus gained greater autonomy, with the state renamed to "Czecho-Slovakia". After Nazi Germany threatened to annex part of Slovakia, allowing the remaining regions to be partitioned by Hungary and Poland, Slovakia chose to maintain its national and territorial integrity, seceding from Czecho-Slovakia in March 1939, and allying itself, as demanded by Germany, with Hitler's coalition.

The remaining Czech territory was occupied by Germany, which transformed it into the so-called Protectorate of Bohemia and Moravia. The protectorate was proclaimed part of the Third Reich, and the president and prime minister were subordinated to the Nazi Germany's "Reichsprotektor". Subcarpathian Rus declared independence as the Republic of Carpatho-Ukraine on 15 March 1939 but was invaded by Hungary the same day and formally annexed the next day. Approximately 345,000 Czechoslovak citizens, including 277,000 Jews, were killed or executed while hundreds of thousands of others were sent to prisons and Nazi concentration camps or used as forced labor. Up to two-thirds of the citizens were in groups targeted by the Nazis for deportation or death. One concentration camp was located within the Czech territory at Terezín, north of Prague. The Nazi "Generalplan Ost" called for the extermination, expulsion, Germanization or enslavement of most or all Czechs for the purpose of providing more living space for the German people.

There was Czech resistance to Nazi occupation, both at home and abroad, most notably with the assassination of Nazi German leader Reinhard Heydrich by Czechoslovakian soldiers Jozef Gabčík and Jan Kubiš in a Prague suburb on 27 May 1942. On 9 June 1942 Hitler ordered bloody reprisals against the Czechs as a response to the Czech anti-Nazi resistance. The Edvard Beneš's Czechoslovak government-in-exile and its army fought against the Germans and were acknowledged by the Allies; Czech/Czechoslovak troops fought from the very beginning of the war in Poland, France, the UK, North Africa, the Middle East and the Soviet Union (see I Czechoslovakian Corps). The German occupation ended on 9 May 1945, with the arrival of the Soviet and American armies and the Prague uprising. An estimated 140,000 Soviet soldiers died in liberating Czechoslovakia from German rule.
In 1945–1946, almost the entire German-speaking minority in Czechoslovakia, about 3 million people, were expelled to Germany and Austria (see also Beneš decrees). During this time, thousands of Germans were held in prisons and detention camps or used as forced labor. In the summer of 1945, there were several massacres, such as the Postoloprty massacre. Research by a joint German and Czech commission of historians in 1995 found that the death toll of the expulsions was at least 15,000 persons and that it could range up to a maximum of 30,000 dead. The only Germans not expelled were some 250,000 who had been active in the resistance against the Nazi Germans or were considered economically important, though many of these emigrated later. Following a Soviet-organized referendum, the Subcarpathian Rus never returned under Czechoslovak rule but became part of the Ukrainian Soviet Socialist Republic, as the Zakarpattia Oblast in 1946.

Czechoslovakia uneasily tried to play the role of a "bridge" between the West and East. However, the Communist Party of Czechoslovakia rapidly increased in popularity, with a general disillusionment with the West, because of the pre-war Munich Agreement, and a favourable popular attitude towards the Soviet Union, because of the Soviets' role in liberating Czechoslovakia from German rule. In the 1946 elections, the Communists gained 38% of the votes and became the largest party in the Czechoslovak parliament. They formed a coalition government with other parties of the National Front and moved quickly to consolidate power. A significant change came in 1948 with coup d'état by the Communist Party. The Communist People's Militias secured control of key locations in Prague, and a single party government was formed.
For the next 41 years, Czechoslovakia was a Communist state within the Eastern Bloc. This period is characterized by lagging behind the West in almost every aspect of social and economic development. The country's GDP per capita fell from the level of neighboring Austria below that of Greece or Portugal in the 1980s. The Communist government completely nationalized the means of production and established a command economy. The economy grew rapidly during the 1950s but slowed down in the 1960s and 1970s and stagnated in the 1980s.

The political climate was highly repressive during the 1950s, including numerous show trials (the most famous victims: Milada Horáková and Rudolf Slánský) and hundreds of thousands of political prisoners, but became more open and tolerant in the late 1960s, culminating in Alexander Dubček's leadership in the 1968 Prague Spring, which tried to create "socialism with a human face" and perhaps even introduce political pluralism. This was forcibly ended by invasion by all Warsaw Pact member countries with the exception of Romania and Albania on 21 August 1968. Student Jan Palach became a symbol of resistance to the occupation, when he committed self-immolation as a political protest.

The invasion was followed by a harsh program of "Normalization" in the late 1960s and the 1970s. Until 1989, the political establishment relied on censorship of the opposition. Dissidents published Charter 77 in 1977, and the first of a new wave of protests were seen in 1988. Between 1948 and 1989 about 250,000 Czechs and Slovaks were sent to prison for political reasons, and over 400,000 emigrated.

In November 1989, Czechoslovakia returned to a liberal democracy through the peaceful "Velvet Revolution" (led by Václav Havel and his Civic Forum). However, Slovak national aspirations strengthened (see Hyphen War) and on 1 January 1993, the country peacefully split into the independent countries of the Czech Republic and Slovakia. Both countries went through economic reforms and privatisations, with the intention of creating a market economy. This process was largely successful; in 2006 the Czech Republic was recognized by the World Bank as a "developed country", and in 2009 the Human Development Index ranked it as a nation of "Very High Human Development".

From 1991, the Czech Republic, originally as part of Czechoslovakia and since 1993 in its own right, has been a member of the Visegrád Group and from 1995, the OECD. The Czech Republic joined NATO on 12 March 1999 and the European Union on 1 May 2004. On 21 December 2007 the Czech Republic joined the Schengen Area. Until 2017, either the Social Democrats (under Miloš Zeman, Vladimír Špidla, Stanislav Gross, Jiří Paroubek and Bohuslav Sobotka), or liberal-conservatives (under Václav Klaus, Mirek Topolánek and Petr Nečas) led the government of the Czech Republic.

The Czech Republic lies mostly between latitudes 48° and 51° N (a small area lies north of 51°), and longitudes 12° and 19° E.

The Czech landscape is exceedingly varied. Bohemia, to the west, consists of a basin drained by the Elbe () and the Vltava rivers, surrounded by mostly low mountains, such as the Krkonoše range of the Sudetes. The highest point in the country, Sněžka at , is located here. Moravia, the eastern part of the country, is also quite hilly. It is drained mainly by the Morava River, but it also contains the source of the Oder River ().

Water from the landlocked Czech Republic flows to three different seas: the North Sea, Baltic Sea and Black Sea. The Czech Republic also leases the Moldauhafen, a lot in the middle of the Hamburg Docks, which was awarded to Czechoslovakia by Article 363 of the Treaty of Versailles, to allow the landlocked country a place where goods transported down river could be transferred to seagoing ships. The territory reverts to Germany in 2028.

Phytogeographically, the Czech Republic belongs to the Central European province of the Circumboreal Region, within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of the Czech Republic can be subdivided into four ecoregions: the Western European broadleaf forests, Central European mixed forests, Pannonian mixed forests, and Carpathian montane conifer forests.

There are four national parks in the Czech Republic. The oldest is Krkonoše National Park (Biosphere Reserve), and the others are Šumava National Park (Biosphere Reserve), Podyjí National Park, Bohemian Switzerland.

The three historical lands of the Czech Republic (formerly the core countries of the Bohemian Crown) correspond almost perfectly with the river basins of the Elbe () and the Vltava basin for Bohemia, the Morava one for Moravia, and the Oder river basin for Czech Silesia (in terms of the Czech territory).

The Czech Republic mostly has a temperate oceanic climate, with warm summers and cold, cloudy and snowy winters. The temperature difference between summer and winter is relatively high, due to the landlocked geographical position.

Within the Czech Republic, temperatures vary greatly, depending on the elevation. In general, at higher altitudes, the temperatures decrease and precipitation increases. The wettest area in the Czech Republic is found around Bílý Potok in Jizera Mountains and the driest region is the Louny District to the northwest of Prague. Another important factor is the distribution of the mountains; therefore, the climate is quite varied.

At the highest peak of Sněžka (), the average temperature is only , whereas in the lowlands of the South Moravian Region, the average temperature is as high as . The country's capital, Prague, has a similar average temperature, although this is influenced by urban factors.

The coldest month is usually January, followed by February and December. During these months, there is usually snow in the mountains and sometimes in the major cities and lowlands. During March, April, and May, the temperature usually increases rapidly, especially during April, when the temperature and weather tends to vary widely during the day. Spring is also characterized by high water levels in the rivers, due to melting snow with occasional flooding.

The warmest month of the year is July, followed by August and June. On average, summer temperatures are about – higher than during winter. Summer is also characterized by rain and storms.

Autumn generally begins in September, which is still relatively warm and dry. During October, temperatures usually fall below or and deciduous trees begin to shed their leaves. By the end of November, temperatures usually range around the freezing point.

The coldest temperature ever measured was in Litvínovice near České Budějovice in 1929, at and the hottest measured, was at in Dobřichovice in 2012.

Most rain falls during the summer. Sporadic rainfall is relatively constant throughout the year (in Prague, the average number of days per month experiencing at least of rain varies from 12 in September and October to 16 in November) but concentrated heavy rainfall (days with more than per day) are more frequent in the months of May to August (average around two such days per month). Severe thunderstorms, producing damaging straight-line winds, hail, and even occasional tornadoes occur, especially during the summer period.

The Czech Republic ranks as the 27th most environmentally conscious country in the world in Environmental Performance Index. The Czech Republic has four National Parks (Šumava National Park, Krkonoše National Park, České Švýcarsko National Park, Podyjí National Park) and 25 Protected Landscape Areas.

The Czech Republic is a pluralist multi-party parliamentary representative democracy, with the President as head of state and Prime Minister as head of government. The Parliament ("Parlament České republiky") is bicameral, with the Chamber of Deputies () (200 members) and the Senate () (81 members).

The president is a formal head of state with limited and specific powers, most importantly to return bills to the parliament, appoint members to the board of the Czech National Bank, nominate constitutional court judges for the Senate's approval and dissolve the Chamber of Deputies under certain special and unusual circumstances. The president and vice president of the Supreme Court are appointed by the President of the Republic. He also appoints the prime minister, as well the other members of the cabinet on a proposal by the prime minister. From 1993 until 2012, the President of the Czech Republic was selected by a joint session of the parliament for a five-year term, with no more than two consecutive terms (2x Václav Havel, 2x Václav Klaus). Since 2013 the presidential election is direct. Miloš Zeman was the first directly elected Czech President.

The Government of the Czech Republic's exercise of executive power derives from the Constitution. The members of the government are the Prime Minister, Deputy prime ministers and other ministers. The Government is responsible to the Chamber of Deputies.

The Prime Minister is the head of government and wields considerable powers, such as the right to set the agenda for most foreign and domestic policy and choose government ministers. The current Prime Minister of the Czech Republic is Andrej Babiš, serving since 6 December 2017 as the 12th Prime Minister.

The members of the Chamber of Deputies are elected for a four-year term by proportional representation, with a 5% election threshold. There are 14 voting districts, identical to the country's administrative regions. The Chamber of Deputies, the successor to the Czech National Council, has the powers and responsibilities of the now defunct federal parliament of the former Czechoslovakia.

The members of the Senate are elected in single-seat constituencies by two-round runoff voting for a six-year term, with one-third elected every even year in the autumn. The first election was in 1996, for differing terms. This arrangement is modeled on the U.S. Senate, but each constituency is roughly the same size and the voting system used is a two-round runoff.

The Czech Republic is a unitary state with a civil law system based on the continental type, rooted in Germanic legal culture. The basis of the legal system is the Constitution of the Czech Republic adopted in 1993. The Penal Code is effective from 2010. A new Civil code became effective in 2014. The court system includes district, county and supreme courts and is divided into civil, criminal, and administrative branches. The Czech judiciary has a triumvirate of supreme courts. The Constitutional Court consists of 15 constitutional judges and oversees violations of the Constitution by either the legislature or by the government. The Supreme Court is formed of 67 judges and is the court of highest appeal for almost all legal cases heard in the Czech Republic. The Supreme Administrative Court decides on issues of procedural and administrative propriety. It also has jurisdiction over many political matters, such as the formation and closure of political parties, jurisdictional boundaries between government entities, and the eligibility of persons to stand for public office. The Supreme Court and the Supreme Administrative Court are both based in Brno, as is the Supreme Public Prosecutor's Office.

The Czech Republic ranks as the 11th safest or most peaceful country. It is a member of the United Nations, the European Union, NATO, OECD, Council of Europe and is an observer to the Organization of American States. The embassies of most countries with diplomatic relations with the Czech Republic are located in Prague, while consulates are located across the country.

The Czech passport is one of the least restricted by visas. According to the 2018 Henley & Partners Visa Restrictions Index, Czech citizens have visa-free access to 173 countries, which ranks them 7th along with Malta and New Zealand. The World Tourism Organization ranks the Czech passport 24th. The US Visa Waiver Program applies to Czech nationals.

The Prime Minister and Minister of Foreign Affairs have primary roles in setting foreign policy, although the President has considerable influence and also represents the country abroad. Membership in the European Union and NATO is central to the Czech Republic's foreign policy. The Office for Foreign Relations and Information (ÚZSI) serves as the foreign intelligence agency responsible for espionage and foreign policy briefings, as well as protection of Czech Republic's embassies abroad.

The Czech Republic has strong ties with Slovakia, Poland and Hungary as a member of the Visegrad Group, as well as with Germany, Israel, the United States and the European Union and its members.

Czech officials have supported dissenters in Belarus, Moldova, Myanmar and Cuba.

The Czech armed forces consist of the Czech Land Forces, the Czech Air Force and of specialized support units. The armed forces are managed by the Ministry of Defence. The President of the Czech Republic is Commander-in-chief of the armed forces. In 2004 the army transformed itself into a fully professional organization and compulsory military service was abolished. The country has been a member of NATO since 12 March 1999. Defence spending is approximately 1.19% of the GDP (2019). The armed forces are charged with protecting the Czech Republic and its allies, promoting global security interests, and contributing to NATO.

Currently, as a member of NATO, the Czech military are participating in the Resolute Support and KFOR operations and have soldiers in Afghanistan, Mali, Bosnia and Herzegovina, Kosovo, Egypt, Israel and Somalia. The Czech Air Force also served in the Baltic states and Iceland. The main equipment of the Czech military includes JAS 39 Gripen multi-role fighters, Aero L-159 Alca combat aircraft, Mi-35 attack helicopters, armored vehicles (Pandur II, OT-64, OT-90, BVP-2) and tanks (T-72 and T-72M4CZ).

Since 2000, the Czech Republic has been divided into thirteen regions (Czech: "kraje", singular "kraj") and the capital city of Prague. Every region has its own elected regional assembly ("krajské zastupitelstvo") and "hejtman" (a regional governor). In Prague, the assembly and presidential powers are executed by the city council and the mayor.

The older seventy-six districts ("okresy", singular "okres") including three "statutory cities" (without Prague, which had special status) lost most of their importance in 1999 in an administrative reform; they remain as territorial divisions and seats of various branches of state administration.

The Czech Republic has a developed, high-income export-oriented social market economy based in services, manufacturing and innovation, that maintains a welfare state and the European social model. The Czech Republic participates in the European Single Market as a member of the European Union, and is therefore a part of the economy of the European Union, but uses its own currency, the Czech koruna, instead of the euro. It has a per capita GDP rate that is 91% of the EU average and is a member of the OECD. Monetary policy is conducted by the Czech National Bank, whose independence is guaranteed by the Constitution. The Czech Republic ranks 13th in the UN inequality-adjusted human development and 14th in World Bank Human Capital Index ahead of countries such as the United States, the United Kingdom and France. It was described by "The Guardian" as "one of Europe’s most flourishing economies".

, the country's GDP per capita at purchasing power parity is $37,370 (similar to Israel, Italy or Slovenia) and $22,850 at nominal value. According to Allianz A.G., in 2018 the country was an MWC (mean wealth country), ranking 26th in net financial assets. The country experienced a 4.5% GDP growth in 2017, giving the Czech economy one of the highest growth rates in the European Union. The 2016 unemployment rate was the lowest in the EU at 2.4%, and the 2016 poverty rate was the second lowest of OECD members behind only Denmark. Czech Republic ranks 24th in both the Index of Economic Freedom and the Global Innovation Index , 29th in the Global Competitiveness Report<ref name="GCR 2018/19"></ref> 30th in the ease of doing business index and 25th in the Global Enabling Trade Report.
The Czech Republic has a highly diverse economy that ranks 7th in the 2016 Economic Complexity Index. The industrial sector accounts for 37.5% of the economy, while services account for 60% and agriculture for 2.5%. The largest trading partner for both export and import is Germany and the EU in general. Dividends worth CZK 270 billion were paid to the foreign owners of Czech companies in 2017, which has become a political issue in the Czech Republic. The country has been a member of the Schengen Area since 1 May 2004, having abolished border controls, completely opening its borders with all of its neighbors (Germany, Austria, Poland and Slovakia) on 21 December 2007. The Czech Republic became a member of the World Trade Organization on 1 January 1995.

In 2018 the largest companies by revenue in the Czech Republic were: one of the largest car automobile manufacturers in Central Europe Škoda Auto, utility company ČEZ Group, conglomerate Agrofert, energy trading company EPH, oil processing company Unipetrol, electronics manufacturer Foxconn CZ and steel producer Moravia Steel. Other Czech transportation companies include: Škoda Transportation (tramways, trolleybuses, metro), Tatra (heavy trucks, the second oldest car maker in the world), Avia (medium trucks), Karosa and SOR Libchavy (buses), Aero Vodochody (military aircraft), Let Kunovice (civil aircraft), Zetor (tractors), Jawa Moto (motorcycles) and Čezeta (electric scooters).

Škoda Transportation is the fourth largest tramway producer in the world; nearly one third of all trams in the world come from Czech factories. The Czech Republic is also the world's largest vinyl records manufacturer, with GZ Media producing about 6 million pieces annually in Loděnice. Česká zbrojovka is among the ten largest firearms producers in the world and five who produce automatic weapons.

In the food industry succeeded companies Agrofert, Kofola, Hamé and Bageterie Boulevard.

Production of Czech electricity exceeds consumption by about 10 TWh per year, which are exported. Nuclear power presently provides about 30 percent of the total power needs, its share is projected to increase to 40 percent. In 2005, 65.4 percent of electricity was produced by steam and combustion power plants (mostly coal); 30 percent by nuclear plants; and 4.6 percent from renewable sources, including hydropower. The largest Czech power resource is Temelín Nuclear Power Station, another nuclear power plant is in Dukovany.

The Czech Republic is reducing its dependence on highly polluting low-grade brown coal as a source of energy. Natural gas is procured from Russian Gazprom, roughly three-fourths of domestic consumption and from Norwegian companies, which make up most of the remaining one-fourth. Russian gas is imported via Ukraine, Norwegian gas is transported through Germany. Gas consumption (approx. 100 TWh in 2003–2005) is almost double electricity consumption. South Moravia has small oil and gas deposits.

The road network in the Czech Republic is long. There are 1,232 km of motorways as of 2017. The speed limit is 50 km/h within towns, 90 km/h outside of towns and 130 km/h on motorways.
The Czech Republic has the densest rail network in the world with of tracks. Of that number, is electrified, are single-line tracks and are double and multiple-line tracks. České dráhy (the Czech Railways) is the main railway operator in the Czech Republic, with about 180 million passengers carried yearly. Maximum speed is limited to 160 km/h. In 2006 seven Italian tilting trainsets Pendolino ČD Class 680 entered service.

Václav Havel Airport in Prague is the main international airport in the country. In 2017, it handled 15 million passengers, which makes it the one of the busiest airports in Central Europe. In total, the Czech Republic has 46 airports with paved runways, six of which provide international air services in Brno, Karlovy Vary, Mošnov (near Ostrava), Pardubice, Prague and Kunovice (near Uherské Hradiště).

Russia, via pipelines through Ukraine and to a lesser extent, Norway, via pipelines through Germany, supply the Czech Republic with liquid and natural gas.

The Czech Republic ranks in the top 10 countries worldwide with the fastest average internet speed. By the beginning of 2008, there were over 800 mostly local WISPs, with about 350,000 subscribers in 2007. Plans based on either GPRS, EDGE, UMTS or CDMA2000 are being offered by all three mobile phone operators (T-Mobile, O2, Vodafone) and internet provider U:fon. Government-owned Český Telecom slowed down broadband penetration. At the beginning of 2004, local-loop unbundling began and alternative operators started to offer ADSL and also SDSL. This and later privatisation of Český Telecom helped drive down prices.

On 1 July 2006, Český Telecom was acquired by globalized company (Spain-owned) Telefónica group and adopted the new name Telefónica O2 Czech Republic. , VDSL and ADSL2+ are offered in many variants, with download speeds of up to 50 Mbit/s and upload speeds of up to 5 Mbit/s. Cable internet is gaining popularity with its higher download speeds ranging from 50 Mbit/s to 1 Gbit/s.

Two major computer security companies, Avast and AVG, were founded in the Czech Republic. In 2016, Avast led by Pavel Baudiš bought rival AVG for US$1.3 billion, together at the time, these companies had a user base of about 400 million people and 40% of the consumer market outside of China. Avast is the leading provider of antivirus software, with a 20.5% market share.

The Czech lands have a long and rich scientific tradition. The research based on cooperation between universities, Academy of Sciences and specialized research centers brings new inventions and impulses in this area. Important inventions include the modern contact lens, the separation of modern blood types, the modern genetics, many inventions in nanotechnologies and the production of Semtex plastic explosive.

Cyril and Methodius laid the foundations of education and the Czech theological thinking in the 9th century. Original theological and philosophical stream – Hussitism – originated in the Middle Ages. It was represented by Jan Hus, Jerome of Prague or Petr Chelčický. At the end of the Middle Ages, Jan Amos Comenius substantially contributed to the development of modern pedagogy. Jewish philosophy in the Czech lands was represented mainly by Judah Loew ben Bezalel (known for the legend of the Golem of Prague). Bernard Bolzano was the personality of German-speaking philosophy in the Czech lands. Bohuslav Balbín was a key Czech philosopher and historian of the Baroque era. He also started the struggle for rescuing the Czech language. This culminated in the Czech national revival in the first half of the 19th century. Linguistics (Josef Dobrovský, Pavel Jozef Šafařík, Josef Jungmann), ethnography (Karel Jaromír Erben, František Ladislav Čelakovský) and history (František Palacký) played a big role in revival. Palacký was the eminent personality. He wrote the first synthetic history of the Czech nation. He was also the first Czech modern politician and geopolitician (see also Austro-Slavism). He is often called "The Father of the Nation".

In the second half of the 19th century and at the beginning of the 20th century there was a huge development of social sciences. Tomáš Garrigue Masaryk laid the foundations of Czech sociology. Konstantin Jireček founded Byzantology (see also Jireček Line). Alois Musil was a prominent orientalist, Emil Holub ethnographer. Lubor Niederle was a founder of modern Czech archeology. Sigmund Freud established psychoanalysis. Edmund Husserl defined a new philosophical doctrine – phenomenology. Joseph Schumpeter brought genuine economic ideas of "creative destruction" of capitalism. Hans Kelsen was significant legal theorist. Karl Kautsky influenced the history of Marxism. On the contrary, economist Eugen Böhm von Bawerk led a campaign against Marxism. Max Wertheimer was one of the three founders of Gestalt psychology. Musicologists Eduard Hanslick and Guido Adler influenced debates on the development of classical music in Vienna.

The new Czechoslovak republic (1918–1938) wanted to develop sciences. Significant linguistic school was established in Prague – Prague Linguistic Circle (Vilém Mathesius, Jan Mukařovský, René Wellek), moreover linguist Bedřich Hrozný deciphered the ancient Hittite language and linguist Julius Pokorny deepened knowledge about Celtic languages. Philosopher Herbert Feigl was a member of the Vienna Circle. Ladislav Klíma has developed a special version of Nietzschean philosophy. In the second half of the 20th century can be mentioned philosopher Ernest Gellner who is considered one of the leading theoreticians on the issue of nationalism. Also Czech historian Miroslav Hroch analyzed modern nationalism. Vilém Flusser developed the philosophy of technology and image. Marxist Karel Kosík was a major philosopher in the background of the Prague Spring 1968. Jan Patočka and Václav Havel were the main ideologists of the Charter 77. Egon Bondy was a major philosophical spokesman of the Czech underground in the 1970s and 1980s. Czech Egyptology has scored some successes, its main representative is Miroslav Verner. Czech psychologist Stanislav Grof developed a method of "Holotropic Breathwork". Experimental archaeologist Pavel Pavel made several attempts, they had to answer the question how ancient civilizations transported heavy weights.

Famous scientists who were born on the territory of the current Czech Republic:

A number of other scientists are also connected in some way with the Czech lands. The following taught at the University of Prague: astronomers Johannes Kepler and Tycho Brahe, physicists Christian Doppler, Nikola Tesla, and Albert Einstein, and geologist Joachim Barrande.

The Czech economy gets a substantial income from tourism. Prague is the fifth most visited city in Europe after London, Paris, Istanbul and Rome. In 2001, the total earnings from tourism reached 118 billion CZK, making up 5.5% of GNP and 9% of overall export earnings. The industry employs more than 110,000 people – over 1% of the population.
The country's reputation has suffered with guidebooks and tourists reporting overcharging by taxi drivers and pickpocketing problems mainly in Prague, though the situation has improved recently. Since 2005, Prague's mayor, Pavel Bém, has worked to improve this reputation by cracking down on petty crime and, aside from these problems, Prague is a safe city. Also, the Czech Republic as a whole generally has a low crime rate. For tourists, the Czech Republic is considered a safe destination to visit. The low crime rate makes most cities and towns very safe to walk around.

One of the most visited tourist attractions in the Czech Republic is the Nether district Vítkovice in Ostrava, a post-industrial city in the east within the country. The territory was formerly the site of steel production, but now it hosts a technical museum with many interactive expositions for tourists.

The Czech Republic boasts 14 UNESCO World Heritage Sites. All of them are in the cultural category. , further 18 sites are on the tentative list.
There are several centres of tourist activity. The spa towns, such as Karlovy Vary, Mariánské Lázně and Františkovy Lázně and Jáchymov, are particularly popular relaxing holiday destinations. Architectural heritage is another object of interest to visitors – it includes many castles and châteaux from different historical epoques, namely Karlštejn Castle, Český Krumlov and the Lednice–Valtice area.

There are 12 cathedrals and 15 churches elevated to the rank of basilica by the Pope, calm monasteries, many modern and ancient churches – for example Pilgrimage Church of Saint John of Nepomuk is one of those inscribed on the World Heritage List. Away from the towns, areas such as Český ráj, Šumava and the Krkonoše Mountains attract visitors seeking outdoor pursuits.

The country is also known for its various museums. Puppetry and marionette exhibitions are very popular, with a number of puppet festivals throughout the country. Aquapalace Praha in Čestlice near Prague, is the biggest water park in central Europe.

The Czech Republic has a number of beer festivals, including: Czech Beer Festival (the biggest Czech beer festival, it is usually 17 days long and held every year in May in Prague), Pilsner Fest (every year in August in Plzeň), The Olomoucký pivní festival (in Olomouc) or festival Slavnosti piva v Českých Budějovicích (in České Budějovice).

The total fertility rate (TFR) in 2015 was estimated at 1.57 children born/woman, which is below the replacement rate of 2.1, and one of the lowest in the world. The Czech Republic subsequently has one of the oldest populations in the world, with an average age of 42.5 years. The life expectancy in 2013 was estimated at 77.56 years (74.29 years male, 81.01 years female). Immigration increased the population by almost 1% in 2007. About 77,000 people immigrate to the Czech Republic annually. Vietnamese immigrants began settling in the Czech Republic during the Communist period, when they were invited as guest workers by the Czechoslovak government. In 2009, there were about 70,000 Vietnamese in the Czech Republic. Most decide to stay in the country permanently.

According to preliminary results of the 2011 census, the majority of the inhabitants of the Czech Republic are Czechs (63.7%), followed by Moravians (4.9%), Slovaks (1.4%), Poles (0.4%), Germans (0.2%) and Silesians (0.1%). As the 'nationality' was an optional item, a substantial number of people left this field blank (26.0%). According to some estimates, there are about 250,000 Romani people in the Czech Republic. The Polish minority resides mainly in the Zaolzie region.

There were 496,413 (4.5% of population) foreigners residing in the country in 2016, according to the Czech Statistical Office, with the largest groups being Ukrainian (22%), Slovak (22%), Vietnamese (12%), Russian (7%), German (4%) and from other countries (33%). Most of the foreign population lives in Prague (37.3%) and Central Bohemia Region (13.2%).

The Jewish population of Bohemia and Moravia, 118,000 according to the 1930 census, was virtually annihilated by the Nazi Germans during the Holocaust. There were approximately 4,000 Jews in the Czech Republic in 2005. The former Czech prime minister, Jan Fischer, is of Jewish faith.

At the turn of the 20th century, Chicago was the city with the third largest Czech population, after Prague and Vienna. According to the 2010 US census, there are 1,533,826 Americans of full or partial Czech descent.

The Czech Republic has one of the least religious populations in the world with 75% to 79% of people not declaring any religion or faith in polls and the percentage of convinced atheists being third highest (30%) only behind China (47%) and Japan (31%). The Czech people have been historically characterized as "tolerant and even indifferent towards religion".

Christianization in the 9th and 10th centuries introduced Catholicism. After the Bohemian Reformation, most Czechs became followers of Jan Hus, Petr Chelčický and other regional Protestant Reformers. Taborites and Utraquists were major Hussite groups. During the Hussite Wars, Utraquists sided with the Catholic Church. Following the joint Utraquist—Catholic victory, Utraquism was accepted as a distinct form of Christianity to be practiced in Bohemia by the Catholic Church while all remaining Hussite groups were prohibited. After the Reformation, some Bohemians went with the teachings of Martin Luther, especially Sudeten Germans. In the wake of the Reformation, Utraquist Hussites took a renewed increasingly anti-Catholic stance, while some of the defeated Hussite factions (notably Taborites) were revived. After the Habsburgs regained control of Bohemia, the whole population was forcibly converted to Catholicism—even the Utraquist Hussites. Going forward, Czechs have become more wary and pessimistic of religion as such. A long history of resistance to the Catholic Church followed. It suffered a schism with the neo-Hussite Czechoslovak Hussite Church in 1920, lost the bulk of its adherents during the Communist era and continues to lose in the modern, ongoing secularization. Protestantism never recovered after the Counter-Reformation was introduced by the Austrian Habsburgs in 1620.

According to the 2011 census, 34% of the population stated they had no religion, 10.3% was Catholic, 0.8% was Protestant (0.5% Czech Brethren and 0.4% Hussite), and 9% followed other forms of religion both denominational or not (of which 863 people answered they are Pagan). 45% of the population did not answer the question about religion. From 1991 to 2001 and further to 2011 the adherence to Catholicism decreased from 39% to 27% and then to 10%; Protestantism similarly declined from 3.7% to 2% and then to 0.8%. The Muslim population is estimated to be 20,000 representing 0.2% of the Czech population.

Education in the Czech Republic is compulsory for 9 years and citizens have access to a tuition-free university education, while the average number of years of education is 13.1. Additionally, the Czech Republic has a relatively equal educational system in comparison with other countries in Europe. Founded in 1348, Charles University was the first university in Central Europe. Other major universities in the country are Masaryk University, Czech Technical University, Palacký University, Academy of Performing Arts and University of Economics. And more universities in the Czech Republic.

The Programme for International Student Assessment, coordinated by the OECD, currently ranks the Czech education system as the 15th most successful in the world, higher than the OECD average. The UN Education Index ranks the Czech Republic 10th (positioned behind Denmark and ahead of South Korea).

Healthcare in the Czech Republic is similar in quality to other developed nations. The Czech universal health care system is based on a compulsory insurance model, with fee-for-service care funded by mandatory employment-related insurance plans. According to the 2016 Euro health consumer index, a comparison of healthcare in Europe, the Czech healthcare is 13th, ranked behind Sweden and two positions ahead of the United Kingdom.

Venus of Dolní Věstonice is the treasure of prehistoric art. Theodoric of Prague was the most famous Czech painter in the Gothic era. For example, he decorated the castle Karlstejn. In the Baroque era, the famous painters were Wenceslaus Hollar, Jan Kupecký, Karel Škréta, Anton Raphael Mengs or Petr Brandl, sculptors Matthias Braun and Ferdinand Brokoff. In the first half of the 19th century, Josef Mánes joined the romantic movement. In the second half of the 19th century had the main say the so-called "National Theatre generation": sculptor Josef Václav Myslbek and painters Mikoláš Aleš, Václav Brožík, Vojtěch Hynais or Julius Mařák. At the end of the century came a wave of Art Nouveau. Alfons Mucha became the main representative. He is today the most famous Czech painter. He is mainly known for Art Nouveau posters and his cycle of 20 large canvases named the Slav Epic, which depicts the history of Czechs and other Slavs.

, the Slav Epic can be seen in the Veletržní Palace of the National Gallery in Prague, which manages the largest collection of art in the Czech Republic. Max Švabinský was another important Art nouveau painter. The 20th century brought avant-garde revolution. In the Czech lands mainly expressionist and cubist: Josef Čapek, Emil Filla, Bohumil Kubišta, Jan Zrzavý. Surrealism emerged particularly in the work of Toyen, Josef Šíma and Karel Teige. In the world, however, he pushed mainly František Kupka, a pioneer of abstract painting. As illustrators and cartoonists in the first half of the 20th century gained fame Josef Lada, Zdeněk Burian or Emil Orlík. Art photography has become a new field (František Drtikol, Josef Sudek, later Jan Saudek or Josef Koudelka).

The Czech Republic is known worldwide for its individually made, mouth blown and decorated Bohemian glass.

The earliest preserved stone buildings in Bohemia and Moravia date back to the time of the Christianization in the 9th and 10th century. Since the Middle Ages, the Czech lands have been using the same architectural styles as most of Western and Central Europe. The oldest still standing churches were built in the Romanesque style (St. George's Basilica, St. Procopius Basilica in Třebíč). During the 13th century it was replaced by the Gothic style (Charles Bridge, Bethlehem Chapel, Old New Synagogue, Sedlec Ossuary, Old Town Hall with Prague astronomical clock, Church of Our Lady before Týn). In the 14th century Emperor Charles IV invited to his court in Prague talented architects from France and Germany, Matthias of Arras and Peter Parler (Karlštejn, St. Vitus Cathedral, St. Barbara's Church in Kutná Hora). During the Middle Ages, many fortified castles were built by the king and aristocracy, as well as many monasteries (Strahov Monastery, Špilberk, Křivoklát Castle, Vyšší Brod Monastery). During the Hussite wars, many of them were damaged or destroyed.

The Renaissance style penetrated the Bohemian Crown in the late 15th century when the older Gothic style started to be slowly mixed with Renaissance elements (architects Matěj Rejsek, Benedikt Rejt and their Powder Tower). An outstanding example of the pure Renaissance architecture in Bohemia is the Royal Summer Palace, which was situated in a newly established garden of Prague Castle. Evidence of the general reception of the Renaissance in Bohemia, involving a massive influx of Italian architects, can be found in spacious châteaux with elegant arcade courtyards and geometrically arranged gardens (Litomyšl Castle, Hluboká Castle). Emphasis was placed on comfort, and buildings that were built for entertainment purposes also appeared.

In the 17th century, the Baroque style spread throughout the Crown of Bohemia. Very outstanding are the architectural projects of the Czech nobleman and imperial generalissimo Albrecht von Wallenstein from the 1620s (Wallenstein Palace). His architects Andrea Spezza and Giovanni Pieroni reflected the most recent Italian production and were very innovative at the same time. Czech Baroque architecture is considered to be a unique part of the European cultural heritage thanks to its extensiveness and extraordinariness (Kroměříž Castle, Holy Trinity Column in Olomouc, St. Nicholas Church at Malá Strana, Karlova Koruna Chateau). In the first third of the 18th century the Bohemian lands were one of the leading artistic centers of the Baroque style. In Bohemia there was completed the development of the Radical Baroque style created in Italy by Francesco Borromini and Guarino Guarini in a very original way. Leading architects of the Bohemian Baroque were Jean-Baptiste Mathey, František Maxmilián Kaňka, Christoph Dientzenhofer, and his son Kilian Ignaz Dientzenhofer.

In the 18th century Bohemia produced an architectural peculiarity – the "Baroque Gothic style", a synthesis of the Gothic and Baroque styles. This was not a simple return to Gothic details, but rather an original Baroque transformation. The main representative and originator of this style was Jan Blažej Santini-Aichel, who used this style in renovating medieval monastic buildings or in Pilgrimage Church of Saint John of Nepomuk.

During the 19th century, the revival architectural styles were very popular in the Bohemian monarchy. Many churches were restored to their presumed medieval appearance and there were constructed many new buildings in the Neo-Romanesque, Neo-Gothic and Neo-Renaissance styles (National Theatre, Lednice–Valtice Cultural Landscape, Cathedral of St. Peter and Paul in Brno). At the turn of the 19th and 20th centuries the new art style appeared in the Czech lands – Art Nouveau. The best-known representatives of Czech Art Nouveau architecture were Osvald Polívka, who designed the Municipal House in Prague, Josef Fanta, the architect of the Prague Main Railway Station, Jan Letzel, Josef Hoffmann and Jan Kotěra.

Bohemia contributed an unusual style to the world's architectural heritage when Czech architects attempted to transpose the Cubism of painting and sculpture into architecture (House of the Black Madonna). During the first years of the independent Czechoslovakia (after 1918), a specifically Czech architectural style, called "Rondo-Cubism", came into existence. Together with the pre-war Czech Cubist architecture it is unparalleled elsewhere in the world. The first Czechoslovak president T. G. Masaryk invited the prominent Slovene architect Jože Plečnik to Prague, where he modernized the Castle and built some other buildings (Church of the Most Sacred Heart of Our Lord).

Between World Wars I and II, Functionalism, with its sober, progressive forms, took over as the main architectural style in the newly established Czechoslovak Republic. In the city of Brno, one of the most impressive functionalist works has been preserved – Villa Tugendhat, designed by the architect Ludwig Mies van der Rohe. The most significant Czech architects of this era were Adolf Loos, Pavel Janák and Josef Gočár.

After World War II and the Communist coup in 1948, art in Czechoslovakia became strongly Soviet influenced. Hotel International in Prague is a brilliant example of the so-called Socialist realism, the Stalinistic art style of the 1950s. The Czechoslovak avant-garde artistic movement known as the "Brussels style" (named after the Brussels World's Fair Expo 58) became popular in the time of political liberalization of Czechoslovakia in the 1960s. Brutalism dominated in the 1970s and 1980s (Kotva Department Store).

Even today, the Czech Republic is not shying away from the most modern trends of international architecture. This fact is attested to by a number of projects by world-renowned architects (Frank Gehry and his Dancing House, Jean Nouvel, Ricardo Bofill, and John Pawson). There are also contemporary Czech architects whose works can be found all over the world (Vlado Milunić, Eva Jiřičná, Jan Kaplický).

In a strict sense, Czech literature is the literature written in the Czech language. A more liberal definition incorporates all literary works written in the Czech lands regardless of language. The literature from the area of today's Czech Republic was mostly written in Czech, but also in Latin and German or even Old Church Slavonic. Thus Franz Kafka, who—while bilingual in Czech and German—wrote his works ("The Trial", "The Castle") in German, during the era of Austrian rule, can represent the Czech, German or Austrian literature depending on the point of view.

Influential Czech authors who wrote in Latin include Cosmas of Prague († 1125), Martin of Opava († 1278), Peter of Zittau († 1339), John Hus († 1415), Bohuslav Hasištejnský z Lobkovic (1461–1510), Jan Dubravius (1486–1553), Tadeáš Hájek (1525–1600), Johannes Vodnianus Campanus (1572–1622), John Amos Comenius (1592–1670), and Bohuslav Balbín (1621–1688).

In the second half of the 13th century, the royal court in Prague became one of the centers of the German Minnesang and courtly literature (Reinmar von Zweter, Heinrich von Freiberg, Ulrich von Etzenbach, Wenceslaus II of Bohemia). The most famous Czech medieval German-language work is the "Ploughman of Bohemia" ("Der Ackermann aus Böhmen"), written around 1401 by Johannes von Tepl. The heyday of Czech German-language literature can be seen in the first half of the 20th century, which is represented by the well-known names of Franz Kafka, Max Brod, Franz Werfel, Rainer Maria Rilke, Karl Kraus, Egon Erwin Kisch, and others.

Bible translations played an important role in the development of Czech literature and the standard Czech language. The oldest Czech translation of the Psalms originated in the late 13th century and the first complete Czech translation of the Bible was finished around 1360. The first complete printed Czech Bible was published in 1488 (Prague Bible). The first complete Czech Bible translation from the original languages was published between 1579 and 1593 and is known as the Bible of Kralice. The Codex Gigas from the 12th century is the largest extant medieval manuscript in the world.

Czech-language literature can be divided into several periods: the Middle Ages (Chronicle of Dalimil); the Hussite period (Tomáš Štítný ze Štítného, Jan Hus, Petr Chelčický); the Renaissance humanism (Henry the Younger of Poděbrady, Luke of Prague, Wenceslaus Hajek, Jan Blahoslav, Daniel Adam z Veleslavína); the Baroque period (John Amos Comenius, Adam Václav Michna z Otradovic, Bedřich Bridel, Jan František Beckovský); the Enlightenment and Czech reawakening in the first half of the 19th century (Václav Matěj Kramerius, Karel Hynek Mácha, Karel Jaromír Erben, Karel Havlíček Borovský, Božena Němcová, Ján Kollár, Josef Kajetán Tyl), modern literature in second half of the 19th century (Jan Neruda, Alois Jirásek, Viktor Dyk, Jaroslav Vrchlický, Julius Zeyer, Svatopluk Čech); the avant-garde of the interwar period (Karel Čapek, Jaroslav Hašek, Vítězslav Nezval, Jaroslav Seifert, Jiří Wolker, Vladimír Holan); the years under Communism and the Prague Spring (Josef Škvorecký, Bohumil Hrabal, Milan Kundera, Arnošt Lustig, Václav Havel, Pavel Kohout, Ivan Klíma); and the literature of the post-Communist Czech Republic (Ivan Martin Jirous, Michal Viewegh, Jáchym Topol, Patrik Ouředník, Kateřina Tučková).

Noted journalists include Julius Fučík, Milena Jesenská, and Ferdinand Peroutka.

Jaroslav Seifert was the only Czech writer awarded the Nobel Prize in Literature. The famous antiwar comedy novel "The Good Soldier Švejk" by Jaroslav Hašek is the most translated Czech book in history. It was adapted by Karel Steklý in two color films "The Good Soldier Schweik" in 1956 and 1957. Widely translated Czech books are also Milan Kundera's "The Unbearable Lightness of Being" and Karel Čapek's "War with the Newts".

The international literary award the Franz Kafka Prize is awarded in the Czech Republic.

The Czech Republic has the densest network of libraries in Europe. At its center stands the National Library of the Czech Republic, based in the baroque complex Klementinum.

Czech literature and culture played a major role on at least two occasions when Czechs lived under oppression and political activity was suppressed. On both of these occasions, in the early 19th century and then again in the 1960s, the Czechs used their cultural and literary effort to strive for political freedom, establishing a confident, politically aware nation.

The musical tradition of the Czech lands arose from first church hymns, whose first evidence is suggested at the break of 10th and 11th century. The first significant pieces of Czech music include two chorales, which in their time performed the function of anthems: "Hospodine pomiluj ny" (Lord, Have Mercy on Us) from around 1050, unmistakably the oldest and most faithfully preserved popular spiritual song to have survived to the present, and the hymn "Svatý Václave" (Saint Wenceslas) or "Saint Wenceslas Chorale" from around 1250. Its roots can be found in the 12th century and it still belongs to the most popular religious songs to this day. In 1918, in the beginning of the Czechoslovak state, the song was discussed as one of the possible choices for the national anthem. The authorship of the anthem "Lord, Have Mercy on Us" is ascribed by some historians to Saint Adalbert of Prague (sv.Vojtěch), bishop of Prague, living between 956 and 997.

The wealth of musical culture in the Czech Republic lies in the long-term high-culture classical music tradition during all historical periods, especially in the Baroque, Classicism, Romantic, modern classical music and in the traditional folk music of Bohemia, Moravia and Silesia. Since the early era of artificial music, Czech musicians and composers have often been influenced the folk music of the region and dances (e.g. the polka, which originated in Bohemia). Among the most notable Czech composers are Adam Michna, Jan Dismas Zelenka, Jan Václav Antonín Stamic, Jiří Antonín Benda, Jan Křtitel Vaňhal, Josef Mysliveček, Heinrich Biber, Antonín Rejcha, František Xaver Richter, František Brixi and Jan Ladislav Dussek in baroque era, Bedřich Smetana and Antonín Dvořák in romanticism, Gustav Mahler, Josef Suk, Leoš Janáček, Bohuslav Martinů, Vítězslav Novák, Zdeněk Fibich, Alois Hába, Viktor Ullmann, Ervín Schulhoff, Pavel Haas, Josef Bohuslav Foerster in modern classical music, Miloslav Kabeláč and Petr Eben in contemporary classical music.

Other examples of famous musicians, interpreters and conductors are František Benda, Rafael Kubelík, Jan Kubelík, David Popper, Alice Herz-Sommer, Rudolf Serkin, Heinrich Wilhelm Ernst, Otakar Ševčík, Václav Neumann, Václav Talich, Karel Ančerl, Jiří Bělohlávek, Wojciech Żywny, Emma Destinnová, Magdalena Kožená, Rudolf Firkušný, Czech Philharmonic Orchestra, Panocha Quartet or non-classical musicians: Julius Fučík (brass band), Karel Svoboda and Erich Wolfgang Korngold (film music), Ralph Benatzky, Rudolf Friml and Oskar Nedbal (operetta), Jan Hammer and Karel Gott (pop), Jaroslav Ježek and Miroslav Vitouš (jazz), Karel Kryl (folk).

Czech music can be considered to have been beneficial in both the European and worldwide context, several times co-determined or even determined a newly arriving era in musical art, above all of Classical era, as well as by original attitudes in Baroque, Romantic and modern classical music. The most famous Czech musical works are Smetana's "The Bartered Bride" and "Má vlast", Dvořák's "New World Symphony", "Rusalka" and "Slavonic Dances" or Janáček's "Sinfonietta" and operas, above all "Jenůfa".

The most famous music festival in the country is Prague Spring International Music Festival of classical music, a permanent showcase for outstanding performing artists, symphony orchestras and chamber music ensembles of the world.

The roots of Czech theatre can be found in the Middle Ages, especially in cultural life of gothic period. In the 19th century, the theatre played an important role in the national awakening movement and later, in the 20th century it became a part of the modern European theatre art. Original Czech cultural phenomenon came into being at the end of the 1950s. This project called Laterna magika (The Magic Lantern) was the brainchild of renowned film and theater director Alfred Radok, resulting in productions that combined theater, dance and film in a poetic manner, considered the first multimedia art project in international context.

The most famous Czech drama is Karel Čapek's play "R.U.R.", which introduced the word "robot".

The tradition of Czech cinematography started in the second half of the 1890s. Peaks of the production in the era of silent movies include the historical drama "The Builder of the Temple" and the social and erotic (very controversial and innovative at that time) drama "Erotikon" directed by Gustav Machatý. The early Czech sound film era was very productive, above all in mainstream genres, especially the comedies of Martin Frič or Karel Lamač. However, dramatic movies were more internationally successful. Among the most successful being the romantic drama "Ecstasy" by Gustav Machatý and the romantic "The River" by Josef Rovenský.

After the repressive period of Nazi occupation and early communist official dramaturgy of socialist realism in movies at the turn of the 1940s and 1950s with a few exceptions such as "Krakatit" by Otakar Vávra or "Men without wings" by František Čáp (awarded by Palme d'Or of the Cannes Film Festival in 1946), a new era of the Czech film began with outstanding animated films by important filmmakers such as Karel Zeman, a pioneer with special effects (culminating in successful films such as artistically exceptional "Vynález zkázy" ("A Deadly Invention"), performed in anglophone countries under the name "The Fabulous World of Jules Verne" from 1958, which combined acted drama with animation, and Jiří Trnka, the founder of the modern puppet film. This began a strong tradition of animated films (Zdeněk Miler's "Mole" etc.). Another Czech cultural phenomenon came into being at the end of the 1950s. This project called "Laterna magika" ("The Magic Lantern"), resulting in productions that combined theater, dance and film in a poetic manner, considered the first multimedia art project in international context (mentioned also in Theatre section above).

In the 1960s, so called Czech New Wave (also Czechoslovak New Wave) received international acclaim. It is linked with names of Miloš Forman, Věra Chytilová, Jiří Menzel, Ján Kadár, Elmar Klos, Evald Schorm, Vojtěch Jasný, Ivan Passer, Jan Schmidt, Juraj Herz, Juraj Jakubisko, Jan Němec, Jaroslav Papoušek, etc. The hallmark of the films of this movement were long, often improvised dialogues, black and absurd humor and the occupation of non-actors. Directors are trying to preserve natural atmosphere without refinement and artificial arrangement of scenes. The unique personality of the 1960s and the beginning of the 1970s with original manuscript, deep psychological impact and extraordinarily high quality art is the director František Vláčil. His films Marketa Lazarová, Údolí včel ("The Valley of The Bees") or Adelheid belong to the artistic peaks of Czech cinema production. The film "Marketa Lazarová" was voted the all-time best Czech movie in a prestigious 1998 poll of Czech film critics and publicists. Another internationally well-known author is Jan Švankmajer (in the beginning of the career conjoined with above mentioned project "Laterna Magika"), a filmmaker and artist whose work spans several media. He is a self-labeled surrealist known for his animations and features, which have greatly influenced many artists worldwide.

Kadár & Klos's "The Shop on Main Street" (1965), Menzel's "Closely Watched Trains" (1967) and Jan Svěrák's "Kolya" (1996) won the Academy Award for Best Foreign Language Film while six others earned a nomination: "Loves of a Blonde" (1966), "The Fireman's Ball" (1968), "My Sweet Little Village" (1986), "The Elementary School" (1991), "Divided We Fall" (2000) and "Želary" (2003).

The Czech Lion is the highest Czech award for film achievement. Herbert Lom, Karel Roden and Libuše Šafránková (known from Christmas classic "Three Nuts for Cinderella", especially popular in Norway) among the best known Czech actors.

The Barrandov Studios in Prague are the largest film studios in country and one of the largest in Europe with many many popular film locations in the country. Filmmakers have come to Prague to shoot scenery no longer found in Berlin, Paris and Vienna. The city of Karlovy Vary was used as a location for the 2006 James Bond film Casino Royale.

Karlovy Vary International Film Festival is one of the oldest in the world and has become Central and Eastern Europe's leading film event. It is also one of few film festivals have been given competitive status by the FIAPF. Other film festivals held in the country include Febiofest, Jihlava International Documentary Film Festival, One World Film Festival, Zlín Film Festival and Fresh Film Festival.

Since the Czech Republic is a democratic republic, journalists and media enjoy a great degree of freedom. There are restrictions only against writing in support of Nazism, racism or violating Czech law. The Czech press was ranked as the 23rd most free press in the World Freedom Index by Reporters Without Borders in 2017.> American Radio Free Europe/Radio Liberty has its headquarters in Prague.

The most watched main news program is TV Nova. The most trusted news webpage in the Czech Republic is ct24.cz, which is owned by Czech Television – the only national public television service – and its 24-hour news channel ČT24. Other public services include the Czech Radio and the Czech News Agency (ČTK). Privately owned television services such as TV Nova, TV Prima and TV Barrandov are also very popular, with TV Nova being the most popular channel in the Czech Republic.

Newspapers are quite popular in the Czech Republic. The best-selling daily national newspapers are Blesk (average 1.15M daily readers), Mladá fronta DNES (average 752,000 daily readers), Právo (average 260,00 daily readers) and Deník (average 72,000 daily readers).

The Czech Republic is home to several globally successful video game developers, including Illusion Softworks (2K Czech), Bohemia Interactive, Keen Software House, Amanita Design and Madfinger Games. The Czech video game development scene has a long history, and a number of Czech games were produced for the ZX Spectrum, PMD 85 and Atari systems in the 1980s. In the early 2000s, a number of Czech games achieved international acclaim, including "Hidden & Dangerous", "", "Kingdom Come: Deliverance," "Vietcong" and "". Today, the most globally successful Czech games include "ARMA", "DayZ", "Space Engineers", "Machinarium", "Euro Truck Simulator", "American Truck Simulator", "", "18 Wheels of Steel", "Bus Driver", "Shadowgun" and "Blackhole". The Czech Game of the Year Awards are held annually to recognize accomplishments in video game development.

Czech cuisine is marked by a strong emphasis on meat dishes. Pork is quite common; beef and chicken are also popular. Goose, duck, rabbit and venison are served. Fish is less common, with the occasional exception of fresh trout and carp, which is served at Christmas.

Czech beer has a long and important history. The first brewery is known to have existed in 993 and the Czech Republic has the highest beer consumption per capita in the world. The famous "pilsner style beer" (pils) originated in the western Bohemian city of Plzeň, where the world's first-ever blond lager Pilsner Urquell is still being produced, making it the inspiration for more than two-thirds of the beer produced in the world today. Further south the town of České Budějovice, known as Budweis in German, lent its name to its beer, eventually known as Budweiser Budvar. Apart from these and other major brands, the Czech Republic also has a growing number of small breweries and mini-breweries.

Tourism is slowly growing around the Southern Moravian region too, which has been producing wine since the Middle Ages; about 94% of vineyards in the Czech Republic are Moravian. Aside from slivovitz, Czech beer and wine, the Czechs also produce two unique liquors, Fernet Stock and Becherovka. Kofola is a non-alcoholic domestic cola soft drink which competes with Coca-Cola and Pepsi in popularity.

Some popular Czech dishes include:


There is also a large variety of local sausages, wurst, pâtés, and smoked and cured meats. Czech desserts include a wide variety of whipped cream, chocolate, and fruit pastries and tarts, crêpes, creme desserts and cheese, poppy-seed-filled and other types of traditional cakes such as "buchty", "koláče" and "štrúdl".
Sports play a part in the life of many Czechs, who are generally loyal supporters of their favorite teams or individuals. The two leading sports in the Czech Republic are ice hockey and football. The most watched events in the Czech Republic are Olympic Ice hockey tournaments and Ice Hockey World Championships. Tennis is also a very popular sport in the Czech Republic. The many other sports with professional leagues and structures include basketball, volleyball, team handball, track and field athletics and floorball.

The country has won 14 gold medals in summer (plus 49 as Czechoslovakia) and five gold medals (plus two as Czechoslovakia) in winter Olympic history. Famous Olympians are Věra Čáslavská, Emil Zátopek, Jan Železný, Barbora Špotáková, Martina Sáblíková, Martin Doktor, Štěpánka Hilgertová or Kateřina Neumannová. Sports legends are also runner Jarmila Kratochvílová or chess-player Wilhelm Steinitz.

Czech hockey school has a good reputation. The Czech ice hockey team won the gold medal at the 1998 Winter Olympics and has won twelve gold medals at the World Championships (including 6 as Czechoslovakia), including three straight from 1999 to 2001. Former NHL superstars Jaromír Jágr and Dominik Hašek are among the best known Czech hockey players of all time as well as current Czech NHL star David Pastrňák of the Boston Bruins.

The Czechoslovakia national football team was a consistent performer on the international scene, with eight appearances in the FIFA World Cup Finals, finishing in second place in 1934 and 1962. The team also won the European Football Championship in 1976, came in third in 1980 and won the Olympic gold in 1980. After dissolution of Czechoslovakia, the Czech national football team finished in second (1996) and third (2004) place at the European Football Championship. The most famous Czech footballers were Oldřich Nejedlý, Antonín Puč, František Plánička, Josef Bican, Josef Masopust (Ballon d'or 1962), Ladislav Novák, Svatopluk Pluskal, Antonín Panenka, Ivo Viktor, Pavel Nedvěd (Ballon d'or 2003), Karel Poborský, Vladimír Šmicer, Jan Koller, Milan Baroš, Marek Jankulovski, Tomáš Rosický and Petr Čech.

The Czech Republic also has a great influence in tennis, with such players as Karolína Plíšková, Tomáš Berdych, Jan Kodeš, Jaroslav Drobný, Hana Mandlíková, Wimbledon Women's Singles winners Petra Kvitová and Jana Novotná, 8-time Grand Slam singles champion Ivan Lendl, and 18-time Grand Slam champion Martina Navratilova.

The Czech Republic men's national volleyball team won a silver medal at the 1964 Summer Olympics and two gold medals in the FIVB Volleyball World Championship in 1956 and 1966. Czech Republic women's national basketball team won the EuroBasket 2005 Women. Czechoslovakia national basketball team won EuroBasket 1946. Czech Republic will host the EuroBasket 2021 along with Georgia (Tbilisi), Germany (Berlin, Cologne) and Italy (Milan). It will take place in Prague for the Group Phase matches. Czech Republic hosted the EuroBasket Women 2017 recently.

Sport is a source of strong waves of patriotism, usually rising several days or weeks before an event. The events considered the most important by Czech fans are: the Ice Hockey World Championships, Olympic Ice hockey tournament, UEFA European Football Championship, UEFA Champions League, FIFA World Cup and qualification matches for such events. In general, any international match of the Czech ice hockey or football national team draws attention, especially when played against a traditional rival.

Czechs are also generally keen on engaging in sports activities themselves. One of the most popular sports Czechs do is hiking, mainly in the mountains. The word for "tourist" in the Czech language, "turista", also means "trekker" or "hiker". For hikers, thanks to the more than 120-year-old tradition, there is a Czech Hiking Markers System of trail blazing, that has been adopted by countries worldwide. There is a network of around 40,000 km of marked short- and long-distance trails crossing the whole country and all the Czech mountains.

The most significant sports venues are Eden Arena (e.g. 2013 UEFA Super Cup, 2015 UEFA European Under-21 Championship; home venue of SK Slavia Prague), O2 Arena (2015 European Athletics Indoor Championships, 2015 IIHF World Championship; home venue of HC Sparta Prague), Generali Arena (home venue of AC Sparta Prague), Masaryk Circuit (annual Czech Republic motorcycle Grand Prix), Strahov Stadium (mass games of Sokol and Spartakiades in communist era), Tipsport Arena (1964 World Men's Handball Championship, EuroBasket 1981, 1990 World Men's Handball Championship; home venue of ex-KHL's HC Lev Praha) and Stadion Evžena Rošického (1978 European Athletics Championships).





Government

Statistics

Trade

Travel


</doc>
<doc id="5322" url="https://en.wikipedia.org/wiki?curid=5322" title="Czechoslovakia">
Czechoslovakia

Czechoslovakia, or Czecho-Slovakia (; Czech and , "Česko-Slovensko"), was a sovereign state in Central Europe that existed from October 1918, when it declared its independence from the Austro-Hungarian Empire, until its peaceful dissolution into the Czech Republic and Slovakia on 1 January 1993.

From 1939 to 1945, following its forced division and partial incorporation into Nazi Germany, the state did not "de facto" exist but its government-in-exile continued to operate.

From 1948 to 1990, Czechoslovakia was part of the Eastern Bloc with a command economy. Its economic status was formalized in membership of Comecon from 1949 and its defense status in the Warsaw Pact of May 1955. A period of political liberalization in 1968, known as the Prague Spring, was forcibly ended when the Soviet Union, assisted by several other Warsaw Pact countries, invaded Czechoslovakia. In 1989, as Marxist–Leninist governments and socialism were ending all over Europe, Czechoslovaks peacefully deposed their government in the Velvet Revolution; state price controls were removed after a period of preparation. In 1993, Czechoslovakia split into the two sovereign states of the Czech Republic and Slovakia.


The country was of generally irregular terrain. The western area was part of the north-central European uplands. The eastern region was composed of the northern reaches of the Carpathian Mountains and lands of the Danube River basin.

The weather is mild winters and mild summers. Influenced by the Atlantic Ocean from the west, Baltic Sea from the north, and Mediterranean Sea from the south. There is no continental weather.


The area was long a part of the Austro-Hungarian Empire until the empire collapsed at the end of World War I. The new state was founded by Tomáš Garrigue Masaryk (1850–1937), who served as its first president from 14 November 1918 to 14 December 1935. He was succeeded by his close ally, Edvard Beneš (1884–1948).

The roots of Czech nationalism go back to the 19th century, when philologists and educators, influenced by Romanticism, promoted the Czech language and pride in the Czech people. Nationalism became a mass movement in the second half of the 19th century. Taking advantage of the limited opportunities for participation in political life under Austrian rule, Czech leaders such as historian František Palacký (1798–1876) founded many patriotic, self-help organizations which provided a chance for many of their compatriots to participate in communal life prior to independence. Palacký supported Austro-Slavism and worked for a reorganized and federal Austrian Empire, which would protect the Slavic speaking peoples of Central Europe against Russian and German threats.

An advocate of democratic reform and Czech autonomy within Austria-Hungary, Masaryk was elected twice to the "Reichsrat" (Austrian Parliament), first from 1891 to 1893 for the Young Czech Party, and again from 1907 to 1914 for the Czech Realist Party, which he had founded in 1889 with Karel Kramář and Josef Kaizl.

During World War I small numbers of Czechs and Slovaks, the Czechoslovak Legions, fought with the Allies in France and Italy, while large numbers deserted to Russia in exchange for its support for the independence of Czechoslovakia from the Austrian Empire. With the outbreak of World War I, Masaryk began working for Czech independence in a union with Slovakia. With Edvard Beneš and Milan Rastislav Štefánik, Masaryk visited several Western countries and won support from influential publicists.

The Bohemian Kingdom ceased to exist in 1918 when it was incorporated into Czechoslovakia. Czechoslovakia was founded in October 1918, as one of the successor states of the Austro-Hungarian Empire at the end of World War I and as part of the Treaty of Saint-Germain-en-Laye. It consisted of the present day territories of Bohemia, Moravia, Slovakia and Carpathian Ruthenia. Its territory included some of the most industrialized regions of the former Austria-Hungary.

The new country was a multi-ethnic state, with Czechs and Slovaks as "constituent peoples". The population consisted of Czechs (51%), Slovaks (16%), Germans (22%), Hungarians (5%) and Rusyns (4%). Many of the Germans, Hungarians, Ruthenians and Poles and some Slovaks, felt oppressed because the political elite did not generally allow political autonomy for minority ethnic groups. This policy led to unrest among the non-Czech population, particularly in German-speaking Sudetenland, which initially had proclaimed itself part of the Republic of German-Austria in accordance with the self-determination principle.

The state proclaimed the official ideology that there were no separate Czech and Slovak nations, but only one nation of Czechoslovaks (see Czechoslovakism), to the disagreement of Slovaks and other ethnic groups. Once a unified Czechoslovakia was restored after World War II (after the country had been divided during the war), the conflict between the Czechs and the Slovaks surfaced again. The governments of Czechoslovakia and other eastern European nations deported ethnic Germans to the West, reducing the presence of minorities in the nation. Most of the Jews had been killed during the war by the Nazis and their allies.

During the period between the two world wars, democracy thrived in Czechoslovakia. Of all the new states established in central Europe after 1918, only Czechoslovakia preserved a democratic government until the war broke out. Thus, despite regional disparities, its level of development was much higher than that of neighboring states. The population was generally literate, and contained fewer alienated groups. The influence of these conditions was augmented by the political values of Czechoslovakia's leaders and the policies they adopted. Under Tomas Masaryk, Czech and Slovak politicians promoted progressive social and economic conditions that served to defuse discontent.

Foreign minister Beneš became the prime architect of the Czechoslovak-Romanian-Yugoslav alliance (the "Little Entente", 1921–38) directed against Hungarian attempts to reclaim lost areas. Beneš worked closely with France. Far more dangerous was the German element, which after 1933 became allied with the Nazis in Germany. The increasing feeling of inferiority among the Slovaks, who were hostile to the more numerous Czechs, weakened the country in the late 1930s. Many Slovaks supported an extreme nationalist movement and welcomed the puppet Slovak state set up under Hitler's control in 1939.

After 1933, Czechoslovakia remained the only democracy in central and eastern Europe.

In September 1938, Adolf Hitler demanded control of the Sudetenland. On 29 September 1938, Britain and France ceded control in the Appeasement at the Munich Conference; France ignored the military alliance it had with Czechoslovakia. During October 1938, Nazi Germany occupied and annexed the Sudetenland border region, effectively crippling Czechoslovak defences.

On 15 March 1939, the remainder ("rump") of Czechoslovakia was invaded and divided into the Protectorate of Bohemia and Moravia and the puppet Slovak State.

Much of Slovakia and all of Carpathian Ruthenia were annexed by Hungary. Poland occupied Zaolzie, an area whose population was majority Polish, in October 1938.

The eventual goal of the German state under Nazi leadership was to eradicate Czech nationality through assimilation, deportation, and extermination of the Czech intelligentsia; the intellectual elites and middle class made up a considerable number of the 200,000 people who passed through concentration camps and the 250,000 who died during German occupation. Under Generalplan Ost, it was assumed that around 50% Czechs would be fit for Germanization. The Czech intellectual elites were to be removed not only from Czech territories but from Europe completely. The authors of Generalplan Ost believed it would be best if they emigrated overseas, as even in Siberia they were considered a threat to German rule. Just like Jews, Poles, Serbs, and several other nations, Czechs were considered to be untermenschen by the Nazi state. In 1940, in a secret Nazi plan for the Germanization of the Protectorate of Bohemia and Moravia it was declared that those considered to be of racially Mongoloid origin and the Czech intelligentsia were not to be Germanized.

The deportation of Jews to concentration camps was organized under the direction of Reinhard Heydrich, and the fortress town of Terezín was made into a ghetto way station for Jewish families. On 4 June 1942 Heydrich died after being wounded by an assassin in Operation Anthropoid. Heydrich's successor, Colonel General Kurt Daluege, ordered mass arrests and executions and the destruction of the villages of Lidice and Ležáky. In 1943 the German war effort was accelerated. Under the authority of Karl Hermann Frank, German minister of state for Bohemia and Moravia, some 350,000 Czech laborers were dispatched to the Reich. Within the protectorate, all non-war-related industry was prohibited. Most of the Czech population obeyed quiescently up until the final months preceding the end of the war, while thousands were involved in the resistance movement.

For the Czechs of the Protectorate Bohemia and Moravia, German occupation was a period of brutal oppression. Czech losses resulting from political persecution and deaths in concentration camps totaled between 36,000 and 55,000. The Jewish population of Bohemia and Moravia (118,000 according to the 1930 census) was virtually annihilated. Many Jews emigrated after 1939; more than 70,000 were killed; 8,000 survived at Terezín. Several thousand Jews managed to live in freedom or in hiding throughout the occupation.

Despite the estimated 136,000 deaths at the hands of the Nazi regime, the population in the Reichsprotektorate saw a net increase during the war years of approximately 250,000 in line with an increased birth rate.

On 6 May 1945, the third US Army of General Patton entered Pilsen from the south west. On 9 May 1945, Soviet Red Army troops entered Prague.

After World War II, pre-war Czechoslovakia was re-established, with the exception of Subcarpathian Ruthenia, which was annexed by the Soviet Union and incorporated into the Ukrainian Soviet Socialist Republic. The Beneš decrees were promulgated concerning ethnic Germans (see Potsdam Agreement) and ethnic Hungarians. Under the decrees, citizenship was abrogated for people of German and Hungarian ethnic origin who had accepted German or Hungarian citizenship during the occupations. In 1948, this provision was cancelled for the Hungarians, but only partially for the Germans. The government then confiscated the property of the Germans and expelled about 90% of the ethnic German population, over 2 million people. Those who remained were collectively accused of supporting the Nazis after the Munich Agreement, as 97.32% of Sudeten Germans had voted for the NSDAP in the December 1938 elections. Almost every decree explicitly stated that the sanctions did not apply to antifascists. Some 250,000 Germans, many married to Czechs, some antifascists, and also those required for the post-war reconstruction of the country, remained in Czechoslovakia. The Beneš Decrees still cause controversy among nationalist groups in the Czech Republic, Germany, Austria and Hungary.

Carpathian Ruthenia (Podkarpatská Rus) was occupied by (and in June 1945 formally ceded to) the Soviet Union. In the 1946 parliamentary election, the Communist Party of Czechoslovakia was the winner in the Czech lands, and the Democratic Party won in Slovakia. In February 1948 the Communists seized power. Although they would maintain the fiction of political pluralism through the existence of the National Front, except for a short period in the late 1960s (the Prague Spring) the country had no liberal democracy. Since citizens lacked significant electoral methods of registering protest against government policies, periodically there were street protests that became violent. For example, there were riots in the town of Plzeň in 1953, reflecting economic discontent. Police and army units put down the rebellion, and hundreds were injured but no one was killed. While its economy remained more advanced than those of its neighbors in Eastern Europe, Czechoslovakia grew increasingly economically weak relative to Western Europe.

The currency reform of 1953 caused dissatisfaction among Czechoslovak laborers. Prior to World War II, the Czech purchasing power surpassed that of the Soviet Union by 115-144%. This disparity was noted after Czechoslovakia came under the Soviet Bloc. To equalize the wage rate, Czechoslovaks had to turn in their old money for new at a decreased value. This lowered the real value of wages by about 11%. The banks also confiscated savings and bank deposits to control the amount of money in circulation. The economy continued to suffer as production achievements of bituminous coal was less than anticipated. Bituminous coal powered 85% of Czechoslovakia's economy. Because of low production, coal was utilized in industry only. Pre-war years, consumers used both coal and lignite for fuel, however due to low production, coal was for industrial use only which meant the consumer was only able to utilize lignite. In 1929, a typical family of four consumed approximately 2.34 tons of lignite, but by 1953 it was allowed to use only 1.6-1.8 tons per year.

In 1968, when the reformer Alexander Dubček was appointed to the key post of First Secretary of the Czechoslovak Communist Party, there was a brief period of liberalization known as the Prague Spring. In response, after failing to persuade the Czechoslovak leaders to change course, five other members of the Warsaw Pact invaded. Soviet tanks rolled into Czechoslovakia on the night of 20–21 August 1968. Soviet Communist Party General Secretary Leonid Brezhnev viewed this intervention as vital for the preservation of the Soviet, socialist system and vowed to intervene in any state that sought to replace Marxism-Leninism with capitalism. In the week after the invasion there was a spontaneous campaign of civil resistance against the occupation. This resistance involved a wide range of acts of non-cooperation and defiance: this was followed by a period in which the Czechoslovak Communist Party leadership, having been forced in Moscow to make concessions to the Soviet Union, gradually put the brakes on their earlier liberal policies. In April 1969 Dubček was finally dismissed from the First Secretaryship of the Czechoslovak Communist Party. Meanwhile, one plank of the reform program had been carried out: in 1968-69, Czechoslovakia was turned into a federation of the Czech Socialist Republic and Slovak Socialist Republic. The theory was that under the federation, social and economic inequities between the Czech and Slovak halves of the state would be largely eliminated. A number of ministries, such as education, now became two formally equal bodies in the two formally equal republics. However, the centralized political control by the Czechoslovak Communist Party severely limited the effects of federalization.

The 1970s saw the rise of the dissident movement in Czechoslovakia, represented among others by Václav Havel. The movement sought greater political participation and expression in the face of official disapproval, manifested in limitations on work activities, which went as far as a ban on professional employment, the refusal of higher education for the dissidents' children, police harassment and prison.

In 1989, the Velvet Revolution restored democracy. This occurred at around the same time as the fall of communism in Romania, Bulgaria, Hungary and Poland.

The word "socialist" was removed from the country's full name on 29 March 1990 and replaced by "federal".

In 1992, because of growing nationalist tensions in the government, Czechoslovakia was peacefully dissolved by parliament. On 1 January 1993 it formally separated into two independent countries, the Czech Republic and the Slovak Republic.

After World War II, a political monopoly was held by the Communist Party of Czechoslovakia (KSČ). Gustáv Husák was elected first secretary of the KSČ in 1969 (changed to general secretary in 1971) and president of Czechoslovakia in 1975. Other parties and organizations existed but functioned in subordinate roles to the KSČ. All political parties, as well as numerous mass organizations, were grouped under umbrella of the National Front. Human rights activists and religious activists were severely repressed.

Czechoslovakia had the following constitutions during its history (1918–1992):


In the 1930s, the nation formed a military alliance with France, which collapsed in the Munich Agreement of 1938. After World War II, active participant in Council for Mutual Economic Assistance (Comecon), Warsaw Pact, United Nations and its specialized agencies; signatory of conference on Security and Cooperation in Europe.


Before World War II, the economy was about the fourth in all industrial states in Europe. The state was based on strong economy, manufacturing cars (Škoda, Tatra), trams, aircraft (Aero, Avia), ships, ship engines (Škoda), canons, shoes (Baťa), turbines, guns (Zbrojovka Brno). It was the industrial workshop for Austro-Hungarian empire. The Slovak lands were more in agriculture.

After World War II, the economy was centrally planned, with command links controlled by the communist party, similarly to the Soviet Union. The large metallurgical industry was dependent on imports of iron and non-ferrous ores.

After World War II, the country was short of energy, relying on imported crude oil and natural gas from Soviet Union, domestic brown coal, and nuclear and hydroelectric energy. Energy constraints were a major factor in the 1980s.

Slightly after the foundation of Czechoslovakia in 1918, there was a lack of needful infrastructure in many areas – paved roads, railways, bridges etc. Massive improvement in the following years enabled Czechoslovakia to develop its industry. Prague's civil airport in Ruzyně became one of the most modern terminals in the world, when it was finished in 1937. Tomáš Baťa, Czech entrepreneur and visionary outlined his ideas in the publication "Budujme stát pro 40 milionů lidí", where he described the future motorway system. Construction of the first motorways in Czechoslovakia begun in 1939, nevertheless, they were stopped after Nazi occupation during the World War II.

Education was free at all levels and compulsory from age 6 to 15. The vast majority of the population was literate. There was a highly developed system of apprenticeship training and vocational schools supplemented general secondary schools and institutions of higher education.

In 1991: Roman Catholics 46%, Evangelical Lutheran 5.3%, Atheist 30%, n/a 17%, but there were huge differences in religious practices between the two constituent republics; see Czech Republic and Slovakia.

After World War II, free health care was available to all citizens. National health planning emphasised preventive medicine; factory and local health care centres supplemented hospitals and other inpatient institutions. There was substantial improvement in rural health care during the 1960s and 1970s.

During the era between the World Wars, Czechoslovak democracy and liberalism facilitated conditions for free publication. The most significant daily newspapers in these times were Lidové noviny, Národní listy, Český deník and Československá republika.

During Communist rule, the mass media in Czechoslovakia were controlled by the Communist Party. Private ownership of any publication or agency of the mass media was generally forbidden, although churches and other organizations published small periodicals and newspapers. Even with this information monopoly in the hands of organizations under KSČ control, all publications were reviewed by the government's Office for Press and Information.

The Czechoslovakia national football team was a consistent performer on the international scene, with eight appearances in the FIFA World Cup Finals, finishing in second place in 1934 and 1962. The team also won the European Football Championship in 1976, came in third in 1980 and won the Olympic gold in 1980.

Well-known football players such as Pavel Nedvěd, Antonín Panenka, Milan Baroš, Tomáš Rosický, Vladimír Šmicer or Petr Čech were all born in Czechoslovakia.

The International Olympic Committee code for Czechoslovakia is TCH, which is still used in historical listings of results.

The Czechoslovak national ice hockey team won many medals from the world championships and Olympic Games. Peter Šťastný, Jaromír Jágr, Dominik Hašek, Peter Bondra, Petr Klíma, Marián Gáborík, Marián Hossa, Miroslav Šatan and Pavol Demitra all come from Czechoslovakia.

Emil Zátopek, winner of four Olympic gold medals in athletics, is considered one of the top athletes in Czechoslovak history.

Věra Čáslavská was an Olympic gold medallist in gymnastics, winning seven gold medals and four silver medals. She represented Czechoslovakia in three consecutive Olympics.

Several accomplished professional tennis players including Ivan Lendl, Jan Kodeš, Miloslav Mečíř, Hana Mandlíková, Martina Hingis, Martina Navratilova, Jana Novotna, Petra Kvitová and Daniela Hantuchová were born in Czechoslovakia.






Maps with Hungarian-language rubrics:


</doc>
<doc id="5323" url="https://en.wikipedia.org/wiki?curid=5323" title="Computer science">
Computer science

Computer science (sometimes called computation science) is the study of processes that interact with data and that can be represented as data in the form of programs. It enables the use of algorithms to manipulate, store, and communicate digital information. A computer scientist studies the theory of computation and the practice of designing software systems.

Its fields can be divided into theoretical and practical disciplines. Computational complexity theory is highly abstract, while computer graphics emphasizes real-world applications. Programming language theory considers approaches to the description of computational processes, while software engineering involves the use of programming languages and complex systems. Human–computer interaction considers the challenges in making computers useful, usable, and accessible.

The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.

Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first "automatic mechanical calculator", his Difference Engine, in 1822, which eventually gave him the idea of the first "programmable mechanical calculator", his Analytical Engine. He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer". "A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".

During the 1940s, as new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC were developed, the term "computer" came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.

Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population. It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704 and later the IBM 709 computers, which were widely used during the exploration period of such devices. "Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again". During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.

The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, called the Transistor Computer. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications. The metal–oxide–silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. The MOSFET made it possible to build high-density integrated circuit chips, leading to what is known as the computer revolution or microcomputer revolution.

Time has seen significant improvements in the usability and effectiveness of computing technology. Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of humanitarian aid was needed for efficient use—in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.

Although first proposed in 1956, the term "computer science" appears in a 1959 article in "Communications of the ACM",
in which Louis Fein argues for the creation of a "Graduate School in Computer Sciences" analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term "computing science", to emphasize precisely that difference. Danish scientist Peter Naur suggested the term "datalogy", to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.

Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the "Communications of the ACM"—"turingineer", "turologist", "flow-charts-man", "applied meta-mathematician", and "applied epistemologist". Three months later in the same journal, "comptologist" was suggested, followed next year by "hypologist". The term "computics" has also been suggested. In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. "informatique" (French), "Informatik" (German), "informatica" (Italian, Dutch), "informática" (Spanish, Portuguese), "informatika" (Slavic languages and Hungarian) or "pliroforiki" ("πληροφορική", which means informatics) in Greek. Similar words have also been adopted in the UK (as in "the School of Informatics of the University of Edinburgh").
"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain."

A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes." The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, statistics, and logic.

Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.

The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.

The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.

A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.
CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: "theory of computation", "algorithms and data structures", "programming methodology and languages", and "computer elements and architecture". In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.

"Theoretical Computer Science" is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies. All studies related to mathematical, logic and formal concepts and methods could be considered as theoretical computer science, provided that the motivation is clearly drawn from the field of computing.

Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.

According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated?" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.

The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.

Information theory is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.

Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.

Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.

Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.

Computer performance analysis is the study of work flowing through computers with the general goals of improving throughput, controlling response time, using resources efficiently, eliminating bottlenecks, and predicting performance under anticipated peak loads.
Benchmarks are used to compare the performance of systems carrying different chips and/or system architectures.

Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.

This branch of computer science aims to manage networks between computers worldwide.

Computer security is a branch of computer technology with an objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.

A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages.

Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.

Research that develops theories, principles, and guidelines for user interface designers, so they can create satisfactory user experiences with desktop, laptop, and mobile devices.

Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.

Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.

Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal maintenance and arrangement.

The philosopher of computing Bill Rapaport noted three "Great Insights of Computer Science":


Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:


Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.

Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.

Computer Science, known by its near synonyms, Computing, Computer Studies, Information Technology (IT) and Information and Computing Technology (ICT), has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.

In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.

Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.

In many countries, there is a significant gender gap in computer science education. In 2012, only 20 percent of computer science degrees in the United States were awarded to women. The gender gap is also a problem in other western countries. The gap is smaller, or nonexistent, in some parts of the world. In 2011, women earned half of the computer science degrees in Malaysia. In 2001, 55 percent of computer science graduates in Guyana were women.










</doc>
<doc id="5324" url="https://en.wikipedia.org/wiki?curid=5324" title="Catalan">
Catalan

Catalan may refer to:
From, or related to Catalonia:




Mathematical concepts named after mathematician Eugène Catalan:





</doc>
<doc id="5326" url="https://en.wikipedia.org/wiki?curid=5326" title="Creationism">
Creationism

Creationism is the religious belief that nature, and aspects such as the universe, Earth, life, and humans, originated with supernatural acts of divine creation. 
In its broadest sense, creationism includes a continuum of religious views, 
which vary in their acceptance or rejection of scientific explanations such as evolution that describe the origin and development of natural phenomena.

The term "creationism" most often refers to belief in special creation; the claim that the universe and lifeforms were created as they exist today by divine action, and that the only true explanations are those which are compatible with a Christian fundamentalist literal interpretation of the creation myths found in the Bible's Genesis creation narrative. Since the 1970s, the commonest form of this has been young Earth creationism which posits special creation of the universe and lifeforms within the last 10,000 years on the basis of Flood geology, and promotes pseudoscientific creation science. From the 18th century onwards, old Earth creationism accepted geological time harmonized with Genesis through gap or day-age theory, while supporting anti-evolution. Modern old-Earth creationists support progressive creationism and continue to reject evolutionary explanations. Following political controversy, creation science was reformulated as intelligent design and neo-creationism.

Mainline Protestants and the Catholic Church reconcile modern science with their faith in Creation through forms of theistic evolution which hold that God purposefully created through the laws of nature, and accept evolution. Some groups call their belief evolutionary creationism.

Less prominently, there are also members of the Islamic and Hindu faiths who are creationists.

Use of the term "creationist" in this context dates back to Charles Darwin's unpublished 1842 sketch draft for what became "On the Origin of Species", and he used the term later in letters to colleagues. Asa Gray published a 1873 article in "The Nation" saying a "special creationist" maintaining that species "were supernaturally originated just as they are, by the very terms of his doctrine places them out of the reach of scientific explanation."

The basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Old Testament, especially from stories from the book of Genesis:


A further important element is the interpretation of the Biblical chronology, the elaborate system of life-spans, "generations," and other means by which the Bible measures the passage of events from the creation (Genesis 1:1) to the Book of Daniel, the last biblical book in which it appears. Recent decades have seen attempts to de-link creationism from the Bible and recast it as science; these include creation science and intelligent design. There are also non-Christian forms of creationism, notably Islamic creationism and Hindu creationism.

To counter the common misunderstanding that the creation–evolution controversy was a simple dichotomy of views, with "creationists" set against "evolutionists", Eugenie Scott of the National Center for Science Education produced a diagram and description of a continuum of religious views as a spectrum ranging from extreme literal Biblical creationism to materialist evolution, grouped under main headings. This was used in public presentations, then published in 1999 in "Reports of the NCSE". Other versions of a "taxonomy" of creationists were produced, and comparisons made between the different groupings. In 2009 Scott produced a revised continuum taking account of these issues, emphasising that intelligent design creationism overlaps other types, and each type is a grouping of various beliefs and positions. The revised diagram is labelled to shows a spectrum relating to positions on the age of the Earth, and the part played by special creation as against evolution. This was published in the book "Evolution Vs. Creationism: An Introduction", and the NCSE website rewritten on the basis of the book version.

The main general types are listed below.

Young Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, literally as described in the Genesis creation narrative, within the approximate time-frame of biblical genealogies (detailed for example in the Ussher chronology). Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Creationist cosmologies give the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.

The Christian organizations Institute for Creation Research (ICR) and the Creation Research Society (CRS) both promote young Earth creationism in the US. Another organization with similar views, Answers in Genesis (AiG)—based in both the U.S. and the United Kingdom—has opened the Creation Museum in Petersburg, Kentucky, to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the US, and the UK. Among Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas. In 2007, Ken Ham founded the Creation Museum and Ark Encounter in northern Kentucky.

Old Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.

Old Earth creationism itself comes in at least three types:

Gap creationism, also called "restoration creationism," holds that life was recently created on a pre-existing old Earth. This version of creationism relies on a particular interpretation of . It is considered that the words "formless" and "void" in fact denote waste and ruin, taking into account the original Hebrew and other places these words are used in the Old Testament. Genesis 1:1–2 is consequently translated:

Thus, the six days of creation (verse 3 onwards) start sometime after the Earth was "without form and void." This allows an indefinite "gap" of time to be inserted after the original creation of the universe, but prior to the creation according to Genesis, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.

Some gap creationists expand the basic version of creationism by proposing a "primordial creation" of biological life within the "gap" of time. This is thought to be "the world that then was" mentioned in 2 Peter 3:3–7. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this "world that then was," which may also be associated with Lucifer's rebellion. These views became popular with publications of Hebrew Lexicons such as "Strong's Concordance", and Bible commentaries such as the "Scofield Reference Bible" and "The Companion Bible".

Day-age creationism states that the "six days" of the Book of Genesis are not ordinary 24-hour days, but rather much longer periods (for instance, each "day" could be the equivalent of millions, or billions of years of human time). The physicist Gerald Schroeder is one such proponent of this view. This version of creationism often states that the Hebrew word "yôm," in the context of Genesis 1, can be properly interpreted as "age."

Strictly speaking, day-age creationism is not so much a version of creationism as a hermeneutic option which may be combined with other versions of creationism such as progressive creationism.

Progressive creationism holds that species have changed or evolved in a process continuously guided by God, with various ideas as to how the process operated—though it is generally taken that God directly intervened in the natural order at key moments in Earth history. This view accepts most of modern physical science including the age of the Earth, but rejects much of modern evolutionary biology or looks to it for evidence that evolution by natural selection alone is incorrect. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.

Progressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.

Creation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in "created kinds" or "Baramin" (see creationist biology) due to mutations.

Neo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in "Edwards v. Aguillard" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.

One of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term "Darwinism", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.

Unlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.

Intelligent design (ID) is the pseudoscientific view that "certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection." All of its leading proponents are associated with the Discovery Institute, a think tank whose Wedge strategy aims to replace the scientific method with "a science consonant with Christian and theistic convictions" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as "intelligent design creationism."

ID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.

In the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and "cannot uncouple itself from its creationist, and thus religious, antecedents," and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in "Edwards v. Aguillard" and "Epperson v. Arkansas" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.

In astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the Cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.

Articles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis for example, Joshua 10:12 where the Sun and Moon are said to stop in the sky, and Psalms 93:1 where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published "Galileo Was Wrong: The Church Was Right" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. Most contemporary creationist organizations reject such perspectives.

The Omphalos hypothesis argues that in order for the world to be functional, God must have created a mature Earth with mountains and canyons, rock strata, trees with growth rings, and so on; therefore "no" evidence that we can see of the presumed age of the Earth and age of the universe can be taken as reliable. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the "starlight problem". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.

Theistic evolution, or evolutionary creation, is a belief that "the personal God of the Bible created the universe and life through evolutionary processes." According to the American Scientific Affiliation:

Through the 19th century the term "creationism" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of "Vestiges of the Natural History of Creation", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When "On the Origin of Species" was published, the cleric Charles Kingsley wrote of evolution as "just as noble a conception of Deity." Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to "creation," though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or "modus operandi", of the first cause, design, and published a pamphlet defending the book in theistic terms, "Natural Selection not inconsistent with Natural Theology". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.

Some theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as "evolutionary creation." In Evolution versus Creationism, Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.

It generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a "literal" description, but rather as a literary framework or allegory.

From a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.

In one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies "have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man." Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as "creationism" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation–evolution controversy, its proponents generally take the "evolutionist" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.

While supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. 

In the creation myth taught by Bahá'u'lláh, the Bahá'í Faith founder, the universe has "neither beginning nor ending," and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in "Some Answered Questions", "Paris Talks" and "The Promulgation of Universal Peace". `Abdu'l-Bahá described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.

, most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation myth. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.

Most contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former Archbishop of Canterbury, Rowan Williams, "...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time."

Leaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.

Many Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical "Humani generis". In 1996, Pope John Paul II stated that "new knowledge has led to the recognition of the theory of evolution as more than a hypothesis," but, referring to previous papal writings, he concluded that "if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God."

In the US, Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life.

Jehovah's Witnesses adhere to a combination of gap creationism and day-age creationism, asserting that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length.

The historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1–2:3 and Genesis 2:4–25, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.

Christian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or "spiritual" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.

Hindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: "Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago." Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.

Islamic creationism is the belief that the universe (including humanity) was directly created by God as explained in the Qur'an. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Qur'an are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.

Islam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Qur'an. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.

Writing for "The Boston Globe", Drake Bennett noted: "Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims." However, some Muslims, such as Adnan Oktar (also known as Harun Yahya), do not agree that one species can develop from another.

Since the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.

There are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the universe, Big Bang and Big Crunch theories:

The Ahmadiyya movement actively promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that humans have gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of Godas opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a "guided evolution," viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus "Revelation, Rationality, Knowledge & Truth" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.

For Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for "world" in the Hebrew language—עולם (Olam)—means hidden—נעלם (Neh-Eh-Lahm). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the universe was first determined are in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.

Some contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson Others state that although the world was physically created in six 24 hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.

Most vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in "Science", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.

A 2009 Nielsen poll showed that 23% of Australians believe "the biblical account of human origins," 42% believe in a "wholly scientific" explanation for the origins of life, while 32% believe in an evolutionary process "guided by God".

A 2013 survey conducted by Auspoll and the Australian Academy of Science found that 80% of Australians believe in evolution (70% believe it is currently occurring, 10% believe in evolution but do not think it is currently occurring), 12% were not sure and 9% stated they do not believe in evolution.

A 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked "Where did human beings come fromdid we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?"

In Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.

In the UK, a 2006 poll on the "origin and development of life", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake.

In Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.

There continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled "The dangers of creationism in education" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.

Serbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana Čolić, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. "After a deluge of protest from scientists, teachers and opposition parties" says the BBC report, Čolić's deputy made the statement, "I have come here to confirm Charles Darwin is still alive" and announced that the decision was reversed. Čolić resigned after the government said that she had caused "problems that had started to reflect on the work of the entire government."

Poland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Mirosław Orzechowski, denounced evolution as "one of many lies" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, "as long as most scientists in our country say that it is the right theory." Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.

A 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. Another 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that "God created humans in their present form at one time within the last 10,000 years" when asked for their views on the origin and development of human beings, which Gallup noted was the lowest level in 35 years.

According to a 2014 Gallup poll, about 42% of Americans believe that "God created human beings pretty much in their present form at one time within the last 10,000 years or so." Another 31% believe that "human beings have developed over millions of years from less advanced forms of life, but God guided this process,"and 19% believe that "human beings have developed over millions of years from less advanced forms of life, but God had no part in this process."

Belief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, "Newsweek" reported: "By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'"

A 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.

According to a study published in "Science", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).

According to a 2011 Fox News poll, 45% of Americans believe in Creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).

In September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.

In the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to "Teach the Controversy" in science classes have conflated science with religion.

People for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with Creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:
In such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.

Most Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an "endeavor designed to demonstrate that religion and science can be compatible."

In his 2002 article "Intelligent Design as a Theological Problem," George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking "of a God who acted openly and left his fingerprints on all the evidence."). Murphy argues that this view of God is incompatible with the Christian understanding of God as "the one revealed in the cross and resurrection of Christ." The basis of this theology is Isaiah 45:15, "Verily thou art a God that hidest thyself, O God of Israel, the Saviour."

Murphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or "empty" himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:

Let this mind be in you, which was also in Christ Jesus: Who, being in the form of God, thought it not robbery to be equal with God: But made himself of no reputation, and took upon him the form of a servant, and was made in the likeness of men: And being found in fashion as a man, he humbled himself, and became obedient unto death, even the death of the cross.

Murphy concludes that,Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.For Murphy, a theology of the cross requires that Christians accept a "methodological" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a "metaphysical" naturalism, which proposes that nature is all that there is.

The Jesuit priest George Coyne has stated that is "unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis." He argues that "...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God."

Other Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was "a kind of category mistake, as if the Bible were a theory like other theories." He also said: "My worry is creationism can end up reducing the doctrine of creation rather than enhancing it." The views of the Episcopal Churcha major American-based branch of the Anglican Communionon teaching creationism resemble those of Williams.

The National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.

In April 2010, the American Academy of Religion issued "Guidelines for Teaching About Religion in K‐12 Public Schools in the United States", which included guidance that creation science or intelligent design should not be taught in science classes, as "Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning." However, they, as well as other "worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others."

Randy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article "The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?" They conclude that "Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States."

Science is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.

Some scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.






</doc>
<doc id="5329" url="https://en.wikipedia.org/wiki?curid=5329" title="History of Chad">
History of Chad

Chad (; ), officially the Republic of Chad, is a landlocked country in West Africa. It borders Libya to the north, Sudan to the east, the Central African Republic to the south, Cameroon and Nigeria to the southwest, and Niger to the west. Due to its distance from the sea and its largely desert climate, the country is sometimes referred to as the "Dead Heart of Africa".

The territory now known as Chad possesses some of the richest archaeological sites in Africa. A hominid skull was found by Michel Brunet in 2002, in Borkou, that is more than 7 million years old, the oldest discovered anywhere in the world; it has been given the name Sahelanthropus tchadensis. In 1996 Michel Brunet had unearthed a hominid jaw which he named Australopithecus bahrelghazali, and unofficially dubbed Abel. It was dated using Beryllium based Radiometric dating as living circa. 3.6 million years ago.

During the 7th millennium BC, the northern half of Chad was part of a broad expanse of land, stretching from the Indus River in the east to the Atlantic Ocean in the west, in which ecological conditions favored early human settlement. Rock art of the "Round Head" style, found in the Ennedi region, has been dated to before the 7th millennium BC and, because of the tools with which the rocks were carved and the scenes they depict, may represent the oldest evidence in the Sahara of Neolithic industries. Many of the pottery-making and Neolithic activities in Ennedi date back further than any of those of the Nile Valley to the east.

In the prehistoric period, Chad was much wetter than it is today, as evidenced by large game animals depicted in rock paintings in the Tibesti and Borkou regions.

Recent linguistic research suggests that all of Africa's major language groupings south of the Sahara Desert (except Khoisan, which is not considered a valid genetic grouping anyway), i. e. the Afro-Asiatic, Nilo-Saharan and Niger–Congo phyla, originated in prehistoric times in a narrow band between Lake Chad and the Nile Valley. The origins of Chad's peoples, however, remain unclear. Several of the proven archaeological sites have been only partially studied, and other sites of great potential have yet to be mapped.

At the end of the 1st millennium AD, the formation of states began across central Chad in the sahelian zone between the desert and the savanna. For almost the next 1,000 years, these states, their relations with each other, and their effects on the peoples who lived in stateless societies along their peripheries dominated Chad's political history. Recent research suggests that indigenous Africans founded of these states, not migrating Arabic-speaking groups, as was believed previously. Nonetheless, immigrants, Arabic-speaking or otherwise, played a significant role, along with Islam, in the formation and early evolution of these states.

Most states began as kingdoms, in which the king was considered divine and endowed with temporal and spiritual powers. All states were militaristic (or they did not survive long), but none was able to expand far into southern Chad, where forests and the tsetse fly complicated the use of cavalry. Control over the trans-Saharan trade routes that passed through the region formed the economic basis of these kingdoms. Although many states rose and fell, the most important and durable of the empires were Kanem-Bornu, Baguirmi, and Ouaddai, according to most written sources (mainly court chronicles and writings of Arab traders and travelers).

The Kanem Empire originated in the 9th century AD to the northeast of Lake Chad. Historians agree that the leaders of the new state were ancestors of the Kanembu people. Toward the end of the 11th century the Sayfawa king (or "mai", the title of the Sayfawa rulers) Hummay, converted to Islam. In the following century the Sayfawa rulers expanded southward into Kanem, where was to rise their first capital, Njimi. Kanem's expansion peaked during the long and energetic reign of Mai Dunama Dabbalemi (c. 1221–1259).
By the end of the 14th century, internal struggles and external attacks had torn Kanem apart. Finally, around 1396 the Bulala invaders forced "Mai" Umar Idrismi to abandon Njimi and move the Kanembu people to Bornu on the western edge of Lake Chad. Over time, the intermarriage of the Kanembu and Bornu peoples created a new people and language, the Kanuri, and founded a new capital, Ngazargamu.

Kanem-Bornu peaked during the reign of the outstanding statesman "Mai" Idris Aluma (c. 1571–1603). Aluma is remembered for his military skills, administrative reforms, and Islamic piety. The administrative reforms and military brilliance of Aluma sustained the empire until the mid-17th century, when its power began to fade. By the early 19th century, Kanem-Bornu was clearly an empire in decline, and in 1808 Fulani warriors conquered Ngazargamu. Bornu survived, but the Sayfawa dynasty ended in 1846 and the Empire itself fell in 1893.

In addition to Kanem-Bornu, two other states in the region, Baguirmi and Ouaddai, achieved historical prominence. Baguirmi emerged to the southeast of Kanem-Bornu in the 16th century. Islam was adopted, and the state became a sultanate. Absorbed into Kanem-Bornu, Baguirmi broke free later in the 17th century, only to be returned to tributary status in the mid-18th century. Early in the 19th century, Baguirmi fell into decay and was threatened militarily by the nearby kingdom of Ouaddai. Although Baguirmi resisted, it accepted tributary status in order to obtain help from Ouaddai in putting down internal dissension. When the capital was burned in 1893, the sultan sought and received protectorate status from the French.

Located in northeast of Baguirmi, Ouaddai was a non-Muslim kingdom that emerged in the 16th century as an offshoot of the state of Darfur (in present-day Sudan). Early in the 12th century, groups in the region rallied to Abd al-Karim Sabun, who overthrew the ruling Tunjur group, transforming Ouaddai into an Islamic sultanate. During much of the 18th century, Ouaddai resisted reincorporation into Darfur.

In about 1804, under the rule of Sabun, the sultanate began to expand its power. A new trade route north was discovered, and Sabun outfitted royal caravans to take advantage of it. He began minting his own coinage and imported chain mail, firearms, and military advisers from North Africa. Sabun's successors were less able than he, and Darfur took advantage of a disputed political succession in 1838 to put its own candidate in power. This tactic backfired when Darfur's choice, Muhammad Sharif, rejected Darfur and asserted his own authority. In doing so, he gained acceptance from Ouaddai's various factions and went on to become Ouaddai's ablest ruler. Sharif eventually established Ouaddai's hegemony over Baguirmi and kingdoms as far away as the Chari River. The Ouaddai opposed French domination until well into the 20th century.

The French first penetrated Chad in 1891, establishing their authority through military expeditions primarily against the Muslim kingdoms. The decisive colonial battle for Chad was fought on April 22, 1900 at Battle of Kousséri between forces of French Major Amédée-François Lamy and forces of the Sudanese warlord Rabih az-Zubayr. Both leaders were killed in the battle.

In 1905, administrative responsibility for Chad was placed under a governor-general stationed at Brazzaville, capital of French Equatorial Africa (AEF). Chad did not have a separate colonial status until 1920, when it was placed under a lieutenant-governor stationed in Fort-Lamy (today N'Djamena).

Two fundamental themes dominated Chad's colonial experience with the French: an absence of policies designed to unify the territory and an exceptionally slow pace of modernization. In the French scale of priorities, the colony of Chad ranked near the bottom, and the French came to perceive Chad primarily as a source of raw cotton and untrained labour to be used in the more productive colonies to the south.

Throughout the colonial period, large areas of Chad were never governed effectively: in the huge BET Prefecture, the handful of French military administrators usually left the people alone, and in central Chad, French rule was only slightly more substantive. Truly speaking, France managed to govern effectively only the south.

During World War II, Chad was the first French colony to rejoin the Allies (August 26, 1940), after the defeat of France by Germany. Under the administration of Félix Éboué, France's first black colonial governor, a military column, commanded by Colonel Philippe Leclerc de Hauteclocque, and including two battalions of Sara troops, moved north from N'Djamena (then Fort Lamy) to engage Axis forces in Libya, where, in partnership with the British Army's Long Range Desert Group, they captured Kufra. On 21 January 1942, N'Djamena was bombed by a German aircraft.

After the war ended, local parties started to develop in Chad. The first to be born was the radical Chadian Progressive Party (PPT) in February 1947, initially headed by Panamanian born Gabriel Lisette, but from 1959 headed by François Tombalbaye. The more conservative Chadian Democratic Union (UDT) was founded in November 1947 and represented French commercial interests and a bloc of traditional leaders composed primarily of Muslim and Ouaddaïan nobility. The confrontation between the PPT and UDT was more than simply ideological; it represented different regional identities, with the PPT representing the Christian and animist south and the UDT the Islamic north.

The PPT won the May 1957 pre-independence elections thanks to a greatly expanded franchise, and Lisette led the government of the Territorial Assembly until he lost a confidence vote on 11 February 1959. After a referendum on territorial autonomy on 28 September 1958, French Equatorial Africa was dissolved, and its four constituent states – Gabon, Congo (Brazzaville), the Central African Republic, and Chad became autonomous members of the French Community from 28 November 1958. Following Lisette's fall in February 1959 the opposition leaders Gontchome Sahoulba and Ahmed Koulamallah could not form a stable government, so the PPT was again asked to form an administration - which it did under the leadership of François Tombalbaye on 26 March 1959. On 12 July 1960 France agreed to Chad becoming fully independent. On 11 August 1960, Chad became an independent country and François Tombalbaye became its first president.

One of the most prominent aspects of Tombalbaye's rule to prove itself was his authoritarianism and distrust of democracy. Already in January 1962 he banned all political parties except his own PPT, and started immediately concentrating all power in his own hands. His treatment of opponents, real or imagined, was extremely harsh, filling the prisons with thousands of political prisoners.

What was even worse was his constant discrimination against the central and northern regions of Chad, where the southern Chadian administrators came to be perceived as arrogant and incompetent. This resentment at last exploded in a tax revolt on November 1, 1965, in the Guéra Prefecture, causing 500 deaths. The year after saw the birth in Sudan of the National Liberation Front of Chad (FROLINAT), created to militarily oust Tombalbaye and the Southern dominance. It was the start of a bloody civil war.

Tombalbaye resorted to calling in French troops; while moderately successful, they were not fully able to quell the insurgency. Proving more fortunate was his choice to break with the French and seek friendly ties with Libyan president Gaddafi, taking away the rebels' principal source of supplies.

But while he had reported some success against the rebels, Tombalbaye started behaving more and more irrationally and brutally, continuously eroding his consensus among the southern elites, which dominated all key positions in the army, the civil service and the ruling party. As a consequence on April 13, 1975, several units of N'Djamena's gendarmerie killed Tombalbaye during a coup.

The coup d'état that terminated Tombalbaye's government received an enthusiastic response in N'Djamena. The southerner General Félix Malloum emerged early as the chairman of the new "junta".

The new military leaders were unable to retain for long the popularity that they had gained through their overthrow of Tombalbaye. Malloum proved himself unable to cope with the FROLINAT and at the end decided his only chance was in coopting some of the rebels: in 1978 he allied himself with the insurgent leader Hissène Habré, who entered the government as prime minister.

Internal dissent within the government led Prime Minister Habré to send his forces against Malloum's national army in the capital in February 1979. Malloum was ousted from the presidency, but the resulting civil war amongst the 11 emergent factions was so widespread that it rendered the central government largely irrelevant. At that point, other African governments decided to intervene.

A series of four international conferences held first under Nigerian and then Organization of African Unity (OAU) sponsorship attempted to bring the Chadian factions together. At the fourth conference, held in Lagos, Nigeria, in August 1979, the Lagos Accord was signed. This accord established a transitional government pending national elections. In November 1979, the Transitional Government of National Unity (GUNT) was created with a mandate to govern for 18 months. Goukouni Oueddei, a northerner, was named president; Colonel Kamougué, a southerner, Vice President; and Habré, Minister of Defense. This coalition proved fragile; in January 1980, fighting broke out again between Goukouni's and Habré's forces. With assistance from Libya, Goukouni regained control of the capital and other urban centers by year’s end. However, Goukouni’s January 1981 statement that Chad and Libya had agreed to work for the realization of complete unity between the two countries generated intense international pressure and Goukouni's subsequent call for the complete withdrawal of external forces.

Libya's partial withdrawal to the Aozou Strip in northern Chad cleared the way for Habré's forces to enter N’Djamena in June. French troops and an OAU peacekeeping force of 3,500 Nigerian, Senegalese, and Zairian troops (partially funded by the United States) remained neutral during the conflict.
Habré continued to face armed opposition on various fronts, and was brutal in his repression of suspected opponents, massacring and torturing many during his rule. In the summer of 1983, GUNT forces launched an offensive against government positions in northern and eastern Chad with heavy Libyan support. In response to Libya's direct intervention, French and Zairian forces intervened to defend Habré, pushing Libyan and rebel forces north of the 16th parallel. In September 1984, the French and the Libyan governments announced an agreement for the mutual withdrawal of their forces from Chad. By the end of the year, all French and Zairian troops were withdrawn. Libya did not honor the withdrawal accord, and its forces continued to occupy the northern third of Chad.

Rebel commando groups (Codos) in southern Chad were broken up by government massacres in 1984. In 1985 Habré briefly reconciled with some of his opponents, including the Democratic Front of Chad (FDT) and the Coordinating Action Committee of the Democratic Revolutionary Council. Goukouni also began to rally toward Habré, and with his support Habré successfully expelled Libyan forces from most of Chadian territory. A cease-fire between Chad and Libya held from 1987 to 1988, and negotiations over the next several years led to the 1994 International Court of Justice decision granting Chad sovereignty over the Aouzou strip, effectively ending Libyan occupation.

However, rivalry between Hadjerai, Zaghawa and Gorane groups within the government grew in the late 1980s. In April 1989, Idriss Déby, one of Habré's leading generals and a Zaghawa, defected and fled to Darfur in Sudan, from which he mounted a Zaghawa-supported series of attacks on Habré (a Gorane). In December 1990, with Libyan assistance and no opposition from French troops stationed in Chad, Déby’s forces successfully marched on N’Djamena. After 3 months of provisional government, Déby’s Patriotic Salvation Movement (MPS) approved a national charter on February 28, 1991, with Déby as president.

During the next two years, Déby faced at least two coup attempts. Government forces clashed violently with rebel forces, including the Movement for Democracy and Development, MDD, National Revival Committee for Peace and Democracy (CSNPD), Chadian National Front (FNT) and the Western Armed Forces (FAO), near Lake Chad and in southern regions of the country. Earlier French demands for the country to hold a National Conference resulted in the gathering of 750 delegates representing political parties (which were legalized in 1992), the government, trade unions and the army to discuss the creation of a pluralist democratic regime.

However, unrest continued, sparked in part by large-scale killings of civilians in southern Chad. The CSNPD, led by Kette Moise and other southern groups entered into a peace agreement with government forces in 1994, which later broke down. Two new groups, the Armed Forces for a Federal Republic (FARF) led by former Kette ally Laokein Barde and the Democratic Front for Renewal (FDR), and a reformulated MDD clashed with government forces from 1994 to 1995.

Talks with political opponents in early 1996 did not go well, but Déby announced his intent to hold presidential elections in June. Déby won the country’s first multi-party presidential elections with support in the second round from opposition leader Kebzabo, defeating General Kamougue (leader of the 1975 coup against Tombalbaye). Déby’s MPS party won 63 of 125 seats in the January 1997 legislative elections. International observers noted numerous serious irregularities in presidential and legislative election proceedings.

By mid-1997 the government signed peace deals with FARF and the MDD leadership and succeeded in cutting off the groups from their rear bases in the Central African Republic and Cameroon. Agreements also were struck with rebels from the National Front of Chad (FNT) and Movement for Social Justice and Democracy in October 1997. However, peace was short-lived, as FARF rebels clashed with government soldiers, finally surrendering to government forces in May 1998. Barde was killed in the fighting, as were hundreds of other southerners, most civilians.

Since October 1998, Chadian Movement for Justice and Democracy (MDJT) rebels, led by Youssuf Togoimi until his death in September 2002, have skirmished with government troops in the Tibesti region, resulting in hundreds of civilian, government, and rebel casualties, but little ground won or lost. No active armed opposition has emerged in other parts of Chad, although Kette Moise, following senior postings at the Ministry of Interior, mounted a smallscale local operation near Moundou which was quickly and violently suppressed by government forces in late 2000.

Déby, in the mid-1990s, gradually restored basic functions of government and entered into agreements with the World Bank and IMF to carry out substantial economic reforms. Oil exploitation in the southern Doba region began in June 2000, with World Bank Board approval to finance a small portion of a project, the Chad-Cameroon Petroleum Development Project, aimed at transport of Chadian crude through a 1000-km buried pipeline through Cameroon to the Gulf of Guinea. The project established unique mechanisms for World Bank, private sector, government, and civil society collaboration to guarantee that future oil revenues benefit local populations and result in poverty alleviation. Success of the project depended on multiple monitoring efforts to ensure that all parties keep their commitments. These "unique" mechanisms for monitoring and revenue management have faced intense criticism from the beginning. Debt relief was accorded to Chad in May 2001.

Déby won a flawed 63% first-round victory in May 2001 presidential elections after legislative elections were postponed until spring 2002. Having accused the government of fraud, six opposition leaders were arrested (twice) and one opposition party activist was killed following the announcement of election results. However, despite claims of government corruption, favoritism of Zaghawas, and abuses by the security forces, opposition party and labor union calls for general strikes and more active demonstrations against the government have been unsuccessful. Despite movement toward democratic reform, power remains in the hands of a northern ethnic oligarchy.

In 2003, Chad began receiving refugees from the Darfur region of western Sudan. More than 200,000 refugees fled the fighting between two rebel groups and government-supported militias known as Janjaweed. A number of border incidents led to the Chadian-Sudanese War.

The war started on December 23, 2005, when the government of Chad declared a state of war with Sudan and called for the citizens of Chad to mobilize themselves against the "common enemy," which the Chadian government sees as the Rally for Democracy and Liberty (RDL) militants, Chadian rebels, backed by the Sudanese government, and Sudanese militiamen. Militants have attacked villages and towns in eastern Chad, stealing cattle, murdering citizens, and burning houses. Over 200,000 refugees from the Darfur region of northwestern Sudan currently claim asylum in eastern Chad. Chadian president Idriss Déby accuses Sudanese President Omar Hasan Ahmad al-Bashir of trying to "destabilize our country, to drive our people into misery, to create disorder and export the war from Darfur to Chad."

An attack on the Chadian town of Adre near the Sudanese border led to the deaths of either one hundred rebels, as every news source other than CNN has reported, or three hundred rebels. The Sudanese government was blamed for the attack, which was the second in the region in three days, but Sudanese foreign ministry spokesman Jamal Mohammed Ibrahim denies any Sudanese involvement, "We are not for any escalation with Chad. We technically deny involvement in Chadian internal affairs." This attack was the final straw that led to the declaration of war by Chad and the alleged deployment of the Chadian airforce into Sudanese airspace, which the Chadian government denies.

An attack on N'Djamena was defeated on April 13, 2006 in the Battle of N'Djamena. The President on national radio stated that the situation was under control, but residents, diplomats and journalists reportedly heard shots of weapons fire.

On November 25, 2006, rebels captured the eastern town of Abeche, capital of the Ouaddaï Region and center for humanitarian aid to the Darfur region in Sudan. On the same day, a separate rebel group Rally of Democratic Forces had captured Biltine. On November 26, 2006, the Chadian government claimed to have recaptured both towns, although rebels still claimed control of Biltine. Government buildings and humanitarian aid offices in Abeche were said to have been looted. The Chadian government denied a warning issued by the French Embassy in N'Djamena that a group of rebels was making its way through the Batha Prefecture in central Chad. Chad insists that both rebel groups are supported by the Sudanese government.

Nearly 100 children at the center of an international scandal that left them stranded at an orphanage in remote eastern Chad returned home after nearly five months March 14, 2008. The 97 children were taken from their homes in October 2007 by a then-obscure French charity, Zoé's Ark, which claimed they were orphans from Sudan's war-torn Darfur region.

On Friday, February 1, 2008, rebels, an opposition alliance of leaders Mahamat Nouri, a former defense minister, and Timane Erdimi, a nephew of Idriss Déby who was his chief of staff, attacked the Chadian capital of Ndjamena - even surrounding the Presidential Palace. But Idris Deby with government troops fought back. French forces flew in ammunition for Chadian government troops but took no active part in the fighting. UN has said that up to 20,000 people left the region, taking refuge in nearby Cameroon and Nigeria. Hundreds of people were killed, mostly civilians. The rebels accuse Deby of corruption and embezzling millions in oil revenue. While many Chadians may share that assessment, the uprising appears to be a power struggle within the elite that has long controlled Chad. The French government believes that the opposition has regrouped east of the capital. Déby has blamed Sudan for the current unrest in Chad.





</doc>
<doc id="5330" url="https://en.wikipedia.org/wiki?curid=5330" title="Geography of Chad">
Geography of Chad

Chad is one of the 48 land-locked countries in the world and is located in North Central Africa, measuring , nearly twice the size of France and slightly more than three times the size of California. Most of its ethnically and linguistically diverse population lives in the south, with densities ranging from 54 persons per square kilometer in the Logone River basin to 0.1 persons in the northern B.E.T. (Borkou-Ennedi-Tibesti) desert region, which itself is larger than France. The capital city of N'Djaména, situated at the confluence of the Chari and Logone Rivers, is cosmopolitan in nature, with a current population in excess of 700,000 people. 

Chad has four bioclimatic zones. The northernmost Saharan zone averages less than of rainfall annually. The sparse human population is largely nomadic, with some livestock, mostly small ruminants and camels. The central Sahelian zone receives between rainfall and has vegetation ranging from grass/shrub steppe to thorny, open savanna. The southern zone, often referred to as the Sudan zone, receives between , with woodland savanna and deciduous forests for vegetation. Rainfall in the Guinea zone, located in Chad's southwestern tip, ranges between .

The country's topography is generally flat, with the elevation gradually rising as one moves north and east away from Lake Chad. The highest point in Chad is Emi Koussi, a mountain that rises in the northern Tibesti Mountains. The Ennedi Plateau and the Ouaddaï highlands in the east complete the image of a gradually sloping basin, which descends towards Lake Chad. There are also central highlands in the Guera region rising to .

Lake Chad is the second largest lake in west Africa and is one of the most important wetlands on the continent. Home to 120 species of fish and at least that many species of birds, the lake has shrunk dramatically in the last four decades due to increased water usage from an expanding population and low rainfall. Bordered by Chad, Niger, Nigeria, and Cameroon, Lake Chad currently covers only 1350 square kilometers, down from 25,000 square kilometers in 1963. The Chari and Logone Rivers, both of which originate in the Central African Republic and flow northward, provide most of the surface water entering Lake Chad. Chad is also next to Niger.

Located in north-central Africa, Chad stretches for about 1,800 kilometers from its northernmost point to its southern boundary. Except in the far northwest and south, where its borders converge, Chad's average width is about 800 kilometers. Its area of 1,284,000 square kilometers is roughly equal to the combined areas of Idaho, Wyoming, Utah, Nevada, and Arizona. Chad's neighbors include Libya to the north, Niger and Nigeria to the west, Sudan to the east, Central African Republic to the south, and Cameroon to the southwest.

Chad exhibits two striking geographical characteristics. First, the country is landlocked. N'Djamena, the capital, is located more than 1,100 kilometers northeast of the Atlantic Ocean; Abéché, a major city in the east, lies 2,650 kilometers from the Red Sea; and Faya-Largeau, a much smaller but strategically important center in the north, is in the middle of the Sahara Desert, 1,550 kilometers from the Mediterranean Sea. These vast distances from the sea have had a profound impact on Chad's historical and contemporary development.

The second noteworthy characteristic is that the country borders on very different parts of the African continent: North Africa, with its Islamic culture and economic orientation toward the Mediterranean Basin and West Africa, with its diverse religions and cultures and its history of highly developed states and regional economies;

Chad also borders Northeast Africa, oriented toward the Nile Valley and the Red Sea region - and Central or Equatorial Africa, some of whose people have retained classical African religions while others have adopted Christianity, and whose economies were part of the great Congo River system. Although much of Chad's distinctiveness comes from this diversity of influences, since independence the diversity has also been an obstacle to the creation of a national identity.

Although Chadian society is economically, socially, and culturally fragmented, the country's geography is unified by the Lake Chad Basin. Once a huge inland sea (the Pale-Chadian Sea) whose only remnant is shallow Lake Chad, this vast depression extends west into Nigeria and Niger. The larger, northern portion of the basin is bounded within Chad by the Tibesti Mountains in the northwest, the Ennedi Plateau in the northeast, the Ouaddaï Highlands in the east along the border with Sudan, the Guéra Massif in central Chad, and the Mandara Mountains along Chad's southwestern border with Cameroon. The smaller, southern part of the basin falls almost exclusively in Chad. It is delimited in the north by the Guéra Massif, in the south by highlands 250 kilometers south of the border with Central African Republic, and in the southwest by the Mandara Mountains.

Lake Chad, located in the southwestern part of the basin at an altitude of 282 meters, surprisingly does not mark the basin's lowest point; instead, this is found in the Bodele and Djourab regions in the north-central and northeastern parts of the country, respectively. This oddity arises because the great stationary dunes (ergs) of the Kanem region create a dam, preventing lake waters from flowing to the basin's lowest point. At various times in the past, and as late as the 1870s, the Bahr el Ghazal Depression, which extends from the northeastern part of the lake to the Djourab, acted as an overflow canal; since independence, climatic conditions have made overflows impossible.

North and northeast of Lake Chad, the basin extends for more than 800 kilometers, passing through regions characterized by great rolling dunes separated by very deep depressions. Although vegetation holds the dunes in place in the Kanem region, farther north they are bare and have a fluid, rippling character. From its low point in the Djourab, the basin then rises to the plateaus and peaks of the Tibesti Mountains in the north. The summit of this formation—as well as the highest point in the Sahara Desert—is Emi Koussi, a dormant volcano that reaches 3,414 meters above sea level.

The basin's northeastern limit is the Ennedi Plateau, whose limestone bed rises in steps etched by erosion. East of the lake, the basin rises gradually to the Ouaddaï Highlands, which mark Chad's eastern border and also divide the Chad and Nile watersheds. These highland areas are part of the East Saharan montane xeric woodlands ecoregion.

Southeast of Lake Chad, the regular contours of the terrain are broken by the Guéra Massif, which divides the basin into its northern and southern parts. South of the lake lie the floodplains of the Chari and Logone rivers, much of which are inundated during the rainy season. Farther south, the basin floor slopes upward, forming a series of low sand and clay plateaus, called koros, which eventually climb to 615 meters above sea level. South of the Chadian border, the koros divide the Lake Chad Basin from the Ubangi-Zaire river system.

Permanent streams do not exist in northern or central Chad. Following infrequent rains in the Ennedi Plateau and Ouaddaï Highlands, water may flow through depressions called enneris and wadis. Often the result of flash floods, such streams usually dry out within a few days as the remaining puddles seep into the sandy clay soil. The most important of these streams is the Batha, which in the rainy season carries water west from the Ouaddaï Highlands and the Guéra Massif to Lake Fitri.

Chad's major rivers are the Chari and the Logone and their tributaries, which flow from the southeast into Lake Chad. Both river systems rise in the highlands of Central African Republic and Cameroon, regions that receive more than 1,250 millimeters of rainfall annually. Fed by rivers of Central African Republic, as well as by the Bahr Salamat, Bahr Aouk, and Bahr Sara rivers of southeastern Chad, the Chari River is about 1,200 kilometers long. From its origins near the city of Sarh, the middle course of the Chari makes its way through swampy terrain; the lower Chari is joined by the Logone River near N'Djamena. The Chari's volume varies greatly, from 17 cubic meters per second during the dry season to 340 cubic meters per second during the wettest part of the year.

The Logone River is formed by tributaries flowing from Cameroon and Central African Republic. Both shorter and smaller in volume than the Chari, it flows northeast for 960 kilometers; its volume ranges from five to eighty-five cubic meters per second. At N'Djamena the Logone empties into the Chari, and the combined rivers flow together for thirty kilometers through a large delta and into Lake Chad. At the end of the rainy season in the fall, the river overflows its banks and creates a huge floodplain in the delta.

The seventh largest lake in the world (and the fourth largest in Africa), Lake Chad is located in the sahelian zone, a region just south of the Sahara Desert. The Chari River contributes 95 percent of Lake Chad's water, an average annual volume of 40 billion cubic meters, 95% of which is lost to evaporation. The size of the lake is determined by rains in the southern highlands bordering the basin and by temperatures in the Sahel. Fluctuations in both cause the lake to change dramatically in size, from 9,800 square kilometers in the dry season to 25,500 at the end of the rainy season.

Lake Chad also changes greatly in size from one year to another. In 1870 its maximum area was 28,000 square kilometers. The measurement dropped to 12,700 in 1908. In the 1940s and 1950s, the lake remained small, but it grew again to 26,000 square kilometers in 1963. The droughts of the late 1960s, early 1970s, and mid-1980s caused Lake Chad to shrink once again, however. The only other lakes of importance in Chad are Lake Fitri, in Batha Prefecture, and Lake Iro, in the marshy southeast.

The Lake Chad Basin embraces a great range of tropical climates from north to south, although most of these climates tend to be dry. Apart from the far north, most regions are characterized by a cycle of alternating rainy and dry seasons. In any given year, the duration of each season is determined largely by the positions of two great air masses—a maritime mass over the Atlantic Ocean to the southwest and a much drier continental mass.

During the rainy season, winds from the southwest push the moister maritime system north over the African continent where it meets and slips under the continental mass along a front called the "intertropical convergence zone". At the height of the rainy season, the front may reach as far as Kanem Prefecture. By the middle of the dry season, the intertropical convergence zone moves south of Chad, taking the rain with it. This weather system contributes to the formation of three major regions of climate and vegetation.

The Saharan region covers roughly the northern half of the country, including Borkou-Ennedi-Tibesti Prefecture along with the northern parts of Kanem, Batha, and Biltine prefectures. Much of this area receives only traces of rain during the entire year; at Faya Largeau, for example, annual rainfall averages less than . Scattered small oases and occasional wells provide water for a few date palms or small plots of millet and garden crops.

In much of the north, the average daily maximum temperature is about during January, the coolest month of the year, and about during May, the hottest month. On occasion, strong winds from the northeast produce violent sandstorms. In northern Biltine Prefecture, a region called the Mortcha plays a major role in animal husbandry. Dry for nine months of the year, it receives or more of rain, mostly during July and August.

A carpet of green springs from the desert during this brief wet season, attracting herders from throughout the region who come to pasture their cattle and camels. Because very few wells and springs have water throughout the year, the herders leave with the end of the rains, turning over the land to the antelopes, gazelles, and ostriches that can survive with little groundwater. Northern Chad averages over 3500 hours of sunlight per year, the south somewhat less.

The semiarid sahelian zone, or Sahel, forms a belt about wide that runs from Lac and Chari-Baguirmi prefectures eastward through Guéra, Ouaddaï, and northern Salamat prefectures to the Sudanese frontier. The climate in this transition zone between the desert and the southern soudanian zone is divided into a rainy season (from June to early September) and a dry period (from October to May).

In the northern Sahel, thorny shrubs and acacia trees grow wild, while date palms, cereals, and garden crops are raised in scattered oases. Outside these settlements, nomads tend their flocks during the rainy season, moving southward as forage and surface water disappear with the onset of the dry part of the year. The central Sahel is characterized by drought-resistant grasses and small woods. Rainfall is more abundant there than in the Saharan region. For example, N'Djamena records a maximum annual average rainfall of , while Ouaddaï Prefecture receives just a bit less.

During the hot season, in April and May, maximum temperatures frequently rise above . In the southern part of the Sahel, rainfall is sufficient to permit crop production on unirrigated land, and millet and sorghum are grown. Agriculture is also common in the marshlands east of Lake Chad and near swamps or wells. Many farmers in the region combine subsistence agriculture with the raising of cattle, sheep, goats, and poultry.

The humid "soudanian" zone includes the Sahel, the southern prefectures of Mayo-Kebbi, Tandjilé, Logone Occidental, Logone Oriental, Moyen-Chari, and southern Salamat. Between April and October, the rainy season brings between of precipitation. Temperatures are high throughout the year. Daytime readings in Moundou, the major city in the southwest, range from in the middle of the cool season in January to about in the hot months of March, April, and May.

The soudanian region is predominantly East Sudanian savanna, or plains covered with a mixture of tropical or subtropical grasses and woodlands. The growth is lush during the rainy season but turns brown and dormant during the five-month dry season between November and March. Over a large part of the region, however, natural vegetation has yielded to agriculture.

On 22 June, the temperature reached in Faya, breaking a record set in 1961 at the same location. Similar temperature rises were also reported in Niger, which began to enter a famine situation.

On 26 July the heat reached near-record levels over Chad and Niger.

Area:
<br>"total:"
1.284 million km²
<br>"land:"
1,259,200 km²
<br>"water:"
24,800 km²

Area - comparative:
<br>Canada: smaller than the Northwest Territories
<br>US: slightly more than three times the size of California

Land boundaries:
<br>"total:"
6,406 km
<br>"border countries:"
Cameroon 1,116 km, Central African Republic 1,556 km, Libya 1,050 km, Niger 1,196 km, Nigeria 85 km, Sudan 1,403 km

Coastline:
0 km (landlocked)

Maritime claims:
none (landlocked)

Elevation extremes:
<br>"lowest point:"
Djourab Depression 160 m
<br>"highest point:'
Emi Koussi 3,415 m

Natural resources:
petroleum, uranium, natron, kaolin, fish (Chari River, Logone River), gold, limestone, sand and gravel, salt

Land use:
<br>"arable land:"
3.89%
<br>"permanent crops:"
0.03%
<br>"other:"
96.08% (2012)

Irrigated land:
302.7 km² (2003)

Total renewable water resources:
43 km (2011)

Freshwater withdrawal (domestic/industrial/agricultural):
<br>"total:"
0.88 km/yr (12%/12%/76%)
<br>"per capita:"
84.81 m/yr (2005)

Natural hazards:
hot, dry, dusty, Harmattan winds occur in north; periodic droughts; locust plagues

Environment - current issues:
inadequate supplies of potable water; improper waste disposal in rural areas contributes to soil and water pollution; desertification

See also: 2010 Sahel famine

This is a list of the extreme points of Chad, the points that are farther north, south, east or west than any other location.


"*Note: technically Chad does not have an easternmost point, the eastern-most section of the border being formed by the 24° of longitude"




</doc>
<doc id="5331" url="https://en.wikipedia.org/wiki?curid=5331" title="Demographics of Chad">
Demographics of Chad

The people of Chad speak more than 100 different languages and divide themselves into many ethnic groups. However, language and ethnicity are not the same. Moreover, neither element can be tied to a particular physical type.

Although the possession of a common language shows that its speakers have lived together and have a common history, peoples also change languages. This is particularly so in Chad, where the openness of the terrain, marginal rainfall, frequent drought and famine, and low population densities have encouraged physical and linguistic mobility. Slave raids among non-Muslim peoples, internal slave trade, and exports of captives northward from the ninth to the twentieth centuries also have resulted in language changes.

Anthropologists view ethnicity as being more than genetics. Like language, ethnicity implies a shared heritage, partly economic, where people of the same ethnic group may share a livelihood, and partly social, taking the form of shared ways of doing things and organizing relations among individuals and groups. Ethnicity also involves a cultural component made up of shared values and a common worldview. Like language, ethnicity is not immutable. Shared ways of doing things change over time and alter a group's perception of its own identity.

Not only do the social aspects of ethnic identity change but the biological composition (or gene pool) also may change over time. Although most ethnic groups emphasize intermarriage, people are often proscribed from seeking partners among close relatives—a prohibition that promotes biological variation. In all groups, the departure of some individuals or groups and the integration of others also changes the biological component.

The Chadian government has avoided official recognition of ethnicity. With the exception of a few surveys conducted shortly after independence, little data were available on this important aspect of Chadian society. Nonetheless, ethnic identity was a significant component of life in Chad.

The peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa.

Chad's languages fall into ten major groups, each of which belongs to either the
Nilo-Saharan, Afro-Asiatic, or Niger–Congo language family. These represent three of the four major language families in Africa; only the Khoisan languages of southern Africa are not represented. The presence of such different languages suggests that the Lake Chad Basin may have been an important point of dispersal in ancient times.

According to the total population was in , compared to only 2 429 000 in 1950. The proportion of children below the age of 15 in 2010 was 45.4%, 51.7% was between 15 and 65 years of age, while 2.9% was 65 years or older

Registration of vital events is in Chad not complete. The Population Departement of the United Nations prepared the following estimates.

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Fertility data as of 2014-2015 (DHS Program):

The separation of religion from social structure in Chad represents a false dichotomy, for they are perceived as two sides of the same coin. Three religious traditions coexist in Chad- classical African religions, Islam, and Christianity. None is monolithic. The first tradition includes a variety of ancestor and/or place-oriented religions whose expression is highly specific. Islam, although characterized by an orthodox set of beliefs and observances, also is expressed in diverse ways. Christianity arrived in Chad much more recently with the arrival of Europeans. Its followers are divided into Roman Catholics and Protestants (including several denominations); as with Chadian Islam, Chadian Christianity retains aspects of pre-Christian religious belief.

The number of followers of each tradition in Chad is unknown. Estimates made in 1962 suggested that 35 percent of Chadians practiced classical African religions, 55 percent were Muslims, and 10 percent were Christians. In the 1970s and 1980s, this distribution undoubtedly changed. Observers report that Islam has spread among the Hajerai and among other non-Muslim populations of the Saharan and sahelian zones. However, the proportion of Muslims may have fallen because the birthrate among the followers of traditional religions and Christians in southern Chad is thought to be higher than that among Muslims. In addition, the upheavals since the mid-1970s have resulted in the departure of some missionaries; whether or not Chadian Christians have been numerous enough and organized enough to have attracted more converts since that time is unknown.

Demographic statistics according to the World Population Review in 2019.


The following demographic statistics are from the CIA World Factbook.

The peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa. 

About 5,000 French citizens live in Chad.


Works cited


</doc>
<doc id="5332" url="https://en.wikipedia.org/wiki?curid=5332" title="Politics of Chad">
Politics of Chad

Politics of Chad takes place in a framework of a presidential republic, whereby the President of Chad is both head of state and head of government. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Chad is one of the most corrupt countries in the world.

In May 2013, security forces in Chad foiled a coup against the President Idriss Deby that had been in preparation for several months.

A strong executive branch headed by President Idriss Déby dominates the Chadian political system. Following his military overthrow of Hissène Habré in December 1990, Déby won presidential elections in 1996 and 2001. The constitutional basis for the government is the 1996 constitution, under which the president was limited to two terms of office until Déby had that provision repealed in 2005. The president has the power to appoint the Council of State (or cabinet), and exercises considerable influence over appointments of judges, generals, provincial officials and heads of Chad’s parastatal firms. In cases of grave and immediate threat, the president, in consultation with the National Assembly President and Council of State, may declare a state of emergency. Most of the Déby's key advisors are members of the Zaghawa clan, although some southern and opposition personalities are represented in his government.

According to the 1996 constitution, the National Assembly deputies are elected by universal suffrage for 4-year terms. The Assembly holds regular sessions twice a year, starting in March and October, and can hold special sessions as necessary and called by the prime minister. Deputies elect a president of the National Assembly every 2 years. Assembly deputies or members of the executive branch may introduce legislation; once passed by the Assembly, the president must take action to either sign or reject the law within 15 days. The National Assembly must approve the prime minister’s plan of government and may force the prime minister to resign through a majority vote of no-confidence. However, if the National Assembly rejects the executive branch’s program twice in one year, the president may disband the Assembly and call for new legislative elections. In practice, the president exercises considerable influence over the National Assembly through the MPS party structure.

Despite the constitution’s guarantee of judicial independence from the executive branch, the president names most key judicial officials. The Supreme Court is made up of a chief justice, named by the president, and 15 councilors chosen by the president and National Assembly; appointments are for life. The Constitutional Council, with nine judges elected to 9-year terms, has the power to review all legislation, treaties and international agreements prior to their adoption. The constitution recognizes customary and traditional law in locales where it is recognized and to the extent it does not interfere with public order or constitutional guarantees of equality for all citizens.

ACCT, 
ACP, 
AfDB, 
AU, 
BDEAC, 
CEMAC, 
FAO, 
FZ, 
G-77, 
IBRD, 
ICAO, 
ICCt, 
ICFTU, 
ICRM, 
IDA, 
IDB, 
IFAD, 
IFC, 
IFRCS, 
ILO, 
IMF, 
Interpol, 
IOC, 
ITU, 
MIGA, 
NAM, 
OIC, 
ONUB, 
OPCW, 
UN, 
UNCTAD, 
UNESCO, 
UNIDO, 
UNOCI, 
UPU, 
WCL, 
WHO, 
WIPO, 
WMO, 
WToO, 
WTrO


</doc>
<doc id="5333" url="https://en.wikipedia.org/wiki?curid=5333" title="Economy of Chad">
Economy of Chad

The economy of Chad suffers from the landlocked country's geographic remoteness, drought, lack of infrastructure, and political turmoil. About 85% of the population depends on agriculture, including the herding of livestock. Of Africa's Francophone countries, Chad benefited least from the 50% devaluation of their currencies in January 1994. Financial aid from the World Bank, the African Development Bank, and other sources is directed largely at the improvement of agriculture, especially livestock production. Because of lack of financing, the development of oil fields near Doba, originally due to finish in 2000, was delayed until 2003. It was finally developed and is now operated by Exxon Mobil Corporation.

The following table shows the main economic indicators in 1980–2017.
GDP:
purchasing power parity - $28.62 billion (2017 est.) 

GDP - real growth rate:
-3.1% (2017 est.) 

GDP - per capita:
$2,300 (2017 est.) 

Gross national saving: 
15.5% of GDP (2017 est.)

GDP - composition by sector:
"agriculture:"
52.3% (2017 est.) 
"industry:"
14.7% (2017 est.) 
"services:"
33.1% (2017 est.) 

Population below poverty line::
46.7% (2011 est.) 

Distribution of family income - Gini index:
43.3 (2011 est.) 

Inflation rate (consumer prices):
-0.9% (2017 est.) 

Labor force:
5.654 million (2017 est.) 

Labor force - by occupation:
agriculture 80%, industry and services 20% (2006 est.)

Budget:
"revenues:"
1.337 billion (2017 est.) 
"expenditures:"
1.481 billion (2017 est.) 

Budget surplus (+) or deficit (-):
-1.5% (of GDP) (2017 est.) 

Public debt: 
52.5% of GDP (2017 est.) 
Industries:
oil, cotton textiles, brewing, natron (sodium carbonate), soap, cigarettes, construction materials 

Industrial production growth rate:
-4% (2017 est.) 

electrification: total population: 4% (2013) 

electrification: urban areas: 14% (2013)

electrification: rural areas: 1% (2013)

Electricity - production:
224.3 million kWh (2016 est.) 

Electricity - production by source:
<br>"fossil fuel:"
98%
<br>"hydro:"
0%
<br>"nuclear:"
0%
<br>"other renewable:"
3% (2017)

Electricity - consumption:
208.6 million kWh (2016 est.) 

Electricity - exports:
0 kWh (2016 est.) 

Electricity - imports:
0 kWh (2016 est.) 

Agriculture - products:
cotton, sorghum, millet, peanuts, sesame, corn, rice, potatoes, onions, cassava (manioc, tapioca), cattle, sheep, goats, camels 

Exports:
$2.464 billion (2017 est.) 

Exports - commodities:
oil, livestock, cotton, sesame, gum arabic, shea butter 

Exports - partners:
US 38.7%, China 16.6%, Netherlands 15.7%, UAE 12.2%, India 6.3% (2017) 

Imports:
$2.16 billion (2017 est.) 

Imports - commodities:
machinery and transportation equipment, industrial goods, foodstuffs, textiles 

Imports - partners:
China 19.9%, Cameroon 17.2%, France 17%, US 5.4%, India 4.9%, Senegal 4.5% (2017) 

Debt - external:
$1.724 billion (31 December 2017 est.) 

Reserves of foreign exchange and gold:
$22.9 million (31 December 2017 est.)





</doc>
<doc id="5334" url="https://en.wikipedia.org/wiki?curid=5334" title="Telecommunications in Chad">
Telecommunications in Chad

Telecommunications in Chad include radio, television, fixed and mobile telephones, and the Internet.

Radio stations:

Radios:
1.7 million (1997).

Television stations:

Television sets:
10,000 (1997).

Radio is the most important medium of mass communication. State-run Radiodiffusion Nationale Tchadienne operates national and regional radio stations. Around a dozen private radio stations are on the air, despite high licensing fees, some run by religious or other non-profit groups. The BBC World Service (FM 90.6) and Radio France Internationale (RFI) broadcast in the capital, N'Djamena. The only television station, Tele Tchad, is state-owned.

State control of many broadcasting outlets allows few dissenting views. Journalists are harassed and attacked. On rare occasions journalists are warned in writing by the High Council for Communication to produce more "responsible" journalism or face fines. Some journalists and publishers practice self-censorship. On 10 October 2012, the High Council on Communications issued a formal warning to La Voix du Paysan, claiming that the station's live broadcast on 30 September incited the public to "insurrection against the government." The station had broadcast a sermon by a bishop who criticized the government for allegedly failing to use oil wealth to benefit the region.

Calling code: +235

International call prefix: 00

Main lines:

Mobile cellular:

Telephone system: inadequate system of radiotelephone communication stations with high costs and low telephone density; fixed-line connections for less than 1 per 100 persons coupled with mobile-cellular subscribership base of only about 35 per 100 persons (2011).

Satellite earth stations: 1 Intelsat (Atlantic Ocean) (2011).

Top-level domain: .td

Internet users:

Fixed broadband: 18,000 subscriptions, 132nd in the world; 0.2% of the population, 161st in the world (2012).

Wireless broadband: Unknown (2012).

Internet hosts:

IPv4: 4,096 addresses allocated, less than 0.05% of the world total, 0.4 addresses per 1000 people (2012).

There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms.

The constitution provides for freedom of opinion, expression, and press, but the government does not always respect these rights. Private individuals are generally free to criticize the government without reprisal, but reporters and publishers risk harassment from authorities when publishing critical articles. The 2010 media law abolished prison sentences for defamation and insult, but prohibits "inciting racial, ethnic, or religious hatred," which is punishable by one to two years in prison and a fine of one to three million CFA francs ($2,000 to $6,000).




</doc>
<doc id="5335" url="https://en.wikipedia.org/wiki?curid=5335" title="Transport in Chad">
Transport in Chad

Transport infrastructure within Chad is generally poor, especially in the north and east of the country. River transport is limited to the south-west corner. As of 2011 Chad had no railways though two lines are planned - from the capital to the Sudanese and Cameroonian borders.during the wet season, especially in the southern half of the country. In the north, roads are merely tracks across the desert and land mines continue to present a danger. Draft animals (horses, donkeys and camels) remain important in much of the country.

Fuel supplies can be erratic, even in the south-west of the country, and are expensive. Elsewhere they are practically non-existent.

As of 2011 Chad had no railways. Two lines were planned to Sudan and Cameroon from the capital, with construction expected to start in 2012.No operative lines were listed as at 2019. 

As at 2018 Chad had a total of 44,000 km of roads of which approximately 260 km are paved. Some, but not all of the roads in the capital N'Djamena are paved. Outside of N'Djamena there is one paved road which runs from Massakory in the north, through N'Djamena and then south, through the cities of Guélengdeng, Bongor, Kélo and Moundou, with a short spur leading in the direction of Kousseri, Cameroon, near N'Djamena. Expansion of the road towards Cameroon through Pala and Léré is reportedly in the preparatory stages.

Most rivers flow but intermittently. On the Chari, between N’Djamena and Lake Chad, transportation is possible all year round. In September and October, the Logone is navigable between N’Djamena and Moundou, and the Chari between N’Djamena and Sarh. Total waterways cover 4,800 km (3,000 mi), of which 2,000 km (1,250 mi) are navigable all year.

As at 2012, Chari and Logone Rivers were navigable only in wet season (2002). Both flow northwards, from the south of Chad, into Lake Chad.

Since 2003, a 1,070 km pipeline has been used to export crude oil from the oil fields around Doba to offshore oil-loading facilities on Cameroon's Atlantic coast at Kribi.
The CIA World Factbook however cites only 582km of pipeline in Chad itself as at 2013.

None (landlocked).

Chad's main routes to the sea are:

In colonial times, the main access was by road to Bangui, in the Central African Republic, then by river boat to Brazzaville, and onwards by rail from Brazzaville to Pointe Noire, on Congo's Atlantic coast. This route is now little used.

There is also a route across Sudan, to the Red Sea, but very little trade goes this way.

Links with Niger, north of Lake Chad, are practically nonexistent; it is easier to reach Niger via Cameroon and Nigeria. 

 Chad had an estimated 58 airports, only 9 of which had paved runways. Air Tchad (60 percent state owned) provides internal service to 12 locations but suffers from lack of fuel and equipment. The international airport at N’Djamena was damaged in fighting in 1981, but is now served by several international carriers including Air Afrique, which is partly owned by Chad. Another major airport, developed as a military staging area, is located at Sarh. In 2015, scheduled airlines in Chad carried approximately 28,332 passengers on domestic and international flights.

Statistics on airports with paved runways as of 2017:
List of airports with paved runways:

Statistics on airports with unpaved runways as of 2013:
SAGA Airline of Chad - see http://www.airsaga.com

The Ministry is represented at the regional level by the Regional Delegations, which have jurisdiction over a part of the National Territory as defined by Decree No. 003 / PCE / CTPT / 91. Their organization and responsibilities are defined by Order No. 006 / MTPT / SE / DG / 92.

The Regional Delegations are:

Each Regional Delegation is organized into regional services, namely: the Regional Roads Service, the Regional Transport Service, the Civilian Buildings Regional Service and, as needed, other regional services may be established in one or more Delegations .




</doc>
<doc id="5336" url="https://en.wikipedia.org/wiki?curid=5336" title="Military of Chad">
Military of Chad

The military of Chad consists of the National Army (includes Ground Forces, Air Force, and Gendarmerie), Republican Guard, Rapid Intervention Force, Police, and National and Nomadic Guard (GNNT). Currently the main task of the Chadian military is to combat the various rebel forces inside the country.

From independence through the period of the presidency of Félix Malloum (1975–79), the official national army was known as the Chadian Armed Forces (Forces Armées Tchadiennes—FAT). Composed mainly of soldiers from southern Chad, FAT had its roots in the army recruited by France and had military traditions dating back to World War I. FAT lost its status as the legal state army when Malloum's civil and military administration disintegrated in 1979. Although it remained a distinct military body for several years, FAT was eventually reduced to the status of a regional army representing the south.

After Habré consolidated his authority and assumed the presidency in 1982, his victorious army, the Armed Forces of the North (Forces Armées du Nord—FAN), became the nucleus of a new national army. The force was officially constituted in January 1983, when the various pro-Habré contingents were merged and renamed the Chadian National Armed Forces (Forces Armées Nationales Tchadiennes—FANT).

The Military of Chad was dominated by members of Toubou, Zaghawa, Kanembou, Hadjerai, and Massa ethnic groups during the presidency of Hissène Habré. Current Chadian president Idriss Déby, revolted and fled to the Sudan, taking with him many Zaghawa and Hadjerai soldiers in 1989.

Chad's armed forces numbered about 36,000 at the end of the Habré regime, but swelled to an estimated 50,000 in the early days of Déby's rule. With French support, a reorganization of the armed forces was initiated early in 1991 with the goal of reducing its numbers and making its ethnic composition reflective of the country as a whole. Neither of these goals was achieved, and the military is still dominated by the Zaghawa.

In 2004, the government discovered that many of the soldiers it was paying did not exist and that there were only about 19,000 soldiers in the army, as opposed to the 24,000 that had been previously believed. Government crackdowns against the practice are thought to have been a factor in a failed military mutiny in May 2004.

The current conflict, in which the Chadian military is involved, is the civil war against Sudanese-backed rebels. Chad successfully manages to repel the rebel movements, but recently, with some losses (see Battle of N'Djamena (2008)). The army uses its artillery systems and tanks, but well-equipped insurgents have probably managed to destroy over 20 of Chad's 60 t-55 tanks, and probably shot down a Mi-24 Hind gunship, which bombed enemy positions near the border with Sudan. In November 2006 Libya supplied Chad with four Aermacchi SF.260W light attack planes. They are used to strike enemy positions by the Chadian Air Force, but one was shot down by rebels. During the last battle of N'Djamena gunships and tanks have been put to good use, pushing armed militia forces back from the Presidential palace. The battle impacted the highest levels of the army leadership, as Daoud Soumain, its Chief of Staff, was killed.

The CIA World Factbook estimates the military budget of Chad to be 4.2% of GDP as of 2006.. Given the then GDP ($7.095 bln) of the country, military spending was estimated to be about $300 million. This estimate however dropped after the end of the Civil war in Chad (2005–2010) to 2.0% as estimated by the World Bank for the year 2011. There aren't any more recent estimates available for 2012, 2013.

Chad participated in a peace mission under the authority of African Union in the neighboring Central African Republic to try to pacify the recent conflict, but has chosen to withdraw after its soldiers were accused of shooting into a marketplace, unprovoked, according to BBC. 
"Currently, Cameroon has an ongoing military-military relationship with Chad, which includes associates training for Chadian military in Cameroon. There are four brigade Chado-Cameroonian in January 2012. Cameroon and Chad are developing excellent relations".




</doc>
<doc id="5337" url="https://en.wikipedia.org/wiki?curid=5337" title="Foreign relations of Chad">
Foreign relations of Chad

The foreign relations of Chad are significantly influenced by the desire for oil revenue and investment in Chadian oil industry and support for Chadian President Idriss Déby. Chad is officially non-aligned but has close relations with France, the former colonial power. Relations with neighbouring Libya, and Sudan vary periodically. Lately, the Idris Déby regime has been waging an intermittent proxy war with Sudan. Aside from those two countries, Chad generally enjoys good relations with its neighbouring states.

Although relations with Libya improved with the presidency of Idriss Déby, strains persist. Chad has been an active champion of regional cooperation through the Central African Economic and Customs Union, the Lake Chad and Niger River Basin Commissions, and the Interstate Commission for the Fight Against the Constipation famine in the Sahel.

Delimitation of international boundaries in the vicinity of Lake Chad, the lack of which led to border incidents in the past, has been completed and awaits ratification by Cameroon, Chad, Niger, and Nigeria.

Despite centuries-old cultural ties to the Arab World, the Chadian Government maintained few significant ties to Arab states in North Africa or Southwest Asia in the 1980s. Chad has not recognised the State of Israel since former Chadian President François (Ngarta) Tombalbaye broke off relations in September 1972. President Habré hoped to pursue closer relations with Arab states as a potential opportunity to break out of his Chad's post-imperial dependence on France, and to assert Chad's unwillingness to serve as an arena for superpower rivalries. In addition, as a northern Christian, Habré represented a constituency that favored co-operation and solidarity with Arabs, both African, and Asian. For these reasons, he was expected to seize opportunities during the 1990s to pursue closer ties with the Arab World. In 1988, Chad recognized the State of Palestine, which maintains a mission in N'Djamena.

During the 1980s, Arab opinion on the Chadian-Libyan conflict over the Aozou Strip was divided. Several Arab states supported Libyan territorial claims to the Strip, among the most outspoken of which was Algeria, which provided training for anti-Habré forces, although most recruits for its training programs were from Nigeria or Cameroon, recruited and flown to Algeria by Libya. Lebanon's Progressive Socialist Party also sent troops to support Qadhafi's efforts against Chad in 1987. In contrast, numerous other Arab states opposed the Libyan actions, and expressed their desire to see the dispute over the Aozou Strip settled peacefully. By the end of 1987, Algiers and N'Djamena were negotiating to improve relations.

In November 2018, President Deby visited Israel and announced his intention to restore diplomatic relations. Chad and Israel re-established diplomatic relations in January 2019.
Chad is officially non-aligned but has close relations with France, the former colonial power, which has about 1,200 troops stationed in the capital N'Djamena. It receives economic aid from countries of the European Community, the United States, and various international organizations. Libya supplies aid and has an ambassador resident in N'Djamena. Traditionally strong ties with the Western community have weakened over the past two years due to a dispute between the Government of Chad and the World Bank over how the profits from Chad's petroleum reserves are allocated. Although oil output to the West has resumed and the dispute has officially been resolved, resentment towards what the Déby administration considered foreign meddling lingers.

Chad belongs to the following international organizations:




</doc>
<doc id="5342" url="https://en.wikipedia.org/wiki?curid=5342" title="Commentary">
Commentary

Commentary or commentaries may refer to:





</doc>
<doc id="5346" url="https://en.wikipedia.org/wiki?curid=5346" title="Colloid">
Colloid

In chemistry, a colloid is a mixture in which one substance of microscopically dispersed insoluble or soluble particles is suspended throughout another substance. Sometimes the dispersed substance alone is called the colloid; the term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word "suspension" is distinguished from colloids by larger particle size). Unlike a solution, whose solute and solvent constitute only one phase, a colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension) that arise by phase separation. To qualify as a colloid, the mixture must be one that does not settle or would take a very long time to settle appreciably.

The dispersed-phase particles have a diameter between approximately 1 and 1000 nanometers. Such particles are normally easily visible in an optical microscope, although at the smaller size range (), an ultramicroscope or an electron microscope may be required. Homogeneous mixtures with a dispersed phase in this size range may be called "colloidal aerosols", "colloidal emulsions", "colloidal foams", "colloidal dispersions", or "hydrosols". The dispersed-phase particles or droplets are affected largely by the surface chemistry present in the colloid.

Some colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color. The cytoplasm of living cells is an example of a colloid, containing many types of biomolecular condensate.

Colloidal suspensions are the subject of interface and colloid science. This field of study was introduced in 1845 by Italian chemist Francesco Selmi and further investigated since 1861 by Scottish scientist Thomas Graham.

Because the size of the dispersed phase may be difficult to measure, and because colloids have the appearance of solutions, colloids are sometimes identified and characterized by their physico-chemical and transport properties. For example, if a colloid consists of a solid phase dispersed in a liquid, the solid particles will not diffuse through a membrane, whereas with a true solution the dissolved ions or molecules will diffuse through a membrane. Because of the size exclusion, the colloidal particles are unable to pass through the pores of an ultrafiltration membrane with a size smaller than their own dimension. The smaller the size of the pore of the ultrafiltration membrane, the lower the concentration of the dispersed colloidal particles remaining in the ultrafiltered liquid. The measured value of the concentration of a truly dissolved species will thus depend on the experimental conditions applied to separate it from the colloidal particles also dispersed in the liquid. This is particularly important for solubility studies of readily hydrolyzed species such as Al, Eu, Am, Cm, or organic matter complexing these species.
Colloids can be classified as follows:
Based on the nature of interaction between the dispersed phase and the dispersion medium, colloids can be classified as: Hydrophilic colloids: The colloid particles are attracted toward water. They are also called reversible sols. Hydrophobic colloids: These are opposite in nature to hydrophilic colloids. The colloid particles are repelled by water. They are also called irreversible sols.

In some cases, a colloid suspension can be considered a homogeneous mixture. This is because the distinction between "dissolved" and "particulate" matter can be sometimes a matter of approach, which affects whether or not it is homogeneous or heterogeneous.

The following forces play an important role in the interaction of colloid particles:

There are two principal ways to prepare colloids:

The stability of a colloidal system is defined by particles remaining suspended in solution at equilibrium.

Stability is hindered by aggregation and sedimentation phenomena, which are driven by the colloid's tendency to reduce surface energy. Reducing the interfacial tension will stabilize the colloidal system by reducing this driving force. 
Aggregation is due to the sum of the interaction forces between particles. If attractive forces (such as van der Waals forces) prevail over the repulsive ones (such as the electrostatic ones) particles aggregate in clusters.

Electrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.
A combination of the two mechanisms is also possible (electrosteric stabilization). All the above-mentioned mechanisms for minimizing particle aggregation rely on the enhancement of the repulsive interaction forces.

Electrostatic and steric stabilization do not directly address the sedimentation/floating problem.

Particle sedimentation (and also floating, although this phenomenon is less common) arises from a difference in the density of the dispersed and of the continuous phase. The higher the difference in densities, the faster the particle settling.
The method consists in adding to the colloidal suspension a polymer able to form a gel network and characterized by shear thinning properties. Examples of such substances are xanthan and guar gum.

Particle settling is hindered by the stiffness of the polymeric matrix where particles are trapped. In addition, the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles.

The rheological shear thinning properties find beneficial in the preparation of the suspensions and in their use, as the reduced viscosity at high shear rates facilitates deagglomeration, mixing and in general the flow of the suspensions.

Unstable colloidal dispersions can form either "flocs" or "aggregates" as the particles assemble due to interparticle attractions. Flocs are loose and flexible conglomerates of the particles, whereas aggregates are compact and rigid entities. There are methods that distinguish between flocculation and aggregation, such as acoustic spectroscopy. Destabilization can be accomplished by different methods:

Unstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles fall to the bottom of the suspension (or float to the top if the particles are less dense than the suspending medium) once the clusters are of sufficient size for the Brownian forces that work to keep the particles in suspension to be overcome by gravitational forces. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.

Multiple light scattering coupled with vertical scanning is the most widely used technique to monitor the dispersion state of a product, hence identifying and quantifying destabilisation phenomena. It works on concentrated dispersions without dilution. When light is sent through the sample, it is backscattered by the particles / droplets. The backscattering intensity is directly proportional to the size and volume fraction of the dispersed phase. Therefore, local changes in concentration ("e.g."Creaming and Sedimentation) and global changes in size ("e.g." flocculation, coalescence) are detected and monitored.

The kinetic process of destabilisation can be rather long (up to several months or even years for some products) and it is often required for the formulator to use further accelerating methods in order to reach reasonable development time for new product design. Thermal methods are the most commonly used and consists in increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only the viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.
Mechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. However, some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Moreover, segregation of different populations of particles have been highlighted when using centrifugation and vibration.

In physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. In addition, phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.

A colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.

Thus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg’s law, in a matter analogous to the scattering of X-rays in crystalline solids.

The large number of experiments exploring the physics and chemistry of these so-called "colloidal crystals" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.

Colloidal phase separation is an important organising principle for compartmentalisation of both the cytoplasm and nucleus of cells, similar in importance to compartmentalisation via lipid bilayer membranes. The term biomolecular condensate has been used to refer to clusters of macromolecules that arise via liquid-liquid, liquid-gel, or liquid-solid phase separation within the cytosol. Macromolecular crowding strongly enhances colloidal phase separation and formation of biomolecular condensates.

Colloidal particles can also serve as transport vector
of diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks
(e.g. limestone, sandstone, granite). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (e.g. clay particles, silicates, iron oxy-hydroxides), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term ""eigencolloid"" is used to designate pure phases, i.e., pure Tc(OH), U(OH), or Am(OH). Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations
because of the process of ultrafiltration occurring in dense clay membrane.
The question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.

In soil science, the colloidal fraction in soils consists of tiny clay and humus particles that are less than 1ɥm in diameter and carry either positive and/or negative electrostatic charges that vary depending on the chemical conditions of the soil sample, i.e. soil pH.

Colloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increase the intravascular volume, whereas other types of volume expanders called crystalloids also increase the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids.


</doc>
<doc id="5347" url="https://en.wikipedia.org/wiki?curid=5347" title="Chinese">
Chinese

Chinese can refer to:





</doc>
<doc id="5350" url="https://en.wikipedia.org/wiki?curid=5350" title="Riding shotgun">
Riding shotgun

Riding shotgun was used to describe the guard who rode alongside a stagecoach driver, ready to use his shotgun to ward off bandits or hostile Native Americans. In modern use, it refers to the practice of sitting alongside the driver in a moving vehicle. The phrase has been used to mean giving actual or figurative support or aid to someone in a situation. The coining of this phrase dates to 1905 at latest.

The expression "riding shotgun" is derived from "shotgun messenger", a colloquial term for "express messenger", when stagecoach travel was popular during the American Wild West and the Colonial period in Australia. The person rode alongside the driver. The first known use of the phrase "riding shotgun" was in the 1905 novel "The Sunset Trail" by Alfred Henry Lewis. 

It was later used in print and especially film depiction of stagecoaches and wagons in the Old West in danger of being robbed or attacked by bandits. A special armed employee of the express service using the stage for transportation of bullion or cash would sit beside the driver, carrying a short shotgun (or alternatively a rifle), to provide an armed response in case of threat to the cargo, which was usually a strongbox. Absence of an armed person in that position often signaled that the stage was not carrying a strongbox, but only passengers.

On the evening of March 15, 1881, a Kinnear & Company stagecoach carrying US$26,000 in silver bullion () was en route from the boom town of Tombstone, Arizona Territory to Benson, Arizona, the nearest freight terminal. Bob Paul, who had run for Pima County Sheriff and was contesting the election he lost due to ballot-stuffing, was temporarily working once again as the Wells Fargo shotgun messenger. He had taken the reins and driver's seat in Contention City because the usual driver, a well-known and popular man named Eli "Budd" Philpot, was ill. Philpot was riding shotgun.

Near Drew's Station, just outside Contention City, a man stepped into the road and commanded them to "Hold!" Three Cowboys attempted to rob the stage. Paul, in the driver's seat, fired his shotgun and emptied his revolver at the robbers, wounding a Cowboy later identified as Bill Leonard in the groin. Philpot, riding shotgun, and passenger Peter Roerig, riding in the rear dickey seat, were both shot and killed. The horses spooked and Paul wasn't able to bring the stage under control for almost a mile, leaving the robbers with nothing. Paul, who normally rode shotgun, later said he thought the first shot killing Philpot had been meant for him.
When Wyatt Earp first arrived in Tombstone in December 1879, he initially took a job as a stagecoach shotgun messenger for Wells Fargo, guarding shipments of silver bullion. When Wyatt Earp was appointed Pima County Deputy Sheriff on July 27, 1881, his brother Morgan Earp took over his job.

When Wells, Fargo & Co. began regular stagecoach service from Tipton, Missouri to San Francisco, California in 1858, they issued shotguns to its drivers and guards for defense along the perilous 2,800 mile route. The guard was called a shotgun messenger and they were issued a Coach gun, typically a 10-gauge or 12-gauge, short, double-barreled shotgun.

More recently, the term has been applied to a game, usually played by groups of friends to determine who rides beside the driver in a car. Typically, this involves claiming the right to ride shotgun by being the first person to call out "shotgun". The game creates an environment that is fair by forgetting and leaving out most seniority except for that moms and significant others automatically get shotgun, and this meanwhile leaves out any conflicts that may have previously occurred when deciding who gets to ride shotgun. Therefore, it is best played and seen mainly within friend groups because of the lack of seniority, and it is when most people enjoy participating in games. Also, the front passenger seat is typically most wanted because of the small perks it contains like more leg room and easier access to the radio and air controls of the car. Calling shotgun does not apply to bi-directional trips; shotgun must be called before each journey when within sight of the vehicle.




</doc>
<doc id="5355" url="https://en.wikipedia.org/wiki?curid=5355" title="Cooking">
Cooking

Cooking or cookery is the art, technology, science and craft of preparing food for consumption. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. Types of cooking also depend on the skill levels and training of cooks. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, such as in ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.

Preparing food with heat or fire is an activity unique to humans. It may have started around 2 million years ago, though archaeological evidence for it reaches no more than 1 million years ago.

The expansion of agriculture, commerce, trade, and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as the invention of pottery for holding and boiling water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation to further enhance the flavor of the dish served.

Phylogenetic analysis suggests that human ancestors may have invented cooking as far back as 1.8 million to 2.3 million years ago. Re-analysis of burnt bone fragments and plant ashes from the Wonderwerk Cave in South Africa has provided evidence supporting control of fire by early humans by 1 million years ago. There is evidence that "Homo erectus" was cooking their food as early as 500,000 years ago. Evidence for the controlled use of fire by "Homo erectus" beginning some 400,000 years ago has wide scholarly support. Archaeological evidence from 300,000 years ago, in the form of ancient hearths, earth ovens, burnt animal bones, and flint, are found across Europe and the Middle East. Anthropologists think that widespread cooking fires began about 250,000 years ago when hearths first appeared.

Recently, the earliest hearths have been reported to be at least 790,000 years old.
Communication between the Old World and the New World in the Columbian Exchange influenced the history of cooking. The movement of foods across the Atlantic from the New World, such as potatoes, tomatoes, maize, beans, bell pepper, chili pepper, vanilla, pumpkin, cassava, avocado, peanut, pecan, cashew, pineapple, blueberry, sunflower, chocolate, gourds, and squash, had a profound effect on Old World cooking. The movement of foods across the Atlantic from the Old World, such as cattle, sheep, pigs, wheat, oats, barley, rice, apples, pears, peas, chickpeas, green beans, mustard, and carrots, similarly changed New World cooking.

In the seventeenth and eighteenth centuries, food was a classic marker of identity in Europe. In the nineteenth-century "Age of Nationalism" cuisine became a defining symbol of national identity.

The Industrial Revolution brought mass-production, mass-marketing, and standardization of food. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast. In the 1920s, freezing methods, cafeterias, and fast food restaurants emerged.

Starting early in the 20th century, governments issued nutrition guidelines that led to the food pyramid (introduced in Sweden in 1974). The 1916 "Food For Young Children" became the first USDA guide to give specific dietary guidelines. Updated in the 1920s, these guides gave shopping suggestions for different-sized families along with a Depression Era revision which included four cost levels. In 1943, the USDA created the "Basic Seven" chart to promote nutrition. It included the first-ever Recommended Daily Allowances from the National Academy of Sciences. In 1956, the "Essentials of an Adequate Diet" brought recommendations which cut the number of groups that American school children would learn about down to four. In 1979, a guide called "Food" addressed the link between excessive amounts of unhealthy foods and chronic diseases. Fats, oils, and sweets were added to the four basic food groups.

Most ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.

Naturally occurring ingredients contain various amounts of molecules called "proteins", "carbohydrates" and "fats". They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.

Carbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (made by enzymatic splitting of sucrose) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot and potato.

The interaction of heat and carbohydrate is complex. Long-chain sugars such as starch tend to break down into simpler sugars when cooked, while simple sugars can form syrups. If sugars are heated so that all water of crystallisation is driven off, then caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins elicits the Maillard reaction, a basic flavor-enhancing technique.

An emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.

Types of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including maize and flax oils. Fats are used in a number of ways in cooking and baking. To prepare stir fries, grilled cheese or pancakes, the pan or griddle is often coated with fat or oil. Fats are also used as an ingredient in baked goods such as cookies, cakes and pies. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying, deep frying or sautéing. Fats are used to add flavor to food (e.g., butter or bacon fat), prevent food from sticking to pans and create a desirable texture.

Edible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable – meat becomes "cooked" and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.

Cooking often involves water, frequently present in other liquids, which is both added in order to immerse the substances being cooked (typically water, stock or wine), and released from the foods themselves. A favorite method of adding flavor to dishes is to save the liquid for use in other recipes. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients – this is a critical component of both stewing and sauce making.

Vitamins and minerals are required for normal metabolism but which the body cannot manufacture itself and which must therefore come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium, sodium chloride and sulfur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking. The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure. Blanching or steaming vegetables is a way of minimizing vitamin and mineral loss in cooking.

There are very many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the end result because some foods are more appropriate to some methods than others. Some major hot cooking techniques include:


Cooking can prevent many foodborne illnesses that would otherwise occur if the food is eaten raw. When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and "Toxoplasma gondii". Food poisoning and other illness from uncooked or poorly prepared food may be caused by bacteria such as of "Escherichia coli", "Salmonella typhimurium" and "Campylobacter", viruses such as noroviruses, and protozoa such as "Entamoeba histolytica". Bacteria, viruses and parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.

The sterilizing effect of cooking depends on temperature, cooking time, and technique used. Some food spoilage bacteria such as "Clostridium botulinum" or "Bacillus cereus" can form spores that survive boiling, which then germinate and regrow after the food has cooled. This makes it unsafe to reheat cooked food more than once.

Cooking increases the digestibility of many foods which are inedible or poisonous when raw. For example, raw cereal grains are hard to digest, while kidney beans are toxic when raw or improperly cooked due to the presence of phytohaemagglutinin, which is inactivated by cooking for at least ten minutes at .

Food safety depends on the safe preparation, handling, and storage of food. Food spoilage bacteria proliferate in the "Danger zone" temperature range from , food therefore should not be stored in this temperature range. Washing of hands and surfaces, especially when handling different meats, and keeping raw food separate from cooked food to avoid cross-contamination, are good practices in food preparation. Foods prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones. Washing and disinfecting cutting boards, especially after use with raw meat, poultry, or seafood, reduces the risk of contamination.

Proponents of raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water and becomes degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin. However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.

German research in 2003 showed significant benefits in reducing breast cancer risk when large amounts of raw vegetable matter are included in the diet. The authors attribute some of this effect to heat-labile phytonutrients. Sulforaphane, a glucosinolate breakdown product, which may be found in vegetables such as broccoli, has been shown to be protective against prostate cancer, however, much of it is destroyed when the vegetable is boiled.

The USDA has studied retention data for 16 vitamins, 8 minerals, and alcohol for approximately 290 foods for various cooking methods.

In a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers. Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet. Some of these cancers may be caused by carcinogens in food generated during the cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk. Many foods, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.

Several studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While avoiding meat or eating meat raw may be the only ways to avoid HCAs in meat fully, the National Cancer Institute states that cooking meat below creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90% by reducing the time needed for the meat to be cooked at high heat. Nitrosamines are found in some food, and may be produced by some cooking processes from proteins or from nitrites used as food preservatives; cured meat such as bacon has been found to be carcinogenic, with links to colon cancer. Ascorbate, which is added to cured meat, however, reduces nitrosamine formation.

Research has shown that grilling, barbecuing and smoking meat and fish increases levels of carcinogenic polycyclic aromatic hydrocarbons (PAH). In Europe, grilled meat and smoked fish generally only contribute a small proportion of dietary PAH intake since they are a minor component of diet – most intake comes from cereals, oils and fats. However, in the US, grilled/barbecued meat is the second highest contributor of the mean daily intake of a known PAH carcinogen benzo[a]pyrene at 21% after ‘bread, cereal and grain’ at 29%.

Baking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide, a known carcinogen from animal studies; its potential to cause cancer in humans at normal exposures is uncertain. Public health authorities recommend reducing the risk by avoiding overly browning starchy foods or meats when frying, baking, toasting or roasting them.

Cooking dairy products may reduce a protective effect against colon cancer. Researchers at the University of Toronto suggest that ingesting uncooked or unpasteurized dairy products (see also Raw milk) may reduce the risk of colorectal cancer. Mice and rats fed uncooked sucrose, casein, and beef tallow had one-third to one-fifth the incidence of microadenomas as the mice and rats fed the same ingredients cooked. This claim, however, is contentious. According to the Food and Drug Administration of the United States, health benefits claimed by raw milk advocates do not exist. "The small quantities of antibodies in milk are not absorbed in the human intestinal tract," says Barbara Ingham, PhD, associate professor and extension food scientist at the University of Wisconsin-Madison. "There is no scientific evidence that raw milk contains an anti-arthritis factor or that it enhances resistance to other diseases."

Heating sugars with proteins or fats can produce advanced glycation end products ("glycotoxins").

Deep fried food in restaurants may contain high level of trans fat, which is known to increase levels of low-density lipoprotein that in turn may increase risk of heart diseases and other conditions. However, many fast food chains have now switched to trans-fat-free alternatives for deep-frying.

The application of scientific knowledge to cooking and gastronomy has become known as molecular gastronomy. This is a subdiscipline of food science. Important contributions have been made by scientists, chefs and authors such as Herve This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Heston Blumenthal (chef), Ferran Adria (chef), Robert Wolke (chemist, author) and Pierre Gagnaire (chef).

Chemical processes central to cooking include the Maillard reaction – a form of non-enzymatic browning involving an amino acid, a reducing sugar and heat.

Home cooking has traditionally been a process carried out informally in a home or around a communal fire, and can be enjoyed by all members of the family, although in many cultures women bear primary responsibility. Cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many "ready-to-eat" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal. The nutritional value of including more commercially prepared foods has been found to be inferior to home-made foods. Home-cooked meals tend to be healthier with fewer calories, and less saturated fat, cholesterol and sodium on a per calorie basis while providing more fiber, calcium, and iron. The ingredients are also directly sourced, so there is control over authenticity, taste, and nutritional value. The superior nutritional quality of home-cooking could therefore play a role in preventing chronic disease. Cohort studies following the elderly over 10 years show that adults who cook their own meals have significantly lower mortality, even when controlling for confounding variables.

"Home-cooking" may be associated with comfort food, and some commercially produced foods and restaurant meals are presented through advertising or packaging as having been "home-cooked," regardless of their actual origin. This trend began in the 1920s and is attributed to people in urban areas of the U.S. wanting homestyle food even though their schedules and smaller kitchens made cooking harder.




</doc>
<doc id="5360" url="https://en.wikipedia.org/wiki?curid=5360" title="Card game">
Card game

A card game is any game using playing cards as the primary device with which the game is played, be they traditional or game-specific.

Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules, but most are folk games whose rules vary by region, culture, and person.

A card game is played with a "deck" or "pack" of playing cards which are identical in size and shape. Each card has two sides, the "face" and the "back". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single "pack" or "shoe".

Games using playing cards exploit the fact that cards are individually identifiable from one side only, so that each player knows only the cards they hold and not those held by anyone else. For this reason card games are often characterized as games of chance or “imperfect information”—as distinct from games of strategy or “perfect information,” where the current position is fully visible to all players throughout the game. Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their gameplay.

Some games that are placed in the card game genre involve a board. The distinction is that the gameplay of a card game chiefly depends on the use of the cards by players (the board is simply a guide for scorekeeping or for card placement), while board games (the principal non-card game genre to use cards) generally focus on the players' positions on the board, and use the cards for some secondary purpose.

The object of a trick-taking game is based on the play of multiple rounds, or tricks, in each of which each player plays a single card from their hand, and based on the values of played cards one player wins or "takes" the trick. The specific object varies with each game and can include taking as many tricks as possible, taking as many scoring cards within the tricks won as possible, taking as few tricks (or as few penalty cards) as possible, taking a particular trick in the hand, or taking an exact number of tricks. Bridge, Whist, Euchre, 500, Spades, and the various Tarot card games are popular examples.

The object of Rummy, and various other melding or matching games, is to acquire the required groups of matching cards before an opponent can do so. In Rummy, this is done through drawing and discarding, and the groups are called melds. Mahjong is a very similar game played with tiles instead of cards. Non-Rummy examples of match-type games generally fall into the "fishing" genre and include the children's games Go Fish and Old Maid.

In a shedding game, players start with a hand of cards, and the object of the game is to be the first player to discard all cards from one's hand. Common shedding games include Crazy Eights (commercialized by Mattel as Uno) and Daihinmin. Some matching-type games are also shedding-type games; some variants of Rummy such as Phase 10, Rummikub, the bluffing game I Doubt It, and the children's game Old Maid, fall into both categories.

The object of an accumulating game is to acquire all cards in the deck. Examples include most War type games, and games involving slapping a discard pile such as Slapjack. Egyptian Ratscrew has both of these features.

In fishing games, cards from the hand are played against cards in a layout on the table, capturing table cards if they match. Fishing games are popular in many nations, including China, where there are many diverse fishing games. Scopa is considered one of the national card games of Italy. Cassino is the only fishing game to be widely played in English-speaking countries. Zwicker has been described as a "simpler and jollier version of Cassino", played in Germany. Seep is a classic Indian fishing card game mainly popular in northern parts of India. Tablanet (tablić) is fishing-style game popular in Balkans.

Comparing card games are those where hand values are compared to determine the winner, also known as "vying" or "showdown" games. Poker, blackjack, and baccarat are examples of comparing card games. As seen, nearly all of these games are designed as gambling games.

Solitaire games are designed to be played by one player. Most games begin with a specific layout of cards, called a tableau, and the object is then either to construct a more elaborate final layout, or to clear the tableau and/or the draw pile or "stock" by moving all cards to one or more "discard" or "foundation" piles.

Drinking card games are drinking games using cards, in which the object in playing the game is either to drink or to force others to drink. Many games are simply ordinary card games with the establishment of "drinking rules"; President, for instance, is virtually identical to Daihinmin but with additional rules governing drinking. Poker can also be played using a number of drinks as the wager. Another game often played as a drinking game is Toepen, quite popular in the Netherlands. Some card games are designed specifically to be played as drinking games.

Many card games borrow elements from more than one type. The most common combination is matching and shedding, as in some variants of Rummy, Old Maid, and Go Fish. However, many multi-genre games involve different stages of play for each hand. The most common multi-stage combination is a "trick-and-meld" game, such as Pinochle or Belote. Other multi-stage, multi-genre games include Poke, Gleek, Skitgubbe, and Tichu.

Collectible card games (CCG) are proprietary playing card games. CCGs are games of strategy between two players though multiplayer exists too. Both have their own personally built deck constructed from a very large pool of individually unique cards in the commercial market. The cards have different effects, costs, and art. Obtaining the different cards makes the game a collectible and cards are sold or traded on the secondary market. "" and "Yu-Gi-Oh!" are well-known collectible card games.

These games revolve around wagers of money. Though virtually any game in which there are winning and losing outcomes can be wagered on, these games are specifically designed to make the betting process a strategic part of the game. Some of these games involve players betting against each other, such as poker, while in others, like blackjack, players wager against the house.

Poker is a family of gambling games in which players bet into a pool, called the pot, the value of which changes as the game progresses that the value of the hand they carry will beat all others according to the ranking system. Variants largely differ on how cards are dealt and the methods by which players can improve a hand. For many reasons, including its age and its popularity among Western militaries, it is one of the most universally known card games in existence.

Many other card games have been designed and published on a commercial or amateur basis. In some cases, the game uses the standard 52-card deck, but the object is unique. In Eleusis, for example, players play single cards, and are told whether the play was legal or illegal, in an attempt to discover the underlying rules made up by the dealer.

Most of these games however typically use a specially made deck of cards designed specifically for the game (or variations of it). The decks are thus usually proprietary, but may be created by the game's players. Uno, Phase 10, Set, and 1000 Blank White Cards are popular dedicated-deck card games; 1000 Blank White Cards is unique in that the cards for the game are designed by the players of the game while playing it; there is no commercially available deck advertised as such.

A deck of either customised dedicated cards or a standard deck of playing cards with assigned meanings is used to simulate the actions of another activity, for example card football.

Many games, including card games, are fabricated by science fiction authors and screenwriters to distance a culture depicted in the story from present-day Western culture. They are commonly used as filler to depict background activities in an atmosphere like a bar or rec room, but sometimes the drama revolves around the play of the game. Some of these games become real card games as the holder of the intellectual property develops and markets a suitable deck and ruleset for the game, while others, such as "Exploding Snap" from the Harry Potter franchise, lack sufficient descriptions of rules, or depend on cards or other hardware that are infeasible or physically impossible.

Any specific card game imposes restrictions on the number of players. The most significant dividing lines run between one-player games and two-player games, and between two-player games and multi-player games. Card games for one player are known as "solitaire" or "patience" card games. (See list of solitaire card games.) Generally speaking, they are in many ways special and atypical, although some of them have given rise to two- or multi-player games such as Spite and Malice.

In card games for two players, usually not all cards are distributed to the players, as they would otherwise have perfect information about the game state. Two-player games have always been immensely popular and include some of the most significant card games such as piquet, bezique, sixty-six, klaberjass, gin rummy and cribbage. Many multi-player games started as two-player games that were adapted to a greater number of players. For such adaptations a number of non-obvious choices must be made beginning with the choice of a game orientation.

One way of extending a two-player game to more players is by building two teams of equal size. A common case is four players in two fixed partnerships, sitting crosswise as in whist and contract bridge. Partners sit opposite to each other and cannot see each other's hands. If communication between the partners is allowed at all, then it is usually restricted to a specific list of permitted signs and signals. 17th-century French partnership games such as triomphe were special in that partners sat next to each other and were allowed to communicate freely so long as they did not exchange cards or play out of order.

Another way of extending a two-player game to more players is as a "cut-throat" game, in which all players fight on their own, and win or lose alone. Most cut-throat card games are "round games", i.e. they can be played by any number of players starting from two or three, so long as there are enough cards for all.

For some of the most interesting games such as ombre, tarot and skat, the associations between players change from hand to hand. Ultimately players all play on their own, but for each hand, some game mechanism divides the players into two teams. Most typically these are "solo games", i.e. games in which one player becomes the soloist and has to achieve some objective against the others, who form a team and win or lose all their points jointly. But in games for more than three players, there may also be a mechanism that selects two players who then have to play against the others.

The players of a card game normally form a circle around a table or other space that can hold cards. The "game orientation" or "direction of play", which is only relevant for three or more players, can be either clockwise or counterclockwise. It is the direction in which various roles in the game proceed. Most regions have a traditional direction of play, such as:


Europe is roughly divided into a clockwise area in the north and a counterclockwise area in the south. The boundary runs between England, Ireland, Netherlands, Germany, Austria (mostly), Slovakia, Ukraine and Russia (clockwise) and France, Switzerland, Spain, Italy, Slovenia, Balkans, Hungary, Romania, Bulgaria, Greece and Turkey (anticlockwise).

Games that originate in a region with a strong preference are often initially played in the original direction, even in regions that prefer the opposite direction. For games that have official rules and are played in tournaments, the direction of play is often prescribed in those rules.

Most games have some form of asymmetry between players. The roles of players are normally expressed in terms of the "dealer", i.e. the player whose task it is to shuffle the cards and distribute them to the players. Being the dealer can be a (minor or major) advantage or disadvantage, depending on the game. Therefore, after each played hand, the deal normally passes to the next player according to the game orientation.

As it can still be an advantage or disadvantage to be the first dealer, there are some standard methods for determining who is the first dealer. A common method is by cutting, which works as follows. One player shuffles the deck and places it on the table. Each player lifts a packet of cards from the top, reveals its bottom card, and returns it to the deck. The player who reveals the highest (or lowest) card becomes dealer. In case of a tie, the process is repeated by the tied players. For some games such as whist this process of cutting is part of the official rules, and the hierarchy of cards for the purpose of cutting (which need not be the same as that used otherwise in the game) is also specified. But in general any method can be used, such as tossing a coin in case of a two-player game, drawing cards until one player draws an ace, or rolling dice.

A "hand" is a unit of the game that begins with the dealer shuffling and dealing the cards as described below, and ends with the players scoring and the next dealer being determined. The set of cards that each player receives and holds in his or her hands is also known as that player's hand.

The hand is over when the players have finished playing their hands. Most often this occurs when one player (or all) has no cards left. The player who sits after the dealer in the direction of play is known as eldest hand (or in two-player games as elder hand) or forehand. A "game round" consists of as many hands as there are players. After each hand, the deal is passed on in the direction of play, i.e. the previous eldest hand becomes the new dealer. Normally players score points after each hand. A game may consist of a fixed number of rounds. Alternatively it can be played for a fixed number of points. In this case it is over with the hand in which a player reaches the target score.

Shuffling is the process of bringing the cards of a pack into a random order. There are a large number of techniques with various advantages and disadvantages. "Riffle shuffling" is a method in which the deck is divided into two roughly equal-sized halves that are bent and then released, so that the cards interlace. Repeating this process several times randomizes the deck well, but the method is harder to learn than some others and may damage the cards. The "overhand shuffle" and the "Hindu shuffle" are two techniques that work by taking batches of cards from the top of the deck and reassembling them in the opposite order. They are easier to learn but must be repeated more often. A method suitable for small children consists in spreading the cards on a large surface and moving them around before picking up the deck again. This is also the most common method for shuffling tiles such as dominoes.

For casino games that are played for large sums it is vital that the cards be properly randomised, but for many games this is less critical, and in fact player experience can suffer when the cards are shuffled too well. The official skat rules stipulate that the cards are "shuffled well", but according to a decision of the German skat court, a one-handed player should ask another player to do the shuffling, rather than use a shuffling machine, as it would shuffle the cards "too" well. French belote rules go so far as to prescribe that the deck never be shuffled between hands.

The dealer takes all of the cards in the pack, arranges them so that they are in a uniform stack, and shuffles them. In strict play, the dealer then offers the deck to the previous player (in the sense of the game direction) for "cutting". If the deal is clockwise, this is the player to the dealer's right; if counterclockwise, it is the player to the dealer's left. The invitation to cut is made by placing the pack, face downward, on the table near the player who is to cut: who then lifts the upper portion of the pack clear of the lower portion and places it alongside. (Normally the two portions have about equal size. Strict rules often indicate that each portion must contain a certain minimum number of cards, such as three or five.) The formerly lower portion is then replaced on top of the formerly upper portion. Instead of cutting, one may also knock on the deck to indicate that one trusts the dealer to have shuffled fairly.

The actual "deal" (distribution of cards) is done in the direction of play, beginning with eldest hand. The dealer holds the pack, face down, in one hand, and removes cards from the top of it with his or her other hand to distribute to the players, placing them face down on the table in front of the players to whom they are dealt. The cards may be dealt one at a time, or in batches of more than one card; and either the entire pack or a determined number of cards are dealt out. The undealt cards, if any, are left face down in the middle of the table, forming the "stock" (also called the talon, widow, skat or kitty depending on the game and region).

Throughout the shuffle, cut, and deal, the dealer should prevent the players from seeing the faces of any of the cards. The players should not try to see any of the faces. Should a player accidentally see a card, other than one's own, proper etiquette would be to admit this. It is also dishonest to try to see cards as they are dealt, or to take advantage of having seen a card. Should a card accidentally become exposed, (visible to all), any player can demand a redeal (all the cards are gathered up, and the shuffle, cut, and deal are repeated) or that the card be replaced randomly into the deck ("burning" it) and a replacement dealt from the top to the player who was to receive the revealed card.

When the deal is complete, all players pick up their cards, or "hand", and hold them in such a way that the faces can be seen by the holder of the cards but not the other players, or vice versa depending on the game. It is helpful to fan one's cards out so that if they have corner indices all their values can be seen at once. In most games, it is also useful to sort one's hand, rearranging the cards in a way appropriate to the game. For example, in a trick-taking game it may be easier to have all one's cards of the same suit together, whereas in a rummy game one might sort them by rank or by potential combinations.

A new card game starts in a small way, either as someone's invention, or as a modification of an existing game. Those playing it may agree to change the rules as they wish. The rules that they agree on become the "house rules" under which they play the game. A set of house rules may be accepted as valid by a group of players wherever they play, as it may also be accepted as governing all play within a particular house, café, or club.

When a game becomes sufficiently popular, so that people often play it with strangers, there is a need for a generally accepted set of rules. This need is often met when a particular set of house rules becomes generally recognized. For example, when Whist became popular in 18th-century England, players in the Portland Club agreed on a set of house rules for use on its premises. Players in some other clubs then agreed to follow the "Portland Club" rules, rather than go to the trouble of codifying and printing their own sets of rules. The Portland Club rules eventually became generally accepted throughout England and Western cultures.

There is nothing static or "official" about this process. For the majority of games, there is no one set of universal rules by which the game is played, and the most common ruleset is no more or less than that. Many widely played card games, such as Canasta and Pinochle, have no official regulating body. The most common ruleset is often determined by the most popular distribution of rulebooks for card games. Perhaps the original compilation of popular playing card games was collected by Edmund Hoyle, a self-made authority on many popular parlor games. The U.S. Playing Card Company now owns the eponymous Hoyle brand, and publishes a series of rulebooks for various families of card games that have largely standardized the games' rules in countries and languages where the rulebooks are widely distributed. However, players are free to, and often do, invent "house rules" to supplement or even largely replace the "standard" rules.

If there is a sense in which a card game can have an "official" set of rules, it is when that card game has an "official" governing body. For example, the rules of tournament bridge are governed by the World Bridge Federation, and by local bodies in various countries such as the American Contract Bridge League in the U.S., and the English Bridge Union in England. The rules of skat are governed by The International Skat Players Association and, in Germany, by the "Deutscher Skatverband" which publishes the "Skatordnung". The rules of French tarot are governed by the Fédération Française de Tarot. The rules of Poker's variants are largely traditional, but enforced by the World Series of Poker and the World Poker Tour organizations which sponsor tournament play. Even in these cases, the rules must only be followed exactly at games sanctioned by these governing bodies; players in less formal settings are free to implement agreed-upon supplemental or substitute rules at will.

An infraction is any action which is against the rules of the game, such as playing a card when it is not one's turn to play or the accidental exposure of a card, informally known as "bleeding."

In many official sets of rules for card games, the rules specifying the penalties for various infractions occupy more pages than the rules specifying how to play correctly. This is tedious, but necessary for games that are played seriously. Players who intend to play a card game at a high level generally ensure before beginning that all agree on the penalties to be used. When playing privately, this will normally be a question of agreeing house rules. In a tournament there will probably be a tournament director who will enforce the rules when required and arbitrate in cases of doubt.

If a player breaks the rules of a game deliberately, this is cheating. The rest of this section is therefore about accidental infractions, caused by ignorance, clumsiness, inattention, etc.

As the same game is played repeatedly among a group of players, precedents build up about how a particular infraction of the rules should be handled. For example, "Sheila just led a card when it wasn't her turn. Last week when Jo did that, we agreed ... etc." Sets of such precedents tend to become established among groups of players, and to be regarded as part of the house rules. Sets of house rules may become formalized, as described in the previous section. Therefore, for some games, there is a "proper" way of handling infractions of the rules. But for many games, without governing bodies, there is no standard way of handling infractions.

In many circumstances, there is no need for special rules dealing with what happens after an infraction. As a general principle, the person who broke a rule should not benefit by it, and the other players should not lose by it. An exception to this may be made in games with fixed partnerships, in which it may be felt that the partner(s) of the person who broke a rule should also not benefit. The penalty for an accidental infraction should be as mild as reasonable, consistent with there being no possible benefit to the person responsible.

The same kind of games can also be played with tiles made of wood, plastic, bone, or similar materials. The most notable examples of such tile sets are dominoes, mahjong tiles and Rummikub tiles. Chinese dominoes are also available as playing cards. It is not clear whether Emperor Muzong of Liao really played with domino cards as early as 969, though. Legend dates the invention of dominoes in the year 1112, and the earliest known domino rules are from the following decade. 500 years later domino cards were reported as a new invention.

The first playing cards appeared in the 9th century during Tang-dynasty China.
The first reference to the card game in world history dates no later than the 9th century, when the "Collection of Miscellanea at Duyang", written by Tang Dynasty writer Su E, described Princess (daughter of Emperor Yizong of Tang) playing the "leaf game" with members of the Wei clan (the family of the princess' husband) in 868 . The Song dynasty statesman and historian Ouyang Xiu has noted that paper playing cards arose in connection to an earlier development in the book format from scrolls to pages. During the Ming dynasty (1368–1644), characters from popular novels such as the "Water Margin" were widely featured on the faces of playing cards. A precise description of Chinese money playing cards (in four suits) survived from the 15th century. Mahjong tiles are a 19th-century invention based on three-suited money playing card decks, similar to the way in which Rummikub tiles were derived recently from modern Western playing cards.

Playing cards first appeared in Europe in the last quarter of the 14th century. The earliest European references speak of a Saracen or Moorish game called "naib", and in fact an almost complete Mamluk Egyptian deck of 52 cards in a distinct oriental design has survived from around the same time, with the four suits "swords", "polo sticks", "cups" and "coins" and the ranks "king", "governor", "second governor", and "ten" to "one".

The 1430s in Italy saw the invention of the tarot deck, a full Latin-suited deck augmented by suitless cards with painted motifs that played a special role as trumps. Tarot card games are still played with (subsets of) these decks in parts of Central Europe. A full tarot deck contains 14 cards in each suit; low cards labeled 1–10, and court cards (jack), (cavalier/knight), (queen), and (king), plus the fool or excuse card, and 21 trump cards. In the 18th century the card images of the traditional Italian tarot decks became popular in cartomancy and evolved into "esoteric" decks used primarily for the purpose; today most tarot decks sold in North America are the occult type, and are closely associated with fortune telling. In Europe, "playing tarot" decks remain popular for games, and have evolved since the 18th century to use regional suits (spades, hearts, diamonds and clubs in France; leaves, hearts, bells and acorns in Germany) as well as other familiar aspects of the Anglo-American deck such as corner card indices and "stamped" card symbols for non-court cards. Decks differ regionally based on the number of cards needed to play the games; the French tarot consists of the "full" 78 cards, while Germanic, Spanish and Italian Tarot variants remove certain values (usually low suited cards) from the deck, creating a deck with as few as 32 cards.

The French suits were introduced around 1480 and, in France, mostly replaced the earlier Latin suits of "swords", "clubs", "cups" and "coins". (which are still common in Spanish- and Portuguese-speaking countries as well as in some northern regions of Italy) The suit symbols, being very simple and single-color, could be stamped onto the playing cards to create a deck, thus only requiring special full-color card art for the court cards. This drastically simplifies the production of a deck of cards versus the traditional Italian deck, which used unique full-color art for each card in the deck. The French suits became popular in English playing cards in the 16th century (despite historic animosity between France and England), and from there were introduced to British colonies including North America. The rise of Western culture has led to the near-universal popularity and availability of French-suited playing cards even in areas with their own regional card art.

In Japan, a distinct 48-card hanafuda deck is popular. It is derived from 16th-century Portuguese decks, after undergoing a long evolution driven by laws enacted by the Tokugawa shogunate attempting to ban the use of playing cards

The best-known deck internationally is the Anglo-American pattern of the 52-card French deck used for such games as poker and contract bridge. It contains one card for each unique combination of thirteen "ranks" and the four French "suits" "spades", "hearts", "diamonds", and "clubs". The ranks (from highest to lowest in bridge and poker) are "ace", "king", "queen", "jack" (or "knave"), and the numbers from "ten" down to "two" (or "deuce"). The trump cards and "knight" cards from the French playing tarot are not included.

Originally the term "knave" was more common than "jack"; the card had been called a jack as part of the terminology of All-Fours since the 17th century, but the word was considered vulgar. (Note the exclamation by Estella in Charles Dickens's novel "Great Expectations": "He calls the knaves, Jacks, this boy!") However, because the card abbreviation for knave ("Kn") was so close to that of the king, it was very easy to confuse them, especially after suits and rankings were moved to the corners of the card in order to enable people to fan them in one hand and still see all the values. (The earliest known deck to place suits and rankings in the corner of the card is from 1693, but these cards did not become common until after 1864 when Hart reintroduced them along with the knave-to-jack change.) However, books of card games published in the third quarter of the 19th century evidently still referred to the "knave", and the term with this definition is still recognized in the United Kingdom.

In the 17th century, a French, five-trick, gambling game called Bête became popular and spread to Germany, where it was called La Bete and England where it was named Beast. It was a derivative of Triomphe and was the first card game in history to introduce the concept of bidding.
Chinese handmade mother-of-pearl gaming counters were used in scoring and bidding of card games in the West during the approximate period of 1700–1840. The gaming counters would bear an engraving such as a coat of arms or a monogram to identify a family or individual. Many of the gaming counters also depict Chinese scenes, flowers or animals. Queen Charlotte, wife of George III, is one prominent British individual who is known to have played with the Chinese gaming counters. Card games such as Ombre, Quadrille and Pope Joan were popular at the time and required counters for scoring. The production of counters declined after Whist, with its different scoring method, became the most popular card game in the West.

Based on the association of card games and gambling, Pope Benedict XIV banned card games on October 17, 1750.

Since the 19th century some decks have been specially printed for certain games. Old Maid, Phase 10, Rook, and Uno are examples of games that can be played with one or more 52-card decks but are usually played with custom decks. Cards play an important role in board games like Risk and Monopoly.




</doc>
<doc id="5361" url="https://en.wikipedia.org/wiki?curid=5361" title="Cross-stitch">
Cross-stitch

Cross-stitch is a form of sewing and a popular form of counted-thread embroidery in which X-shaped stitches in a tiled, raster-like pattern are used to form a picture. The stitcher counts the threads on a piece of evenweave fabric (such as linen) in each direction so that the stitches are of uniform size and appearance. This form of cross-stitch is also called counted cross-stitch in order to distinguish it from other forms of cross-stitch. Sometimes cross-stitch is done on designs printed on the fabric (stamped cross-stitch); the stitcher simply stitches over the printed pattern. Cross-stitch is often executed on easily countable fabric called aida cloth whose weave creates a plainly visible grid of squares with holes for the needle at each corner.

Fabrics used in cross-stitch include linen, aida, and mixed-content fabrics called 'evenweave' such as jobelan. All cross-stitch fabrics are technically "evenweave" as the term refers to the fact that the fabric is woven to make sure that there are the same number of threads per inch in both the warp and the weft (i.e. vertically and horizontally). Fabrics are categorized by threads per inch (referred to as 'count'), which can range from 11 to 40 count.

Counted cross-stitch projects are worked from a gridded pattern called a chart and can be used on any count fabric; the count of the fabric and the number of threads per stitch determine the size of the finished stitching. For example, if a given design is stitched on a 28 count cross-stitch fabric with each cross worked over two threads, the finished stitching size is the same as it would be on a 14 count aida fabric with each cross worked over one square. These methods are referred to as "2 over 2" (2 embroidery threads used to stitch over 2 fabric threads) and "1 over 1" (1 embroidery thread used to stitch over 1 fabric thread or square), respectively. There are different methods of stitching a pattern, including the cross-country method where one colour is stitched at a time, or the parking method where one block of fabric is stitched at a time and the end of the thread is "parked" at the next point the same colour occurs in the pattern.

Cross-stitch is the oldest form of embroidery and can be found all over the world since the middle ages. Many folk museums show examples of clothing decorated with cross-stitch, especially from continental Europe, Asia, and Eastern and Central Europe.

The cross-stitch sampler is called that because it was generally stitched by a young girl to learn how to stitch and to record alphabet and other patterns to be used in her household sewing. These samples of her stitching could be referred back to over the years. Often, motifs and initials were stitched on household items to identify their owner, or simply to decorate the otherwise-plain cloth. The earliest known cross stitch sampler made in the United States is currently housed at Pilgrim Hall in Plymouth, Massachusetts. The sampler was created by Loara Standish, daughter of Captain Myles Standish and pioneer of the Leviathan stitch, circa 1653.

Traditionally, cross-stitch was used to embellish items like household linens, tablecloths, dishcloths, and doilies (only a small portion of which would actually be embroidered, such as a border). Although there are many cross-stitchers who still employ it in this fashion, it is now increasingly popular to work the pattern on pieces of fabric and hang them on the wall for decoration. Cross-stitch is also often used to make greeting cards, pillowtops, or as inserts for box tops, coasters and trivets.

Multicoloured, shaded, painting-like patterns as we know them today are a fairly modern development, deriving from similar shaded patterns of Berlin wool work of the mid-nineteenth century. Besides designs created expressly for cross-stitch, there are software programs that convert a photograph or a fine art image into a chart suitable for stitching. One example of this is in the cross-stitched reproduction of the Sistine Chapel charted and stitched by Joanna Lopianowski-Roberts.

There are many cross-stitching "guilds" and groups across the United States and Europe which offer classes, collaborate on large projects, stitch for charity, and provide other ways for local cross-stitchers to get to know one another. Individually owned local needlework shops (LNS) often have stitching nights at their shops, or host weekend stitching retreats.

Today, cotton floss is the most common embroidery thread. It is a thread made of mercerized cotton, composed of six strands that are only loosely twisted together and easily separable. While there are other manufacturers, the two most-commonly used (and oldest) brands are DMC and Anchor, both of which have been manufacturing embroidery floss since the 1800s.

Other materials used are pearl (or perle) cotton, Danish flower thread, silk and Rayon. Different wool threads, metallic threads or other novelty threads are also used, sometimes for the whole work, but often for accents and embellishments. Hand-dyed cross-stitch floss is created just as the name implies—it is dyed by hand. Because of this, there are variations in the amount of color throughout the thread. Some variations can be subtle, while some can be a huge contrast. Some also have more than one color per thread, which in the right project, creates amazing results.

Cross-stitch is widely used in traditional Palestinian dressmaking.

Other stitches are also often used in cross-stitch, among them quarter-, half-, and three-quarter-stitches and backstitches.

Cross-stitch is often used together with other stitches. A cross-stitch can come in a variety of prostational forms. It is sometimes used in crewel embroidery, especially in its more modern derivatives. It is also often used in needlepoint.

A specialized historical form of embroidery using cross-stitch is Assisi embroidery.

There are many stitches which are related to cross-stitch and were used in similar ways in earlier times. The best known are Italian cross-stitch, Celtic Cross Stitch, Irish Cross Stitch, long-armed cross-stitch, Ukrainian cross-stitch and Montenegrin stitch. Italian cross-stitch and Montenegrin stitch are reversible, meaning the work looks the same on both sides. These styles have a slightly different look than ordinary cross-stitch. These more difficult stitches are rarely used in mainstream embroidery, but they are still used to recreate historical pieces of embroidery or by the creative and adventurous stitcher.

The double cross-stitch, also known as a Leviathan stitch or Smyrna cross-stitch, combines a cross-stitch with an upright cross-stitch.

Berlin wool work and similar petit point stitchery resembles the heavily shaded, opulent styles of cross-stitch, and sometimes also used charted patterns on paper.

Cross-stitch is often combined with other popular forms of embroidery, such as Hardanger embroidery or blackwork embroidery. Cross-stitch may also be combined with other work, such as canvaswork or drawn thread work. Beadwork and other embellishments such as paillettes, charms, small buttons and specialty threads of various kinds may also be used.

Cross-stitch has become increasingly popular with the younger generation of Europe in recent years. Retailers such as John Lewis experienced a 17% rise in sales of haberdashery products between 2009 and 2010. Hobbycraft, a chain of stores selling craft supplies, also enjoyed an 11% increase in sales over the year to February 22, 2009.

Knitting and cross-stitching have become more popular hobbies for a younger market, in contrast to its traditional reputation as a hobby for retirees. Sewing and craft groups such as Stitch and Bitch London have resurrected the idea of the traditional craft club. At Clothes Show Live 2010 there was a new area called "Sknitch" promoting modern sewing, knitting and embroidery.

In a departure from the traditional designs associated with cross-stitch, there is a current trend for more postmodern or tongue-in-cheek designs featuring retro images or contemporary sayings. It is linked to a concept known as 'subversive cross-stitch', which involves more risque designs, often fusing the traditional sampler style with sayings designed to shock or be incongruous with the old-fashioned image of cross-stitch.

Stitching designs on other materials can be accomplished by using waste canvas. This is a temporary gridded canvas similar to regular canvas used for embroidery that is held together by a water-soluble glue, which is removed after completion of stitch design. Other crafters have taken to cross-stitching on all manner of gridded objects as well including old kitchen strainers or chain-link fences.

In the 21st Century, an emphasis on feminist design has emerged within cross-stitch communities. There are collections of patterns available with feminist themes, and many more feminist patterns online. Some cross-stitchers have commented on the way that the practice of embroidery makes them feel connected to the women who practised it before them. There is a push for all embroidery, including cross-stitch, to be respected as a significant art form.

An increasingly popular activity for cross-stitchers is to watch and make YouTube videos detailing their hobby. Flosstubers, as they are known, typically cover WIPs (Works in Progress), FOs (Finished Objects), and Haul (new patterns, thread, and fabric, as well as cross-stitching accessories, such as needleminders).





</doc>
<doc id="5362" url="https://en.wikipedia.org/wiki?curid=5362" title="Casino game">
Casino game

Games available in most casinos are commonly called casino games. In a casino game, the players gamble casino chips on various possible random outcomes or combinations of outcomes. Casino games are also available in online casinos, where permitted by law. Casino games can also be played outside casinos for entertainment purposes like in parties or in school competitions, some on machines that simulate gambling.

There are three general categories of casino games: table games, electronic gaming machines, and random number ticket games such as keno. Gaming machines, such as slot machines and pachinko, are usually played by one player at a time and do not require the involvement of casino employees to play. Random number games are based upon the selection of random numbers, either from a computerized random number generator or from other gaming equipment. Random number games may be played at a table, such as roulette, or through the purchase of paper tickets or cards, such as keno or bingo.



Casino games typically provide a predictable long-term advantage to the casino, or "house", while offering the players the possibility of a short-term gain that in some cases can be large. Some casino games have a skill element, where the players' decisions have an impact on the results. Players possessing sufficient skills to eliminate the inherent long-term disadvantage (the "house edge" or vigorish) in a casino game are referred to as advantage players.

The players' disadvantage is a result of the casino not paying winning wagers according to the game's "true odds", which are the payouts that would be expected considering the odds of a wager either winning or losing. For example, if a game is played by wagering on the number that would result from the roll of one die, true odds would be 5 times the amount wagered since there is a 1 in 6 chance of any single number appearing, assuming that the player gets the original amount wagered back. However, the casino may only pay 4 times the amount wagered for a winning wager.

The house edge or vigorish is defined as the casino profit expressed as the percentage of the player's original bet. (In games such as blackjack or Spanish 21, the final bet may be several times the original bet, if the player double and splits.)

In American Roulette, there are two "zeroes" (0, 00) and 36 non-zero numbers (18 red and 18 black). This leads to a higher house edge compared to European Roulette. The chances of a player, who bets 1 unit on red, winning is 18/38 and his chances of losing 1 unit is 20/38. The player's expected value is EV = (18/38 x 1) + (20/38 x -1) = 18/38 - 20/38 = -2/38 = -5.26%. Therefore, the house edge is 5.26%. After 10 spins, betting 1 unit per spin, the average house profit will be 10 x 1 x 5.26% = 0.53 units. European roulette wheels have only one "zero" and therefore the house advantage (ignoring the en prison rule) is equal to 1/37 = 2.7%.

The house edge of casino games varies greatly with the game, with some games having an edge as low as 0.3%. Keno can have house edges up to 25%, slot machines having up to 15%.

The calculation of the roulette house edge was a trivial exercise; for other games, this is not usually the case. Combinatorial analysis and/or computer simulation is necessary to complete the task.

In games which have a skill element, such as Blackjack or Spanish 21, the house edge is defined as the house advantage from optimal play (without the use of advanced techniques such as card counting), on the first hand of the shoe (the container that holds the cards). The set of the optimal plays for all possible hands is known as "basic strategy" and is highly dependent on the specific rules and even the number of decks used. Good blackjack and Spanish 21 games have house edges below 0.5%.

Traditionally, the majority of casinos have refused to reveal the house edge information for their slots games and due to the unknown number of symbols and weightings of the reels, in most cases it is much more difficult to calculate the house edge than that in other casino games. However, due to some online properties revealing this information and some independent research conducted by Michael Shackleford in the offline sector, this pattern is slowly changing.

The luck factor in a casino game is quantified using standard deviations (SD). The standard deviation of a simple game like Roulette can be calculated using the binomial distribution. In the binomial distribution, SD = sqrt ("npq" ), where "n" = number of rounds played, "p" = probability of winning, and "q" = probability of losing. The binomial distribution assumes a result of 1 unit for a win, and 0 units for a loss, rather than -1 units for a loss, which doubles the range of possible outcomes. Furthermore, if we flat bet at 10 units per round instead of 1 unit, the range of possible outcomes increases 10 fold.

SD (Roulette, even-money bet) = 2"b" sqrt("npq" ), where "b" = flat bet per round, "n" = number of rounds, "p" = 18/38, and "q" = 20/38.

For example, after 10 rounds at 1 unit per round, the standard deviation will be 2 x 1 x sqrt(10 x 18/38 x 20/38) = 3.16 units. After 10 rounds, the expected loss will be 10 x 1 x 5.26% = 0.53. As you can see, standard deviation is many times the magnitude of the expected loss.

The standard deviation for pai gow poker is the lowest out of all common casinos. Many casino games, particularly slots, have extremely high standard deviations. The bigger size of the potential payouts, the more the standard deviation may increase.

As the number of rounds increases, eventually, the expected loss will exceed the standard deviation, many times over. From the formula, we can see the standard deviation is proportional to the square root of the number of rounds played, while the expected loss is proportional to the number of rounds played. As the number of rounds increases, the expected loss increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to expected loss that fools gamblers into thinking that they can win.

It is important for a casino to know both the house edge and variance for all of their games. The house edge tells them what kind of profit they will make as percentage of turnover, and the variance tells them how much they need in the way of cash reserves. The mathematicians and computer programmers that do this kind of work are called gaming mathematicians and gaming analysts. Casinos do not have in-house expertise in this field, so outsource their requirements to experts in the gaming analysis field.



</doc>
<doc id="5363" url="https://en.wikipedia.org/wiki?curid=5363" title="Video game">
Video game

A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a two- or three-dimensional video display device such as a touchscreen, virtual reality headset or monitor/TV set. Since the 1980s, video games have become an increasingly important part of the entertainment industry, and whether they are also a form of art is a matter of dispute.

The electronic systems used to play video games are called platforms. Video games are developed and released for one or several platforms and may not be available on others. Specialized platforms such as arcade games, which present the game in a large, typically coin-operated chassis, were common in the 1980s in video arcades, but declined in popularity as other, more affordable platforms became available. These include dedicated devices such as video game consoles, as well as general-purpose computers like a laptop, desktop or handheld computing devices.

The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets.

Since the 2010s, the commercial importance of the video game industry has been increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2018, video games generated sales of US$134.9 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.

Early games used interactive electronic devices with various display formats. The earliest example is from 1947—a "Cathode ray tube Amusement Device" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen. Other early examples include: The Nimrod computer at the 1951 Festival of Britain; "OXO" a tic-tac-toe Computer game by Alexander S. Douglas for the EDSAC in 1952; "Tennis for Two", an electronic interactive game engineered by William Higinbotham in 1958; "Spacewar!", written by MIT students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961; and the hit ping pong-style "Pong", a 1972 game by Atari. Each game used different means of display: NIMROD used a panel of lights to play the game of Nim, OXO used a graphical display to play tic-tac-toe "Tennis for Two" used an oscilloscope to display a side view of a tennis court, and "Spacewar!" used the DEC PDP-1's vector display to have two spaceships battle each other.

In 1971, "Computer Space", created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips. The game was featured in the 1973 science fiction film "Soylent Green". "Computer Space" was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television. These were followed by two versions of Atari's "Pong"; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity. The commercial success of "Pong" led numerous other companies to develop "Pong" clones and their own systems, spawning the video game industry.

A flood of "Pong" clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game "Space Invaders", marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market. The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores. The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby. "Space Invaders" was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales. This helped Atari recover from their earlier losses, and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System, which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles.

A number of video game developers emerged in Britain in the late 1970s and early 1980s.

The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate. The term "system" is also commonly used. The distinctions below are not always clear and there may be games that bridge one or more platforms. In addition to laptop/desktop computers and mobile devices, there are other devices which have the ability to play games but are not primarily video game machines, such as PDAs and graphing calculators.

In common use a "PC game" refers to a form of media that involves a player interacting with a personal computer connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game on different hardware. Also, the openness allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of modifications ("mods"), open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others. A Gaming computer is a PC or laptop intended specifically for gaming.

A "console game" is played on a specialized electronic device ("home video game console") that connects to a common television set or composite video monitor, unlike PCs, which can run all sorts of computer programs, a console is a dedicated video game platform manufactured by a specific company. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox, PlayStation, and Nintendo.

A "handheld" gaming device is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardware. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games.

"Arcade game" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them.

The web browser has also established itself as platform in its own right in the 2000s, while providing a cross-platform environment for video games designed to be played on a wide spectrum of platforms. In turn, this has generated new terms to qualify classes of web browser-based games. These games may be identified based on the website that they appear, such as with "Miniclip" games. Others are named based on the programming platform used to develop them, such as Java and Flash games.

With the advent of standard operating systems for mobile devices such as iOS and Android and devices with greater hardware performance, mobile gaming has become a significant platform. These games may utilize unique features of mobile devices that are not necessary present on other platforms, such as global positing information and camera devices to support augmented reality gameplay. Mobile games also led into the development of microtransactions as a valid revenue model for casual games.

Virtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit.

A new platform of video games emerged in late 2017 in which users could take ownership of game assets (digital assets) using Blockchain technologies. An example of this is Cryptokitties.

A video game, like most other forms of media, may be categorized into genres. Video game genres are used to categorize video games based on their gameplay interaction rather than visual or narrative differences. A video game genre is defined by a set of gameplay challenges and are classified independent of their setting or game-world content, unlike other works of fiction such as films or books. For example, a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space.

Because genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with games specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as multiplayer online battle arena (MOBA), and massively multiplayer online role-playing games (MMORPG). It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games.

Casual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system. Examples of genres within this category are match three, hidden object, time management, puzzle or many of the tower defense style games. Casual games are generally available through app stores and online retailers such as PopCap and GameHouse or provided for free play through web portals such as Newgrounds. While casual games are most commonly played on personal computers, phones or tablets, they can also be found on many of the on-line console system download services (e.g., the PlayStation Network, WiiWare or Xbox Live).

Serious games are games that are designed primarily to convey information or a learning experience to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning programs, etc.) and the primary distinction would appear to be based on the game's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule. Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exercise games, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.
One of the longest-running serious games franchises is "Microsoft Flight Simulator", first published in 1982 under that name. The United States military uses virtual reality-based simulations, such as VBS1 for training exercises, as do a growing number of first responder roles (e.g., police, firefighters, EMTs). One example of a non-game environment utilized as a platform for serious game development would be the virtual world of "Second Life", which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs and businesses (e.g., IBM, Cisco Systems) for meetings and training.

Tactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point. For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include "Darfur is Dying", "Finding Zoe", and "In The Balance". All of these games bring awareness to important issues and events.

On 23 September 2009, U.S. President Barack Obama launched a campaign called "Educate to Innovate" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments. This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup. Both of these bring a focus to relevant and important current issues through gaming. www.NobelPrize.org entices the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun video game. There are many different types and styles of educational games, including counting to spelling to games for kids, to games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game.

Video game can use several types of input devices to translate human actions to a game, the most common game controllers are keyboard and mouse for "PC games, consoles usually come with specific gamepads, handheld consoles have built in buttons. Other game controllers are commonly used for specific games like racing wheels, light guns or dance pads. Digital cameras can also be used as game controllers capturing movements of the body of the player.

As technology continues to advance, more can be added onto the controller to give the player a more immersive experience when playing different games. There are some controllers that have presets so that the buttons are mapped a certain way to make playing certain games easier. Along with the presets, a player can sometimes custom map the buttons to better accommodate their play style. On keyboard and mouse, different actions in the game are already preset to keys on the keyboard. Most games allow the player to change that so that the actions are mapped to different keys that are more to their liking. The companies that design the controllers are trying to make the controller visually appealing and also feel comfortable in the hands of the consumer.

An example of a technology that was incorporated into the controller was the touchscreen. It allows the player to be able to interact with the game differently than before. The person could move around in menus easier and they are also able to interact with different objects in the game. They can pick up some objects, equip others, or even just move the objects out of the player's path. Another example is motion sensor where a person's movement is able to be captured and put into a game. Some motion sensor games are based on where the controller is. The reason for that is because there is a signal that is sent from the controller to the console or computer so that the actions being done can create certain movements in the game. Other type of motion sensor games are webcam style where the player moves around in front of it, and the actions are repeated by a game character.

Video game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.

In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the "one-man shop" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some PDAs).

With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.

A phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with "Grand Theft Auto IV" ("" and ""), or Bethesda with "Fallout 3" and its expansions. New gameplay modes can also become available, for instance, "Call of Duty" and its zombie modes, a multiplayer mode for "Mushroom Wars" or a higher difficulty level for "". Smaller packages of DLC are also common, ranging from better in-game weapons ("Dead Space", "Just Cause 2"), character outfits ("LittleBigPlanet", "Minecraft"), or new songs to perform ("SingStar", "Rock Band", "Guitar Hero").

A variation of downloadable content is expansion packs. Unlike DLC, expansion packs add a whole section to the game that either already exists in the game's code or is developed after the game is released. Expansions add new maps, missions, weapons, and other things that weren't previously accessible in the original game. An example of an expansion is Bungie's "Destiny", which had the "" expansion. The expansion added new weapons, new maps, and higher levels, and remade old missions.

Expansions are added to the base game to help prolong the life of the game itself until the company is able to produce a sequel or a new game altogether. Developers may plan out their game's life and already have the code for the expansion in the game, but inaccessible by players, who later unlock these expansions, sometimes for free and sometimes at an extra cost. Some developers make games and add expansions later, so that they could see what additions the players would like to have. There are also expansions that are set apart from the original game and are considered a stand-alone game, such as Ubisoft's expansion " Freedom's Cry", which features a different character than the original game.

Many games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games. This allows for the kind of success seen by popular mods such as the "Half-Life" mod "Counter-Strike".

Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers, modification of game code by third parties, or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer. Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls. Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.

Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as "patches") to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.

Easter eggs are hidden messages or jokes left in games by developers that are not part of the main game. Easter eggs are secret responses that occur as a result of an undocumented set of commands. The results can vary from a simple printed message or image, to a page of programmer credits or a small videogame hidden inside an otherwise serious piece of software. Videogame cheat codes are a specific type of Easter egg, in which entering a secret command will unlock special powers or new levels for the player.

Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from "Star Trek", arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as "Tron", "eXistenZ" and "The Last Starfighter".

Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game "Tomb Raider", saying that "the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it." Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.

While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term "emergent narrative" has been used to describe how, in a simulated environment, storyline can be created simply by "what happens to the player." However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.

An emulator is a program that replicates the behavior of a video game console, allowing games to run on a different platform from the original hardware. Emulators exist for PCs, smartphones and consoles other than the original. Emulators are generally used to play old games, hack existing games, translate unreleased games in a specific region, or add enhanced features to games like improved graphics, speed up or down, bypass regional lockouts, or online multiplayer support.

Some manufacturers have released official emulators for their own consoles. For example, Nintendo's Virtual Console allows users to play games for old Nintendo consoles on the Wii, Wii U, and 3DS. Virtual Console is part of Nintendo's strategy for deterring video game piracy. In November 2015, Microsoft launched backwards compatibility of Xbox 360 games on Xbox One console via emulation. Also, Sony announced relaunching PS2 games on PS4 via emulation. According to "Sony Computer Entertainment America v. Bleem", creating an emulator for a proprietary video game console is legal. However, Nintendo claims that emulators promote the distribution of illegally copied games.

The November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between men and women. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB, almost 41% of PC gamers are women. Participation among African-Americans is lower. One survey of over 2000 game developers returned responses from only 2.5% who identified as black.

When comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:

A 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group. A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group. Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games.

Video game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music.

Multiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. "Tennis for Two", arguably the first video game, was a two player game, as was its successor "Pong". The first commercially available game console, the Magnavox Odyssey, had two controller inputs. Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.

Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; "Eve Online" set a record with 65,303 players on a single server in 2013.

It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects.

In Steven Johnson's book, "Everything Bad Is Good for You", he argues that video games in fact demand far more from a player than traditional games like "Monopoly". To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books. Some research suggests video games may even increase players' attention capacities.

Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be "learning by doing" while playing video games while fostering creative thinking.

The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people. According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to do problem solving. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.

An experiment carried out by Richard De Lisi and Jennifer Woldorf demonstrates the positive effect that video games may have on spatial skills. De Lisi and Woldorf took two groups of third graders, one control group and one experiment group. Both groups took a paper-and-pencil test of mental rotation skills. After this test, the experiment group only played 11 sessions of the game "Tetris". This game was chosen as it requires mental rotation. After this game, both groups took the test again. The result showed that the scores of the experiment group raised higher than that of the control group, thereby confirming this theory.

The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups. In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects. A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games. Action gamers are not only better at ignoring distractions, but also at focusing on the main task.

Like other media, video games have been the subject of objections, controversies, and censorship, for depictions of violence, criminal activities, sexual themes, alcohol, tobacco and other drugs, propaganda, profanity, or advertisements. Critics of video games include parents' groups, politicians, religious groups, scientists and other advocacy groups. Claims that some video games cause addiction or violent behavior continue to be made and to be disputed.

There have been a number of societal and scientific arguments about whether the content of video games change the behavior and attitudes of a player, and whether this is reflected in video game culture overall. Since the early 1980s, advocates of video games have emphasized their use as an expressive medium, arguing for their protection under the laws governing freedom of speech and also as an educational tool. Detractors argue that video games are harmful and therefore should be subject to legislative oversight and restrictions. The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated. A study was done that claimed that young people who have had a greater exposure to violence in video games ended up behaving more aggressively towards people in a social environment.

In 2018, the World Health Organization declared "gaming disorder" a mental disorder for people who are addicted to video games. Some studies have claimed video games can negatively affect health and mental state for some players.

In spite of the alleged negative effects of video games, certain studies indicate that they may have value in terms of academic performance, perhaps because of the skills that are developed in the process. "When you play games you’re solving puzzles to move to the next level and that involves using some of the general knowledge and skills in maths, reading and science that you’ve been taught during the day", said Alberto Posso an Associate Professor at the Royal Melbourne Institute of Technology, after analysing data from the results of standardized testing completed by over 12,000 high school students across Australia. As summarized by "The Guardian", the study (published in the "International Journal of Communication") "found that students who played online games almost every day scored 15 points above average in maths and reading tests and 17 points above average in science." However, the reporter added an important comment that was not provided by some of the numerous websites that published a brief summary of the Australian study: "[the] methodology cannot prove that playing video games were the cause of the improvement." "The Guardian" also reported that a Columbia University study indicated that extensive video gaming by students in the 6 to 11 age group provided a greatly increased chance of high intellectual functioning and overall school competence.

In an interview with CNN, Edward Castronova, a professor of Telecommunications at Indiana University Bloomington said he was not surprised by the outcome of the Australian study but also discussed the issue of causal connection. "Though there is a link between gaming and higher math and science scores, it doesn't mean playing games caused the higher scores. It could just be that kids who are sharp are looking for a challenge, and they don't find it on social media, and maybe they do find it on board games and video games," he explained.

Video games have also been proven to raise self-esteem and build confidence. It gives people an opportunity to do things that they cannot do offline, and to discover new things about themselves. There is a social aspect to gaming as well – research has shown that a third of video game players make good friends online. As well as that, video games are also considered to be therapeutic as it helps to relieve stress. Although short term, studies have shown that children with developmental delays gain a temporary physical improvement in health when they interact and play video games on a regular, and consistent basis due to the cognitive benefits and the use of hand eye coordination.

Self-determination theory (SDT) is a macro theory of human motivation based around competence, autonomy, and relatedness to facilitate positive outcomes. SDT provides a framework for understanding the effects of playing video games; well-being, problem solving, group relations, physical activities. These factors can be measured to determine the effect video games can have on people.

The ability to create an ideal image of one's self and being given multiple options to change that image gives a sense of satisfaction. This topic has much controversy; it is unknown whether this freedom can be beneficial to one's character or detrimental. With increased game usage, a player can become too invested in a fictionally generated character, where the desire to look that way overpowers the enjoyment of the game. Players see this character creation as entertainment and a release, creating a self-image they could not obtain in reality, bringing comfort outside of the game from lack of investment to the fictional character. Problems that arise based on character design may be link to personality disorders.

Cognitive skills can be enhanced through repetition of puzzles, memory games, spatial abilities and attention control. Most video games present opportunities to use these skills with the ability to try multiple times even after failure. Many of these skills can be translated to reality and problem solving. This allows the player to learn from mistakes and fully understand how and why a solution to a problem may work. Some researchers believe that continual exposure to challenges may lead players to develop greater persistence over time after a study was shown that frequent players spent more time on puzzles in task that did not involve video games. Although players were shown to spend more time on puzzles, much of that could have been due to the positive effects of problem solving in games, which involve forming strategy and weighing option before testing a solution.

Representatives of Game Academy claim that such games as Civilization, Total War, or X-Com, where strategy and resource management are key, help players to develop skills that are of great use to managers. Also, they found out that IT workers play unusual puzzle games like Portal or tower defense games like Defense Grid more often than specialists from other fields.

In a study that followed students through school, students that played video games showed higher levels of problem solving than students who did not. This contradicts the previous study in that higher success rate was seen in video game players. Time being a factor for problem solving led to different conclusions in the different studies. See video game controversies for more.

Online gaming being on the rise allows for video game players to communicate and work together in order to accomplish a certain task. Being able to work as a group in a game translates well to reality and jobs, where people must work together to accomplish a task. Research on players in violent and non-violent games show similar results, where the players' relations improved to improve synergy.

With the introduction of Wii Fit and VR (virtual reality), "exergame" popularity has been increasing, allowing video game players to experience more active rather than sedentary game play. Mobile apps have tried to expand this concept with the introduction of "Pokémon Go," which involves walking to progress in the game. Due to "exergaming" being relatively new, there is still much to be researched. No major differences were seen in tests with children that played on the Wii vs. a non-active game after 12 weeks. Testing a larger range of ages may show better results.

Cognitive remediation therapies using tailored video games to improve cognitive deficits, which are associated with poorer outcomes, have a well established efficacy. Recent studies show that commercial video games modify similar brain areas to these specialized training programs. Such games may help in the treatment of schizophrenia.

Video game laws vary from country to country. Console manufacturers usually exercise tight control over the games that are published on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, tablet computers or PDAs.

Various organisations in different regions are responsible for giving content ratings to video games.

The Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated "T" for "Teen" if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an "M" for "Mature" rating, which means that no one under 17 should play it. The rating "A/O", for "Adults Only", indicates games with massive violence or nudity. There are no laws that prohibit children from purchasing "M" rated games in the United States. Laws attempting to prohibit minors from purchasing "M" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment. However, many stores have opted to not sell such games to children anyway. One of the most controversial games of all time, "Manhunt 2" by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience.

Pan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material, it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age.

Stricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle (USK) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is considered to be harmful to young people (for example because of extremely violent, pornographic or racist content), it may be referred to the Bundesprüfstelle für jugendgefährdende Medien (BPjM) who may opt to place it on the Index upon which the game may not be sold openly or advertised in the open media. It is considered a felony to supply these games to a child.

The Computer Entertainment Rating Organization (CERO) that rates video games and PC games (except dating sims, visual novels, and eroge) in Japan with levels of rating that informs the customer of the nature of the product and for what age group it is suitable. It was established in July 2002 as a branch of Computer Entertainment Supplier's Association, and became an officially recognized non-profit organization in 2003. These ratings are:

According to the market research firm SuperData, as of May 2015, the global games market was worth US$74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play MMOs 8.7 billion, social games $7.9 billion, PC DLC 7.5 billion, and other categories $3 billion or less each.

In the United States, also according to SuperData, the share of video games in the entertainment market grew from 5% in 1985 to 13% in 2015, becoming the third-largest market segment behind broadcast and cable television. The research firm anticipated that Asia would soon overtake North America as the largest video game market due to the strong growth of free-to-play and mobile games.

Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more handheld games than console games and especially PC games, with a strong preference for games catering to local tastes. Another key difference is that, despite the decline of arcades in the West, arcade games remain an important sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.

Gaming conventions are an important showcase of the industry. The annual gamescom in Cologne in August is the world's leading expo for video games in attendance. The E3 in June in Los Angeles is also of global importance, but is an event for industry insiders only. The Tokyo Game Show in September is the main fair in Asia. Other notable conventions and trade fairs include Brasil Game Show in October, Paris Games Week in October–November, EB Games Expo (Australia) in October, KRI, ChinaJoy in July and the annual Game Developers Conference. Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.

Short for electronic sports, are video game competitions played most by professional players individually or in teams that gained popularity from the late 2000s, the most common genres are fighting, first-person shooter (FPS), multiplayer online battle arena (MOBA) and real-time strategy. There are certain games that are made for just competitive multiplayer purposes. With those type of games, players focus entirely on choosing the right character or obtaining the right equipment in the game to help them when facing other players. Tournaments are held so that people in the area or from different regions can play against other players of the same game and see who is the best. Major League Gaming (MLG) is a company that reports tournaments that are held across the country. The players that compete in these tournaments are given a rank depending on their skill level in the game that they choose to play in and face other players that play that game. The players that also compete are mostly called professional players for the fact that they have played the game they are competing in for many, long hours. Those players have been able to come up with different strategies for facing different characters. The professional players are able to pick a character to their liking and be able to master how to use that character very effectively. With strategy games, players tend to know how to get resources quick and are able to make quick decisions about where their troops are to be deployed and what kind of troops to create.

Creators will nearly always copyright their games. Laws that define copyright, and the rights that are conveyed over a video game vary from country to country. Usually a fair use copyright clause allows consumers some ancillary rights, such as for a player of the game to stream a game online. This is a vague area in copyright law, as these laws predate the advent of video games. This means that rightsholders often must define what they will allow a consumer to do with the video game.

There are many video game museums around the world, including the National Videogame Museum in Frisco, Texas, which serves as the largest museum wholly dedicated to the display and preservation of the industry's most important artifacts. Europe hosts video game museums such as the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, DC has three video games on permanent display: "Pac-Man", "Dragon's Lair", and "Pong".

The Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on "The Art of Video Games". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.



</doc>
<doc id="5367" url="https://en.wikipedia.org/wiki?curid=5367" title="Cambrian">
Cambrian

The Cambrian Period ( ) was the first geological period of the Paleozoic Era, and of the Phanerozoic Eon. The Cambrian lasted 55.6 million years from the end of the preceding Ediacaran Period 541 million years ago (mya) to the beginning of the Ordovician Period mya. Its subdivisions, and its base, are somewhat in flux. The period was established (as “Cambrian series”) by Adam Sedgwick, who named it after Cambria, the Latin name of Wales, where Britain's Cambrian rocks are best exposed. The Cambrian is unique in its unusually high proportion of sedimentary deposits, sites of exceptional preservation where "soft" parts of organisms are preserved as well as their more resistant shells. As a result, our understanding of the Cambrian biology surpasses that of some later periods.

The Cambrian marked a profound change in life on Earth; prior to the Cambrian, the majority of living organisms on the whole were small, unicellular and simple; the Precambrian "Charnia" being exceptional. Complex, multicellular organisms gradually became more common in the millions of years immediately preceding the Cambrian, but it was not until this period that mineralized—hence readily fossilized—organisms became common. The rapid diversification of life forms in the Cambrian, known as the Cambrian explosion, produced the first representatives of all modern animal phyla. Phylogenetic analysis has supported the view that during the Cambrian radiation, metazoa (animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.

Although diverse life forms prospered in the oceans, the land is thought to have been comparatively barren—with nothing more complex than a microbial soil crust and a few molluscs that emerged to browse on the microbial biofilm. Most of the continents were probably dry and rocky due to a lack of vegetation. Shallow seas flanked the margins of several continents created during the breakup of the supercontinent Pannotia. The seas were relatively warm, and polar ice was absent for much of the period.

Despite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that the Cambrian system/period was internationally ratified. The base of the Cambrian lies atop a complex assemblage of trace fossils known as the "Treptichnus pedum" assemblage.
The use of "Treptichnus pedum", a reference ichnofossil to mark the lower boundary of the Cambrian, is difficult since the occurrence of very similar trace fossils belonging to the Treptichnids group are found well below the "T. pedum" in Namibia, Spain and Newfoundland, and possibly in the western USA. The stratigraphic range of "T. pedum" overlaps the range of the Ediacaran fossils in Namibia, and probably in Spain.

The Cambrian Period followed the Ediacaran Period and was followed by the Ordovician Period. The Cambrian is divided into four epochs (series) and ten ages (stages). Currently only three series and six stages are named and have a GSSP (an internationally agreed-upon stratigraphic reference point).

Because the international stratigraphic subdivision is not yet complete, many local subdivisions are still widely used. In some of these subdivisions the Cambrian is divided into three series (epochs) with locally differing names – the Early Cambrian (Caerfai or Waucoban, mya), Middle Cambrian (St Davids or Albertan, mya) and Furongian ( mya; also known as Late Cambrian, Merioneth or Croixan). Rocks of these epochs are referred to as belonging to the Lower, Middle, or Upper Cambrian.

Trilobite zones allow biostratigraphic correlation in the Cambrian.

Each of the local series is divided into several stages. The Cambrian is divided into several regional faunal stages of which the Russian-Kazakhian system is most used in international parlance:

<nowiki>*</nowiki>Most Russian paleontologists define the lower boundary of the Cambrian at the base of the Tommotian Stage, characterized by diversification and global distribution of organisms with mineral skeletons and the appearance of the first Archaeocyath bioherms.

The International Commission on Stratigraphy list the Cambrian period as beginning at and ending at .

The lower boundary of the Cambrian was originally held to represent the first appearance of complex life, represented by trilobites. The recognition of small shelly fossils before the first trilobites, and Ediacara biota substantially earlier, led to calls for a more precisely defined base to the Cambrian period.

After decades of careful consideration, a continuous sedimentary sequence at Fortune Head, Newfoundland was settled upon as a formal base of the Cambrian period, which was to be correlated worldwide by the earliest appearance of "Treptichnus pedum". Discovery of this fossil a few metres below the GSSP led to the refinement of this statement, and it is the "T. pedum" ichnofossil assemblage that is now formally used to correlate the base of the Cambrian.

This formal designation allowed radiometric dates to be obtained from samples across the globe that corresponded to the base of the Cambrian. Early dates of quickly gained favour, though the methods used to obtain this number are now considered to be unsuitable and inaccurate. A more precise date using modern radiometric dating yield a date of . The ash horizon in Oman from which this date was recovered corresponds to a marked fall in the abundance of carbon-13 that correlates to equivalent excursions elsewhere in the world, and to the disappearance of distinctive Ediacaran fossils ("Namacalathus", " Cloudina"). Nevertheless, there are arguments that the dated horizon in Oman does not correspond to the Ediacaran-Cambrian boundary, but represents a facies change from marine to evaporite-dominated strata — which would mean that dates from other sections, ranging from 544 or 542 Ma, are more suitable.

Plate reconstructions suggest a global supercontinent, Pannotia, was in the process of breaking up early in the period, with Laurentia (North America), Baltica, and Siberia having separated from the main supercontinent of Gondwana to form isolated land masses. Most continental land was clustered in the Southern Hemisphere at this time, but was drifting north. Large, high-velocity rotational movement of Gondwana appears to have occurred in the Early Cambrian.

With a lack of sea ice – the great glaciers of the Marinoan Snowball Earth were long melted – the sea level was high, which led to large areas of the continents being flooded in warm, shallow seas ideal for sea life. The sea levels fluctuated somewhat, suggesting there were 'ice ages', associated with pulses of expansion and contraction of a south polar ice cap.

In Baltoscandia a Lower Cambrian transgression transformed large swathes of the Sub-Cambrian peneplain into an epicontinental sea.

The Earth was generally cold during the early Cambrian, probably due to the ancient continent of Gondwana covering the South Pole and cutting off polar ocean currents. However, average temperatures were 7 degrees Celsius higher than today. There were likely polar ice caps and a series of glaciations, as the planet was still recovering from an earlier Snowball Earth. It became warmer towards the end of the period; the glaciers receded and eventually disappeared, and sea levels rose dramatically. This trend would continue into the Ordovician period.

Although there were a variety of macroscopic marine plants no land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches 500 mya., and microbes forming microbial Earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation.

Most animal life during the Cambrian was aquatic. Trilobites were once assumed to be the dominant life form at that time, but this has proven to be incorrect. Arthropods were by far the most dominant animals in the ocean, but trilobites were only a minor part of the total arthropod diversity. What made them so apparently abundant was their heavy armor reinforced by calcium carbonate (CaCO), which fossilized far more easily than the fragile chitinous exoskeletons of other arthropods, leaving numerous preserved remains.

The period marked a steep change in the diversity and composition of Earth's biosphere. The Ediacaran biota suffered a mass extinction at the start of the Cambrian Period, which corresponded to an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the Cambrian, burrowing animals had destroyed the mats in many areas through bioturbation, and gradually turned the seabeds into what they are today. As a consequence, many of those organisms that were dependent on the mats became extinct, while the other species adapted to the changed environment that now offered new ecological niches.
Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla except the Bryozoa, which appeared in the Lower Ordovician. However, many of those phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.

While the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when there occurred a sharp drop in biodiversity. About 515 million years ago, the number of species going extinct exceeded the number of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also, the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 million years ago, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the level of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and showed evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids became extinct. This declining trend did not change until the Great Ordovician Biodiversification Event.

Some Cambrian organisms ventured onto land, producing the trace fossils "Protichnites" and "Climactichnites". Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the "Protichnites". Fossils of the track-maker of "Climactichnites" have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusc.

In contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor; and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.

Many modes of preservation are unique to the Cambrian, and some preserve soft body parts, resulting in an abundance of .

The United States Federal Geographic Data Committee uses a "barred capital C" character to represent the Cambrian Period.
The Unicode character is .




</doc>
<doc id="5370" url="https://en.wikipedia.org/wiki?curid=5370" title="Category of being">
Category of being

In ontology, the different kinds or ways of being are called categories of being; or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.

The process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:
Secondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.

An alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence "This is a house" the substantive subject "house" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists "inter alia" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.

In a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or "derivative" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, "Community" was an example that Kant gave of such a derivative category; the second, "Modality", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, "Spirit" or "Will" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.

In the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a "halo" or "corona" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with "a galaxy of ideas" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. "university"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions "the house is on the creek" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and "the house is eighteenth century" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition "the house is impressive or sublime" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.

One of Aristotle’s early interests lay in the classification of the natural world, how for example the genus "animal" could be first divided into "two-footed animal" and then into "wingless, two-footed animal". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition "this animal is ..." Aristotle stated in his work on the Categories that there were ten kinds of predicate where ...

"... each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon".

He realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the "categorical" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example "this is a horse running". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the "hypothetical" and "disjunctive" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.

"Category" came into use with Aristotle's essay "Categories", in which he discussed univocal and equivocal terms, predication, and ten categories:

Plotinus in writing his "Enneads" around AD 250 recorded that "philosophy at a very early age investigated the number and character of the existents ... some found ten, others less ... to some the genera were the first principles, to others only a generic classification of existents". He realised that some categories were reducible to others saying "why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue "Parmenides" and which comprised the following three coupled terms: 

Plotinus called these "the hearth of reality" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as "the three moments of the Neoplatonic world process":

Plotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. "From a single root all being multiplies". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying "Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity".

In the "Critique of Pure Reason" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of "a priori" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the "Critique", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.
Table of Judgements

Mathematical
Dynamical

Table of Categories

Mathematical
Dynamical

Criticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term "Community", and declared that the tables "do open violence to truth, treating it as nature was treated by old-fashioned gardeners", and secondly, by W.T.Stace who in his book "The Philosophy of Hegel" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.

G.W.F. Hegel in his "Science of Logic" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed "the first principle of the world, the Absolute, is a system of categories ... the categories must be the reason of which the world is a consequent".

Using his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:
Schopenhauer's category that corresponded with Notion was that of Idea, which in his ""Four-Fold Root of Sufficient Reason"" he complemented with the category of the Will. The title of his major work was "The World as Will and Idea". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the "Farbenlehre" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, "the primordial relations which belong both to nature and vision". Hegel in his "Science of Logic" accordingly asks us to see his system not as a tree but as a circle.

Charles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. 

Although Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a "compound of triadic relations". Ferdinand de Saussure, who was developing "semiology" in France just as Peirce was developing "semiotics" in the US, likened each term of a proposition to "the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge".

Edmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.

For Gilbert Ryle (1949), a category (in particular a "category mistake") is an important semantic concept, but one having only loose affinities to an ontological category.

Contemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).





</doc>
