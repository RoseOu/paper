<doc id="6799" url="https://en.wikipedia.org/wiki?curid=6799" title="COBOL">
COBOL

COBOL (; an acronym for "common business-oriented language") is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages. Most programming in COBOL is now purely to maintain existing applications.

COBOL was designed in 1959 by CODASYL and was partly based on previous programming language design work by Grace Hopper, commonly referred to as "the (grand)mother of COBOL". It was created as part of a US Department of Defense effort to create a portable programming language for data processing. It was originally seen as a stopgap, but the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption. It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is "ISO/IEC 1989:2014".

COBOL statements have an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like , COBOL has a more English-like syntax (in this case, ).
COBOL code is split into four "divisions" (identification, environment, data and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.

Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for business, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.
COBOL has been criticized throughout its life, for its verbosity, design process, and poor support for structured programming. These weaknesses result in monolithic and, though intended to be English-like, not easily comprehensible and verbose programs.

In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever-increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.

On 8 April 1959, Mary K. Hawes, a computer scientist at Burroughs Corporation, called a meeting of representatives from academia, computer users, and manufacturers at the University of Pennsylvania to organize a formal meeting on common business languages. Representatives included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.

At the April meeting, the group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD, who thought that they "thoroughly understood" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.

Phillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.

On 28 and 29 May 1959 (exactly one year after the ZÃ¼rich ALGOL 58 meeting), a meeting was held at the Pentagon to discuss the creation of a common programming language for business. It was attended by 41 people and was chaired by Phillips. The Department of Defense was concerned about whether it could run the same data processing programs on different computers. FORTRAN, the only mainstream language at the time, lacked the features needed to write such programs.

Representatives enthusiastically described a language that could work in a wide variety of environments, from banking and insurance to utilities and inventory control. They agreed unanimously that more people should be able to program and that the new language should not be restricted by the limitations of contemporary technology. A majority agreed that the language should make maximal use of English, be capable of change, be machine-independent and be easy to use, even at the expense of power.

The meeting resulted in the creation of a steering committee and short-, intermediate- and long-range committees. The short-range committee was given to September (three months) to produce specifications for an interim language, which would then be improved upon by the other committees. Their official mission, however, was to identify the strengths and weaknesses of existing programming languages and did not explicitly direct them to create a new language.
The deadline was met with disbelief by the short-range committee.
One member, Betty Holberton, described the three-month deadline as "gross optimism" and doubted that the language really would be a stopgap.

The steering committee met on 4 June and agreed to name the entire activity as the "Committee on Data Systems Languages", or CODASYL, and to form an executive committee.

The short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology). The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.

The committee mainly examined the FLOW-MATIC, AIMACO and COMTRAN programming languages.
The FLOW-MATIC language was particularly influential because it had been implemented and because AIMACO was a derivative of it with only minor changes.
FLOW-MATIC's inventor, Grace Hopper, also served as a technical adviser to the committee. FLOW-MATIC's major contributions to COBOL were long variable names, English words for commands and the separation of data descriptions and instructions.

IBM's COMTRAN language, invented by Bob Bemer, was regarded as a competitor to FLOW-MATIC by a short-range committee made up of colleagues of Grace Hopper.
Some of its features were not incorporated into COBOL so that it would not look like IBM had dominated the design process, and Jean Sammet said in 1981 that there had been a "strong anti-IBM bias" from some committee members (herself included).
In one case, after Roy Goldfinger, author of the COMTRAN manual and intermediate-range committee member, attended a subcommittee meeting to support his language and encourage the use of algebraic expressions, Grace Hopper sent a memo to the short-range committee reiterating Sperry Rand's efforts to create a language based on English.
In 1980, Grace Hopper commented that "COBOL 60 is 95% FLOW-MATIC" and that COMTRAN had had an "extremely small" influence. Furthermore, she said that she would claim that work was influenced by both FLOW-MATIC and COMTRAN only to "keep other people happy [so they] wouldn't try to knock us out".
Features from COMTRAN incorporated into COBOL included formulas, the clause, an improved codice_1 statement, which obviated the need for GO TOs, and a more robust file management system.

The usefulness of the committee's work was subject of great debate. While some members thought the language had too many compromises and was the result of design by committee, others felt it was better than the three languages examined. Some felt the language was too complex; others, too simple.
Controversial features included those some considered useless or too advanced for data processing users. Such features included boolean expressions, formulas and table "" (indices). Another point of controversy was whether to make keywords context-sensitive and the effect that would have on readability. Although context-sensitive keywords were rejected, the approach was later used in PL/I and partially in COBOL from 2002. Little consideration was given to interactivity, interaction with operating systems (few existed at that time) and functions (thought of as purely mathematical and of no use in data processing).

The specifications were presented to the Executive Committee on 4 September. They fell short of expectations: Joseph Wegstein noted that "it contains rough spots and requires some additions", and Bob Bemer later described them as a "hodgepodge". The subcommittee was given until December to improve it.

At a mid-September meeting, the committee discussed the new language's name. Suggestions included "BUSY" (Business System), "INFOSYL" (Information System Language) and "COCOSYL" (Common Computer Systems Language). The name "COBOL" was suggested by Bob Bemer.

In October, the intermediate-range committee received copies of the FACT language specification created by Roy Nutt. Its features impressed the committee so much that they passed a resolution to base COBOL on it.
This was a blow to the short-range committee, who had made good progress on the specification. Despite being technically superior, FACT had not been created with portability in mind or through manufacturer and user consensus. It also lacked a demonstrable implementation, allowing supporters of a FLOW-MATIC-based COBOL to overturn the resolution. RCA representative Howard Bromberg also blocked FACT, so that RCA's work on a COBOL implementation would not go to waste.

It soon became apparent that the committee was too large for any further progress to be made quickly. A frustrated Howard Bromberg bought a $15 tombstone with "COBOL" engraved on it and sent it to Charles Phillips to demonstrate his displeasure.
A sub-committee was formed to analyze existing languages and was made up of six individuals:


The sub-committee did most of the work creating the specification, leaving the short-range committee to review and modify their work before producing the finished specification.

The specifications were approved by the Executive Committee on 8 January 1960, and sent to the government printing office, which printed these as "COBOL 60". The language's stated objectives were to allow efficient, portable programs to be easily written, to allow users to move to new systems with minimal effort and cost, and to be suitable for inexperienced programmers.
The CODASYL Executive Committee later created the COBOL Maintenance Committee to answer questions from users and vendors and to improve and expand the specifications.

During 1960, the list of manufacturers planning to build COBOL compilers grew. By September, five more manufacturers had joined CODASYL (Bendix, Control Data Corporation, General Electric (GE), National Cash Register and Philco), and all represented manufacturers had announced COBOL compilers. GE and IBM planned to integrate COBOL into their own languages, GECOM and COMTRAN, respectively. In contrast, International Computers and Tabulators planned to replace their language, CODEL, with COBOL.

Meanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.
On 6 and 7 December, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.

The relative influences of which languages were used continues to this day in the recommended advisory printed in all COBOL reference manuals:
Many logical flaws were found in "COBOL 60", leading GE's Charles Katz to warn that it could not be interpreted unambiguously. A reluctant short-term committee enacted a total cleanup and, by March 1963, it was reported that COBOL's syntax was as definable as ALGOL's, although semantic ambiguities remained.

Early COBOL compilers were primitive and slow. A 1962 US Navy evaluation found compilation speeds of 3â11 statements per minute. By mid-1964, they had increased to 11â1000 statements per minute. It was observed that increasing memory would drastically increase speed and that compilation costs varied wildly: costs per statement were between $0.23 and $18.91.

In late 1962, IBM announced that COBOL would be their primary development language and that development of COMTRAN would cease.

The COBOL specification was revised three times in the five years after its publication.
COBOL-60 was replaced in 1961 by COBOL-61. This was then replaced by the COBOL-61 Extended specifications in 1963, which introduced the sort and report writer facilities.
The added facilities corrected flaws identified by Honeywell in late 1959 in a letter to the short-range committee.
COBOL Edition 1965 brought further clarifications to the specifications and introduced facilities for handling mass storage files and tables.

Efforts began to standardize COBOL to overcome incompatibilities between versions. In late 1962, both ISO and the United States of America Standards Institute (now ANSI) formed groups to create standards. ANSI produced "USA Standard COBOL X3.23" in August 1968, which became the cornerstone for later versions. This version was known as American National Standard (ANS) COBOL and was adopted by ISO in 1972.

By 1970, COBOL had become the most widely used programming language in the world.

Independently of the ANSI committee, the CODASYL Programming Language Committee was working on improving the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and file merging facilities as well as improved string-handling and library inclusion features.
Although CODASYL was independent of the ANSI committee, the "CODASYL Journal of Development" was used by ANSI to identify features that were popular enough to warrant implementing.
The Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.

The Programming Language Committee was not well-known, however. The vice-president, William Rinehuls, complained that two-thirds of the COBOL community did not know of the committee's existence. It was also poor, lacking the funds to make public documents, such as minutes of meetings and change proposals, freely available.

In 1974, ANSI published a revised version of (ANS) COBOL, containing new features such as file organizations, the statement and the segmentation module.
Deleted features included the statement, the statement (which was replaced by ) and the implementer-defined random access module (which was superseded by the new sequential and relative I/O modules). These made up 44 changes, which rendered existing statements incompatible with the new standard.
The report writer was slated to be removed from COBOL, but was reinstated before the standard was published. ISO later adopted the updated standard in 1978.

In June 1978, work began on revising COBOL-74. The proposed standard (commonly called COBOL-80) differed significantly from the previous one, causing concerns about incompatibility and conversion costs. In January 1981, Joseph T. Brophy, Senior Vice-President of Travelers Insurance, threatened to sue the standard committee because it was not upwards compatible with COBOL-74. Mr. Brophy described previous conversions of their 40-million-line code base as "non-productive" and a "complete waste of our programmer resources".
Later that year, the Data Processing Management Association (DPMA) said it was "strongly opposed" to the new standard, citing "prohibitive" conversion costs and enhancements that were "forced on the user".

During the first public review period, the committee received 2,200 responses, of which 1,700 were negative form letters.
Other responses were detailed analyses of the effect COBOL-80 would have on their systems; conversion costs were predicted to be at least 50 cents per line of code. Fewer than a dozen of the responses were in favor of the proposed standard.

ISO TC97-SC5 installed in 1979 the international COBOL Experts Group, on initiative of Wim Ebbinkhuijsen. The group consisted of COBOL experts from many countries, including the USA. Its goal was to achieve mutual understanding and respect between ANSI and the rest of the world with regard to the need of new COBOL features. After three years, ISO changed the status of the group to a formal Working Group: WG4 COBOL. The group took primary ownership and development of the COBOL standard, where ANSI did most of the proposals.

In 1983, the DPMA withdrew its opposition to the standard, citing the responsiveness of the committee to public concerns. In the same year, a National Bureau of Standards study concluded that the proposed standard would present few problems. A year later, a COBOL-80 compiler was released to DEC VAX users, who noted that conversion of COBOL-74 programs posed few problems. The new codice_2 statement and inline codice_3 were particularly well received and improved productivity, thanks to simplified control flow and debugging.

The second public review drew another 1,000 (mainly negative) responses, while the last drew just 25, by which time many concerns had been addressed.

In 1985, the ISO Working Group 4 accepted the then-version of the ANSI proposed standard, made several changes and set it as the new ISO standard COBOL 85. It was published in late 1985.

Sixty features were changed or deprecated and many were added, such as:


The new standard was adopted by all national standard bodies, including ANSI.

Two amendments followed in 1989 and 1993, the first introducing intrinsic functions and the other providing corrections.,

In 1997, Gartner Group estimated that there were a total of 200 billion lines of COBOL in existence, which ran 80% of all business programs.

In the early 1990s, work began on adding object-orientation in the next full revision of COBOL. Object-oriented features were taken from C++ and Smalltalk.
The initial estimate was to have this revision completed by 1997, and an ISO Committee Draft (CD) was available by 1997. Some vendors (including Micro Focus, Fujitsu, and IBM) introduced object-oriented syntax based on drafts of the full revision. The final approved ISO standard was approved and published in late 2002.

Fujitsu/GTSoftware, Micro Focus and RainCode introduced object-oriented COBOL compilers targeting the .NET Framework.

There were many other new features, many of which had been in the "CODASYL COBOL Journal of Development" since 1978 and had missed the opportunity to be included in COBOL-85. These other features included:


Three corrigenda were published for the standard: two in 2006 and one in 2009.

Between 2003 and 2009, three technical reports were produced describing object finalization, XML processing and collection classes for COBOL.

COBOL 2002 suffered from poor support: no compilers completely supported the standard. Micro Focus found that it was due to a lack of user demand for the new features and due to the abolition of the NIST test suite, which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.

COBOL 2014 includes the following changes:

COBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, z/VSE, VME, Unix, OpenVMS and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code and 5 billion lines more being written annually.

Near the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields. After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.
The authors said that the survey data suggest "a gradual decline in the importance of Cobol in application development over the [following] 10 years unless ... integration with other languages and technologies can be adopted".

In 2006 and 2012, "Computerworld" surveys found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software. 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it was cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.

Testimony before the House of Representatives in 2016 indicated that COBOL is still in use by many federal agencies.

COBOL has an English-like syntax, which is used to describe nearly everything in a program. For example, a condition can be expressed as Â  or more concisely as Â Â  or Â . More complex conditions can be "abbreviated" by removing repeated conditions and variables. For example, Â Â  can be shortened to . As a consequence of this English-like syntax, COBOL has over 300 keywords. Some of the keywords are simple alternative or pluralized spellings of the same word, which provides for more English-like statements and clauses; e.g., the and keywords can be used interchangeably, as can and , and and .

Each COBOL program is made up of four basic lexical items: words, literals, picture character-strings (see ) and separators. Words include reserved words and user-defined identifiers. They are up to 31 characters long and may include letters, digits, hyphens and underscores. Literals include numerals (e.g. ) and strings (e.g. ). Separators include the space character and commas and semi-colons followed by a space.

A COBOL program is split into four divisions: the identification division, the environment division, the data division and the procedure division. The identification division specifies the name and type of the source element and is where classes and interfaces are specified. The environment division specifies any program features that depend on the system running it, such as files and character sets. The data division is used to declare variables and parameters. The procedure division contains the program's statements. Each division is sub-divided into sections, which are made up of paragraphs.

COBOL's syntax is usually described with a unique metalanguage using braces, brackets, bars and underlining. The metalanguage was developed for the original COBOL specifications. Although BackusâNaur form did exist at the time, the committee had not heard of it.

As an example, consider the following description of an codice_12 statement:

formula_1

This description permits the following variants:
ADD 1 TO x
ADD 1, a, b TO x ROUNDED, y, z ROUNDED

ADD a, b TO c
END-ADD

ADD a TO b
COBOL can be written in two formats: fixed (the default) or free. In fixed-format, code must be aligned to fit in certain areas (a hold-over from using punched cards). Until COBOL 2002, these were:

In COBOL 2002, Areas A and B were merged to form the program-text area, which now ends at an implementor-defined column.

COBOL 2002 also introduced free-format code. Free-format code can be placed in any column of the file, as in newer programming languages. Comments are specified using codice_13, which can be placed anywhere and can also be used in fixed-format source code. Continuation lines are not present, and the codice_14 directive replaces the codice_15 indicator.

The identification division identifies the following code entity and contains the definition of a class or interface.

Classes and interfaces have been in COBOL since 2002. Classes have factory objects, containing class methods and variables, and instance objects, containing instance methods and variables. Inheritance and interfaces provide polymorphism. Support for generic programming is provided through parameterized classes, which can be instantiated to use any class or interface. Objects are stored as references which may be restricted to a certain type. There are two ways of calling a method: the statement, which acts similarly to , or through inline method invocation, which is analogous to using functions.
INVOKE my-class "foo" RETURNING var
MOVE my-class::"foo" TO var *> Inline method invocation
COBOL does not provide a way to hide methods. Class data can be hidden, however, by declaring it without a clause, which leaves the user with no way to access it. Method overloading was added in COBOL 2014.

The environment division contains the configuration section and the input-output section. The configuration section is used to specify variable features such
as currency signs, locales and character sets. The input-output section contains file-related information.

COBOL supports three file formats, or ': sequential, indexed and relative. In sequential files, records are contiguous and must be traversed sequentially, similarly to a linked list. Indexed files have one or more indexes which allow records to be randomly accessed and which can be sorted on them. Each record must have a unique key, but other, ', record keys need not be unique. Implementations of indexed files vary between vendors, although common implementations, such as CâISAM and VSAM, are based on IBM's ISAM. Relative files, like indexed files, have a unique record key, but they do not have alternate keys. A relative record's key is its ordinal position; for example, the 10th record has a key of 10. This means that creating a record with a key of 5 may require the creation of (empty) preceding records. Relative files also allow for both sequential and random access.

A common non-standard extension is the " organization, used to process text files. Records in a file are terminated by a newline and may be of varying length.

The data division is split into six sections which declare different items: the file section, for file records; the working-storage section, for static variables; the local-storage section, for automatic variables; the linkage section, for parameters and the return value; the report section and the screen section, for text-based user interfaces.

Data items in COBOL are declared hierarchically through the use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level data items, with a level-number of 1, are called '. Items that have subordinate aggregate data are called '; those that do not are called ". Level-numbers used to describe standard data items are between 1 and 49.
In the above example, elementary item and group item are subordinate to the record , while elementary items , , and are part of the group item .

Subordinate items can be disambiguated with the (or ) keyword. For example, consider the example code above along with the following example:

The names , , and are ambiguous by themselves, since more than one data item is defined with those names. To specify a particular data item, for instance one of the items contained within the group, the programmer would use (or the equivalent ). (This syntax is similar to the "dot notation" supported by most contemporary languages.)

A level-number of 66 is used to declare a re-grouping of previously defined items, irrespective of how those items are structured. This data level, also referred to by the associated , is rarely used and, circa 1988, was usually found in old programs. Its ability to ignore the hierarchical and logical structure data meant its use was not recommended and many installations forbade its use.

A 77 level-number indicates the item is stand-alone, and in such situations is equivalent to the level-number 01. For example, the following code declares two 77-level data items, and , which are non-group data items that are independent of (not subordinate to) any other data items:

An 88 level-number declares a " (a so-called 88-level) which is true when its parent data item contains one of the values specified in its clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the data item. When the data item contains a value of , the condition-name is true, whereas when it contains a value of or , the condition-name is true. If the data item contains some other value, both of the condition-names are false.

Standard COBOL provides the following data types:

Type safety is variable in COBOL. Numeric data is converted between different representations and sizes silently and alphanumeric data can be placed in any data item that can be stored as a string, including numeric and group data. In contrast, object references and pointers may only be assigned from items of the same type and their values may be restricted to a certain type.

A (or ) clause is a string of characters, each of which represents a portion of the data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a indicates a decimal digit, and an indicates that the item is signed. Other picture characters (called ' and ' characters) specify how an item should be formatted. For example, a series of characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, is equivalent to . Picture specifications containing only digit () and sign () characters define purely ' data items, while picture specifications containing alphabetic () or alphanumeric () characters define ' data items. The presence of other formatting characters define ' or ' data items.

The clause declares the format data is stored in. Depending on the data type, it can either complement or be used instead of a clause. While it can be used to declare pointers and object references, it is mostly geared towards specifying numeric types. These numeric formats are:


The report writer is a declarative facility for creating reports. The programmer need only specify the report layout and the data required to produce it, freeing them from having to write code to handle things like page breaks, data formatting, and headings and footings.

Reports are associated with report files, which are files which may only be written to through report writer statements.
Each report is defined in the report section of the data division. A report is split into report groups which define the report's headings, footings and details. Reports work around hierarchical ". Control breaks occur when a key variable changes it value; for example, when creating a report detailing customers' orders, a control break could occur when the program reaches a different customer's orders. Here is an example report description for a report which gives a salesperson's sales and which warns of any invalid records:

The above report description describes the following layout:
Four statements control the report writer: , which prepares the report writer for printing; , which prints a report group; , which suppresses the printing of a report group; and , which terminates report processing. For the above sales report example, the procedure division might look like this:

Use of the Report Writer facility tended to vary considerably; some organizations used it extensively and some not at all. In addition, implementations of Report Writer ranged in quality, with those at the lower end sometimes using excessive amounts of memory at runtime.

The sections and paragraphs in the procedure division (collectively called procedures) can be used as labels and as simple subroutines. Unlike in other divisions, paragraphs do not need to be in sections.
Execution goes down through the procedures of a program until it is terminated.
To use procedures as subroutines, the verb is used.

A statement somewhat resembles a procedure call in a modern language in the sense that execution returns to the code following the statement at the end of the called code; however, it does not provide any mechanism for parameter passing or for returning a result value. If a subroutine is invoked using a simple statement like , then control returns at the end of the called procedure. However, is unusual in that it may be used to call a range spanning a sequence of several adjacent procedures. This is done with the construct:
PROCEDURE so-and-so.
ALPHA.
BETA.
GAMMA.
The output of this program will be: "A A B C".

The reason is that COBOL, rather than a "return address", operates with what may be called a continuation address. When control flow reaches the end of any procedure, the continuation address is looked up and control is transferred to that address. Before the program runs, the continuation address for every procedure is initialised to the start address of the procedure that comes next in the program text so that, if no statements happen, control flows from top to bottom through the program. But when a statement executes, it modifies the continuation address of the called procedure (or the last procedure of the called range, if was used), so that control will return to the call site at the end. The original value is saved and is restored afterwards, but there is only one storage position. If two nested invocations operate on overlapping code, they may interfere which each other's management of the continuation address in several ways.

The following example (taken from Veerman/Verhoeven, 2006) illustrates the problem:
LABEL1.
LABEL2.
LABEL3.
LABEL4.
One might expect that the output of this program would be "1 2 3 4 3": After displaying "2", the second causes "3" and "4" to be displayed, and then the first invocation continues on with "3". In traditional COBOL implementations, this is not the case. Rather, the first statement sets the continuation address at the end of so that it will jump back to the call site inside . The second statement sets the return at the end of but does not modify the continuation address of , expecting it to be the default continuation. Thus, when the inner invocation arrives at the end of , it jumps back to the outer statement, and the program stops having printed just "1 2 3". On the other hand, in some COBOL implementations like the open-source TinyCOBOL compiler, the two statements do not interfere with each other and the output is indeed "1 2 3 4 3". Therefore, the behaviour in such cases is not only (perhaps) surprising, it is also not portable.

A special consequence of this limitation is that cannot be used to write recursive code. Another simple example to illustrate this (slightly simplified from Veerman/Verhoeven, 2006):
LABEL.
One might expect that the output is "1 2 3 END END END", and in fact that is what some COBOL compilers will produce. But some compilers, like IBM COBOL, will produce code that prints "1 2 3 END END END END ..." and so on, printing "END" over and over in an endless loop. Since there is limited space to store backup continuation addresses, the backups get overwritten in the course of recursive invocations, and all that can be restored is the jump back to .

COBOL 2014 has 47 statements (also called "), which can be grouped into the following broad categories: control flow, I/O, data manipulation and the report writer. The report writer statements are covered in the report writer section.

COBOL's conditional statements are and . is a switch-like statement with the added capability of evaluating multiple values and conditions. This can be used to implement decision tables. For example, the following might be used to control a CNC lathe: 
EVALUATE TRUE ALSO desired-speed ALSO current-speed
END-EVALUATE
The statement is used to define loops which are executed a condition is true (not true, which is more common in other languages). It is also used to call procedures or ranges of procedures (see the procedures section for more details). and call subprograms and methods, respectively. The name of the subprogram/method is contained in a string which may be a literal or a data item. Parameters can be passed by reference, by content (where a copy is passed by reference) or by value (but only if a prototype is available).

The statement is a return statement and the statement stops the program. The statement has six different formats: it can be used as a return statement, a break statement, a continue statement, an end marker or to leave a procedure.

Exceptions are raised by a statement and caught with a handler, or ", defined in the portion of the procedure division. Declaratives are sections beginning with a statement which specify the errors to handle. Exceptions can be names or objects. is used in a declarative to jump to the statement after the one that raised the exception or to a procedure outside the . Unlike other languages, uncaught exceptions may not terminate the program and the program can proceed unaffected.

File I/O is handled by the self-describing , , , and statements along with a further three: , which updates a record; , which selects subsequent records to access by finding a record with a certain key; and , which releases a lock on the last record accessed.

User interaction is done using and .

The following verbs manipulate data:

Files and tables are sorted using and the verb merges and sorts files. The verb provides records to sort and retrieves sorted records in order.

Some statements, such as and , may themselves contain statements. Such statements may be terminated in two ways: by a period (""), which terminates "all" unterminated statements contained, or by a scope terminator, which terminates the nearest matching open statement.
IF invalid-record

IF invalid-record
END-IF
Nested statements terminated with a period are a common source of bugs. For example, examine the following code:
IF x
Here, the intent is to display codice_19 and codice_20 if condition codice_21 is true. However, codice_20 will be displayed whatever the value of codice_21 because the codice_1 statement is terminated by an erroneous period after .

Another bug is a result of the dangling else problem, when two codice_1 statements can associate with an codice_26.
IF x
ELSE
In the above fragment, the codice_26 associates with the Â Â  statement instead of the Â Â  statement, causing a bug. Prior to the introduction of explicit scope terminators, preventing it would require Â Â  to be placed after the inner codice_1.

The original (1959) COBOL specification supported the infamous Â Â  statement, for which many compilers generated self-modifying code. codice_29 and codice_30 are procedure labels, and the single Â Â  statement in procedure codice_29 executed after such an statement means Â Â  instead. Many compilers still support it,
but it was deemed obsolete in the COBOL 1985 standard and deleted in 2002.

The statement was poorly regarded because it undermined "locality of context" and made a program's overall logic difficult to comprehend. As textbook author Daniel D. McCracken wrote in 1976, when "someone who has never seen the program before must become familiar with it as quickly as possible, sometimes under critical time pressure because the program has failed ... the sight of a GO TO statement in a paragraph by itself, signaling as it does the existence of an unknown number of ALTER statements at unknown locations throughout the program, strikes fear in the heart of the bravest programmer."

A "Hello, world" program in COBOL:

When the â now famous â "Hello, World!" program example in "The C Programming Language" was first published in 1978 a similar mainframe COBOL program sample would have been submitted through JCL, very likely using a punch card reader, and 80 column punch cards. The listing below, "with an empty DATA DIVISION", was tested using GNU/Linux and the System/370 Hercules emulator running MVS 3.8J. The JCL, written in July 2015, is derived from the Hercules tutorials and samples hosted by Jay Moseley. In keeping with COBOL programming of that era, HELLO, WORLD is displayed in all capital letters.
//COBUCLG JOB (001),'COBOL BASE TEST', 00010000
// CLASS=A,MSGCLASS=A,MSGLEVEL=(1,1) 00020000
//BASETEST EXEC COBUCLG 00030000
//COB.SYSIN DD * 00040000
//LKED.SYSLIB DD DSNAME=SYS1.COBLIB,DISP=SHR 00190000
// DD DSNAME=SYS1.LINKLIB,DISP=SHR 00200000
//GO.SYSPRINT DD SYSOUT=A 00210000
// 00220000
After submitting the JCL, the MVS console displayed:
"Line 10 of the console listing above is highlighted for effect, the highlighting is not part of the actual console output".

The associated compiler listing generated over four pages of technical detail and job run information, for the single line of output from the 14 lines of COBOL.

In the 1970s, adoption of the structured programming paradigm was becoming increasingly widespread. Edsger Dijkstra, a preeminent computer scientist, wrote a letter to the editor of Communications of the ACM, published 1975 entitled "How do we tell truths that might hurt?", in which he was critical of COBOL and several other contemporary languages; remarking that "the use of COBOL cripples the mind".
In a published dissent to Dijkstra's remarks, the computer scientist Howard E. Tompkins claimed that unstructured COBOL tended to be "written by programmers that have never had the benefit of structured COBOL taught well", arguing that the issue was primarily one of training.

One cause of spaghetti code was the statement. Attempts to remove s from COBOL code, however, resulted in convoluted programs and reduced code quality. s were largely replaced by the statement and procedures, which promoted modular programming and gave easy access to powerful looping facilities. However, could only be used with procedures so loop bodies were not located where they were used, making programs harder to understand.

COBOL programs were infamous for being monolithic and lacking modularization.
COBOL code could only be modularized through procedures, which were found to be inadequate for large systems. It was impossible to restrict access to data, meaning a procedure could access and modify data item. Furthermore, there was no way to pass parameters to a procedure, an omission Jean Sammet regarded as the committee's biggest mistake.
Another complication stemmed from the ability to a specified sequence of procedures. This meant that control could jump to and return from any procedure, creating convoluted control flow and permitting a programmer to break the single-entry single-exit rule.

This situation improved as COBOL adopted more features. COBOL-74 added subprograms, giving programmers the ability to control the data each part of the program could access. COBOL-85 then added nested subprograms, allowing programmers to hide subprograms. Further control over data and code came in 2002 when object-oriented programming, user-defined functions and user-defined data types were included.

Nevertheless, much important legacy COBOL software uses unstructured code, which has become unmaintainable. It can be too risky and costly to modify even a simple section of code, since it may be used from unknown places in unknown ways.

COBOL was intended to be a highly portable, "common" language. However, by 2001, around 300 dialects had been created. One source of dialects was the standard itself: the 1974 standard was composed of one mandatory nucleus and eleven functional modules, each containing two or three levels of support. This permitted 104,976 official variants.

COBOL-85 was not fully compatible with earlier versions, and its development was controversial. Joseph T. Brophy, the CIO of Travelers Insurance, spearheaded an effort to inform COBOL users of the heavy reprogramming costs of implementing the new standard. As a result, the ANSI COBOL Committee received more than 2,200 letters from the public, mostly negative, requiring the committee to make changes. On the other hand, conversion to COBOL-85 was thought to increase productivity in future years, thus justifying the conversion costs.

COBOL syntax has often been criticized for its verbosity. Proponents say that this was intended to make the code self-documenting, easing program maintenance. COBOL was also intended to be easy for programmers to learn and use, while still being readable to non-technical staff such as managers.
The desire for readability led to the use of English-like syntax and structural elements, such as nouns, verbs, clauses, sentences, sections, and divisions. Yet by 1984, maintainers of COBOL programs were struggling to deal with "incomprehensible" code and the main changes in COBOL-85 were there to help ease maintenance.

Jean Sammet, a short-range committee member, noted that "little attempt was made to cater to the professional programmer, in fact people whose main interest is programming tend to be very unhappy with COBOL" which she attributed to COBOL's verbose syntax.

The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled. Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing. The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new BackusâNaur form which the committee did not know of. This resulted in "severe" criticism.

Later, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear (with Richard D. Irwin publishing a college textbook on COBOL in 1966). By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress. University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a "trade school" nature. Donald Nelson, chair of the CODASYL COBOL committee, said in 1984 that "academics ... hate COBOL" and that computer science graduates "had 'hate COBOL' drilled into them". A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.
In contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.

There was also significant condescension towards COBOL in the business community from users of other languages, for example FORTRAN or assembler, implying that COBOL could be used only for non-challenging problems.

Doubts have been raised about the competence of the standards committee. Short-term committee member Howard Bromberg said that there was "little control" over the development process and that it was "plagued by discontinuity of personnel and ... a lack of talent." Jean Sammet and Jerome Garfunkel also noted that changes introduced in one revision of the standard would be reverted in the next, due as much to changes in who was in the standard committee as to objective evidence.

COBOL standards have repeatedly suffered from delays: COBOL-85 arrived five years later than hoped,
COBOL 2002 was five years late,
and COBOL 2014 was six years late.
To combat delays, the standard committee allowed the creation of optional addenda which would add features more quickly than by waiting for the next standard revision. However, some committee members raised concerns about incompatibilities between implementations and frequent modifications of the standard.

COBOL's data structures influenced subsequent programming languages. Its record and file structure influenced PL/I and Pascal, and the codice_32 clause was a predecessor to Pascal's variant records. Explicit file structure definitions preceded the development of database management systems and aggregated data was a significant advance over Fortran's arrays.
codice_16 data declarations were incorporated into PL/I, with minor changes.

COBOL's facility, although considered "primitive",
influenced the development of include directives.

The focus on portability and standardization meant programs written in COBOL could be portable and facilitated the spread of the language to a wide variety of hardware platforms and operating systems. Additionally, the well-defined division structure restricts the definition of external references to the Environment Division, which simplifies platform changes in particular.





</doc>
<doc id="6801" url="https://en.wikipedia.org/wiki?curid=6801" title="Crew">
Crew

A crew is a body or a class of people who work at a common activity, generally in a structured or hierarchical organization. A location in which a crew works is called a crewyard or a workyard. The word has nautical resonances: the tasks involved in operating a ship, particularly a sailing ship, providing numerous specialities within a ship's crew, often organised with a chain of command. Traditional nautical usage strongly distinguishes officers from crew, though the two groups combined form the ship's company. Members of a crew are often referred to by the title "Crewman".

"Crew" also refers to the sport of rowing, where teams row competitively in racing shells.




</doc>
<doc id="6803" url="https://en.wikipedia.org/wiki?curid=6803" title="CCD">
CCD

CCD may refer to:












</doc>
<doc id="6804" url="https://en.wikipedia.org/wiki?curid=6804" title="Charge-coupled device">
Charge-coupled device

A charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated, such as conversion into a digital value. This is achieved by "shifting" the signals between stages within the device one at a time. CCDs move charge between capacitive "bins" in the device, with the shift allowing for the transfer of charge between bins.

CCD is a major technology for digital imaging. In a CCD image sensor, pixels are represented by p-doped metalâoxideâsemiconductor (MOS) capacitors. These MOS capacitors, the basic building blocks of a CCD, are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges. Although CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data are required. In applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors, also known as CMOS sensors (complementary MOS sensors), are generally used. However, the large quality advantage CCDs enjoyed early on has narrowed over time.

The basis for the CCD is the metalâoxideâsemiconductor (MOS) structure, with MOS capacitors being the basic building blocks of a CCD, and a depleted MOS structure used as the photodetector in early CCD devices. MOS technology was originally invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.

In the late 1960s, Willard Boyle and George E. Smith at Bell Labs were researching MOS technology while working on semiconductor bubble memory. They realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next. This led to the invention of the charge-coupled device by Boyle and Smith in 1969. They conceived of the design of what they termed, in their notebook, "Charge 'Bubble' Devices".

The initial paper describing the concept in April 1970 listed possible uses as memory, a delay line, and an imaging device. The device could also be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s.

The first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds. It was demonstrated by Gil Amelio, Michael Francis Tompsett and George Smith in April 1970. This was the first experimental application of the CCD in image sensor technology, and used a depleted MOS structure as the photodetector. The first patent () on the application of CCDs to imaging was assigned to Tompsett, who filed the application in 1971.

The first working CCD made with integrated circuit technology was a simple 8-bit shift register, reported by Tompsett, Amelio and Smith in August 1970. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device.
Development of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.
Several companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2-D 100 x 100 pixel device. Steven Sasson, an electrical engineer working for Kodak, invented the first digital still camera using a Fairchild CCD in 1975.

The interline transfer (ILT) CCD device was proposed by L. Walsh and R. Dyck at Fairchild in 1973 to reduce smear and eliminate a mechanical shutter. To further reduce smear from bright light sources, the frame-interline-transfer (FIT) CCD architecture was developed by K. Horii, T. Kuroda and T. Kunii at Matsushita (now Panasonic) in 1981.

The first KH-11 KENNEN reconnaissance satellite equipped with charge-coupled device array ( pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982; subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution. The first mass-produced consumer CCD video camera was released by Sony in 1983, based on a prototype developed by Yoshiaki Hagiwara in 1981.

Early CCD sensors suffered from shutter lag. This was largely resolved with the invention of the pinned photodiode (PPD). It was invented by Nobukazu Teranishi, Hiromitsu Shiraki and Yasuo Ishihara at NEC in 1980. They recognized that lag can be eliminated if the signal carriers could be transferred from the photodiode to the CCD. This led to their invention of the pinned photodiode, a photodetector structure with low lag, low noise, high quantum efficiency and low dark current. It was first publicly reported by Teranishi and Ishihara with A. Kohono, E. Oda and K. Arai in 1982, with the addition of an anti-blooming structure. The new photodetector structure invented at NEC was given the name "pinned photodiode" (PPD) by B.C. Burkey at Kodak in 1984. In 1987, the PPD began to be incorporated into most CCD devices, becoming a fixture in consumer electronic video cameras and then digital still cameras. Since then, the PPD has been used in nearly all CCD sensors and then CMOS sensors.

In January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics, for their invention of the CCD concept. Michael Tompsett was awarded the 2010 National Medal of Technology and Innovation, for pioneering work and electronic technologies including the design and development of the first CCD imagers. He was also awarded the 2012 IEEE Edison Medal for "pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers".

In a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).

An image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.

Before the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly "p"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of a "n" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.
Then, when electronâhole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:

The last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 10 electrons per pixel.

The photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly "p" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:
This thin layer (= 0.2â0.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2â3 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.

Later in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.

Channel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or "charge carrying", regions.

Channel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).

The clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the pân junction and will collect and move the charge packets beneath the gatesâand within the channelsâof the device.

CCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.

Another version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.

The CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.

In a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.

With a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.

The interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.

The choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.

CCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.

Most common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1Â Î¼m. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.

Cooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.

The frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.

The normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as "vertical smear" and cause a strong light source to create a vertical line above and below its exact location. In addition, the CCD cannot be used to collect light while it is being read out. Unfortunately, a faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.

A frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.

The disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.

An intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.

An image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.

An image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called "gating" and therefore ICCDs are also called gateable CCD cameras.

Besides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.

ICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand, EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around 170 K. This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.

ICCDs are used in night vision devices and in various scientific applications.

An electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by e2v Ltd., GB, L3CCD or Impactron CCD, a now-discontinued product offered in the past by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small ("P" < 2%), but as the number of elements is large (N > 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. The use of avalanche breakdown for amplification of photo charges had already been described in the in 1973 by George E. Smith/Bell Telephone Laboratories.

EMCCDs show a similar sensitivity to intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the "exact" gain that has been applied to a pixel's charge is impossible to know. At high gains (> 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron â or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:

formula_2 if formula_3

where "P" is the probability of getting "n" output electrons given "m" input electrons and a total mean multiplication register gain of "g".

Because of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system â using either thermoelectric cooling or liquid nitrogen â to cool the chip down to temperatures in the range of . This cooling system unfortunately adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.

The low-light capabilities of EMCCDs find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-PÃ©rot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.

In terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.

Due to the high quantum efficiencies of charge-coupled device (CCD) (for a quantum efficiency of 100%, one count equals one photon), linearity of their outputs, ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.

Thermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the dark frame average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD.

The Hubble Space Telescope, in particular, has a highly developed series of steps (âdata reduction pipelineâ) to convert the raw CCD data to useful images.

CCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.

An unusual astronomical application of CCDs, called drift-scanning, uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to a survey of over a quarter of the sky.

In addition to imagers, CCDs are also used in an array of analytical instrumentation including spectrometers and interferometers.

Digital color cameras generally use a Bayer mask over the CCD. Each square of four pixels has one filtered red, one blue, and two green (the human eye is more sensitive to green than either red or blue). The result of this is that luminance information is collected at every pixel, but the color resolution is lower than the luminance resolution.

Better color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (and therefore higher light sensitivity for a given aperture size). This is because in a 3CCD device most of the light entering the aperture is captured by a sensor, while a Bayer mask absorbs a high proportion (about 2/3) of the light falling on each CCD pixel.

For still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).

Sensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8â³ or 2/3â³ called the optical format. This measurement actually originates back in the 1950s and the time of Vidicon tubes.

When a CCD exposure is long enough, eventually the electrons that collect in the "bins" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.

Some anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.
James M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity.



</doc>
<doc id="6806" url="https://en.wikipedia.org/wiki?curid=6806" title="Computer memory">
Computer memory

In computing, memory refers to a device that is used to store information for immediate use in a computer or related computer hardware device. It typically refers to semiconductor memory, specifically metalâoxideâsemiconductor (MOS) memory, where data is stored within MOS memory cells on a silicon integrated circuit chip. The term "memory" is often synonymous with the term "primary storage". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage; a very common way of doing this is through a memory management technique called "virtual memory". An archaic synonym for memory is store.

The term "memory", meaning "primary storage" or "main memory", is often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based MOS transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory areal density than DRAM.

Most semiconductor memory is organized into memory cells or bistable flip-flops, each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multiple bits per cell (called MLC, Multiple Level Cell). The memory cells are grouped into words of fixed word length, for example 1, 2, 4, 8, 16, 32, 64 or 128 bit. Each word can be accessed by a binary address of "N" bit, making it possible to store 2 raised by "N" words in the memory. This implies that processor registers normally are not considered as memory, since they only store one word and do not include an addressing mechanism.

Typical secondary storage devices are hard disk drives and solid-state drives.

In the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of octal-base radio vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits which were held in the vacuum tube accumulators.

The next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.

Two alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams would invent the Williams tube, which would be the first random-access computer memory. The Williams tube would prove more capacious than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube would nevertheless prove to be frustratingly sensitive to environmental disturbances.

Efforts began in the late 1940s to find non-volatile memory. Magnetic-core memory allowed for recall of memory after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialised with the Whirlwind computer in 1953. Magnetic-core memory would become the dominant form of memory until the development of MOS semiconductor memory in the 1960s.

Semiconductor memory began in the early 1960s with bipolar memory, which used bipolar transistors. Bipolar semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. The same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor. The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965. While bipolar memory offered improved performance over magnetic-core memory, it could not compete with the lower price of magnetic-core, which remained dominant up until the late 1960s. Bipolar memory failed to replace magnetic-core memory because bipolar flip-flop circuits were too large and expensive.

The invention of the MOSFET (metalâoxideâsemiconductor field-effect transistor, or MOS transistor), by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959, enabled the practical use of metalâoxideâsemiconductor (MOS) transistors as memory cell storage elements, a function previously served by magnetic cores. MOS memory was developed by John Schmidt at Fairchild Semiconductor in 1964. In addition to higher performance, MOS semiconductor memory was cheaper and consumed less power than magnetic core memory. In 1965, J. Wood and R. Ball of the Royal Radar Establishment proposed digital storage systems that use CMOS (complementary MOS) memory cells, in addition to MOSFET power devices for the power supply, switched cross-coupling, switches and delay line storage. The development of silicon-gate MOS integrated circuit (MOS IC) technology by Federico Faggin at Fairchild in 1968 enabled the production of MOS memory chips. NMOS memory was commercialized by IBM in the early 1970s. MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s.

The two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. SRAM became an alternative to magnetic-core memory, but required six MOS transistors for each bit of data. Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95.

Toshiba introduced bipolar DRAM memory cells for its Toscal BC-1411 electronic calculator in 1965. While it offered improved performance over magnetic-core memory, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. MOS technology is the basis for modern DRAM. In 1966, Dr. Robert H. Dennard at the IBM Thomas J. Watson Research Center was working on MOS memory. While examining the characteristics of MOS technology, he found it was capable of building capacitors, and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. In 1967, Dennard filed a patent under IBM for a single-transistor DRAM memory cell, based on MOS technology. This led to the first commercial DRAM IC chip, the Intel 1103, in October 1970. Synchronous dynamic random-access memory (SDRAM) later debuted with the Samsung KM48SL2000 chip in 1992.

The term "memory" is also often used to refer to non-volatile memory, specifically flash memory. It has origins in read-only memory (ROM). Programmable read-only memory (PROM) was invented by Wen Tsing Chow in 1956, while working for the Arma Division of the American Bosch Arma Corporation. In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable read-only memory (ROM), which led to Dov Frohman of Intel inventing EPROM (erasable PROM) in 1971. EEPROM (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972. Flash memory was invented by Fujio Masuoka at Toshiba in the early 1980s. Masuoka and colleagues presented the invention of NOR flash in 1984, and then NAND flash in 1987. Toshiba commercialized NAND flash memory in 1987.

Developments in technology and economies of scale have made possible so-called Very Large Memory (VLM) computers.

Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). SRAM retains its contents as long as the power is connected and is simpler for interfacing, but uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.

SRAM is not worthwhile for desktop system memory, where DRAM dominates, but is used for their cache memories. SRAM is commonplace in small embedded systems, which might only need tens of kilobytes or less. Forthcoming volatile memory technologies that aim at replacing or competing with SRAM and DRAM include Z-RAM and A-RAM.

Non-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.

Forthcoming non-volatile memory technologies include FERAM, CBRAM, PRAM, STT-RAM, SONOS, RRAM, racetrack memory, NRAM, 3D XPoint, and millipede memory.

A third category of memory is "semi-volatile". The term is used to describe a memory which has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide high performance/durability/etc. associated with volatile memories, while providing some benefits of a true non-volatile memory.

For example, some non-volatile memory types can wear out, where a "worn" cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits. As long as the location is updated within some known retention time, the data stays valid. If the retention time "expires" without an update, then the value is copied to a less-worn circuit with longer retention. Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits.

As a second example, an STT-RAM can be made non-volatile by building large cells, but the cost per bit and write power go up, while the write speed goes down. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold.

The term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types. For example, a volatile and a non-volatile memory may be combined, where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed without copying, the data is lost. Or, a battery-backed volatile memory, and if external power is lost there is some known period where the battery can continue to power the volatile memory, but if power is off for an extended time, the battery runs down and data is lost.

Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.

Nearly everything computer programmers do requires them to consider how to manage memory. Even storing a number in memory requires the programmer to specify how the memory should store it.

Improper management of memory is a common cause of bugs, including the following types:


In early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.

This approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.

Virtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides in what physical location to place the program's code and data.

This offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user's computer will have enough memory. It also allows multiple types of memory to be used. For example, some data can be stored in physical RAM chips while other data is stored on a hard drive (e.g. in a swapfile), functioning as an extension of the cache hierarchy. This drastically increases the amount of memory available to programs. The operating system will place actively used data in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.

Protected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.

Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs; in the modern age, this is generally considered bad programming practice for application programs, but it may be used by system development tools such as debuggers, for example to insert breakpoints or hooks.

Protected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional).

Protected memory systems almost always include virtual memory as well.



</doc>
<doc id="6809" url="https://en.wikipedia.org/wiki?curid=6809" title="CDC">
CDC

CDC may refer to:













</doc>
<doc id="6811" url="https://en.wikipedia.org/wiki?curid=6811" title="Centers for Disease Control and Prevention">
Centers for Disease Control and Prevention

The Centers for Disease Control and Prevention (CDC) is the leading national public health institute of the United States. The CDC is a United States federal agency under the Department of Health and Human Services and is headquartered in Atlanta, Georgia.

Its main goal is to protect public health and safety through the control and prevention of disease, injury, and disability in the US and internationally. The CDC focuses national attention on developing and applying disease control and prevention. It especially focuses its attention on infectious disease, food borne pathogens, environmental health, occupational safety and health, health promotion, injury prevention and educational activities designed to improve the health of United States citizens. In addition, the CDC researches and provides information on non-infectious diseases such as obesity and diabetes and is a founding member of the International Association of National Public Health Institutes.

The Communicable Disease Center was founded July 1, 1946, as the successor to the World War II Malaria Control in War Areas program of the Office of National Defense Malaria Control Activities.

Preceding its founding, organizations with global influence in malaria control were the Malaria Commission of the League of Nations and the Rockefeller Foundation. The Rockefeller Foundation greatly supported malaria control, sought to have the governments take over some of its efforts, and collaborated with the agency.

The new agency was a branch of the U.S. Public Health Service and Atlanta was chosen as the location because malaria was endemic in the Southern United States. The agency changed names (see infobox on top) before adopting the name "Communicable Disease Center" in 1946. Offices were located on the sixth floor of the Volunteer Building on Peachtree Street.

With a budget at the time of about $1million, 59 percent of its personnel were engaged in mosquito abatement and habitat control with the objective of control and eradication of malaria in the United States (see National Malaria Eradication Program).

Among its 369 employees, the main jobs at CDC were originally entomology and engineering. In CDC's initial years, more than six and a half million homes were sprayed, mostly with DDT. In 1946, there were only seven medical officers on duty and an early organization chart was drawn, somewhat fancifully, in the shape of a mosquito. Under Joseph Walter Mountin, the CDC continued to advocate for public health issues and pushed to extend its responsibilities to many other communicable diseases.

In 1947, the CDC made a token payment of $10 to Emory University for of land on Clifton Road in DeKalb County, still the home of CDC headquarters as of 2019. CDC employees collected the money to make the purchase. The benefactor behind the "gift" was Robert W. Woodruff, chairman of the board of The Coca-Cola Company. Woodruff had a long-time interest in malaria control, which had been a problem in areas where he went hunting. The same year, the PHS transferred its San Francisco based plague laboratory into the CDC as the Epidemiology Division, and a new Veterinary Diseases Division was established. An Epidemic Intelligence Service (EIS) was established in 1951, originally due to biological warfare concerns arising from the Korean War; it evolved into two-year postgraduate training program in epidemiology, and a prototype for Field Epidemiology Training Programs (FETP), now found in numerous countries, reflecting CDC's influence in promoting this model internationally.

The mission of CDC expanded beyond its original focus on malaria to include sexually transmitted diseases when the Venereal Disease Division of the U.S. Public Health Service (PHS) was transferred to the CDC in 1957. Shortly thereafter, Tuberculosis Control was transferred (in 1960) to the CDC from PHS, and then in 1963 the Immunization program was established.

It became the "National Communicable Disease Center (NCDC)" effective July 1, 1967. The organization was renamed the "Center for Disease Control (CDC)" on June 24, 1970, and "Centers for Disease Control" effective October 14, 1980. An act of the United States Congress appended the words "and Prevention" to the name effective October 27, 1992. However, Congress directed that the initialism "CDC" be retained because of its name recognition.

Currently, the CDC focus has broadened to include chronic diseases, disabilities, injury control, workplace hazards, environmental health threats, and terrorism preparedness. CDC combats emerging diseases and other health risks, including birth defects, West Nile virus, obesity, avian, swine, and pandemic flu, E. coli, and bioterrorism, to name a few. The organization would also prove to be an important factor in preventing the abuse of penicillin. In May 1994 the CDC admitted having sent several biological warfare agents to the Iraqi government from 1984 through 1989, including Botulinum toxin, West Nile virus, Yersinia pestis and Dengue fever virus.

On April 21, 2005, thenâCDC Director Julie Gerberding formally announced the reorganization of CDC to "confront the challenges of 21st-century health threats". The four Coordinating Centersâestablished under the G. W. Bush Administration and Gerberdingâ"diminished the influence of national centers under [their] umbrella", and were ordered cut under the Obama Administration in 2009.

Today, the CDC's Biosafety Level 4 laboratories are among the few that exist in the world, and serve as one of only two official repositories of smallpox in the world. The second smallpox store resides at the State Research Center of Virology and Biotechnology VECTOR in the Russian Federation. The CDC revealed in 2014 that it had discovered several misplaced smallpox samples and also that lab workers had potentially been infected with anthrax.

The CDC is organized into "Centers, Institutes, and Offices" (CIOs), with each organizational unit implementing the agency's activities in a particular area of expertise while also providing intra-agency support and resource-sharing for cross-cutting issues and specific health threats. Generally, CDC "Offices" are subdivided into Centers, which in turn are composed of Divisions and Branches. However, the Center for Global Health and the National Institute for Occupational Safety and Health are freestanding organizational units and do not belong to a parent Office.

As of August 2019, the CIOs are:


The Office of Public Health Preparedness was created during the 2001 anthrax attacks shortly after the terrorist attacks of September 11, 2001. Its purpose was to coordinate among the government the response to a range of biological terrorism threats.

CDC's budget for fiscal year 2018 is $11.9billion.

In addition to its Atlanta headquarters, CDC's other domestic locations are Anchorage, Fort Collins, Hyattsville, Research Triangle Park, San Juan (Puerto Rico), and Washington, D.C., while NIOSH operates its own facilities in Cincinnati, Morgantown, Pittsburgh, Spokane, Denver, and Anchorage. In addition, CDC operates quarantine facilities in 20 cities in the U.S.

The CDC offers grants that help many organizations each year advance health, safety and awareness at the community level throughout the United States. The CDC awards over 85 percent of its annual budget through these grants.

, CDC staff numbered approximately 15,000 (including 6,000 contractors and 840 Commissioned Corps officers) in 170 occupations. Eighty percent held bachelor's degrees or higher; almost half had advanced degrees (a master's degree or a doctorate such as a PhD, D.O., or M.D.).

Common CDC job titles include engineer, entomologist, epidemiologist, biologist, physician, veterinarian, behavioral scientist, nurse, medical technologist, economist, public health advisor, health communicator, toxicologist, chemist, computer scientist, and statistician.

The CDC also operates a number of notable training and fellowship programs, including those indicated below.

The Epidemic Intelligence Service (EIS) is composed of "boots-on-the-ground disease detectives" who investigate public health problems domestically and globally. When called upon by a governmental body, EIS officers may embark on short-term epidemiological assistance assignments, or "Epi-Aids", to provide technical expertise in containing and investigating disease outbreaks. The EIS program is a model for the international Field Epidemiology Training Program.

The CDC also operates the Public Health Associate Program (PHAP), a two-year paid fellowship for recent college graduates to work in public health agencies all over the United States. PHAP was founded in 2007 and currently has 159 associates in 34 states.

The Director of CDC is a Senior Executive Service position that may be filled either by a career employee, or as a political appointment that does not require Senate confirmation, with the latter method typically being used. The director serves at the pleasure of the President and may be fired at any time. The CDC director concurrently serves as the Administrator of the Agency for Toxic Substances and Disease Registry.

Sixteen directors have served the CDC or its predecessor agencies.



The CDC's programs address more than 400 diseases, health threats, and conditions that are major causes of death, disease, and disability. The CDC's website has information on various infectious (and noninfectious) diseases, including smallpox, measles, and others.

The CDC has launched campaigns targeting the transmission of influenza, including the H1N1 swine flu, and launched websites to educate people in proper hygiene.

Within the division are two programs: the Federal Select Agent Program (FSAP) and the Import Permit Program. The FSAP is run jointly with an office within the U.S. Department of Agriculture, regulating agents that can cause disease in humans, animals, and plants. The Import Permit Program regulates the importation of "infectious biological materials."

The CDC runs a program that protects the public from rare and dangerous substances such as anthrax and the Ebola virus. The program, called the Federal Select Agent Program, calls for inspections of labs in the U.S. that work with dangerous pathogens.

During the 2014 Ebola outbreak in West Africa, the CDC helped coordinate the return of two infected American aid workers for treatment at Emory University Hospital, the home of a special unit to handle highly infectious diseases.

As a response to the 2014 Ebola outbreak, Congress passed a Continuing Appropriations Resolution allocating $30,000,000 towards CDC's efforts to fight the virus.

The CDC also works on non-communicable diseases, including chronic diseases caused by obesity, physical inactivity and tobacco-use.

The CDC implemented their "National Action Plan for Combating Antibiotic Resistant Bacteria" as a measure against the spread of antibiotic resistance in the United States. This initiative has a budget of $161million and includes the development of the Antibiotic Resistance Lab Network.

The CDC works with other organizations around the world to address global health challenges and contain disease threats at their source. It works closely with many international organizations such as the World Health Organization (WHO) as well as ministries of health and other groups on the front lines of outbreaks. The agency maintains staff in more than 60 countries, including some from the U.S. but even more from the countries in which it operates. The agency's global divisions include the Division of Global HIV and TB (DGHT), the Division of Parasitic Diseases and Malaria (DPDM), the Division of Global Health Protection (DGHP), and the Global Immunization Division (GID).

The CDC is integral in working with the WHO to implement the "International Health Regulations" (IHR), a legally binding agreement between 196 countries to prevent, control, and report on the international spread of disease, through initiatives including the Global Disease Detection Program (GDD).

The CDC is also a lead implementer of key U.S. global health initiatives such as the President's Emergency Plan for AIDS Relief (PEPFAR) and the President's Malaria Initiative.

The CDC collects and publishes health information for travelers in a comprehensive book, "CDC Health Information for International Travel", which is commonly known as the "yellow book." The book is available online and in print as a new edition every other year and includes current travel health guidelines, vaccine recommendations, and information on specific travel destinations. The CDC also issues travel health notices on its website, consisting of three levels:

"Watch": Level 1 (practice usual precautions)

"Alert": Level 2 (practice enhanced precautions)

"Warning": Level 3 (avoid nonessential travel)

The CDC Foundation operates independently from CDC as a private, nonprofit 501(c)(3) organization incorporated in the State of Georgia. The creation of the Foundation was authorized by section 399F of the Public Health Service Act to support the mission of CDC in partnership with the private sector, including organizations, foundations, businesses, educational groups, and individuals.

Because of the Lead contamination in Washington, D.C. drinking water the United States House of Representatives conducted an investigation, that uncovered that the CDC had made claims in a report that had indicated there was no risk from high lead levels - although it is the opposite.

For 15 years, the CDC had direct oversight over the Tuskegee syphilis experiment. In the study, which lasted from 1932 to 1972, a group of African American men (nearly 400 of whom had syphilis) were studied to learn more about the disease. Notably, the disease was left untreated in the research subjects and they never gave their informed consent to serve as research subjects. The Tuskegee Study was initiated in 1932 by the Public Health Service. The CDC took over the study in 1957.

The CDC's response to the AIDS crisis in the 1980s has been criticized for promoting some public health policies that harmed HIV+ people and for providing ineffective public education. The agency's response to the 2001 anthrax attacks was also criticized for ineffective communication with other public health agencies and with the public.

On May 16, 2011, the Centers for Disease Control and Prevention's blog what to do to prepare for a zombie invasion. While the article did not claim that such a scenario was possible, it did use the popular culture appeal as a means of urging citizens to prepare for all potential hazards, such as earthquakes, tornadoes, and floods.

According to David Daigle, the Associate Director for Communications, Public Health Preparedness and Response, the idea arose when his team was discussing their upcoming hurricane-information campaign and Daigle mused that "we say pretty much the same things every year, in the same way, and I just wonder how many people are paying attention." A social-media employee mentioned that the subject of zombies had come up a lot on Twitter when she had been tweeting about the Fukushima Daiichi nuclear disaster and radiation. The team realized that a campaign like this would most likely reach a different audience from the one that normally pays attention to hurricane-preparedness warnings and went to work on the zombie campaign, launching it right before hurricane season began. "The whole idea was, if you're prepared for a zombie apocalypse, you're prepared for pretty much anything," said Daigle.

Once the blog article became popular, the CDC announced an open contest for YouTube submissions of the most creative and effective videos covering preparedness for a zombie apocalypse (or apocalypse of any kind), to be judged by the "CDC Zombie Task Force". Submissions were open until October 11, 2011. They also released a zombie-themed graphic novella available on their website. Zombie-themed educational materials for teachers are available on the site.

One area of current partisan dispute related to CDC funding is studying gun violence. The 1996 Dickey Amendment states "none of the funds made available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control". Advocates for gun control oppose the amendment and have tried to overturn it.

In 1992, Mark L. Rosenberg and five CDC colleagues founded the CDC's National Center for Injury Prevention and Control, with an annual budget of c. $260,000 that focused on "identifying the root causes of firearm deaths and the best methods to prevent them". Their first report which was published in the "New England Journal of Medicine" in 1993, entitled "Gun Ownership as a Risk Factor for Homicide in the Home" reported that the "mere presence of a gun in a home increased the risk of a firearm-related death by 2.7 percent, and suicide fivefoldâa "huge" increase." In response, the NRA launched a "campaign to shut down the Injury Center." Doctors for Responsible Gun Ownership and Doctors for Integrity and Policy Research joined the pro-gun effort and by 1995, politicians also supported the pro-gun initiative. In 1996, Jay Dickey (R) Arkansas introduced the Dickey Amendment statement "which stated "none of the funds made available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control" as a rider in the 1996 appropriations bill." In 1997, "Congress redirected all of the money previously earmarked for gun violence research to the study of traumatic brain injury." David Satcher, who was the CDC head from 1993 to 1998 advocated for gun violence research until he left in 1998. In 1999 Rosenberg was fired. Over a dozen "public health insiders, including current and former CDC senior leaders" told "The Trace" interviewers that CDC senior leaders took an overly cautious stance in their interpretation of the Dickey amendment. They could have done much more. Rosenberg told "The Trace" in 2016, "Right now, there is nothing stopping them from addressing this life-and-death national problem."

The American Medical Association, the American Psychological Association and the American Academy of Pediatrics sent a letter to the leaders of the Senate Appropriations Committee in 2013 asking them "to support at least $10million within the Centers for Disease Control and Prevention (CDC) in FY 2014 along with sufficient new funding at the National Institutes of Health to support research into the causes and prevention of gun violence. Furthermore, we urge Members to oppose any efforts to reduce, eliminate, or condition CDC funding related to gun violence prevention research." Congress maintained the ban in subsequent budgets.

In December 2017, "The Washington Post" reported that the Trump administration had issued a list of seven words that were forbidden in official CDC documentation. Yuval Levin, after contacting HHS officials, wrote in the traditionally conservative-leaning "National Review" that the "Post" story was not accurate.




</doc>
<doc id="6813" url="https://en.wikipedia.org/wiki?curid=6813" title="Chandrasekhar limit">
Chandrasekhar limit

The Chandrasekhar limit () is the maximum mass of a stable white dwarf star. The currently accepted value of the Chandrasekhar limit is about ().

White dwarfs resist gravitational collapse primarily through electron degeneracy pressure (compare main sequence stars, which resist collapse through thermal pressure). The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction. Consequently, a white dwarf with a mass greater than the limit is subject to further gravitational collapse, evolving into a different type of stellar remnant, such as a neutron star or black hole. Those with masses under the limit remain stable as white dwarfs.

The limit was named after Subrahmanyan Chandrasekhar, an Indian astrophysicist who improved upon the accuracy of the calculation in 1930, at the age of 20, in India by calculating the limit for a polytrope model of a star in hydrostatic equilibrium, and comparing his limit to the earlier limit found by E. C. Stoner for a uniform density star. Importantly, the existence of a limit, based on the conceptual breakthrough of combining relativity with Fermi degeneracy, was indeed first established in separate papers published by Wilhelm Anderson and E. C. Stoner in 1929. The limit was initially ignored by the community of scientists because such a limit would logically require the existence of black holes, which were considered a scientific impossibility at the time. The fact that the roles of Stoner and Anderson are often forgotten in the astronomy community has been noted.

Electron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons increases on compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure.

In the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form , where is the pressure, is the mass density, and is a constant. Solving the hydrostatic equation leads to a model white dwarf that is a polytrope of index â and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass.

As the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses. The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form . This yields a polytrope of index 3, which has a total mass, say, depending only on .

For a fully relativistic treatment, the equation of state used interpolates between the equations for small and for large . When this is done, the model radius still decreases with mass, but becomes zero at . This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. has been set equal to 2. Radius is measured in standard solar radii or kilometers, and mass in standard solar masses.

Calculated values for the limit vary depending on the nuclear composition of the mass. Chandrasekhar gives the following expression, based on the equation of state for an ideal Fermi gas:
where:
As is the Planck mass, the limit is of the order of

A more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle SchrÃ¶dinger equation.

In 1926, the British physicist Ralph H. Fowler observed that the relationship between the density, energy, and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei that obey FermiâDirac statistics. This Fermi gas model was then used by the British physicist Edmund Clifton Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming they were homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately . In 1930, Stoner derived the internal energyâdensity equation of state for a Fermi gas, and was then able to treat the massâradius relationship in a fully relativistic manner, giving a limiting mass of approximately (for ). Stoner went on to derive the pressureâdensity equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community.

A series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930, where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture. This value was also computed in 1932 by the Soviet physicist Lev Davidovich Landau, who, however, did not apply it to white dwarfs and concluded that quantum laws might be invalid for stars heavier than 1.5 solar mass.

Chandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied:

Eddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law universally applicable, even for large . Although Niels Bohr, Fowler, Wolfgang Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar. Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of "Empire of the Stars", Arthur I. Miller's biography of Chandrasekhar. In Miller's view:

The core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process are exhausted, and the core collapses, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse.

If a main-sequence star is not too massive (less than approximately 8 solar masses), it eventually sheds enough mass to form a white dwarf having mass below the Chandrasekhar limit, which consists of the former core of the star. For more-massive stars, electron degeneracy pressure does not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy on the order of 10Â joules (100Â foes). Most of this energy is carried away by the emitted neutrinos. This process is believed responsible for supernovae of types Ib, Ic, and II.

Type Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbonâoxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass. As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation, which disrupts the star and causes the supernova.

A strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, is approximately â19.3, with a standard deviation of no more than 0.3. A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy.

In April 2003, the Supernova Legacy Survey observed a type Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4Â billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf that grew to twice the mass of the Sun before exploding. They believe that the star, dubbed the "Champagne Supernova" by University of Oklahoma astronomer David R. Branch, may have been spinning so fast that a centrifugal tendency allowed it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type Ia supernovae as standard candles.

Since the observation of the Champagne Supernova in 2003, more very bright type Ia supernovae have been observed that are thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4â2.8 solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely.

After a supernova explosion, a neutron star may be left behind. These objects are even more compact than white dwarfs and are also supported, in part, by degeneracy pressure. A neutron star, however, is so massive and compressed that electrons and protons have combined to form neutrons, and the star is thus supported by neutron degeneracy pressure (as well as short-range repulsive neutron-neutron interactions mediated by the strong force) instead of electron degeneracy pressure. The limiting value for neutron star mass, analogous to the Chandrasekhar limit, is known as the TolmanâOppenheimerâVolkoff limit.




</doc>
<doc id="6814" url="https://en.wikipedia.org/wiki?curid=6814" title="Congregationalist polity">
Congregationalist polity

Congregationalist polity, or congregational polity, often known as congregationalism, is a system of ecclesiastical polity in which every local church congregation is independent, ecclesiastically sovereign, or "autonomous". Its first articulation in writing is the Cambridge Platform of 1648 in New England. Among those major Protestant Christian traditions that employ congregationalism are those Congregational churches known by the "Congregationalist" name that descended from the Independent Reformed wing of the Anglo-American Puritan movement of the 17th century, Quakerism, the Baptist churches, as well as the Congregational Methodist Church. More recent generations have witnessed also a growing number of nondenominational churches, which are most often congregationalist in their governance.

Congregationalism is distinguished from episcopal polity which is governance by a hierarchy of bishops, and is distinct from presbyterian polity in which higher assemblies of congregational representatives can exercise considerable authority over individual congregations.

Congregationalism is not limited only to organization of Christian church congregations. The principles of congregationalism have been inherited by the Unitarian Universalist Association and the Canadian Unitarian Council. Most Jewish synagogues, many Sikh Gurdwaras and most Islamic mosques in the US operate under congregational government, with no hierarchies.

The term "congregationalist polity" describes a form of church governance that is based on the local congregation. Each local congregation is independent and self-supporting, governed by its own members. Some band into loose voluntary associations with other congregations that share similar beliefs (e.g., the Willow Creek Association and the American Unitarian Association). Others join "conventions", such as the Southern Baptist Convention, the National Baptist Convention or the American Baptist Churches USA (formerly the Northern Baptist Convention). In Quaker Congregationalism, monthly meetings, which are the most basic unit of administration, may be organized into larger Quarterly meetings or Yearly Meetings. Monthly, quarterly, or yearly meetings may also be associated with large "umbrella" associations such as Friends General Conference or Friends United Meeting. These conventions generally provide stronger ties between congregations, including some doctrinal direction and pooling of financial resources. Congregations that belong to associations and conventions are still independently governed. Most non-denominational churches are organized along congregationalist lines. Many do not see these voluntary associations as "denominations", because they "believe that there is no church other than the local church, and denominations are in variance to Scripture."

The earmarks of Congregationalism can be traced back to the Pilgrim societies of the United States in the early 17th century. Congregationalism expressed the viewpoint that (1) every local church is a full realization in miniature of the entire Church of Jesus Christ; and (2) the Church, while on earth, besides the local church, can only be invisible and ideal. While other theories may insist on the truth of the former, the latter precept of congregationalism gives the entire theory a unique character among plans of church government. There is no other reference than the local congregation for the "visible church" in Congregationalism. And yet, the connection of all Christians is also asserted, albeit in a way that defenders of this view usually decline, often intentionally, to elaborate more clearly or consistently. This first, foundational principle by which congregationalism is guided results in confining it to operate with the consent of each gathering of believers.

Although "congregational rule" may seem to suggest that pure democracy reigns in congregational churches, this is seldom the case. It is granted, with few exceptions (namely in some Anabaptist churches), that God has given the government of the Church into the hands of an ordained ministry. What makes congregationalism unique is its system of checks and balances, which constrains the authority of the clergy, the lay officers, and the members.

Most importantly, the boundaries of the powers of the ministers and church officers are set by clear and constant reminders of the freedoms guaranteed by the Gospel to the laity, collectively and individually. With that freedom comes the responsibility upon each member to govern himself or herself under Christ. This requires lay people to exercise great charity and patience in debating issues with one another and to seek the glory and service of God as the foremost consideration in all of their decisions.

The authority of all of the people, including the officers, is limited in the local congregation by a definition of union, or a covenant, by which the terms of their cooperation together are spelled out and agreed to. This might be something as minimal as a charter specifying a handful of doctrines and behavioral expectations, or even a statement only guaranteeing specific freedoms. Or, it may be a constitution describing a comprehensive doctrinal system and specifying terms under which the local church is connected to other local churches, to which participating congregations give their assent. In congregationalism, rather uniquely, the church is understood to be a truly voluntary association.

Finally, the congregational theory strictly forbids ministers from ruling their local churches by themselves. Not only does the minister serve by the approval of the congregation, but committees further constrain the pastor from exercising power without consent by either the particular committee, or the entire congregation. It is a contradiction of the congregational principle if a minister makes decisions concerning the congregation without the vote of these other officers.

The other officers may be called "deacons", "elder" or "session" (borrowing Presbyterian terminology), or even "vestry" (borrowing the Anglican term)Â â it is not their label that is important to the theory, but rather their lay status and their equal vote, together with the pastor, in deciding the issues of the church. While other forms of church government are more likely to define "tyranny" as "the imposition of unjust rule", a congregationally governed church would more likely define "tyranny" as "transgression of liberty" or equivalently, "rule by one man". To a congregationalist, no abuse of authority is worse than the concentration of all decisive power in the hands of one ruling body, or one person.

Following this sentiment, congregationalism has evolved over time to include even more participation of the congregation, more kinds of lay committees to whom various tasks are apportioned, and more decisions subject to the vote of the entire membership.

One of the most notable characteristics of New England (or British)-heritage Congregationalism has been its consistent leadership role in the formation of "unions" with other churches. Such sentiments especially grew strong in the late 19th and early 20th centuries, when ecumenism evolved out of a liberal, non-sectarian perspective on relations to other Christian groups that accompanied the relaxation of Calvinist stringencies held by earlier generations. The congregationalist theory of independence within a union has been a cornerstone of most ecumenical movements since the 18th century.

Most Baptists hold that no denominational or ecclesiastical organization has inherent authority over an individual Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control. Exceptions to this local form of local governance include the Episcopal Baptists that have an episcopal system.

Independent Baptist churches have no formal organizational structure above the level of the local congregation. More generally among Baptists, a variety of parachurch agencies and evangelical educational institutions may be supported generously or not at all, depending entirely upon the local congregation's customs and predilections. Usually doctrinal conformity is held as a first consideration when a church makes a decision to grant or decline financial contributions to such agencies, which are legally external and separate from the congregations they serve. These practices also find currency among non-denominational fundamentalist or charismatic fellowships, many of which derive from Baptist origins, culturally if not theologically.

Most Southern Baptist and National Baptist congregations, by contrast, generally relate more closely to external groups such as mission agencies and educational institutions than do those of independent persuasion. However, they adhere to a very similar ecclesiology, refusing to permit outside control or oversight of the affairs of the local church.

Ecclesiastical government is congregational rather than denominational. Churches of Christ purposefully have no central headquarters, councils, or other organizational structure above the local church level. Rather, the independent congregations are a network with each congregation participating at its own discretion in various means of service and fellowship with other congregations. Churches of Christ are linked by their shared commitment to restoration principles.

Congregations are generally overseen by a plurality of elders (also known in some congregations as shepherds, bishops, or pastors) who are sometimes assisted in the administration of various works by deacons. Elders are generally seen as responsible for the spiritual welfare of the congregation, while deacons are seen as responsible for the non-spiritual needs of the church. Deacons serve under the supervision of the elders, and are often assigned to direct specific ministries. Successful service as a deacon is often seen as preparation for the eldership. Elders and deacons are chosen by the congregation based on the qualifications found in Timothy 3 and Titus 1. Congregations look for elders who have a mature enough understanding of scripture to enable them to supervise the minister and to teach, as well as to perform governance functions. In lieu of willing men who meet these qualifications, congregations are sometimes overseen by an unelected committee of the congregation's men.

While the early Restoration Movement had a tradition of itinerant preachers rather than "located Preachers", during the 20th century a long-term, formally trained congregational minister became the norm among Churches of Christ. Ministers are understood to serve under the oversight of the elders. While the presence of a long-term professional minister has sometimes created "significant "de facto" ministerial authority" and led to conflict between the minister and the elders, the eldership has remained the "ultimate locus of authority in the congregation". There is a small group within the Churches of Christ which oppose a single preacher and, instead, rotate preaching duties among qualified elders (this group tends to overlap with groups which oppose Sunday School and also have only one cup to serve the Lord's Supper).

Churches of Christ hold to the priesthood of all believers. No special titles are used for preachers or ministers that would identify them as clergy. Churches of Christ emphasize that there is no distinction between "clergy" and "laity" and that every member has a gift and a role to play in accomplishing the work of the church.

Methodists who disagreed with the episcopal polity of the Methodist Episcopal Church, South (MECS) left their mother church to form the Congregational Methodist Church, which retains Wesleyan-Arminian theology but adopts congregationalist polity as a distinctive.




</doc>
<doc id="6816" url="https://en.wikipedia.org/wiki?curid=6816" title="Cavalry">
Cavalry

Cavalry (from the French "cavalerie", cf. "cheval" 'horse') or horsemen are soldiers or warriors who fight mounted on horseback. Cavalry were historically the most mobile of the combat arms. An individual soldier in the cavalry is known by a number of designations such as cavalryman, horseman, dragoon, or trooper. The designation of cavalry was not usually given to any military forces that used other animals, such as camels, mules or elephants. Infantry who moved on horseback, but dismounted to fight on foot, were known in the 17th and early 18th centuries as dragoons, a class of mounted infantry which later evolved into cavalry proper while retaining their historic title.

Cavalry had the advantage of improved mobility, and a man fighting from horseback also had the advantages of greater height, speed, and inertial mass over an opponent on foot. Another element of horse mounted warfare is the psychological impact a mounted soldier can inflict on an opponent.

The speed, mobility, and shock value of cavalry was greatly appreciated and exploited in armed forces in the Ancient and Middle Ages; some forces were mostly cavalry, particularly in nomadic societies of Asia, notably the Huns of Attila and the later Mongol armies. In Europe cavalry became increasingly armoured (heavy), and eventually evolving into the mounted knights of the medieval period. During the 17th century cavalry in Europe lost most of its armor, ineffective against the muskets and cannon which were coming into use, and by the mid-19th century armor had mainly fallen into disuse, although some regiments retained a small thickened cuirass that offered protection against lances and sabres and some protection against shot.

In the period between the World Wars, many cavalry units were converted into motorized infantry and mechanized infantry units, or reformed as tank troops. However, some cavalry still served during World War II, notably in the Red Army, the Mongolian People's Army, the Royal Italian Army, the Romanian Army, the Polish Land Forces, and light reconnaissance units within the Waffen SS. Most cavalry units that are horse-mounted in modern armies serve in purely ceremonial roles, or as mounted infantry in difficult terrain such as mountains or heavily forested areas. Modern usage of the term generally refers to units performing the role of reconnaissance, surveillance, and target acquisition (RSTA).

In many modern armies, the term "cavalry" is still often used to refer to units that are a combat arm of the armed forces which in the past filled the traditional horse-borne land combat light cavalry roles. These include scouting, skirmishing with enemy reconnaissance elements to deny them knowledge of the disposition of the main body of troops, forward security, offensive reconnaissance by combat, defensive screening of friendly forces during retrograde movement, retreat, restoration of command and control, deception, battle handover and passage of lines, relief in place, linkup, breakout operations, and raiding. The shock role, traditionally filled by heavy cavalry, is generally filled by units with the "armored" designation.

Before the Iron Age, the role of cavalry on the battlefield was largely performed by light chariots. The chariot originated with the Sintashta-Petrovka culture in Central Asia and spread by nomadic or semi-nomadic Indo-Iranians. The chariot was quickly adopted by settled peoples both as a military technology and an object of ceremonial status, especially by the pharaohs of the New Kingdom of Egypt from 1550 BC as well as the Assyrian army and Babylonian royalty.

The power of mobility given by mounted units was recognized early on, but was offset by the difficulty of raising large forces and by the inability of horses (then mostly small) to carry heavy armor. Nonetheless, there are indications that, from the 15th century BC onwards, horseback riding was practiced amongst the military elites of the great states of the ancient Near East, most notably those in Egypt, Assyria, the Hittite Empire, and Mycenaean Greece .

Cavalry techniques, and the rise of true cavalry, were an innovation of equestrian nomads of the Central Asian and Iranian steppe and pastoralist tribes such as the Iranic Parthians and Sarmatians.
The photograph above left shows Assyrian cavalry from reliefs of 865â860 BC. At this time, the men had no spurs, saddles, saddle cloths, or stirrups. Fighting from the back of a horse was much more difficult than mere riding. The cavalry acted in pairs; the reins of the mounted archer were controlled by his neighbour's hand. Even at this early time, cavalry used swords, shields, and bows. The sculpture implies two types of cavalry, but this might be a simplification by the artist. Later images of Assyrian cavalry show saddle cloths as primitive saddles, allowing each archer to control his own horse.

As early as 490 BC a breed of large horses was bred in the Nisaean plain in Media to carry men with increasing amounts of armour (Herodotus 7,40 & 9,20), but large horses were still very exceptional at this time. By the fourth century BC the Chinese during the Warring States period (403â221 BC) began to use cavalry against rival states, and by 331 BC when Alexander the Great defeated the Persians the use of chariots in battle was obsolete in most nations; despite a few ineffective attempts to revive scythed chariots. The last recorded use of chariots as a shock force in continental Europe was during the Battle of Telamon in 225 BC.
However, chariots remained in use for ceremonial purposes such as carrying the victorious general in a Roman triumph, or for racing.

Outside of mainland Europe, the southern Britons met Julius Caesar with chariots in 55 and 54 BC, but by the time of the Roman conquest of Britain a century later chariots were obsolete, even in Britannia. The last mention of chariot use in Britain was by the Caledonians at the Mons Graupius, in 84 AD.

During the classical Greek period cavalry were usually limited to those citizens who could afford expensive war-horses. Three types of cavalry became common: light cavalry, whose riders, armed with javelins, could harass and skirmish; heavy cavalry, whose troopers, using lances, had the ability to close in on their opponents; and finally those whose equipment allowed them to fight either on horseback or foot. The role of horsemen did however remain secondary to that of the hoplites or heavy infantry who comprised the main strength of the citizen levies of the various city states.

Cavalry played a relatively minor role in ancient Greek city-states, with conflicts decided by massed armored infantry. However, Thebes produced Pelopidas, her first great cavalry commander, whose tactics and skills were absorbed by Phillip II of Macedon when Phillip was a guest-hostage in Thebes. Thessaly was widely known for producing competent cavalrymen, and later experiences in wars both with and against the Persians taught the Greeks the value of cavalry in skirmishing and pursuit. The Athenian author and soldier Xenophon in particular advocated the creation of a small but well-trained cavalry force; to that end, he wrote several manuals on horsemanship and cavalry operations.

The Macedonian Kingdom in the north, on the other hand, developed a strong cavalry force that culminated in the "hetairoi" (Companion cavalry) of Philip II of Macedon and Alexander the Great. In addition to these heavy cavalry, the Macedonian army also employed lighter horsemen called prodromoi for scouting and screening, as well as the Macedonian pike phalanx and various kinds of light infantry. There were also the "Ippiko" (or "Horserider"), Greek "heavy" cavalry, armed with kontos (or cavalry lance), and sword. These wore leather armour or mail plus a helmet. They were medium rather than heavy cavalry, meaning that they were better suited to be scouts, skirmishers, and pursuers rather than front line fighters. The effectiveness of this combination of cavalry and infantry helped to break enemy lines and was most dramatically demonstrated in Alexander's conquests of Persia, Bactria, and northwestern India.

The cavalry in the early Roman Republic remained the preserve of the wealthy landed class known as the "equites"âmen who could afford the expense of maintaining a horse in addition to arms and armor heavier than those of the common legions. Horses were provided by the Republic and could be withdrawn if neglected or misused, together with the status of being a cavalryman.

As the class grew to be more of a social elite instead of a functional property-based military grouping, the Romans began to employ Italian socii for filling the ranks of their cavalry. The weakness of Roman cavalry was demonstrated by Hannibal Barca during the Second Punic War where he used his superior mounted forces to win several battles. The most notable of these was the Battle of Cannae, where he inflicted a catastrophic defeat on the Romans. At about the same time the Romans began to recruit foreign auxiliary cavalry from among Gauls, Iberians, and Numidians, the last being highly valued as mounted skirmishers and scouts (see Numidian cavalry). Julius Caesar had a high opinion of his escort of Germanic mixed cavalry, giving rise to the "Cohortes Equitatae". Early emperors maintained an ala of Batavian cavalry as their personal bodyguards until the unit was dismissed by Galba after the Batavian Rebellion.

For the most part, Roman cavalry during the early Republic functioned as an adjunct to the legionary infantry and formed only one-fifth of the standing force comprising a consular army. Except in times of major mobilisation about 1,800 horsemen were maintained, with three hundred attached to each legion. 
The relatively low ratio of horsemen to infantry does not mean that the utility of cavalry should be underestimated, as its strategic role in scouting, skirmishing, and outpost duties was crucial to the Romans' capability to conduct operations over long distances in hostile or unfamiliar territory. On some occasions Roman cavalry also proved its ability to strike a decisive tactical blow against a weakened or unprepared enemy, such as the final charge at the Battle of Aquilonia.

After defeats such as the Battle of Carrhae, the Romans learned the importance of large cavalry formations from the Parthians. 
At the same time heavy spears and shields modelled on those favoured by the horsemen of the Greek city-states were adopted to replace the lighter weaponry of early Rome. 
These improvements in tactics and equipment reflected those of a thousand years earlier when the first Iranians to reach the Iranian Plateau forced the Assyrians to undertake similar reform. Nonetheless, the Romans would continue to rely mainly on their heavy infantry supported by auxiliary cavalry.

In the army of the late Roman Empire, cavalry played an increasingly important role. The Spatha, the classical sword throughout most of the 1st millennium was adopted as the standard model for the Empire's cavalry forces.

The most widespread employment of heavy cavalry at this time was found in the forces of the Iranian empires, the Parthians and their Persian Sasanian successors. Both, but especially the former, were famed for the cataphract (fully armored cavalry armed with lances) even though the majority of their forces consisted of lighter horse archers. The West first encountered this eastern heavy cavalry during the Hellenistic period with further intensive contacts during the eight centuries of the RomanâPersian Wars. At first the Parthians' mobility greatly confounded the Romans, whose armoured close-order infantry proved unable to match the speed of the Parthians. However, later the Romans would successfully adapt such heavy armor and cavalry tactics by creating their own units of cataphracts and "clibanarii".

The decline of the Roman infrastructure made it more difficult to field large infantry forces, and during the 4th and 5th centuries cavalry began to take a more dominant role on the European battlefield, also in part made possible by the appearance of new, larger breeds of horses. The replacement of the Roman saddle by variants on the Scythian model, with pommel and cantle, was also a significant factor as was the adoption of stirrups and the concomitant increase in stability of the rider's seat. Armored cataphracts began to be deployed in eastern Europe and the Near East, following the precedents established by Persian forces, as the main striking force of the armies in contrast to the earlier roles of cavalry as scouts, raiders, and outflankers.

The late-Roman cavalry tradition of organized units in a standing army differed fundamentally from the nobility of the Germanic invadersâindividual warriors who could afford to provide their own horses and equipment. While there was no direct linkage with these predecessors the early medieval knight also developed as a member of a social and martial elite, able to meet the considerable expenses required by his role from grants of land and other incomes.

Xiongnu, Tujue, Avars, Kipchaks, Mongols, Don Cossacks and the various Turkic peoples are also examples of the horse-mounted groups that managed to gain substantial successes in military conflicts with settled agrarian and urban societies, due to their strategic and tactical mobility. As European states began to assume the character of bureaucratic nation-states supporting professional standing armies, recruitment of these mounted warriors was undertaken in order to fill the strategic roles of scouts and raiders.
The best known instance of the continued employment of mounted tribal auxiliaries were the Cossack cavalry regiments of the Russian Empire. In eastern Europe, Russia, and out onto the steppes, cavalry remained important much longer and dominated the scene of warfare until the early 17th century and even beyond, as the strategic mobility of cavalry was crucial for the semi-nomadic pastoralist lives that many steppe cultures led. Tibetans also had a tradition of cavalry warfare, in several military engagements with the Chinese Tang dynasty (618â907 AD).

Further east, the military history of China, specifically northern China, held a long tradition of intense military exchange between Han Chinese infantry forces of the settled dynastic empires and the mounted nomads or "barbarians" of the north. The naval history of China was centered more to the south, where mountains, rivers, and large lakes necessitated the employment of a large and well-kept navy.

In 307 BC, King Wuling of Zhao, the ancient Chinese ruler of the former State of Jin territory, ordered his military commanders and troops to adopt the trousers of the nomads as well as practice the nomads' form of mounted archery to hone their new cavalry skills.
The adoption of massed cavalry in China also broke the tradition of the chariot-riding Chinese aristocracy in battle, which had been in use since the ancient Shang Dynasty (c 1600â1050 BC). By this time large Chinese infantry-based armies of 100,000 to 200,000 troops were now buttressed with several hundred thousand mounted cavalry in support or as an effective striking force. The handheld pistol-and-trigger crossbow was invented in China in the fourth century BC; it was written by the Song dynasty scholars Zeng Gongliang, Ding Du, and Yang Weide in their book "Wujing Zongyao" (1044 AD) that massed missile fire by crossbowmen was the most effective defense against enemy cavalry charges.
On many occasions the Chinese studied nomadic cavalry tactics and applied the lessons in creating their own potent cavalry forces, while in others they simply recruited the tribal horsemen wholesale into their armies; and in yet other cases nomadic empires proved eager to enlist Chinese infantry and engineering, as in the case of the Mongol Empire and its sinicized part, the Yuan Dynasty (1279â1368). The Chinese recognized early on during the Han Dynasty (202 BC â 220 AD) that they were at a disadvantage in lacking the number of horses the northern nomadic peoples mustered in their armies. Emperor Wu of Han (r 141â87 BC) went to war with the Dayuan for this reason, since the Dayuan were hoarding a massive amount of tall, strong, Central Asian bred horses in the HellenizedâGreek region of Fergana (established slightly earlier by Alexander the Great). Although experiencing some defeats early on in the campaign, Emperor Wu's war from 104 BC to 102 BC succeeded in gathering the prized tribute of horses from Fergana.

Cavalry tactics in China were enhanced by the invention of the saddle-attached stirrup by at least the 4th century, as the oldest reliable depiction of a rider with paired stirrups was found in a Jin Dynasty tomb of the year 322 AD. The Chinese invention of the horse collar by the 5th century was also a great improvement from the breast harness, allowing the horse to haul greater weight without heavy burden on its skeletal structure.

The horse warfare of Korea was first started during the ancient Korean kingdom Gojoseon. Since at least the 3rd century BC, there was influence of northern nomadic peoples and Yemaek peoples on Korean warfare. By roughly the first century BC, the ancient kingdom of Buyeo also had mounted warriors. The cavalry of Goguryeo, one of the Three Kingdoms of Korea, were called "Gaemamusa" (ê°ë§ë¬´ì¬, é§é¦¬æ­¦å£«), and were renowned as a fearsome heavy cavalry force. King Gwanggaeto the Great often led expeditions into the Baekje, Gaya confederacy, Buyeo, Later Yan and against Japanese invaders with his cavalry.
In the 12th century, Jurchen tribes began to violate the GoryeoâJurchen borders, and eventually invaded Goryeo Korea. After experiencing the invasion by the Jurchen, Korean general Yun Gwan realized that Goryeo lacked efficient cavalry units. He reorganized the Goryeo military into a professional army that would contain decent and well-trained cavalry units. In 1107, the Jurchen were ultimately defeated, and surrendered to Yun Gwan. To mark the victory, General Yun built nine fortresses to the northeast of the GoryeoâJurchen borders (ëë¶ 9ì±, æ±å ä¹å).

The ancient Japanese of the Kofun period also adopted cavalry and equine culture by the 5th century AD. The emergence of the samurai aristocracy led to the development of armoured horse archers, themselves to develop into charging lancer cavalry as gunpowder weapons rendered bows obsolete.

An example is Yabusame (æµéé¦¬?), a type of mounted archery in traditional Japanese archery. An archer on a running horse shoots three special "turnip-headed" arrows successively at three wooden targets.

This style of archery has its origins at the beginning of the Kamakura period. Minamoto no Yoritomo became alarmed at the lack of archery skills his samurai had. He organized yabusame as a form of practice.
Currently, the best places to see yabusame performed are at the Tsurugaoka Hachiman-gÅ« in Kamakura and Shimogamo Shrine in Kyoto (during Aoi Matsuri in early May). It is also performed in Samukawa and on the beach at Zushi, as well as other locations.

Kasagake or Kasakake (ç¬ æ¸, ãããã lit. "hat shooting") is a type of Japanese mounted archery. In contrast to yabusame, the types of targets are various and the archer shoots without stopping the horse. While yabusame has been played as a part of formal ceremonies, kasagake has developed as a game or practice of martial arts, focusing on technical elements of horse archery.

In the Indian subcontinent, cavalry played a major role from the Gupta Dynasty (320â600) period onwards. India has also the oldest evidence for the introduction of toe-stirrups.

Indian literature contains numerous references to the cavalry forces of the Central Asian horse nomads like the Sakas, Kambojas, Yavanas, Pahlavas and Paradas. Numerous Puranic texts refer to a conflict in ancient India (16th century BC) in which the cavalry forces of five nations, called five hordes ("paÃ±ca.ganan") or Ká¹£atriya hordes ("Ká¹£atriya ganah"), attacked and captured the throne of Ayudhya by dethroning its Vedic King Bahu

The Mahabharata, Ramayana, numerous Puranas and some foreign sources numerously attest that Kamboja cavalry was frequently requisitioned in ancient wars. V. R. Ramachandra Dikshitar writes: "Both the Puranas and the epics agree that the horses of the Sindhu and Kamboja regions were of the finest breed, and that the services of the Kambojas as cavalry troopers were requisitioned in ancient wars". J.A.O.S. writes: "Most famous horses are said to come either from Sindhu or Kamboja; of the latter (i.e. the Kamboja), the Indian epic Mahabharata speaks among the finest horsemen".

Mahabharata (c 950 BC) speaks of the esteemed cavalry of the Kambojas, Sakas, Yavanas and Tusharas, all of whom had participated in the Kurukshetra war under the supreme command of Kamboja ruler Sudakshin Kamboj.

Mahabharata and Vishnudharmottara Purana especially styles the Kambojas, Yavansa, Gandharas etc. as "Ashva.yuddha.kushalah" (expert cavalrymen). In the Mahabharata war, the Kamboja cavalry along with that of the Sakas, Yavanas is reported to have been enlisted by the Kuru king Duryodhana of Hastinapura.

Herodotus (c 484 â c 425 BC) attests that the Gandarian mercenaries (i.e. "Gandharans/Kambojans" of Gandari Strapy of Achaemenids) from the 20th strapy of the Achaemenids were recruited in the army of emperor Xerxes I (486â465 BC), which he led against the Hellas. Similarly, the "men of the Mountain Land " from north of Kabol-River equivalent to medieval Kohistan (Pakistan), figure in the army of Darius III against Alexander at Arbela with a cavalry and 15 elephants. This obviously refers to Kamboja cavalry south of Hindukush.

The Kambojas were famous for their horses, as well as cavalrymen ("asva-yuddha-Kushalah"). On account of their supreme position in horse (Ashva) culture, they were also popularly known as Ashvakas, i.e. the "horsemen" and their land was known as "Home of Horses". They are the Assakenoi and Aspasioi of the Classical writings, and the Ashvakayanas and Ashvayanas in PÄá¹ini's Ashtadhyayi. The Assakenoi had faced Alexander with 30,000 infantry, 20,000 cavalry and 30 war elephants. Scholars have identified the Assakenoi and Aspasioi clans of Kunar and Swat valleys as a section of the Kambojas. These hardy tribes had offered stubborn resistance to Alexander (c 326 BC) during latter's campaign of the Kabul, Kunar and Swat valleys and had even extracted the praise of the Alexander's historians. These highlanders, designated as ""parvatiya Ayudhajivinah"" in PÄá¹ini's Astadhyayi, were rebellious, fiercely independent and freedom-loving cavalrymen who never easily yielded to any overlord.

The Sanskrit drama "Mudra-rakashas" by "Visakha Dutta" and the Jaina work "Parishishtaparvan" refer to Chandragupta's (c 320 BC â c 298 BC) alliance with Himalayan king "Parvataka". The Himalayan alliance gave Chandragupta a formidable composite army made up of the cavalry forces of the Shakas, Yavanas, Kambojas, Kiratas, Parasikas and Bahlikas as attested by Mudra-Rakashas (Mudra-Rakshasa 2). These hordes had helped Chandragupta Maurya defeat the ruler of Magadha and placed Chandragupta on the throne, thus laying the foundations of Mauryan Dynasty in Northern India.

The cavalry of Hunas and the Kambojas is also attested in the Raghu Vamsa epic poem of Sanskrit poet Kalidasa. Raghu of Kalidasa is believed to be Chandragupta II ("Vikaramaditya") (375â413/15 AD), of the well-known Gupta Dynasty.

As late as mediaeval era, the Kamboja cavalry had also formed part of the Gurjara-Pratihara armed forces from the eighth to the 10th centuries AD. They had come to Bengal with the Pratiharas when the latter conquered part of the province.

Ancient Kambojas were constituted into military "Sanghas" and Srenis (Corporations) to manage their political and military affairs, as Arthashastra of Kautiliya as well as the Mahabharata amply attest for us. They are attested to be living as "Ayuddha-jivi" or "Shastr-opajivis" (Nation-in-arms), which also means that the Kamboja cavalry offered its military services to other nations as well. There are numerous references to Kambojas having been requisitioned as cavalry troopers in ancient wars by outside nations.

As the quality and availability of heavy infantry declined in Europe with the fall of the Roman Empire, heavy cavalry became more effective. Infantry that lack the cohesion and discipline of tight formations are more susceptible to being broken and scattered by shock combatâthe main role of heavy cavalry, which rose to become the dominant force on the European battlefield.

As heavy cavalry increased in importance, it became the main focus of military development. The arms and armour for heavy cavalry increased, the high-backed saddle developed, and stirrups and spurs were added, increasing the advantage of heavy cavalry even more.

This shift in military importance was reflected in society as well; knights took centre stage both on and off the battlefield. These are considered the "ultimate" in heavy cavalry: well-equipped with the best weapons, state-of-the-art armour from head to foot, leading with the lance in battle in a full-gallop, close-formation "knightly charge" that might prove irresistible, winning the battle almost as soon as it begun.

But knights remained the minority of total available combat forces; the expense of arms, armour, and horses was only affordable to a select few. While mounted men-at-arms focused on a narrow combat role of shock combat, medieval armies relied on a large variety of foot troops to fulfill all the rest (skirmishing, flank guards, scouting, holding ground, etc.). Medieval chroniclers tended to pay undue attention to the knights at the expense of the common soldiers, which led early students of military history to suppose that heavy cavalry was the only force that mattered on medieval European battlefields. But well-trained and disciplined infantry could defeat knights.

Massed English longbowmen triumphed over French cavalry at CrÃ©cy, Poitiers and Agincourt, while at Gisors (1188), Bannockburn (1314), and Laupen (1339), foot-soldiers proved they could resist cavalry charges as long as they held their formation. Once the Swiss developed their pike squares for offensive as well as defensive use, infantry started to become the principal arm. This aggressive new doctrine gave the Swiss victory over a range of adversaries, and their enemies found that the only reliable way to defeat them was by the use of an even more comprehensive combined arms doctrine, as evidenced in the Battle of Marignano. The introduction of missile weapons that required less skill than the longbow, such as the crossbow and hand cannon, also helped remove the focus somewhat from cavalry elites to masses of cheap infantry equipped with easy-to-learn weapons. These missile weapons were very successfully used in the Hussite Wars, in combination with Wagenburg tactics.

This gradual rise in the dominance of infantry led to the adoption of dismounted tactics. From the earliest times knights and mounted men-at-arms had frequently dismounted to handle enemies they could not overcome on horseback, such as in the Battle of the Dyle (891) and the Battle of Bremule (1119), but after the 1350s this trend became more marked with the dismounted men-at-arms fighting as super-heavy infantry with two-handed swords and poleaxes. In any case, warfare in the Middle Ages tended to be dominated by raids and sieges rather than pitched battles, and mounted men-at-arms rarely had any choice other than dismounting when faced with the prospect of assaulting a fortified position.

The Islamic Prophet Muhammad made use of cavalry in many of his military campaigns including the Expedition of Dhu Qarad, and the expedition of Zaid ibn Haritha in al-Is which took place in September, 627 AD, fifth month of 6 AH of the Islamic calendar.

Early organized Arab mounted forces under the Rashidun caliphate comprised a light cavalry armed with lance and sword. Its main role was to attack the enemy flanks and rear. These relatively lightly armored horsemen formed the most effective element of the Muslim armies during the later stages of the Islamic conquest of the Levant. The best use of this lightly armed fast moving cavalry was revealed at the Battle of Yarmouk (636 AD) in which Khalid ibn Walid, knowing the skills of his horsemen, used them to turn the tables at every critical instance of the battle with their ability to engage, disengage, then turn back and attack again from the flank or rear. A strong cavalry regiment was formed by Khalid ibn Walid which included the veterans of the campaign of Iraq and Syria. Early Muslim historians have given it the name "Mutaharrik tulai'a"( ÙØªØ­Ø±Ù Ø·ÙÙØ¹Ø© ), or the Mobile guard. This was used as an advance guard and a strong striking force to route the opposing armies with its greater mobility that give it an upper hand when maneuvering against any Byzantine army. With this mobile striking force, the conquest of Syria was made easy.

The Battle of Talas in 751 AD was a conflict between the Arab Abbasid Caliphate and the Chinese Tang dynasty over the control of Central Asia. Chinese infantry were routed by Arab cavalry near the bank of the River Talas.

Later Mamluks were trained as cavalry soldiers. Mamluks were to follow the dictates of al-furusiyya, a code of conduct that included values like courage and generosity but also doctrine of cavalry tactics, horsemanship, archery and treatment of wounds.

The Islamic Berber states of North Africa employed elite horse mounted cavalry armed with spears and following the model of the original Arab occupiers of the region. Horse-harness and weapons were manufactured locally and the six-monthly stipends for horsemen were double those of their infantry counterparts. During the 8th century Islamic conquest of Iberia large numbers of horses and riders were shipped from North Africa, to specialise in raiding and the provision of support for the massed Berber footmen of the main armies.

Qizilbash, were a class of Safavid militant warriors in Iran during the 15th to 18th centuries, who often fought as elite cavalry.

The Mughal armies ("lashkar") were primarily a cavalry force. The elite corps were the "ahadi" who provided direct service to the Emperor and acted as guard cavalry. Supplementary cavalry or "dakhilis" were recruited, equipped and paid by the central state. This was in contrast to the "tabinan" horsemen who were the followers of individual noblemen. Their training and equipment varied widely but they made up the backbone of the Mughal cavalry. Finally there were tribal irregulars led by and loyal to tributary chiefs. These included Hindus, Afghans and Turks summoned for military service when their autonomous leaders were called on by the Imperial government.

Ironically, the rise of infantry in the early 16th century coincided with the "golden age" of heavy cavalry; a French or Spanish army at the beginning of the century could have up to half its numbers made up of various kinds of light and heavy cavalry, whereas in earlier medieval and later 17th-century armies the proportion of cavalry was seldom more than a quarter.

Knighthood largely lost its military functions and became more closely tied to social and economic prestige in an increasingly capitalistic Western society. With the rise of drilled and trained infantry, the mounted men-at-arms, now sometimes called "gendarmes" and often part of the standing army themselves, adopted the same role as in the Hellenistic age, that of delivering a decisive blow once the battle was already engaged, either by charging the enemy in the flank or attacking their commander-in-chief.

From the 1550s onwards, the use of gunpowder weapons solidified infantry's dominance of the battlefield and began to allow true mass armies to develop. This is closely related to the increase in the size of armies throughout the early modern period; heavily armored cavalrymen were expensive to raise and maintain and it took years to replace a skilled horseman or a trained horse, while arquebusiers and later musketeers could be trained and kept in the field at much lower cost, and were much easier to replace.

The Spanish tercio and later formations relegated cavalry to a supporting role. The pistol was specifically developed to try to bring cavalry back into the conflict, together with manoeuvres such as the caracole. The caracole was not particularly successful, however, and the charge (whether with sword, pistol, or lance) remained as the primary mode of employment for many types of European cavalry, although by this time it was delivered in much deeper formations and with greater discipline than before. The demi-lancers and the heavily armored sword-and-pistol reiters were among the types of cavalry whose heyday was in the 16th and 17th centuries, as for the Polish winged hussars, a heavy cavalry force that achieved great success against Swedes, Russians, and Turks.

Cavalry retained an important role in this age of regularization and standardization across European armies. First and foremost they remained the primary choice for confronting enemy cavalry. Attacking an unbroken infantry force head-on usually resulted in failure, but extended linear infantry formations were vulnerable to flank or rear attacks. Cavalry was important at Blenheim (1704), Rossbach (1757), Eylau and Friedland (1807), remaining significant throughout the Napoleonic Wars.

Even with the increasing prominence of infantry, cavalry still had an important place in armies. They often patrolled the fringes of army encampments or provided outpost pickets in advance of the main body. During battle they might charge and override artillery cannon, plugging the touchholes with iron spikes. They also pursued enemy infantrymen who had broken from formation, as well as snipers.

The greatest cavalry charge of modern history was at the 1807 battle of Eylau, when the entire 11,000-strong French cavalry reserve, led by MarÃ©chal Murat, launched a huge charge on and through the Russian infantry lines. However, in 1815 at the Battle of Waterloo, repeated charges by up to 9,000 French cavalrymen failed to break the line of the British and German infantry, who had formed squares.

Massed infantry was deadly to cavalry, but offered an excellent target for artillery. Once the bombardment had disordered the infantry formation, cavalry were able to rout and pursue the scattered foot soldiers. It was not until individual firearms gained accuracy and improved rates of fire that cavalry was diminished in this role as well. Even then light cavalry remained an indispensable tool for scouting, screening the army's movements, and harassing the enemy's supply lines until military aircraft supplanted them in this role in the early stages of World War I.

By the 19th century, European cavalry fell into four main categories:


There were cavalry variations for individual nations as well: France had the "chasseurs Ã  cheval"; Prussia had the "JÃ¤ger zu Pferd"; Bavaria, Saxony and Austria had the "Chevaulegers"; and Russia had Cossacks. Britain, from the mid-18th century, had Light Dragoons as light cavalry and Dragoons, Dragoon Guards and Household Cavalry as heavy cavalry. Only after the end of the Napoleonic wars were the Household Cavalry equipped with cuirasses, and some other regiments were converted to lancers. In the United States Army the cavalry were almost always dragoons. The Imperial Japanese Army had its cavalry uniformed as hussars, but they fought as dragoons.

In the Crimean War, the Charge of the Light Brigade and the Thin Red Line at the Battle of Balaclava showed the vulnerability of cavalry, when deployed without effective support.

In the early American Civil War the regular United States Army mounted rifle, dragoon, and two existing cavalry regiments were reorganized and renamed cavalry regiments, of which there were six. Over a hundred other federal and state cavalry regiments were organized, but the infantry played a much larger role in many battles due to its larger numbers, lower cost per rifle fielded, and much easier recruitment. However, cavalry saw a role as part of screening forces and in foraging and scouting. The later phases of the war saw the Federal army developing a truly effective cavalry force fighting as scouts, raiders, and, with repeating rifles, as mounted infantry. The distinguished 1st Virginia Cavalry ranks as one of the most effectual and successful cavalry units on the Confederate side. Noted cavalry commanders included Confederate general J.E.B. Stuart, Nathan Bedford Forrest, and John Singleton Mosby (a.k.a. "The Grey Ghost") and on the Union side, Philip Sheridan and George Armstrong Custer.
Post Civil War, as the volunteer armies disbanded, the regular army cavalry regiments increased in number from six to ten, among them Custer's U.S. 7th Cavalry Regiment of Little Bighorn fame, and the African-American U.S. 9th Cavalry Regiment and U.S. 10th Cavalry Regiment. The black units, along with others (both cavalry and infantry), collectively became known as the Buffalo Soldiers. According to Robert M. Utley: 

These regiments, which rarely took the field as complete organizations, served throughout the American Indian Wars through the close of the frontier in the 1890s. Volunteer cavalry regiments like the Rough Riders consisted of horsemen such as cowboys, ranchers and other outdoorsmen, that served as a cavalry in the United States Military.

During the Franco-Prussian War, at the Battle of Mars-la-Tour in 1870, a Prussian cavalry brigade decisively smashed the centre of the French battle line, after skilfully concealing their approach. This event became known as Von Bredow's Death Ride after the brigade commander Adalbert von Bredow; it would be used in the following decades to argue that massed cavalry charges still had a place on the modern battlefield.

Cavalry found a new role in colonial campaigns (irregular warfare), where modern weapons were lacking and the slow moving infantry-artillery train or fixed fortifications were often ineffective against indigenous insurgents (unless the latter offered a fight on an equal footing, as at Tel-el-Kebir, Omdurman, etc.). Cavalry "flying columns" proved effective, or at least cost-effective, in many campaignsâalthough an astute native commander (like Samori in western Africa, Shamil in the Caucasus, or any of the better Boer commanders) could turn the tables and use the greater mobility of their cavalry to offset their relative lack of firepower compared with European forces.

In 1903 the British Indian Army maintained forty regiments of cavalry, numbering about 25,000 Indian sowars (cavalrymen), with British and Indian officers.

Several of these formations are still active, though they now are armoured formations, for example the Guides Cavalry in Pakistan.

The French Army maintained substantial cavalry forces in Algeria and Morocco from 1830 until the end of the Second World War. Much of the Mediterranean coastal terrain was suitable for mounted action and there was a long established culture of horsemanship amongst the Arab and Berber inhabitants. The French forces included Spahis, Chasseurs d' Afrique, Foreign Legion cavalry and mounted Goumiers. Both Spain and Italy raised cavalry regiments from amongst the indigenous horsemen of their North African territories (see regulares, Italian Spahis and savari respectively).

Imperial Germany employed mounted formations in South West Africa as part of the Schutztruppen (colonial army) garrisoning the territory.

At the beginning of the 20th century all armies still maintained substantial cavalry forces, although there was contention over whether their role should revert to that of mounted infantry (the historic dragoon function). Following the experience of the South African War of 1899â1902 (where mounted Boer citizen commandos fighting on foot from cover proved more effective than regular cavalry) the British Army withdrew lances for all but ceremonial purposes and placed a new emphasis on training for dismounted action. In 1908 however the six British lancer regiments in existence resumed use of this impressive but obsolete weapon for active service.

In 1882 the Imperial Russian Army converted all its line hussar and lancer regiments to dragoons, with an emphasis on mounted infantry training. In 1910 these regiments reverted to their historic roles, designations and uniforms.

By 1909 official regulations dictating the role of the Imperial German cavalry had been revised to indicate an increasing realization of the realities of modern warfare. The massive cavalry charge in three waves which had previously marked the end of annual maneuvers was discontinued and a new emphasis was placed in training on scouting, raiding and pursuit; rather than main battle involvement.

In spite of significant experience in mounted warfare in Morocco during 1908-14, the French cavalry remained a highly conservative institution. The traditional tactical distinctions between heavy, medium and light cavalry branches were retained. French cuirassiers wore breastplates and plumed helmets unchanged from the Napoleonic period, during the early months of World War I. Dragoons were similarly equipped, though they did not wear cuirasses and did carry lances.
Light cavalry were described as being "a blaze of colour". French cavalry of all branches were well mounted and were trained to change position and charge at full gallop.

In August 1914 all combatant armies still retained substantial numbers of cavalry and the mobile nature of the opening battles on both Eastern and Western Fronts provided a number of instances of traditional cavalry actions, though on a smaller and more scattered scale than those of previous wars. The Imperial German cavalry, while as colourful and traditional as any in peacetime appearance, had adopted a practice of falling back on infantry support when any substantial opposition was encountered. These cautious tactics aroused derision amongst their more conservative French and Russian opponents but proved appropriate to the new nature of warfare. A single attempt by the German army, on 12 August 1914, to use six regiments of massed cavalry to cut off the Belgian field army from Antwerp foundered when they were driven back in disorder by rifle fire. The two German cavalry brigades involved lost 492 men and 843 horses in repeated charges against dismounted Belgian lancers and infantry. Once the front lines stabilised on the Western Front, a combination of barbed wire, machine guns and rapid fire rifles proved deadly to horse mounted troops.

On the Eastern Front a more fluid form of warfare arose from flat open terrain favorable to mounted warfare. On the outbreak of war in 1914 the bulk of the Russian cavalry was deployed at full strength in frontier garrisons and during the period that the main armies were mobilizing scouting and raiding into East Prussia and Austrian Galacia was undertaken by mounted troops trained to fight with sabre and lance in the traditional style. On 21 August 1914 the 4th Austro-Hungarian "Kavalleriedivison" fought a major mounted engagement at Jaroslavic with the Russian 10th Cavalry Division, in what was arguably the final historic battle to involve thousands of horsemen on both sides. While this was the last massed cavalry encounter on the Eastern Front, the absence of good roads limited the use of mechanized transport and even the technologically advanced Imperial German Army continued to deploy up to twenty-four horse-mounted divisions in the East, as late as 1917.

For the remainder of the War on the Western Front cavalry had virtually no role to play. The British and French armies dismounted many of their cavalry regiments and used them in infantry and other roles: the Life Guards for example spent the last months of the War as a machine gun corps; and the Australian Light Horse served as light infantry during the Gallipoli campaign. In September 1914 cavalry comprised 9.28% of the total manpower of the British Expeditionary Force in Franceâby July 1918 this proportion had fallen to 1.65%. As early as the first winter of the war most French cavalry regiments had dismounted a squadron each, for service in the trenches. The French cavalry numbered 102,000 in May 1915 but had been reduced to 63,000 by October 1918.
The German Army dismounted nearly all their cavalry in the West, maintaining only one mounted division on that front by January 1917.
Italy entered the war in 1915 with thirty regiments of line cavalry, lancers and light horse. While employed effectively against their Austro-Hungarian counterparts during the initial offensives across the Isonzo River, the Italian mounted forces ceased to have a significant role as the front shifted into mountainous terrain. By 1916 all cavalry machine-gun sections and two complete cavalry divisions had been dismounted and seconded to the infantry.

Some cavalry were retained as mounted troops behind the lines in anticipation of a penetration of the opposing trenches that it seemed would never come. Tanks, introduced on the Western Front by the British in September 1916, had the capacity to achieve such breakthroughs but did not have the reliable range to exploit them. In their first major use at the Battle of Cambrai (1917), the plan was for a cavalry division to follow behind the tanks, however they were not able to cross a canal because a tank had broken the only bridge. It was not until the German Army had been forced to retreat in the Hundred Days Offensive of 1918, that cavalry were again able to operate in their intended role. There was a successful charge by the British 7th Dragoon Guards on the last day of the war.

In the wider spaces of the Eastern Front a more fluid form of warfare continued and there was still a use for mounted troops. Some wide-ranging actions were fought, again mostly in the early months of the war. However, even here the value of cavalry was overrated and the maintenance of large mounted formations at the front by the Russian Army put a major strain on the railway system, to little strategic advantage. In February 1917 the Russian regular cavalry (exclusive of Cossacks) was reduced by nearly a third from its peak number of 200,000, as two squadrons of each regiment were dismounted and incorporated into additional infantry battalions. Their Austro-Hungarian opponents, plagued by a shortage of trained infantry, had been obliged to progressively convert most horse cavalry regiments to dismounted rifle units starting in late 1914.

In the Middle East, during the Sinai and Palestine Campaign mounted forces (British, Indian, Ottoman, Australian, Arab and New Zealand) retained an important strategic role both as mounted infantry and cavalry.

In Egypt the mounted infantry formations like the New Zealand Mounted Rifles Brigade and Australian Light Horse of ANZAC Mounted Division, operating as mounted infantry, drove German and Ottoman forces back from Romani to Magdhaba and Rafa and out of the Egyptian Sinai Peninsula in 1916.

After a stalemate on the GazaâBeersheba line between March and October 1917, Beersheba was captured by the Australian Mounted Division's 4th Light Horse Brigade. Their mounted charge succeeded after a coordinated attack by the British Infantry and Yeomanry cavalry and the Australian and New Zealand Light Horse and Mounted Rifles brigades. A series of coordinated attacks by these Egyptian Expeditionary Force infantry and mounted troops were also successful at the Battle of Mughar Ridge, during which the British infantry divisions and the Desert Mounted Corps drove two Ottoman armies back to the JaffaâJerusalem line. The infantry with mainly dismounted cavalry and mounted infantry fought in the Judean Hills to eventually almost encircle Jerusalem which was occupied shortly after.

During a pause in operations necessitated by the Spring Offensive in 1918 on the Western Front joint infantry and mounted infantry attacks towards Amman and Es Salt resulted in retreats back to the Jordan Valley which continued to be occupied by mounted divisions during the summer of 1918.

The Australian Mounted Division was armed with swords and in September, after the successful breaching of the Ottoman line on the Mediterranean coast by the British Empire infantry XXI Corps was followed by cavalry attacks by the 4th Cavalry Division, 5th Cavalry Division and Australian Mounted Divisions which almost encircled two Ottoman armies in the Judean Hills forcing their retreat. Meanwhile, Chaytor's Force of infantry and mounted infantry in ANZAC Mounted Division held the Jordan Valley, covering the right flank to later advance eastwards to capture Es Salt and Amman and half of a third Ottoman army. A subsequent pursuit by the 4th Cavalry Division and the Australian Mounted Division followed by the 5th Cavalry Division to Damascus. Armoured cars and 5th Cavalry Division lancers were continuing the pursuit of Ottoman units north of Aleppo when the Armistice of Mudros was signed by the Ottoman Empire.

A combination of military conservatism in almost all armies and post-war financial constraints prevented the lessons of 1914â1918 being acted on immediately. There was a general reduction in the number of cavalry regiments in the British, French, Italian and other Western armies but it was still argued with conviction (for example in the 1922 edition of the "EncyclopÃ¦dia Britannica") that mounted troops had a major role to play in future warfare. The 1920s saw an interim period during which cavalry remained as a proud and conspicuous element of all major armies, though much less so than prior to 1914.

Cavalry was extensively used in the Russian Civil War and the Soviet-Polish War. The last major cavalry battle was the Battle of KomarÃ³w in 1920, between Poland and the Russian Bolsheviks. Colonial warfare in Morocco, Syria, the Middle East and the North West Frontier of India provided some opportunities for mounted action against enemies lacking advanced weaponry.

The post-war German Army (Reichsheer) was permitted a large proportion of cavalry (18 regiments or 16.4% of total manpower) under the conditions of the Treaty of Versailles.

The British Army mechanised all cavalry regiments between 1929 and 1941, redefining their role from horse to armoured vehicles to form the Royal Armoured Corps together with the Royal Tank Regiment. The U.S. Cavalry abandoned its sabres in 1934 and commenced the conversion of its horsed regiments to mechanized cavalry, starting with the First Regiment of Cavalry in January 1933.

During the 1930s the French Army experimented with integrating mounted and mechanised cavalry units into larger formations. Dragoon regiments were converted to motorised infantry (trucks and motor cycles), and cuirassiers to armoured units; while light cavalry (Chasseurs a' Cheval, Hussars and Spahis) remained as mounted sabre squadrons. The theory was that mixed forces comprising these diverse units could utilise the strengths of each according to circumstances. In practice mounted troops proved unable to keep up with fast moving mechanised units over any distance.

The thirty-nine cavalry regiments of the British Indian Army were reduced to twenty-one as the result of a series of amalgamations immediately following World War I. The new establishment remained unchanged until 1936 when three regiments were redesignated as permanent training units, each with six, still mounted, regiments linked to them. In 1938 the process of mechanization began with the conversion of a full cavalry brigade (two Indian regiments and one British) to armoured car and tank units. By the end of 1940 all of the Indian cavalry had been mechanized initially, in the majority of cases, to motorized infantry transported in 15cwt trucks. The last horsed regiment of the British Indian Army (other than the Viceregal Bodyguard and some Indian States Forces regiments) was the 19th King George's Own Lancers which had its final mounted parade at Rawalpindi on 28 October 1939. This unit still exists in the Pakistan Army as an armored regiment.

While most armies still maintained cavalry units at the outbreak of World War II in 1939, significant mounted action was largely restricted to the Polish, Balkan and Soviet campaigns. Rather than charge their mounts into battle, cavalry units were either used as mounted infantry (using horses to move into position and then dismounting for combat) or as reconnaissance units (especially in areas not suited to tracked or wheeled vehicles).

A popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This arose from misreporting of a single clash on 1 September near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabres scattered German infantry before being caught in the open by German armoured cars.
Two examples illustrate how the myth developed. First, because motorised vehicles were in short supply, the Poles used horses to pull anti-tank weapons into position. Second, there were a few incidents when Polish cavalry was trapped by German tanks, and attempted to fight free. However, this did not mean that the Polish army chose to attack tanks with horse cavalry. Later, on the Eastern Front, the Red Army did deploy cavalry units effectively against the Germans.

A more correct term would be "mounted infantry" instead of "cavalry", as horses were primarily used as a means of transportation, for which they were very suitable in view of the very poor road conditions in pre-war Poland. Another myth describes Polish cavalry as being armed with both sabres and lances; lances were used for peacetime ceremonial purposes only and the primary weapon of the Polish cavalryman in 1939 was a rifle. Individual equipment did include a sabre, probably because of well-established tradition, and in the case of a melee combat this secondary weapon would probably be more effective than a rifle and bayonet. Moreover, the Polish cavalry brigade order of battle in 1939 included, apart from the mounted soldiers themselves, light and heavy machine guns (wheeled), the Anti-tank rifle, model 35, anti-aircraft weapons, anti tank artillery such as the Bofors 37 mm, also light and scout tanks, etc. The last cavalry vs. cavalry mutual charge in Europe took place in Poland during the Battle of KrasnobrÃ³d, when Polish and German cavalry units clashed with each other.

The last classical cavalry charge of the war took place on March 1, 1945 during the Battle of Schoenfeld by the 1st "Warsaw" Independent Cavalry Brigade. Infantry and tanks had been employed to little effect against the German position, both of which floundered in the open wetlands only to be dominated by infantry and antitank fire from the German fortifications on the forward slope of Hill 157, overlooking the wetlands. The Germans had not taken cavalry into consideration when fortifying their position which, combined with the "Warsaw"s swift assault, overran the German anti-tank guns and consolidated into an attack into the village itself, now supported by infantry and tanks.

The Italian invasion of Greece in October 1940 saw mounted cavalry used effectively by the Greek defenders along the mountainous frontier with Albania. Three Greek cavalry regiments (two mounted and one partially mechanized) played an important role in the Italian defeat in this difficult terrain.

By the final stages of the war only the Soviet Union was still fielding mounted units in substantial numbers, some in combined mechanized and horse units. The advantage of this approach was that in exploitation mounted infantry could keep pace with advancing tanks. Other factors favouring the retention of mounted forces included the high quality of Russian Cossacks and other horse cavalry; and the relative lack of roads suitable for wheeled vehicles in many parts of the Eastern Front. Another consideration was that the logistic capacity required to support very large motorised forces exceeded that necessary for mounted troops. The main usage of Soviet cavalry involved infiltration through front lines with subsequent deep raids, which disorganised German supply lines. Another role was the pursuit of retreating enemy forces during major frontline operations and breakthroughs.

The last mounted sabre charge by Italian cavalry occurred on August 24, 1942 at Isbuscenski (Russia), when a squadron of the Savoia Cavalry Regiment charged the 812th Siberian Infantry Regiment. The remainder of the regiment, together with the Novara Lancers made a dismounted attack in an action that ended with the retreat of the Russians after heavy losses on both sides. The final Italian cavalry action occurred on October 17, 1942 in Poloj (now Croatia) by a squadron of the Alexandria Cavalry Regiment against a large group of Yugoslav partisans.

Romanian, Hungarian and Italian cavalry were dispersed or disbanded following the retreat of the Axis forces from Russia. Germany still maintained some mounted (mixed with bicycles) SS and Cossack units until the last days of the War.

Finland used mounted troops against Russian forces effectively in forested terrain during the Continuation War. The last Finnish cavalry unit was not disbanded until 1947.

The U.S. Army's last horse cavalry actions were fought during World War II: a) by the 26th Cavalry Regimentâa small mounted regiment of Philippine Scouts which fought the Japanese during the retreat down the Bataan peninsula, until it was effectively destroyed by January 1942; and b) on captured German horses by the mounted reconnaissance section of the U.S. 10th Mountain Division in a spearhead pursuit of the German Army across the Po Valley in Italy in April 1945. The last horsed U.S. Cavalry (the Second Cavalry Division) were dismounted in March 1944.

All British Army cavalry regiments had been mechanised since 1 March 1942 when the Queen's Own Yorkshire Dragoons (Yeomanry) was converted to a motorised role, following mounted service against the Vichy French in Syria the previous year. The final cavalry charge by British Empire forces occurred on 21 March 1942 when a 60 strong patrol of the Burma Frontier Force encountered Japanese infantry near Toungoo airfield in central Myanmar. The Sikh sowars of the Frontier Force cavalry, led by Captain Arthur Sandeman of The Central India Horse (21st King George V's Own Horse), charged in the old style with sabres and most were killed.

In the early stages of World War II, mounted units of the Mongolian People's Army were involved in the Battle of Khalkhin Gol against invading Japanese forces. Soviet forces under the command of Georgy Zhukov, together with Mongolian forces, defeated the Japanese Sixth army and effectively ended the SovietâJapanese Border Wars. After the SovietâJapanese Neutrality Pact of 1941, Mongolia remained neutral throughout most of the war, but its geographical situation meant that the country served as a buffer between Japanese forces and the Soviet Union. In addition to keeping around 10% of the population under arms, Mongolia provided half a million trained horses for use by the Soviet Army. In 1945 a partially mounted Soviet-Mongolian Cavalry Mechanized Group played a supporting role on the western flank of the Soviet invasion of Manchuria. The last active service seen by cavalry units of the Mongolian Army occurred in 1946â1948, during border clashes between Mongolia and the Republic of China.

While most modern "cavalry" units have some historic connection with formerly mounted troops this is not always the case. The modern Irish Defence Force (DF) includes a "Cavalry Corps" equipped with armoured cars and Scorpion tracked combat reconnaissance vehicles. The DF has never included horse cavalry since its establishment in 1922 (other than a small mounted escort of Blue Hussars drawn from the Artillery Corps when required for ceremonial occasions). However, the mystique of the cavalry is such that the name has been introduced for what was always a mechanised force.

Some engagements in late 20th and early 21st century guerrilla wars involved mounted troops, particularly against partisan or guerrilla fighters in areas with poor transport infrastructure. Such units were not used as cavalry but rather as mounted infantry. Examples occurred in Afghanistan, Portuguese Africa and Rhodesia. The French Army used existing mounted squadrons of Spahis to a limited extent for patrol work during the Algerian War (1954â62). The Swiss Army maintained a mounted dragoon regiment for combat purposes until 1973. The Portuguese Army used horse mounted cavalry with some success in the wars of independence in Angola and Mozambique in the 1960s and 1970s. During the 1964â79 Rhodesian Bush War the Rhodesian Army created an elite mounted infantry unit called Grey's Scouts to fight unconventional actions against the rebel forces of Robert Mugabe and Joshua Nkomo. The horse mounted infantry of the Scouts were effective and reportedly feared by their opponents in the rebel African forces. In the 1978 to present Afghan Civil War period there have been several instances of horse mounted combat.

Central and South American armies maintained mounted cavalry for longer than those of Asia, Europe, or North America. The Mexican Army included a number of horse mounted cavalry regiments as late as the mid-1990s and the Chilean Army had five such regiments in 1983 as mounted mountain troops.

The Soviet Army retained horse cavalry divisions until 1955, and even at the dissolution of the Soviet Union in 1991, there was an independent horse mounted cavalry squadron in Kyrgyzstan.

Today the Indian Army's 61st Cavalry is reported to be the largest existing horse-mounted cavalry unit still having operational potential. It was raised in 1951 from the amalgamated state cavalry squadrons of Gwalior, Jodhpur, and Mysore. While primarily utilised for ceremonial purposes, the regiment can be deployed for internal security or police roles if required. The 61st Cavalry and the President's Body Guard parade in full dress uniform in New Delhi each year in what is probably the largest assembly of traditional cavalry still to be seen in the world. Both the Indian and the Pakistani armies maintain armoured regiments with the titles of Lancers or Horse, dating back to the 19th century.

As of 2007 the Chinese People's Liberation Army employed two battalions of horse-mounted border guards in Xinjing Military District for border patrol work. The PLA mounted units last saw action during border clashes with Vietnam in the 1970s and 80s, after which most cavalry units were disbanded as part of the major military downsizing of the 1980s.
In the wake of the 2008 Sichuan earthquake, there were calls to rebuild the army horse inventory for disaster relief in difficult terrain. Subsequent Chinese media reporting confirms that the Chinese Army maintains operational horse cavalry at squadron strength in the Mongolia Autonomous Region for scouting and logistical purposes.

The Chilean Army still maintains a mixed armoured cavalry regiment, with elements of it acting as mounted mountain exploration troops, based in the city of Angol, being part of the III Mountain Division, and another independent exploration cavalry detachment in the town of Chaiten. The rugged mountain terrain calls for the use of special horses, suited for that use.

Cavalry or mounted gendarmerie units continue to be maintained for purely or primarily ceremonial purposes by the Algerian, Argentine, Bolivian, Brazilian, British, Bulgarian, Canadian, Chilean, Danish, Dutch, Finnish, French, Hungarian, Indian, Italian, Jordanian, Malaysian, Moroccan, Nepalese, Nigerian, North Korean, Omani, Pakistani, Panamanian, Paraguayan, Peruvian, Polish, Portuguese, Russian, Senegalese, Spanish, Swedish, Thai, Tunisian, Turkmenistan, United States, and Venezuelan armed forces.

A number of armoured regiments in the British Army retain the historic designations of Hussars, Dragoons, Light Dragoons, Dragoon Guards, Lancers and Yeomanry. Only the Household Cavalry (consisting of the Life Guards' mounted squadron, The Blues and Royals' mounted squadron, the State Trumpeters of The Household Cavalry and the Household Cavalry Mounted Band) are maintained for mounted (and dismounted) ceremonial duties in London.

The French Army still has regiments with the historic designations of Cuirassiers, Hussars, Chasseurs, Dragoons and Spahis. Only the cavalry of the Republican Guard and a ceremonial "fanfare" detachment of trumpeters for the cavalry/armoured branch as a whole are now mounted.

In the Canadian Army, a number of regular and reserve units have cavalry roots, including The Royal Canadian Hussars (Montreal), the Governor General's Horse Guards, Lord Strathcona's Horse, The British Columbia Dragoons, The Royal Canadian Dragoons, and the South Alberta Light Horse. Of these, only Lord Strathcona's Horse and the Governor General's Horse Guards maintain an official ceremonial horse-mounted cavalry troop or squadron.

In 2002 the Army of the Russian Federation reintroduced a ceremonial mounted squadron wearing historic uniforms.

Both the Australian and New Zealand armies follow the British practice of maintaining traditional titles (Light Horse or Mounted Rifles) for modern mechanised units. However, neither country retains a horse-mounted unit.

Several armored units of the modern United States Army retain the designation of "Armored cavalry". The United States also has "air cavalry" units equipped with helicopters. The Horse Cavalry Detachment of the U.S. Army's 1st Cavalry Division, made up of active duty soldiers, still functions as an active unit; trained to approximate the weapons, tools, equipment and techniques used by the United States Cavalry in the 1880s.

The First Troop Philadelphia City Cavalry is a volunteer unit within the Pennsylvania Army National Guard which serves as a combat force when in federal service but acts in a mounted disaster relief role when in state service. In addition, the Parsons' Mounted Cavalry is a Reserve Officer Training Corps unit which forms part of the Corps of Cadets at Texas A&M University. Valley Forge Military Academy and College also has a Mounted Company, known as D-Troop .

Some individual U.S. states maintain cavalry units as a part of their respective state defense forces. The Maryland Defense Force includes a cavalry unit, Cavalry Troop A, which serves primarily as a ceremonial unit. The unit training includes a saber qualification course based upon the 1926 U.S. Army course. Cavalry Troop A also assists other Maryland agencies as a rural search and rescue asset. In Massachusetts, The National Lancers trace their lineage to a volunteer cavalry militia unit established in 1836 and are currently organized as an official part of the Massachusetts Organized Militia. The National Lancers maintain three units, Troops A, B, and C, which serve in a ceremonial role and assist in search and rescue missions. In July 2004, the National Lancers were ordered into active state service to guard Camp Curtis Guild during the 2004 Democratic National Convention. The Governor's Horse Guard of Connecticut maintains two companies which are trained in urban crowd control.

Historically, cavalry was divided into horse archers, light and heavy cavalry. The differences were their role in combat, the size of the mount, and how much armor was worn by the mount and rider.

Early light cavalry (like the auxiliaries of the Roman army) were typically used to scout and skirmish, to cut down retreating infantry, and for defeating enemy missile troops. Armoured cavalry such as the Byzantine cataphract were used as shock troopsâthey would charge the main body of the enemy and in many cases, their actions decided the outcome of the battle, hence the later term "battle cavalry".

During the Gunpowder Age, armored cavalry units still retained cuirasses and helmets for their protective value against sword and bayonet strikes, and the morale boost these provide to the wearers. By this time the main difference between light and heavy cavalry was their training; the former was regarded as a tool for harassment and reconnaissance, while the latter was considered best for close-order charges.

Since the development of armored warfare, the distinction between light and heavy armor has persisted basically along the same lines. Armored cars and light tanks have adopted the reconnaissance role while medium and heavy tanks are regarded as the decisive shock troops.

From the beginning of civilization to the 20th century, ownership of heavy cavalry horses has been a mark of wealth amongst settled peoples. A cavalry horse involves considerable expense in breeding, training, feeding, and equipment, and has very little productive use except as a mode of transport.

For this reason, and because of their often decisive military role, the cavalry has typically been associated with high social status. This was most clearly seen in the feudal system, where a lord was expected to enter combat armored and on horseback and bring with him an entourage of lightly armed peasants on foot. If landlords and peasant levies came into conflict, the poorly trained footmen would be ill-equipped to defeat armored knights.

In later national armies, service as an officer in the cavalry was generally a badge of high social status. For instance prior to 1914 most officers of British cavalry regiments came from a socially privileged background and the considerable expenses associated with their role generally required private means, even after it became possible for officers of the line infantry regiments to live on their pay. Options open to poorer cavalry officers in the various European armies included service with less fashionable (though often highly professional) frontier or colonial units. These included the British Indian cavalry, the Russian Cossacks or the French Chasseurs d' Afrique.

During the 19th and early 20th centuries most monarchies maintained a mounted cavalry element in their royal or imperial guards. These ranged from small units providing ceremonial escorts and palace guards, through to large formations intended for active service. The mounted escort of the Spanish Royal Household provided an example of the former and the twelve cavalry regiments of the Prussian Imperial Guard an example of the latter. In either case the officers of such units were likely to be drawn from the aristocracies of their respective societies.

Some sense of the noise and power of a cavalry charge can be gained from the 1970 film "Waterloo", which featured some 2,000 cavalrymen, some of them Cossacks. It included detailed displays of the horsemanship required to manage animal and weapons in large numbers at the gallop (unlike the real battle of Waterloo, where deep mud significantly slowed the horses). The Gary Cooper movie "They Came to Cordura" contains a scene of a cavalry regiment deploying from march to battle line formation. A smaller-scale cavalry charge can be seen in "" (2003); although the finished scene has substantial computer-generated imagery, raw footage and reactions of the riders are shown in the Extended Version DVD Appendices.

Other films that show cavalry actions include:







</doc>
<doc id="6818" url="https://en.wikipedia.org/wiki?curid=6818" title="Citric acid cycle">
Citric acid cycle

The citric acid cycle (CAC)Â â also known as the" TCA cycle (tricarboxylic acid cycle) " or the" Krebs cycleÂ "â is a series of chemical reactions used by all aerobic organisms to release stored energy through the oxidation of acetyl-CoA derived from carbohydrates, fats, and proteins, into adenosine triphosphate (ATP) and carbon dioxide. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest established components of cellular metabolism and may have originated abiogenically. Even though it is branded as a 'cycle', it is not necessary for metabolites to follow only one specific route; at least three segments of the citric acid cycle have been recognized.

The name of this metabolic pathway is derived from the citric acid (a type of tricarboxylic acid, often called citrate, as the ionized form predominates at biological pH) that is consumed and then regenerated by this sequence of reactions to complete the cycle. The cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD to NADH, and produces carbon dioxide as a waste byproduct. The NADH generated by the citric acid cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.

In eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria, which lack mitochondria, the citric acid cycle reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion. The overall yield of energy-containing compounds from the TCA cycle is three NADH, one FADH, and one GTP.

Several of the components and reactions of the citric acid cycle were established in the 1930s by the research of Albert Szent-GyÃ¶rgyi, who received the Nobel Prize in Physiology or Medicine in 1937 specifically for his discoveries pertaining to fumaric acid, a key component of the cycle. He was able to make this discovery successful with the help of pigeon breast muscle. Because this tissue maintains its oxidative capacity well after breaking down in the "Latapie" mill and releasing in aqueous solutions, breast muscle of the pigeon was very well qualified for the study of oxidative reactions. The citric acid cycle itself was finally identified in 1937 by Hans Adolf Krebs and William Arthur Johnson while at the University of Sheffield, for which the former received the Nobel Prize for Physiology or Medicine in 1953, and for whom the cycle is sometimes named (Krebs cycle).

The citric acid cycle is a key metabolic pathway that connects carbohydrate, fat, and protein metabolism. The reactions of the cycle are carried out by eight enzymes that completely oxidize acetate (a two carbon molecule), in the form of acetyl-CoA, into two molecules each of carbon dioxide and water. Through catabolism of sugars, fats, and proteins, the two-carbon organic product acetyl-CoA (a form of acetate) is produced which enters the citric acid cycle. The reactions of the cycle also convert three equivalents of nicotinamide adenine dinucleotide (NAD) into three equivalents of reduced NAD (NADH), one equivalent of flavin adenine dinucleotide (FAD) into one equivalent of FADH, and one equivalent each of guanosine diphosphate (GDP) and inorganic phosphate (P) into one equivalent of guanosine triphosphate (GTP). The NADH and FADH generated by the citric acid cycle are, in turn, used by the oxidative phosphorylation pathway to generate energy-rich ATP.

One of the primary sources of acetyl-CoA is from the breakdown of sugars by glycolysis which yield pyruvate that in turn is decarboxylated by the pyruvate dehydrogenase complex generating acetyl-CoA according to the following reaction scheme:

The product of this reaction, acetyl-CoA, is the starting point for the citric acid cycle. Acetyl-CoA may also be obtained from the oxidation of fatty acids. Below is a schematic outline of the cycle:

There are ten basic steps in the citric acid cycle, as outline below. The cycle is continuously supplied with new carbon in the form of acetyl-CoA, entering at step 0 in the table.
Two carbon atoms are oxidized to CO, the energy from these reactions is transferred to other metabolic processes through GTP (or ATP), and as electrons in NADH and QH. The NADH generated in the citric acid cycle may later be oxidized (donate its electrons) to drive ATP synthesis in a type of process called oxidative phosphorylation. FADH is covalently attached to succinate dehydrogenase, an enzyme which functions both in the CAC and the mitochondrial electron transport chain in oxidative phosphorylation. FADH, therefore, facilitates transfer of electrons to coenzyme Q, which is the final electron acceptor of the reaction catalyzed by the succinate:ubiquinone oxidoreductase complex, also acting as an intermediate in the electron transport chain.

Mitochondria in animals, including humans, possess two succinyl-CoA synthetases: one that produces GTP from GDP, and another that produces ATP from ADP. Plants have the type that produces ATP (ADP-forming succinyl-CoA synthetase). Several of the enzymes in the cycle may be loosely associated in a multienzyme protein complex within the mitochondrial matrix.

The GTP that is formed by GDP-forming succinyl-CoA synthetase may be utilized by nucleoside-diphosphate kinase to form ATP (the catalyzed reaction is GTP + ADP â GDP + ATP).

Products of the first turn of the cycle are one GTP (or ATP), three NADH, one QH and two CO.

Because two acetyl-CoA molecules are produced from each glucose molecule, two cycles are required per glucose molecule. Therefore, at the end of two cycles, the products are: two GTP, six NADH, two QH, and four CO.

The above reactions are balanced if P represents the HPO ion, ADP and GDP the ADP and GDP ions, respectively, and ATP and GTP the ATP and GTP ions, respectively.

The total number of ATP molecules obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38.

The theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent NADH and 2 ATP per UQH). In eukaryotes, two equivalents of NADH and four equivalents of ATP are generated in glycolysis, which takes place in the cytoplasm. Transport of two of these equivalents of NADH into the mitochondria consumes two equivalents of ATP, thus reducing the net production of ATP to 36. Furthermore, inefficiencies in oxidative phosphorylation due to leakage of protons across the mitochondrial membrane and slippage of the ATP synthase/proton pump commonly reduces the ATP yield from NADH and UQH to less than the theoretical maximum yield. The observed yields are, therefore, closer to ~2.5 ATP per NADH and ~1.5 ATP per UQH, further reducing the total net production of ATP to approximately 30. An assessment of the total ATP yield with newly revised proton-to-ATP ratios provides an estimate of 29.85 ATP per glucose molecule.

While the citric acid cycle is in general highly conserved, there is significant variability in the enzymes found in different taxa (note that the diagrams on this page are specific to the mammalian pathway variant).

Some differences exist between eukaryotes and prokaryotes. The conversion of D-"threo"-isocitrate to 2-oxoglutarate is catalyzed in eukaryotes by the NAD-dependent EC 1.1.1.41, while prokaryotes employ the NADP-dependent EC 1.1.1.42. Similarly, the conversion of ("S")-malate to oxaloacetate is catalyzed in eukaryotes by the NAD-dependent EC 1.1.1.37, while most prokaryotes utilize a quinone-dependent enzyme, EC 1.1.5.4.

A step with significant variability is the conversion of succinyl-CoA to succinate. Most organisms utilize EC 6.2.1.5, succinateâCoA ligase (ADP-forming) (despite its name, the enzyme operates in the pathway in the direction of ATP formation). In mammals a GTP-forming enzyme, succinateâCoA ligase (GDP-forming) (EC 6.2.1.4) also operates. The level of utilization of each isoform is tissue dependent. In some acetate-producing bacteria, such as "Acetobacter aceti", an entirely different enzyme catalyzes this conversionÂ â EC 2.8.3.18, succinyl-CoA:acetate CoA-transferase. This specialized enzyme links the TCA cycle with acetate metabolism in these organisms. Some bacteria, such as "Helicobacter pylori", employ yet another enzyme for this conversionÂ â succinyl-CoA:acetoacetate CoA-transferase (EC 2.8.3.5).

Some variability also exists at the previous stepÂ â the conversion of 2-oxoglutarate to succinyl-CoA. While most organisms utilize the ubiquitous NAD-dependent 2-oxoglutarate dehydrogenase, some bacteria utilize a ferredoxin-dependent 2-oxoglutarate synthase (EC 1.2.7.3).
Other organisms, including obligately autotrophic and methanotrophic bacteria and archaea, bypass succinyl-CoA entirely, and convert 2-oxoglutarate to succinate via succinate semialdehyde, using EC 4.1.1.71, 2-oxoglutarate decarboxylase, and EC 1.2.1.79, succinate-semialdehyde dehydrogenase.

Allosteric regulation by metabolites. The regulation of the citric acid cycle is largely determined by product inhibition and substrate availability. If the cycle were permitted to run unchecked, large amounts of metabolic energy could be wasted in overproduction of reduced coenzyme such as NADH and ATP. The major eventual substrate of the cycle is ADP which gets converted to ATP. A reduced amount of ADP causes accumulation of precursor NADH which in turn can inhibit a number of enzymes. NADH, a product of all dehydrogenases in the citric acid cycle with the exception of succinate dehydrogenase, inhibits pyruvate dehydrogenase, isocitrate dehydrogenase, Î±-ketoglutarate dehydrogenase, and also citrate synthase. Acetyl-coA inhibits pyruvate dehydrogenase, while succinyl-CoA inhibits alpha-ketoglutarate dehydrogenase and citrate synthase. When tested in vitro with TCA enzymes, ATP inhibits citrate synthase and Î±-ketoglutarate dehydrogenase; however, ATP levels do not change more than 10% in vivo between rest and vigorous exercise. There is no known allosteric mechanism that can account for large changes in reaction rate from an allosteric effector whose concentration changes less than 10%.

Citrate is used for feedback inhibition, as it inhibits phosphofructokinase, an enzyme involved in glycolysis that catalyses formation of fructose 1,6-bisphosphate, a precursor of pyruvate. This prevents a constant high rate of flux when there is an accumulation of citrate and a decrease in substrate for the enzyme.

Regulation by calcium. Calcium is also used as a regulator in the citric acid cycle. Calcium levels in the mitochondrial matrix can reach up to the tens of micromolar levels during cellular activation. It activates pyruvate dehydrogenase phosphatase which in turn activates the pyruvate dehydrogenase complex. Calcium also activates isocitrate dehydrogenase and Î±-ketoglutarate dehydrogenase. This increases the reaction rate of many of the steps in the cycle, and therefore increases flux throughout the pathway.

Transcriptional regulation. Recent work has demonstrated an important link between intermediates of the citric acid cycle and the regulation of hypoxia-inducible factors (HIF). HIF plays a role in the regulation of oxygen homeostasis, and is a transcription factor that targets angiogenesis, vascular remodeling, glucose utilization, iron transport and apoptosis. HIF is synthesized constitutively, and hydroxylation of at least one of two critical proline residues mediates their interaction with the von Hippel Lindau E3 ubiquitin ligase complex, which targets them for rapid degradation. This reaction is catalysed by prolyl 4-hydroxylases. Fumarate and succinate have been identified as potent inhibitors of prolyl hydroxylases, thus leading to the stabilisation of HIF.

Several catabolic pathways converge on the citric acid cycle. Most of these reactions add intermediates to the citric acid cycle, and are therefore known as anaplerotic reactions, from the Greek meaning to "fill up". These increase the amount of acetyl CoA that the cycle is able to carry, increasing the mitochondrion's capability to carry out respiration if this is otherwise a limiting factor. Processes that remove intermediates from the cycle are termed "cataplerotic" reactions.

In this section and in the next, the citric acid cycle intermediates are indicated in "italics" to distinguish them from other substrates and end-products.

Pyruvate molecules produced by glycolysis are actively transported across the inner mitochondrial membrane, and into the matrix. Here they can be oxidized and combined with coenzyme A to form CO, "acetyl-CoA", and NADH, as in the normal cycle.

However, it is also possible for pyruvate to be carboxylated by pyruvate carboxylase to form "oxaloacetate". This latter reaction "fills up" the amount of "oxaloacetate" in the citric acid cycle, and is therefore an anaplerotic reaction, increasing the cycleâs capacity to metabolize "acetyl-CoA" when the tissue's energy needs (e.g. in muscle) are suddenly increased by activity.

In the citric acid cycle all the intermediates (e.g. "citrate", "iso-citrate", "alpha-ketoglutarate", "succinate", "fumarate", "malate", and "oxaloacetate") are regenerated during each turn of the cycle. Adding more of any of these intermediates to the mitochondrion therefore means that that additional amount is retained within the cycle, increasing all the other intermediates as one is converted into the other. Hence the addition of any one of them to the cycle has an anaplerotic effect, and its removal has a cataplerotic effect. These anaplerotic and cataplerotic reactions will, during the course of the cycle, increase or decrease the amount of "oxaloacetate" available to combine with "acetyl-CoA" to form "citric acid". This in turn increases or decreases the rate of ATP production by the mitochondrion, and thus the availability of ATP to the cell.

"Acetyl-CoA", on the other hand, derived from pyruvate oxidation, or from the beta-oxidation of fatty acids, is the only fuel to enter the citric acid cycle. With each turn of the cycle one molecule of "acetyl-CoA" is consumed for every molecule of "oxaloacetate" present in the mitochondrial matrix, and is never regenerated. It is the oxidation of the acetate portion of "acetyl-CoA" that produces CO and water, with the energy thus released captured in the form of ATP. The three steps of beta-oxidation resemble the steps that occur in the production of oxaloacetate from succinate in the TCA cycle. Acyl-CoA is oxidized to trans-Enoyl-CoA while FAD is reduced to FADH, which is similar to the oxidation of succinate to fumarate. Following, trans-Enoyl-CoA is hydrated across the double bond to beta-hydroxyacyl-CoA, just like fumarate is hydrated to malate. Lastly, beta-hydroxyacyl-CoA is oxidized to beta-ketoacyl-CoA while NAD+ is reduced to NADH, which follows the same process as the oxidation of malate to oxaloacetate.

In the liver, the carboxylation of cytosolic pyruvate into intra-mitochondrial "oxaloacetate" is an early step in the gluconeogenic pathway which converts lactate and de-aminated alanine into glucose, under the influence of high levels of glucagon and/or epinephrine in the blood. Here the addition of "oxaloacetate" to the mitochondrion does not have a net anaplerotic effect, as another citric acid cycle intermediate ("malate") is immediately removed from the mitochondrion to be converted into cytosolic oxaloacetate, which is ultimately converted into glucose, in a process that is almost the reverse of glycolysis.

In protein catabolism, proteins are broken down by proteases into their constituent amino acids. Their carbon skeletons (i.e. the de-aminated amino acids) may either enter the citric acid cycle as intermediates (e.g. "alpha-ketoglutarate" derived from glutamate or glutamine), having an anaplerotic effect on the cycle, or, in the case of leucine, isoleucine, lysine, phenylalanine, tryptophan, and tyrosine, they are converted into "acetyl-CoA" which can be burned to CO and water, or used to form ketone bodies, which too can only be burned in tissues other than the liver where they are formed, or excreted via the urine or breath. These latter amino acids are therefore termed "ketogenic" amino acids, whereas those that enter the citric acid cycle as intermediates can only be cataplerotically removed by entering the gluconeogenic pathway via "malate" which is transported out of the mitochondrion to be converted into cytosolic oxaloacetate and ultimately into glucose. These are the so-called "glucogenic" amino acids. De-aminated alanine, cysteine, glycine, serine, and threonine are converted to pyruvate and can consequently either enter the citric acid cycle as "oxaloacetate" (an anaplerotic reaction) or as "acetyl-CoA" to be disposed of as CO and water.

In fat catabolism, triglycerides are hydrolyzed to break them into fatty acids and glycerol. In the liver the glycerol can be converted into glucose via dihydroxyacetone phosphate and glyceraldehyde-3-phosphate by way of gluconeogenesis. In many tissues, especially heart and skeletal muscle tissue, fatty acids are broken down through a process known as beta oxidation, which results in the production of mitochondrial "acetyl-CoA", which can be used in the citric acid cycle. Beta oxidation of fatty acids with an odd number of methylene bridges produces propionyl-CoA, which is then converted into "succinyl-CoA" and fed into the citric acid cycle as an anaplerotic intermediate.

The total energy gained from the complete breakdown of one (six-carbon) molecule of glucose by glycolysis, the formation of 2 "acetyl-CoA" molecules, their catabolism in the citric acid cycle, and oxidative phosphorylation equals about 30 ATP molecules, in eukaryotes. The number of ATP molecules derived from the beta oxidation of a 6 carbon segment of a fatty acid chain, and the subsequent oxidation of the resulting 3 molecules of "acetyl-CoA" is 40.

In this subheading, as in the previous one, the TCA intermediates are identified by "italics".

Several of the citric acid cycle intermediates are used for the synthesis of important compounds, which will have significant cataplerotic effects on the cycle.
"Acetyl-CoA" cannot be transported out of the mitochondrion. To obtain cytosolic acetyl-CoA, "citrate" is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to mitochondrion as "malate" (and then converted back into "oxaloacetate" to transfer more "acetyl-CoA" out of the mitochondrion). The cytosolic acetyl-CoA is used for fatty acid synthesis and the production of cholesterol. Cholesterol can, in turn, be used to synthesize the steroid hormones, bile salts, and vitamin D.

The carbon skeletons of many non-essential amino acids are made from citric acid cycle intermediates. To turn them into amino acids the alpha keto-acids formed from the citric acid cycle intermediates have to acquire their amino groups from glutamate in a transamination reaction, in which pyridoxal phosphate is a cofactor. In this reaction the glutamate is converted into "alpha-ketoglutarate", which is a citric acid cycle intermediate. The intermediates that can provide the carbon skeletons for amino acid synthesis are "oxaloacetate" which forms aspartate and asparagine; and "alpha-ketoglutarate" which forms glutamine, proline, and arginine.

Of these amino acids, aspartate and glutamine are used, together with carbon and nitrogen atoms from other sources, to form the purines that are used as the bases in DNA and RNA, as well as in ATP, AMP, GTP, NAD, FAD and CoA.

The pyrimidines are partly assembled from aspartate (derived from "oxaloacetate"). The pyrimidines, thymine, cytosine and uracil, form the complementary bases to the purine bases in DNA and RNA, and are also components of CTP, UMP, UDP and UTP.

The majority of the carbon atoms in the porphyrins come from the citric acid cycle intermediate, "succinyl-CoA". These molecules are an important component of the hemoproteins, such as hemoglobin, myoglobin and various cytochromes.

During gluconeogenesis mitochondrial "oxaloacetate" is reduced to "malate" which is then transported out of the mitochondrion, to be oxidized back to oxaloacetate in the cytosol. Cytosolic oxaloacetate is then decarboxylated to phosphoenolpyruvate by phosphoenolpyruvate carboxykinase, which is the rate limiting step in the conversion of nearly all the gluconeogenic precursors (such as the glucogenic amino acids and lactate) into glucose by the liver and kidney.

Because the citric acid cycle is involved in both catabolic and anabolic processes, it is known as an amphibolic pathway.

The metabolic role of lactate is well recognized as a fuel for tissues and tumors. In the classical Cori cycle, muscles produce lactate which is then taken up by the liver for gluconeogenesis. New studies suggest that lactate can be used as a source of carbon for the TCA cycle.

It is believed that components of the citric acid cycle were derived from anaerobic bacteria, and that the TCA cycle itself may have evolved more than once. Theoretically, several alternatives to the TCA cycle exist; however, the TCA cycle appears to be the most efficient. If several TCA alternatives had evolved independently, they all appear to have converged to the TCA cycle.




</doc>
<doc id="6821" url="https://en.wikipedia.org/wiki?curid=6821" title="Military engineering vehicle">
Military engineering vehicle

A military engineering vehicle is a vehicle built for the construction work or for the transportation of combat engineers on the battlefield. These vehicles may be modified civilian equipment (such as the armoured bulldozers that many nations field) or purpose-built military vehicles (such as the AVRE). The first appearance of such vehicles coinsided with the appearance of the first tanks, these vehicles were modified Mark V tanks for bridging and mine clearance. Modern "military engineering vehicles" are expected to fulfil numerous roles, as such they undertake numerous forms, examples of roles include; Bulldozers, cranes, graders, excavators, dump trucks, Breaching vehicles, Bridging vehicles, Military ferries, amphibious crossing vehicles, and Combat Engineer Section Carriers. 

A Heavy RE tank was developed shortly after World War I by Major Giffard LeQuesne Martel RE. This vehicle was a modified Mark V tank. Two support functions for these Engineer Tanks were developed: bridging and mine clearance. The bridging component involved an assault bridge, designed by Major Charles Inglis RE, called the Canal Lock Bridge, which had sufficient length to span a canal lock. Major Martel mated the bridge with the tank and used hydraulic power generated by the tank's engine to manoeuvre the bridge into place. For mine clearance the tanks were equipped with 2 ton rollers.

Between the wars various experimental bridging tanks were used to test a series of methods for bridging obstacles and developed by the Experimental Bridging Establishment (EBE). Captain SG Galpin RE conceived a prototype Light Tank Mk V to test the Scissors Assault Bridge. This concept was realised by Captain SA Stewart RE with significant input from a Mr DM Delany, a scientific civil servant in the employ of the EBE. MB Wild & Co, Birmingham, also developed a bridge that could span gaps of 26 feet using a complex system of steel wire ropes and a travelling jib, where the front section was projected and then attached to the rear section prior to launching the bridge. This system had to be abandoned due to lack of success in getting it to work, however the idea was later used successfully on the Beaver Bridge Laying Tank.

Once World War Two had begun, the development of armoured vehicles for use by engineers in the field was accelerated under Delaney's direction. The EBE rapidly developed an assault bridge carried on a modified Covenanter tank capable of deploying a 24-ton tracked load capacity bridge (Class 24) that could span gaps of 30 feet. However, it did not see service in the British armed forces, and all vehicles were passed onto Allied forces such as Australia and Czechoslovakia.

A Class 30 design superseded the Class 24 with no real re-design, simply the substitution of the Covenanter tank with a suitably modified Valentine.
As tanks in the war got heavier, a new bridge capable of supporting them was developed. A heavily modified Churchill used a single-piece bridge mounted on a turret-less tank and was able to lay the bridge in 90 seconds; this bridge was able to carry a 60-ton tracked or 40-ton wheeled load.

Hobart's Funnies were a number of unusually modified tanks operated during the Second World War by the 79th Armoured Division of the British Army or by specialists from the Royal Engineers. They were designed in light of problems that more standard tanks experienced during the amphibious Dieppe Raid, so that the new models would be able to overcome the problems of the planned Invasion of Normandy. These tanks played a major part on the Commonwealth beaches during the landings. They were forerunners of the modern combat engineering vehicle and were named after their commander, Major General Percy Hobart.

Hobart's unusual, specialized tanks, nicknamed "funnies", included:

In U.S. Forces, Sherman tanks were also fitted with dozer blades, and anti-mine roller devices were developed, enabling engineering operations and providing similar capabilities.

Post war, the value of the combat engineering vehicles had been proven, and armoured multi-role engineering vehicles have been added to the majority of armoured forces.

Military engineering can employ a wide variety of heavy equipment in the same or similar ways to how this equipment is used outside the military. Bulldozers, cranes, graders, excavators, dump trucks, loaders, and backhoes all see extensive use by military engineers.

Military engineers may also use civilian heavy equipment which was modified for military applications. Typically, this involves adding armour for protection from battlefield hazards such as artillery, unexploded ordnance, mines, and small arms fire. Often this protection is provided by armour plates and steel jackets. Some examples of armoured civilian heavy equipment are the IDF Caterpillar D9, American D7 TPK, Canadian D6 armoured bulldozer, cranes, graders, excavators, and M35 2-1/2 ton cargo truck.

Militarized heavy equipment may also take on the form of traditional civilian equipment designed and built to unique military specifications. These vehicles typically sacrifice some depth of capability from civilian models in order to gain greater speed and independence from prime movers. Examples of this type of vehicle include high speed backhoes such as the Australian Army's High Mobility Engineering Vehicle (HMEV) from Thales or the Canadian Army's Multi-Purpose Engineer Vehicle (MPEV) from Arva.

"The main article for civilian heavy equipment is:" Heavy equipment (construction)

Typically based on the platform of a main battle tank, these vehicles go by different names depending upon the country of use or manufacture. In the US the term "combat engineer vehicle (CEV)" is used, in the UK the term "Armoured Vehicle Royal Engineers (AVRE)" is used, while in Canada and other commonwealth nations the term "armoured engineer vehicle (AEV)" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large calibre demolition cannon, augers, winches, excavator arms and cranes or lifting booms.

These vehicles are designed to directly conduct obstacle breaching operations and to conduct other earth-moving and engineering work on the battlefield. Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle. Although the term "armoured engineer vehicle" is used specifically to describe these multi-purpose tank based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank based engineering vehicles used in the support of mechanized forces. Thus, "armoured engineer vehicle" used generically would refer to AEV, AVLB, Assault Breachers, and so on.

Lighter and less multi-functional than the CEVs or AEVs described above, these vehicles are designed to conduct earth-moving work on the battlefield. These vehicles have greater high speed mobility than traditional heavy equipment and are protected against the effects of blast and fragmentation. Good examples are the American M9 ACE and the UK FV180 Combat Engineer Tractor.

These vehicles are equipped with mechanical or other means for the breaching of man made obstacles. Common types of breaching vehicles include mechanical flails, mine plough vehicles, and mine roller vehicles. In some cases, these vehicles will also mount Mine-clearing line charges. Breaching vehicles may be either converted armoured fighting vehicles or purpose built vehicles. In larger militaries, converted AFV are likely to be used as "assault breachers" while the breached obstacle is still covered by enemy observation and fire, and then purpose built breaching vehicles will create additional lanes for following forces.

Good examples of breaching vehicles include the USMC M1 Assault Breacher Vehicle, the UK Aardvark JSFU, and the Singaporean Trailblazer.

Several types of military bridging vehicles have been developed. An armoured vehicle-launched bridge (AVLB) is typically a modified tank hull converted to carry a bridge into battle in order to support crossing ditches, small waterways, or other gap obstacles.

Another type of bridging vehicle is the truck launched bridge. The Soviet TMM bridging truck could carry and launch a 10-meter bridge that could be daisy-chained with other TMM bridges to cross larger obstacles. More recent developments have seen the conversion of AVLB and truck launched bridge with launching systems that can be mounted on either tank or truck for bridges that are capable of supporting heavy main battle tanks.

Earlier examples of bridging vehicles include a type in which a converted tank hull is the bridge. On these vehicles, the hull deck comprises the main portion of the tread way while ramps extend from the front and rear of the vehicle to allow other vehicles to climb over the bridging vehicle and cross obstacles. An example of this type of armoured bridging vehicle was the Churchill Ark used in the Second World War.

Another type of CEVs are armoured fighting vehicles which are used to transport sappers (combat engineers) and can be fitted with a bulldozer's blade and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).

One of the major tasks of military engineering is crossing major rivers. Several military engineering vehicles have been developed in various nations to achieve this task. One of the more common types is the amphibious ferry such as the M3 Amphibious Rig. These vehicles are self-propelled on land, they can transform into raft type ferries when in the water, and often multiple vehicles can connect to form larger rafts or floating bridges. Other types of military ferries, such as the Soviet "Plavayushij Transportyor - Srednyj", are able to load while still on land and transport other vehicles cross country and over water.

In addition to amphibious crossing vehicles, military engineers may also employ several types of boats. Military assault boats are small boats propelled by oars or an outboard motor and used to ferry dismounted infantry across water.

Most CEVs are armoured fighting vehicles that may be based on a tank chassis and have special attachments in order to breach obstacles. Such attachments may include dozer blades, mine rollers, cranes etc. An example of an engineering vehicle of this kind is a bridgelaying tank, which replaces the turret with a segmented hydraulic bridge. The Hobart's Funnies of the Second World War were a wide variety of armoured vehicles for combat engineering tasks. They were allocated to the initial beachhead assaults by the British and Commonwealth forces in the D-Day landings

The British Churchill tank because of its good cross-country performance and capacious interior with side hatches became the most adapted with modifications, the base unit being the AVRE carrying a large demolition gun.












</doc>
<doc id="6822" url="https://en.wikipedia.org/wiki?curid=6822" title="Catalonia">
Catalonia

Catalonia (; ; ; ) is an autonomous community on the northeastern corner of Spain, self-designated as a "nationality" by its Statute of Autonomy. Catalonia consists of four provinces: Barcelona, Girona, Lleida, and Tarragona. The capital and largest city is Barcelona, the second-most populated municipality in Spain and the core of the sixth most populous urban area in the European Union. It comprises most of the territory of the former Principality of Catalonia (with the remainder Roussillon now part of France's PyrÃ©nÃ©es-Orientales, Occitanie). It is bordered by France (Occitanie) and Andorra to the north, the Mediterranean Sea to the east, and the Spanish autonomous communities of Aragon to the west and Valencia to the south. The official languages are Catalan, Spanish, and the Aranese dialect of Occitan.

In the late 8th century, the counties of the March of Gothia and the Hispanic March were established by the Frankish kingdom as feudal vassals across and near the eastern Pyrenees as a defensive barrier against Muslim invasions. The eastern counties of these marches were united under the rule of the Frankish vassal, the count of Barcelona. In the 10th century the County of Barcelona became independent "de facto". In 1137, Barcelona and the Kingdom of Aragon were united by marriage under the Crown of Aragon. The "de jure" end of Frankish rule was ratified by French and Aragonese monarchs in the Treaty of Corbeil in 1258. The Principality of Catalonia developed its own institutional system, such as courts (parliament), and constitutions, becoming the base for the Crown of Aragon's naval power, trade and expansionism in the Mediterranean. In the later Middle Ages, Catalan literature flourished. During the last Medieval centuries natural disasters, social turmoils and military conflicts affected the Principality. Between 1469 and 1516, the king of Aragon and the queen of Castile were married and ruled their realms together, retaining all of their distinct institutions and legislation.

During the Franco-Spanish War (1635â1659), Catalonia revolted (1640â1652) against a large and burdensome presence of the royal army in its territory, being briefly proclaimed a republic under French protection. Within a brief period France took full control of Catalonia, until it was largely reconquered by the Spanish army. Under the terms of the Treaty of the Pyrenees in 1659, the Spanish Crown ceded the northern parts of Catalonia, mostly the County of Roussillon, to France. During the War of the Spanish Succession (1701â1714), the Crown of Aragon sided against the Bourbon Philip V of Spain; following Catalan defeat on 11 September 1714, Philip V, inspired by the model of France imposed a unifying administration across Spain, enacting the Nueva Planta decrees, suppressing the main Catalan institutions and rights like in the other realms of the Crown of Aragon. This led to the eclipse of Catalan as a language of government and literature, replaced by Spanish. Throughout the 18th century, Catalonia experienced economic growth, reinforced in the late quarter of the century when the Castile's trade monopoly with American colonies ended.

In the 19th century, Catalonia was severely affected by the Napoleonic and Carlist Wars. In the second third of the century, Catalonia experienced significant industrialisation. As wealth from the industrial expansion grew, Catalonia saw a cultural renaissance coupled with incipient nationalism while several workers movements appeared. In 1914, the four Catalan provinces formed a commonwealth, and with the return of democracy during the Second Spanish Republic (1931â1939), the Generalitat of Catalonia was restored as an autonomous government. After the Spanish Civil War, the Francoist dictatorship enacted repressive measures, abolishing Catalan self-government and banning the official use of the Catalan language again. After a first period of autarky, from the late 1950s through to the 1970s Catalonia saw rapid economic growth, drawing many workers from across Spain, making Barcelona one of Europe's largest industrial metropolitan areas and turning Catalonia into a major tourist destination. Since the Spanish transition to democracy (1975â1982), Catalonia has regained considerable autonomy in political, educational, environmental, and cultural affairs and is now one of the most economically dynamic communities of Spain. In the 2010s there has been growing support for Catalan independence. On 27 October 2017, the Catalan Parliament declared independence from Spain following a disputed referendum. The Spanish Senate voted in favour of enforcing direct rule by removing the entire Catalan government and calling a snap regional election for 21 December. On 2 November of the same year, the Spanish Supreme Court imprisoned seven former ministers of the Catalan government on charges of rebellion and misuse of public funds, while several othersâincluding then-President of Catalonia, Carles Puigdemontâfled to other European countries (such as Belgium, in Puigdemont's case).

The name Cataloniaâ"Catalunya" in Catalan, spelled "Cathalonia", or "Cathalaunia" in Medieval Latinâbegan to be used for the homeland of the Catalans ("Cathalanenses") in the late 11th century and was probably used before as a territorial reference to the group of counties that comprised part of the March of Gothia and March of Hispania under the control of the Count of Barcelona and his relatives. The origin of the name "Catalunya" is subject to diverse interpretations because of a lack of evidence.

One theory suggests that "Catalunya" derives from the name "Gothia" (or "Gauthia") "Launia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, known as "Gothia", whence "Gothland" > "Gothlandia" > "Gothalania" > "Cathalaunia" > "Catalonia" theoretically derived. During the Middle Ages, Byzantine chroniclers claimed that "Catalania" derives from the local medley of Goths with Alans, initially constituting a "Goth-Alania".

Other less plausible or recent theories suggest:


In English, "Catalonia" is pronounced . The native name, "Catalunya", is pronounced in Central Catalan, the most widely spoken variety, whose pronunciation is considered standard. The Spanish name is "CataluÃ±a" (), and the Aranese name is "Catalonha" ().

The first known human settlements in what is now Catalonia were at the beginning of the Middle Paleolithic. The oldest known trace of human occupation is a mandible found in Banyoles, described by some sources as pre-Neanderthal some 200,000 years old; other sources suggest it to be only about one third that old. From the next prehistoric era, the Epipalaeolithic or Mesolithic, important remains survive, the greater part dated between 8000 and 5000Â BC, such as those of Sant Gregori (Falset) and el Filador (Margalef de Montsant). The most important sites from these eras, all excavated in the region of MoianÃ¨s, are the Balma del Gai (Epipaleolithic) and the Balma de l'Espluga (late Epipaleolithic and Early Neolithic).

The Neolithic era began in Catalonia around 5000 BC, although the population was slower to develop fixed settlements than in other places, thanks to the abundance of woods, which allowed the continuation of a fundamentally hunter-gatherer culture. An example of such settlements would be La Draga, an "early Neolithic village which dates from the end of the 6th millennium BC."

The Chalcolithic period developed in Catalonia between 2500 and 1800 BC, with the beginning of the construction of copper objects. The Bronze Age occurred between 1800 and 700 BC. There are few remnants of this era, but there were some known settlements in the low Segre zone. The Bronze Age coincided with the arrival of the Indo-Europeans through the Urnfield Culture, whose successive waves of migration began around 1200 BC, and they were responsible for the creation of the first proto-urban settlements. Around the middle of the 7th century BC, the Iron Age arrived in Catalonia.

In pre-Roman times, the area that is now called Catalonia in the north-east of Iberian Peninsula â like the rest of the Mediterranean side of the peninsula â was populated by the Iberians. The Iberians of this area â the Ilergetes, Indigetes and Lacetani (Cerretains) â also maintained relations with the peoples of the Mediterranean. Some urban agglomerations became relevant, including Ilerda (Lleida) inland, Hibera (perhaps Amposta or Tortosa) or Indika (Ullastret). Coastal trading colonies were established by the ancient Greeks, who settled around the Gulf of Roses, in Emporion (EmpÃºries) and Roses in the 8th century BC. The Carthaginians briefly ruled the territory in the course of the Second Punic War and traded with the surrounding Iberian population.

After the Carthaginian defeat by the Roman Republic, the north-east of Iberia became the first to come under Roman rule and became part of Hispania, the westernmost part of the Roman Empire. Tarraco (modern Tarragona) was one of the most important Roman cities in Hispania and the capital of the province of Tarraconensis. Other important cities of the Roman period are Ilerda (Lleida), Dertosa (Tortosa), Gerunda (Girona) as well as the ports of EmpuriÃ¦ (former Emporion) and Barcino (Barcelona). As for the rest of Hispania, Latin law was granted to all cities under the reign of Vespasian (69-79 AD), while Roman citizenship was granted to all free men of the empire by the Edict of Caracalla in 212 AD (Tarraco, the capital, was already a colony of Roman law since 45 BC). It was a rich agricultural province (olive oil, vine, wheat), and the first centuries of the Empire saw the construction of roads (the most important being the Via Augusta, parallel to Mediterranean coastline) and infrastructure like aqueducts.

Conversion to Christianity, attested in the 3rd century, was completed in urban areas in the 4th century. Although Hispania remained under Roman rule and did not fall under the rule of Vandals, Swabians and Alans in the 5th century, the main cities suffered frequent sacking and some deurbanization.

After the fall of the Western Roman Empire, the area was conquered by the Visigoths and was ruled as part of the Visigothic Kingdom for almost two and a half centuries. In 718, it came under Muslim control and became part of Al-Andalus, a province of the Umayyad Caliphate. From the conquest of Roussillon in 760, to the conquest of Barcelona in 801, the Frankish empire took control of the area between Septimania and the Llobregat river from the Muslims and created heavily militarised, self-governing counties. These counties formed part of the Gothic and Hispanic marches, a buffer zone in the south of the Frankish empire in the former province of Septimania and in the northeast of the Iberian Peninsula, to act as a defensive barrier for the Frankish empire against further Muslim invasions from Al-Andalus.

These counties came under the rule of the counts of Barcelona, who were Frankish vassals nominated by the emperor of the Franks, to whom they were feudatories (801â987). The earliest known use of the name "Catalonia" for these counties dates to 1117. During the 9th century, the Count Wilfred the Hairy made its title hereditary and founded the dynasty of the House of Barcelona, which ruled Catalonia until 1410.

In 987 Borrell II, Count of Barcelona, did not recognise the new French king Hugh Capet as his king, making his successors (from Ramon Borrell I to Ramon Berenguer IV) de facto independent of the Capetian crown whom they regarded as usurpers of the Carolingian Frankish realm. At the start of eleventh century the Catalan Counties suffer an important process of feudalisation, partially controlled by the Peace and Truce Assemblies and by the power and negotiation skills of the Counts of Barcelona like Ramon Berenguer I. In 1137, Ramon Berenguer IV, Count of Barcelona decided to accept King Ramiro II of Aragon's proposal to marry Queen Petronila, establishing the dynastic union of the County of Barcelona with the Kingdom of Aragon, joining the Crown of Aragon and making the Catalan counties that were united under the county of Barcelona into a principality of the Aragonese Crown.

In 1258, by means of the Treaty of Corbeil, James I of Aragon King of Aragon and Count of Barcelona , of Mallorca and of Valencia, renounced his family rights and dominions in Occitania and recognised the king of France as heir of the Carolingian Dynasty. The king of France formally relinquished his nominal feudal lordship over all the Catalan counties, except the County of Foix, despite the opposition of the king of Aragon and count of Barcelona. This treaty transformed the principality's "de facto" union with Aragon into a "de jure" one and was the origin of the definitive separation between the geographical areas of Catalonia and Languedoc.

As a coastal territory, Catalonia became the base of the Aragonese Crown's maritime forces, which spread the power of the Aragonese Crown in the Mediterranean, and made Barcelona into a powerful and wealthy city. In the period of 1164â1410, new territories, the Kingdom of Valencia, the Kingdom of Majorca, Sardinia, the Kingdom of Sicily, Corsica, and (briefly) the Duchies of Athens and Neopatras, were incorporated into the dynastic domains of the House of Aragon.

At the same time, the Principality of Catalonia developed a complex institutional and political system based in the concept of a pact between the estates of the realm and the king. Laws had to be approved in the General Court of Catalonia, one of the first parliamentary bodies of Europe that banned the royal power to create legislation unilaterally (since 1283). The Courts were composed of the three Estates, were presided over by the king of Aragon, and approved the constitutions, which created a compilation of rights for the citizenship of the Principality. In order to collect general taxes, the Courts of 1359 established a permanent representative of deputies position, called the Deputation of the General (and later usually known as Generalitat), which gained political power over the next centuries.

The domains of the Aragonese Crown were severely affected by the Black Death pandemic and by later outbreaks of the plague. Between 1347 and 1497 Catalonia lost 37 percent of its population. In 1410, King Martin I died without surviving descendants. Under the Compromise of Caspe, Ferdinand from the Castilian House of TrastÃ¡mara received the Crown of Aragon as Ferdinand I of Aragon. During the reign of his son, John II, social and political tensions caused the Catalan Civil War (1462â1472).

Ferdinand II of Aragon, the grandson of Ferdinand I, and Queen Isabella I of Castile were married in 1469, later taking the title the Catholic Monarchs; subsequently, this event was seen by historiographers as the dawn of a unified Spain. At this time, though united by marriage, the Crowns of Castile and Aragon maintained distinct territories, each keeping its own traditional institutions, parliaments, laws and currency. Castile commissioned expeditions to the Americas and benefited from the riches acquired in the Spanish colonisation of the Americas, but, in time, also carried the main burden of military expenses of the united Spanish kingdoms. After Isabella's death, Ferdinand II personally ruled both kingdoms.

By virtue of descent from his maternal grandparents, Ferdinand II of Aragon and Isabella I of Castile, in 1516 Charles I of Spain became the first king to rule the Crowns of Castile and Aragon simultaneously by his own right. Following the death of his paternal (House of Habsburg) grandfather, Maximilian I, Holy Roman Emperor, he was also elected Charles V, Holy Roman Emperor, in 1519.
Over the next few centuries, the Principality of Catalonia was generally on the losing side of a series of wars that led steadily to more centralization of power in Spain. Despite this fact, between the 16th and 18th centuries, the participation of the political community in the local and the general Catalan government was increased, while the kings remained absent and its constitutional system continued to consolidate. The Reapers' War (1640â1652) saw Catalonia rebel (briefly as a republic led by Pau Claris) with French help against the Spanish Crown for overstepping Catalonia's rights during the Thirty Years' War. Most of Catalonia was reconquered by the Spanish monarchy but Catalan rights were recognised. Roussillon was lost to France by the Peace of the Pyrenees (1659).

The most significant conflict concerning the governing monarchy was the War of the Spanish Succession, which began when the childless Charles II of Spain, the last Spanish Habsburg, died without an heir in 1700. Charles II had chosen Philip V of Spain from the French House of Bourbon. Catalonia, like other territories that formed the Crown of Aragon, rose up in support of the Austrian Habsburg pretender Charles VI, Holy Roman Emperor, in his claim for the Spanish throne as Charles III of Spain. The fight between the houses of Bourbon and Habsburg for the Spanish Crown split Spain and Europe.

The fall of Barcelona on 11 September 1714 to the Bourbon king Philip V militarily ended the Habsburg claim to the Spanish Crown, which became legal fact in the Treaty of Utrecht. Philip felt that he had been betrayed by the Catalan Courts, as it had initially sworn its loyalty to him when he had presided over it in 1701. In retaliation for the betrayal, the first Bourbon king introduced the Nueva Planta decrees that incorporated the territories of the Crown of Aragon, including Catalonia, as provinces under the Crown of Castile in 1716, terminating their separate institutions, laws and rights, within a united kingdom of Spain. During the second half of 18th century Catalonia started a successful process of proto-industrialization.

At the beginning of the nineteenth century Catalonia was severely affected by the Napoleonic Wars. In 1808 it was occupied by the French troops, the resistance against the occupation eventually developed into the Peninsular War. The rejection to French dominion was institutionalized with the creation of "juntas" (councils) who, remaining loyal to the Bourbons, exercised the sovereignty and representation of the territory due to the disappearance of the old institutions. Napoleon took direct control of Catalonia to establish order, creating the Government of Catalonia under the rule of Marshall Augereau, and making Catalan briefly an official language again. Between 1812 and 1814 Catalonia was annexed to France and organized as four dÃ©partements. The French troops evacuated Catalan territory at the end of 1814. After the Bourbon restoration in Spain and the death of the abolutist king Ferdinand VII, Carlist Wars erupted against the new born liberal state of Isabella II. Catalonia was divided, the coast and most industrialized areas support liberalism, while many inland areas were in the hands of Carlists, as the last ones proposed to reestablish the intitutional systems suppressed in the Nueva Planta decrees in all the ancient realms of the Crown of Aragon.
In the second third of the 19th century, it became an industrial center. This process was boosted by, amongst other things, national (although the policy of the Spanish government during those times changed many times between free trade and protectionism) and the conditions of proto-industrialization of the prior two centuries of the Catalan urban areas and its countryside. Along the century, textile industry flourished in urban areas and in the countryside, usually in the form of company towns. To this day it remains one of the most industrialised areas of Spain. In 1832 it was inaugurated in Barcelona the factory Bonaplata, the first of the country which worked with steam engine. During those years, Barcelona was the focus of important revolutionary uprisings, called "bullangues", causing a difficult relation between many sectors of Catalan society and the central government and, in Catalonia, a republican current began to develop; also, inevitably, many Catalans favored a more federal Spain. Meanwhile, the Catalan language saw a cultural renaissance (the "RenaixenÃ§a") at popular and bourgeois level. After the fall of the First Spanish Republic and the restoration of the Bourbon dynasty (1874), Catalan nationalism grew in importance.
The Anarchists had been active throughout the early 20th century, founding the CNT trade union and achieving one of the first eight-hour workday in Europe in 1919. Growing resentment of conscription and of the military culminated in the Tragic Week in Barcelona in 1909. In the first third of the 20th century, Catalonia gained and lost varying degrees of autonomy several times. In 1914, the four Catalan provinces were authorized to create a Commonwealth ("Mancomunitat"), without any legislative power or specific autonomy which carried out an ambitious program of modernization, but that was disbanded in 1925 by the dictatorship of Primo de Rivera (1923-1930). During the last steps of the Dictatorship, Barcelona celebrated the 1929 International Exposition, while Spain started to suffer an economical crisis.

After the fall of the dictator and a brief proclamation of the Catalan Republic, it received its first Statute of Autonomy during the Second Spanish Republic (1931â1939), establishing an autonomous body, the Generalitat of Catalonia, which included a parliament, a government and a court of appeal, and the left-wing independentist leader Francesc MaciÃ  was elected its first president. The governments of the Republican Generalitat, led by the Republican Left of Catalonia (ERC) members Francesc MaciÃ  (1931-1933) and LluÃ­s Companys (1933-1940) tried to implement an advanced social program, despite the internal difficulties. This period was marked by political unrest, the effects of the economic crisis and their social repercussions. The Statute was suspended in 1934, due to the Events of 6 October in Barcelona, as a response to the accession of right-wing Spanish nationalist party CEDA to the government of the Republic, considered close to fascism.
After the electoral victory of the Popular Front in February 1936, the Government of Catalonia was pardoned and the self-government restored.

The defeat of the military rebellion against the Republican government in Barcelona placed Catalonia firmly in the Republican side of the Spanish Civil War. During the war, there were two rival powers in Catalonia: the de jure power of the Generalitat and the de facto power of the armed popular militias. Violent confrontations between the workers' parties (CNT-FAI and POUM against the PSUC) culminated in the defeat of the first ones in 1937. The situation resolved itself progressively in favor of the Generalitat, but at the same time the Generalitat was partially losing its autonomous power within Republican Spain. In 1938 Franco's troops broke the Republican territory in two, isolating Catalonia from the rest of the Republic. The defeat of the Republican army in the Battle of the Ebro led in 1938 and 1939 to the occupation of Catalonia by Franco's forces.

The defeat of the Spanish Republic in the Spanish Civil War brought to power the dictatorship of Francisco Franco, whose first ten-year rule was particularly violent, autocratic, and repressive both in a political, cultural, social, and economical sense. In Catalonia, any kind of public activities associated with Catalan nationalism, republicanism, anarchism, socialism, liberalism, democracy or communism, including the publication of books on those subjects or simply discussion of them in open meetings, was banned. Franco's regime banned the use of Catalan in government-run institutions and during public events, and also the Catalan institutions of self-government were abolished. The pro-Republic of Spain president of Catalonia, LluÃ­s Companys, was taken to Spain from his exile in the German-occupied France, and was tortured and executed in the MontjuÃ¯c Castle of Barcelona for the crime of 'military rebellion'.

During later stages of Francoist Spain, certain folkloric and religious celebrations in Catalan resumed and were tolerated. Use of Catalan in the mass media had been forbidden, but was permitted from the early 1950s in the theatre. Despite the ban during the first years and the difficulties of the next period, publishing in Catalan continued throughout his rule.

The years after the war were extremely hard. Catalonia, like many other parts of Spain, had been devastated by the war. Recovery from the war damage was slow and made more difficult by the international trade embargo and the autarkic politics of Franco's regime. By the late 1950s the region had recovered its pre-war economic levels and in the 1960s was the second fastest growing economy in the world in what became known as the Spanish miracle. During this period there was a spectacular growth of industry and tourism in Catalonia that drew large numbers of workers to the region from across Spain and made the area around Barcelona into one of Europe's largest industrial metropolitan areas.

After Franco's death in 1975, Catalonia voted for the adoption of a democratic Spanish Constitution in 1978, in which Catalonia recovered political and cultural autonomy, restoring the Generalitat (exiled since the end of the Civil War in 1939) in 1977 and adopting a new Statute of Autonomy in 1979. First election to the Parliament of Catalonia under this Statute gave the Catalan presidency to Jordi Pujol, a position he would hold until 2003. During this time, he also led ConvergÃ¨ncia i UniÃ³ (CiU), a center-right Catalan nationalist electoral coalition. Throughout the 1980s and 1990s, the institutions of Catalan autonomy continued to develop, among them an autonomous police force ("Mossos d'Esquadra", in 1983), and the broadcasting network TelevisiÃ³ de Catalunya and its first channel TV3, created in 1983. Today, Catalonia is one of the most economically dynamic communities of Spain. The Catalan capital and largest city, Barcelona, is a major international cultural centre and a major tourist destination. In 1992, Barcelona hosted the Summer Olympic Games.

In November 2003, elections to the Parliament of Catalonia gave the government to a left-wing catalanist coalition formed by the Socialists' Party of Catalonia (PSC-PSOE), Republican Left of Catalonia (ERC) and Initiative for Catalonia Greens (ICV), and the socialist Pasqual Maragall was appointed President. The new government redacted a new version of the Statute of Autonomy, which consolidated and extended certain aspects of self-government.

The new Statute of Autonomy of Catalonia, approved after a referendum in 2006, was contested by important sectors of the Spanish society, especially by the conservative People's Party, which sent the law to the Constitutional Court of Spain. In 2010, the Court declared non-valid some of the articles that established an autonomous Catalan system of Justice, better aspects of the financing, a new territorial division, the status of Catalan language or the symbolical declaration of Catalonia as a nation. This decision was severely contested by large sectors of Catalan society, which increased the demands of independence.

A controversial independence referendum was held in Catalonia on 1 October 2017, using a disputed voting process. It was declared illegal and suspended by the Constitutional Court of Spain, because it breached the 1978 Constitution. Subsequent developments saw, on 27 October 2017, a symbolic declaration of independence by the Parliament of Catalonia, the enforcement of direct rule by the Spanish government through the use of Article 155 of the Constitution, the dismissal of the Executive Council and the dissolution of the Parliament, with a snap regional election called for 21 December 2017, which ended with a victory of pro-independence parties. Former President Carles Puigdemont and five former cabinet ministers fled Spain, whereas nine other cabinet members, including vice-president Oriol Junqueras, were jailed under various charges of rebellion, sedition, and misuse of public funds. Quim Torra became the 131st President of the Government of Catalonia on 17 May 2018, after the Spanish courts blocked three other candidates.

In 2018, the Assemblea Nacional Catalana joined the Unrepresented Nations and Peoples Organization (UNPO) on behalf of Catalonia.

On 14 October 2019 the Spanish Supreme court sentenced several Catalan political leaders involved in organizing a referendum on Catalonia's independence from Spain were convicted on charges ranging from sedition to misuse of public funds, with sentences ranging from 9 to 13 years in prison. This decision sparked demonstrations around Catalonia.

The climate of Catalonia is diverse. The populated areas lying by the coast in Tarragona, Barcelona and Girona provinces feature a Hot-summer Mediterranean climate (KÃ¶ppen "Csa"). The inland part (including the Lleida province and the inner part of Barcelona province) show a mostly Mediterranean climate (KÃ¶ppen "Csa"). The Pyrenean peaks have a continental (KÃ¶ppen "D") or even Alpine climate (KÃ¶ppen "ET") at the highest summits, while the valleys have a maritime or oceanic climate sub-type (KÃ¶ppen "Cfb").

In the Mediterranean area, summers are dry and hot with sea breezes, and the maximum temperature is around . Winter is cool or slightly cold depending on the location. It snows frequently in the Pyrenees, and it occasionally snows at lower altitudes, even by the coastline. Spring and autumn are typically the rainiest seasons, except for the Pyrenean valleys, where summer is typically stormy.

The inland part of Catalonia is hotter and drier in summer. Temperature may reach , some days even . Nights are cooler there than at the coast, with the temperature of around . Fog is not uncommon in valleys and plains; it can be especially persistent, with freezing drizzle episodes and subzero temperatures during winter, mainly along the Ebro and Segre valleys and in Plain of Vic.

Catalonia has a marked geographical diversity, considering the relatively small size of its territory. The geography is conditioned by the Mediterranean coast, with of coastline, and large relief units of the Pyrenees to the north. The Catalan territory is divided into three main geomorphological units:

The Catalan Pyrenees represent almost half in length of the Pyrenees, as it extends more than . Traditionally differentiated the Axial Pyrenees (the main part) and the Pre-Pyrenees (southern from the Axial) which are mountainous formations parallel to the main mountain ranges but with lower altitudes, less steep and a different geological formation. The highest mountain of Catalonia, located north of the comarca of Pallars SobirÃ  is the Pica d'Estats (3,143Â m), followed by the PuigpedrÃ³s (2,914Â m). On the Pre-Pyrenees is located the Serra del CadÃ­, that separates the valley of Cerdanya from the Central Depression.

Central Catalan Depression is a plain located between the Pyrenees and Pre-Coastal Mountains. The Depression lands are located between . The plains and the water that descend from the Pyrenees have made it fertile territory for agriculture and there are built numerous irrigation canals. Other important plain is the EmpordÃ , located on the northeast.

The Catalan Mediterranean system is based on two (more or less) parallel ranges to the coast, in a Northwest direction towards the Southwest. These two mountain ranges are the Coastal and the Pre-Coastal. The Coastal Range is minor extent and it has lower altitudes, while the Pre-Coastal is larger in both length and height. The most relevant mountains of this area are Montserrat, Montseny and Ports. Within the ranges are a series of plains, the entities over which form the Coastal and the Pre-Coastal Depressions. The Coastal Depression is located on the East of the Coastal Range towards the coast. The Pre-Coastal, on the other hand, is located in the interior, between the two mountain ranges, and constitutes the basis of the plains of VallÃ¨s and PenedÃ¨s.

Catalonia is a showcase of European landscapes on a small scale. Just over hosting a variety of substrates, soils, climates, directions, altitudes and distances to the sea. The area is of great ecological diversity and a remarkable wealth of landscapes, habitats and species.

The fauna of Catalonia comprises a minority of animals endemic to the region and a majority of non-native animals. Much of Catalonia enjoys a Mediterranean climate (except mountain areas), which makes many of the animals that live there adapted to Mediterranean ecosystems. Of mammals, there are plentiful wild boar, red foxes, as well as roe deer and in the Pyrenees, the Pyrenean chamois. Other large species such as the bear have been recently reintroduced.

Waters of Balearic Sea are rich in biodiversity, and even the megafaunas of ocean; various type of whales (such as fin, sperm, and pilot) and dolphins live within the area.

Most of Catalonia belongs to the Mediterranean Basin. The Catalan hydrographic network consists of two important basins, the one of the Ebro and the one that comprises the internal basins of Catalonia (respectively covering 46.84% and 51.43% of the territory), all of them flow to the Mediterranean. Furthermore, there is the Garona river basin that flows to the Atlantic Ocean, but it only covers 1.73% of the Catalan territory.

The hydrographic network can be divided in two sectors, an occidental slope or Ebro river slope and one oriental slope constituted by minor rivers that flow to the Mediterranean along the Catalan coast. The first slope provides an average of per year, while the second only provides an average of /year. The difference is due to the big contribution of the Ebro river, from which the Segre is an important tributary. Moreover, in Catalonia there is a relative wealth of groundwaters, although there is inequality between "comarques", given the complex geological structure of the territory. In the Pyrenees there are many small lakes, remnants of the ice age. The biggest are the lake of Banyoles and the recently recovered lake of Ivars.

The Catalan coast is almost rectilinear, with a length of and few landformsâthe most relevant are the Cap de Creus and the Gulf of Roses to the north and the Ebro Delta to the south. The Catalan Coastal Range hugs the coastline, and it is split into two segments, one between L'Estartit and the town of Blanes (the Costa Brava), and the other at the south, at the Costes del Garraf.

The principal rivers in Catalonia are the Ter, Llobregat, and the Ebro (Catalan: ), all of which run into the Mediterranean.

The majority of Catalan population is concentrated in 30% of the territory, mainly in the coastal plains. Intensive agriculture, livestock farming and industrial activities have been accompanied by a massive tourist influx (more than 20 million annual visitors), a rate of urbanization and even of major metropolisation which has led to a strong urban sprawl: a third of Catalans live in the urban area of Barcelona, while the proportion of urbanized soils increased from 4.2% in 1993 to 6.2% in 2009, a growth of 48.6% in sixteen years, complemented with a dense network of transport infrastructure. This is accompanied by a certain agricultural abandonment (decrease of 15% of all areas cultivated in Catalonia between 1993 and 2009) and a global threat to natural environment. Human activities have also put some animal species at risk, or even led to their disappearance from the territory, like the gray wolf and probably the brown bear of the Pyrenees. The pressure created by this model of life means that the country's ecological footprint exceeds its administrative area.

Faced with this problems, Catalan authorities initiated several measures whose purpose is to protect natural ecosystems. Thus, in 1990, the Catalan government created the Nature Conservation Council (Catalan: ), an advisory body with the aim to study, protect and manage the natural environments and landscapes of Catalonia. In addition, the Generalitat has carried out the Plan of Spaces of Natural Interest ( or PEIN) in 1992 while eighteen Natural Spaces of Special Protection ( or ENPE) have been instituted.

There's a National Park, AigÃ¼estortes i Estany de Sant Maurici; fourteen Natural Parks, Alt Pirineu, Aiguamolls de l'EmpordÃ , CadÃ­-MoixerÃ³, Cap de Creus, Sources of Ter and Freser, Collserola, Ebro Delta, Ports, MontgrÃ­, Medes Islands and Baix Ter, Montseny, Montserrat, Sant LlorenÃ§ del Munt and l'Obac, Serra de Montsant and the Garrotxa Volcanic Zone; as well as three Natural Places of National Interest ( or PNIN), the Pedraforca, the Poblet Forest and the AlbÃ¨res.

After Franco's death in 1975 and the adoption of a democratic constitution in Spain in 1978, Catalonia recovered and extended the powers that it had gained in the Statute of Autonomy of 1932 but lost with the fall of the Second Spanish Republic at the end of the Spanish Civil War in 1939.

This autonomous community has gradually achieved more autonomy since the approval of the Spanish Constitution of 1978. The Generalitat holds exclusive jurisdiction in education, health, culture, environment, communications, transportation, commerce, public safety and local government, and only shares jurisdiction with the Spanish government in justice. In all, some analysts argue that formally the current system grants Catalonia with "more self-government than almost any other corner in Europe".

The support for Catalan nationalism ranges from a demand for further autonomy and the federalisation of Spain to the desire for independence from the rest of Spain, expressed by Catalan independentists. The first survey following the Constitutional Court ruling that cut back elements of the 2006 Statute of Autonomy, published by "La Vanguardia" on 18 July 2010, found that 46% of the voters would support independence in a referendum. In February of the same year, a poll by the Open University of Catalonia gave more or less the same results. Other polls have shown lower support for independence, ranging from 40 to 49%. Although it is established in the whole of the territory, support for independence is significantly higher in the hinterland and the northeast, away from the more populated coastal areas such as Barcelona.

Since 2011 when the question started to be regularly surveyed by the governmental Center for Public Opinion Studies (CEO), support for Catalan independence has been on the rise. According to the CEO opinion poll from July 2016, 47.7% of Catalans would vote for independence and 42.4% against it while, about the question of preferences, according to the CEO opinion poll from March 2016, a 57.2 claim to be "absolutely" or "fairly" in favour of independence. Other polls have shown lower support for independence, ranging from 40 to 49%. Other polls show more variable results, according with the Spanish CIS, as of December 2016, 47% of Catalans rejected independence and 45% supported it.

In hundreds of non-binding local referendums on independence, organised across Catalonia from 13 September 2009, a large majority voted for independence, although critics argued that the polls were mostly held in pro-independence areas. In December 2009, 94% of those voting backed independence from Spain, on a turn-out of 25%. The final local referendum was held in Barcelona, in April 2011. On 11 September 2012, a pro-independence march pulled in a crowd of between 600,000 (according to the Spanish Government), 1.5 million (according to the GuÃ rdia Urbana de Barcelona), and 2 million (according to its promoters); whereas poll results revealed that half the population of Catalonia supported secession from Spain.

Two major factors were Spain's Constitutional Court's 2010 decision to declare part of the 2006 Statute of Autonomy of Catalonia unconstitutional, as well as the fact that Catalonia contributes 19.49% of the central government's tax revenue, but only receives 14.03% of central government's spending.

Parties that consider themselves either Catalan nationalist or independentist have been present in all Catalan governments since 1980. The largest Catalan nationalist party, Convergence and Union, ruled Catalonia from 1980 to 2003, and returned to power in the 2010 election. Between 2003 and 2010, a leftist coalition, composed by the Catalan Socialists' Party, the pro-independence Republican Left of Catalonia and the leftist-environmentalist Initiative for Catalonia-Greens, implemented policies that widened Catalan autonomy.

In the 25 November 2012 Catalan parliamentary election, sovereigntist parties supporting a secession referendum gathered 59.01% of the votes and held 87 of the 135 seats in the Catalan Parliament. Parties supporting independence from the rest of Spain obtained 49.12% of the votes and a majority of 74 seats.

Artur Mas, then the president of Catalonia, organised early elections that took place on 27 September 2015. In these elections, ConvergÃ¨ncia and Esquerra Republicana decided to join, and they presented themselves under the coalition named "Junts pel SÃ­" (in Catalan, "Together for Yes"). "Junts pel SÃ­" won 62 seats and was the most voted party, and CUP (Candidatura d'Unitat Popular, a far-left and independentist party) won another 10, so the sum of all the independentist forces/parties was 72 seats, reaching an absolute majority, but not in number of individual votes, comprising 47,74% of the total.

The Statute of Autonomy of Catalonia is the fundamental organic law, second only to the Spanish Constitution from which the Statute originates.

In the Spanish Constitution of 1978 Catalonia, along with the Basque Country and Galicia, was defined as a "nationality". The same constitution gave Catalonia the automatic right to autonomy, which resulted in the Statute of Autonomy of Catalonia of 1979.

Both the 1979 Statute of Autonomy and the current one, approved in 2006, state that "Catalonia, as a nationality, exercises its self-government constituted as an Autonomous Community in accordance with the Constitution and with the Statute of Autonomy of Catalonia, which is its basic institutional law, always under the law in Spain".

The Preamble of the 2006 Statute of Autonomy of Catalonia states that the Parliament of Catalonia has defined Catalonia as a nation, but that "the Spanish Constitution recognizes Catalonia's national reality as a nationality". While the Statute was approved by and sanctioned by both the Catalan and Spanish parliaments, and later by referendum in Catalonia, it has been subject to a legal challenge by the surrounding autonomous communities of Aragon, Balearic Islands and Valencia, as well as by the conservative People's Party. The objections are based on various issues such as disputed cultural heritage but, especially, on the Statute's alleged breaches of the principle of "solidarity between regions" in fiscal and educational matters enshrined by the Constitution.

Spain's Constitutional Court assessed the disputed articles and on 28 June 2010, issued its judgment on the principal allegation of unconstitutionality presented by the People's Party in 2006. The judgment granted clear passage to 182 articles of the 223 that make up the fundamental text. The court approved 73 of the 114 articles that the People's Party had contested, while declaring 14 articles unconstitutional in whole or in part and imposing a restrictive interpretation on 27 others. The court accepted the specific provision that described Catalonia as a "nation", however ruled that it was a historical and cultural term with no legal weight, and that Spain remained the only nation recognised by the constitution.

The Catalan Statute of Autonomy establishes that Catalonia is organised politically through the Generalitat of Catalonia (in Catalan: ), conformed by the Parliament, the Presidency of the Generalitat, the Government or Executive Council and the other institutions created by the Parliament, among them the Ombudsman (), the Office of Auditors () or the Council for Statutory Guarantees ()

The Parliament of Catalonia (in Catalan: ) is the legislative body of the Generalitat and represents the citizens of Catalonia. It is elected every four years by universal suffrage, and it has powers to legislate in different matters such as education, health, culture, internal institutional and territorial organization, election and control of the president of the Generalitat and the Government, budget and other affairs, according with the Statute of Autonomy. The last Catalan election was held on 21 December 2017, and its current president is Roger Torrent, incumbent since January 2018.

The president of the Generalitat of Catalonia (in Catalan: ) is the highest representative of Catalonia, and is also responsible of leading the government's action. Since the restoration of the Generalitat on the return of democracy in Spain, the presidents of Catalonia have been Josep Tarradellas (1977â1980, president in exile since 1954), Jordi Pujol (1980â2003), Pasqual Maragall (2003â2006), JosÃ© Montilla (2006â2010), Artur Mas (2010â2016), Carles Puigdemont (2016â2017) and, after the imposition of direct rule from Madrid, Quim Torra (2018â).

The Executive Council (in Catalan: ) or Government (), is the body responsible of the government of the Generalitat, it holds executive and regulatory power. It comprises the president of the Generalitat, the First Minister (or the Vice President) and the Ministers (). Its seat is the Palau de la Generalitat, in Barcelona.

Catalonia has its own police force, the (officially called ), whose origins date back to the 18th century. Since 1980 they have been under the command of the Generalitat, and since 1994 they have expanded in number in order to replace the national Civil Guard and National Police Corps, which report directly to the Homeland Department of Spain. The national bodies retain personnel within Catalonia to exercise functions of national scope such as overseeing ports, airports, coasts, international borders, custom offices, the identification of documents and arms control, immigration control, terrorism prevention, arms trafficking prevention, amongst others.

Most of the justice system is administered by national judicial institutions, the highest body and last judicial instance in the Catalan jurisdiction, integrating the Spanish judiciary, is the High Court of Justice of Catalonia. The criminal justice system is uniform throughout Spain, while civil law is administered separately within Catalonia. The civil laws that are subject to autonomous legislation have been codified in the Civil Code of Catalonia () since 2002.

Navarre, the Basque Country and Catalonia are the Spanish communities with the highest degree of autonomy in terms of law enforcement.

Catalonia is organised territorially into provinces, further subdivided into comarques and municipalities. The 2006 Statute of Autonomy of Catalonia establishes the administrative organisation of three local authorities: vegueries, comarques, and municipalities.

Catalonia is divided administratively into four provinces, the governing body of which is the Provincial Deputation (, ). The four provinces and their populations are:


Comarques (singular: "comarca") are entities composed by the municipalities to manage their responsibilities and services. The current regional division has its roots in a decree of the Generalitat de Catalunya of 1936, in effect until 1939, when it was suppressed by Franco. In 1987 the Government adopted the territorial division again and in 1988 three new comarques were added (Alta RibagorÃ§a, Pla d'Urgell and Pla de l'Estany), and in 2015 was created another comarca, the MoianÃ¨s. At present there are 41. Every comarca is administered by a comarcal council ().

The Val d'Aran (Aran Valley), until 2015 considered as a comarca, is officially defined today as "unique territorial entity", has a special status and its autonomous government is named .

There are at present 948 municipalities () in Catalonia. Each municipality is run by a council () elected every four years by the residents in local elections. The council consists of a number of members () depending on population, who elect the mayor ( or ). Its seat is the town hall (, or ).

The "vegueria" is a new type of division defined as a specific territorial area for the exercise of government and inter-local cooperation with legal personality. The current Statute of Autonomy states vegueries are intended to supersede provinces in Catalonia, and take over many of functions of the comarques.

The territorial plan of Catalonia () provided six general functional areas, but was amended by Law 24/2001, of 31 December, recognizing the Alt Pirineu i Aran as a new functional area differentiated of Ponent. On 14 July 2010 the Catalan Parliament approved the creation of the functional area of the PenedÃ¨s.


A highly industrialized land, the nominal GDP of Catalonia in 2014 was â¬200 billion (usually the highest in Spain) and the per capita GDP was â¬27,000 ($30,000), behind Madrid (â¬31,000), the Basque Country (â¬30,000), and Navarre (â¬28,000). In that year, the GDP growth was 1.4%. In recent years there has been a negative net relocation rate of companies based in Catalonia moving to other autonomous communities of Spain. In 2014, for example, Catalonia lost 987 companies to other parts of Spain (mainly Madrid), gaining 602 new ones from the rest of the country.

Catalonia's long-term credit rating is BB (Non-Investment Grade) according to Standard & Poor's, Ba2 (Non-Investment Grade) according to Moody's, and BBB- (Low Investment Grade) according to Fitch Ratings. Catalonia's rating is tied for worst with between 1 and 5 other autonomous communities of Spain, depending on the rating agency.

In the context of the financial crisis of 2007â2008, Catalonia was expected to suffer a recession amounting to almost a 2% contraction of its regional GDP in 2009. Catalonia's debt in 2012 was the highest of all Spain's autonomous communities, reaching â¬13,476 million, i.e. 38% of the total debt of the 17 autonomous communities, but in recent years its economy recovered a positive evolution and the GDP grew a 3.3% in 2015.

Catalonia is amongst the List of country subdivisions by GDP over 100 billion US dollars and is a member of the Four Motors for Europe organisation.

The distribution of sectors is as follows:


The main tourist destinations in Catalonia are the city of Barcelona, the beaches of the Costa Brava in Girona, the beaches of the Costa del Maresme and Costa del Garraf from Malgrat de Mar to Vilanova i la GeltrÃº and the Costa Daurada in Tarragona. In the High Pyrenees there are several ski resorts, near Lleida. On 1 November 2012, Catalonia started charging a tourist tax. The revenue is used to promote tourism, and to maintain and upgrade tourism-related infrastructure.
Many savings banks are based in Catalonia, with 10 of the 46 Spanish savings banks having headquarters in the region. This list includes Europe's premier savings bank, La Caixa. The first private bank in Catalonia is Banc Sabadell, ranked fourth among all Spanish private banks.

The stock market of Barcelona, which in 2016 had a volume of around â¬152 billion, is the second largest of Spain after Madrid, and Fira de Barcelona organizes international exhibitions and congresses to do with different sectors of the economy.

The main economic cost for the Catalan families is the purchase of a home. According to data from the Society of Appraisal on 31 December 2005 Catalonia is, after Madrid, the second most expensive region in Spain for housing: 3,397Â â¬/m on average (see Spanish property bubble).

The unemployment rate stood at 11.5% in 2018 and was lower than the national average.

Airports in Catalonia are owned and operated by Aena (a Spanish Government entity) except two airports in Lleida which are operated by Aeroports de Catalunya (an entity belonging to the Government of Catalonia).


Since the Middle Ages, Catalonia has been well integrated into international maritime networks. The port of Barcelona (owned and operated by , a Spanish Government entity) is an industrial, commercial and tourist port of worldwide importance. With 1,950,000 TEUs in 2015, it is the first container port in Catalonia, the third in Spain after Valencia and Algeciras in Andalusia, the 9th in the Mediterranean Sea, the 14th in Europe and the 68th in the world. It is sixth largest cruise port in the world, the first in Europe and the Mediterranean with 2,364,292 passengers in 2014. The ports of Tarragona (owned and operated by Puertos del Estado) in the southwest and PalamÃ³s near Girona at northeast are much more modest. The port of PalamÃ³s and the other ports in Catalonia (26) are operated and administered by , a Catalan Government entity.

The development of these infrastructures, resulting from the topography and history of the Catalan territory, responds strongly to the administrative and political organization of this autonomous community.

There are of roads throughout Catalonia.

The principal highways are Â AP-7Â  () and Â A-7Â  (). They follow the coast from the French border to Valencia, Murcia and Andalusia. The main roads generally radiate from Barcelona. The Â AP-2Â  () and Â A-2Â  () connect inland and onward to Madrid.

Other major roads are:

Public-own roads in Catalonia are either managed by the autonomous government of Catalonia (e.g., Â C-Â  roads) or the Spanish government (e.g., Â AP-Â , Â A-Â , Â N-Â  roads).

Catalonia saw the first railway construction in the Iberian Peninsula in 1848, linking Barcelona with MatarÃ³. Given the topography most lines radiate from Barcelona. The city has both suburban and inter-city services. The main east coast line runs through the province connecting with the SNCF (French Railways) at Portbou on the coast.

There are two publicly owned railway companies operating in Catalonia: the Catalan FGC that operates commuter and regional services, and the Spanish national RENFE that operates long-distance and high-speed rail services (AVE and Avant) and the main commuter and regional service , administered by the Catalan government since 2010.

High-speed rail (AVE) services from Madrid currently reach Lleida, Tarragona and Barcelona. The official opening between Barcelona and Madrid took place 20 February 2008. The journey between Barcelona and Madrid now takes about two-and-a-half hours. A connection to the French high-speed TGV network has been completed (called the PerpignanâBarcelona high-speed rail line) and the Spanish AVE service began commercial services on the line 9 January 2013, later offering services to Marseille on their high speed network. This was shortly followed by the commencement of commercial service by the French TGV on 17 January 2013, leading to an average travel time on the Paris-Barcelona TGV route of 7h 42m. This new line passes through Girona and Figueres with a tunnel through the Pyrenees.

As of 2017, the official population of Catalonia was 7,522,596. 1,194,947 residents did not have Spanish citizenship, accounting for about 16% of the population.

The Urban Region of Barcelona includes 5,217,864 people and covers an area of . The metropolitan area of the Urban Region includes cities such as L'Hospitalet de Llobregat, Sabadell, Terrassa, Badalona, Santa Coloma de Gramenet and CornellÃ  de Llobregat.

In 1900, the population of Catalonia was 1,966,382 people and in 1970 it was 5,122,567. The sizeable increase of the population was due to the demographic boom in Spain during the 60s and early 70s as well as in consequence of large-scale internal migration from the rural economically weak regions to its more prospering industrial cities. In Catalonia that wave of internal migration arrived from several regions of Spain, especially from Andalusia, Murcia and Extremadura.

Immigrants from other countries settled in Catalonia since the 1990s; a large percentage comes from Africa, Latin America and Eastern Europe, and smaller numbers from Asia and Southern Europe, often settling in urban centers such as Barcelona and industrial areas. In 2017, Catalonia had 1,194,497 foreign residents (15.9% of the total population) with non-Spanish ID cards, without including those who acquired the Spanish citizenship.

Historically, all the Catalan population was Christian, specifically Catholic, but since the 1980s there has been a trend of decline of Christianity and parallel growth of irreligion (including stances of atheism and agnosticism) and other religions. According to the most recent study sponsored by the government of Catalonia, as of 2016, 61.9% of the Catalans identify as Christians, up from 56.5% in 2014, of whom 58.0% Catholics, 3.0% Protestants and Evangelicals, 0.9% Orthodox Christians and 0.6% Jehovah's Witnesses. At the same time, 16.0% of the population identify as atheists, 11.9% as agnostics, 4.8% as Muslims, 1.3% as Buddhists, and a further 2.4% as being of other religions.

According to the linguistic census held by the Government of Catalonia in 2013, Spanish is the most spoken language in Catalonia (46.53% claim Spanish as "their own language"), followed by Catalan (37.26% claim Catalan as "their own language"). In everyday use, 11.95% of the population claim to use both languages equally, whereas 45.92% mainly use Spanish and 35.54% mainly use Catalan. There is a significant difference between the Barcelona metropolitan area (and, to a lesser extent, the Tarragona area), where Spanish is more spoken than Catalan, and the more rural and small town areas, where Catalan clearly prevails over Spanish.

Originating in the historic territory of Catalonia, Catalan has enjoyed special status since the approval of the Statute of Autonomy of 1979 which declares it to be "Catalonia's own language", a term which signifies a language given special legal status within a Spanish territory, or which is historically spoken within a given region. The other languages with official status are Spanish, which has official status throughout Spain, and Aranese Occitan, which enjoys co-official status with Catalan and Spanish in the Val d'Aran.

Since the Statute of Autonomy of 1979, Aranese (a dialect of Gascon Occitan) has also been official and subject to special protection in Val d'Aran. This small area of 7,000 inhabitants was the only place where a dialect of Occitan has received full official status. Then, on 9 August 2006, when the new Statute came into force, Occitan became official throughout Catalonia. Occitan is the mother tongue of 22.4% of the population of Val d'Aran. Catalan Sign Language is also officially recognised.

Although not considered an "official language" in the same way as Catalan, Spanish, and Aranese, Catalan Sign Language, with about 18,000 users in Catalonia, is granted official recognition and support: "The public authorities shall guarantee the use of Catalan sign language and conditions of equality for deaf people who choose to use this language, which shall be the subject of education, protection and respect."

Under Francoist Spain, Catalan was excluded from the public education system and all other official use, so that for example families were not allowed to officially register children with Catalan names. Although never completely banned, Catalan language publishing was severely restricted during the early 1940s, with only religious texts and small-run self-published texts being released. Some books were published clandestinely or circumvented the restrictions by showing publishing dates prior to 1936. This policy was changed in 1946, when restricted publishing in Catalan resumed.

Ruralâurban migration originating in other parts of Spain also reduced the social use of Catalan in urban areas and increased the use of Spanish. Lately, a similar sociolinguistic phenomenon has occurred with foreign immigration. Catalan cultural activity increased in the 1960s and Catalan classes began thanks to the initiative of associations such as Ãmnium Cultural.

After the end of Francoist Spain, the newly established self-governing democratic institutions in Catalonia embarked on a long-term language policy to recover the use of Catalan and has, since 1983, enforced laws which attempt to protect and extend the use of Catalan. This policy, known as the "linguistic normalisation" ( in Catalan, in Spanish) has been supported by the vast majority of Catalan political parties through the last thirty years. Some groups consider these efforts a way to discourage the use of Spanish, whereas some others, including the Catalan government and the European Union consider the policies respectful, or even as an example which "should be disseminated throughout the Union".

Today, Catalan is the main language of the Catalan autonomous government and the other public institutions that fall under its jurisdiction. Basic public education is given basically in Catalan, but also there are some hours per week of Spanish medium instruction. Businesses are required to display all information (e.g. menus, posters) at least in Catalan, under penalty of fines. There is no obligation to display this information in either Occitan or Spanish, although there is no restriction on doing so in these or other languages. The use of fines was introduced in a 1997 linguistic law that aims to increase the public use of Catalan and defend the rights of Catalan speakers. In the other hand, the constitution of Spain obligates every citizen to know Spanish.

The law ensures that both Catalan and Spanish â being official languages â can be used by the citizens without prejudice in all public and private activities. The Generalitat uses Catalan in its communications and notifications addressed to the general population, but citizens can also receive information from the Generalitat in Spanish if they so desire. Debates in the Catalan Parliament take place almost exclusively in Catalan and the Catalan public television broadcasts programs basically in Catalan.

Due to the intense immigration which Spain in general and Catalonia in particular experienced in the first decade of the 21st century, many foreign languages are spoken in various cultural communities in Catalonia, of which Rif-Berber, Moroccan Arabic, Romanian and Urdu are the most common ones.

In Catalonia, there is a high social and political consensus on the language policies favoring Catalan, also among Spanish speakers and speakers of other languages. However, some of these policies have been criticised for trying to promote Catalan by imposing fines on businesses. For example, following the passage of the law on Catalan cinema in March 2010, which established that half of the movies shown in Catalan cinemas had to be in Catalan, a general strike of 75% of the cinemas took place. The Catalan government gave in and dropped the clause that forced 50% of the movies to be dubbed or subtitled in Catalan before the law came to effect. On the other hand, organisations such as Plataforma per la Llengua reported different violations of the linguistic rights of the Catalan speakers in Catalonia and the other Catalan-speaking territories in Spain, most of them caused by the institutions of the Spanish government in these territories.

The Catalan language policy has been challenged by some political parties in the Catalan Parliament. Citizens, currently the main opposition party, has been one of the most consistent critics of the Catalan language policy within Catalonia. The Catalan branch of the People's Party has a more ambiguous position on the issue: on one hand, it demands a bilingual CatalanâSpanish education and a more balanced language policy that would defend Catalan without favoring it over Spanish, whereas on the other hand, a few local PP politicians have supported in their municipalities measures privileging Catalan over Spanish and it has defended some aspects of the official language policies, sometimes against the positions of its colleagues from other parts of Spain.

Catalonia has given to the world many important figures in the area of the art. Catalan painters internationally known are, among others, Salvador DalÃ­, Joan MirÃ³ and Antoni TÃ pies. Closely linked with the Catalan pictorial atmosphere, Pablo Picasso lived in Barcelona during his youth, training them as an artist and creating the movement of cubism. Other important artists are Claudi Lorenzale for the medieval Romanticism that marked the artistic RenaixenÃ§a, MariÃ  Fortuny for the Romanticism and Catalan Orientalism of the nineteenth century, Ramon Casas or Santiago RusiÃ±ol, main representatives of the pictorial current of Catalan modernism from the end of the nineteenth century to the beginning of the twentieth century, Josep Maria Sert for early 20th-century Noucentisme, or Josep Maria Subirachs for expressionist or abstract sculpture and painting of the late twentieth century.
The most important painting museums of Catalonia are the Teatre-Museu DalÃ­ in Figueres, the National Art Museum of Catalonia (MNAC), Picasso Museum, FundaciÃ³ Antoni TÃ pies, Joan MirÃ³ Foundation, the Barcelona Museum of Contemporary Art (MACBA), the Centre of Contemporary Culture of Barcelona (CCCB) and the CaixaForum.

In the field of architecture were developed and adapted to Catalonia different artistic styles prevalent in Europe, leaving footprints in many churches, monasteries and cathedrals, of Romanesque (the best examples of which are located in the northern half of the territory) and Gothic styles. The Gothic developed in Barcelona and its area of influence is known as Catalan Gothic, with some particular characteristics. The church of Santa Maria del Mar is an example of this kind of style. During the Middle Ages, many fortified castles were built by feudal nobles to mark their powers.

There are some examples of Renaissance (such as the Palau de la Generalitat), Baroque and Neoclassical architectures. In the late nineteenth century Modernism (Art Nouveau) appeared as the national art. The world-renowned Catalan architects of this style are Antoni GaudÃ­, LluÃ­s DomÃ¨nech i Montaner and Josep Puig i Cadafalch. Thanks to the urban expansion of Barcelona during the last decades of the century and the first ones of the next, many buildings of the Eixample are modernists. In the field of architectural rationalism, which turned especially relevant in Catalonia during the Republican era (1931-1939) highlighting Josep LluÃ­s Sert and Josep Torres i ClavÃ©, members of the GATCPAC and, in contemporany architecture, Ricardo Bofill and Enric Miralles.

There are several UNESCO World Heritage Sites in Catalonia:


The oldest surviving literary use of the Catalan language is considered to be the religious text known as Homilies d'OrganyÃ , written either in late 11th or early 12th century.

There are two historical moments of splendor of Catalan literature. The first begins with the historiographic chronicles of the 13th century (chronicles written between the thirteenth and fourteenth centuries narrating the deeds of the monarchs and leading figures of the Crown of Aragon) and the subsequent Golden Age of the 14th and 15th centuries. After that period, between the 16th and 19th centuries the Romantic historiography defined this era as the , considered as the "decadent" period in Catalan literature because of a general falling into disuse of the vernacular language in cultural contexts and lack of patronage among the nobility.
The second moment of splendor began in the 19th century with the cultural and political (Renaissance) represented by writers and poets such as Jacint Verdaguer, VÃ­ctor CatalÃ  (pseudonym of Caterina Albert i ParadÃ­s), NarcÃ­s Oller, Joan Maragall and Ãngel GuimerÃ . During the 20th century, avant-garde movements developed, initiated by the Generation of '14 (called Noucentisme in Catalonia), represented by Eugeni d'Ors, Joan Salvat-Papasseit, Josep Carner, Carles Riba, J.V. Foix and others. During the dictatorship of Primo de Rivera, the Civil War (Generation of '36) and the Francoist period, Catalan literature was maintained despite the repression against the Catalan language, being often produced in exile. The most outstanding authors of this period are Salvador Espriu, Josep Pla, Josep Maria de Sagarra (the latter three being considered as the main responsible of the renewal of Catalan prose), MercÃ¨ Rodoreda, Joan Oliver SallarÃ¨s or "Pere Quart", Pere Calders, Gabriel Ferrater, Manuel de Pedrolo, AgustÃ­ Bartra or Miquel MartÃ­ i Pol. In addition, several foreign writers who fought in the framework of the International Brigades then recount their experiences of fighting in their works, historical or fictional, with for example "Homage to Catalonia" of the British George Orwell in 1938 or in 1962 and "The Georgics" in 1981 by Frenchman Claude Simon.

After the transition to democracy (1975â1978) and the restoration of the Generalitat (1977), literary life and the editorial market have returned to normality and literary production in Catalan is being bolstered with a number of language policies intended to protect Catalan culture. Besides the aforementioned authors, other relevant 20th-century writers of the Francoist and democracy periods include Joan Brossa, AgustÃ­ Bartra, Manuel de Pedrolo, Pere Calders or Quim MonzÃ³.

Ana MarÃ­a Matute, Jaime Gil de Biedma, Manuel VÃ¡zquez MontalbÃ¡n and Juan Goytisolo are among the most prominent Catalan writers in the Spanish language since the democratic restoration in Spain.

Castells are one of the main manifestations of Catalan popular culture. The activity consists in constructing human towers by competing (teams). This practice originated in Valls, on the region of the Camp de Tarragona, during the 18th century, and later it was extended along the next two centuries to the rest of the territory. The tradition of els Castells i els Castellers was declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO in 2010.

In main celebrations, other elements of the Catalan popular culture are also usually present: parades with (giants), bigheads, stick-dancers and musicians, and the , where devils and monsters dance and spray showers of sparks using firecrackers. Another traditional celebration in Catalonia is , declared a Masterpiece of the Oral and Intangible Heritage of Humanity by the UNESCO on 25 November 2005.
Christmas in Catalonia lasts two days, plus Christmas Eve. On the 25th, Christmas is celebrated, followed by a similar feast on the 26, called Sant Esteve (Saint Steve's Day). This allows families to visit and dine with different sectors of the extended family, or get together with friends on the second day.

One of the most deeply-rooted and curious Christmas traditions is the popular figure of the , consisting of an (often hollow) log with a face painted on it and often two little front legs appended, usually wearing a Catalan hat and scarf. Note that the word has nothing to do with the Spanish word "tÃ­o", meaning uncle. "TiÃ³" means log in Catalan. The log is sometimes "found in the woods" (in an event staged for children) and then adopted and taken home, where it is fed and cared for during a month or so. On Christmas Day or on Christmas Eve, a game is played where children march around the house singing a song requesting the log to poop, then they tap the log gently with a stick, as if a magic wand, to make it poop, and lo and behold, as if through magic, it poops candy, and sometimes other small gifts. Usually the larger or main gifts are brought by the Three Kings on 6 January, and the tiÃ³ only brings small things. 
Another custom is to make a (nativity scene) in the home or in shop windows, the latter sometimes competing in originality or shear size and detail. Churches often host exhibits of numerous dioramas by nativity scene makers, or a single nativity scene they put out, and town halls generally put out a nativity scene in the central square. In Barcelona, every year, the main nativity scene is designed by different artists, and often ends up being an interesting, post-modern or conceptual and strange creation. In the home, the nativity scene often consists of strips of cork bark to represent cliffs or mountains in the background, moss as grass in the foreground, some wood chips or other as dirt, and aluminum foil for rivers and lakes. The traditional figurines often included are the three wise men on camels or horses, which are moved every day or so to go closer to the manger, a star with a long tail in the background to lead people to the spot, the annunciation with shepherds having a meal and an angel appearing (hanging from something), a washer lady washing clothes in the pond, sheep, ducks, people carrying packages on their backs, a donkey driver with a load of twigs, and atrezzo such as a starry sky, miniature towns placed in the distance, either Oriental-styled or local-looking, a bridge over the river, trees, etc.

One of the most astonishing and sui-generis figurines traditionally placed in the nativity scene, to the great glee of children, is the , a person depicted in the act of defecating. This figurine is hidden in some corner of the nativity scene and the game is to detect it. Of course, churches forgo this figurine, and the main nativity scene of Barcelona, for instance, likewise does not feature it. The caganer is so popular it has, together with the tiÃ³, long been a major part of the Christmas markets, where they come in the guise of your favorite politicians or other famous people, as well as the traditional figures of a Catalan farmer. People often buy a figurine of a caganer in the guise of a famous person they are actually fond of, contrary to what one would imagine, though sometimes people buy a caganer in the guise of someone they dislike, although this means they have to look at them in the home...

Another (extended) Christmas tradition is the celebration of the Epiphany on 6 January, which is called "Reis", meaning Three Kings Day. This is every important in Catalonia and the Catalan-speaking areas, and families go to watch major parades on the eve of the Epiphany, where they can greet the kings and watch them pass by in pomp and circumstance, on floats and preceded and followed by pages, musicians, dancers, etc. They often give the kings letters with their gift requests, which are collected by the pages. On the next day, the children find the gifts the three kings brought for them.

In addition to traditional local Catalan culture, traditions from other parts of Spain can be found as a result of migration from other regions, for instance the celebration of the Andalusian in Catalonia.

On 28 July 2010, second only after the Canary Islands, Catalonia became another Spanish territory to forbid bullfighting. The ban, which went into effect on 1 January 2012, had originated in a popular petition supported by over 180,000 signatures.

The sardana is considered to be the most characteristic Catalan folk dance, interpreted to the rhythm of tamborÃ­, tible and tenora (from the oboe family), trumpet, trombÃ³ (trombone), fiscorn (family of bugles) and contrabaix with three strings played by a cobla, and are danced in a circle dance. Other tunes and dances of the traditional music are the contrapÃ s (obsolete today), ball de bastons (the "dance of sticks"), the moixiganga, the goigs (popular songs), the galops or the jota in the southern part. The havaneres are characteristic in some marine localities of the Costa Brava, especially during the summer months when these songs are sung outdoors accompanied by a of burned rum.

Art music was first developed, up to the nineteenth century and, as in much of Europe, in a liturgical setting, particularly marked by the Escolania de Montserrat. The main Western musical trends have marked these productions, medieval monodies or polyphonies, with the work of Abbot Oliba in the eleventh century or the compilation Llibre Vermell de Montserrat ("Red Book of Montserrat") from the fourteenth century. Through the Renaissance there were authors such as Pere Albert Vila, Joan Brudieu or the two Mateu Fletxa ("The Old" and "The Young"). Baroque had composers like Joan Cererols. The Romantic music was represented by composers such as Fernando Sor, Josep Anselm ClavÃ© (father of choir movement in Catalonia and responsible of the music folk reviving) or Felip Pedrell.

Modernisme also expressed in musical terms from the end of the 19th century onwards, mixing folkloric and post-romantic influences, through the works of Isaac AlbÃ©niz and Enric Granados. The avant-garde spirit initiated by the modernists is prolonged throughout the twentieth century, thanks to the activities of the OrfeÃ³ CatalÃ , a choral society founded in 1891, with its monumental concert hall, the Palau de la MÃºsica Catalana in Catalan, built by LluÃ­s DomÃ¨nech i Montaner from 1905 to 1908, the Barcelona Symphony Orchestra created in 1944 and composers, conductors and musicians engaged against the Francoism like Robert Gerhard, Eduard ToldrÃ  and Pau Casals.

Performances of opera, mostly imported from Italy, began in the 18th century, but some native operas were written as well, including the ones by DomÃ¨nec Terradellas, Carles Baguer, Ramon Carles, Isaac AlbÃ©niz and Enric Granados. The Barcelona main opera house, Gran Teatre del Liceu (opened in 1847), remains one of the most important in Spain, hosting one of the most prestigious music schools in Barcelona, the Conservatori Superior de MÃºsica del Liceu. Several lyrical artists trained by this institution gained international renown during the 20th century, such as Victoria de los Ãngeles, Montserrat CaballÃ©, Giacomo Aragall and Josep Carreras.

Cellist Pau Casals is admired as an outstanding player. Other popular musical styles were born in the second half of the 20th century such as Nova CanÃ§Ã³ from the 1960s with LluÃ­s Llach and the group Els Setze Jutges, the Catalan rumba in the 1960s with Peret, Catalan Rock from the late 1970s with La Banda Trapera del RÃ­o and Decibelios for Punk Rock, Sau, Els Pets, Sopa de Cabra or Lax'n'Busto for Pop Rock or SangtraÃ¯t for hard rock, electropop since the 1990s with OBK and indie pop from the 1990s.

Catalonia is the autonomous community, along with Madrid, that has the most media (TV, Magazines, Newspapers etc.). In Catalonia there is a wide variety of local and comarcal media. With the restoration of democracy, many newspapers and magazines, until then in the hands of the Franco government, were recovered in order to convert them into free and democratic media, while local radios and televisions were implemented.

TelevisiÃ³ de Catalunya, which broadcasts entirely in the Catalan language, is the main Catalan public TV. It has five channels: TV3, El 33/Super3, 3/24, Esport3 and TV3CAT. In 2018, TV3 became the first television channel to be the most viewed one for nine consecutive years in Catalonia. State televisions that broadcast in Catalonia in Spanish language include TelevisiÃ³n EspaÃ±ola (with few emissions in Catalan), Antena 3, Cuatro, Telecinco, and La Sexta. Other smaller Catalan television channels include; 8TV (owned by Grup GodÃ³), , BarÃ§a TV and the local televisions, the greatest exponent of which is , the TV channel of Barcelona, which also broadcasts in Catalan.

The two main Catalan newspapers of general information are "El PeriÃ³dico de Catalunya" and "La Vanguardia", both with editions in Catalan and Spanish. Catalan only published newspapers include "Ara" and "El Punt Avui" (from the fusion of "El Punt" and "Avui" in 2011), as well as most part of the local press. The Spanish newspapers, such as "El PaÃ­s", "El Mundo" or "La RazÃ³n", can be also acquired.

Catalonia has a long tradition of use of radio, the first regular regular radio broadcast in the country was from RÃ dio Barcelona in 1924. Today, the public Catalunya RÃ dio (owned by Catalan Media Corporation) and the private RAC 1 (belonging to Grup GodÃ³) are the two main radios of Catalonia, both in Catalan.
Regarding the cinema, after the democratic transition, three styles have dominated since then. First, auteur cinema, in the continuity of the Barcelona School, emphasizes experimentation and form, while focusing on developing social and political themes. Worn first by Josep Maria Forn or Bigas Luna, then by Marc Recha, Jaime Rosales and Albert Serra, this genre has achieved some international recognition. Then, the documentary became another genre particularly representative of contemporary Catalan cinema, boosted by Joaquim JordÃ  i CatalÃ  and JosÃ© Luis GuerÃ­n. Later, horror films and thrillers have also emerged as a specialty of the Catalan film industry, thanks in particular to the vitality of the Sitges Film Festival, created in 1968. Several directors have gained worldwide renown thanks to this genre, starting with Jaume BalaguerÃ³ and his series "REC" (co-directed with Valencian Paco Plaza), Juan Antonio Bayona and "El Orfanato" or Jaume Collet-Serra with "Orphan", "Unknown" and "Non-Stop".

Catalan actors have shot for Spanish and international productions, such as Sergi LÃ³pez.

The Museum of Cinema - TomÃ s Mallol Collection (Museu del Cinema - Col.lecciÃ³ TomÃ s Mallol in Catalan) of Girona is home of important permanent exhibitions of cinema and pre-cinema objects. Other important institutions for the promotion of cinema are the GaudÃ­ Awards (Premis GaudÃ­ in Catalan, which replaced from 2009 Barcelona Film Awards themselves created in 2002), serving as equivalent for Catalonia to the Spanish Goya or French CÃ©sar.

 is a form of ancestral Catalan wisdom or sensibleness. It involves well-pondered perception of situations, level-headedness, awareness, integrity, and right action. Many Catalans consider seny something unique to their culture, is based on a set of ancestral local customs stemming from the scale of values and social norms of their society.

Sport has an important incidence in Catalan life and culture since the beginning of the 20th century and, as a result, it has a well developed sport infraestructure. The main sports are football, basketball, handball, rink hockey, tennis and motorsport.

Despite the fact that the most popular sports are represented outside by the Spanish national teams, Catalonia can officially play as itself in some others, like korfball, futsal or rugby league. Most of Catalan Sports Federations have a long tradition and some of them participated in the foundation of international sports federations, as the Catalan Federation of Rugby, that was one of the founder members of the FÃ©dÃ©ration Internationale de Rugby Amateur (FIRA) in 1934. The majority of Catalan sport federations are part of the Sports Federation Union of Catalonia (Catalan: ), founded in 1933.

The Catalan Football Federation also periodically fields a national team against international opposition, organizing friendly matches. In the recent years they have played with Bulgaria, Argentina, Brazil, Basque Country, Colombia, Nigeria, Cape Verde and Tunisia. The biggest football clubs are FC Barcelona (also known as BarÃ§a), who have won five European Cups (UEFA Champions League), and RCD Espanyol, who have twice been runner-up of the UEFA Cup. Both play in La Liga.

The Catalan waterpolo is one of the main powers of the Iberian Peninsula. The Catalans won triumphs in waterpolo competitions at European and world level by club (the Barcelona was champion of Europe in 1981/82 and the Catalonia in 1994/95) and national team (one gold and one silver in Olympic Games and World Championships). It also has many international synchronized swimming champions.

Motorsport has a long tradition in Catalonia, which involving many people, with some world champions and several competitions organized since the beginning of the 20th century. The Circuit de Catalunya, built in 1991, is one of the main motorsport venues, holding the Catalan motorcycle Grand Prix, the Spanish F1 Grand Prix, a DTM race, and several other races.

Catalonia hosted many relevant international sport events, such as the 1992 Summer Olympics in Barcelona, and also the 1955 Mediterranean Games, the 2013 World Aquatics Championships or the 2018 Mediterranean Games. It held annually the fourth-oldest still-existing cycling stage race in the world, the Volta a Catalunya (Tour of Catalonia).

Catalonia has its own representative and distinctive national symbols such as:


Catalan gastronomy has a long culinary tradition. Various local food recipes have been described in documents dating from the fifteenth century. As with all the cuisines of the Mediterranean, Catatonian dishes make abundant use of fish, seafood, olive oil, bread and vegetables. Regional specialties include the (bread with tomato), which consists of bread (sometimes toasted), and tomato seasoned with olive oil and salt. Often the dish is accompanied with any number of sausages (cured botifarres, fuet, iberic ham, etc.), ham or cheeses. Others dishes include the , , (fish stew), and a dessert, Catalan cream.

Catalan vineyards also have several wines, such as: Priorat, Montsant, PenedÃ¨s and EmpordÃ . There is also a sparkling wine, the cava.

Catalonia is internationally recognized for its fine dining. Three of The World's 50 Best Restaurants are in Catalonia, and four restaurants have three Michelin stars, including restaurants like El Bulli or El Celler de Can Roca, both of which regularly dominate international rankings of restaurants.





</doc>
<doc id="6823" url="https://en.wikipedia.org/wiki?curid=6823" title="Constantine Kanaris">
Constantine Kanaris

Constantine Kanaris or Canaris (; 1793 or 17952 September 1877) was a Greek Prime Minister, admiral and politician who in his youth was a freedom fighter in the Greek War of Independence.

He was born and grew up on the island of Psara, close to the island of Chios, in the Aegean. His exact year of birth is unknown. The official records of the Hellenic Navy indicate 1795 but modern Greek historians believe that 1793 is more probable.

Constantine was left an orphan at a young age. Having to support himself, he chose to become a seaman like most members of his family since the beginning of the 18th century. He was hired as a boy on the brig of his uncle Dimitris Bourekas.

Constantine gained his fame during the Greek War of Independence (1821â1829). Unlike most other prominent figures of the War, he had never been initiated into the Filiki Eteria (Friendly Society), which played a significant role in the revolution against the Ottoman Empire, primarily by secret recruitment of supporters against the Empire.

By early 1821, it had gained enough support to declare a revolution. This declaration seems to have surprised Constantine, who was absent at Odessa. He returned to Psara in haste and was there when the island joined the Revolution on 10 April 1821.

The island formed its own fleet of ships and the famed seamen of Psara, already known for their successful naval combats against pirates and their well-equipped ships, proved to be effective at full naval war. Constantine soon distinguished himself as a fire ship captain.

At Chios, on the moonless night of 6â7 June 1822 forces under his command destroyed the flagship of the Turkish admiral Nasuhzade Ali Pasha in revenge for the Chios massacre. The admiral was holding a celebration (Bayram), so Kanaris and his men managed to place a fire ship next to it without being noticed. When the flagship's powder store caught fire, all men aboard were instantly killed. The Ottoman casualties comprised 2000 men, both naval officers and common sailors, as well as Kara-Ali himself.

Later in the year he led another successful attacks against the Turkish fleet at Tenedos in November 1822. 

The Turkish fleet captured Psara on 21 June 1824. A part of the population, including Kanaris, managed to flee the island, but those who didn't were either sold into slavery or slaughtered.

After the destruction of his home island, Kanaris continued to lead his men into attacks against the Turks. He took part to sea fights in the Dodecanese in August 1824.

In August 1825, Kanaris led the raid on Alexandria, a daring attempt to destroy the Egyptian fleet via fire ships that might have been successful if the wind had not failed just after the Greek ships entered Alexandria harbor.

Following the end of the war and the independence of Greece, Constantine became an officer of the new Greek Navy, reaching the rank of admiral, and later became a prominent politician.

Constantine Kanaris was one of the few with the personal confidence of Ioannis Kapodistrias the first Head of State of independent Greece. Kanaris served as Minister in various governments and then as Prime Minister, in the provisional government, from 11 March to 11 April 1844. He served a second term (27 October 184824 December 1849), and as Navy Minister in Mavrokordatos' 1854 cabinet.

In 1862, he was one of the few War of Independence veterans that helped in the bloodless revolution that deposed King Otto of Greece and put Prince William of Denmark on the Greek throne as King George I of Greece. Under George I, he served as a prime minister for a third term (17 March 28 April 1864), fourth term (7 August 18649 February 1865) and fifth and last term (7 June 14 September 1877).

Kanaris died on 2 September 1877 whilst still serving in office as Prime Minister. Following his death his government remained in power until 14 September 1877 without agreeing on a replacement at its head. He was buried in the First Cemetery of Athens, where most Greek prime ministers and celebrated figures are also buried. After his death he was honored as a national hero.

To honour Kanaris, three ships of the Hellenic Navy have been named after him;

In 1817, he married Despina Maniatis, from a historical family of Psara. They had seven children:

Wilhelm Canaris, a German Admiral, speculated that he might be a descendant of Constantine Kanaris. An official genealogical family history that was researched in 1938 showed that he was unrelated and that his family was from Italy.



 


</doc>
<doc id="6824" url="https://en.wikipedia.org/wiki?curid=6824" title="Carl Sagan">
Carl Sagan

Carl Edward Sagan (; November 9, 1934December 20, 1996) was an American astronomer, cosmologist, astrophysicist, astrobiologist, author, science popularizer, and science communicator in astronomy and other natural sciences. He is best known as a science popularizer and communicator. His best known scientific contribution is research on extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation. Sagan assembled the first physical messages sent into space: the Pioneer plaque and the Voyager Golden Record, universal messages that could potentially be understood by any extraterrestrial intelligence that might find them. Sagan argued the now accepted hypothesis that the high surface temperatures of Venus can be attributed to and calculated using the greenhouse effect.

Sagan published more than 600 scientific papers and articles and was author, co-author or editor of more than 20 books. He wrote many popular science books, such as "The Dragons of Eden", "Broca's Brain" and "Pale Blue Dot", and narrated and co-wrote the award-winning 1980 television series "". The most widely-watched series in the history of American public television, "Cosmos" has been seen by at least 500 million people across 60 different countries. The book "Cosmos" was published to accompany the series. He also wrote the science fiction novel "Contact", the basis for a 1997 film of the same name. His papers, containing 595,000 items, are archived at The Library of Congress.

Sagan advocated scientific skeptical inquiry and the scientific method, pioneered exobiology and promoted the Search for Extra-Terrestrial Intelligence (SETI). He spent most of his career as a professor of astronomy at Cornell University, where he directed the Laboratory for Planetary Studies. Sagan and his works received numerous awards and honors, including the NASA Distinguished Public Service Medal, the National Academy of Sciences Public Welfare Medal, the Pulitzer Prize for General Non-Fiction for his book "The Dragons of Eden", and, regarding "Cosmos: A Personal Voyage", two Emmy Awards, the Peabody Award, and the Hugo Award. He married three times and had five children. After suffering from myelodysplasia, Sagan died of pneumonia at the age of 62, on December 20, 1996.

Carl Sagan was born in Brooklyn, New York. His father, Samuel Sagan, was an immigrant garment worker from Kamianets-Podilskyi, then in the Russian Empire, in today's Ukraine. His mother, Rachel Molly Gruber, was a housewife from New York. Carl was named in honor of Rachel's biological mother, Chaiya Clara, in Sagan's words, "the mother she never knew".

He had a sister, Carol, and the family lived in a modest apartment near the Atlantic Ocean, in Bensonhurst, a Brooklyn neighborhood. According to Sagan, they were Reform Jews, the most liberal of North American Judaism's four main groups. Carl and his sister agreed that their father was not especially religious, but that their mother "definitely believed in God, and was active in the temple; ... and served only kosher meat". During the depths of the Depression, his father worked as a theater usher.

According to biographer Keay Davidson, Sagan's "inner war" was a result of his close relationship with both of his parents, who were in many ways "opposites". Sagan traced his later analytical urges to his mother, a woman who had been extremely poor as a child in New York City during World War I and the 1920s. As a young woman, she had held her own intellectual ambitions, but they were frustrated by social restrictions: her poverty, her status as a woman and a wife, and her Jewish ethnicity. Davidson notes that she therefore "worshipped her only son, Carl. He would fulfill her unfulfilled dreams."

However, he claimed that his sense of wonder came from his father, who in his free time gave apples to the poor or helped soothe labor-management tensions within New York's garment industry. Although he was awed by Carl's intellectual abilities, he took his son's inquisitiveness in stride and saw it as part of his growing up. In his later years as a writer and scientist, Sagan would often draw on his childhood memories to illustrate scientific points, as he did in his book "Shadows of Forgotten Ancestors". Sagan describes his parents' influence on his later thinking:
Sagan recalls that one of his most defining moments was when his parents took him to the 1939 New York World's Fair when he was four years old. The exhibits became a turning point in his life. He later recalled the moving map of the "America of Tomorrow" exhibit: "It showed beautiful highways and cloverleaves and little General Motors cars all carrying people to skyscrapers, buildings with lovely spires, flying buttressesâand it looked great!" At other exhibits, he remembered how a flashlight that shone on a photoelectric cell created a crackling sound, and how the sound from a tuning fork became a wave on an oscilloscope. He also witnessed the future media technology that would replace radio: television. Sagan wrote:
He also saw one of the Fair's most publicized events, the burial of a time capsule at Flushing Meadows, which contained mementos of the 1930s to be recovered by Earth's descendants in a future millennium. "The time capsule thrilled Carl", writes Davidson. As an adult, Sagan and his colleagues would create similar time capsulesâcapsules that would be sent out into the galaxy; these were the Pioneer plaque and the "Voyager Golden Record" prÃ©cis, all of which were spinoffs of Sagan's memories of the World's Fair.

During World WarÂ II Sagan's family worried about the fate of their European relatives. Sagan, however, was generally unaware of the details of the ongoing war. He wrote, "Sure, we had relatives who were caught up in the Holocaust. Hitler was not a popular fellow in our household... But on the other hand, I was fairly insulated from the horrors of the war." His sister, Carol, said that their mother "above all wanted to protect Carl... She had an extraordinarily difficult time dealing with World WarÂ II and the Holocaust." Sagan's book "The Demon-Haunted World" (1996) included his memories of this conflicted period, when his family dealt with the realities of the war in Europe but tried to prevent it from undermining his optimistic spirit.

Soon after entering elementary school he began to express a strong inquisitiveness about nature. Sagan recalled taking his first trips to the public library alone, at the age of five, when his mother got him a library card. He wanted to learn what stars were, since none of his friends or their parents could give him a clear answer:
At about age six or seven, he and a close friend took trips to the American Museum of Natural History across the East River in Manhattan. While there, they went to the Hayden Planetarium and walked around the museum's exhibits of space objects, such as meteorites, and displays of dinosaurs and animals in natural settings. Sagan writes about those visits:
His parents helped nurture his growing interest in science by buying him chemistry sets and reading materials. His interest in space, however, was his primary focus, especially after reading science fiction stories by writers such as H. G. Wells and Edgar Rice Burroughs, which stirred his imagination about life on other planets such as Mars. According to biographer Ray Spangenburg, these early years as Sagan tried to understand the mysteries of the planets became a "driving force in his life, a continual spark to his intellect, and a quest that would never be forgotten".

In 1947 he discovered "Astounding Science Fiction" magazine, which introduced him to more hard science fiction speculations than those in Burroughs's novels. That same year inaugurated the "flying saucer" mass hysteria with the young Carl suspecting that the "discs" might be alien spaceships.

Sagan had lived in Bensonhurst, where he went to David A. Boody Junior High School. He had his bar mitzvah in Bensonhurst when he turned 13. The following year, 1948, his family moved to the nearby town of Rahway, New Jersey, for his father's work, where Sagan then entered Rahway High School. He graduated in 1951. Rahway was an older industrial town, and the Sagans were among its few Jewish families.
Sagan was a straight-A student but was bored due to unchallenging classes and uninspiring teachers. His teachers realized this and tried to convince his parents to send him to a private school, the administrator telling them, "This kid ought to go to a school for gifted children, he has something really remarkable." This they couldn't do, partly because of the cost.

Sagan was made president of the school's chemistry club, and at home he set up his own laboratory. He taught himself about molecules by making cardboard cutouts to help him visualize how molecules were formed: "I found that about as interesting as doing [chemical] experiments", he said. Sagan remained mostly interested in astronomy as a hobby, and in his junior year made it a career goal after he learned that astronomers were paid for doing what he always enjoyed: "That was a splendid dayâwhen I began to suspect that if I tried hard I could do astronomy full-time, not just part-time."

Before the end of high school, he entered an essay contest in which he posed the question of whether human contact with advanced life forms from another planet might be as disastrous for people on Earth as it was for Native Americans when they first had contact with Europeans. The subject was considered controversial, but his rhetorical skill won over the judges, and they awarded him first prize. By graduation, his classmates had voted him "most likely to succeed" and put him in line to be valedictorian.

Sagan attended the University of Chicago, which was one of the few colleges he applied to that would consider admitting a 16-year-old, despite his excellent high-school grades. Its Chancellor, Robert Hutchins, structured the school as an "ideal meritocracy", with no age requirement. The school also employed a number of the nation's leading scientists, including Enrico Fermi and Edward Teller, along with operating the famous Yerkes Observatory.

During his time as an honors program undergraduate, Sagan worked in the laboratory of the geneticist H. J. Muller and wrote a thesis on the origins of life with physical chemist Harold Urey. Sagan joined the Ryerson Astronomical Society, received a B.A. degree in laughingly self-proclaimed "nothing" with general and special honors in 1954, and a B.S. degree in physics in 1955. He went on to earn a M.S. degree in physics in 1956, before earning a Ph.D. degree in 1960 with his thesis "Physical Studies of Planets" submitted to the Department of Astronomy and Astrophysics.

He used the summer months of his graduate studies to work with his dissertation director, planetary scientist Gerard Kuiper, as well as physicist George Gamow and chemist Melvin Calvin. The title of Sagan's dissertation reflects his shared interests with Kuiper, who throughout the 1950s had been president of the International Astronomical Union's commission on "Physical Studies of Planets and Satellites". In 1958, the two worked on the classified military Project A119, the secret Air Force plan to detonate a nuclear warhead on the Moon.

Sagan had a Top Secret clearance at the U.S. Air Force and a Secret clearance with NASA. While working on his doctoral dissertation, Sagan revealed US Government classified titles of two Project A119 papers when he applied for a University of California at Berkeley scholarship in 1959. The leak was not publicly revealed until 1999, when it was published in the journal "Nature". A follow-up letter to the journal by project leader Leonard Reiffel confirmed Sagan's security leak.

From 1960 to 1962 Sagan was a Miller Fellow at the University of California, Berkeley. Meanwhile, he published an article in 1961 in the journal "Science" on the atmosphere of Venus, while also working with NASA's Mariner 2 team, and served as a "Planetary Sciences Consultant" to the RAND Corporation.

After the publication of Sagan's "Science" article, in 1961 Harvard University astronomers Fred Whipple and Donald Menzel offered Sagan the opportunity to give a colloquium at Harvard and subsequently offered him a lecturer position at the institution. Sagan instead asked to be made an assistant professor, and eventually Whipple and Menzel were able to convince Harvard to offer Sagan the assistant professor position he requested. Sagan lectured, performed research, and advised graduate students at the institution from 1963 until 1968, as well as working at the Smithsonian Astrophysical Observatory, also located in Cambridge, Massachusetts.

In 1968, Sagan was denied tenure at Harvard. He later indicated that the decision was very much unexpected. The tenure denial has been blamed on several factors, including that he focused his interests too broadly across a number of areas (while the norm in academia is to become a renowned expert in a narrow specialty), and perhaps because of his well-publicized scientific advocacy, which some scientists perceived as borrowing the ideas of others for little more than self-promotion. An advisor from his years as an undergraduate student, Harold Urey, wrote a letter to the tenure committee recommending strongly against tenure for Sagan.

Long before the ill-fated tenure process, Cornell University astronomer Thomas Gold had courted Sagan to move to Ithaca, New York, and join the faculty at Cornell. Following the denial of tenure from Harvard, Sagan accepted Gold's offer and remained a faculty member at Cornell for nearly 30 years until his death in 1996. Unlike Harvard, the smaller and more laid-back astronomy department at Cornell welcomed Sagan's growing celebrity status. Following two years as an associate professor, Sagan became a full professor at Cornell in 1970 and directed the Laboratory for Planetary Studies there. From 1972 to 1981, he was associate director of the Center for Radiophysics and Space Research (CRSR) at Cornell. In 1976, he became the David Duncan Professor of Astronomy and Space Sciences, a position he held for the remainder of his life.

Sagan was associated with the U.S. space program from its inception. From the 1950s onward, he worked as an advisor to NASA, where one of his duties included briefing the Apollo astronauts before their flights to the Moon. Sagan contributed to many of the robotic spacecraft missions that explored the Solar System, arranging experiments on many of the expeditions. Sagan assembled the first physical message that was sent into space: a gold-plated plaque, attached to the space probe "PioneerÂ 10", launched in 1972. "PioneerÂ 11", also carrying another copy of the plaque, was launched the following year. He continued to refine his designs; the most elaborate message he helped to develop and assemble was the Voyager Golden Record, which was sent out with the Voyager space probes in 1977. Sagan often challenged the decisions to fund the Space Shuttle and the International Space Station at the expense of further robotic missions.

Former student David Morrison describes Sagan as "an 'idea person' and a master of intuitive physical arguments and 'back of the envelope' calculations", and Gerard Kuiper said that "Some persons work best in specializing on a major program in the laboratory; others are best in liaison between sciences. Dr. Sagan belongs in the latter group."

Sagan's contributions were central to the discovery of the high surface temperatures of the planet Venus. In the early 1960s no one knew for certain the basic conditions of Venus' surface, and Sagan listed the possibilities in a report later depicted for popularization in a Time Life book "Planets". His own view was that Venus was dry and very hot as opposed to the balmy paradise others had imagined. He had investigated radio waves from Venus and concluded that there was a surface temperature of . As a visiting scientist to NASA's Jet Propulsion Laboratory, he contributed to the first Mariner missions to Venus, working on the design and management of the project. Mariner 2 confirmed his conclusions on the surface conditions of Venus in 1962.

Sagan was among the first to hypothesize that Saturn's moon Titan might possess oceans of liquid compounds on its surface and that Jupiter's moon Europa might possess subsurface oceans of water. This would make Europa potentially habitable. Europa's subsurface ocean of water was later indirectly confirmed by the spacecraft "Galileo". The mystery of Titan's reddish haze was also solved with Sagan's help. The reddish haze was revealed to be due to complex organic molecules constantly raining down onto Titan's surface.

Sagan further contributed insights regarding the atmospheres of Venus and Jupiter, as well as seasonal changes on Mars. He also perceived global warming as a growing, man-made danger and likened it to the natural development of Venus into a hot, life-hostile planet through a kind of runaway greenhouse effect. Sagan and his Cornell colleague Edwin Ernest Salpeter speculated about life in Jupiter's clouds, given the planet's dense atmospheric composition rich in organic molecules. He studied the observed color variations on Mars' surface and concluded that they were not seasonal or vegetational changes as most believed, but shifts in surface dust caused by windstorms.

Sagan is also known for his research on the possibilities of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by radiation.

He is also the 1994 recipient of the Public Welfare Medal, the highest award of the National Academy of Sciences for "distinguished contributions in the application of science to the public welfare". He was denied membership in the Academy, reportedly because his media activities made him unpopular with many other scientists.

, Sagan is the most cited SETI scientist and one of the most cited planetary scientists.

In 1980 Sagan co-wrote and narrated the award-winning 13-part PBS television series "", which became the most widely watched series in the history of American public television. The show has been seen by at least 500 million people across 60 different countries. The book, "Cosmos", written by Sagan, was published to accompany the series.

Because of his earlier popularity as a science writer from his best-selling books, including "The Dragons of Eden", which won him a Pulitzer Prize in 1977, he was asked to write and narrate the show. It was targeted to a general audience of viewers, who Sagan felt had lost interest in science, partly due to a stifled educational system.

Each of the 13 episodes was created to focus on a particular subject or person, thereby demonstrating the synergy of the universe. They covered a wide range of scientific subjects including the origin of life and a perspective of humans' place on Earth.

The show won an Emmy, along with a Peabody Award, and transformed Sagan from an obscure astronomer into a pop-culture icon. "Time" magazine ran a cover story about Sagan soon after the show broadcast, referring to him as "creator, chief writer and host-narrator of the show". In 2000, "Cosmos" was released on a remastered set of DVDs.

Sagan was invited to frequent appearances on "The Tonight Show Starring Johnny Carson".
After "Cosmos" aired, he became associated with the catchphrase "billions and billions", although he never actually used the phrase in the "Cosmos" series. He rather used the term "billions "upon" billions". Carson, however, would sometimes use the phrase during his parodies of Sagan.

As a humorous tribute to Sagan and his association with the catchphrase "billions and billions", a "sagan" has been defined as a unit of measurement equivalent to a very large numberÂ â technically at least four billion (two billion plus two billion)Â â of anything.

Sagan's ability to convey his ideas allowed many people to understand the cosmos betterâsimultaneously emphasizing the value and worthiness of the human race, and the relative insignificance of the Earth in comparison to the Universe. He delivered the 1977 series of Royal Institution Christmas Lectures in London.

Sagan was a proponent of the search for extraterrestrial life. He urged the scientific community to listen with radio telescopes for signals from potential intelligent extraterrestrial life-forms. Sagan was so persuasive that by 1982 he was able to get a petition advocating SETI published in the journal "Science", signed by 70 scientists, including seven Nobel Prize winners. This signaled a tremendous increase in the respectability of a then-controversial field. Sagan also helped Frank Drake write the Arecibo message, a radio message beamed into space from the Arecibo radio telescope on November 16, 1974, aimed at informing potential extraterrestrials about Earth.

Sagan was chief technology officer of the professional planetary research journal "Icarus" for 12 years. He co-founded The Planetary Society and was a member of the SETI Institute Board of Trustees. Sagan served as Chairman of the Division for Planetary Science of the American Astronomical Society, as President of the Planetology Section of the American Geophysical Union, and as Chairman of the Astronomy Section of the American Association for the Advancement of Science (AAAS).
At the height of the Cold War, Sagan became involved in nuclear disarmament efforts by promoting hypotheses on the effects of nuclear war, when Paul Crutzen's "Twilight at Noon" concept suggested that a substantial nuclear exchange could trigger a nuclear twilight and upset the delicate balance of life on Earth by cooling the surface. In 1983 he was one of five authorsâthe "S"âin the follow-up "TTAPS" model (as the research article came to be known), which contained the first use of the term "nuclear winter", which his colleague Richard P. Turco had coined. In 1984 he co-authored the book "" and in 1990 the book "A Path Where No Man Thought: Nuclear Winter and the End of the Arms Race", which explains the nuclear-winter hypothesis and advocates nuclear disarmament. Sagan received a great deal of skepticism and disdain for the use of media to disseminate a very uncertain hypothesis. A personal correspondence with nuclear physicist Edward Teller around 1983 began amicably, with Teller expressing support for continued research to ascertain the credibility of the winter hypothesis. However, Sagan and Teller's correspondence would ultimately result in Teller writing: "A propagandist is one who uses incomplete information to produce maximum persuasion. I can compliment you on being, indeed, an excellent propagandist, remembering that a propagandist is the better the less he appears to be one". Biographers of Sagan would also comment that from a scientific viewpoint, nuclear winter was a low point for Sagan, although, politically speaking, it popularized his image amongst the public.

The adult Sagan remained a fan of science fiction, although disliking stories that were not realistic (such as ignoring the inverse-square law) or, he said, did not include "thoughtful pursuit of alternative futures". He wrote books to popularize science, such as "Cosmos", which reflected and expanded upon some of the themes of "A Personal Voyage" and became the best-selling science book ever published in English; "The Dragons of Eden: Speculations on the Evolution of Human Intelligence", which won a Pulitzer Prize; and "Broca's Brain: Reflections on the Romance of Science". Sagan also wrote the best-selling science fiction novel "Contact" in 1985, based on a film treatment he wrote with his wife, Ann Druyan, in 1979, but he did not live to see the book's 1997 motion-picture adaptation, which starred Jodie Foster and won the 1998 Hugo Award for Best Dramatic Presentation.

Sagan wrote a sequel to "Cosmos", "Pale Blue Dot: A Vision of the Human Future in Space", which was selected as a notable book of 1995 by "The New York Times". He appeared on PBS's "Charlie Rose" program in January 1995. Sagan also wrote the introduction for Stephen Hawking's bestseller "A Brief History of Time". Sagan was also known for his popularization of science, his efforts to increase scientific understanding among the general public, and his positions in favor of scientific skepticism and against pseudoscience, such as his debunking of the Betty and Barney Hill abduction. To mark the tenth anniversary of Sagan's death, David Morrison, a former student of Sagan, recalled "Sagan's immense contributions to planetary research, the public understanding of science, and the skeptical movement" in "Skeptical Inquirer".

Following Saddam Hussein's threats to light Kuwait's oil wells on fire in response to any physical challenge to Iraqi control of the oil assets, Sagan together with his "TTAPS" colleagues and Paul Crutzen, warned in January 1991 in the "Baltimore Sun" and "Wilmington Morning Star" newspapers that if the fires were left to burn over a period of several months, enough smoke from the 600 or so 1991 Kuwaiti oil fires "might get so high as to disrupt agriculture in much of South AsiaÂ ..." and that this possibility should "affect the war plans"; these claims were also the subject of a televised debate between Sagan and physicist Fred Singer on January 22, aired on the ABC News program "Nightline". In the televised debate, Sagan argued that the effects of the smoke would be similar to the effects of a nuclear winter, with Singer arguing to the contrary. After the debate, the fires burnt for many months before extinguishing efforts were complete. The results of the smoke did not produce continental-sized cooling. Sagan later conceded in "The Demon-Haunted World" that the prediction did not turn out to be correct: "it "was" pitch black at noon and temperatures dropped 4â6Â Â°C over the Persian Gulf, but not much smoke reached stratospheric altitudes and Asia was spared".

In his later years Sagan advocated the creation of an organized search for asteroids/near-Earth objects (NEOs) that might impact the Earth but to forestall or postpone developing the technological methods that would be needed to defend against them. He argued that all of the numerous methods proposed to alter the orbit of an asteroid, including the employment of nuclear detonations, created a deflection dilemma: if the ability to deflect an asteroid away from the Earth exists, then one would also have the ability to divert a non-threatening object towards Earth, creating an immensely destructive weapon. In a 1994 paper he co-authored, he ridiculed a 3-day long "Near-Earth Object Interception Workshop" held by Los Alamos National Laboratory (LANL) in 1993 that did not, "even in passing" state that such interception and deflection technologies could have these "ancillary dangers".

Sagan remained hopeful that the natural NEO impact threat and the intrinsically double-edged essence of the methods to prevent these threats would serve as a "new and potent motivation to maturing international relations". Later acknowledging that, with sufficient international oversight, in the future a "work our way up" approach to implementing nuclear explosive deflection methods could be fielded, and when sufficient knowledge was gained, to use them to aid in mining asteroids. His interest in the use of nuclear detonations in space grew out of his work in 1958 for the Armour Research Foundation's Project A119, concerning the possibility of detonating a nuclear device on the lunar surface.

Sagan was a critic of Plato, having said of the ancient Greek philosopher: "Science and mathematics were to be removed from the hands of the merchants and the artisans. This tendency found its most effective advocate in a follower of Pythagoras named Plato" and

He (Plato) believed that ideas were far more real than the natural world. He advised the astronomers not to waste their time observing the stars and planets. It was better, he believed, just to think about them. Plato expressed hostility to observation and experiment. He taught contempt for the real world and disdain for the practical application of scientific knowledge. Plato's followers succeeded in extinguishing the light of science and experiment that had been kindled by Democritus and the other Ionians.
Speaking about his activities in popularizing science, Sagan said that there were at least two reasons for scientists to share the purposes of science and its contemporary state. Simple self-interest was one: much of the funding for science came from the public, and the public therefore had the right to know how the money was being spent. If scientists increased public admiration for science, there was a good chance of having more public supporters. The other reason was the excitement of communicating one's own excitement about science to others.

While Sagan was widely adored by the general public, his reputation in the scientific community was more polarized. Critics sometimes characterized his work as fanciful, non-rigorous, and self-aggrandizing, and others complained in his later years that he neglected his role as a faculty member to foster his celebrity status.

One of Sagan's harshest critics, Harold Urey, felt that Sagan was getting too much publicity for a scientist and was treating some scientific theories too casually. Urey and Sagan were said to have different philosophies of science, according to Davidson. While Urey was an "old-time empiricist" who avoided theorizing about the unknown, Sagan was by contrast willing to speculate openly about such matters. Fred Whipple wanted Harvard to keep Sagan there, but learned that because Urey was a Nobel laureate, his opinion was an important factor in Harvard denying Sagan tenure.

Sagan's Harvard friend Lester Grinspoon also stated: "I know Harvard well enough to know there are people there who certainly do not like people who are outspoken." Grinspoon added:
Some, like Urey, later came to realize that Sagan's popular brand of scientific advocacy was beneficial to the science as a whole. Urey especially liked Sagan's 1977 book "The Dragons of Eden" and wrote Sagan with his opinion: "I like it very much and am amazed that someone like you has such an intimate knowledge of the various features of the problem... I congratulate you... You are a man of many talents."

Sagan was accused of borrowing some ideas of others for his own benefit and countered these claims by explaining that the misappropriation was an unfortunate side effect of his role as a science communicator and explainer, and that he attempted to give proper credit whenever possible.

Sagan believed that the Drake equation, on substitution of reasonable estimates, suggested that a large number of extraterrestrial civilizations would form, but that the lack of evidence of such civilizations highlighted by the Fermi paradox suggests technological civilizations tend to self-destruct. This stimulated his interest in identifying and publicizing ways that humanity could destroy itself, with the hope of avoiding such a cataclysm and eventually becoming a spacefaring species. Sagan's deep concern regarding the potential destruction of human civilization in a nuclear holocaust was conveyed in a memorable cinematic sequence in the final episode of "Cosmos", called "Who Speaks for Earth?" Sagan had already resigned from the Air Force Scientific Advisory Board's UFO investigating Condon Committee and voluntarily surrendered his top-secret clearance in protest over the Vietnam War. Following his marriage to his third wife (novelist Ann Druyan) in June 1981, Sagan became more politically activeâparticularly in opposing escalation of the nuclear arms race under President Ronald Reagan.

In March 1983, Reagan announced the Strategic Defense Initiativeâa multibillion-dollar project to develop a comprehensive defense against attack by nuclear missiles, which was quickly dubbed the "Star Wars" program. Sagan spoke out against the project, arguing that it was technically impossible to develop a system with the level of perfection required, and far more expensive to build such a system than it would be for an enemy to defeat it through decoys and other meansâand that its construction would seriously destabilize the "nuclear balance" between the United States and the Soviet Union, making further progress toward nuclear disarmament impossible.

When Soviet leader Mikhail Gorbachev declared a unilateral moratorium on the testing of nuclear weapons, which would begin on August 6, 1985âthe 40thÂ anniversary of the atomic bombing of Hiroshimaâthe Reagan administration dismissed the dramatic move as nothing more than propaganda and refused to follow suit. In response, US anti-nuclear and peace activists staged a series of protest actions at the Nevada Test Site, beginning on Easter Sunday in 1986 and continuing through 1987. Hundreds of people in the "Nevada Desert Experience" group were arrested, including Sagan, who was arrested on two separate occasions as he climbed over a chain-link fence at the test site during the underground Operation Charioteer and United States's Musketeer nuclear test series of detonations.

Sagan was also a vocal advocate of the controversial notion of testosterone poisoning, arguing in 1992 that human males could become gripped by an "unusually severe [case of] testosterone poisoning" and this could compel them to become genocidal. In his review of Moondance magazine writer Daniela Gioseffi's 1990 book "Women on War", he argues that females are the only half of humanity "untainted by testosterone poisoning". One chapter of his 1993 book "Shadows of Forgotten Ancestors" is dedicated to testosterone and its alleged poisonous effects.

Sagan was married three times. In 1957, he married biologist Lynn Margulis. The couple had two children, Jeremy and Dorion Sagan. After Carl Sagan and Margulis divorced, he married artist Linda Salzman in 1968 and they also had a child together, Nick Sagan. During these marriages, Carl Sagan focused heavily on his career, a factor which may have contributed to Sagan's first divorce. In 1981, Sagan married author Ann Druyan and they later had two children, Alexandra and Samuel Sagan. Carl Sagan and Druyan remained married until his death in 1996. He lived in an Egyptian revival house in Ithaca perched on the edge of a cliff that had formerly been the headquarters of a Cornell secret society. Daughter Alexandra (also known as Sasha) published a book in 2019 on her father's primary legacy: skepticism does not mean pessimism.

Isaac Asimov described Sagan as one of only two people he ever met whose intellect surpassed his own. The other, he claimed, was the computer scientist and artificial intelligence expert Marvin Minsky.

Sagan wrote frequently about religion and the relationship between religion and science, expressing his skepticism about the conventional conceptualization of God as a sapient being. For example: Some people think God is an outsized, light-skinned male with a long white beard, sitting on a throne somewhere up there in the sky, busily tallying the fall of every sparrow. Othersâfor example Baruch Spinoza and Albert Einsteinâconsidered God to be essentially the sum total of the physical laws which describe the universe. I do not know of any compelling evidence for anthropomorphic patriarchs controlling human destiny from some hidden celestial vantage point, but it would be madness to deny the existence of physical laws.

In another description of his view on the concept of God, Sagan wrote: The idea that God is an oversized white male with a flowing beard who sits in the sky and tallies the fall of every sparrow is ludicrous. But if by God one means the set of physical laws that govern the universe, then clearly there is such a God. This God is emotionally unsatisfyingÂ ... it does not make much sense to pray to the law of gravity.

On atheism, Sagan commented in 1981: An atheist is someone who is certain that God does not exist, someone who has compelling evidence against the existence of God. I know of no such compelling evidence. Because God can be relegated to remote times and places and to ultimate causes, we would have to know a great deal more about the universe than we do now to be sure that no such God exists. To be certain of the existence of God and to be certain of the nonexistence of God seem to me to be the confident extremes in a subject so riddled with doubt and uncertainty as to inspire very little confidence indeed.

Sagan also commented on Christianity and the Jefferson Bible, stating "My long-time view about Christianity is that it represents an amalgam of two seemingly immiscible parts, the religion of Jesus and the religion of Paul. Thomas Jefferson attempted to excise the Pauline parts of the New Testament. There wasn't much left when he was done, but it was an inspiring document."

Regarding spirituality and its relationship with science, Sagan stated: 'Spirit' comes from the Latin word 'to breathe'. What we breathe is air, which is certainly matter, however thin. Despite usage to the contrary, there is no necessary implication in the word 'spiritual' that we are talking of anything other than matter (including the matter of which the brain is made), or anything
outside the realm of science. On occasion, I will feel free to use the word. Science is not only compatible with spirituality; it is a profound source of spirituality. When we recognize our place in an immensity of light-years and in the passage of ages, when we grasp the intricacy, beauty, and subtlety of life, then that soaring feeling, that sense of elation and humility combined, is surely spiritual.

An environmental appeal, "Preserving and Cherishing the Earth", signed by Sagan with other noted scientists in January 1990, stated that "The historical record makes clear that religious teaching, example, and leadership are powerfully able to influence personal conduct and commitment... Thus, there is a vital role for religion and science."

In reply to a question in 1996 about his religious beliefs, Sagan answered, "I'm agnostic." Sagan maintained that the idea of a creator God of the Universe was difficult to prove or disprove and that the only conceivable scientific discovery that could challenge it would be an infinitely old universe. Sagan's views on religion have been interpreted as a form of pantheism comparable to Einstein's belief in Spinoza's God. His son, Dorion Sagan said, "My father believed in the God of Spinoza and Einstein, God not behind nature but as nature, equivalent to it." His last wife, Ann Druyan, stated: When my husband died, because he was so famous and known for not being a believer, many people would come up to meâit still sometimes happensâand ask me if Carl changed at the end and converted to a belief in an afterlife. They also frequently ask me if I think I will see him again. Carl faced his death with unflagging courage and never sought refuge in illusions. The tragedy was that we knew we would never see each other again. I don't ever expect to be reunited with Carl.

In 2006, Ann Druyan edited Sagan's 1985 Glasgow "Gifford Lectures in Natural Theology" into a book, "", in which he elaborates on his views of divinity in the natural world.

Sagan is also widely regarded as a freethinker or skeptic; one of his most famous quotations, in "Cosmos", was, "Extraordinary claims require extraordinary evidence" (called the "Sagan standard" by some). This was based on a nearly identical statement by fellow founder of the Committee for the Scientific Investigation of Claims of the Paranormal, Marcello Truzzi, "An extraordinary claim requires extraordinary proof." This idea had been earlier aphorized in ThÃ©odore Flournoy's work "From India to the Planet Mars" (1899) from a longer quote by Pierre-Simon Laplace (1749â1827), a French mathematician and astronomer, as the Principle of Laplace: "The weight of the evidence should be proportioned to the strangeness of the facts."

Late in his life, Sagan's books elaborated on his naturalistic view of the world. In "The Demon-Haunted World", he presented tools for testing arguments and detecting fallacious or fraudulent ones, essentially advocating wide use of critical thinking and the scientific method. The compilation "Billions and Billions: Thoughts on Life and Death at the Brink of the Millennium", published in 1997 after Sagan's death, contains essays written by Sagan, such as his views on abortion, as well as an account by his widow, Ann Druyan, of his death in relation to his having been an agnostic and freethinker.

Sagan warned against humans' tendency towards anthropocentrism. He was the faculty adviser for the Cornell Students for the Ethical Treatment of Animals. In the "Cosmos" chapter "Blues For a Red Planet", Sagan wrote, "If there is life on Mars, I believe we should do nothing with Mars. Mars then belongs to the Martians, even if the Martians are only microbes."

Sagan was a user and advocate of marijuana. Under the pseudonym "Mr.Â X", he contributed an essay about smoking cannabis to the 1971 book "Marihuana Reconsidered". The essay explained that marijuana use had helped to inspire some of Sagan's works and enhance sensual and intellectual experiences. After Sagan's death, his friend Lester Grinspoon disclosed this information to Sagan's biographer, Keay Davidson. The publishing of the biography, "Carl Sagan: A Life", in 1999 brought media attention to this aspect of Sagan's life. Not long after his death, his widow Ann Druyan went on to preside over the board of directors of the National Organization for the Reform of Marijuana Laws (NORML), a non-profit organization dedicated to reforming cannabis laws.

In 1994, engineers at Apple Computer code-named the Power MacintoshÂ 7100 "Carl Sagan" in the hope that Apple would make "billions and billions" with the sale of the PowerMacÂ 7100. The name was only used internally, but Sagan was concerned that it would become a product endorsement and sent Apple a cease-and-desist letter. Apple complied, but engineers retaliated by changing the internal codename to "BHA" for "Butt-Head Astronomer". Sagan then sued Apple for libel in federal court. The court granted Apple's motion to dismiss Sagan's claims and opined in dicta that a reader aware of the context would understand Apple was "clearly attempting to retaliate in a humorous and satirical way", and that "It strains reason to conclude that Defendant was attempting to criticize Plaintiff's reputation or competency as an astronomer. One does not seriously attack the expertise of a scientist using the undefined phrase 'butt-head'." Sagan then sued for Apple's original use of his name and likeness, but again lost. Sagan appealed the ruling. In November 1995, an out-of-court settlement was reached and Apple's office of trademarks and patents released a conciliatory statement that "Apple has always had great respect for Dr.Â Sagan. It was never Apple's intention to cause Dr.Â Sagan or his family any embarrassment or concern." Apple's third and final code name for the project was "LAW", short for "Lawyers are Wimps".

Sagan briefly served as an adviser on Stanley Kubrick's film "". Sagan proposed that the film suggest, rather than depict, extraterrestrial superintelligence.

In 1947, the year that inaugurated the "flying saucer" craze, the young Sagan suspected the "discs" might be alien spaceships.

Sagan's interest in UFO reports prompted him on August 3, 1952, to write a letter to U.S.Â Secretary of State Dean Acheson to ask how the United States would respond if flying saucers turned out to be extraterrestrial. He later had several conversations on the subject in 1964 with Jacques VallÃ©e. Though quite skeptical of any extraordinary answer to the UFO question, Sagan thought scientists should study the phenomenon, at least because there was widespread public interest in UFO reports.

Stuart Appelle notes that Sagan "wrote frequently on what he perceived as the logical and empirical fallacies regarding UFOs and the abduction experience. Sagan rejected an extraterrestrial explanation for the phenomenon but felt there were both empirical and pedagogical benefits for examining UFO reports and that the subject was, therefore, a legitimate topic of study."

In 1966 Sagan was a member of the AdÂ Hoc Committee to Review Project Blue Book, the U.S.Â Air Force's UFO investigation project. The committee concluded Blue Book had been lacking as a scientific study, and recommended a university-based project to give the UFO phenomenon closer scientific scrutiny. The result was the Condon Committee (1966â68), led by physicist Edward Condon, and in their final report they formally concluded that UFOs, regardless of what any of them actually were, did not behave in a manner consistent with a threat to national security.

Sociologist Ron Westrum writes that "The high point of Sagan's treatment of the UFO question was the AAAS' symposium in 1969. A wide range of educated opinions on the subject were offered by participants, including not only proponents such as James McDonald and J.Â Allen Hynek but also skeptics like astronomers William Hartmann and Donald Menzel. The roster of speakers was balanced, and it is to Sagan's credit that this event was presented in spite of pressure from Edward Condon." With physicist Thornton Page, Sagan edited the lectures and discussions given at the symposium; these were published in 1972 as "UFO's: A Scientific Debate". Some of Sagan's many books examine UFOs (as did one episode of "Cosmos") and he claimed a religious undercurrent to the phenomenon.

Sagan again revealed his views on interstellar travel in his 1980 "Cosmos" series. In one of his last written works, Sagan argued that the chances of extraterrestrial spacecraft visiting Earth are vanishingly small. However, Sagan did think it plausible that Cold War concerns contributed to governments misleading their citizens about UFOs, and wrote that "some UFO reports and analyses, and perhaps voluminous files, have been made inaccessible to the public which pays the billsÂ ... It's time for the files to be declassified and made generally available." He cautioned against jumping to conclusions about suppressed UFO data and stressed that there was no strong evidence that aliens were visiting the Earth either in the past or present.

Sagan's contribution to the 1969 symposium was an attack on the belief that UFOs are piloted by extraterrestrial beings. Applying several logical assumptions (see Drake equation), Sagan calculated the possible number of advanced civilizations capable of interstellar travel to be about one million. He projected that any civilization wishing to check on all the others on a regular basis of, say, once a year would have to launch 10,000 spacecraft annually. Not only does that seem like an unreasonable number of launchings, but it would take all the material in one percent of the universe's stars to produce all the spaceships needed for all the civilizations to seek each other out.

To argue that the Earth was being chosen for regular visitations, Sagan said, one would have to assume that the planet is somehow unique and that assumption "goes exactly against the idea that there are lots of civilizations around. Because if there are then our sort of civilization must be pretty common. And if we're not pretty common then there aren't going to be many civilizations advanced enough to send visitors".

This argument, which some called Sagan's paradox, helped to establish a new school of thought, namely the belief that extraterrestrial life exists, but it has nothing to do with UFOs. The new belief had a salutary effect on UFO studies. It helped separate researchers who wanted to distinguish UFOs from those who wanted to identify their pilots and it gave scientists opportunities to search the universe for intelligent life unencumbered by the stigma associated with UFOs.

After suffering from myelodysplasia for two years and receiving three bone marrow transplants from his sister, Sagan died from pneumonia at the age of 62, at the Fred Hutchinson Cancer Research Center in Seattle, Washington, on December 20, 1996.
Burial took place at Lake View Cemetery in Ithaca, New York.


The 1997 film "Contact", based on Sagan's only fiction novel of the same name and finished after his death, ends with the dedication "For Carl". His photo can also be seen in the film.

In 1997 the Sagan Planet Walk was opened in Ithaca, New York. It is a walking-scale model of the Solar System, extending 1.2Â km from the center of The Commons in downtown Ithaca to the Sciencenter, a hands-on museum. The exhibition was created in memory of Carl Sagan, who was an Ithaca resident and Cornell Professor. Professor Sagan had been a founding member of the museum's advisory board.

The landing site of the unmanned "Mars Pathfinder" spacecraft was renamed the Carl Sagan Memorial Station on July 5, 1997. Asteroid 2709Â Sagan is named in his honor, as is the Carl Sagan Institute for the search of habitable planets.

Sagan's son, Nick Sagan, wrote several episodes in the "Star Trek" franchise. In an episode of "" entitled "Terra Prime", a quick shot is shown of the relic rover "Sojourner", part of the "Mars Pathfinder" mission, placed by a historical marker at Carl Sagan Memorial Station on the Martian surface. The marker displays a quote from Sagan: "Whatever the reason you're on Mars, I'm glad you're there, and I wish I was with you." Sagan's student Steve Squyres led the team that landed the rovers "Spirit" and "Opportunity" successfully on Mars in 2004.

On November 9, 2001, on what would have been Sagan's 67th birthday, the Ames Research Center dedicated the site for the Carl Sagan Center for the Study of Life in the Cosmos. "Carl was an incredible visionary, and now his legacy can be preserved and advanced by a 21stÂ century research and education laboratory committed to enhancing our understanding of life in the universe and furthering the cause of space exploration for all time", said NASA Administrator Daniel Goldin. Ann Druyan was at the Center as it opened its doors on October 22, 2006.

Sagan has at least three awards named in his honor:

August 2007 the Independent Investigations Group (IIG) awarded Sagan posthumously a Lifetime Achievement Award. This honor has also been awarded to Harry Houdini and James Randi.

In September 2008, a musical compositor Benn Jordan released his album Pale Blue Dot as a tribute to Carl Sagan's life.

Beginning in 2009, a musical project known as Symphony of Science sampled several excerpts of Sagan from his series "Cosmos" and remixed them to electronic music. To date, the videos have received over 21Â million views worldwide on YouTube.

The 2014 Swedish science fiction short film "Wanderers" uses excerpts of Sagan's narration of his book "Pale Blue Dot", played over digitally-created visuals of humanity's possible future expansion into outer space.

In February 2015, the Finnish-based symphonic metal band Nightwish released the song "Sagan" as a non-album bonus track for their single "Ãlan". The song, written by the band's songwriter/composer/keyboardist Tuomas Holopainen, is an homage to the life and work of the late Carl Sagan.

In August 2015, it was announced that a biopic of Sagan's life was being planned by Warner Bros.

Footnotes
Citations
 


</doc>
<doc id="6827" url="https://en.wikipedia.org/wiki?curid=6827" title="Cuban Missile Crisis">
Cuban Missile Crisis

The Cuban Missile Crisis, also known as the October Crisis of 1962 (), the Caribbean Crisis (), or the Missile Scare, was a 13-day (October 16â28, 1962) confrontation between the United States and the Soviet Union initiated by the American discovery of Soviet ballistic missile deployment in Cuba. The confrontation is often considered the closest the Cold War came to escalating into a full-scale nuclear war.

In response to the failed Bay of Pigs Invasion of 1961 and the presence of American Jupiter ballistic missiles in Italy and Turkey, Soviet leader Nikita Khrushchev agreed to Cuba's request to place nuclear missiles on the island to deter a future invasion. An agreement was reached during a secret meeting between Khrushchev and Fidel Castro in July 1962, and construction of a number of missile launch facilities started later that summer.

Meanwhile, the 1962 United States elections were under way, and the White House had for months denied charges that it was ignoring dangerous Soviet missiles from Florida. The missile preparations were confirmed when an Air Force U-2 spy plane produced clear photographic evidence of medium-range (SS-4) and intermediate-range (R-14) ballistic missile facilities. The US established a naval blockade on October 22 to prevent further missiles from reaching Cuba; Oval Office tapes during the crisis revealed that Kennedy had also put the blockade in place, as an attempt to provoke Soviet-backed forces in Berlin as well. The US announced it would not permit offensive weapons to be delivered to Cuba and demanded that the weapons already in Cuba be dismantled and returned to the Soviet Union.

After several days of tense negotiations, an agreement was reached between US President John F. Kennedy and Khrushchev. Publicly, the Soviets would dismantle their offensive weapons in Cuba and return them to the Soviet Union, subject to United Nations verification, in exchange for a US public declaration and agreement to avoid invading Cuba again. Secretly, the United States agreed that it would dismantle all US-built Jupiter MRBMs, which had been deployed in Turkey against the Soviet Union; there has been debate on whether or not Italy was included in the agreement as well.

When all offensive missiles and Ilyushin Il-28 light bombers had been withdrawn from Cuba, the blockade was formally ended on November 21, 1962. The negotiations between the United States and the Soviet Union pointed out the necessity of a quick, clear, and direct communication line between Washington and Moscow. As a result, the MoscowâWashington hotline was established. A series of agreements later reduced USâSoviet tensions for several years until both parties began to build their nuclear arsenal even further.

With the end of World War II and the start of the Cold War, the United States had grown concerned about the expansion of communism. A Latin American country openly allying with the Soviet Union was regarded by the US as unacceptable. It would, for example, defy the Monroe Doctrine, a US policy limiting US involvement in European colonies and European affairs but holding that the Western Hemisphere was in the US sphere of influence.

The Kennedy administration had been publicly embarrassed by the failed Bay of Pigs Invasion in April 1961, which had been launched under President John F. Kennedy by CIA-trained forces of Cuban exiles. Afterward, former President Dwight Eisenhower told Kennedy that "the failure of the Bay of Pigs will embolden the Soviets to do something that they would otherwise not do." The half-hearted invasion left Soviet premier Nikita Khrushchev and his advisers with the impression that Kennedy was indecisive and, as one Soviet adviser wrote, "too young, intellectual, not prepared well for decision making in crisis situations... too intelligent and too weak". US covert operations against Cuba continued in 1961 with the unsuccessful Operation Mongoose.

In addition, Khrushchev's impression of Kennedy's weaknesses was confirmed by the President's response during the Berlin Crisis of 1961, particularly to the building of the Berlin Wall. Speaking to Soviet officials in the aftermath of the crisis, Khrushchev asserted, "I know for certain that Kennedy doesn't have a strong background, nor, generally speaking, does he have the courage to stand up to a serious challenge." He also told his son Sergei that on Cuba, Kennedy "would make a fuss, make more of a fuss, and then agree".

In January 1962, US Army General Edward Lansdale described plans to overthrow the Cuban government in a top-secret report (partially declassified 1989), addressed to Kennedy and officials involved with Operation Mongoose. CIA agents or "pathfinders" from the Special Activities Division were to be infiltrated into Cuba to carry out sabotage and organization, including radio broadcasts. In February 1962, the US launched an embargo against Cuba, and Lansdale presented a 26-page, top-secret timetable for implementation of the overthrow of the Cuban government, mandating guerrilla operations to begin in August and September. "Open revolt and overthrow of the Communist regime" would occur in the first two weeks of October.

When Kennedy ran for president in 1960, one of his key election issues was an alleged "missile gap" with the Soviets leading. Actually, the US at that time "led" the Soviets by a wide margin that would only increase. In 1961, the Soviets had only four intercontinental ballistic missiles (R-7 Semyorka). By October 1962, they may have had a few dozen, with some intelligence estimates as high as 75.

The US, on the other hand, had 170 ICBMs and was quickly building more. It also had eight - and ballistic missile submarines, with the capability to launch 16 Polaris missiles, each with a range of . Khrushchev increased the perception of a missile gap when he loudly boasted to the world that the Soviets were building missiles "like sausages" but Soviet missiles' numbers and capabilities were nowhere close to his assertions. The Soviet Union had medium-range ballistic missiles in quantity, about 700 of them, but they were very unreliable and inaccurate. The US had a considerable advantage in total number of nuclear warheads (27,000 against 3,600) and in the technology required for their accurate delivery. The US also led in missile defensive capabilities, naval and air power; but the Soviets had a 2â1 advantage in conventional ground forces, more pronounced in field guns and tanks, particularly in the European theater.

In 2017, in "The Putin Interviews" with the director Oliver Stone, the Russian president Vladimir Putin said that the placement of Russian missiles in Cuba was a Russian reaction to the earlier stationing of American missiles in Turkey in 1961-62 (PGM-19 Jupiter). It was Khrushchev's attempt to achieve a balance of power.

In May 1962, Soviet Premier Nikita Khrushchev was persuaded by the idea of countering the US's growing lead in developing and deploying strategic missiles by placing Soviet intermediate-range nuclear missiles in Cuba, despite the misgivings of the Soviet Ambassador in Havana, Alexandr Ivanovich Alexeyev, who argued that Castro would not accept the deployment of the missiles. Khrushchev faced a strategic situation in which the US was perceived to have a "splendid first strike" capability that put the Soviet Union at a huge disadvantage. In 1962, the Soviets had only 20 ICBMs capable of delivering nuclear warheads to the US from inside the Soviet Union. The poor accuracy and reliability of the missiles raised serious doubts about their effectiveness. A newer, more reliable generation of ICBMs would become operational only after 1965.

Therefore, Soviet nuclear capability in 1962 placed less emphasis on ICBMs than on medium and intermediate-range ballistic missiles (MRBMs and IRBMs). The missiles could hit American allies and most of Alaska from Soviet territory but not the Contiguous United States. Graham Allison, the director of Harvard University's Belfer Center for Science and International Affairs, points out, "The Soviet Union could not right the nuclear imbalance by deploying new ICBMs on its own soil. In order to meet the threat it faced in 1962, 1963, and 1964, it had very few options. Moving existing nuclear weapons to locations from which they could reach American targets was one."

A second reason that Soviet missiles were deployed to Cuba was because Khrushchev wanted to bring West Berlin, controlled by the American, British and French within Communist East Germany, into the Soviet orbit. The East Germans and Soviets considered western control over a portion of Berlin a grave threat to East Germany. Khrushchev made West Berlin the central battlefield of the Cold War. Khrushchev believed that if the US did nothing over the missile deployments in Cuba, he could muscle the West out of Berlin using said missiles as a deterrent to western countermeasures in Berlin. If the US tried to bargain with the Soviets after it became aware of the missiles, Khrushchev could demand trading the missiles for West Berlin. Since Berlin was strategically more important than Cuba, the trade would be a win for Khrushchev, as Kennedy recognised: "The advantage is, from Khrushchev's point of view, he takes a great chance but there are quite some rewards to it."

Thirdly, from the perspective of the Soviet Union and of Cuba, it seemed that the United States wanted to increase its presence in Cuba. With actions including the attempt to expel Cuba from the Organization of American States, placing economic sanctions on the nation and conducting secret operations on containing communism and Cuba, it was assumed that America was trying to invade Cuba. As a result, to try and prevent this, the USSR would place missiles in Cuba and neutralise the threat. This would ultimately serve to secure Cuba against attack and keep the country in the Socialist Bloc.
Another major reason why Khrushchev planned to place missiles on Cuba undetected was to "level the playing field" with the evident American nuclear threat. America had the upper hand as they could launch from Turkey and destroy the USSR before they would have a chance to react. After the transmission of nuclear missiles, Khrushchev had finally established mutually assured destruction, meaning that if the U.S. decided to launch a nuclear strike against the USSR, the latter would react by launching a retaliatory nuclear strike against the U.S.

Additionally, placing nuclear missiles on Cuba was a way for the USSR to show their support for Cuba and support the Cuban people who viewed the United States as a threatening force, as the latter had become their ally after the Cuban Revolution of 1959. According to Khrushchev, the Soviet Union's motives were "aimed at allowing Cuba to live peacefully and develop as its people desire".

In early 1962, a group of Soviet military and missile construction specialists accompanied an agricultural delegation to Havana. They obtained a meeting with Cuban leader Fidel Castro. The Cuban leadership had a strong expectation that the US would invade Cuba again and enthusiastically approved the idea of installing nuclear missiles in Cuba. According to another source, Castro objected to the missiles deployment that would have made him look like a Soviet puppet, but he was persuaded that missiles in Cuba would be an irritant to the US and help the interests of the entire socialist camp. Also, the deployment would include short-range tactical weapons (with a range of 40Â km, usable only against naval vessels) that would provide a "nuclear umbrella" for attacks upon the island.

By May, Khrushchev and Castro agreed to place strategic nuclear missiles secretly in Cuba. Like Castro, Khrushchev felt that a US invasion of Cuba was imminent and that to lose Cuba would do great harm to the communists, especially in Latin America. He said he wanted to confront the Americans "with more than words... the logical answer was missiles". The Soviets maintained their tight secrecy, writing their plans longhand, which were approved by Marshal of the Soviet Union Rodion Malinovsky on July 4 and Khrushchev on July 7.

From the very beginning, the Soviets' operation entailed elaborate denial and deception, known as "maskirovka". All the planning and preparation for transporting and deploying the missiles were carried out in the utmost secrecy, with only a very few told the exact nature of the mission. Even the troops detailed for the mission were given misdirection by being told that they were headed for a cold region and being outfitted with ski boots, fleece-lined parkas, and other winter equipment. The Soviet codename was Operation Anadyr. The Anadyr River flows into the Bering Sea, and Anadyr is also the capital of Chukotsky District and a bomber base in the far eastern region. All the measures were meant to conceal the program from both internal and external audiences.

Specialists in missile construction under the guise of "machine operators," "irrigation specialists," and "agricultural specialists" arrived in July. A total of 43,000 foreign troops would ultimately be brought in. Chief Marshal of Artillery Sergei Biryuzov, Head of the Soviet Rocket Forces, led a survey team that visited Cuba. He told Khrushchev that the missiles would be concealed and camouflaged by palm trees.

The Cuban leadership was further upset when in September, the US Congress approved Joint Resolution 230, which expressed Congress's resolve to prevent the creation of an externally-supported military establishment. On the same day, the US announced a major military exercise in the Caribbean, PHIBRIGLEX-62, which Cuba denounced as a deliberate provocation and proof that the US planned to invade Cuba.

The Soviet leadership believed, based on its perception of Kennedy's lack of confidence during the Bay of Pigs Invasion, that he would avoid confrontation and accept the missiles as a "fait accompli". On September 11, the Soviet Union publicly warned that a US attack on Cuba or on Soviet ships that were carrying supplies to the island would mean war. The Soviets continued the "Maskirovka" program to conceal their actions in Cuba. They repeatedly denied that the weapons being brought into Cuba were offensive in nature. On September 7, Soviet Ambassador to the United States Anatoly Dobrynin assured United States Ambassador to the United Nations Adlai Stevenson that the Soviet Union was supplying only defensive weapons to Cuba. On September 11, the Telegrafnoe Agentstvo Sovetskogo Soyuza (Soviet News Agency TASS) announced that the Soviet Union had no need or intention to introduce offensive nuclear missiles into Cuba. On October 13, Dobrynin was questioned by former Undersecretary of State Chester Bowles about whether the Soviets planned to put offensive weapons in Cuba. He denied any such plans. On October 17, Soviet embassy official Georgy Bolshakov brought President Kennedy a personal message from Khrushchev reassuring him that "under no circumstances would surface-to-surface missiles be sent to Cuba."

As early as August 1962, the US suspected the Soviets of building missile facilities in Cuba. During that month, its intelligence services gathered information about sightings by ground observers of Russian-built MiG-21 fighters and Il-28 light bombers. U-2 spyplanes found S-75 Dvina (NATO designation "SA-2") surface-to-air missile sites at eight different locations. CIA director John A. McCone was suspicious. Sending antiaircraft missiles into Cuba, he reasoned, "made sense only if Moscow intended to use them to shield a base for ballistic missiles aimed at the United States". On August 10, he wrote a memo to Kennedy in which he guessed that the Soviets were preparing to introduce ballistic missiles into Cuba.

With important Congressional elections scheduled for November, the crisis became enmeshed in American politics. On August 31, Senator Kenneth Keating (R-New York) warned on the Senate floor that the Soviet Union was "in all probability" constructing a missile base in Cuba. He charged the Kennedy administration with covering up a major threat to the US, thereby starting the crisis. He may have received this initial "remarkably accurate" information from his friend, former congresswoman and ambassador Clare Booth Luce, who in turn received it from Cuban exiles. A later confirming source for Keating's information possibly was the West German ambassador to Cuba, who had received information from dissidents inside Cuba that Soviet troops had arrived in Cuba in early August and were seen working "in all probability on or near a missile base" and who passed this information to Keating on a trip to Washington in early October. Air Force General Curtis LeMay presented a pre-invasion bombing plan to Kennedy in September, and spy flights and minor military harassment from US forces at Guantanamo Bay Naval Base were the subject of continual Cuban diplomatic complaints to the US government.
The first consignment of R-12 missiles arrived on the night of September 8, followed by a second on September 16. The R-12 was a medium-range ballistic missile, capable of carrying a thermonuclear warhead. It was a single-stage, road-transportable, surface-launched, storable liquid propellant fueled missile that could deliver a megaton-class nuclear weapon. The Soviets were building nine sitesâsix for R-12 medium-range missiles (NATO designation "SS-4 Sandal") with an effective range of and three for R-14 intermediate-range ballistic missiles (NATO designation "SS-5 Skean") with a maximum range of .

On October 7, Cuban President Osvaldo DorticÃ³s Torrado spoke at the UN General Assembly: "If... we are attacked, we will defend ourselves. I repeat, we have sufficient means with which to defend ourselves; we have indeed our inevitable weapons, the weapons, which we would have preferred not to acquire, and which we do not wish to employ." On October 10 in another Senate speech Sen. Keating reaffirmed his earlier warning of August 31 and stated that, "Construction has begun on at least a half dozen launching sites for intermediate range tactical missiles."

The missiles in Cuba allowed the Soviets to effectively target most of the Continental US. The planned arsenal was forty launchers. The Cuban populace readily noticed the arrival and deployment of the missiles and hundreds of reports reached Miami. US intelligence received countless reports, many of dubious quality or even laughable, most of which could be dismissed as describing defensive missiles.

Only five reports bothered the analysts. They described large trucks passing through towns at night that were carrying very long canvas-covered cylindrical objects that could not make turns through towns without backing up and maneuvering. Defensive missiles could turn. The reports could not be satisfactorily dismissed.

The United States had been sending U-2 surveillance over Cuba since the failed Bay of Pigs Invasion. The first issue that led to a pause in reconnaissance flights took place on August 30, when a U-2 operated by the US Air Force's Strategic Air Command flew over Sakhalin Island in the Soviet Far East by mistake. The Soviets lodged a protest and the US apologized. Nine days later, a Taiwanese-operated U-2 was lost over western China to an SA-2 surface-to-air missile. US officials were worried that one of the Cuban or Soviet SAMs in Cuba might shoot down a CIA U-2, initiating another international incident. In a meeting with members of the Committee on Overhead Reconnaissance (COMOR) on September 10, Secretary of State Dean Rusk and National Security Advisor McGeorge Bundy heavily restricted further U-2 flights over Cuban airspace. The resulting lack of coverage over the island for the next five weeks became known to historians as the "Photo Gap". No significant U-2 coverage was achieved over the interior of the island. US officials attempted to use a Corona photoreconnaissance satellite to obtain coverage over reported Soviet military deployments, but imagery acquired over western Cuba by a Corona KH-4 mission on October 1 was heavily covered by clouds and haze and failed to provide any usable intelligence. At the end of September, Navy reconnaissance aircraft photographed the Soviet ship "Kasimov", with large crates on its deck the size and shape of Il-28 jet bomber fuselages.

In September 1962, analysts from the Defense Intelligence Agency (DIA) noticed that Cuban surface-to-air missile sites were arranged in a pattern similar to those used by the Soviet Union to protect its ICBM bases, leading DIA to lobby for the resumption of U-2 flights over the island. Although in the past the flights had been conducted by the CIA, pressure from the Defense Department led to that authority being transferred to the Air Force. Following the loss of a CIA U-2 over the Soviet Union in May 1960, it was thought that if another U-2 were shot down, an Air Force aircraft arguably being used for a legitimate military purpose would be easier to explain than a CIA flight.

When the reconnaissance missions were reauthorized on October 9, poor weather kept the planes from flying. The US first obtained U-2 photographic evidence of the missiles on October 14, when a U-2 flight piloted by Major Richard Heyser took 928 pictures on a path selected by DIA analysts, capturing images of what turned out to be an SS-4 construction site at San CristÃ³bal, Pinar del RÃ­o Province (now in Artemisa Province), in western Cuba.

On October 15, the CIA's National Photographic Interpretation Center (NPIC) reviewed the U-2 photographs and identified objects that they interpreted as medium range ballistic missiles. This identification was made, in part, on the strength of reporting provided by Oleg Penkovsky, a double agent in the GRU working for CIA and MI6. Although he provided no direct reports of the Soviet missile deployments to Cuba, technical and doctrinal details of Soviet missile regiments that had been provided by Penkovsky in the months and years prior to the Crisis helped NPIC analysts correctly identify the missiles on U-2 imagery.

That evening, the CIA notified the Department of State and at 8:30Â pm EDT, Bundy chose to wait until the next morning to tell the President. McNamara was briefed at midnight. The next morning, Bundy met with Kennedy and showed him the U-2 photographs and briefed him on the CIA's analysis of the images. At 6:30Â pm EDT, Kennedy convened a meeting of the nine members of the National Security Council and five other key advisors, in a group he formally named the Executive Committee of the National Security Council (EXCOMM) after the fact on October 22 by the National Security Action Memorandum 196. Without informing the members of EXCOMM, President Kennedy tape recorded all of their proceedings, and Sheldon M. Stern, head of the Kennedy library transcribed some of them.

The US had no plan in place because its intelligence had been convinced that the Soviets would never install nuclear missiles in Cuba. EXCOMM, of which Vice President Lyndon B. Johnson was a member, quickly discussed several possible courses of action:


The Joint Chiefs of Staff unanimously agreed that a full-scale attack and invasion was the only solution. They believed that the Soviets would not attempt to stop the US from conquering Cuba. Kennedy was skeptical:

Kennedy concluded that attacking Cuba by air would signal the Soviets to presume "a clear line" to conquer Berlin. Kennedy also believed that US allies would think of the country as "trigger-happy cowboys" who lost Berlin because they could not peacefully resolve the Cuban situation.
The EXCOMM then discussed the effect on the strategic balance of power, both political and military. The Joint Chiefs of Staff believed that the missiles would seriously alter the military balance, but McNamara disagreed. An extra 40, he reasoned, would make little difference to the overall strategic balance. The US already had approximately 5,000 strategic warheads, but the Soviet Union had only 300. McNamara concluded that the Soviets having 340 would not therefore substantially alter the strategic balance. In 1990, he reiterated that "it made "no" difference... The military balance wasn't changed. I didn't believe it then, and I don't believe it now."

The EXCOMM agreed that the missiles would affect the "political" balance. Kennedy had explicitly promised the American people less than a month before the crisis that "if Cuba should possess a capacity to carry out offensive actions against the United States... the United States would act." Also, credibility among US allies and people would be damaged if the Soviet Union appeared to redress the strategic balance by placing missiles in Cuba. Kennedy explained after the crisis that "it would have politically changed the balance of power. It would have appeared to, and appearances contribute to reality."
On October 18, Kennedy met with Soviet Minister of Foreign Affairs, Andrei Gromyko, who claimed the weapons were for defensive purposes only. Not wanting to expose what he already knew and to avoid panicking the American public, Kennedy did not reveal that he was already aware of the missile buildup. By October 19, frequent U-2 spy flights showed four operational sites.

Two Operational Plans (OPLAN) were considered. OPLAN 316 envisioned a full invasion of Cuba by Army and Marine units, supported by the Navy following Air Force and naval airstrikes. Army units in the US would have had trouble fielding mechanized and logistical assets, and the US Navy could not supply enough amphibious shipping to transport even a modest armored contingent from the Army.

OPLAN 312, primarily an Air Force and Navy carrier operation, was designed with enough flexibility to do anything from engaging individual missile sites to providing air support for OPLAN 316's ground forces.

Kennedy met with members of EXCOMM and other top advisers throughout October 21, considering two remaining options: an air strike primarily against the Cuban missile bases or a naval blockade of Cuba. A full-scale invasion was not the administration's first option. McNamara supported the naval blockade as a strong but limited military action that left the US in control. The term "blockade" was problematic. According to international law, a blockade is an act of war, but the Kennedy administration did not think that the Soviets would be provoked to attack by a mere blockade. Additionally, legal experts at the State Department and Justice Department concluded that a declaration of war could be avoided if another legal justification, based on the Rio Treaty for defense of the Western Hemisphere, was obtained from a resolution by a two-thirds vote from the members of the Organization of American States (OAS).

Admiral Anderson, Chief of Naval Operations wrote a position paper that helped Kennedy to differentiate between what they termed a "quarantine" of offensive weapons and a blockade of all materials, claiming that a classic blockade was not the original intention. Since it would take place in international waters, Kennedy obtained the approval of the OAS for military action under the hemispheric defense provisions of the Rio Treaty:

On October 19, the EXCOMM formed separate working groups to examine the air strike and blockade options, and by the afternoon most support in the EXCOMM shifted to the blockade option. Reservations about the plan continued to be voiced as late as the October 21, the paramount concern being that once the blockade was put into effect, the Soviets would rush to complete some of the missiles. Consequently, the US could find itself bombing operational missiles if blockade failed to force Khrushchev to remove the missiles already on the island.

At 3:00Â pm EDT on October 22, President Kennedy formally established the Executive Committee (EXCOMM) with National Security Action Memorandum (NSAM) 196. At 5:00Â pm, he met with Congressional leaders who contentiously opposed a blockade and demanded a stronger response. In Moscow, Ambassador Foy D. Kohler briefed Khrushchev on the pending blockade and Kennedy's speech to the nation. Ambassadors around the world gave notice to non-Eastern Bloc leaders. Before the speech, US delegations met with Canadian Prime Minister John Diefenbaker, British Prime Minister Harold Macmillan, West German Chancellor Konrad Adenauer, French President Charles de Gaulle and Secretary-General of the Organization of American States, JosÃ© Antonio Mora to brief them on the US intelligence and their proposed response. All were supportive of the US position, except Macmillan who advocated appeasement.

Shortly before his speech, Kennedy called former President Dwight Eisenhower. Kennedy's conversation with the former President also revealed that the two were consulting during the Cuban Missile Crisis. The two also anticipated that Khrushchev would respond to the Western world in a manner that was similar to his response during the Suez Crisis and would possibly wind up trading off Berlin.

On October 22 at 7:00Â pm EDT, Kennedy delivered a nationwide televised address on all of the major networks announcing the discovery of the missiles. He noted:

Kennedy described the administration's plan:

During the speech, a directive went out to all US forces worldwide, placing them on DEFCON 3. The heavy cruiser was designated flagship for the blockade, with as "Newport News"s destroyer escort.

On October 23, at 11:24Â am EDT, a cable, drafted by George Wildman Ball to the US Ambassador in Turkey and NATO, notified them that they were considering making an offer to withdraw what the US knew to be nearly-obsolete missiles from Italy and Turkey, in exchange for the Soviet withdrawal from Cuba. Turkish officials replied that they would "deeply resent" any trade involving the US missile presence in their country. Two days later, on the morning of October 25, American journalist Walter Lippmann proposed the same thing in his syndicated column. Castro reaffirmed Cuba's right to self-defense and said that all of its weapons were defensive and Cuba would not allow an inspection.

Three days after Kennedy's speech, the Chinese "People's Daily" announced that "650,000,000 Chinese men and women were standing by the Cuban people." In West Germany, newspapers supported the US response by contrasting it with the weak American actions in the region during the preceding months. They also expressed some fear that the Soviets might retaliate in Berlin. In France on October 23, the crisis made the front page of all the daily newspapers. The next day, an editorial in "Le Monde" expressed doubt about the authenticity of the CIA's photographic evidence. Two days later, after a visit by a high-ranking CIA agent, the newspaper accepted the validity of the photographs. Also in France, in the October 29 issue of "Le Figaro", Raymond Aron wrote in support of the American response. On October 24, Pope John XXIII sent a message to the Soviet embassy in Rome to be transmitted to the Kremlin in which he voiced his concern for peace. In this message he stated, "We beg all governments not to remain deaf to this cry of humanity. That they do all that is in their power to save peace."

The crisis was continuing unabated, and in the evening of October 24, the Soviet news agency TASS broadcast a telegram from Khrushchev to Kennedy in which Khrushchev warned that the United States's "outright piracy" would lead to war. That was followed at 9:24Â pm by a telegram from Khrushchev to Kennedy, which was received at 10:52Â pm EDT. Khrushchev stated, "if you weigh the present situation with a cool head without giving way to passion, you will understand that the Soviet Union cannot afford not to decline the despotic demands of the USA" and that the Soviet Union views the blockade as "an act of aggression" and their ships will be instructed to ignore it. After October 23, Soviet communications with the USA increasingly showed indications of having being rushed. Undoubtedly a product of pressure, it was not uncommon for Khrushchev to repeat himself and send messages lacking simple editing. With President Kennedy making his aggressive intentions of a possible air-strike followed by an invasion on Cuba known, Khrushchev rapidly sought after a diplomatic compromise. Communications between the two super-powers had entered into a unique and revolutionary period; with the newly developed threat of mutual destruction through the deployment of nuclear weapons, diplomacy now demonstrated how power and coercion could dominate negotiations.

The US requested an emergency meeting of the United Nations Security Council on October 25. US Ambassador to the United Nations Adlai Stevenson confronted Soviet Ambassador Valerian Zorin in an emergency meeting of the Security Council, challenging him to admit the existence of the missiles. Ambassador Zorin refused to answer. The next day at 10:00Â pm EDT, the US raised the readiness level of SAC forces to DEFCON 2. For the only confirmed time in US history, B-52 bombers went on continuous airborne alert, and B-47 medium bombers were dispersed to various military and civilian airfields and made ready to take off, fully equipped, on 15Â minutes' notice. One eighth of SAC's 1,436 bombers were on airborne alert, and some 145 intercontinental ballistic missiles stood on ready alert, some of which targeted Cuba, and Air Defense Command (ADC) redeployed 161 nuclear-armed interceptors to 16 dispersal fields within nine hours, with one third maintaining 15-minute alert status. Twenty-three nuclear-armed B-52s were sent to orbit points within striking distance of the Soviet Union so that it would believe that the US was serious. Jack J. Catton later estimated that about 80 percent of SAC's planes were ready for launch during the crisis; David A. Burchinal recalled that, by contrast:

By October 22, Tactical Air Command (TAC) had 511 fighters plus supporting tankers and reconnaissance aircraft deployed to face Cuba on one-hour alert status. TAC and the Military Air Transport Service had problems. The concentration of aircraft in Florida strained command and support echelons, which faced critical undermanning in security, armaments, and communications; the absence of initial authorization for war-reserve stocks of conventional munitions forced TAC to scrounge; and the lack of airlift assets to support a major airborne drop necessitated the call-up of 24 Reserve squadrons.

On October 25 at 1:45Â am EDT, Kennedy responded to Khrushchev's telegram by stating that the US was forced into action after receiving repeated assurances that no offensive missiles were being placed in Cuba, and when the assurances proved to be false, the deployment "required the responses I have announced... I hope that your government will take necessary action to permit a restoration of the earlier situation."
At 7:15Â am EDT on October 25, and attempted to intercept "Bucharest" but failed to do so. Fairly certain that the tanker did not contain any military material, the US allowed it through the blockade. Later that day, at 5:43Â pm, the commander of the blockade effort ordered the destroyer to intercept and board the Lebanese freighter "Marucla". That took place the next day, and "Marucla" was cleared through the blockade after its cargo was checked.

At 5:00Â pm EDT on October 25, William Clements announced that the missiles in Cuba were still actively being worked on. That report was later verified by a CIA report that suggested there had been no slowdown at all. In response, Kennedy issued Security Action Memorandum 199, authorizing the loading of nuclear weapons onto aircraft under the command of SACEUR, which had the duty of carrying out first air strikes on the Soviet Union. During the day, the Soviets responded to the blockade by turning back 14 ships that were presumably carrying offensive weapons. The first indication of this came from a report from the British GCHQ sent to the White House Situation Room containing intercepted communications from Soviet ships reporting their positions. On October 24, "Kislovodsk," a Soviet cargo ship, reported a position north-east of where it had been 24 hours earlier indicating it had "discontinued" its voyage and turned back towards the Baltic. The next day, reports showed more ships originally bound for Cuba had altered their course.

The next morning, October 26, Kennedy informed the EXCOMM that he believed only an invasion would remove the missiles from Cuba. He was persuaded to give the matter time and continue with both military and diplomatic pressure. He agreed and ordered the low-level flights over the island to be increased from two per day to once every two hours. He also ordered a crash program to institute a new civil government in Cuba if an invasion went ahead.

At this point, the crisis was ostensibly at a stalemate. The Soviets had shown no indication that they would back down and had made several comments to the contrary. The US had no reason to believe otherwise and was in the early stages of preparing for an invasion, along with a nuclear strike on the Soviet Union if it responded militarily, which was assumed. Kennedy had no intention of keeping these plans a secret; with an array of Cuban and Soviet spies forever present, Khrushchev was quickly made aware of this looming danger.

The sword of Damocles looming over the USSR in the form of an air strike followed by invasion allowed the United States to exert pressure in future talks. It was precisely this sword that played such an influential role in accelerating Khrushchev's proposal for a compromise. Throughout the closing stages of October, Soviet telegrams were typically rushed and showed signs of immense pressure. Khrushchev's tendency to use platitudinous and ambiguous language assisted the United States in exerting linguistic dominance throughout the compromise negotiations. Leading Soviet figures consistently failed to mention that only Cuban government could agree to inspections of the territory and continually made arrangements relating to Cuba without the knowledge of Fidel Castro himself. According to Dean Rusk, Khrushchev "blinked", he began to panic from the consequences of his own plan and it became clear that his nervousness lead to communicative failures that allowed the US to largely dominate negotiations in late October.

At 1:00Â pm EDT on October 26, John A. Scali of ABC News had lunch with Aleksandr Fomin, the cover name of Alexander Feklisov, the KGB station chief in Washington, at Fomin's request. Following the instructions of the Politburo of the CPSU, Fomin noted, "War seems about to break out." He asked Scali to use his contacts to talk to his "high-level friends" at the State Department to see if the US would be interested in a diplomatic solution. He suggested that the language of the deal would contain an assurance from the Soviet Union to remove the weapons under UN supervision and that Castro would publicly announce that he would not accept such weapons again in exchange for a public statement by the US that it would avoid invading Cuba. The US responded by asking the Brazilian government to pass a message to Castro that the US would be "unlikely to invade" if the missiles were removed.

On October 26 at 6:00Â pm EDT, the State Department started receiving a message that appeared to be written personally by Khrushchev. It was Saturday at 2:00Â am in Moscow. The long letter took several minutes to arrive, and it took translators additional time to translate and transcribe it.

Robert F. Kennedy described the letter as "very long and emotional". Khrushchev reiterated the basic outline that had been stated to Scali earlier in the day: "I propose: we, for our part, will declare that our ships bound for Cuba are not carrying any armaments. You will declare that the United States will not invade Cuba with its troops and will not support any other forces which might intend to invade Cuba. Then the necessity of the presence of our military specialists in Cuba will disappear." At 6:45Â pm EDT, news of Fomin's offer to Scali was finally heard and was interpreted as a "set up" for the arrival of Khrushchev's letter. The letter was then considered official and accurate although it was later learned that Fomin was almost certainly operating of his own accord without official backing. Additional study of the letter was ordered and continued into the night.

Castro, on the other hand, was convinced that an invasion of Cuba was soon at hand, and on October 26, he sent a telegram to Khrushchev that appeared to call for a pre-emptive nuclear strike on the US in case of attack. In a 2010 interview, Castro expressed regret about his earlier stance on first use: "After I've seen what I've seen, and knowing what I know now, it wasn't worth it at all." Castro also ordered all anti-aircraft weapons in Cuba to fire on any US aircraft: the orders had been to fire only on groups of two or more. At 6:00Â am EDT on October 27, the CIA delivered a memo reporting that three of the four missile sites at San Cristobal and the two sites at Sagua la Grande appeared to be fully operational. It also noted that the Cuban military continued to organize for action but was under order not to initiate action unless attacked.

At 9:00Â am EDT on October 27, Radio Moscow began broadcasting a message from Khrushchev. Contrary to the letter of the night before, the message offered a new trade: the missiles on Cuba would be removed in exchange for the removal of the Jupiter missiles from Italy and Turkey. At 10:00Â am EDT, the executive committee met again to discuss the situation and came to the conclusion that the change in the message was because of internal debate between Khrushchev and other party officials in the Kremlin. Kennedy realized that he would be in an "insupportable position if this becomes Khrushchev's proposal" because the missiles in Turkey were not militarily useful and were being removed anyway and "It's gonna â to any man at the United Nations or any other rational man, it will look like a very fair trade." Bundy explained why Khrushchev's public acquiescence could not be considered: "The current threat to peace is not in Turkey, it is in Cuba."

McNamara noted that another tanker, the "Grozny", was about out and should be intercepted. He also noted that they had not made the Soviets aware of the blockade line and suggested relaying that information to them via U Thant at the United Nations.
While the meeting progressed, at 11:03Â am EDT a new message began to arrive from Khrushchev. The message stated, in part:

"You are disturbed over Cuba. You say that this disturbs you because it is ninety-nine miles by sea from the coast of the United States of America. But... you have placed destructive missile weapons, which you call offensive, in Italy and Turkey, literally next to us... I therefore make this proposal: We are willing to remove from Cuba the means which you regard as offensive... Your representatives will make a declaration to the effect that the United States... will remove its analogous means from Turkey... and after that, persons entrusted by the United Nations Security Council could inspect on the spot the fulfillment of the pledges made."

The executive committee continued to meet through the day.

Throughout the crisis, Turkey had repeatedly stated that it would be upset if the Jupiter missiles were removed. Italy's Prime Minister Amintore Fanfani, who was also Foreign Minister "ad interim", offered to allow withdrawal of the missiles deployed in Apulia as a bargaining chip. He gave the message to one of his most trusted friends, Ettore Bernabei, the general manager of RAI-TV, to convey to Arthur M. Schlesinger Jr. Bernabei was in New York to attend an international conference on satellite TV broadcasting. Unknown to the Soviets, the US regarded the Jupiter missiles as obsolescent and already supplanted by the Polaris nuclear ballistic submarine missiles.
On the morning of October 27, a U-2F (the third CIA U-2A, modified for air-to-air refueling) piloted by USAF Major Rudolf Anderson, departed its forward operating location at McCoy AFB, Florida. At approximately 12:00Â pm EDT, the aircraft was struck by an SA-2 surface-to-air missile launched from Cuba. The aircraft was shot down, and Anderson was killed. The stress in negotiations between the Soviets and the US intensified; it was only later believed that the decision to fire the missile was made locally by an undetermined Soviet commander, acting on his own authority. Later that day, at about 3:41Â pm EDT, several US Navy RF-8A Crusader aircraft, on low-level photoreconnaissance missions, were fired upon.

On October 28, 1962, Khrushchev told his son Sergei that the shooting down of Anderson's U-2 was by the "Cuban military at the direction of Raul Castro".

At 4:00Â pm EDT, Kennedy recalled members of EXCOMM to the White House and ordered that a message should immediately be sent to U Thant asking the Soviets to suspend work on the missiles while negotiations were carried out. During the meeting, General Maxwell Taylor delivered the news that the U-2 had been shot down. Kennedy had earlier claimed he would order an attack on such sites if fired upon, but he decided to not act unless another attack was made. Forty years later, McNamara said:

Ellsberg said that Robert Kennedy (RFK) told him in 1964 that after the U-2 was shot down and the pilot killed, he (RFK) told Soviet ambassador Dobrynin, "You have drawn first blood ... . [T]he president had decided against advice ... not to respond militarily to that attack, but he [Dobrynin] should know that if another plane was shot at, ... we would take out all the SAMs and antiaircraft ... . And that would almost surely be followed by an invasion."

Emissaries sent by both Kennedy and Khrushchev agreed to meet at the Yenching Palace Chinese restaurant in the Cleveland Park neighborhood of Washington, DC, on Saturday evening, October 27. Kennedy suggested to take Khrushchev's offer to trade away the missiles. Unknown to most members of the EXCOMM, but with the support of his brother the president, Robert Kennedy had been meeting with the Soviet Ambassador Dobrynin in Washington to discover whether the intentions were genuine. The EXCOMM was generally against the proposal because it would undermine NATO's authority, and the Turkish government had repeatedly stated it was against any such trade.

As the meeting progressed, a new plan emerged, and Kennedy was slowly persuaded. The new plan called for him to ignore the latest message and instead to return to Khrushchev's earlier one. Kennedy was initially hesitant, feeling that Khrushchev would no longer accept the deal because a new one had been offered, but Llewellyn Thompson argued that it was still possible. White House Special Counsel and Adviser Ted Sorensen and Robert Kennedy left the meeting and returned 45Â minutes later, with a draft letter to that effect. The President made several changes, had it typed, and sent it.

After the EXCOMM meeting, a smaller meeting continued in the Oval Office. The group argued that the letter should be underscored with an oral message to Dobrynin that stated that if the missiles were not withdrawn, military action would be used to remove them. Rusk added one proviso that no part of the language of the deal would mention Turkey, but there would be an understanding that the missiles would be removed "voluntarily" in the immediate aftermath. The president agreed, and the message was sent.

At Rusk's request, Fomin and Scali met again. Scali asked why the two letters from Khrushchev were so different, and Fomin claimed it was because of "poor communications". Scali replied that the claim was not credible and shouted that he thought it was a "stinking double cross". He went on to claim that an invasion was only hours away, and Fomin stated that a response to the US message was expected from Khrushchev shortly and urged Scali to tell the State Department that no treachery was intended. Scali said that he did not think anyone would believe him, but he agreed to deliver the message. The two went their separate ways, and Scali immediately typed out a memo for the EXCOMM.

Within the US establishment, it was well understood that ignoring the second offer and returning to the first put Khrushchev in a terrible position. Military preparations continued, and all active duty Air Force personnel were recalled to their bases for possible action. Robert Kennedy later recalled the mood: "We had not abandoned all hope, but what hope there was now rested with Khrushchev's revising his course within the next few hours. It was a hope, not an expectation. The expectation was military confrontation by Tuesday (October 30), and possibly tomorrow (October 29) ..."

At 8:05Â pm EDT, the letter drafted earlier in the day was delivered. The message read, "As I read your letter, the key elements of your proposalsâwhich seem generally acceptable as I understand themâare as follows: 1) You would agree to remove these weapons systems from Cuba under appropriate United Nations observation and supervision; and undertake, with suitable safe-guards, to halt the further introduction of such weapon systems into Cuba. 2) We, on our part, would agreeâupon the establishment of adequate arrangements through the United Nations, to ensure the carrying out and continuation of these commitments (a) to remove promptly the quarantine measures now in effect and (b) to give assurances against the invasion of Cuba." The letter was also released directly to the press to ensure it could not be "delayed". With the letter delivered, a deal was on the table. As Robert Kennedy noted, there was little expectation it would be accepted. At 9:00Â pm EDT, the EXCOMM met again to review the actions for the following day. Plans were drawn up for air strikes on the missile sites as well as other economic targets, notably petroleum storage. McNamara stated that they had to "have two things ready: a government for Cuba, because we're going to need one; and secondly, plans for how to respond to the Soviet Union in Europe, because sure as hell they're going to do something there".

At 12:12Â am EDT, on October 27, the US informed its NATO allies that "the situation is growing shorter... the United States may find it necessary within a very short time in its interest and that of its fellow nations in the Western Hemisphere to take whatever military action may be necessary." To add to the concern, at 6:00Â am, the CIA reported that all missiles in Cuba were ready for action.

On October 27, Khrushchev also received a letter from Castro, what is now known as the Armageddon Letter (dated the day before), which was interpreted as urging the use of nuclear force in the event of an attack on Cuba: "I believe the imperialists' aggressiveness is extremely dangerous and if they actually carry out the brutal act of invading Cuba in violation of international law and morality, that would be the moment to eliminate such danger forever through an act of clear legitimate defense, however harsh and terrible the solution would be," Castro wrote.

Later that same day, what the White House later called "Black Saturday," the US Navy dropped a series of "signaling depth charges" (practice depth charges the size of hand grenades) on a Soviet submarine () at the blockade line, unaware that it was armed with a nuclear-tipped torpedo with orders that allowed it to be used if the submarine was damaged by depth charges or surface fire. As the submarine was too deep to monitor any radio traffic, the captain of the "B-59", Valentin Grigorievitch Savitsky, decided that a war might already have started and wanted to launch a nuclear torpedo. The decision to launch these required agreement from all three officers on board, but one of them, Vasily Arkhipov, objected and so the nuclear launch was narrowly averted.

On the same day a U-2 spy plane made an accidental, unauthorized ninety-minute overflight of the Soviet Union's far eastern coast. The Soviets responded by scrambling MiG fighters from Wrangel Island; in turn, the Americans launched F-102 fighters armed with nuclear air-to-air missiles over the Bering Sea.

On Saturday, October 27, after much deliberation between the Soviet Union and Kennedy's cabinet, Kennedy secretly agreed to remove all missiles set in Turkey and possibly southern Italy, the former on the border of the Soviet Union, in exchange for Khrushchev removing all missiles in Cuba. There is some dispute as to whether removing the missiles from Italy was part of the secret agreement. Khrushchev wrote in his memoirs that it was, and when the crisis had ended McNamara gave the order to dismantle the missiles in both Italy and Turkey.

At this point, Khrushchev knew things the US did not: First, that the shooting down of the U-2 by a Soviet missile violated direct orders from Moscow, and Cuban antiaircraft fire against other US reconnaissance aircraft also violated direct orders from Khrushchev to Castro. Second, the Soviets already had 162 nuclear warheads on Cuba that the US did not then believe were there. Third, the Soviets and Cubans on the island would almost certainly have responded to an invasion by using those nuclear weapons, even though Castro believed that every human in Cuba would likely die as a result. Khrushchev also knew but may not have considered the fact that he had submarines armed with nuclear weapons that the US Navy may not have known about.

Khrushchev knew he was losing control. President Kennedy had been told in early 1961 that a nuclear war would likely kill a third of humanity, with most or all of those deaths concentrated in the US, the USSR, Europe and China; Khrushchev may well have received similar reports from his military.

With this background, when Khrushchev heard Kennedy's threats relayed by Robert Kennedy to Soviet Ambassador Dobrynin, he immediately drafted his acceptance of Kennedy's latest terms from his dacha without involving the Politburo, as he had previously, and had them immediately broadcast over Radio Moscow, which he believed the US would hear. In that broadcast at 9:00Â am EST, on October 28, Khrushchev stated that "the Soviet government, in addition to previously issued instructions on the cessation of further work at the building sites for the weapons, has issued a new order on the dismantling of the weapons which you describe as 'offensive' and their crating and return to the Soviet Union."
At 10:00Â am, October 28, Kennedy first learned of Khrushchev's solution to the crisis with the US removing the 15 Jupiters in Turkey and the Soviets would remove the rockets from Cuba. Khrushchev had made the offer in a public statement for the world to hear. Despite almost solid opposition from his senior advisers, Kennedy quickly embraced the Soviet offer. "This is a pretty good play of his," Kennedy said, according to a tape recording that he made secretly of the Cabinet Room meeting. Kennedy had deployed the Jupiters in March of the year, causing a stream of angry outbursts from Khrushchev. "Most people will think this is a rather even trade and we ought to take advantage of it," Kennedy said. Vice President Lyndon Johnson was the first to endorse the missile swap but others continued to oppose the offer. Finally, Kennedy ended the debate. "We can't very well invade Cuba with all its toil and blood," Kennedy said, "when we could have gotten them out by making a deal on the same missiles on Turkey. If that's part of the record, then you don't have a very good war."

Kennedy immediately responded to Khrushchev's letter, issuing a statement calling it "an important and constructive contribution to peace". He continued this with a formal letter:

Kennedy's planned statement would also contain suggestions he had received from his adviser Schlesinger Jr. in a "Memorandum for the President" describing the "Post Mortem on Cuba".

Kennedy's Oval Office telephone conversation with Eisenhower soon after Khrushchev's message arrived revealed that the President was planning to use the Cuban Missile Crisis to escalate tensions with Khrushchev and in the long run, Cuba as well. The President also claimed that he thought the crisis would result in direct military confrontations in Berlin by the end of the next month. He also claimed in his conversation with Eisenhower that the Soviet leader had offered to withdraw from Cuba in exchange for the withdrawal of missiles from Turkey and that while the Kennedy Administration had agreed not to invade Cuba, they were only in process of determining Khrushchev's offer to withdraw from Turkey.

When former US President Harry Truman called President Kennedy the day of Khrushchev's offer, the President informed him that his Administration had rejected the Soviet leader's offer to withdraw missiles from Turkey and was planning on using the Soviet setback in Cuba to escalate tensions in Berlin.

The US continued the blockade; in the following days, aerial reconnaissance proved that the Soviets were making progress in removing the missile systems. The 42 missiles and their support equipment were loaded onto eight Soviet ships. On November 2, 1962, Kennedy addressed the US via radio and television broadcasts regarding the dismantlement process of the Soviet R-12 missile bases located in the Caribbean region. The ships left Cuba on November 5 to 9. The US made a final visual check as each of the ships passed the blockade line. Further diplomatic efforts were required to remove the Soviet Il-28 bombers, and they were loaded on three Soviet ships on December 5 and 6. Concurrent with the Soviet commitment on the Il-28s, the US government announced the end of the blockade from 6:45Â pm EST on November 20, 1962. 

At the time when the Kennedy administration thought that the Cuban Missile Crisis was resolved, nuclear tactical rockets stayed in Cuba since they were not part of the Kennedy-Khrushchev understandings and the Americans did not know about them. The Soviets changed their minds, fearing possible future Cuban militant steps, and on November 22, 1962, Deputy Premier of the Soviet Union Anastas Mikoyan told Castro that the rockets with the nuclear warheads were being removed as well.

In his negotiations with the Soviet Ambassador Anatoly Dobrynin, Robert Kennedy informally proposed that the Jupiter missiles in Turkey would be removed "within a short time after this crisis was over". The last US missiles were disassembled by April 24, 1963, and were flown out of Turkey soon afterward.

The practical effect of the Kennedy-Khrushchev Pact was that the US would remove their rockets from Italy and Turkey and that the Soviets had no intention of resorting to nuclear war if they were out-gunned by the US. Because the withdrawal of the Jupiter missiles from NATO bases in Italy and Turkey was not made public at the time, Khrushchev appeared to have lost the conflict and become weakened. The perception was that Kennedy had won the contest between the superpowers and that Khrushchev had been humiliated. Both Kennedy and Khrushchev took every step to avoid full conflict despite pressures from their respective governments. Khrushchev held power for another two years.

By the time of the crisis in October 1962, the total amount of nuclear weapons in the stockpiles of each country numbered approximately 26,400 for the United States and 3,300 for the Soviet Union. At the peak of the crisis, the U.S. had some 3,500 nuclear weapons ready to be used on command with a combined yield of approximately 6,300 megatons. The Soviets had considerably less strategic firepower at their disposal (some 300-320 bombs and warheads), lacking submarine-based weapons in a position to threaten the U.S. mainland and having most of their intercontinental delivery systems based on bombers that would have difficulty penetrating North American air defense systems. The U.S. had approximately 4,375 nuclear weapons deployed in Europe, most of which were tactical weapons such as nuclear artillery, with around 450 of them for ballistic missiles, cruise missiles, and aircraft; the Soviets had more than 550 similar weapons in Europe.



The enormity of how close the world came to thermonuclear war impelled Khrushchev to propose a far-reaching easing of tensions with the US. In a letter to President Kennedy dated October 30, 1962, Khrushchev outlined a range of bold initiatives to forestall the possibility of a further nuclear crisis, including proposing a non-aggression treaty between the North Atlantic Treaty Organization (NATO) and the Warsaw Pact or even disbanding these military blocs, a treaty to cease all nuclear weapons testing and even the elimination of all nuclear weapons, resolution of the hot-button issue of Germany by both East and West formally accepting the existence of West Germany and East Germany, and US recognition of the government of mainland China. The letter invited counter-proposals and further exploration of these and other issues through peaceful negotiations. Khrushchev invited Norman Cousins, the editor of a major US periodical and an anti-nuclear weapons activist, to serve as liaison with President Kennedy, and Cousins met with Khrushchev for four hours in December 1962.

Kennedy's response to Khrushchev's proposals was lukewarm but Kennedy expressed to Cousins that he felt constrained in exploring these issues due to pressure from hardliners in the US national security apparatus. The US and the USSR did shortly thereafter agree on a treaty banning atmospheric testing of nuclear weapons, known as the "Partial Nuclear Test Ban Treaty".

Further after the crisis, the US and the Soviet Union created the MoscowâWashington hotline, a direct communications link between Moscow and Washington. The purpose was to have a way that the leaders of the two Cold War countries could communicate directly to solve such a crisis.

The compromise embarrassed Khrushchev and the Soviet Union because the withdrawal of US missiles from Italy and Turkey was a secret deal between Kennedy and Khrushchev. Khrushchev went to Kennedy as he thought that the crisis was getting out of hand, but the Soviets were seen as retreating from circumstances that they had started.

Khrushchev's fall from power two years later was in part because of the Soviet Politburo's embarrassment at both Khrushchev's eventual concessions to the US and this ineptitude in precipitating the crisis in the first place. According to Dobrynin, the top Soviet leadership took the Cuban outcome as "a blow to its prestige bordering on humiliation".

Cuba perceived the outcome as a betrayal by the Soviets, as decisions on how to resolve the crisis had been made exclusively by Kennedy and Khrushchev. Castro was especially upset that certain issues of interest to Cuba, such as the status of the US Naval Base in GuantÃ¡namo, were not addressed. That caused CubanâSoviet relations to deteriorate for years to come. On the other hand, Cuba continued to be protected from invasion.

The worldwide US Forces DEFCON 3 status was returned to DEFCON 4 on November 20, 1962. General Curtis LeMay told the President that the resolution of the crisis was the "greatest defeat in our history"; his was a minority position. He had pressed for an immediate invasion of Cuba as soon as the crisis began and still favored invading Cuba even after the Soviets had withdrawn their missiles. Twenty-five years later, LeMay still believed that "We could have gotten not only the missiles out of Cuba, we could have gotten the Communists out of Cuba at that time."

Critics, including Seymour Melman, and Seymour Hersh suggested that the Cuban Missile Crisis encouraged the United States' use of military means, such as the case in the later Vietnam War.

U-2 pilot Anderson's body was returned to the US and was buried with full military honors in South Carolina. He was the first recipient of the newly created Air Force Cross, which was awarded posthumously. Although Anderson was the only combatant fatality during the crisis, 11 crew members of three reconnaissance Boeing RB-47 Stratojets of the 55th Strategic Reconnaissance Wing were also killed in crashes during the period between September 27 and November 11, 1962. Seven crew died when a Military Air Transport Service Boeing C-135B Stratolifter delivering ammunition to Guantanamo Bay Naval Base stalled and crashed on approach on October 23.

Schlesinger, a historian and adviser to Kennedy, told National Public Radio in an interview on October 16, 2002 that Castro did not want the missiles, but Khrushchev pressured Castro to accept them. Castro was not completely happy with the idea, but the Cuban National Directorate of the Revolution accepted them, both to protect Cuba against US attack and to aid the Soviet Union. Schlesinger believed that when the missiles were withdrawn, Castro was more angry with Khrushchev than with Kennedy because Khrushchev had not consulted Castro before deciding to remove them. Although Castro was infuriated by Khrushchev, he planned on striking the US with remaining missiles if an invasion of the island occurred.

In early 1992, it was confirmed that Soviet forces in Cuba had, when the crisis broke, already received tactical nuclear warheads for their artillery rockets and Il-28 bombers. Castro stated that he would have recommended their use if the US invaded despite Cuba being destroyed.

Arguably, the most dangerous moment in the crisis was not recognized until the Cuban Missile Crisis Havana conference, in October 2002. Attended by many of the veterans of the crisis, they all learned that on October 27, 1962, had tracked and dropped signaling depth charges (the size of hand grenades) on , a Soviet Project 641 (NATO designation ) submarine. Unknown to the US, it was armed with a 15-kiloton nuclear torpedo. Running out of air, the Soviet submarine was surrounded by American warships and desperately needed to surface. An argument broke out among three officers aboard "B-59", including submarine captain Valentin Savitsky, political officer Ivan Semonovich Maslennikov, and Deputy brigade commander Captain 2nd rank (US Navy Commander rank equivalent) Vasily Arkhipov. An exhausted Savitsky became furious and ordered that the nuclear torpedo on board be made combat ready. Accounts differ about whether Arkhipov convinced Savitsky not to make the attack or whether Savitsky himself finally concluded that the only reasonable choice left open to him was to come to the surface. During the conference, McNamara stated that nuclear war had come much closer than people had thought. Thomas Blanton, director of the National Security Archive, said, "A guy called Vasili Arkhipov saved the world."

Fifty years after the crisis, Graham T. Allison wrote:

BBC journalist Joe Matthews published the story, on October 13, 2012, behind the 100 tactical nuclear warheads mentioned by Graham Allison in the excerpt above. Khrushchev feared that Castro's hurt pride and widespread Cuban indignation over the concessions he had made to Kennedy might lead to a breakdown of the agreement between the Soviet Union and the US. To prevent that, Khrushchev decided to offer to give Cuba more than 100 tactical nuclear weapons that had been shipped to Cuba along with the long-range missiles but, crucially, had escaped the notice of US intelligence. Khrushchev determined that because the Americans had not listed the missiles on their list of demands, keeping them in Cuba would be in the Soviet Union's interests.

Anastas Mikoyan was tasked with the negotiations with Castro over the missile transfer deal that was designed to prevent a breakdown in the relations between Cuba and the Soviet Union. While in Havana, Mikoyan witnessed the mood swings and paranoia of Castro, who was convinced that Moscow had made the agreement with the US at the expense of Cuba's defense. Mikoyan, on his own initiative, decided that Castro and his military not be given control of weapons with an explosive force equal to 100 Hiroshima-sized bombs under any circumstances. He defused the seemingly intractable situation, which risked re-escalating the crisis, on November 22, 1962. During a tense, four-hour meeting, Mikoyan convinced Castro that despite Moscow's desire to help, it would be in breach of an unpublished Soviet law, which did not actually exist, to transfer the missiles permanently into Cuban hands and provide them with an independent nuclear deterrent. Castro was forced to give way and, much to the relief of Khrushchev and the rest of the Soviet government, the tactical nuclear weapons were crated and returned by sea to the Soviet Union during December 1962.

The American popular media, especially television, made frequent use of the events of the missile crisis and both fictional and documentary forms. Jim Willis includes the Crisis as one of the 100 "media moments that changed America". Sheldon Stern finds that a half century later there are still many "misconceptions, half-truths, and outright lies" that have shaped media versions of what happened in the White House during those harrowing two weeks.

Historian William Cohn argued in a 1976 article that television programs are typically the main source used by the American public to know about and interpret the past. According to Cold War historian Andrei Kozovoi, the Soviet media proved somewhat disorganized as it was unable to generate a coherent popular history. Khrushchev lost power and was airbrushed out of the story. Cuba was no longer portrayed as a heroic David against the American Goliath. One contradiction that pervaded the Soviet media campaign was between the pacifistic rhetoric of the peace movement that emphasizes the horrors of nuclear war and the militancy of the need to prepare Soviets for war against American aggression.









</doc>
<doc id="6828" url="https://en.wikipedia.org/wiki?curid=6828" title="Aquilegia">
Aquilegia

Aquilegia (common names: granny's bonnet, columbine) is a genus of about 60â70 species of perennial plants that are found in meadows, woodlands, and at higher altitudes throughout the Northern Hemisphere, known for the spurred petals of their flowers.

The genus name "Aquilegia" is derived from the Latin word for eagle ("aquila"), because of the shape of the flower petals, which are said to resemble an eagle's claw. The common name "columbine" comes from the Latin for "dove", due to the resemblance of the inverted flower to five doves clustered together.

The leaves of this plant are compound and the flowers contain five sepals, five petals and five pistils. The fruit is a follicle
which holds many seeds and is formed at the end of the pistils. Underneath the flower are spurs which contain nectar, mainly consumed by long-beaked birds such as hummingbirds.

Columbines are closely related to plants in the genera "Actaea" (baneberries) and "Aconitum" (wolfsbanes/monkshoods), which like "Aquilegia" produce cardiogenic toxins.

They are used as food plants by some Lepidoptera (butterfly and moth) caterpillars. These are mainly of noctuid moths â noted for feeding on many poisonous plants without harm â such as cabbage moth ("Mamestra brassicae"), dot moth ("Melanchra persicariae") and mouse moth ("Amphipyra tragopoginis"). the engrailed ("Ectropis crepuscularia"), a geometer moth, also uses columbine as a larval food plant. The larvae of the "Papaipema leucostigma" also feed on columbine.

Plants in the genus "Aquilegia" are a major food source for "Bombus hortorum", a species of bumblebee. Specifically, they have been found to forage on species of "Aquilegia vulgaris" in Belgium and "Aquilegia chrysantha" in North America and Belgium. The bees do not show any preference in color of the flowers.

Columbine is a hardy perennial, which propagates by seed. It will grow to a height of 15 to 20Â inches. It will grow in full sun; however, it prefers growing in partial shade and well drained soil, and is able to tolerate average soils and dry soil conditions. Columbine is rated at hardiness zone 3 in the United States so does not require mulching or protection in the winter.

Large numbers of hybrids are available for the garden, since the European "A. vulgaris" was hybridized with other European and North American varieties.
The British National Collection of "Aquilegia"s is held by Mrs Carrie Thomas at Killay near Swansea.

The flowers of various species of columbine were consumed in moderation by Native Americans as a condiment with other fresh greens, and are reported to be very sweet, and safe if consumed in small quantities. The plant's seeds and roots are highly poisonous however, and contain cardiogenic toxins which cause both severe gastroenteritis and heart palpitations if consumed as food. Native Americans used very small amounts of "Aquilegia" root as a treatment for ulcers. However, the medical use of this plant is better avoided due to its high toxicity; columbine poisonings may be fatal.

An acute toxicity test in mice has demonstrated that ethanol extract mixed with isocytisoside, the main flavonoid compound from the leaves and stems of "Aquilegia vulgaris", can be classified as non-toxic, since a dose of 3000Â mg/kg did not cause mortality.

The Colorado blue columbine ("A. coerulea") is the official state flower of Colorado (see also Columbine, Colorado).

Columbines have been important in the study of evolution. It was found that the Sierra columbine ("A. pubescens") and crimson columbine ("A. formosa") each has adapted specifically to a pollinator. Bees and hummingbirds are the visitors to "A. formosa", while hawkmoths would only visit "A. pubescens" when given a choice. Such a "pollination syndrome", being due to flower color and orientation controlled by their genetics, ensures reproductive isolation and can be a cause of speciation.
"Aquilegia" petals show an enormous range of petal spur length diversity ranging from a centimeter to the 15Â cm spurs of "Aquilegia longissima". Selection from pollinator shifts is suggested to have driven these changes in nectar spur length. 
It was shown that this spur length diversity is achieved solely through changing cell shape, not cell number or cell size. This suggests that a simple microscopic change can result in a dramatic evolutionarily relevant morphological change.

Columbine species include:




</doc>
<doc id="6829" url="https://en.wikipedia.org/wiki?curid=6829" title="Cache (computing)">
Cache (computing)

In computing, a cache ( , or in AuE) is a hardware or software component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. A "cache hit" occurs when the requested data can be found in a cache, while a "cache miss" occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests that can be served from the cache, the faster the system performs.

To be cost-effective and to enable efficient use of data, caches must be relatively small. Nevertheless, caches have proven themselves in many areas of computing, because typical computer applications access data with a high degree of locality of reference. Such access patterns exhibit temporal locality, where data is requested that has been recently requested already, and spatial locality, where data is requested that is stored physically close to data that has already been requested.

There is an inherent trade-off between size and speed (given that a larger resource implies greater physical distances) but also a tradeoff between expensive, premium technologies (such as SRAM) vs cheaper, easily mass-produced commodities (such as DRAM or hard disks).

The buffering provided by a cache benefits both latency and throughput (bandwidth):

A larger resource incurs a significant latency for access e.g. it can take hundreds of clock cycles for a modern 4Â GHz processor to reach DRAM. This is mitigated by reading in large chunks, in the hope that subsequent reads will be from nearby locations. Prediction or explicit prefetching might also guess where future reads will come from and make requests ahead of time; if done correctly the latency is bypassed altogether.

The use of a cache also allows for higher throughput from the underlying resource, by assembling multiple fine grain transfers into larger, more efficient requests. In the case of DRAM circuits, this might be served by having a wider data bus. For example, consider a program accessing bytes in a 32-bit address space, but being served by a 128-bit off-chip data bus; individual uncached byte accesses would allow only 1/16th of the total bandwidth to be used, and 80% of the data movement would be memory addresses instead of data itself. Reading larger chunks reduces the fraction of bandwidth required for transmitting address information.

Hardware implements cache as a block of memory for temporary storage of data likely to be used again. Central processing units (CPUs) and hard disk drives (HDDs) frequently use a cache, as do web browsers and web servers.

A cache is made up of a pool of entries. Each entry has associated "data", which is a copy of the same data in some "backing store". Each entry also has a "tag", which specifies the identity of the data in the backing store of which the entry is a copy. Tagging allows simultaneous cache-oriented algorithms to function in multilayered fashion without differential relay interference.

When the cache client (a CPU, web browser, operating system) needs to access data presumed to exist in the backing store, it first checks the cache. If an entry can be found with a tag matching that of the desired data, the data in the entry is used instead. This situation is known as a cache hit. For example, a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL. In this example, the URL is the tag, and the content of the web page is the data. The percentage of accesses that result in cache hits is known as the hit rate or hit ratio of the cache.

The alternative situation, when the cache is checked and found not to contain any entry with the desired tag, is known as a cache miss. This requires a more expensive access of data from the backing store. Once the requested data is retrieved, it is typically copied into the cache, ready for the next access.

During a cache miss, some other previously existing cache entry is removed in order to make room for the newly retrieved data. The heuristic used to select the entry to replace is known as the replacement policy. One popular replacement policy, "least recently used" (LRU), replaces the oldest entry, the entry that was accessed less recently than any other entry (see cache algorithm). More efficient caching algorithms compute the use-hit frequency against the size of the stored contents, as well as the latencies and throughputs for both the cache and the backing store. This works well for larger amounts of data, longer latencies, and slower throughputs, such as that experienced with hard drives and networks, but is not efficient for use within a CPU cache.

When a system writes data to cache, it must at some point write that data to the backing store as well. The timing of this write is controlled by what is known as the "write policy". There are two basic writing approaches:


A write-back cache is more complex to implement, since it needs to track which of its locations have been written over, and mark them as "dirty" for later writing to the backing store. The data in these locations are written back to the backing store only when they are evicted from the cache, an effect referred to as a "lazy write". For this reason, a read miss in a write-back cache (which requires a block to be replaced by another) will often require two memory accesses to service: one to write the replaced data from the cache back to the store, and then one to retrieve the needed data.

Other policies may also trigger data write-back. The client may make many changes to data in the cache, and then explicitly notify the cache to write back the data.

Since no data is returned to the requester on write operations, a decision needs to be made on write misses, whether or not data would be loaded into the cache.
This is defined by these two approaches:


Both write-through and write-back policies can use either of these write-miss policies, but usually they are paired in this way:


Entities other than the cache may change the data in the backing store, in which case the copy in the cache may become out-of-date or "stale". Alternatively, when the client updates the data in the cache, copies of those data in other caches will become stale. Communication protocols between the cache managers which keep the data consistent are known as coherency protocols.

Small memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop and server microprocessors may have as many as six types of cache (between levels and functions). Examples of caches with a specific function are the D-cache and I-cache and the translation lookaside buffer for the MMU.

Earlier graphics processing units (GPUs) often had limited read-only texture caches, and introduced morton order swizzled textures to improve 2D cache coherency. Cache misses would drastically affect performance, e.g. if mipmapping was not used. Caching was important to leverage 32-bit (and wider) transfers for texture data that was often as little as 4Â bits per pixel, indexed in complex patterns by arbitrary UV coordinates and perspective transformations in inverse texture mapping.

As GPUs advanced (especially with GPGPU compute shaders) they have developed progressively larger and increasingly general caches, including instruction caches for shaders, exhibiting increasingly common functionality with CPU caches. For example, GT200 architecture GPUs did not feature an L2 cache, while the Fermi GPU has 768Â KB of last-level cache, the Kepler GPU has 1536Â KB of last-level cache, and the Maxwell GPU has 2048Â KB of last-level cache. These caches have grown to handle synchronisation primitives between threads and atomic operations, and interface with a CPU-style MMU.

Digital signal processors have similarly generalised over the years. Earlier designs used scratchpad memory fed by DMA, but modern DSPs such as Qualcomm Hexagon often include a very similar set of caches to a CPU (e.g. Modified Harvard architecture with shared L2, split L1 I-cache and D-cache).

A memory management unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations. This specialized cache is called a translation lookaside buffer (TLB).

Information-centric networking (ICN) is an approach to evolve the Internet infrastructure away from a host-centric paradigm, based on perpetual connectivity and the end-to-end principle, to a network architecture in which the focal point is identified information (or content or data). Due to the inherent caching capability of the nodes in an ICN, it can be viewed as a loosely connected network of caches, which has unique requirements of caching policies. However, ubiquitous content caching introduces the challenge to content protection against unauthorized access, which requires extra care and solutions.
Unlike proxy servers, in ICN the cache is a network-level solution. Therefore, it has rapidly changing cache states and higher request arrival rates; moreover, smaller cache sizes further impose a different kind of requirements on the content eviction policies. In particular, eviction policies for ICN should be fast and lightweight. Various cache replication and eviction schemes for different ICN architectures and applications have been proposed.

The Time aware Least Recently Used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid life time. The algorithm is suitable in network cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general. TLRU introduces a new term: TTU (Time to Use). TTU is a time stamp of a content/page which stipulates the usability time for the content based on the locality of the content and the content publisher announcement. Owing to this locality based time stamp, TTU provides more control to the local administrator to regulate in network storage.
In the TLRU algorithm, when a piece of content arrives, a cache node calculates the local TTU value based on the TTU value assigned by the content publisher. The local TTU value is calculated by using a locally defined function. Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node. The TLRU ensures that less popular and small life content should be replaced with the incoming content.
The Least Frequent Recently Used (LFRU) cache replacement scheme combines the benefits of LFU and LRU schemes. LFRU is suitable for âin networkâ cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general. In LFRU, the cache is divided into two partitions called privileged and unprivileged partitions. The privileged partition can be defined as a protected partition. If content is highly popular, it is pushed into the privileged partition. Replacement of the privileged partition is done as follows: LFRU evicts content from the unprivileged partition, pushes content from privileged partition to unprivileged partition, and finally inserts new content into the privileged partition. In the above procedure the LRU is used for the privileged partition and an approximated LFU (ALFU) scheme is used for the unprivileged partition, hence the abbreviation LFRU.
The basic idea is to filter out the locally popular contents with ALFU scheme and push the popular contents to one of the privileged partition.

While CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel.

While the disk buffer, which is an integrated part of the hard disk drive, is sometimes misleadingly referred to as "disk cache", its main functions are write sequencing and read prefetching. Repeated cache hits are relatively rare, due to the small size of the buffer in comparison to the drive's capacity. However, high-end disk controllers often have their own on-board cache of the hard disk drive's data blocks.

Finally, a fast local hard disk drive can also cache information held on even slower data storage devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management. Also, fast flash-based solid-state drives (SSDs) can be used as caches for slower rotational-media hard disk drives, working together as hybrid drives or solid-state hybrid drives (SSHDs).

Web browsers and web proxy servers employ web caches to store previous responses from web servers, such as web pages and images. Web caches reduce the amount of information that needs to be transmitted across the network, as information previously stored in the cache can often be re-used. This reduces bandwidth and processing requirements of the web server, and helps to improve responsiveness for users of the web.

Web browsers employ a built-in web cache, but some Internet service providers (ISPs) or organizations also use a caching proxy server, which is a web cache that is shared among all users of that network.

Another form of cache is P2P caching, where the files most sought for by peer-to-peer applications are stored in an ISP cache to accelerate P2P transfers. Similarly, decentralised equivalents exist, which allow communities to perform the same task for P2P traffic, for example, Corelli.

A cache can store data that is computed on demand rather than retrieved from a backing store. Memoization is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation. It is related to the dynamic programming algorithm design methodology, which can also be thought of as a means of caching.

The BIND DNS daemon caches a mapping of domain names to IP addresses, as does a resolver library.

Write-through operation is common when operating over unreliable networks (like an Ethernet LAN), because of the enormous complexity of the coherency protocol required between multiple write-back caches when communication is unreliable. For instance, web page caches and client-side network file system caches (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.

Search engines also frequently make web pages they have indexed available from their cache. For example, Google provides a "Cached" link next to each search result. This can prove useful when web pages from a web server are temporarily or permanently inaccessible.

Another type of caching is storing computed results that will likely be needed again, or memoization. For example, ccache is a program that caches the output of the compilation, in order to speed up later compilation runs.

Database caching can substantially improve the throughput of database applications, for example in the processing of indexes, data dictionaries, and frequently used subsets of data.

A distributed cache uses networked hosts to provide scalability, reliability and performance to the application. The hosts can be co-located or spread over different geographical regions.

The semantics of a "buffer" and a "cache" are not totally different; even so, there are fundamental differences in intent between the process of caching and the process of buffering.

Fundamentally, caching realizes a performance increase for transfers of data that is being repeatedly transferred. While a caching system may realize a performance increase upon the initial (typically write) transfer of a data item, this performance increase is due to buffering occurring within the caching system.

With read caches, a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache's intermediate storage, deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand,


With typical caching implementations, a data item that is read or written for the first time is effectively being buffered; and in the case of a write, mostly realizing a performance increase for the application from where the write originated. Additionally, the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering. The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering, although this form may negatively impact the performance of at least the initial reads (even though it may positively impact the performance of the sum of the individual reads). In practice, caching almost always involves some form of buffering, while strict buffering does not involve caching.

A buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices. Thus, addressable memory is used as an intermediate stage. Additionally, such a buffer may be feasible when a large block of data is assembled or disassembled (as required by a storage device), or when data may be delivered in a different order than that in which it is produced. Also, a whole buffer of data is usually transferred sequentially (for example to hard disk), so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer's latency as opposed to caching where the intent is to reduce the latency. These benefits are present even if the buffered data are written to the buffer once and read from the buffer once.

A cache also increases transfer performance. A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block. But the main performance-gain occurs because there is a good chance that the same data will be read from cache multiple times, or that written data will soon be read. A cache's sole purpose is to reduce accesses to the underlying slower storage. Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers.



</doc>
<doc id="6830" url="https://en.wikipedia.org/wiki?curid=6830" title="Columbus, Indiana">
Columbus, Indiana

Columbus is a city in and the county seat of Bartholomew County, Indiana, United States. The population was 44,061 at the 2010 census. The relatively small city has provided a unique place for noted Modern architecture and public art, commissioning numerous works since the mid-20th century; the annual program Exhibit Columbus celebrates this legacy. Located about south of Indianapolis, on the east fork of the White River, it is the state's 20th-largest city. It is the principal city of the Columbus, Indiana metropolitan statistical area, which encompasses all of Bartholomew County. Columbus is the birthplace of former Indiana Governor and current Vice President of the United States, Mike Pence.

"National Geographic Traveler" ranked Columbus 11th on its historic destinations list in late 2008, describing the city as "authentic, unique, and unspoiled." Columbus won the national contest "America in Bloom" in 2006, and in 2004 it was named as one of "The Ten Most Playful Towns" by "Nick Jr. Family Magazine". The July 2005 edition of "GQ" magazine, Columbus was named as one of the "62 Reasons to Love Your Country". Columbus is the headquarters of the engine company Cummins, Inc.

The land developed as Columbus was bought by General John Tipton and Luke Bonesteel in 1820. Tipton built a log cabin on Mount Tipton, a small hill overlooking White River and the surrounding flat, heavily forested and swampy valley. It held wetlands of the river. The town was first known as Tiptonia, named in honor of Tipton. The town's name was changed to Columbus on March 20, 1821. General Tipton was upset by the name change and decided to leave the newly founded town. He was later appointed as the highway commissioner for the State of Indiana and was assigned to building a highway from Indianapolis, Indiana to Louisville, Kentucky. When the road reached Columbus, Tipton constructed the first bypass road ever built; it detoured south around the west side of Columbus en route to Seymour.

Joseph McKinney was the first to plot the town of Columbus, but no date was recorded.

Local history books for years said that the land on which Columbus sits was donated by General Tipton. But in 2003, Historic Columbus Indiana acquired a deed showing that General Tipton sold the land.

A ferry was established below the confluence of the Flatrock and Driftwood rivers, which form the White River. A village of three or four log cabins developed around the ferry landing, and a store was added in 1821. Later that year, Bartholomew County was organized by an act of the State Legislature and named to honor the famous Hoosier militiaman, General Joseph Bartholomew. Columbus was incorporated on June 28, 1864.

The first railroad in Indiana was constructed to Columbus from Madison, Indiana in 1844. This eventually became the Madison branch of the Pennsylvania Railroad. The railroad fostered the growth of the community into one of the largest in Indiana, and three more railroads reached the city by 1850.

Columbus is host to the oldest theater in Indiana, The Crump Theatre, which was built in 1889 by John Crump. Today the building is included within the Columbus Historic District. Before it closed permanently in 2010, it was an all-ages venue with occasional musical performances. Columbus was host to the oldest continually operated bookstore in Indiana, Cummins Bookstore, which began operations in 1892. It closed in late 2007.

The Irwin Union Bank building was built in 1954. It was designated as a National Historic Landmark by the National Park Service in 2001 in recognition of its unique architecture. The building consists of a one-story bank structure adjacent to a three-story office annex. A portion of the office annex was built along with the banking hall in 1954. The remaining larger portion, designed by Kevin Roche John Dinkeloo and Associates, was built in 1973. Eero Saarinen designed the bank building with its glazed hall to be set off against the blank background of its three-story brick annex. Two steel and glass vestibule connectors lead from the north side of this structure to the annex. The building was designed to distance the Irwin Union Bank from traditional banking architecture, which mostly echoed imposing, neoclassical style buildings of brick or stone. Tellers were behind iron bars and removed from their customers. Saarinen worked to develop a building that would welcome customers rather than intimidate them.

Columbus has been home to many manufacturing companies, including Noblitt-Sparks Industries (which built radios under the Arvin brand in the 1930s) and Arvin Industries, now Meritor, Inc. After merging with Meritor Automotive on July 10, 2000, the headquarters of the newly created ArvinMeritor Industries was established in Troy, Michigan, the home of parent company, Rockwell International. It was announced in February 2011 that the company name would revert to Meritor, Inc. Cummins, Inc. is by far the region's largest employer, and the Infotech Park accounts for a sizable number of research jobs in Columbus proper. Just south of Columbus are the North American headquarters of Toyota Material Handling, U.S.A., Inc., the world's largest material handling (forklift) manufacturer. Other notable industries include architecture, a discipline for which Columbus is famous worldwide. The late J. Irwin Miller (then president and chairman of Cummins Engine Company) launched the Cummins Foundation, a charitable program that helps subsidize a large number of architectural projects throughout the city by up-and-coming engineers and architects.

Early in the 20th century, Columbus also was home to a number of pioneering car manufacturers, including Reeves, which produced the unusual four-axle Octoauto and the twin rear-axle Sextoauto, both around 1911.

Nearly 19,000 workers commute into the city from the surrounding townships and villages. In recent years city officials have explored ways to revitalize the city. They recognize the value of J. Irwin Miller's support of architectural excellence in the mid-20th century, when the Cummins Foundation made it a mecca of modern architecture. Economic development, widespread beautification innovations, various tax incentives, and increased law enforcement have helped Columbus overcome what some considered a slump during the 1980s and 1990s.

In addition to the Columbus Historic District and Irwin Union Bank, the city has numerous buildings listed on the National Register of Historic Places, including seven National Historic Landmarks of modernist architecture: Bartholomew County Courthouse, Columbus City Hall, First Baptist Church, First Christian Church, Haw Creek Leather Company, Mabel McDowell Elementary School, McEwen-Samuels-Marr House, McKinley School, Miller House, North Christian Church, and The Republic.

Columbus is located at (39.213998, â85.911056). The Driftwood and Flatrock Rivers converge at Columbus to form the East Fork of the White River.

According to the 2010 census, Columbus has a total area of , of which (or 98.62%) is land and (or 1.38%) is water.

Columbus is served by the Columbus Municipal Airport (KBAK). It is located approximately three miles north of Columbus. The airport handles approximately 40,500 operations per year, with roughly 87% general aviation, 4% air taxi, 8% military and <1% commercial service. The airport has two concrete runways; a 6,401 foot runway with approved ILS and GPS approaches (Runway 5-23) and a 5,001 foot crosswind runway, also with GPS approaches, (Runway 14-32).

As of the census of 2010, there were 44,061 people, 17,787 households, and 11,506 families residing in the city. The population density was . There were 19,700 housing units at an average density of . The racial makeup of the city was 86.9% White, 2.7% African American, 0.2% Native American, 5.6% Asian, 0.1% Pacific Islander, 2.5% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 5.8% of the population.

There were 17,787 households of which 33.5% had children under the age of 18 living with them, 48.5% were married couples living together, 11.7% had a female householder with no husband present, 4.5% had a male householder with no wife present, and 35.3% were non-families. 29.7% of all households were made up of individuals and 11.5% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.00.

The median age in the city was 37.1 years. 25.2% of residents were under the age of 18; 8.1% were between the ages of 18 and 24; 27.3% were from 25 to 44; 24.9% were from 45 to 64; and 14.4% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.

As of the census of 2000, there were 39,059 people, 15,985 households, and 10,566 families residing in the city. The population density was 1,505.3 people per square mile (581.1/kmÂ²). There were 17,162 housing units at an average density of 661.4 per square mile (255.3/kmÂ²). The racial makeup of the city was 91.32% White, 2.71% Black or African American, 0.13% Native American, 3.23% Asian, 0.05% Pacific Islander, 1.39% from other races, and 1.19% from two or more races. 2.81% of the population were Hispanic or Latino of any race.

There were 15,985 households out of which 31.8% had children under the age of 18 living with them, 51.9% were married couples living together, 11.0% had a female householder with no husband present, and 33.9% were non-families. 29.1% of all households were composed of individuals and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.39, and the average family size was 2.94.

In the city, the population was spread out with 25.7% under the age of 18, 8.0% from 18 to 24 years, 29.5% from 25 to 44 years, 23.0% from 45 to 64 years, and 13.7% over the age of 65. The median age was 36 years. There were 92.8 males for every 100 females and 89.6 males for every 100 females over age 18.

The median income for a household in the city was $41,723, and the median income for a family was $52,296. Males had a median income of $40,367 versus $24,446 for females, and the per capita income was $22,055. About 6.5% of families and 8.1% of the population were below the poverty line, including 9.7% of those under age 18 and 8.8% of those age 65 or over.

Columbus is a city known for its modern architecture and public art. J. Irwin Miller, 2nd CEO and a nephew of a co-founder of Cummins Inc., the Columbus-headquartered diesel engine manufacturer, instituted a program in which the Cummins Foundation paid the architects' fees, provided the client selected a firm from a list compiled by the foundation. The plan was initiated with public schools and was so successful that the foundation decided to offer such design support to other non-profit and civic organizations. The high number of notable public buildings and public art in the Columbus area, designed by such individuals as Eero Saarinen, I.M. Pei, Robert Venturi, Cesar Pelli, and Richard Meier, led to Columbus earning the nickname "Athens on the Prairie."

Seven buildings, constructed between 1942 and 1965, are National Historic Landmarks, and approximately 60 other buildings sustain the Bartholomew County seat's reputation as a showcase of modern architecture. National Public Radio once devoted an article to the town's architecture.

In 2015, Landmark Columbus was created as a program of Heritage Fund - The Community Foundation of Bartholomew county.





In May 2016, Landmark Columbus launched Exhibit Columbus as a way to continue the ambitious traditions of the past into the future. Exhibit Columbus features annual programming that alternates between symposium and exhibition years.

Columbus High School was home to footwear pioneer Chuck Taylor, who played basketball in Columbus before setting out to promote his now famous shoes and the sport of basketball before being inducted into the Naismith Memorial Basketball Hall of Fame.

Two local high schools compete within the state in various sports. Columbus North and Columbus East both have competitive athletics and have many notable athletes that go on to compete in college and beyond. Columbus North High School houses one of the largest high school gyms in the United States. CNHS vs CEHS

Indiana Diesels of the Premier Basketball League play their home games at the gymnasium at Ceraland Park, with plans to move to a proposed downtown sports complex in the near future. Columbus also boasts a roller derby league, the Terrorz of Tiny Towns. Established in 2010, this league hosts weekly practices at Columbus Skateland. The town also has two cricket teams, both which play under the name of Columbus Indiana Cricket Club; their home ground is at Ceraland Park.

Columbus boasts over of parks and green space and over 20 miles of People Trails. These amenities, in addition to several athletic and community facilities, including Donner Aquatic Center, Lincoln Park Softball Complex, Hamilton Center Ice Arena, Clifty Park, Foundation for Youth/Columbus Gymnastics Center and The Commons, are managed and maintained by the Columbus Parks and Recreation Department.

Columbus uses the Mayor-Council form of government. The council consists of seven members. Five are elected from one of five wards the other two are elected at-large. The Mayor is elected in a citywide vote. The current mayor is Jim Lienhoop.

This is a list of notable people who were born in, or who currently live, or have lived in Columbus.

Columbus is home to a robust learning system that connects learners of all ages to opportunities.

The Bartholomew Consolidated School Corporation(BCSC) operates the largest public school system in the county, serving a diverse student population of more than 11,000 students. Reflecting the county's diversity, there are 54 home languages represented in the student population. The district is committed to providing students with the technology they need to be successful. The computer to student ratio is 1:1.

Bartholomew Consolidated School Corp. offers different pathways for individualized education that include:


Columbus has a public library, a branch of the Bartholomew County Public Library.

Secondary education includes Indiana University â Purdue University Columbus (IUPUC), an Ivy Tech campus, Purdue Polytechnic and an Indiana Wesleyan University education center.






</doc>
<doc id="6834" url="https://en.wikipedia.org/wiki?curid=6834" title="List of computer scientists">
List of computer scientists

This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.

Some persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.


























</doc>
<doc id="6836" url="https://en.wikipedia.org/wiki?curid=6836" title="Cultural production and nationalism">
Cultural production and nationalism

Literature, visual arts, music, and scholarship have complex relationships with ideological forces.

In the 19th century nationalism was an especially potent influence on all of these fields. To summarize, every established national group used cultural productions to assert and strengthen a sense of national unity and destiny; less politically consolidated groups, especially those pursuing the goal of nationhood, used them in the same ways, though often with a note of determination that makes them easier to see from our contemporary point of reference.

Natural admiration for excellence and justifiable pride in a predecessor's achievements is sometimes difficult to sort out from other intentions. Dante was a great poet, the Societa Dantesca Italiana did great work in editing and publishing a usable and affordable text, but the "Divine Comedy" was certainly used by the newly unified Italian government (see History of Italy) to encourage a more homogeneous, Tuscan-influenced dialect for the whole peninsula (see Italian language).

This relationship between ideology and serious work is particularly ambiguous in the academic fields of historical importance. Much as 19th century science is often treated as the inventor of conceptions of evolution and race which had serious negative political and social consequences, many 19th century historians pursued what they intended as reasonably objective research projects in the history of their own and other regions either to end by themselves using the results to support nationalistic goals or to see their work used that way by others. 

More politically consolidated nations sponsored historical research projects which produced results of permanent value - such as the "Monumenta Germaniae Historica" ("Monuments of German History") project. The "MGH" is a vast series (it runs to hundreds of volumes and is still publishing) of edited primary source material essential for scholarly work on late Antiquity and the Middle Ages. However, the term "German" in the title was interpreted in the broadest possible sense, and its initial royal patronage made the connection clear between a perceived unity of Germanness in history and 19th century Germanness.



</doc>
<doc id="6839" url="https://en.wikipedia.org/wiki?curid=6839" title="Reaction kinetics in uniform supersonic flow">
Reaction kinetics in uniform supersonic flow

Reaction kinetics in uniform supersonic flow (, CRESU ) is an experiment investigating chemical reactions taking place at very low temperatures.

The technique involves the expansion of a gas or mixture of gases through a de Laval nozzle from a high pressure reservoir into a vacuum chamber. As it expands, the nozzle collimates the gas into a uniform supersonic beam that is essentially collision free and has a temperature that, in the centre of mass frame, can be significantly below that of the reservoir gas. Each nozzle produces a characteristic temperature. This way, any temperature between room temperature and about 10K can be achieved.

There are relatively few CRESU apparatuses in existence for the simple reason that the gas throughput and pumping requirements are huge, which makes them expensive to run. Two of the leading centres have been the University of Rennes (France) and the University of Birmingham (UK). A more recent development has been a pulsed version of the CRESU, which requires far less gas and therefore smaller pumps.

Most species have a negligible vapour pressure at such low temperatures and this means that they quickly condense on the sides of the apparatus. Essentially, the CRESU technique provides a "wall-less flow tube," which allows the kinetics of gas phase reactions to be investigated at much lower temperatures than otherwise possible.

Chemical kinetics experiments can then be carried out in a pump-probe fashion using a laser to initiate the reaction (for example by preparing one of the reagents by photolysis of a precursor), followed by observation of that same species (for example by laser-induced fluorescence) after a known time delay. The fluorescence signal is captured by a photomultiplier a known distance downstream of the de Laval nozzle. The time delay can be varied up to the maximum corresponding to the flow time over that known distance. By studying how quickly the reagent species disappears in the presence of differing concentrations of a (usually stable) co-reagent species the reaction rate constant at the low temperature of the CRESU flow can be determined.

Reactions studied by the CRESU technique typically have no significant activation energy barrier. In the case of neutral-neutral reactions (i.e., not involving any charged species, ions), these type of barrier-free reactions usually involve free radical species such as molecular oxygen (O), the cyanide radical (CN) or the hydroxyl radical (OH). The energetic driving force for these reactions is typically an attractive long range intermolecular potential.

CRESU experiments have been used to show deviations from Arrhenius kinetics at low temperatures: as the temperature is reduced, the rate constant actually increases. They can explain why chemistry is so prevalent in the interstellar medium, where many different polyatomic species have been detected (by radio astronomy).



</doc>
<doc id="6840" url="https://en.wikipedia.org/wiki?curid=6840" title="Cygwin">
Cygwin

Cygwin ( ) is a POSIX-compatible environment that runs natively on Microsoft Windows. Its goal is to allow programs of Unix-like systems to be recompiled and run natively on Windows with minimal source code modifications by providing them with the same underlying POSIX API they would expect in those systems. 

The Cygwin installation directory behaves like the root and follows a similar directory layout to that found in Unix-like systems, with familiar directories like /bin, /home, /etc, /usr, /var available within it, and includes by default hundreds of programs and command-line tools commonly found in the Unix world, plus the terminal emulator Mintty which is the default command-line interface tool provided to interact with the environment.

Cygwin provides native integration of Windows-based applications, data, and other system resources with applications, software tools, and data of the Unix-like environment. Thus it is possible to launch Windows applications from the Cygwin environment, as well as to use Cygwin tools and applications within the Windows operating context.

Cygwin consists of two parts: a dynamic-link library (DLL) as an API compatibility layer in the form of a C standard library providing a substantial part of the POSIX API functionality, and an extensive collection of software tools and applications that provide a Unix-like look and feel.

Cygwin was originally developed by Cygnus Solutions, which was later acquired by Red Hat (now part of IBM), to port the GNU/Linux toolchain to Win32, including the GNU Compiler Suite. Rather than rewrite all the tools to use Win32 runtimes, Cygwin implemented a POSIX compatible runtime as a DLL. It is free and open-source software, released under the GNU Lesser General Public License version 3. Today it is maintained by volunteers including employees of Red Hat and many others.

Cygwin consists of a library that implements the POSIX system call API in terms of Win32 system calls, a GNU development toolchain (including GCC and GDB) to allow software development, and running of a large number of application programs equivalent to those on Unix systems. Programmers have ported many Unix, GNU, BSD and Linux programs and packages to Cygwin, including the X Window System, K Desktop Environment 3, GNOME, Apache, and TeX. Cygwin permits installing inetd, syslogd, sshd, Apache, and other daemons as standard Windows services, allowing Microsoft Windows systems to emulate Unix and Linux servers.

Cygwin programs are installed by running Cygwin's "setup" program, which downloads the necessary program and feature package files from repositories on the Internet. Setup can install, update, and remove programs and their source code packages. A complete installation will take in excess of 90Â GB of hard disk space, but usable configurations may require as little as 1 or 2Â GB.

Efforts to reconcile concepts that differ between Unix and Windows systems include:

The version of gcc that comes with Cygwin has various extensions for creating Windows DLLs, specifying whether a program is a windowing or console-mode program, adding resources, etc. Support for compiling programs that do not require the POSIX compatibility layer provided by the Cygwin DLL used to be included in the default codice_13, but is provided by cross-compilers contributed by the MinGW-w64 project.

Cygwin is used heavily for porting many popular pieces of software to the Windows platform. It is used to compile Sun Java, LibreOffice, and even web server software like Lighttpd and Hiawatha.

The Cygwin API library is licensed under the GNU Lesser General Public License version 3 (or later) with an exception to allow linking to any free and open-source software whose license conforms to the Open Source Definition (less strict than the Free Software Definition).

Cygwin began in 1995 as a project of Steve Chamberlain, a Cygnus engineer who observed that Windows NT and 95 used COFF as their object file format, and that GNU already included support for x86 and COFF, and the C library newlib. He thought it would be possible to retarget GCC and produce a cross compiler generating executables that could run on Windows. This proved practical and a prototype was quickly developed.

The next step was to attempt to bootstrap the compiler on a Windows system, requiring sufficient emulation of Unix to let the GNU configure shell script run. A Bourne shell-compatible command interpreter, such as bash, was needed and in turn a fork system call emulation and standard input/output. Windows includes similar functionality, so the Cygwin library just needed to provide a POSIX-compatible application programming interface (API) and properly translate calls and manage private versions of data, such as file descriptors.

Initially, Cygwin was called gnuwin32 (not to be confused with the current GnuWin32 project). The name was changed to Cygwin32 to emphasize Cygnus' role in creating it. When Microsoft registered the trademark Win32, the 32 was dropped to simply become Cygwin.

By 1996, other engineers had joined in, because it was clear that Cygwin would be a useful way to provide Cygnus' embedded tools hosted on Windows systems (the previous strategy had been to use DJGPP). It was especially attractive because it was possible to do a three-way cross-compile, for instance to use a hefty Sun Microsystems workstation to build, say, a Windows-x-MIPS cross-compiler, which was faster than using the PC at the time. In 1999, Cygnus offered Cygwin 1.0 as a commercial product of interest in its own right although subsequent versions have not been released, instead relying on continued open source releases.

Geoffrey Noer was the project lead from 1996 to 1999. Christopher Faylor was the project lead from 1999 to mid-2014. Corinna Vinschen became co-lead since 2004 when Faylor left Red Hat and has been lead since mid-2014, when Faylor withdrew from active participation in the project.

Cygwin's base package selection is fairly small (about 100Â MB), containing little more than the bash (interactive user) and dash (installation) shells and the core file and text manipulation utilities expected of a Unix command line. Additional packages are available as optional installs from within Cygwin's package manager ("setup-x86.exe" â 32bit & "setup-x86_64.exe" â 64bit). These include (among many others):

The Cygwin/X project contributes an implementation of the X Window System that allows graphical Unix programs to display their user interfaces on the Windows desktop. This can be used with both local and remote programs. Cygwin/X supports over 500 packages including major X window managers, desktop environments, and applications, for example:

In addition to the low-level Xlib/XCB libraries for developing X applications, Cygwin also ships with various higher-level and cross-platform GUI frameworks, including GTK+ and Qt.

The Cygwin Ports project provided many additional packages that were not available in the Cygwin distribution itself. Examples included GNOME and K Desktop Environment 3 as well as the MySQL database and the PHP scripting language. Most ports have been adopted by volunteer maintainers as Cygwin packages, and Cygwin Ports are no longer maintained. 

Several open-source and proprietary alternatives provide simultaneous access to both Windows and UNIX environments on the same hardware.

Toolsets like Windows Subsystem for Linux, Microsoft Windows Services for UNIX (SFU), UWIN, MKS Toolkit for Enterprise Developers, and Hamilton C shell also aim to provide a Unix-like user- and development-environment. They implement at least a shell and a set of the most popular utilities. Most include the familiar GNU and/or Unix development tools, including make, yacc, lex, and a cc command that acts a wrapper around a supported C compiler. SFU also includes the GCC compiler.

MinGW provides a native software port of the GCC to Microsoft Windows, along with a set of freely-distributable import libraries and header files for the Windows API. MinGW allows developers to create native Microsoft Windows applications. In addition, a component of MinGW known as MSYS ("Minimal SYStem"), which derives from Cygwin version 1.3.3, provides a minimal Unix-like shell environment including bash and a selection of POSIX tools sufficient to enable autoconf scripts to run.

Numerous virtualization solutions provide x86 platform virtualization to run Windows and Unix-like operating systems simultaneously on the same hardware, but without the integration of the environments that Cygwin provides. Some, like VirtualBox and VMware Player run on Windows and Linux hosts and can run many other operating systems. Cooperative Linux (abbreviated "coLinux") runs a full, but modified Linux kernel like a driver under Windows, effectively making Windows and Linux two coroutines, using cooperative multitasking to switch between them.

Winelib, a part of the Wine project, is the inverse of Cygwin â it is a free and open-source compatibility layer for Unix-like operating systems on the x86 or x86-64 architecture that can allow programs written for Microsoft Windows to run on Unix-like operating systems. Unlike Cygwin, which requires "You rebuild your application from source if you want it to run on Windows", the full Wine product supports executing unmodified Windows binaries. Cygwin supports, runs under, and is supported under Wine. 


</doc>
<doc id="6845" url="https://en.wikipedia.org/wiki?curid=6845" title="Corinth">
Corinth

Corinth (; Modern , "KÃ³rinthos", is the successor to an ancient city, and is a former municipality in Corinthia, Peloponnese, which is located in south-central Greece. Since the 2011 local government reform it is part of the municipality of Corinth, of which it is the seat and a municipal unit. It is the capital of Corinthia.

It was founded as Nea Korinthos or New Corinth (ÎÎ­Î± ÎÏÏÎ¹Î½Î¸Î¿Ï) in 1858 after an earthquake destroyed the existing settlement of Corinth, which had developed in and around the site of ancient Corinth.

Located about west of Athens, Corinth is surrounded by the coastal townlets of (clockwise) Lechaio, Isthmia, Kechries, and the inland townlets of Examilia and the archaeological site and village of ancient Corinth. Natural features around the city include the narrow coastal plain of Vocha, the Corinthian Gulf, the Isthmus of Corinth cut by its canal, the Saronic Gulf, the Oneia Mountains, and the monolithic rock of Acrocorinth, where the medieval acropolis was built.

Corinth derives its name from Ancient Corinth, a city-state of antiquity. The site was occupied from before 3000 BC. But historical sources about the city concerns the early 8th century BC, when Corinth began to develop as a commercial center. Between the 8th and 7th centuries, the Bacchiad family ruled Corinth. Cypselus overthrew the Bacchiad family, and between 657 and 550 BC, he and his son Periander ruled Corinth as the Tyrants. 

In about 550 BC, an oligarchical government seized power. This government allied with Sparta within the Peloponnesian League, and Corinth participated in the Persian Wars and Peloponnesian War as an ally of Sparta. After Sparta's victory in the Peloponnesian war, the two allies fell out with one another, and Corinth pursued an independent policy in the various wars of the early 4th century BC. After the Macedonian conquest of Greece, the Acrocorinth was the seat of a Macedonian garrison until 243 BC, when the city was liberated and joined the Achaean League. Nearly a century later, in 146 BC, Corinth was captured and was completely destroyed by the Roman army.

As a newly rebuilt Roman colony in 44 BC, Corinth flourished and became the administrative capital of the Roman province of Achaea.

In 1858, the old city, now known as Ancient Corinth (ÎÏÏÎ±Î¯Î± ÎÏÏÎ¹Î½Î¸Î¿Ï, "Archaia Korinthos"), located south-west of the modern city, was totally destroyed by a magnitude 6.5 earthquake. New Corinth ("Nea Korinthos") was then built to the north-east of it, on the coast of the Gulf of Corinth. In 1928 a magnitude 6.3 earthquake devastated the new city, which was then rebuilt on the same site. In 1933 there was a great fire, and the new city was rebuilt again.

The Municipality of Corinth (ÎÎ®Î¼Î¿Ï ÎÎ¿ÏÎ¹Î½Î¸Î¯ÏÎ½) had a population of 58,192 according to the 2011 census, the second most populous municipality in the Peloponnese Region after Kalamata. The municipal unit of Corinth had 38,132 inhabitants, of which Corinth itself had 30,176 inhabitants, placing it in third place behind Kalamata and Tripoli among the cities of the Peloponnese Region.

The municipal unit of Corinth (ÎÎ·Î¼Î¿ÏÎ¹ÎºÎ® ÎµÎ½ÏÏÎ·ÏÎ± ÎÎ¿ÏÎ¹Î½Î¸Î¯ÏÎ½) includes apart from Corinth proper the town of Archaia Korinthos (2,198 inhabitants in 2011), the town of Examilia (2,905 inhabitants), and the smaller settlements of Xylokeriza (1,316 inhabitants) and Solomos (817 inhabitants). The municipal unit has an area of 102.187Â km.

Corinth is a major industrial hub at a national level. The Corinth Refinery is one of the largest oil refining industrial complexes in Europe. Ceramic tiles, copper cables, gums, gypsum, marble, meat products (including leather), medical equipment, mineral water and beverages, petroleum products, and salt are produced nearby. , a period of deindustrialization commenced as a large pipework complex, a textile factory and a meat packing facility diminished their operations.

Corinth is a major road hub. The A7 toll motorway for Tripoli and Kalamata, (and Sparta via A71 toll), branches off the A8/European route E94 toll motorway from Athens at Corinth. Corinth is the main entry point to the Peloponnesian peninsula, the southernmost area of continental Greece.

KTEL Korinthias provides intercity bus service in the peninsula and to Athens via the Isthmos station southeast of the city center. Local bus service is also available.

In 2005, the city was connected to the Proastiakos, the Athens suburban rail network, following the completion of the new Corinth railway station.

The port of Corinth, located north of the city centre and close to the northwest entrance of the Corinth Canal, at 37 56.0â N / 22 56.0â E, serves the local needs of industry and agriculture. It is mainly a cargo exporting facility.

It is an artificial harbour (depth approximately , protected by a concrete mole (length approximately 930 metres, width 100 metres, mole surface 93,000 m2). A new pier finished in the late 1980s doubled the capacity of the port. The reinforced mole protects anchored vessels from strong northern winds.

Within the port operates a customs office facility and a Hellenic Coast Guard post. Sea traffic is limited to trade in the export of local produce, mainly citrus fruits, grapes, marble, aggregates and some domestic imports. The port operates as a contingency facility for general cargo ships, bulk carriers and ROROs, in case of strikes at Piraeus port.

There was formerly a ferry link to Catania, Sicily and Genoa in Italy.

The Corinth Canal, carrying ship traffic between the western Mediterranean Sea and the Aegean Sea, is about east of the city, cutting through the Isthmus of Corinth that connects the Peloponnesian peninsula to the Greek mainland, thus effectively making the former an island. The builders dug the canal through the Isthmus at sea level; no locks are employed. It is in length and only wide at its base, making it impassable for most modern ships. It now has little economic importance.

The canal was mooted in classical times and an abortive effort was made to build it in the 1st century AD. Julius Caesar and Caligula both considered digging the canal but died before starting the construction. The emperor Nero was the first to attempt to construct the canal. The Roman workforce responsible for the initial digging consisted of 6,000 Jewish prisoners of war. Modern construction started in 1882, after Greece gained independence from the Ottoman Empire, but was hampered by geological and financial problems that bankrupted the original builders. It was completed in 1893, but due to the canal's narrowness, navigational problems and periodic closures to repair landslips from its steep walls, it failed to attract the level of traffic anticipated by its operators. It is now used mainly for tourist traffic.

The city's association football team is Korinthos F.C. ("Î .Î.E. ÎÏÏÎ¹Î½Î¸Î¿Ï"), established in 1999 after the merger of Pankorinthian Football Club ("Î Î±Î³ÎºÎ¿ÏÎ¹Î½Î¸Î¹Î±ÎºÏÏ") and Corinth Football Club ("ÎÏÏÎ¹Î½Î¸Î¿Ï"). During the 2006â2007 season, the team played in the Greek Fourth Division's Regional Group 7. The team went undefeated that season and it earned the top spot. This granted the team a promotion to the Gamma EthnikÃ­ (Third Division) for the 2007â2008 season. For the 2008â2009 season, Korinthos F.C. competed in the Gamma Ethniki (Third Division) southern grouping.

Corinth is twinned with:


Due to its ancient history and the presence of St. Paul the Apostle in Corinth some locations all over the world have been named Corinth.




</doc>
<doc id="6846" url="https://en.wikipedia.org/wiki?curid=6846" title="Colossae">
Colossae

Colossae (; Greek: ÎÎ¿Î»Î¿ÏÏÎ±Î¯) was an ancient city of Phrygia in Asia Minor, and one of the most celebrated cities of southern Anatolia (modern Turkey). The Epistle to the Colossians, an early Christian text traditionally attributed to Paul the Apostle, is addressed to the church in Colossae. A significant city from the 5th century BC onwards, it had dwindled in importance by the time of Paul, but was notable for the existence of its local angel cult. It was part of the Roman â and then Byzantine â province of Phrygia Pacatiana, before being destroyed in 1192/3 and its population relocating to nearby Chonai (modern day Honaz).

Colossae was located in Phrygia, in Asia Minor. It was located 15Â km southeast of Laodicea on the road through the Lycus Valley near the Lycus River at the foot of Mt. Cadmus, the highest mountain in Turkey's western Aegean Region, and between the cities Sardeis and Celaenae, and southeast of the ancient city of Hierapolis. At Colossae, Herodotus describes how, "the river Lycos falls into an opening of the earth and disappears from view, and then after an interval of about five furlongs it comes up to view again, and this river also flows into the Maiander." Despite a treacherously ambiguous cartography and history, Colossae has been clearly distinguished in modern research from nearby "Chonai" (Î§á¿¶Î½Î±Î¹), now called Honaz, with what remains of the buried ruins of Colossae ("the mound") lying 3Â km to the north of Honaz.

The medieval poet Manuel Philes, incorrectly, imagined that the name "Colossae" was connected to the Colossus of Rhodes. More recently, in an interpretation which ties Colossae to an Indo-European root that happens to be shared with the word "kolossos", Jean-Pierre Vernant has connected the name to the idea of setting up a sacred space or shrine. Another proposal relates the name to the Greek "kolazo", "to punish". Others believe the name derives from the manufacture of its famous dyed wool, or "colossinus".

The first mention of the city may be in a 17th-century BC Hittite inscription, which speaks of a city called HuwaluÅ¡ija, which some archeologists believe refer to early Colossae. The Fifth Century geographer Herodotus first mentions Colossae by name and as a "great city in Phrygia", which accommodates the Persian King Xerxes I while en route to wage war against the Greeks - showing the city had already reached a certain level of wealth and size by this time. 
Writing in the 5th Century BC, Xenophon refers to Colossae as "a populous city, wealthy and of considerable magnitude". It was famous for its wool trade. Strabo notes that the city drew great revenue from the flocks, and that the wool of Colossae gave its name to colour "colossinus". 

In 396 BC, Colossae was the site of the execution of the rebellious Persian satrap Tissaphernes who was lured there and slain by an agent of the party of Cyrus the Younger.

Although during the Hellenistic period, the town was of some mercantile importance, by the 1st century it had dwindled greatly in size and significance. Paul's letter to the Colossians point to the existence of an early Christian community. The town was known for its fusion of religious influences (syncretism), which included Jewish, Gnostic, and pagan influences that in the first century AD were described as an angel-cult. This unorthodox cult venerated the archangel Michael who is said to have caused a curative spring to gush from a fissure in the Earth.

The canonical biblical text Epistle to the Colossians is addressed to the Christian community in Colossae. The epistle has traditionally been attributed to Paul the Apostle due to its autobiographical salutation & style, but some modern critical scholars now believe it to be written by another author some time after Paul's death. It is believed that one aim of the letter was to address the challenges that the Colossian community faced in its context of the syncretistic Gnostic religions that were developing in Asia Minor.

According to the Epistle to the Colossians, Epaphras seems to have been a person of some importance in the Christian community in Colossae (; ), and tradition presents him as its first bishop.
The epistle also seems to imply that Paul had never visited the city, because it only speaks of him having "heard" of the Colossians' faith (), and in the Epistle to Philemon Paul tells Philemon of his hope to visit Colossae upon being freed from prison (see ). Tradition also gives Philemon as the second bishop of the see.

The city was decimated by an earthquake in the 60s AD, and was rebuilt independent of the support of Rome.

The Apostolic Constitutions list Philemon as a Bishop of Colossae. On the other hand, the Catholic Encyclopedia considers Philemon doubtful. 

The first historically documented bishop is Epiphanius, who was not personally at the Council of Chalcedon, but whose metropolitan bishop Nunechius of Laodicea, the capital of the Roman province of Phrygia Pacatiana signed the acts on his behalf.

The city's fame and renowned status continued into the Byzantine period, and in 858, it was distinguished as a Metropolitan See. The Byzantines also built the church of St. Michael in the vicinity of Colossae, one of the largest church buildings in the Middle East. Nevertheless, sources suggest that the town may have decreased in size or may even been completely abandoned due to Arab invasions in the seventh and eighth centuries, forcing the population to flee to resettle in the nearby city of Chonai (modern day Honaz).

Colossae's famous church was destroyed in 1192/3 during the Byzantine civil wars. It was a suffragan diocese of Laodicea in Phyrigia Pacatiane but was replaced in the Byzantine period by the Chonae settlement on higher ground

As of 2019, Colossae has never been excavated, as most archeological attention has been focused on nearby Laodicea and Hierapolis, though plans are reported for an Australian-led expedition to the site. The present site exhibits a biconical acropolis almost 100 feet high, and encompasses an area of almost 22 acres. On the eastern slope there sits a theater which probably seated around 5,000 people, suggesting a total population of 25,000 - 30,000 people. The theater was probably built during the Roman period, and may be near an agora that abuts the "Cardo Maximus", or the city's main north-south road. Ceramic finds around the theater confirm the city's early occupation in the third and second millennia BC. Northeast of the tell, and most likely outside the city walls, a necropolis displays Hellenistic tombs with two main styles of burial: one with an antecedent room connected to an inner chamber, and tumuli, or underground chambers accessed by stairs leading to the entrance. Outside the tell there are also remains of sections of columns that may have marked a processional way or the "cardo". Today, the remains of one column marks the location where locals believe a church once stood, possibly that of St. Michael. Near the Lycus River, there is evidence that water channels had been cut out of the rock with a complex of pipes and sluice gates to divert water for bathing and for agricultural and industrial purposes. 

The holiness and healing properties associated with the waters of Colossae during the Byzantine Era continue to this day, particularly at a pool fed by the Lycus River at the GÃ¶z picnic grounds west of Colossae at the foot of Mt. Cadmus. Locals consider the water to be therapeutic. 




</doc>
<doc id="6848" url="https://en.wikipedia.org/wiki?curid=6848" title="Charge of the Goddess">
Charge of the Goddess

The Charge is the promise of the Goddess (who is embodied by the high priestess) to all witches that she will teach and guide them. It has been called "perhaps the most important single theological document in the neo-Pagan movement". It is used not only in Wicca, but as part of the foundational documents of the Reclaiming (neopaganism) tradition of witchcraft co-founded by Starhawk.

Several versions of the Charge exist, though they all have the same basic premise, that of a set of instructions given by the Great Goddess to her worshippers. The best-known version is that compiled by Gerald Gardner. This version, titled "Leviter Veslis"or "Lift Up the Veil", includes material paraphrased from works by Aleister Crowley, primarily from Liber AL (The Book of the Law, particularly from Ch 1, spoken by Nuit, the Star Goddess), and from Liber LXV (The Book of the Heart Girt with a Serpent) and from Crowley's essay "The Law of Liberty", thus linking modern Wicca to the cosmology and revelations of Thelema. It has been shown that Gerald Gardner's book collection which was acquired by Ripley's Believe It or Not! included a copy of Crowley's The Blue Equinox" (1919) which includes all of the Crowley quotations transferred by Gardner to the Charge of the Goddess. 

There are also two versions written by Doreen Valiente in the mid-1950s, after her 1953 Wiccan initiation. The first was a poetic paraphrase which eliminated almost all the material derived from Leland and Crowley. The second was a prose version which is contained within the traditional Gardnerian Book of Shadows and more closely resembles Gardner's "Leviter Veslis" version of 1949.

Several different versions of a Wiccan "Charge of the God" have since been created to mirror and accompany the "Charge of the Goddess".

The opening paragraph names a collection of goddesses, some derived from Greek or Roman mythology, others from Celtic or Arthurian legends, affirming a belief that these various figures represent a single Great Mother:

This theme echoes the ancient Roman belief that the Goddess Isis was known by ten thousand names and also that the Goddess still worshipped today by Wiccans and other neopagans is known under many guises but is in fact one universal divinity.

The second paragraph is largely derived and paraphrased from the words that Aradia, the messianic daughter of Diana, speaks to her followers in Charles Godfrey Leland's 1899 book "Aradia, or the Gospel of the Witches" (London: David Nutt; various reprints). The third paragraph is largely written by Doreen Valiente, with a significant content of phrases loosely from "The Book of the Law" and "The Book of the Heart Girt with the Serpent" by Aleister Crowley.

The charge affirms that "all" acts of love and pleasure are sacred to the Goddess e.g.

"Let my worship be within the heart that rejoices,
for behold, all acts of love and pleasure are my rituals.

Therefore, let there be beauty and strength,

power and compassion, honor and humility,
mirth and reverence within you."

In book eleven, chapter 47 of Apuleius's "The Golden Ass", Isis delivers what Ceisiwr Serith calls "essentially a charge of a goddess". This is rather different from the modern version known in Wicca, though they have the same premise, that of the rules given by a great Mother Goddess to her faithful.

The Charge of the Goddess is also known under the title "Leviter Veslis". This has been identified by the historian Ronald Hutton, cited in an article by Roger Dearnsley "The Influence of Aleister Crowley on "Ye Bok of Ye Art Magical", as a piece of medieval ecclesiastical Latin used to mean "lifting the veil." However, Hutton's interpretation does not reflect the Latin grammar as it currently stands. It may represent Gardner's attempt to write "Levetur Velis", which has the literal meaning of "Let the veil be lifted." This expression would, by coincidence or design, grammatically echo the famous "fiat lux" ("Gen. 1:3") of the Latin Vulgate.

The earliest known Wiccan version is found in a document dating from the late 1940s, Gerald Gardner's ritual notebook titled "Ye Bok of Ye Art Magical" (formerly in the collection of Ripley's International, Toronto).The oldest identifiable source contained in this version is the final line, which is traceable to the 17th-century "Centrum Naturae Concentratum" of Alipili (or Ali Puli). This version also draws extensively from Charles Godfrey Leland's "Aradia, or the Gospel of the Witches" (1899) and other modern sources, particularly from the works of Aleister Crowley.

It is believed to have been compiled by Gerald Gardner or possibly another member of the New Forest coven. Gardner intended his version to be a theological statement justifying the Gardnerian sequence of initiations. Like the Charge found in Freemasonry, where the charge is a set of instructions read to a candidate standing in a temple, the Charge of the Goddess was intended to be read immediately before an initiation.

Valiente felt that the influence of Crowley on the Charge was too obvious, and she did not want "the Craft" ( a common term for Wicca) associated with Crowley. Gardner invited her to rewrite the Charge. She proceeded to do so, her first version being into verse.

The initial verse version by Doreen Valiente consisted of eight verses, the second of which was :

Valiente was unhappy with this version, saying that "people seemed to have some difficulty with this, because of the various goddess-names which they found hard to pronounce", and so she rewrote it as a prose version, much of which differs from her initial version, and is more akin to Gardner's version. This prose version has since been modified and reproduced widely by other authors.




</doc>
<doc id="6849" url="https://en.wikipedia.org/wiki?curid=6849" title="Cy Young">
Cy Young

Denton True "Cy" Young (March 29, 1867 â November 4, 1955) was an American Major League Baseball (MLB) pitcher. Born in Gilmore, Ohio, he worked on his family's farm as a youth before starting his professional baseball career. Young entered the major leagues in 1890 with the National League's Cleveland Spiders and pitched for them until 1898. He was then transferred to the St. Louis Cardinals franchise. In 1901, Young jumped to the American League and played for the Boston Red Sox franchise until 1908, helping them win the 1903 World Series. He finished his career with the Cleveland Naps and Boston Rustlers, retiring in 1911.

Young was one of the hardest-throwing pitchers in the game early in his career. After his speed diminished, he relied more on his control and remained effective into his forties. By the time Young retired, he had established numerous pitching records, some of which have stood for over a century. He holds MLB records for the most career wins, with 511, along with most career innings pitched, games started, and complete games. He led his league in wins during five seasons and pitched three no-hitters, including a perfect game.

Young was elected to the National Baseball Hall of Fame in 1937. In 1956, one year after his death, the Cy Young Award was created to honor the best pitcher in each league for each season.

Cy Young was the oldest child born to McKinzie Young, Jr. a German American and Nancy Mottmiller. He was christened Denton True Young. The couple had four more children: Jesse Carlton, Alonzo, Ella, and Anthony. When the couple married, McKinzie's father gave him the of farm land he owned. Young was born in Gilmore, a tiny farming community located in Washington Township, Tuscarawas County, Ohio. 

He was raised on one of the local farms and went by the name Dent Young in his early years. Young was also known as "Farmer Young" and "Farmboy Young". Young stopped his formal education after he completed the sixth grade so he could help out on the family's farm. In 1885, Young moved with his father to Nebraska, and in the summer of 1887, they returned to Gilmore.

Young played for many amateur baseball leagues during his youth, including a semi-professional Carrollton team in 1888. Young pitched and played second base. The first box score known containing the name Young came from that season. In that game, Young played first base and had three hits in three at-bats. After the season, Young received an offer to play for the minor league Canton team, which started Young's professional career.

Young began his professional career in 1889 with the Canton, Ohio, team of the Tri-State League, a professional minor league. During his tryout, Young impressed the scouts, recalling years later, "I almost tore the boards off the grandstand with my fast ball." Cy Young's nickname came from the fences that he had destroyed using his fastball. The fences looked like a cyclone had hit them. Reporters later shortened the name to "Cy", which became the nickname Young used for the rest of his life. During Young's one year with the Canton team, he won 15 games and lost 15.

Franchises in the National League, the major professional baseball league at the time, wanted the best players available to them. Therefore, in 1890, Young signed with the Cleveland Spiders, a team which had moved from the American Association to the National League the previous year.

On August 6, 1890, Young's major league debut, he pitched a three-hit 8â1 victory over the Chicago Colts. While Young was on the Spiders, Chief Zimmer was his catcher more often than any other player. Bill James, a baseball statistician, estimated that Zimmer caught Young in more games than any other battery in baseball history.

Early on, Young established himself as one of the harder-throwing pitchers in the game. Bill James wrote that Zimmer often put a piece of beefsteak inside his baseball glove to protect his catching hand from Young's fastball. In the absence of radar guns, however, it is impossible to say just how hard Young actually threw. Young continued to perform at a high level during the 1890 season. On the last day of the season, Young won both games of a doubleheader. In the first weeks of Young's career, Cap Anson, the player-manager of the Chicago Colts spotted Young's ability. Anson told Spiders manager Gus Schmelz, "He's too green to do your club much good, but I believe if I taught him what I know, I might make a pitcher out of him in a couple of years. He's not worth it now, but I'm willing to give you $1,000 ($ today) for him." Schmelz replied, "Cap, you can keep your thousand and we'll keep the rube."
Two years after Young's debut, the National League moved the pitcher's position back by . Since 1881, pitchers had pitched within a "box" whose front line was from home base, and since 1887 they had been compelled to toe the back line of the box when delivering the ball. The back line was away from home. In 1893, was added to the back line, yielding the modern pitching distance of . In the book "The Neyer/James Guide to Pitchers", sports journalist Rob Neyer wrote that the speed with which pitchers like Cy Young, Amos Rusie, and Jouett Meekin threw was the impetus that caused the move.

The 1892 regular season was a success for Young, who led the National League in wins (36), ERA (1.93), and shutouts (9). Just as many contemporary Minor League Baseball leagues operate today, the National League was using a split season format during the 1892 season. The Boston Beaneaters won the first-half title, and the Spiders won the second-half title, with a best-of-nine series determining the league champion. Despite the Spiders' second half run, the Beaneaters swept the series, five games to none. Young pitched three complete games in the series, but lost two decisions. He also threw a complete game shutout, but the game ended in a 0â0 tie.

The Spiders faced the Baltimore Orioles in the Temple Cup, a precursor to the World Series, in 1895. Young won three games in the series and Cleveland won the Cup, four games to one. It was around this time that Young added what he called a "slow ball" to his pitching repertoire to reduce stress on his arm. The pitch today is called a changeup.

In 1896, Young lost a no-hitter with two outs in the ninth inning when Ed Delahanty of the Philadelphia Phillies hit a single. On September 18, 1897, Young pitched the first no-hitter of his career in a game against the Cincinnati Reds. Although Young did not walk a batter, the Spiders committed four errors while on defense. One of the errors had originally been ruled a hit, but the Cleveland third baseman sent a note to the press box after the eighth inning, saying he had made an error, and the ruling was changed. Young later said, that, despite his teammate's gesture, he considered the game to be a one-hitter.

Prior to the 1899 season, Frank Robison, the Spiders owner, bought the St. Louis Browns, thus owning two clubs simultaneously. The Browns were renamed the "Perfectos", and restocked with Cleveland talent. Just weeks before the season opener, most of the better Spiders players were transferred to St. Louis, including three future Hall of Famers: Young, Jesse Burkett, and Bobby Wallace. The roster maneuvers failed to create a powerhouse Perfectos team, as St. Louis finished fifth in both 1899 and 1900. Meanwhile, the depleted Spiders lost 134 games, the most in MLB history, before folding. Young spent two years with St. Louis, which is where he found his favorite catcher, Lou Criger. The two men were teammates for a decade.

In 1901, the rival American League declared major league status and set about raiding National League rosters. Young left St. Louis and joined the American League's Boston Americans for a $3,500 contract ($ today). Young would remain with the Boston team until 1909. In his first year in the American League, Young was dominant. Pitching to Criger, who had also jumped to Boston, Young led the league in wins, strikeouts, and ERA, thus earning the colloquial AL Triple Crown for pitchers. Young won almost 42% of his team's games in 1901, accounting for 33 of his team's 79 wins. In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.

The Boston Americans played the Pittsburgh Pirates in the first modern World Series in 1903. Young, who started Game One against the visiting Pirates, thus threw the first pitch in modern World Series history. The Pirates scored four runs in that first inning, and Young lost the game. Young performed better in subsequent games, winning his next two starts. He also drove in three runs in Game Five. Young finished the series with a 2â1 record and a 1.85 ERA in four appearances, and Boston defeated Pittsburgh, five games to three games.
After one-hitting Boston on May 2, 1904, Philadelphia Athletics pitcher Rube Waddell taunted Young to face him so that he could repeat his performance against Boston's ace. Three days later, Young pitched a perfect game against Waddell and the Athletics. It was the first perfect game in American League history. Waddell was the 27th and last batter, and when he flied out, Young shouted, "How do you like that, you hayseed?"

Waddell had picked an inauspicious time to issue his challenge. Young's perfect game was the centerpiece of a pitching streak. Young set major league records for the most consecutive scoreless innings pitched and the most consecutive innings without allowing a hit; the latter record still stands at 25.1 innings, or 76 hitless batters. Even after allowing a hit, Young's scoreless streak reached a then-record 45 shutout innings. Before Young, only two pitchers had thrown perfect games. This occurred in 1880, when Lee Richmond and John Montgomery Ward pitched perfect games within five days of each other, although under somewhat different rules: the front edge of the pitcher's box was only from home base (the modern release point is about farther away); walks required eight balls; and pitchers were obliged to throw side-armed. Young's perfect game was the first under the modern rules established in 1893. One year later, on July 4, 1905, Rube Waddell beat Young and the Americans, 4â2, in a 20-inning matchup. Young pitched 13 consecutive scoreless innings before he gave up a pair of unearned runs in the final inning. Young did not walk a batter and was later quoted: "For my part, I think it was the greatest game of ball I ever took part in." In 1907, Young and Waddell faced off in a scoreless 13-inning tie.

In 1908, Young pitched the third no-hitter of his career. Three months past his 41st birthday, Cy Young was the oldest pitcher to record a no-hitter, a record which would stand 82 years until 43-year-old Nolan Ryan surpassed the feat. Only a walk kept Young from his second perfect game. After that runner was caught stealing, no other batter reached base. At this time, Young was the second-oldest player in either league. In another game one month before his no-hitter, he allowed just one single while facing 28 batters. On August 13, 1908, the league celebrated "Cy Young Day." No American League games were played on that day, and a group of All-Stars from the league's other teams gathered in Boston to play against Young and the Red Sox. When the season ended, he posted a 1.26 ERA, which gave him not only the lowest in his career, but also gave him a major league record of being the oldest pitcher with 150+ innings pitched to post a season ERA under 1.50.

Young was traded back to Cleveland, the place where he played over half his career, before the 1909 season, to the Cleveland Naps of the American League. The following season, 1910, he won his 500th career game on July 19 against Washington. He split 1911, his final year, between the Naps and the Boston Rustlers. On September 22, 1911, Young shut out the Pittsburgh Pirates, 1â0, for his last career victory. In his final start two weeks later, the last eight batters of Young's career combined to hit a triple, four singles, and three doubles. By the time of his retirement, Young's control had faltered. He had also gained weight. In two of his last three years, he was the oldest player in the league.

Young established numerous pitching records, some of which have stood for over a century. Young compiled 511 wins, which is the most in major league history and 94 ahead of Walter Johnson, second on the list. At the time of Young's retirement, Pud Galvin had the second most career wins with 364. In addition to wins, Young still holds the major league records for most career innings pitched (7,356), most career games started (815), and most complete games (749). He also retired with 316 losses, the most in MLB history. Young's career record for strikeouts was broken by Johnson in 1921. Young's 76 career shutouts are fourth all-time.
Young led his league in wins five times (1892, 1895, and 1901â1903), finishing second twice. His career high was 36 in 1892. He won at least thirty games in a season five times. He had fifteen seasons with twenty or more wins, two more than the runners-up, Christy Mathewson and Warren Spahn. Young won two ERA titles during his career, in 1892 (1.93) and in 1901 (1.62), and was three times the runner-up. Young's earned run average was below 2.00 six times, but this was not uncommon during the dead-ball era. Although Young threw over 400 innings in each of his first four full seasons, he did not lead his league until 1902. He had 40 or more complete games nine times. Young also led his league in strikeouts twice (with 140 in 1896, and 158 in 1901), and in shutouts seven times. Young led his league in fewest walks per nine innings fourteen times and finished second one season. Only twice in his 22-year career did Young finish lower than 5th in the category. Although the WHIP ratio was not calculated until well after Young's death, Young was the retroactive league leader in this category seven times and was second or third another seven times. Young is tied with Roger Clemens for the most career wins by a Boston Red Sox pitcher. They each won 192 games while with the franchise. In addition, Young pitched three no-hitters, including the third perfect game in baseball history, first in baseball's "modern era".

Young also was an above average hitting pitcher in his career. He posted a .210 batting average (623-for-2960) with 325 runs, 18 home runs, 290 RBI and drew 81 bases on balls. From 1891 through 1905, he drove in 10 or more runs for 15 straight seasons, with a high of 28 RBI in 1896.

Particularly after his fastball slowed, Young relied upon his control. Young was once quoted as saying, "Some may have thought it was essential to know how to curve a ball before anything else. Experience, to my mind, teaches to the contrary. Any young player who has good control will become a successful curve pitcher long before the pitcher who is endeavoring to master both curves and control at the same time. The curve is merely an accessory to control." In addition to his exceptional control, Young was also a workhorse who avoided injury. For nineteen consecutive years, from 1891 through 1909, Young was in his league's top ten for innings pitched; in fourteen of the seasons, he was in the top five. Not until 1900, a decade into his career, did Young pitch two consecutive incomplete games. By habit, Young restricted his practice throws in spring training. "I figured the old arm had just so many throws in it", said Young, "and there wasn't any use wasting them." Young once described his approach before a game:
I never warmed up ten, fifteen minutes before a game like most pitchers do. I'd loosen up, three, four minutes. Five at the outside. And I never went to the bullpen. Oh, I'd relieve all right, plenty of times, but I went right from the bench to the box, and I'd take a few warm-up pitches and be ready. Then I had good control. I aimed to make the batter hit the ball, and I threw as few pitches as possible. That's why I was able to work every other day.

Beginning in 1912, Young lived and worked on his farm. In 1913, he served as manager of the Cleveland Green Sox of the Federal League, which was at the time an outlaw league. However, he never worked in baseball after that.

Young's wife, Roba, whom he had known since childhood, died in 1933. After she died, Young tried several jobs, and eventually moved in with friends John and Ruth Benedum and did odd jobs for them. Young took part in many baseball events after his retirement. In 1937, 26 years after he retired from baseball, Young was inducted into the Baseball Hall of Fame. He was among the first to donate mementos to the Hall.

By 1940, Young's only source of income was stock dividends of $300 per year ($ today). On November 4, 1955, Young died on the Benedums' farm at the age of 88. He was buried in Peoli, Ohio.

Young's career is seen as a bridge from baseball's earliest days to its modern era; he pitched against stars such as Cap Anson, already an established player when the National League was first formed in 1876, as well as against Eddie Collins, who played until 1930. When Young's career began, pitchers delivered the baseball underhand and fouls were not counted as strikes. The pitcher's mound was not moved back to its present position of until Young's fourth season; he did not wear a glove until his sixth season.

Young was elected to the National Baseball Hall of Fame in 1937. In 1956, about one year after Young's death, the Cy Young Award was created to honor the best pitcher in Major League Baseball for each season. The first award was given to Brooklyn's Don Newcombe. Originally, it was a single award covering the whole of baseball. The honor was divided into two Cy Young Awards in 1967, one for the National League and one for the American League.

On September 23, 1993, a statue dedicated to him was unveiled by Northeastern University on the site of the Red Sox's original stadium, the Huntington Avenue Grounds. It was there that Young had pitched the first game of the 1903 World Series, as well as the first perfect game in the modern era of baseball. A home plate-shaped plaque next to the statue reads:

On October 1, 1903 the first modern World Series between the American League champion Boston Pilgrims (later known as the Red Sox) and the National League champion Pittsburgh Pirates was played on this site. General admission tickets were fifty cents. The Pilgrims, led by twenty-eight game winner Cy Young, trailed the series three games to one but then swept four consecutive victories to win the championship five games to three.
In 1999, 88 years after his final major league appearance and 44 years after his death, editors at "The Sporting News" ranked Young 14th on their list of "Baseball's 100 Greatest Players". That same year, baseball fans named him to the Major League Baseball All-Century Team.




</doc>
<doc id="6851" url="https://en.wikipedia.org/wiki?curid=6851" title="Coronation Street">
Coronation Street

Coronation Street (often referred to as Corrie) is a British soap opera created by Granada Television and shown on ITV since 9 December 1960. The programme centres on Coronation Street in Weatherfield, a fictional town based on inner-city Salford. In the show's fictional history, the street was built in 1902 and named in honour of the coronation of King Edward VII.

The series typically airs six times a week: Monday, Wednesday and Friday 7:30â8Â pm and 8:30â9Â pm. Since 2017, ten sequential classic episodes of the series dated from 1986 onwards have been broadcast weekly on ITV3. The programme was conceived in 1960 by scriptwriter Tony Warren at Granada Television in Manchester. Warren's initial proposal was rejected by the station's founder Sidney Bernstein, but he was persuaded by producer Harry Elton to produce the programme for 13 pilot episodes. Within six months of the show's first broadcast, it had become the most-watched programme on British television, and is now a significant part of British culture.

"Coronation Street" is made by ITV Granada at MediaCityUK and shown in all ITV regions, as well as internationally. On 17 September 2010, it became the world's longest-running television soap opera and was listed in "Guinness World Records". On 23 September 2015, "Coronation Street" was broadcast live to mark ITV's sixtieth anniversary.

Influenced by the conventions of kitchen sink realism, "Coronation Street" is noted for its depiction of a down-to-earth, working-class community, combined with light-hearted humour and strong characters. The show currently averages seven million viewers per episode. The show will its 10,000th episode in an hour-long format on Friday 7 February 2020 at 8â9Â pm.

The was aired on 9 December 1960 at 7Â pm, and was not initially a critical success; "Daily Mirror" columnist Ken Irwin claimed the series would only last three weeks. Granada Television had commissioned only 13 episodes, and some inside the company doubted the show would last beyond its planned production run. Despite the criticism, viewers were immediately drawn into the serial, won over by "Coronation Street"s ordinary characters. The programme also made use of Northern English language and dialect; affectionate local terms like "eh, chuck?", "nowt" (, from "nought", meaning "nothing"), and "by 'eck!" became widely heard on British television for the first time.

Early episodes told the story of student Ken Barlow (William Roache), who had won a place at university, and thus found his working-class backgroundâas well as his parents, Frank (Frank Pemberton) and Ida (Noel Dyson)âsomething of an embarrassment. The character was one of the few to have experienced life outside of Coronation Street. In some ways this predicts the growth of globalisation, and the decline of similar communities. In an episode from 1961, Barlow declares: "You can't go on just thinking about your own street these days. We're living with people on the other side of the world. There's more to worry about than Elsie Tanner (Pat Phoenix) and her boyfriends." Roache is the only remaining member of the original cast, which makes him the longest-serving actor in "Coronation Street", and in British and global soap history.

At the centre of many early stories, there was Ena Sharples (Violet Carson), caretaker of the Glad Tidings Mission Hall, and her friends: timid Minnie Caldwell (Margot Bryant), and bespectacled Martha Longhurst (Lynne Carol). The trio were likened to the Greek chorus, and the three witches in William Shakespeare's "Macbeth", as they would sit in the snug bar of The Rovers Return Inn, passing judgement over family, neighbours and frequently each other. Headstrong Ena often clashed with Elsie Tanner (Pat Phoenix), whom she believed espoused a dauntlessly loose set of morals. Elsie resented Ena's interference and gossip, which most of the time had little basis in reality.

In April 1961, Jed Stone (Kenneth Cope) made his first appearance and returned the following year in 1962. He left in 1963, but returned three years later in 1966. He left again and then returned 42 years later in 2008.

In March 1961, "Coronation Street" reached No. 1 in the television ratings and remained there for the rest of the year. Earlier in 1961, a Television Audience Measurement (TAM) showed that 75% of available viewers (15Â million) tuned into "Corrie", and by 1964 the programme had over 20Â million regular viewers, with ratings peaking on 2 December 1964, at 21.36Â million viewers.

Storylines throughout the decade included Elsie's mystery poison-pen letter, the 1962 marriage of Ken and Valerie Tatlock (Anne Reid), the death of Martha Longhurst in 1964, the birth of the Barlow twins in 1965, Elsie Tanner's wedding to Steve Tanner (Paul Maxwell) and a train crashing from the viaduct (both in 1967), Steve Tanner's murder in 1968, and a coach crash in 1969.

In spite of rising popularity with viewers, "Coronation Street" was criticised by some for its outdated portrayal of the urban working class, and its representation of a community that was a nostalgic fantasy. After the first episode in 1960, the "Daily Mirror" printed: "The programme is doomed from the outsetÂ ... For there is little reality in this new serial, which apparently, we have to suffer twice a week." By 1967, critics were suggesting that the programme no longer reflected life in 1960s Britain, but reflected how life was in the "1950s". Granada hurried to update the programme, with the hope of introducing more issue-driven stories, including Lucille Hewitt (Jennifer Moss) becoming addicted to drugs, Jerry Booth (Graham Haberfield) being in a storyline about homosexuality, Emily Nugent (Eileen Derbyshire) having an out-of-wedlock child, and introducing a black family, but all of these ideas were dropped for fear of upsetting viewers.

The show's production team was tested when many core cast members left the programme in the early 1970s. When Arthur Leslie died suddenly in 1970, his character, Rovers' landlord Jack Walker, died with him. Anne Reid quit as Valerie Barlow; her character was killed off in 1971, electrocuting herself with a faulty hairdryer. Ratings reached a low of eight million in February 1973, when Pat Phoenix quit as Elsie Tanner, Violet Carson as Ena Sharples were written out for most of the year due to illness, and Doris Speed (haughty landlady Annie Walker) took two months' leave due to bereavement. The audience of ITV's other flagship soap opera "Crossroads" increased markedly at this time, as its established cast, such as Meg Richardson (Noele Gordon), grew in popularity. These sudden departures forced the writing team to quickly develop characters who had previously stood in the background. The roles of Bet Lynch (Julie Goodyear), Deirdre Hunt (Anne Kirkbride), Rita Littlewood (Barbara Knox), Mavis Riley (Thelma Barlow) and Ivy Tyldesley (Lynne Perrie) were built up between 1972 and 1973 (with Perrie's character being renamed to the better-known "Tilsley"), and characters such as Gail Potter (Helen Worth), Blanche Hunt (Patricia Cutts/Maggie Jones), and Vera Duckworth (Liz Dawn) first appearing in 1974. These characters would remain at the centre of the programme for many years.

Comic storylines had been popular in the series in the 1960s, but had become sparse during the early 1970s. These were re-introduced by new producer Bill Podmore who joined the series in 1976. He had worked on Granada comedy productions prior to his appointment. Stan (Bernard Youens) and Hilda Ogden (Jean Alexander) were often at the centre of overtly funny storylines, with other comic characters including Eddie Yeats (Geoffrey Hughes), Fred Gee (Fred Feast), and Jack Duckworth (Bill Tarmey) all making their first appearances during the decade.

In 1976, Pat Phoenix returned to her role as Elsie Tanner and, after a spate of ill health, Violet Carson returned on a more regular basis as Ena. "Coronation Street's" stalwart cast slotted back into the programme alongside the newcomers, examining new relationships between characters of different ages and backgrounds: Eddie Yeats became the Ogdens' lodger, Gail Potter and Suzie Birchall (Cheryl Murray) moved in with Elsie, Mike Baldwin (Johnny Briggs) arrived in 1976 as the tough factory boss, and Annie Walker reigned at the Rovers with her trio of staff: Bet Lynch, Fred Gee and Betty Turpin (Betty Driver).

Storylines throughout the decade included a warehouse fire in 1975, the birth of Tracy Langton in 1977, the murder of Ernest Bishop (Stephen Hancock) in 1978, a lorry crashing into the Rovers Return in 1979, and the marriage of Gail to Brian Tilsley (Christopher Quinten) (also in 1979).

For eleven weeks, between August and October 1979, industrial action forced "Coronation Street" and the entire ITV network (apart from the Channel Islands) off the air. When ITV did return, its first evening schedule included a special "catch-up" edition of "Coronation Street". This included storylines which would have taken place during the strike, and they were explained in the form of a narrative chat between Bet Lynch and popular character Len Fairclough (Peter Adamson). For several weeks the channel had very few fresh episodes to show, and episodes of the game show "3-2-1" were screened in its place. "Coronation Street" returned to ITV screens with a regular scheduled time closer to the end of 1979.

"Coronation Street" had little competition within its prime time slot, and certain critics suggested that the programme had grown complacent, moving away from socially viable storylines and again presenting a dated view of working class life.

Between 1980 and 1989, "Coronation Street" underwent some of the biggest changes since its launch. By May 1984, William Roache (Ken Barlow) stood as the only original cast member, after the departures of Violet Carson (Ena Sharples) in 1980, Doris Speed (Annie Walker) in 1983, and both Pat Phoenix (Elsie Tanner) and Jack Howarth (Albert Tatlock) in 1984. In 1983, antihero Len Fairclough (Peter Adamson), one of the show's central male characters since 1961, was killed off, and in 1984, Stan Ogden (Bernard Youens) died. While the press predicted the end of "Corrie", H. V. Kershaw declared that "There are no stars in "Coronation Street"." Writers drew on the show's many archetypes, with established characters stepping into the roles left by the original cast. Phyllis Pearce (Jill Summers) was hailed as the new Ena Sharples in 1982, the Duckworths moved into No.9 in 1983 and slipped into the role once held by the Ogdens, while Percy Sugden (Bill Waddington) appeared in 1983 and took over the grumpy war veteran role from Albert Tatlock. The question of who would take over the Rovers Return after Annie Walker's 1983 exit was answered in 1985 when Bet Lynch (who also mirrored the vulnerability and strength of Elsie Tanner) was installed as landlady. In 1983, Shirley Armitage (Lisa Lewis) became the first major black character in her role as machinist at Baldwin's Casuals.

Ken Barlow married Deirdre Langton (Anne Kirkbride) on 27 July 1981. The episode was watched by over 15Â million viewersÂ â more ITV viewers than the wedding of Prince Charles and Lady Diana two days later. In the 1980s relationships were cemented between established characters: Alf Roberts (Bryan Mosley) married Audrey Potter (Sue Nicholls) in 1985; Kevin Webster (Michael Le Vell) married Sally Seddon (Sally Whittaker) in 1986; Bet Lynch married Alec Gilroy (Roy Barraclough) in 1987; and 1988 saw the marriages of both Ivy Tilsley and Don Brennan (Geoffrey Hinsliff), and the long-awaited union of Mavis Riley and Derek Wilton (Peter Baldwin), after over a decade of on-off romances and a failed marriage attempt in 1979.

In 1982, the arrival of Channel 4, and its edgy new soap opera "Brookside", was one of the biggest changes for "Coronation Street". Unlike "Coronation Street", which had a very nostalgic view of working-class life, "Brookside" brought together working and middle-class families in a more contemporary environment. The dialogue often included expletives and the stories were more hard-hitting, and of the current Zeitgeist. Whereas stories at this time in "Coronation Street" were largely about family affairs, "Brookside" concentrated on social affairs such as industrial action, unemployment, and the black market. The BBC also introduced a new prime time soap opera, "EastEnders" in 1985. Like "Brookside", "EastEnders" had a more gritty premise than "Coronation Street", although unlike "Brookside" it tended to steer clear of blue language and politicised stories.

While ratings for "Coronation Street" remained consistent throughout the decade, "EastEnders" regularly obtained higher viewing figures due to its omnibus episodes shown at weekends. The "Coronation Street" episode broadcast on 2 January 1985 attracted 21.40 million viewers, making it the most-watched episode in the shows history based on a single showing. Subsequent episodes would achieve higher figures when the original broadcast and omnibus edition figures were combined. With prime time competition, "Corrie" was again seen as being old fashioned, with the introduction of the 'normal' Clayton family in 1985 being a failure with viewers. Between 1988 and 1989, many aspects of the show were modernised by new producer David Liddiment. A new exterior set had been built in 1982, and in 1989 it was redeveloped to include new houses and shops. Production techniques were also changed with a new studio being built, and the inclusion of more location filming, which had moved the show from being shot on film to videotape in 1988. Due to new pressures, an introduction of the third weekly episode aired on 20 October 1989, to broadcast each Friday at 7:30Â pm.

The 1980s featured some of the most prominent storylines in the programme's history, such as Deirdre Barlow's affair with Mike Baldwin (Johnny Briggs) in 1983, the first soap storyline to receive widespread media attention. The feud between Ken Barlow and Mike Baldwin would continue for many years, with Mike even marrying Ken's daughter, Susan (Wendy Jane Walker). In 1986, there was a fire at the Rovers Return. The episode that aired on 25 December 1987, attracted a combined audience (original and omnibus) of 26.65Â million â a figure helped by the fact that this episode heralded the departure of immensely-popular character Hilda Ogden (Jean Alexander). Between 1986 and 1989, the story of Rita Fairclough's (Barbara Knox) psychological abuse at the hands of Alan Bradley (Mark Eden), and then his subsequent death under the wheels of a Blackpool tram, was played out. This storyline gave the show its highest combined viewing figure in its history with 26.93Â million for the episode that aired on 15 (and 19) March 1989, where Alan is hiding from the police after trying to kill Rita in the previous episode. This rating is sometimes incorrectly credited to the 8 December 1989 tram death episode. Other stories included the birth of Nicky Tilsley (Warren Jackson) in 1980, Elsie Tanner's departure and Stan Ogden's funeral in 1984, the birth of Sarah-Louise Tilsley (Lynsay King) in 1987, and Brian Tilsley's murder in 1989.

New characters were introduced, such as Terry Duckworth (Nigel Pivaro), Curly Watts (Kevin Kennedy), Martin Platt (Sean Wilson), Reg Holdsworth (Ken Morley), and the McDonald family; one of whom, Simon Gregson, started on the show as Steve McDonald a week after his 15th birthday, and has been on the show ever since.

In spite of updated sets and production changes, "Coronation Street" still received criticism. In 1992, chairman of the Broadcasting Standards Council, Lord Rees-Mogg, criticised the low representation of ethnic minorities, and the programme's portrayal of the cosy familiarity of a bygone era. Some newspapers ran headlines such as ""Coronation Street" shuts out blacks" ("The Times"), and "'Put colour in t'Street" ("Daily Mirror"). Patrick Stoddart of "The Times" wrote: "The millions who watch "Coronation Street"Â â and who will continue to do so despite Lord Rees-MoggÂ â know real life when they see itÂ ... in the most confident and accomplished soap opera television has ever seen". Black and Asian characters had appeared, but it was not until 1999 that the show featured its first regular non-white family, the Desai family. There was also an Irish Traveller family who made a brief appearance for 4 episodes â spear headed by famous Irish actress Rachael McCrudden who played the part of Josie Joyce and her husband Conor McCrudden ( Jonjo Joyce). They were written off after they went on a rampage of drinking Dutch Gold and were caught by police after holding up the local Des Kelly carpets looking for wet finish Lino.

New characters Des (Philip Middlemiss) and Steph Barnes (Amelia Bullmore) moved into one of the new houses in 1990, being dubbed by the media as Yuppies. Raquel Wolstenhulme (Sarah Lancashire) first appeared in 1991 and went on to become one of the most popular characters. The McDonald family were developed and the fiery relationships between Liz (Beverly Callard), Jim (Charles Lawson), Steve (Simon Gregson) and Andy (Nicholas Cochrane) interested viewers. Other newcomers were Maud Grimes (Elizabeth Bradley), Roy Cropper (David Neilson), Gary and Judy Mallett (Ian Mercer and Gaynor Faye), as well as Fred Elliott (John Savident) and Ashley Peacock (Steven Arnold). The amount of slapstick and physical humour in storylines increased during the 1990s, with comical characters such as Reg Holdsworth (Ken Morley) and his water bed.

In the early 1990s storylines included the death of newborn Katie McDonald in 1992, Mike Baldwin's (Johnny Briggs) wedding to Alma Sedgewick (Amanda Barrie) in 1992, Tommy Duckworth being sold by his father Terry (Nigel Pivaro) in 1993, Deirdre Barlow's (Anne Kirkbride) marriage to Moroccan Samir Rachid (Al Nedjari), and the rise of Tanya Pooley (Eva Pope) between 1993 and 1994.

In 1995, Julie Goodyear (Bet Lynch) left the show. She made brief return appearances in 1999, 2002 and 2003.

In 1997, Brian Park took over as producer, with the idea of promoting young characters as opposed to the older cast. On his first day, he cut the characters of Derek Wilton (Peter Baldwin), Don Brennan (Geoffrey Hinsliff), Percy Sugden (Bill Waddington), Bill Webster (Peter Armitage), Billy Williams (Frank Mills) and Maureen Holdsworth (Sherrie Hewson). Thelma Barlow, who played Derek's wife Mavis, was angered by the firing of her co-star and resigned. The production team lost some of its key writers when Barry Hill, Adele Rose and Julian Roach all resigned as well.

In line with Park's suggestion, younger characters were introduced: Nick Tilsley was recast, played by Adam Rickitt, single mother Zoe Tattersall (Joanne Froggatt) first appeared, and the Battersbys moved into No.5. Storylines focussed on tackling 'issues', such as drug dealers, eco-warriors, religious cults, and a transsexual woman. Park quit in 1998, after deciding that he had done what he intended to do; he maintained that his biggest achievement was the introduction of Hayley Patterson (Julie Hesmondhalgh), the first transsexual character in a British soap.

Some viewers were alienated by the new "Coronation Street", and sections of the media voiced their disapproval. Having received criticism of being too out of touch, "Corrie" now struggled to emulate the more modern "Brookside" and "EastEnders". In the "Daily Mirror", Victor Lewis-Smith wrote: "Apparently it doesn't matter that this is a first-class soap opera, superbly scripted and flawlessly performed by a seasoned repertory company."

One of "Coronation Street"'s best known storylines took place in March/April 1998, with Deirdre Rachid (Anne Kirkbride) being wrongfully imprisoned after a relationship with con-man Jon Lindsay (Owen Aaronovitch). The episode in which Deirdre was sent to prison had an audience of 19Â million viewers, and 'Free the Weatherfield One' campaigns sprung up in a media frenzy. Then Prime Minister Tony Blair even passed comment on Deirdre's sentencing in Parliament. Deirdre was freed after three weeks, with Granada stating that they had always intended for her to be released, in spite of the media interest.

On 8 December 2000, the show celebrated its fortieth year by broadcasting a live, hour-long . The Prince of Wales appeared as himself in an ITV News bulletin report. Earlier in the year, 13-year-old Sarah-Louise Platt (Tina O'Brien) had become pregnant and given birth to a baby girl, Bethany, on 4 June. The episode where Gail was told of her daughter's pregnancy was watched by 15Â million viewers. In September 2000, Mike Baldwin married Linda Sykes but shortly afterwards, his drunken son Mark confessed he and Linda had been having an affair behind his dad's back. The episode attracted an audience of 16.8Â million and in the 2000 British Soap Awards won Best Storyline.

From 1999 to 2001, issue-led storylines were introduced such as Toyah Battersby's (Georgia Taylor) rape, Roy and Hayley Cropper (David Neilson and Julie Hesmondhalgh) abducting their foster child, Sarah Platt's Internet chat room abduction and Alma Halliwell's (Amanda Barrie) death from cervical cancer. Such storylines were unpopular with viewers and ratings dropped and in October 2001, Macnaught was abruptly moved to another Granada department and Carolyn Reynolds took over. "Corrie" continued to struggle in the ratings, with "EastEnders" introducing some of its strongest stories. In 2002, Kieran Roberts was appointed as producer and aimed to re-introduce "gentle storylines and humour", after deciding that "the Street" should not try to compete with other soaps. In 2002, Gail Platt (Helen Worth) married Richard Hillman (Brian Capron), a financial advisor who would go on to leave Duggie Ferguson (John Bowe) to die; murder both his ex-wife Patricia (Annabelle Apsion) and local neighbour Maxine Peacock (Claire Casey); and attempt to kill both his mother-in-law Audrey Roberts (Sue Nicholls) and her longtime friend, Emily Bishop (Eileen Derbyshire). After confessing his crimes to Gail in a two-episode handler, Hillman left the street for two weeks before returning with a suicidal impact on himself and his stepfamily; he kidnapped Gail, her children Sarah and David (Jack P. Shepherd), and granddaughter Bethany, before driving them into a canal â though the Platt family survived whilst Richard drowned. The storyline received wide press attention, and viewing figures peaked at 19.4Â million, with Hillman dubbed a "serial killer" by the media. Todd Grimshaw (Bruno Langley) became "Corrie's" first regular homosexual character. In 2003 another gay male character was introduced, Sean Tully (Antony Cotton). The character of Karen McDonald (Suranne Jones) was developed, with her fiery marriage to Steve and warring with Tracy Barlow (Kate Ford). In 2004, "Coronation Street" retconned the Baldwin family when Mike's nephew Danny Baldwin (Bradley Walsh) and his wife Frankie (Debra Stephenson) moved to the area from Essex, with their two sons Jamie (Rupert Hill) and Warren (Danny Young). Until this time, Mike Baldwin had been portrayed as an only child, with his father (also called Frankie and portrayed by Sam Kydd) appearing in the programme between 1980 and 1982 confirming the fact. The bigamy of Peter Barlow (Chris Gascoyne) and his addiction to alcohol, later in the decade, Maya Sharma's (Sasha Behar) revenge on former lover Dev Alahan (Jimmi Harkishin), Charlie Stubbs's (Bill Ward) psychological abuse of Shelley Unwin (Sally Lindsay), and the deaths of Mike Baldwin (Johnny Briggs), Vera Duckworth (Liz Dawn) and Fred Elliott (John Savident). In 2007, Tracy Barlow (Kate Ford) murdered Charlie Stubbs and claiming it was self-defence; the audience during this storyline peaked at 13.3Â million. At the 2007 British Soap Awards, it won Best Storyline, and Ford was voted Best Actress for her portrayal. Other storylines included Leanne Battersby (Jane Danson) becoming a prostitute and the show's first bisexual love triangle (between Michelle Connor (Kym Marsh), Sonny Dhillon (Pal Aron), and Sean Tully (Antony Cotton)). The Connor family were central to many storylines during 2007 â the accidental death of a Polish worker at Underworld due to overworking, Michelle's discovery that her brothers Paul (Sean Gallagher) and Liam (Rob James-Collier) were the cause of her husband's death, Paul's use of an escort service, his kidnapping of Leanne and his subsequent death.

In July 2007, after 34 years in the role of Vera Duckworth, Liz Dawn left the show due to ill health. After conversation between Dawn and producers Kieran Roberts and Steve Frost, the decision was made to kill Vera off. In January 2008, shortly before plans to retire to Blackpool, Vera's husband Jack (William Tarmey) found that she had died in her armchair.

Tina O'Brien revealed in the British press on 4 April 2007 that she would be leaving "Coronation Street". Sarah-Louise, who was involved in some of the decade's most controversial stories, left in December 2007 with her daughter, Bethany Platt (who had been in an ecstasy storyline earlier that year, in which she discovered her uncle David's stash of the drug he was looking after for a friend in one of her dolls, and ended up in hospital after she ate them). In 2008, Michelle learning that Ryan (Ben Thompson) was not her biological son, having been accidentally swapped at birth with Alex Neeson (Dario Coates). Carla Connor (Alison King) turned to Liam for comfort and developed feelings for him. In spite of knowing about her feelings, Liam married Maria Sutherland (Samia Longchambon). Maria and Liam's baby son was stillborn in April, and during an estrangement from Maria upon the death of their baby, Liam had a one-night stand with Carla, a story which helped pave the way for his departure. Gail Platt's (Helen Worth) son David (Jack P. Shepherd) pushed her down the stairs. Enraged that Gail refused to press charges, David vandalised the Street and was sent to a young offenders' facility for several months. In May 2008, Gail finally met Ted Page (Michael Byrne), the father she had never known and in 2009, Gail's boyfriend Joe McIntyre (Reece Dinsdale) became addicted to painkillers, which came to a head when he broke into the medical centre. In August 2008, Jed Stone (Kenneth Cope) returned after 42 years. Liam Connor and his ex-sister-in-law Carla gave into their feelings for each other and began an affair. Carla's fiancÃ©e Tony Gordon (Gray O'Brien) discovered the affair and had Liam killed in a hit-and-run in October. Carla struggled to come to terms with Liam's death, but decided she still loved Tony and married him on 3 December, in an episode attracting 10.3Â million viewers. In April 2009 it was revealed that Eileen Grimshaw's (Sue Cleaver) father, Colin (Edward de Souza) â the son of Elsie Tanner's (Pat Phoenix) cousin Arnley â had slept with Eileen's old classmate, Paula Carp (Sharon Duce) while she was still at school, and that Paula's daughter Julie (Katy Cavanagh) was in fact also Colin's daughter. In May, Norris Cole (Malcolm Hebden) received a blast from the past with the reappearance of his estranged brother Ramsay Clegg (Andrew Sachs) who wanted a reconciliation. Peter Barlow's battle against alcoholism, Ken Barlow's affair with actress Martha Fraser (Stephanie Beacham) after his dog Eccles fell in the canal, Maria giving birth to Liam's son and her subsequent relationship with Liam's killer Tony, Steve McDonald's (Simon Gregson) marriage to Becky Granger (Katherine Kelly) and Kevin Webster's (Michael Le Vell) affair with Molly Dobbs (Vicky Binns). On Christmas Day 2009, Sally Webster (Sally Dynevor) told husband Kevin that she had breast cancer, just as he was about to leave her for lover Molly.

The show began broadcasting in high-definition in May 2010, and on 17 September that year, "Coronation Street" entered "Guinness World Records" as the world's longest-running television soap opera after the American soap opera "As the World Turns" concluded. William Roache was listed as the world's longest-running soap actor.
"Coronation Street" 50th anniversary week was celebrated with seven episodes, plus a special one-hour live episode, broadcast from 6â10 December. The episodes averaged 14Â million viewers, a 52.1% share of the audience. The anniversary was also publicised with ITV specials and news broadcasts. In the storyline, Nick Tilsley and Leanne Battersby's barâThe Joineryâexploded during Peter Barlow's stag party. As a result, the viaduct was destroyed, sending a Metrolink tram careering onto the street, destroying D&S Alahan's Corner Shop and The Kabin. Two characters, Ashley Peacock (Steven Arnold) and Molly Dobbs (Vicky Binns), along with an unknown taxi driver, were killed as a result of the disaster. Rita Sullivan (Barbara Knox) survived, despite being trapped under the rubble of her destroyed shop. Fiz Stape (Jennie McAlpine) prematurely gave birth to a baby girl, Hope. The episode of "EastEnders" broadcast on the same day as "Coronation Street" 50th anniversary episode included a tribute, with the character Dot Branning (June Brown) saying that she never misses an episode of "Coronation Street".

In May 2011, Dennis Tanner (Philip Lowrie) returned after 43 years off screen. On 15 October 2011, Betty Driver, who had played Betty Williams since 1969, died of pneumonia, aged 91. In 2011, the major storyline of John Stape and his murder spree came to an end in May after he jumped off a hospital roof but left before he could be arrested.

On the morning of 1 March 2016, "Coronation Street" creator Tony Warren died aged 79.

Since 1960, "Coronation Street" has featured many characters whose popularity with viewers and critics has differed greatly. The original cast was created by Tony Warren, with the characters of Ena Sharples (Violet Carson), Elsie Tanner (Pat Phoenix) and Annie Walker (Doris Speed) as central figures. These three women remained with the show for 20 years or more, and became archetypes of British soap opera, often being emulated by other serials. Ena was the street's busybody, battle-axe and self-proclaimed moral voice. Elsie was the tart with a heart, who was constantly hurt by men in the search for true love. Annie Walker, landlady of the Rovers Return Inn, had delusions of grandeur and saw herself as better than other residents of "Coronation Street".

"Coronation Street" became known for the portrayal of strong female characters, including original cast characters like Ena, Annie and Elsie, and Hilda Ogden (Jean Alexander), who first appeared in 1964; who became household names during the 1960s. Warren's programme was largely matriarchal, which some commentators put down to the female-dominant environment in which he grew up. Consequently, the show has a long tradition of psychologically abused husbands, most famously Stan Ogden (Bernard Youens) and Jack Duckworth (Bill Tarmey), husbands of Hilda and Vera Duckworth (Liz Dawn), respectively.
Ken Barlow (William Roache) entered the storyline as a young radical, reflecting the youth of 1960s Britain, where figures like the Beatles, the Rolling Stones and the model Twiggy were to reshape the concept of youthful rebellion. Though the rest of the original Barlow family were killed off before the end of the 1970s, Ken, who for 27 years was the only character from the first episode remaining, has remained the constant link throughout the entire series. In 2011, Dennis Tanner (Philip Lowrie), another character from the first episode, returned to "Coronation Street" after a 43-year absence. Since 1984, Ken Barlow has been the show's only remaining original character. Emily Bishop (Eileen Derbyshire) had appeared in the series since late-January 1961, when the show was just weeks old, and was the show's longest-serving female character before she departed on 1 January 2016. Rita Tanner (Barbara Knox) appeared on the show for one episode in December 1964, before returning as a full-time cast member in January 1972. She is currently the second longest-serving original cast member on the show.

Stan and Hilda Ogden were introduced in 1964, with Hilda becoming one of the most famous British soap opera characters of all time. In a 1982 poll, she was voted fourth-most recognisable woman in Britain, after Queen Elizabeth The Queen Mother, Queen Elizabeth II and Diana, Princess of Wales. Hilda's best-known attributes were her pinny, hair curlers, and the "muriel" in her living room with three "flying" duck ornaments. Hilda Ogden's departure on Christmas Day 1987, remains the highest-rated episode of "Coronation Street" ever, with nearly 27,000,000 viewers. Stan Ogden had been killed off in 1984 following the death of actor Bernard Youens after a long illness which had restricted his appearances towards the end.

Bet Lynch (Julie Goodyear) first appeared in 1966, before becoming a regular in 1970, and went on to become one of the most famous "Corrie" characters. Bet stood as the central character of the show from 1985 until departing in 1995, often being dubbed as "Queen of the Street" by the media, and indeed herself. The character briefly returned in June 2002.

"Coronation Street" and its characters often rely heavily on archetypes, with the characterisation of some of its current and recent cast based loosely on past characters. Phyllis Pearce (Jill Summers), Blanche Hunt (Maggie Jones) and Sylvia Goodwin (Stephanie Cole) embodied the role of the acid-tongued busybody originally held by Ena, Sally Webster (Sally Dynevor) has grown snobbish, like Annie, and a number of the programme's female characters, such as Carla Connor (Alison King), mirror the vulnerability of Elsie and Bet. Other recurring archetypes include the war veteran such as Albert Tatlock (Jack Howarth), Percy Sugden (Bill Waddington) and Gary Windass (Mikey North), the bumbling retail manager like Leonard Swindley (Arthur Lowe), Reg Holdsworth (Ken Morley), Norris Cole (Malcolm Hebden), quick-tempered, tough tradesmen like Len Fairclough (Peter Adamson), Jim McDonald (Charles Lawson), Tommy Harris (Thomas Craig) and Owen Armstrong (Ian Puleston-Davies), and the perennial losers such as Stan and Hilda, Jack and Vera, Les Battersby (Bruce Jones), Beth Tinker (Lisa George) and Kirk Sutherland (Andrew Whyment).

Villains are also common character types such as Tracy Barlow (Kate Ford), Alan Bradley (Mark Eden), Jenny Bradley (Sally Ann Matthews), Rob Donovan (Marc Baylis), Frank Foster (Andrew Lancel), Tony Gordon (Gray O'Brien), Caz Hammond (Rhea Bailey), Richard Hillman (Brian Capron), Greg Kelly (Stephen Billington), Will Chatterton (Leon Ockenden), Nathan Curtis (Christopher Harper), Callum Logan (Sean Ward), Karl Munro (John Michie), Pat Phelan (Connor McIntyre), David Platt (Jack P. Shepherd), Maya Sharma (Sasha Behar), Kirsty Soames (Natalie Gumede) and John Stape (Graeme Hawley). The show's former archivist and scriptwriter Daran Little disagreed with the characterisation of the show as a collection of stereotypes. "Rather, remember that Elsie, Ena and others were the first of their kind ever seen on British television. If later characters are stereotypes, it's because they are from the same original mould. It is the hundreds of programmes that have followed which have copied "Coronation Street"."

Over the show's history, "Coronation Street" has highlighted a wide range of different social issues, including: rape, incest, murder, hit-and-run, cancer, adultery, sexual exploitation, child grooming, revenge porn, prostitution, poverty, homelessness, domestic violence, parental abuse, teenage pregnancy, late in life pregnancy, surrogacy, abortion, stillbirth, premature birth, miscarriage, adoption, fostering, male rape, alcoholism, drug addiction, psychotic episodes, gambling addiction, terminal illness, euthanasia, suicide, depression, postpartum depression, bipolar disorder, post-traumatic stress disorder, obsessive compulsive disorder, homosexuality, homosexuality in Islam, child sexual abuse, being transgender, HIV, deafness, epilepsy, osteoporosis, childhood cancer, EhlersâDanlos syndrome, brain aneurysm, multiple sclerosis, Alzheimer's disease, organ transplant, sepsis, amputation, myotonic dystrophy, identity theft, compulsive hoarding, drink driving, coercive control and skin whitening.

Between 9 December 1960 and 3 March 1961, "Coronation Street" was broadcast twice weekly, on Wednesday and Friday. During this period, the Friday episode was broadcast live, with the Wednesday episode being pre-recorded 15Â minutes later. When the programme went fully networked on 6 March 1961, broadcast days changed to Monday and Wednesday. The last regular episode to be shown live was broadcast on 3 February 1961.

The series was transmitted in black and white for the majority of the 1960s. Preparations were made to film episode 923, to be transmitted Wednesday 29 October 1969, in colour. This instalment featured the street's residents on a coach trip to the Lake District. In the end, suitable colour film stock for the cameras could not be found and the footage was shot in black and white. The following episode, transmitted Monday 3 November, was videotaped in colour but featured black and white film inserts and title sequence. Like BBC1, the ITV network was officially broadcast in black and white at this point (though programmes were actually broadcast in colour as early as July that year for colour transmission testing and adjustment) so the episode was seen by most in black and white.

The ITV network, like BBC1, began full colour transmissions on 15 November 1969. Daran Little, for many years the official programme archivist, claims that the first episode to be transmitted in colour was episode 930 shown on 24 November 1969.

In October 1970 a technicians' dispute turned into a work-to-rule when sound staff were denied a pay rise given to camera staff the year before for working with colour recording equipment. The terms of the work-to-rule were that staff refused to work with the new equipment (though the old black and white equipment had been disposed of by then) and therefore programmes were recorded and transmitted in black and white, including "Coronation Street" The dispute was resolved in early 1971 and the last black and white episode was broadcast on 8 February 1971.

Episode 5191, originally broadcast on 7 January 2002, was the first to be broadcast in widescreen format. "Coronation Street" was the last UK-wide soap to make the switch to 16:9 ("Take the High Road" remained in until it finished in 2003).

From 22 March 2010, "Coronation Street" was produced in 1080/50i for transmission on HDTV platforms on ITV HD. The first transmission in this format was episode 7351 on 31 May 2010 with a new set of titles and re-recorded theme tune. On 26 May 2010 ITV previewed the new HD titles on the "Coronation Street" website. Due to copyright reasons only viewers residing in the UK could see them on the ITV site.

"Coronation Street's" creator, Tony Warren, wrote the first 13 episodes of the programme in 1960, and continued to write for the programme intermittently until 1976. He had retained links with "Coronation Street" up to his death in 2016, often advising on storylines.

Harry Kershaw was the script editor for "Coronation Street" when the programme began in 1960, working alongside Tony Warren. Kershaw was also a script writer for the programme and the show's producer between 1962 and 1971. He remains the only person, along with John Finch, to have held the three posts of script editor, writer and producer. Kershaw continued to write for the programme until his retirement in January 1988.

Adele Rose was the longest-serving "Coronation Street" writer, completing 455 scripts between 1961 and 1998. She also created "Byker Grove".

Bill Podmore was the show's longest serving producer. By the time he stepped down in 1988 he had completed 13 years at the production helm. Nicknamed the "godfather" by the tabloid press, he was renowned for his tough, uncompromising style and was feared by both crew and cast alike. He is probably most famous for sacking Peter Adamson, the show's Len Fairclough, in 1983.

Iain Macleod is the current series producer.

Michael Apted, best known for the "Up!" series of documentaries was a director on the programme in the early 1960s. This period of his career marked the first of his many collaborations with writer Jack Rosenthal. Rosenthal, noted for such television plays as "Bar Mitzvah Boy", began his career on the show, writing over 150 episodes between 1961 and 1969. Paul Abbott was a story editor on the programme in the 1980s and began writing episodes in 1989, but left in 1993 to produce "Cracker", for which he later wrote, before creating his own dramas such as "Touching Evil" and "Shameless". Russell T Davies was briefly a storyliner on the programme in the mid-1990s, also writing the script for the direct-to-video special "" He, too, has become a noted writer of his own high-profile television drama programmes, including "Queer as Folk" and the 2005 revival of "Doctor Who". Jimmy McGovern also wrote some episodes.

The show's theme music, a cornet piece, accompanied by a brass band plus clarinet and double bass, reminiscent of northern band music, was written by Eric Spear.

The identity of the trumpeter was not public knowledge until 1994, when jazz musician and journalist Ron Simmonds revealed that it was the Surrey musician Ronnie Hunt. He added, "an attempt was made in later years to re-record that solo, using Stan Roderick, but it sounded too good, and they reverted to the old one." In 2004, the "Manchester Evening News" published a contradictory story that a young musician from Wilmslow called David Browning played the trumpet on both the original recording of the theme in 1960 and a re-recording in 1964, for a one-off payment of Â£36. In June 2009, the "Mail on Sunday" resolved the matter. Browning conceded that Hunt recorded the original in 1960, but believed that his own re-recording in 1964 or 1972 had been used since that date. ITV then confirmed to the "Mail" that a second version had been recorded in the 1970s, but was only used for a very short while before reverting to Hunt's 1960 recording. In the 1980s the same version was converted to stereo.

Ronnie Hunt said he was paid Â£6, and found the experience frustrating as Eric Spear insisted on many takes before obtaining the sound that he wanted. After taking a break in a local pub, Hunt achieved the desired mournful sound by playing very close to the microphone.

A new, completely re-recorded version of the theme tune replaced the original when the series started broadcasting in HD on 31 May 2010. It accompanied a new montage-style credits sequence featuring images of Manchester and Weatherfield.

A reggae version of the theme tune was recorded by The I-Royals and released by Media Marvels and WEA in 1983.

On 31 March 2017, it was revealed on the YouTube channel of Corrie that some of the soap's cast would sing a specially-written lyric, of which will be added to the new theme song that will be played, as of the first episode of the evening of Monday, 3 April 2017, but it turned out to be an April Fools joke.

Episodes in the 1960s, 70s, and 80s, regularly attracted figures of between 18 and 21Â million viewers, and during the 1990s and early 2000s, 14 to 16Â million per episode would be typical. Like most terrestrial television in the UK, a decline in viewership has taken place and the show posts an average audience of just under 9Â million per episode , remaining one of the highest rated programmes in the UK. Since "EastEnders" began airing in 1985 on the BBC, the two programmes have constantly battled it out for first place in the ratings.

"Coronation Street" rates as one of the most watched programmes on UK television for every day it is aired. The episode that aired on 2 January 1985, where Bet Lynch (Julie Goodyear) finds out she has got the job as manager of the Rovers Return, is the highest-rated single episode in the show's history, attracting 21.40 million viewers. The 25 December 1987 episode, where Hilda Ogden (Jean Alexander) leaves the street to start a new life as a housekeeper for long-term employer Dr Lowther, attracted a combined audience of 26.65 million for its original airing and omnibus repeat on 27 December 1987. This is the second-highest combined rating in the show's history. The show attracted its highest-ever combined rating of 26.93 million for the episode that aired on 15 (and 19) March 1989, where Rita Fairclough (Barbara Knox) is in hospital and Alan Bradley (Mark Eden) is hiding from the police after trying to kill Rita in the previous episode.

The regular exterior buildings shown in "Coronation Street" include a row of terrace houses, several townhouses, and communal areas including a newsagents ("The Kabin"), a cafÃ© ("Roy's Rolls"), a general grocery shop ("D&S Alahan's"), a factory ("Underworld") and "Rovers Return Inn" public house. The Rovers Return Inn is the main meeting place for the show's characters.

Between 1960 and 1968, street scenes were filmed before a set constructed in a studio, with the house fronts reduced in scale to 3/4 and constructed from wood. In 1968 Granada built an outside set not all that different from the interior version previously used, with the wooden faÃ§ades from the studio simply being erected on the new site. These were replaced with brick faÃ§ades, and back yards were added in the 1970s.

In 1982, a permanent full-street set was built in the Granada backlot, an area between Quay Street and Liverpool Road in Manchester. The set was constructed from reclaimed Salford brick. The set was updated in 1989 with the construction of a new factory, two shop units and three modern town houses on the south side of the street.

Between 1989 and 1999, the Granada Studios Tour allowed members of the public to visit the set. The exterior set was extended and updated in 1999. This update added to the Rosamund Street and Victoria Street faÃ§ades, and added a viaduct on Rosamund Street. Most interior scenes are shot in the adjoining purpose-built studio.

In 2008, "Victoria Court", an apartment building full of luxury flats, was started on Victoria Street.

In 2014, production moved to a new site at Trafford Wharf, a former dock area about two miles to the east, part of the MediaCityUK complex. The Trafford Wharf backlot is built upon a former truck stop site next to the Imperial War Museum North. It took two years from start to finish to recreate the iconic Street. The houses were built to almost full scale after previously being three-quarter size.

On 5 April 2014, the staff began to allow booked public visits to the old Quay Street set. An advert, with a voiceover from Victoria Wood, appeared on TV to advertise the tour. The tour was discontinued in December 2015.

On 12 March 2018, the extension of the " Victoria Street" set was officially unveiled. The new set features a garden, featuring a memorial bench paying tribute to the 22 victims of the Manchester Arena bombing, including "Coronation Street" super fan Martyn Hett. The precinct includes a Greater Manchester Police station called "Weatherfield Police station". As part of a product placement deal between three companies and ITV Studios, new additions include a Tram stop station which is named "Weatherfield North" with "Transport for Greater Manchester" "Metrolink" branding, and shop front facades of Costa Coffee and the Weatherfield branded Co-op Food store interior scenes have been screened and exterior scenes at the new set first aired on 20 April 2018.
On 20 April 2018, ITV announced that they had been granted official approval of planning permission to allow booked public visits to the MediaCityUK Trafford Wharf set. Tours commenced on weekends from 26 May 2018 onwards.

For years, "Coronation Street" has remained at the centre of ITV's prime time schedule. The programme is shown in the UK in six episodes, over three evenings a week on ITV. From Friday 9 December 1960 until Friday 3 March 1961, the programme was shown in two episodes broadcast Wednesday and Friday at 7Â pm. Schedules were changed and from Monday 6 March 1961 until Wednesday 11 October 1989, the programme was shown in two episodes broadcast Monday and Wednesday at 7:30Â pm. The third weekly episode was introduced on Friday 20 October 1989, broadcast at 7:30Â pm. From 1996, an extra episode was broadcast at 7:30Â pm on Sunday nights. Aside from Granada, the programme originally appeared on the following stations of the ITV network: Anglia Television, Associated-Rediffusion, Television Wales and the West, Scottish Television, Southern Television and Ulster Television. From episode 14 on Wednesday 25 January 1961, Tyne Tees Television broadcast the programme. That left ATV in the Midlands as the only ITV station not carrying the show. When they decided to broadcast the programme, national transmission was changed from Wednesday and Friday at 7Â pm to Monday and Wednesday at 7:30Â pm and the programme became fully networked under this new arrangement from episode 25 on Monday 6 March 1961.

As the ITV network grew over the next few years, the programme was transmitted by these new stations on these dates onward: Westward Television from episode 40 on 1 May 1961, Border Television from episode 76 on 4 September 1961, Grampian Television from episode 84 on 2 October 1961, Channel Television from episode 180 on 3 September 1962 and Teledu Cymru (north and west Wales) from episode 184 on 17 September 1962. At this point, the ITV network became complete and the programme was broadcast almost continuously across the country at 7:30Â pm on Monday and Wednesday for the next twenty-seven years.

From episode 2981 on Friday 20 October 1989 at 7:30Â pm, a third weekly episode was introduced and this increased to four episodes a week from episode 4096 on Sunday 24 November 1996, again at 7:30Â pm. The second Monday episode was introduced in 2002 and was broadcast at 7:30Â pm to usher in the return of Bet Lynch. The Monday 8:30Â pm episode was used intermittently during the popular Richard Hillman story line but has become fully scheduled since episode 5568 on Monday 25 August 2003. Additional episodes have been broadcast during the weekly schedule of ITV at certain times, notably in 2004 when, between 22 and 26 November, eight episodes were shown.

Older episodes had been broadcast by satellite and cable channel Granada Plus from launch in 1996. The first episodes shown were from episode 1588 (originally transmitted on Monday 5 April 1976) onwards. Originally listed and promoted as "Classic Coronation Street", the "classic" was dropped in early 2002, at which stage the episodes were from late 1989. By the time of the channel's closure in 2004, the repeats had reached February 1994.

In addition to this, "specials" were broadcast on Saturday afternoons in the early years of the channel with several episodes based on a particular theme or character(s) were shown. The latest episode shown in these specials was from 1991. In addition, on 27 and 28 December 2003, several Christmas Day editions of the show were broadcast.

From 23 July 2009 "Coronation Street" started to be broadcast in five instalments a week, at 7:30Â  and 8:30Â pm on Mondays and Fridays, and at 8:30Â pm on Thursdays. The Thursday episode replaced the former Wednesday show. Occasional late night episodes of "Coronation Street" begin at 10Â pm, due to the watershed. Repeat episodes, omnibus broadcasts and specials have been shown on ITV and ITV2. In January 2008 the omnibus returned to the main ITV channel where it was aired on Saturday mornings/afternoons depending on the schedule and times. In May 2008 it moved to Sunday mornings until August 2008 when it returned to Saturdays. In January 2009 it moved back to Sunday mornings usually broadcasting at around 9.25am until December 2010. In January 2011 the omnibus moved to Saturday mornings on ITV at 9.25am. During the Rugby World Cup, which took place in New Zealand, matches had to be broadcast on a Saturday morning, so the omnibus moved to Saturday lunchtimes/afternoons during September and October 2011. However, as of 22 October 2011 the omnibus moved back to Saturday mornings at 9.25am on ITV. From January 2012 the omnibus was no longer broadcast on ITV after four years, it stayed on ITV2 for eight years. From January 2020, the omnibus moved to ITV3. 

On 30 June 2011 it was confirmed that "Coronation Street" would return to its traditional 7:30Â pm timeslot on a Wednesday evening in September 2012. A sixth weekly episode aired on Wednesdays at 8:30pm from 20 September 2017. ITV also confirmed on this date that ITV3 would air afternoon timeslot sequential reruns of "Classic Coronation Street". Two classic episodes were retransmitted between Mondays to Fridays at 2:40Â pm until 3:45Â pm from 2 October 2017. The first episodes shown were from episode 2587 (originally transmitted on Wednesday 15 January 1986) onwards.

"Coronation Street" is shown in various countries worldwide. YouTube has the first episode and many others available as reruns.

The programme was first aired in Australia in 1963 on TCN-9 Sydney, GTV-9 Melbourne and NWS-9 Adelaide, and by 1966 "Coronation Street" was more popular in Australia than in the UK. The show eventually left free-to-air television in Australia in the 1970s. It briefly returned to the Nine Network in a daytime slot during 1994â1995. In 2005 STW-9 Perth began to show episodes before the 6Â pm news to improve the lead in to Nine News Perth, but this did not work and the show was cancelled a few months later. In 1996 Pay-TV began and Arena began screening the series in one-hour instalments on Saturdays and Sundays at 6:30Â pm EST. The series was later moved to Pay-TV channel UKTV where it is still shown. "Coronation Street" is shown Mon-Thu at 7:20Â pm EST & a double episode on Fridays. Episodes on UKTV are 1 week behind UK broadcast.

In Canada, "Coronation Street" is broadcast on CBC Television. Until 2011, episodes were shown in Canada approximately 10 months after they aired in Britain; however, beginning in the fall of 2011, the CBC began showing two episodes every weekday, in order to catch up with the ITV showings, at 6:30Â pm and 7Â pm local time Monday-Friday, with an omnibus on Sundays at 7.30am. By May 2014, the CBC was only two weeks behind Britain, so the show was reduced to a single showing weeknights at 6:30Â pm local time. The show debuted on Toronto's CBLT in July 1966. The 2002 edition of the "Guinness Book of Records" recognises the 1,144 episodes sold to the now-defunct CBC-owned Saskatoon, Saskatchewan, TV station CBKST by Granada TV on 31 May 1971 to be the largest number of TV shows ever purchased in one transaction. The show traditionally aired on weekday afternoons in Canada, with a Sunday morning omnibus. In 2004, CBC moved the weekday airings from their daytime slot to prime time. In light of austerity measures imposed on the CBC in 2012, which includes further cutbacks on non-Canadian programming, one of the foreign shows to remain on the CBC schedule is "Coronation Street", according to the CBC's director of content planning Christine Wilson, who commented: "Unofficially I can tell you "Coronation Street" is coming back. If it didn't come back, something would happen on Parliament Hill." Kirstine Stewart, the head of the CBC's English-language division, once remarked: "Coronation Street fans are the most loyal, except maybe for curling viewers, of all CBC viewers." In late September 2014, CBC aired extra episodes to become only one week behind the UK in airing of new episodes.

In the Republic of Ireland, "Coronation Street" aired on Virgin Media Three (formerly named Be3) for same night repeats and currently airs the Sunday omnibus from 1pm. The shows same night repeats currently air for an hour from 10:30pm on Virgin Media Two(formerly named 3e), on Mondayâs,Wednesdayâs and Fridayâs. The show was first aired in 1978, beginning with episodes from 1976. Ireland eventually caught up with the current UK episodes in 1983. Until 1992 it was broadcast on RTÃ2 and from 1992 to 2001 it was broadcast on RTÃ One. In 2001 Granada TV bought 45 percent of TV3, which resulted in TV3 broadcasting series from 2001 to 2014. In 2006, ITV sold its share of the channel but TV3. TV3 continued to buy the soap until the end of 2014 when it moved to UTV Ireland. Coronation Street has broadcast on each of the main Irish networks, except for the Irish language network TG4. From December 2016, "Coronation Street" returned to TV3 (now Virgin Media One). The show is consistently the channels most viewed programme every week.

In South Africa, "Coronation Street" episodes are broadcast three days after the UK air date on ITV Choice.

In New Zealand, "Coronation Street" has been shown locally since 1964, first on NZBC television until 1975, and then on TV One, which broadcasts it in a 4-episode/2-hour block on Fridays from 7:30Â pm. Since September 2014, TV One has added a 2-episode/1-hour block on Saturday from 8:30Â pm. Because TV One has never upgraded to showing the equivalent of five or six episodes per week, New Zealand continues to fall further and further behind with episodes, and is 23 months behind Britain (as of 28 March 2014). During the weekday nights of the week ending 11 April 2014 and previous weeks, Coronation Street was the least watched programme on TV One in the 7:30Â pm slot by a considerable margin in comparison to other weeknights, The serial aired on Tuesdays and Thursdays at 7:30Â pm until October 2011, when the show moved to a 5:30Â pm half-hour slot every weekday. The move proved unpopular with fans, and the series was quickly moved into its present prime-time slot within weeks. Episodes 7883, 7884, 7885 and 7886 were screened on 16 May 2014. These were originally aired in the UK between 4 and 11 June 2012. On 10 May 2018 it was announced that the current 2016 episodes would be moved to 1 p.m. Monday-Friday titled 'Catch-up Episodes' and for primetime Wednesday-Friday express episodes would be airing in New Zealand a week behind The United Kingdom titled '2018 Episodes' these changes would be taking place from 11 June 2018.

In the United States, "Coronation Street" is available by broadcast or cable only in northern markets where CBC coverage from Canada overlaps the border or is available on local cable systems. It was broadcast on CBC's US cable channel, Trio until the CBC sold its stake in the channel to Universal, before it was shut down in 2006. Beginning in 2009, episodes were available in the United States through Amazon.com's on-demand service, one month behind their original UK airdates. The final series of shows available from Amazon appears to be from November 2012, as no new episodes have been uploaded. On 15 January 2013, online distributor Hulu began airing episodes of the show, posting a new episode daily, two weeks after their original airdates. For a time, Hulu's website stated: "New episodes of Coronation Street will be unavailable as of April 7th, 2016", with the same being said for British soap "Hollyoaks", but Hulu is once again showing new episodes of Coronation Street as of April 2017, two weeks behind the UK airdate. The BBC/ITV service Britbox shows new episodes on the same day as the UK airing. "Coronation Street" was also shown on USA Network for an unknown period starting in 1982.

HM Forces and their families stationed overseas can watch "Coronation Street" on ITV, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands. It used to be shown on BFBS1.

Satellite channel ITV Choice shows the programme in Asia, Middle East, Cyprus, and Malta. In the United Arab Emirates, episodes of "Coronation Street" are broadcast one month after their UK showing.

"The Street", a magazine dedicated to the show, was launched in 1989. Edited by Bill Hill, the magazine contained a summary of recent storylines, interviews, articles about classic episodes, and stories that occurred from before 1960. The format was initially A5 size, expanding to A4 from the seventh issue. The magazine folded after issue 23 in 1993 when the publisher's contract with Granada Studios Tour expired and Granada wanted to produce their own magazine.

On 25 June 2010, a video game of the show was released on Nintendo DS. The game was developed by Mindscape, and allowed players to complete tasks in the fictitious city of Weatherfield.

In 1995, to commemorate the programme's 35th anniversary, a CD titled "The Coronation Street Album" was released, featuring cover versions of modern songs and standards by contemporary cast members.

In 2010, an album featuring songs sung by cast members was released to celebrate 50 years of "Coronation Street". The album is titled "Rogues, Angels, Heroes & Fools", and was later developed into a musical.

Granada launched one spin-off in 1965, "Pardon the Expression", following the story of clothing store manager Leonard Swindley (Arthur Lowe) after he left Weatherfield. Swindley's management experience was tested when he was appointed assistant manager at a fictional department store, Dobson and Hawks. Granada produced two series of the spin-off, which ended in 1966.

In 1967, Arthur Lowe returned as Leonard Swindley in "Turn Out the Lights", a short-lived sequel to "Pardon the Expression". It ran for just one series of six episodes before it was cancelled.

From 1985 to 1988 Granada TV produced a sitcom called "The Brothers McGregor" featuring a pair of half-brothers (one black, one white) who had appeared in a single episode of "Coronation Street" as old friends of Eddie Yeats and guests at his wedding. The original actors were unavailable so the characters were recast with Paul Barber and Philip Whitchurch. The show ran for 26 episodes over four series.

In 1985, a sister series, "Albion Market" was launched. It ran for one year, with 100 episodes produced.

In 2010, several actors from the show appeared on "The Jeremy Kyle Show" as their soap characters: David Platt (Jack P. Shepherd), Nick Tilsley (Ben Price) and Tina McIntyre (Michelle Keegan). In the fictional, semi-improvised scenario, David accused Nick (his brother) and Tina (his ex-girlfriend) of sleeping together.

"Coronation Street" and rival soap opera "EastEnders" had a crossover for "Children in Need" in November 2010 called "East Street". "EastEnders" stars that visited Weatherfield include Laurie Brett as Jane Beale, Charlie G. Hawkins as Darren Miller, Kylie Babbington as Jodie Gold, Nina Wadia as Zainab Masood and John Partridge as Christian Clarke.

On 21 December 2012, "Coronation Street" produced a Text Santa special entitled "A Christmas Corrie" which featured Norris Cole in the style of Scrooge, being visited by the ghosts of dead characters. The ghosts were Mike Baldwin, Maxine Peacock, Derek Wilton and Vera Duckworth. Other special guests include Torvill and Dean, Lorraine Kelly and Sheila Reid. The episode concluded with Norris learning the error of his ways and dancing on the cobbles. The original plan for this feature was to have included Jack Duckworth, along with Vera, but actor Bill Tarmey died before filming commenced. In the end a recording of his voice was played.

"Coronation Street: Family Album" was several documentaries about various families living on the street.

"FarewellÂ ..." was several documentaries featuring the best moments of a single character who had recently left the seriesâmost notably, Farewell Blanche (Hunt), Farewell Jack (Duckworth), Farewell Mike (Baldwin), Farewell Vera (Duckworth), Farewell Janice (Battersby), Farewell Liz (McDonald), Farewell Becky (McDonald), and Farewell Tina (McIntyre). Most of these were broadcast on the same day as the character's final scenes in the series.

"Stars on the Street" was aired around Christmas 2009. It featured actors from the soap talking about the famous guest stars who had appeared in the series including people who were in it before they were famous.

In December 2010, ITV made a few special programmes to mark the 50th anniversary. "Coronation Street Uncovered: Live", hosted by Stephen Mulhern was shown after the episode with the tram crash was aired on ITV 2. On 7 and 9 December a countdown on the greatest Corrie moments, "Coronation Street: 50 Years, 50 Moments", the viewers voted "The Barlows at Alcoholics Anonymous" as the greatest moment. On 10 December Paul O'Grady hosted a quiz show, "Coronation Street: The Big 50" with three teams from the soap and a celebrity team answering questions about Coronation Street and other soaps. Also, "Come Dine with Me" and "Celebrity Juice" aired Coronation Street specials in the anniversary week.

The German TV series "LindenstraÃe" took "Coronation Street" as the model. "LindenstraÃe" started in 1985.

Over the years "Coronation Street" has released several straight-to-video films. Unlike other soaps which often used straight-to-video films to cover more contentious plot lines that may not be allowed by the broadcaster, "Coronation Street" has largely used these films to reset their characters in other locations.

In 1995, "Coronation Street: The Cruise" also known as "Coronation Street: The Feature Length Special" was released on VHS to celebrate the 35th anniversary of the show. ITV heavily promoted the programme as a direct-to-video exclusive but broadcast a brief version of it on 24 March 1996. The Independent Television Commission investigated the broadcast, as viewers complained that ITV misled them.

In 1997, following the controversial cruise spin-off, "Coronation Street: Viva Las Vegas!" was released on VHS, featuring Jack Duckworth, Vera Duckworth, Fiona Middleton and Maxine Peacock on a trip to Las Vegas.

In 1999, six special episodes of "Coronation Street" were produced, following the story of Steve McDonald, Vicky McDonald, Vikram Desai, Bet Gilroy and Reg Holdsworth in Brighton. This video was titled "Coronation Street: Open All Hours" and released on VHS.

In 2008, ITV announced filming was to get underway for a new special DVD episode, "", following the Battersby-Brown family, which included the temporary return of Cilla Battersby-Brown.

In 2009, another DVD special, "", was released. The feature-length comedy drama followed Roy, Hayley and Becky as they travelled to Romania for the wedding of a face from their past.

The BBC commissioned a one-off drama called "The Road to Coronation Street", about how the series first came into being. Jessie Wallace plays Pat Phoenix (Elsie Tanner) with Lynda Baron as Violet Carson (Ena Sharples), Celia Imrie as Doris Speed (Annie Walker) and James Roache as his own father William Roache (Ken Barlow). It was broadcast on 16 September 2010 on BBC Four.

On 1 November 2010, "Coronation Street: A Knight's Tale" was released. Reg Holdsworth and Curly Watts returned in the film. Mary tries to take Norris to an apparently haunted castle where she hoped to seduce him. Rosie gets a job there and she takes Jason with her. Brian Capron also guest starred as an assumed relative of Richard Hillman. He rises out of a lake with a comedic "wink to the audience" after Hillman drowned in 2003.

A feature-length film is planned for a 2020 release in order to tie with the show's 60th anniversary.

On 21 December 2008, a web-based miniseries ran on ITV.com; called "Corrie Confidential"; the first episode featured the characters Rosie and Sophie Webster in "Underworld".

ITV.com launched a small spin-off drama series called 'Gary's Army Diaries' which revolves around Gary Windass's experiences in Afghanistan and the loss of his best friend, Quinny. Due to their popularity, the three five-minute episodes were recut into a single 30-minute episode, which was broadcast on ITV2.

William Roache and Anne Kirkbride starred as Ken and Deirdre in a series of ten three-minute internet 'webisodes'. The first episode of the series titled, "Ken and Deirdre's Bedtime Stories" was activated on Valentine's Day 2011.

In 2011, an internet based spin-off starring Helen Flanagan as Rosie Webster followed her on her quest to be a supermodel.

On 3 February 2014, another web-based miniseries ran on ITV.com; called "Streetcar Stories". It showed what Steve and Lloyd get up to during the late nights in their Streetcar cab office. The first episode shows Steve and Lloyd making a cup of tea with "The Stripper" playing in the background, referencing Morecambe and Wise's Breakfast Sketch. The second episode involves the pair having a biscuit dunking competition.

During the 'Who Attacked Ken' storyline, a mini series of police files was run on the official Coronation Street YouTube channel. They outlined the suspects' details and possible motives.

In August 2010, many "Coronation Street" characters were brought to the stage in Jonathan Harvey's comedy play "Corrie!". The play was commissioned to celebrate the 50th Anniversary of the TV series and was presented at The Lowry in Salford, England by ITV Studios and Phil McIntyre Entertainments. Featuring a cast of six actors who alternate roles of favourite characters including Ena Sharples, Hilda Ogden, Hayley and Roy, Richard Hillman, Jack Duckworth, Bet Lynch, Steve, Karen and Becky, the play weaves together some of the most memorable moments from the TV show. It toured UK theatres between February 2011 and July 2011 with guest star narrators including Roy Barraclough, Ken Morley and Gaynor Faye.

The British rock band Queen produced a single "I Want to Break Free" in 1984 which reached number 3 position in UK charts and which is largely known for its music video for which all the band members dressed in women's clothes, which parodied the characters and is considered an homage to the show. The video depicts Mercury as a housewife, loosely based on Bet Lynch, who wants to "break free" from his life. Although Lynch was a blonde in the soap opera, Mercury thought he would look too silly as a blonde and chose a dark wig. May plays another, more relaxed housewife based on Hilda Ogden.

As an April Fools' Day joke in 2019, TheJournal.ie claimed that Leader of the Opposition and Labour Jeremy Corbyn had made an attempt to appear in an episode of "Coronation Street" in response to Prime Minister Theresa May's supposed appearance in a special live episode, where she was to issue a final plea for unity on Brexit. In the joke, Corbyn's plan had not come to fruition, with members of "Coronation Street"'s crew deeming his request inappropriate in light of the devastation already wreaked upon the soap opera's characters following its most recent knicker factory tragedy.

Cadbury was the first sponsor of "Coronation Street" beginning in July 1996. In the summer of 2006, Cadbury Trebor Bassetts had to recall over one million chocolate bars, due to suspected salmonella contamination, and "Coronation Street" stopped the sponsorship for several months. In 2006, Cadbury did not renew their contract, but agreed to sponsor the show until "Coronation Street" found a new sponsor.

In July 2007, an ITV press release announced that Harveys was the new sponsor of "Coronation Street" on the ITV Network. Harveys' sponsorship began on 30 September 2007. In the "Coronation Street: Romanian Holiday" film, Roy and Hayley Cropper are filmed in front of a Harveys store. In "Coronation Street: A Knights Tale", a Harveys truck can be seen driving past Mary Taylor's motorhome to further endorse the brand. On 11 April 2012, it was announced that Harveys had decided not to renew their contract and ceased sponsorship in December 2012. Compare The Market were named as the new sponsor.

In November 2011, a Nationwide Building Society ATM in Dev Alahan's corner shop became the first use of paid-for product placement in a UK primetime show. In 2018, the shop fronts of Co-Op and Costa Coffee were added to the sets, along with characters using shopping bags with the respective logos on as props.

Hyundai are the current sponsor since January 2015 in the Republic of Ireland, aired on Virgin Media One.

"Coronation Street" is the second most award-winning British soap opera in the UK, behind rival soap "EastEnders" and just ahead of "Emmerdale".





</doc>
<doc id="6852" url="https://en.wikipedia.org/wiki?curid=6852" title="Caligula">
Caligula

Caligula (; ; 31 August 12 â 24 January 41 AD) was Roman emperor from 37 to 41 AD. The son of the popular Roman general Germanicus and Augustus's granddaughter Agrippina the Elder, Caligula was born into the first ruling family of the Roman Empire, conventionally known as the Julio-Claudian dynasty. Germanicus's uncle and adoptive father, Tiberius, succeeded Augustus as emperor of Rome in 14 AD.

Although he was born Gaius Caesar, after Julius Caesar, he acquired the nickname "Caligula" (meaning "little [soldier's] boot", the diminutive form of "caliga") from his father's soldiers during their campaign in Germania. When Germanicus died at Antioch in 19, Agrippina returned with her six children to Rome, where she became entangled in a bitter feud with Tiberius. The conflict eventually led to the destruction of her family, with Caligula as the sole male survivor. Untouched by the deadly intrigues, Caligula accepted an invitation in 31 to join the emperor on the island of Capri, where Tiberius had withdrawn five years earlier. Following the death of Tiberius, Caligula succeeded his adoptive grandfather as emperor in 37 AD.

There are few surviving sources about the reign of Caligula, although he is described as a noble and moderate emperor during the first six months of his rule. After this, the sources focus upon his cruelty, sadism, extravagance, and sexual perversion, presenting him as an insane tyrant. While the reliability of these sources is questionable, it is known that during his brief reign, Caligula worked to increase the unconstrained personal power of the emperor, as opposed to countervailing powers within the principate. He directed much of his attention to ambitious construction projects and luxurious dwellings for himself, and initiated the construction of two aqueducts in Rome: the Aqua Claudia and the Anio Novus. During his reign, the empire annexed the client kingdom of Mauretania as a province.

In early 41, Caligula was assassinated as a result of a conspiracy by officers of the Praetorian Guard, senators, and courtiers. The conspirators' attempt to use the opportunity to restore the Roman Republic was thwarted, however. On the day of the assassination of Caligula, the Praetorians declared Caligula's uncle, Claudius, the next Roman emperor. Although the Julio-Claudian dynasty continued to rule the empire until the fall of his nephew Nero in 68, Caligula's death marked the official end of the Julii Caesares in the male line.

Gaius Julius Caesar (named in honor of his famous relative) was born in Antium (modern Anzio and Nettuno) on 31 August 12 AD, the third of six surviving children born to Germanicus and his second cousin Agrippina the Elder. Gaius had two older brothers, Nero and Drusus, as well as three younger sisters, Agrippina the Younger, Julia Drusilla and Julia Livilla. He was also a nephew of Claudius, Germanicus' younger brother and the future emperor.

Agrippina the Elder was the daughter of Marcus Vipsanius Agrippa and Julia the Elder. She was a granddaughter of Augustus and Scribonia on her mother's side. Through Agrippina, Augustus was the maternal great-grandfather of Gaius.

As a boy of just two or three, Gaius accompanied his father, Germanicus, on campaigns in the north of Germania. The soldiers were amused that Gaius was dressed in a miniature soldier's outfit, including boots and armour. He was soon given his nickname "Caligula", meaning "little (soldier's) boot" in Latin, after the small boots (caligae) he wore. Gaius, though, reportedly grew to dislike this nickname.

Suetonius claims that Germanicus was poisoned in Syria by an agent of Tiberius, who viewed Germanicus as a political rival.

After the death of his father, Caligula lived with his mother until her relations with Tiberius deteriorated. Tiberius would not allow Agrippina to remarry for fear her husband would be a rival. Agrippina and Caligula's brother, Nero, were banished in 29 on charges of treason.

The adolescent Caligula was then sent to live with his great-grandmother (and Tiberius's mother) Livia. After her death, he was sent to live with his grandmother Antonia Minor. In 30, his brother, Drusus Caesar, was imprisoned on charges of treason and his brother Nero died in exile from either starvation or suicide. Suetonius writes that after the banishment of his mother and brothers, Caligula and his sisters were nothing more than prisoners of Tiberius under the close watch of soldiers.

In 31, Caligula was remanded to the personal care of Tiberius on Capri, where he lived for six years. To the surprise of many, Caligula was spared by Tiberius. According to historians, Caligula was an excellent natural actor and, recognizing danger, hid all his resentment towards Tiberius. An observer said of Caligula, "Never was there a better servant or a worse master!"

Caligula claimed to have planned to kill Tiberius with a dagger to avenge his mother and brother: however, having brought the weapon into Tiberius's bedroom he did not kill the Emperor but instead threw the dagger down on the floor. Supposedly Tiberius knew of this but never dared to do anything about it. Suetonius claims that Caligula was already cruel and vicious: he writes that, when Tiberius brought Caligula to Capri, his purpose was to allow Caligula to live in order that he "prove the ruin of himself and of all men, and that he was rearing a viper for the Roman people and a Phaethon for the world."

In 33, Tiberius gave Caligula an honorary quaestorship, a position he held until his rise to emperor. Meanwhile, both Caligula's mother and his brother Drusus died in prison. Caligula was briefly married to Junia Claudilla, in 33, though she died in childbirth the following year. Caligula spent time befriending the Praetorian prefect, Naevius Sutorius Macro, an important ally. Macro spoke well of Caligula to Tiberius, attempting to quell any ill will or suspicion the Emperor felt towards Caligula.

In 35, Caligula was named joint heir to Tiberius's estate along with Tiberius Gemellus.

When Tiberius died on 16 March 37 AD, his estate and the titles of the principate were left to Caligula and Tiberius's own grandson, Gemellus, who were to serve as joint heirs. Although Tiberius was 77 and on his death bed, some ancient historians still conjecture that he was murdered. Tacitus writes that the Praetorian Prefect, Macro, smothered Tiberius with a pillow to hasten Caligula's accession, much to the joy of the Roman people, while Suetonius writes that Caligula may have carried out the killing, though this is not recorded by any other ancient historian. Seneca the Elder and Philo, who both wrote during Tiberius's reign, as well as Josephus, record Tiberius as dying a natural death. Backed by Macro, Caligula had Tiberius's will nullified with regard to Gemellus on grounds of insanity, but otherwise carried out Tiberius's wishes.
Caligula accepted the powers of the principate as conferred by the Senate and entered Rome on 28 March amid a crowd that hailed him as "our baby" and "our star", among other nicknames. Caligula is described as the first emperor who was admired by everyone in "all the world, from the rising to the setting sun." Caligula was loved by many for being the beloved son of the popular Germanicus, and because he was not Tiberius. Suetonius said that over 160,000 animals were sacrificed during three months of public rejoicing to usher in the new reign. Philo describes the first seven months of Caligula's reign as completely blissful.

Caligula's first acts were said to be generous in spirit, though many were political in nature. To gain support, he granted bonuses to the military, including the Praetorian Guard, city troops and the army outside Italy. He destroyed Tiberius's treason papers, declared that treason trials were a thing of the past, and recalled those who had been sent into exile. He helped those who had been harmed by the imperial tax system, banished certain sexual deviants, and put on lavish spectacles for the public, including gladiatorial games. Caligula collected and brought back the bones of his mother and of his brothers and deposited their remains in the tomb of Augustus.

In October 37, Caligula fell seriously ill, or perhaps was poisoned. He soon recovered from his illness, but many believed that the illness turned the young emperor toward the diabolical: he started to kill off or exile those who were close to him or whom he saw as a serious threat. Perhaps his illness reminded him of his mortality and of the desire of others to advance into his place. He had his cousin and adopted son Tiberius Gemellus executed â an act that outraged Caligula's and Gemellus's mutual grandmother Antonia Minor. She is said to have committed suicide, although Suetonius hints that Caligula actually poisoned her. He had his father-in-law Marcus Junius Silanus and his brother-in-law Marcus Lepidus executed as well. His uncle Claudius was spared only because Caligula preferred to keep him as a laughing stock. His favourite sister Julia Drusilla died in 38 of a fever: his other two sisters, Livilla and Agrippina the Younger, were exiled. He hated being the grandson of Agrippa and slandered Augustus by repeating a falsehood that his mother was actually conceived as the result of an incestuous relationship between Augustus and his daughter Julia the Elder.

In 38, Caligula focused his attention on political and public reform. He published the accounts of public funds, which had not been made public during the reign of Tiberius. He aided those who lost property in fires, abolished certain taxes, and gave out prizes to the public at gymnastic events. He allowed new members into the equestrian and senatorial orders.

Perhaps most significantly, he restored the practice of democratic elections. Cassius Dio said that this act "though delighting the rabble, grieved the sensible, who stopped to reflect, that if the offices should fall once more into the hands of the manyÂ ... many disasters would result".

During the same year, though, Caligula was criticized for executing people without full trials and for forcing the Praetorian prefect, Macro, to commit suicide. Macro had fallen out of favor with the emperor, probably due to an attempt to ally himself with Gemellus when it appeared that Caligula might die of fever.

According to Cassius Dio, a financial crisis emerged in 39. Suetonius places the beginning of this crisis in 38. Caligula's political payments for support, generosity and extravagance had exhausted the state's treasury. Ancient historians state that Caligula began falsely accusing, fining and even killing individuals for the purpose of seizing their estates.

Historians describe a number of Caligula's other desperate measures. To gain funds, Caligula asked the public to lend the state money. He levied taxes on lawsuits, weddings and prostitution. Caligula began auctioning the lives of the gladiators at shows. Wills that left items to Tiberius were reinterpreted to leave the items instead to Caligula. Centurions who had acquired property by plunder were forced to turn over spoils to the state.

The current and past highway commissioners were accused of incompetence and embezzlement and forced to repay money. According to Suetonius, in the first year of Caligula's reign he squandered 2.7Â billion sesterces that Tiberius had amassed. His nephew Nero Caesar both envied and admired the fact that Gaius had run through the vast wealth Tiberius had left him in so short a time.

However, some historians have shown skepticism towards the large number of sesterces quoted by Suetonius and Dio. According to Wilkinson, Caligula's use of precious metals to mint coins throughout his principate indicates that the treasury most likely never fell into bankruptcy. He does point out, however, that it is difficult to ascertain whether the purported 'squandered wealth' was from the treasury alone due to the blurring of "the division between the private wealth of the emperor and his income as head of state." Furthermore, Alston points out that Caligula's successor, Claudius, was able to donate 15,000 sesterces to each member of the praetorian guard in 41, suggesting the Roman treasury was solvent.
A brief famine of unknown extent occurred, perhaps caused by this financial crisis, but Suetonius claims it resulted from Caligula's seizure of public carriages; according to Seneca, grain imports were disrupted because Caligula re-purposed grain boats for a pontoon bridge.

Despite financial difficulties, Caligula embarked on a number of construction projects during his reign. Some were for the public good, though others were for himself.

Josephus describes Caligula's improvements to the harbours at Rhegium and Sicily, allowing increased grain imports from Egypt, as his greatest contributions. These improvements may have been in response to the famine.

Caligula completed the temple of Augustus and the theatre of Pompey and began an amphitheatre beside the Saepta. He expanded the imperial palace. He began the aqueducts Aqua Claudia and Anio Novus, which Pliny the Elder considered engineering marvels. He built a large racetrack known as the "circus of Gaius and Nero" and had an Egyptian obelisk (now known as the "Vatican Obelisk") transported by sea and erected in the middle of Rome.

At Syracuse, he repaired the city walls and the temples of the gods. He had new roads built and pushed to keep roads in good condition. He had planned to rebuild the palace of Polycrates at Samos, to finish the temple of Didymaean Apollo at Ephesus and to found a city high up in the Alps. He planned to dig a canal through the Isthmus of Corinth in Greece and sent a chief centurion to survey the work.
In 39, Caligula performed a spectacular stunt by ordering a temporary floating bridge to be built using ships as pontoons, stretching for over two miles from the resort of Baiae to the neighbouring port of Puteoli. It was said that the bridge was to rival the Persian king Xerxes' pontoon bridge crossing of the Hellespont. Caligula, who could not swim, then proceeded to ride his favourite horse Incitatus across, wearing the breastplate of Alexander the Great. This act was in defiance of a prediction by Tiberius's soothsayer Thrasyllus of Mendes that Caligula had "no more chance of becoming emperor than of riding a horse across the Bay of Baiae".

Caligula had two large ships constructed for himself (which were recovered from the bottom of Lake Nemi around 1930). The ships were among the largest vessels in the ancient world. The smaller ship was designed as a temple dedicated to Diana. The larger ship was essentially an elaborate floating palace with marble floors and plumbing. The ships burned in 1944 after an attack in the Second World War; almost nothing remains of their hulls, though many archaeological treasures remain intact in the museum at Lake Nemi and in the Museo Nazionale Romano (Palazzo Massimo) at Rome.

In 39, relations between Caligula and the Roman Senate deteriorated. The subject of their disagreement is unknown. A number of factors, though, aggravated this feud. The Senate had become accustomed to ruling without an emperor between the departure of Tiberius for Capri in 26 and Caligula's accession. Additionally, Tiberius' treason trials had eliminated a number of pro-Julian senators such as Asinius Gallus.

Caligula reviewed Tiberius' records of treason trials and decided, based on their actions during these trials, that numerous senators were not trustworthy. He ordered a new set of investigations and trials. He replaced the consul and had several senators put to death. Suetonius reports that other senators were degraded by being forced to wait on him and run beside his chariot.

Soon after his break with the Senate, Caligula faced a number of additional conspiracies against him. A conspiracy involving his brother-in-law was foiled in late 39. Soon afterwards, the Governor of Germany, Gnaeus Cornelius Lentulus Gaetulicus, was executed for connections to a conspiracy.

In 40, Caligula expanded the Roman Empire into Mauretania and made a significant attempt at expanding into Britannia â even challenging Neptune in his campaign. The conquest of Britannia was fully realized by his successors.

Mauretania was a client kingdom of Rome ruled by Ptolemy of Mauretania. Caligula invited Ptolemy to Rome and then suddenly had him executed. Mauretania was annexed by Caligula and subsequently divided into two provinces, Mauretania Tingitana and Mauretania Caesariensis, separated by the river Malua. Pliny claims that division was the work of Caligula, but Dio states that in 42 an uprising took place, which was subdued by Gaius Suetonius Paulinus and Gnaeus Hosidius Geta, and the division only took place after this. This confusion might mean that Caligula decided to divide the province, but the division was postponed because of the rebellion. The first known equestrian governor of the two provinces was Marcus Fadius Celer Flavianus, in office in 44.

Details on the Mauretanian events of 39â44 are unclear. Cassius Dio wrote an entire chapter on the annexation of Mauretania by Caligula, but it is now lost. Caligula's move seemingly had a strictly personal political motive â fear and jealousy of his cousin Ptolemy â and thus the expansion may not have been prompted by pressing military or economic needs. However, the rebellion of Tacfarinas had shown how exposed Africa Proconsularis was to its west and how the Mauretanian client kings were unable to provide protection to the province, and it is thus possible that Caligula's expansion was a prudent response to potential future threats.

There seems to have been a northern campaign to Britannia that was aborted. This campaign is derided by ancient historians with accounts of Gauls dressed up as Germanic tribesmen at his triumph and Roman troops ordered to collect seashells as "spoils of the sea". The few primary sources disagree on what precisely occurred. Modern historians have put forward numerous theories in an attempt to explain these actions. This trip to the English Channel could have merely been a training and scouting mission. The mission may have been to accept the surrender of the British chieftain Adminius. "Seashells", or "conchae" in Latin, may be a metaphor for something else such as female genitalia (perhaps the troops visited brothels) or boats (perhaps they captured several small British boats).

When several client kings came to Rome to pay their respects to him and argued about their nobility of descent, he allegedly cried out the Homeric line: "Let there be one lord, one king." In 40, Caligula began implementing very controversial policies that introduced religion into his political role. Caligula began appearing in public dressed as various gods and demigods such as Hercules, Mercury, Venus and Apollo. Reportedly, he began referring to himself as a god when meeting with politicians and he was referred to as "Jupiter" on occasion in public documents.

A sacred precinct was set apart for his worship at Miletus in the province of Asia and two temples were erected for worship of him in Rome. The Temple of Castor and Pollux on the forum was linked directly to the imperial residence on the Palatine and dedicated to Caligula. He would appear there on occasion and present himself as a god to the public. Caligula had the heads removed from various statues of gods located across Rome and replaced them with his own. It is said that he wished to be worshipped as "Neos Helios", the "New Sun". Indeed, he was represented as a sun god on Egyptian coins.

Caligula's religious policy was a departure from that of his predecessors. According to Cassius Dio, living emperors could be worshipped as divine in the east and dead emperors could be worshipped as divine in Rome. Augustus had the public worship his spirit on occasion, but Dio describes this as an extreme act that emperors generally shied away from. Caligula took things a step further and had those in Rome, including senators, worship him as a tangible, living god.

Caligula needed to quell several riots and conspiracies in the eastern territories during his reign. Aiding him in his actions was his good friend, Herod Agrippa, who became governor of the territories of Batanaea and Trachonitis after Caligula became emperor in 37.

The cause of tensions in the east was complicated, involving the spread of Greek culture, Roman Law and the rights of Jews in the empire.

Caligula did not trust the prefect of Egypt, Aulus Avilius Flaccus. Flaccus had been loyal to Tiberius, had conspired against Caligula's mother and had connections with Egyptian separatists. In 38, Caligula sent Agrippa to Alexandria unannounced to check on Flaccus. According to Philo, the visit was met with jeers from the Greek population who saw Agrippa as the king of the Jews. Flaccus tried to placate both the Greek population and Caligula by having statues of the emperor placed in Jewish synagogues. As a result, riots broke out in the city. Caligula responded by removing Flaccus from his position and executing him.

In 39, Agrippa accused Herod Antipas, the tetrarch of Galilee and Perea, of planning a rebellion against Roman rule with the help of Parthia. Herod Antipas confessed and Caligula exiled him. Agrippa was rewarded with his territories.

Riots again erupted in Alexandria in 40 between Jews and Greeks. Jews were accused of not honouring the emperor. Disputes occurred in the city of Jamnia. Jews were angered by the erection of a clay altar and destroyed it. In response, Caligula ordered the erection of a statue of himself in the Jewish Temple of Jerusalem, a demand in conflict with Jewish monotheism. In this context, Philo wrote that Caligula "regarded the Jews with most especial suspicion, as if they were the only persons who cherished wishes opposed to his".

The Governor of Syria, Publius Petronius, fearing civil war if the order were carried out, delayed implementing it for nearly a year. Agrippa finally convinced Caligula to reverse the order. However, Caligula issued a second order to have his statue erected in the Temple of Jerusalem. In Rome, another statue of himself, of colossal size, was made of gilt brass for the purpose. The Temple of Jerusalem was then transformed into a temple for Caligula, and it was called the Temple of Illustrious Gaius the New Jupiter.

Philo of Alexandria and Seneca the Younger, contemporaries of Caligula, describe him as an insane emperor who was self-absorbed, short-tempered, killed on a whim, and indulged in too much spending and sex. He is accused of sleeping with other men's wives and bragging about it, killing for mere amusement, deliberately wasting money on his bridge, causing starvation, and wanting a statue of himself in the Temple of Jerusalem for his worship. Once, at some games at which he was presiding, he was said to have ordered his guards to throw an entire section of the audience into the arena during the intermission to be eaten by the wild beasts because there were no prisoners to be used and he was bored.

While repeating the earlier stories, the later sources of Suetonius and Cassius Dio provide additional tales of insanity. They accuse Caligula of incest with his sisters, Agrippina the Younger, Drusilla, and Livilla, and say he prostituted them to other men. They state he sent troops on illogical military exercises, turned the palace into a brothel, and, most famously, planned or promised to make his horse, Incitatus, a consul,
and actually appointed him a priest.

The validity of these accounts is debatable. In Roman political culture, insanity and sexual perversity were often presented hand-in-hand with poor government.

Caligula's actions as emperor were described as being especially harsh to the Senate, to the nobility and to the equestrian order. According to Josephus, these actions led to several failed conspiracies against Caligula. Eventually, officers within the Praetorian Guard led by Cassius Chaerea succeeded in murdering the emperor. The plot is described as having been planned by three men, but many in the senate, army and equestrian order were said to have been informed of it and involved in it.

The situation had escalated when, in 40, Caligula announced to the Senate that he planned to leave Rome permanently and to move to Alexandria in Egypt, where he hoped to be worshiped as a living god. The prospect of Rome losing its emperor and thus its political power was the final straw for many. Such a move would have left both the Senate and the Praetorian Guard powerless to stop Caligula's repression and debauchery. With this in mind Chaerea convinced his fellow conspirators, who included Marcus Vinicius and Lucius Annius Vinicianus, to put their plot into action quickly.

According to Josephus, Chaerea had political motivations for the assassination. Suetonius sees the motive in Caligula calling Chaerea derogatory names. Caligula considered Chaerea effeminate because of a weak voice and for not being firm with tax collection. Caligula would mock Chaerea with names like "Priapus" and "Venus".

On 22 January 41 (Suetonius gives the date as 24 January), Cassius Chaerea and other guardsmen accosted Caligula as he addressed an acting troupe of young men beneath the palace, during a series of games and dramatics being held for the Divine Augustus. Details recorded on the events vary somewhat from source to source, but they agree that Chaerea stabbed Caligula first, followed by a number of conspirators. Suetonius records that Caligula's death resembled that of Julius Caesar. He states that both the elder Gaius Julius Caesar (Julius Caesar) and the younger Gaius Julius Caesar (Caligula) were stabbed 30 times by conspirators led by a man named Cassius (Cassius Longinus and Cassius Chaerea). By the time Caligula's loyal Germanic guard responded, the Emperor was already dead. The Germanic guard, stricken with grief and rage, responded with a rampaging attack on the assassins, conspirators, innocent senators and bystanders alike. These wounded conspirators were treated by the physician Arcyon.

The "cryptoporticus" (underground corridor) beneath the imperial palaces on the Palatine Hill where this event took place was discovered by archaeologists in 2008.

The senate attempted to use Caligula's death as an opportunity to restore the Republic. Chaerea tried to persuade the military to support the Senate. The military, though, remained loyal to the idea of imperial monarchy. The grieving Roman people assembled and demanded that Caligula's murderers be brought to justice. Uncomfortable with lingering imperial support, the assassins sought out and killed Caligula's wife, Caesonia, and killed their young daughter, Julia Drusilla, by smashing her head against a wall. They were unable to reach Caligula's uncle, Claudius. After a soldier, Gratus, found Claudius hiding behind a palace curtain, he was spirited out of the city by a sympathetic faction of the Praetorian Guard to their nearby camp.

Claudius became emperor after procuring the support of the Praetorian Guard. He ordered the execution of Chaerea and of any other known conspirators involved in the death of Caligula.
According to Suetonius, Caligula's body was placed under turf until it was burned and entombed by his sisters. He was buried within the Mausoleum of Augustus; in 410, during the Sack of Rome, the ashes in the tomb were scattered.

The facts and circumstances of Caligula's reign are mostly lost to history. Only two sources contemporary with Caligula have survived â the works of Philo and Seneca. Philo's works, "On the Embassy to Gaius" and "Flaccus", give some details on Caligula's early reign, but mostly focus on events surrounding the Jewish population in Judea and Egypt with whom he sympathizes. Seneca's various works give mostly scattered anecdotes on Caligula's personality. Seneca was almost put to death by Caligula in AD 39 likely due to his associations with conspirators.

At one time, there were detailed contemporaneous histories on Caligula, but they are now lost. Additionally, the historians who wrote them are described as biased, either overly critical or praising of Caligula. Nonetheless, these lost primary sources, along with the works of Seneca and Philo, were the basis of surviving secondary and tertiary histories on Caligula written by the next generations of historians. A few of the contemporaneous historians are known by name. Fabius Rusticus and Cluvius Rufus both wrote condemning histories on Caligula that are now lost. Fabius Rusticus was a friend of Seneca who was known for historical embellishment and misrepresentation. Cluvius Rufus was a senator involved in the assassination of Caligula.

Caligula's sister, Agrippina the Younger, wrote an autobiography that certainly included a detailed explanation of Caligula's reign, but it too is lost. Agrippina was banished by Caligula for her connection to Marcus Lepidus, who conspired against him. The inheritance of Nero, Agrippina's son and the future emperor, was seized by Caligula. Gaetulicus, a poet, produced a number of flattering writings about Caligula, but they are lost.

The bulk of what is known of Caligula comes from Suetonius and Cassius Dio. Suetonius wrote his history on Caligula 80 years after his death, while Cassius Dio wrote his history over 180 years after Caligula's death. Cassius Dio's work is invaluable because it alone gives a loose chronology of Caligula's reign.

A handful of other sources add a limited perspective on Caligula. Josephus gives a detailed description of Caligula's assassination. Tacitus provides some information on Caligula's life under Tiberius. In a now lost portion of his "Annals", Tacitus gave a detailed history of Caligula. Pliny the Elder's "Natural History" has a few brief references to Caligula.

There are few surviving sources on Caligula and none of them paints Caligula in a favourable light. The paucity of sources has resulted in significant gaps in modern knowledge of the reign of Caligula. Little is written on the first two years of Caligula's reign. Additionally, there are only limited details on later significant events, such as the annexation of Mauretania, Caligula's military actions in Britannia, and his feud with the Roman Senate.

All surviving sources, except Pliny the Elder, characterize Caligula as insane. However, it is not known whether they are speaking figuratively or literally. Additionally, given Caligula's unpopularity among the surviving sources, it is difficult to separate fact from fiction. Recent sources are divided in attempting to ascribe a medical reason for his behavior, citing as possibilities encephalitis, epilepsy or meningitis. The question of whether or not Caligula was insane (especially after his illness early in his reign) remains unanswered.

Philo of Alexandria, Josephus and Seneca state that Caligula was insane, but describe this madness as a personality trait that came through experience. Seneca states that Caligula became arrogant, angry and insulting once he became emperor and uses his personality flaws as examples his readers can learn from. According to Josephus, power made Caligula incredibly conceited and led him to think he was a god. Philo of Alexandria reports that Caligula became ruthless after nearly dying of an illness in the eighth month of his reign in 37. Juvenal reports he was given a magic potion that drove him insane.

Suetonius said that Caligula suffered from "falling sickness", or epilepsy, when he was young. Modern historians have theorized that Caligula lived with a daily fear of seizures. Despite swimming being a part of imperial education, Caligula could not swim. Epileptics are discouraged from swimming in open waters because unexpected fits in such difficult rescue circumstances can be fatal. Caligula reportedly talked to the full moon: Epilepsy was long associated with the moon.

Suetonius described Caligula as sickly-looking, skinny and pale: "he was tall, very pale, ill-shaped, his neck and legs very slender, his eyes and temples hollow, his brows broad and knit, his hair thin, and the crown of the head bald. The other parts of his body were much covered with hair ... He was crazy both in body and mind, being subject, when a boy, to the falling sickness. When he arrived at the age of manhood he endured fatigue tolerably well. Occasionally he was liable to faintness, during which he remained incapable of any effort". Based on scientific reconstructions of his official painted busts, Caligula had brown hair, brown eyes, and fair skin.

Some modern historians think that Caligula suffered from hyperthyroidism. This diagnosis is mainly attributed to Caligula's irritability and his "stare" as described by Pliny the Elder.

On 17 January 2011, police in Nemi, Italy, announced that they believed they had discovered the site of Caligula's burial, after arresting a thief caught smuggling a statue which they believed to be of the emperor. The claim has been met with scepticism by Cambridge historian Mary Beard.





"Caligula", by French author Albert Camus, is a play in which Caligula returns after deserting the palace for three days and three nights following the death of his beloved sister, Drusilla. The young emperor then uses his unfettered power to "bring the impossible into the realm of the likely".

In the novel "I, Claudius" by English writer Robert Graves, Caligula is presented as being a murderous sociopath from his childhood, who became clinically insane early in his reign. At the age of only ten, he drove his father Germanicus to despair and death by secretly terrorising him. Graves's Caligula commits incest with all three of his sisters and is implied to have murdered Drusilla. In the 1976 BBC TV adaptation, Caligula was played by English actor John Hurt.




 


</doc>
<doc id="6854" url="https://en.wikipedia.org/wiki?curid=6854" title="ChurchâTuring thesis">
ChurchâTuring thesis

In computability theory, the ChurchâTuring thesis (also known as computability thesis, the TuringâChurch thesis, the ChurchâTuring conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a hypothesis about the nature of computable functions. It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine. The thesis is named after American mathematician Alonzo Church and the British mathematician Alan Turing. Before the precise definition of computable function, mathematicians often used the informal term effectively calculable to describe functions that are computable by paper-and-pencil methods. In the 1930s, several independent attempts were made to formalize the notion of computability:


Church and Turing proved that these three formally defined classes of computable functions coincide: a function is Î»-computable if and only if it is Turing computable, and if and only if it is "general recursive". This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes. Other formal attempts to characterize computability have subsequently strengthened this belief (see below).

On the other hand, the ChurchâTuring thesis states that the above three formally-defined classes of computable functions coincide with the "informal" notion of an effectively calculable function. Since, as an informal notion, the concept of effective calculability does not have a formal definition, the thesis, although it has near-universal acceptance, cannot be formally proven.

Since its inception, variations on the original thesis have arisen, including statements about what can physically be realized by a computer in our universe (physical Church-Turing thesis) and what can be efficiently computed (ChurchâTuring thesis (complexity theory)). These variations are not due to Church or Turing, but arise from later work in complexity theory and digital physics. The thesis also has implications for the philosophy of mind (see below).

 addresses the notion of "effective computability" as follows: "Clearly the existence of CC and RC (Church's and Rosser's proofs) presupposes a precise definition of 'effective'. 'Effective method' is here used in the rather special sense of a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps". Thus the adverb-adjective "effective" is used in a sense of "1a: producing a decided, decisive, or desired effect", and "capable of producing a result".

In the following, the words "effectively calculable" will mean "produced by any intuitively 'effective' means whatsoever" and "effectively computable" will mean "produced by a Turing-machine or equivalent mechanical device". Turing's "definitions" given in a footnote in his 1938 Ph.D. thesis "Systems of Logic Based on Ordinals", supervised by Church, are virtually the same:

The thesis can be stated as: "Every effectively calculable function is a computable function".
Church also stated that "No computational procedure will be considered as an algorithm unless it can be represented as a Turing Machine".
Turing stated it this way:
It was statedÂ ... that "a function is effectively calculable if its values can be found by some purely mechanical process". We may take this literally, understanding that by a purely mechanical process one which could be carried out by a machine. The developmentÂ ... leads toÂ ... an identification of computability with effective calculability. [ is the footnote quoted above.]

One of the important problems for logicians in the 1930s was the Entscheidungsproblem of David Hilbert and Wilhelm Ackermann, which asked whether there was a mechanical procedure for separating mathematical truths from mathematical falsehoods. This quest required that the notion of "algorithm" or "effective calculability" be pinned down, at least well enough for the quest to begin. But from the very outset Alonzo Church's attempts began with a debate that continues to this day. the notion of "effective calculability" to be (i) an "axiom or axioms" in an axiomatic system, (ii) merely a "definition" that "identified" two or more propositions, (iii) an "empirical hypothesis" to be verified by observation of natural events, or (iv) just "a proposal" for the sake of argument (i.e. a "thesis").

In the course of studying the problem, Church and his student Stephen Kleene introduced the notion of Î»-definable functions, and they were able to prove that several large classes of functions frequently encountered in number theory were Î»-definable. The debate began when Church proposed to GÃ¶del that one should define the "effectively computable" functions as the Î»-definable functions. GÃ¶del, however, was not convinced and called the proposal "thoroughly unsatisfactory". Rather, in correspondence with Church (c. 1934â35), GÃ¶del proposed "axiomatizing" the notion of "effective calculability"; indeed, in a 1935 letter to Kleene, Church reported that:

But GÃ¶del offered no further guidance. Eventually, he would suggest his recursion, modified by Herbrand's suggestion, that GÃ¶del had detailed in his 1934 lectures in Princeton NJ (Kleene and Rosser transcribed the notes). But he did not think that the two ideas could be satisfactorily identified "except heuristically".

Next, it was necessary to identify and prove the equivalence of two notions of effective calculability. Equipped with the Î»-calculus and "general" recursion, Stephen Kleene with help of Church and J. Barkley Rosser produced proofs (1933, 1935) to show that the two calculi are equivalent. Church subsequently modified his methods to include use of HerbrandâGÃ¶del recursion and then proved (1936) that the Entscheidungsproblem is unsolvable: there is no algorithm that can determine whether a well formed formula .

Many years later in a letter to Davis (c. 1965), GÃ¶del said that "he was, at the time of these [1934] lectures, not at all convinced that his concept of recursion comprised all possible recursions". By 1963â64 GÃ¶del would disavow HerbrandâGÃ¶del recursion and the Î»-calculus in favor of the Turing machine as the definition of "algorithm" or "mechanical procedure" or "formal system".

A hypothesis leading to a natural law?: In late 1936 Alan Turing's paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Post's 1936 paper had appeared and was certified independent of Turing's work. Post strongly disagreed with Church's "identification" of effective computability with the Î»-calculus and recursion, stating:

Rather, he regarded the notion of "effective calculability" as merely a "working hypothesis" that might lead by inductive reasoning to a "natural law" rather than by "a definition or an axiom". This idea was "sharply" criticized by Church.

Thus Post in his 1936 paper was also discounting Kurt GÃ¶del's suggestion to Church in 1934â35 that the thesis might be expressed as an axiom or set of axioms.

Turing adds another definition, Rosser equates all three: Within just a short time, Turing's 1936â37 paper "On Computable Numbers, with an Application to the Entscheidungsproblem" appeared. In it he stated another notion of "effective computability" with the introduction of his a-machines (now known as the Turing machine abstract computational model). And in a proof-sketch added as an "Appendix" to his 1936â37 paper, Turing showed that the classes of functions defined by Î»-calculus and Turing machines coincided. Church was quick to recognise how compelling Turing's analysis was. In his review of Turing's paper he made clear that Turing's notion made "the identification with effectiveness in the ordinary (not explicitly defined) sense evident immediately".

In a few years (1939) Turing would propose, like Church and Kleene before him, that "his" formal definition of mechanical computing agent was the correct one. Thus, by 1939, both Church (1934) and Turing (1939) had individually proposed that their "formal systems" should be "definitions" of "effective calculability"; neither framed their statements as "theses".

Rosser (1939) formally identified the three notions-as-definitions:

Kleene proposes "Church's Thesis": This left the overt expression of a "thesis" to Kleene. In his 1943 paper "Recursive Predicates and Quantifiers" Kleene proposed his "THESIS I":
() references Church 1936; () references Turing 1936â7
Kleene goes on to note that:
(24) references Post 1936 of Post and Church's "Formal definitions in the theory of ordinal numbers", "Fund. Math". vol 28 (1936) pp.11â21 (see ref. #2, ).

Kleene's ChurchâTuring Thesis: A few years later (1952) Kleene, who switched from presenting his work in the mathematical terminology of the lambda calculus of his phd advisor Alonzo Church to the theory of general recursive functions of his other teacher Kurt GÃ¶del, would overtly name the ChurchâTuring thesis in his correction of Turing's paper "The Word Problem in Semi-Groups with Cancellation", defend, and express the two "theses" and then "identify" them (show equivalence) by use of his Theorem XXX:

An attempt to understand the notion of "effective computability" better led Robin Gandy (Turing's student and friend) in 1980 to analyze "machine" computation (as opposed to human-computation acted out by a Turing machine). Gandy's curiosity about, and analysis of, cellular automata (including Conway's game of life), parallelism, and crystalline automata, led him to propose four "principles (or constraints)Â ... which it is argued, any machine must satisfy". His most-important fourth, "the principle of causality" is based on the "finite velocity of propagation of effects and signals; contemporary physics rejects the possibility of instantaneous action at a distance". From these principles and some additional constraintsâ(1a) a lower bound on the linear dimensions of any of the parts, (1b) an upper bound on speed of propagation (the velocity of light), (2) discrete progress of the machine, and (3) deterministic behaviorâhe produces a theorem that "What can be calculated by a device satisfying principles IâIV is computable."

In the late 1990s Wilfried Sieg analyzed Turing's and Gandy's notions of "effective calculability" with the intent of "sharpening the informal notion, formulating its general features axiomatically, and investigating the axiomatic framework". In his 1997 and 2002 work Sieg presents a series of constraints on the behavior of a "computor"â"a human computing agent who proceeds mechanically". These constraints reduce to:

The matter remains in active discussion within the academic community.

The thesis can be viewed as nothing but an ordinary mathematical definition. Comments by GÃ¶del on the subject suggest this view, e.g. "the correct definition of mechanical computability was established beyond any doubt by Turing". The case for viewing the thesis as nothing more than a definition is made explicitly by Robert I. Soare, where it is also argued that Turing's definition of computability is no less likely to be correct than the epsilon-delta definition of a continuous function.

Other formalisms (besides recursion, the Î»-calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Stephen Kleene (1952) adds to the list the functions ""reckonable" in the system S" of Kurt GÃ¶del 1936, and Emil Post's (1943, 1946) ""canonical" [also called "normal"] "systems"". In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see PostâTuring machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into "up-down counters", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): "...Â they just wanted toÂ ... convince themselves that there is no way to extend the notion of computable function."

All these contributions involve proofs that the models are computationally equivalent to the Turing machine; such models are said to be Turing complete. Because all these different attempts at formalizing the concept of "effective calculability/computability" have yielded equivalent results, it is now generally assumed that the ChurchâTuring thesis is correct. In fact, GÃ¶del (1936) proposed something stronger than this; he observed that there was something "absolute" about the concept of "reckonable in S":

Proofs in computability theory often invoke the ChurchâTuring thesis in an informal way to establish the computability of functions while avoiding the (often very long) details which would be involved in a rigorous, formal proof. To establish that a function is computable by Turing machine, it is usually considered sufficient to give an informal English description of how the function can be effectively computed, and then conclude "by the ChurchâTuring thesis" that the function is Turing computable (equivalently, partial recursive).

Dirk van Dalen gives the following example for the sake of illustrating this informal use of the ChurchâTuring thesis:

In order to make the above example completely rigorous, one would have to carefully construct a Turing machine, or Î»-function, or carefully invoke recursion axioms, or at best, cleverly invoke various theorems of computability theory. But because the computability theorist believes that Turing computability correctly captures what can be computed effectively, and because an effective procedure is spelled out in English for deciding the set B, the computability theorist accepts this as proof that the set is indeed recursive.

The success of the ChurchâTuring thesis prompted variations of the thesis to be proposed. For example, the physical ChurchâTuring thesis states: "All physically computable functions are Turing-computable."

The ChurchâTuring thesis says nothing about the efficiency with which one model of computation can simulate another. It has been proved for instance that a (multi-tape) universal Turing machine only suffers a logarithmic slowdown factor in simulating any Turing machine.

A variation of the ChurchâTuring thesis addresses whether an arbitrary but "reasonable" model of computation can be efficiently simulated. This is called the feasibility thesis, also known as the (classical) complexity-theoretic ChurchâTuring thesis or the extended ChurchâTuring thesis, which is not due to Church or Turing, but rather was realized gradually in the development of complexity theory. It states: "A probabilistic Turing machine can efficiently simulate any realistic model of computation." The word 'efficiently' here means up to polynomial-time reductions. This thesis was originally called computational complexity-theoretic ChurchâTuring thesis by Ethan Bernstein and Umesh Vazirani (1997). The complexity-theoretic ChurchâTuring thesis, then, posits that all 'reasonable' models of computation yield the same class of problems that can be computed in polynomial time. Assuming the conjecture that probabilistic polynomial time (BPP) equals deterministic polynomial time (P), the word 'probabilistic' is optional in the complexity-theoretic ChurchâTuring thesis. A similar thesis, called the invariance thesis, was introduced by Cees F. Slot and Peter van Emde Boas. It states: Reasonable' machines can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space." The thesis originally appeared in a paper at STOC'84, which was the first paper to show that polynomial-time overhead and constant-space overhead could be "simultaneously" achieved for a simulation of a Random Access Machine on a Turing machine.

If BQP is shown to be a strict superset of BPP, it would invalidate the complexity-theoretic ChurchâTuring thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original ChurchâTuring thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical complexity-theoretic ChurchâTuring thesis for efficiency reasons. Consequently, the quantum complexity-theoretic ChurchâTuring thesis states: "A quantum Turing machine can efficiently simulate any realistic model of computation."

Eugene Eberbach and Peter Wegner claim that the ChurchâTuring thesis is sometimes interpreted too broadly,
stating "the broader assertion that algorithms precisely capture what can be computed is invalid". They claim that forms of computation not captured by the thesis are relevant today,
terms which they call super-Turing computation.

Philosophers have interpreted the ChurchâTuring thesis as having implications for the philosophy of mind. B. Jack Copeland states that it is an open empirical question whether there are actual deterministic physical processes that, in the long run, elude simulation by a Turing machine; furthermore, he states that it is an open empirical question whether any such processes are involved in the working of the human brain. There are also some important open questions which cover the relationship between the ChurchâTuring thesis and physics, and the possibility of hypercomputation. When applied to physics, the thesis has several possible meanings:


There are many other technical possibilities which fall outside or between these three categories, but these serve to illustrate the range of the concept.

Philosophical aspects of the thesis, regarding both physical and biological computers, are also discussed in Odifreddi's 1989 textbook on recursion theory.

One can formally define functions that are not computable. A well-known example of such a function is the Busy Beaver function. This function takes an input "n" and returns the largest number of symbols that a Turing machine with "n" states can print before halting, when run with no input. Finding an upper bound on the busy beaver function is equivalent to solving the halting problem, a problem known to be unsolvable by Turing machines. Since the busy beaver function cannot be computed by Turing machines, the ChurchâTuring thesis states that this function cannot be effectively computed by any method.

Several computational models allow for the computation of (Church-Turing) non-computable functions. These are known as
hypercomputers.
Mark Burgin argues that super-recursive algorithms such as inductive Turing machines disprove the ChurchâTuring thesis. His argument relies on a definition of algorithm broader than the ordinary one, so that non-computable functions obtained from some inductive Turing machines are called computable. This interpretation of the ChurchâTuring thesis differs from the interpretation commonly accepted in computability theory, discussed above. The argument that super-recursive algorithms are indeed algorithms in the sense of the ChurchâTuring thesis has not found broad acceptance within the computability research community.





</doc>
<doc id="6856" url="https://en.wikipedia.org/wiki?curid=6856" title="Chomsky (surname)">
Chomsky (surname)

Chomsky (, , , , "from (Vyoska) (nearby Brest, now Belarus)") is a surname of Belarusian-Ukrainian origin. Notable people with the surname include:


Elsie, William, Avram Noam, Carol, Marvin, and Aviva are all closely related. William and Elsie were husband and wife. Avram Noam, generally referred by his given name Noam, is their son. Carol and Noam were married until Carol's death in 2008; Aviva is their daughter. Marvin is Noam's cousin. Also, Judith is Noam's sister in-law.



</doc>
<doc id="6857" url="https://en.wikipedia.org/wiki?curid=6857" title="Computer multitasking">
Computer multitasking

In computing, multitasking is the concurrent execution of multiple tasks (also known as processes) over a certain period of time. New tasks can interrupt already started ones before they finish, instead of waiting for them to end. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory. Multitasking automatically interrupts the running program, saving its state (partial results, memory contents and computer register contents) and loading the saved state of another program and transferring control to it. This "context switch" may be initiated at fixed time intervals (pre-emptive multitasking), or the running program may be coded to signal to the supervisory software when it can be interrupted (cooperative multitasking). 

Multitasking does not require parallel execution of multiple tasks at exactly the same time; instead, it allows more than one task to advance over a given period of time. Even on multiprocessor computers, multitasking allows many more tasks to be run than there are CPUs.

Multitasking is a common feature of computer operating systems. It allows more efficient use of the computer hardware; where a program is waiting for some external event such as a user input or an input/output transfer with a peripheral to complete, the central processor can still be used with another program. In a time-sharing system, multiple human operators use the same processor as if it was dedicated to their use, while behind the scenes the computer is serving many users by multitasking their individual programs. In multiprogramming systems, a task runs until it must wait for an external event or until the operating system's scheduler forcibly swaps the running task out of the CPU. Real-time systems such as those designed to control industrial robots, require timely processing; a single processor might be shared between calculations of machine movement, communications, and user interface. 

Often multitasking operating systems include measures to change the priority of individual tasks, so that important jobs receive more processor time than those considered less significant. Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program. 

A processor intended for use with multitasking operating systems may include special hardware to securely support multiple tasks, such as memory protection, and protection rings that ensure the supervisory software cannot be damaged or subverted by user-mode program errors. 

The term "multitasking" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Danish and Norwegian.

In the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was usually very inefficient.

The first computer using a multiprogramming system was the British "Leo III" owned by J. Lyons and Co. During batch processing, several different programs were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.

The use of multiprogramming was enhanced by the arrival of virtual memory and virtual machine technology, which enabled individual programs to make use of memory and operating system resources as if other concurrently running programs were, for all practical purposes, non-existent and invisible to them.

Multiprogramming doesn't give any guarantee that a program will run in a timely manner. Indeed, the very first program may very well run for hours without needing access to a peripheral. As there were no users waiting at an interactive terminal, this was no problem: users handed in a deck of punched cards to an operator, and came back a few hours later for printed results. Multiprogramming greatly reduced wait times when multiple batches were being processed.

Early multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the only scheduling scheme employed by Microsoft Windows and Classic Mac OS to enable multiple applications to run simultaneously. Cooperative multitasking is still used today on RISC OS systems.

As a cooperatively multitasked system relies on each process regularly giving up time to other processes on the system, one poorly designed program can consume all of the CPU time for itself, either by performing extensive calculations or by busy waiting; both would cause the whole system to hang. In a server environment, this is a hazard that makes the entire environment unacceptably fragile.

Preemptive multitasking allows the computer system to more reliably guarantee to each process a regular "slice" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process. Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was implemented in the PDP-6 Monitor and MULTICS in 1964, in OS/360 MFT in 1967, and in Unix in 1969, and was available in some operating systems for computers as small as DEC's PDP-8; it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives, as well as modern versions of Windows.

At any specific time, processes can be grouped into two categories: those that are waiting for input or output (called "I/O bound"), and those that are fully utilizing the CPU ("CPU bound"). In primitive systems, the software would often "poll", or "busywait" while waiting for requested input (such as disk, keyboard or network input). During this time, the system was not performing useful work. With the advent of interrupts and preemptive multitasking, I/O bound processes could be "blocked", or put on hold, pending the arrival of the necessary data, allowing other processes to utilize the CPU. As the arrival of the requested data would generate an interrupt, blocked processes could be guaranteed a timely return to execution.

The earliest preemptive multitasking OS available to home users was Sinclair QDOS on the Sinclair QL, released in 1984, but very few people bought the machine. Commodore's Amiga, released the following year, was the first commercially successful home computer to use the technology, and its multimedia abilities make it a clear ancestor of contemporary multitasking personal computers. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. It was later adopted on the Apple Macintosh by Mac OS X that, as a Unix-like operating system, uses preemptive multitasking for all native applications.

A similar model is used in Windows 9x and the Windows NT family, where native 32-bit applications are multitasked preemptively. 64-bit editions of Windows, both for the x86-64 and Itanium architectures, no longer support legacy 16-bit applications, and thus provide preemptive multitasking for all supported applications.

Another reason for multitasking was in the design of real-time computing systems, where there are a number of possibly unrelated external activities needed to be controlled by a single processor system. In such systems a hierarchical interrupt system is coupled with process prioritization to ensure that key activities were given a greater share of available process time.

As multitasking greatly improved the throughput of computers, programmers started to implement applications as sets of cooperating processes (e.Â g., one process gathering input data, one process processing input data, one process writing out results on disk). This, however, required some tools to allow processes to efficiently exchange data.

Threads were born from the idea that the most efficient way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open files. Threads are described as "lightweight processes" because switching between threads does not involve changing the memory context.

While threads are scheduled preemptively, some operating systems provide a variant to threads, named "fibers", that are scheduled cooperatively. On operating systems that do not provide fibers, an application may implement its own fibers using repeated calls to worker functions. Fibers are even more lightweight than threads, and somewhat easier to program with, although they tend to lose some or all of the benefits of threads on machines with multiple processors.

Some systems directly support multithreading in hardware.

Essential to any multitasking system is to safely and effectively share access to system resources. Access to memory must be strictly managed to ensure that no process can inadvertently or deliberately read or write to memory locations outside the process's address space. This is done for the purpose of general system stability and data integrity, as well as data security.

In general, memory access management is a responsibility of the operating system kernel, in combination with hardware mechanisms that provide supporting functionalities, such as a memory management unit (MMU). If a process attempts to access a memory location outside its memory space, the MMU denies the request and signals the kernel to take appropriate actions; this usually results in forcibly terminating the offending process. Depending on the software and kernel design and the specific error in question, the user may receive an access violation error message such as "segmentation fault".

In a well designed and correctly implemented multitasking system, a given process can never directly access memory that belongs to another process. An exception to this rule is in the case of shared memory; for example, in the System V inter-process communication mechanism the kernel allocates memory to be mutually shared by multiple processes. Such features are often used by database management software such as PostgreSQL.

Inadequate memory protection mechanisms, either due to flaws in their design or poor implementations, allow for security vulnerabilities that may be potentially exploited by malicious software.

Use of a swap file or swap partition is a way for the operating system to provide more memory than is physically available by keeping portions of the primary memory in secondary storage. While multitasking and memory swapping are two completely unrelated techniques, they are very often used together, as swapping memory allows more tasks to be loaded at the same time. Typically, a multitasking system allows another process to run when the running process hits a point where it has to wait for some portion of memory to be reloaded from secondary storage.

Processes that are entirely independent are not much trouble to program in a multitasking environment. Most of the complexity in multitasking systems comes from the need to share computer resources between tasks and to synchronize the operation of co-operating tasks.

Various concurrent computing techniques are used to avoid potential problems caused by multiple tasks attempting to access the same resource.

Bigger systems were sometimes built with a central processor(s) and some number of I/O processors, a kind of asymmetric multiprocessing.

Over the years, multitasking systems have been refined. Modern operating systems generally include detailed mechanisms for prioritizing processes, while symmetric multiprocessing has introduced new complexities and capabilities.



</doc>
<doc id="6859" url="https://en.wikipedia.org/wiki?curid=6859" title="Chiang Kai-shek">
Chiang Kai-shek

Chiang Kai-shek (; 31 October 1887 â 5 April 1975), also known as Chiang Chung-cheng and romanized via Mandarin as Chiang Chieh-shih and Jiang Jieshi, was a Chinese nationalist politician, revolutionary and military leader who served as the leader of the Republic of China between 1928 and 1975, first in mainland China until 1949 and then in Taiwan until his death.

Born in Chekiang Province, Chiang was a member of the Kuomintang and a lieutenant of Sun Yat-sen in the revolution to overthrow the Beiyang government and reunify China. With Soviet and communist (CPC) help, Chiang organized the military for Sun's Canton Nationalist Government and headed the Whampoa Military Academy. Commander in chief of the National Revolutionary Army (from which he came to be known as Generalissimo), he led the Northern Expedition from 1926 to 1928, before defeating a coalition of warlords and nominally reunifying China under a new Nationalist government. Midway through the campaign, the KMTâCPC alliance broke down and Chiang purged the communists inside the party, triggering a civil war with the CCP, which he eventually lost in 1949.

As leader of the Republic of China in the Nanjing decade, Chiang sought to strike a difficult balance between the modernizing China while also devoting resources to defending the nation against the impending Japanese threat. Trying to avoid a war with Japan while hostilities with CCP continued, he was kidnapped in the Xi'an Incident and obliged to form an Anti-Japanese United Front with the CCP. Following the Marco Polo Bridge Incident in 1937, he mobilized China for the Second Sino-Japanese War. For eight years he led the war of resistance against a vastly superior enemy, mostly from the wartime capital Chongqing. As the leader of a major Allied power, Chiang met with British Prime Minister Winston Churchill and U.S. President Franklin D. Roosevelt in the Cairo Conference to discuss terms for Japanese surrender. No sooner had the Second World War ended than the Civil War with the communists, by then led by Mao Zedong, resumed. Chiang's nationalists were mostly defeated in a few decisive battles in 1948.

In 1949 Chiang's government and army retreated to Taiwan, where Chiang imposed martial law and persecuted critics during the White Terror. Presiding over a period of social reforms and economic prosperity, Chiang won five elections to six-year terms as President of the Republic of China and was Director-General of the Kuomintang until his death in 1975, three years into his fifth term as President and just one year before Mao's death.

One of the longest-serving non-royal heads of state in the 20th century, Chiang was the longest-serving non-royal ruler of China having held the post for 46 years. Like Mao, he is regarded as a controversial figure. Supporters credit him with playing a major part in unifying the nation and leading the Chinese resistance against Japan, as well as with countering Soviet-communist encroachment. Detractors and critics denounce him as a dictator at the front of an authoritarian regime who suppressed opponents.

Like many other Chinese historical figures, Chiang used several names throughout his life. The name inscribed in the genealogical records of his family is Chiang Chou-tâai (Chinese:è£å¨æ³°|hp=JiÇng ZhÅutÃ i|w=Chiang3 Chou1-tâai4|c=|s=). This so-called "register name" (è­å) is the one under which his extended relatives knew him, and the one he used in formal occasions, such as when he got married. In deference to tradition, family members did not use the register name in conversation with people outside of the family. The concept of a "real" or original name is/was not as clear-cut in China as it is in the Western world.

In honor of tradition, Chinese families waited a number of years before officially naming their children. In the meantime, they used a "milk name" (), given to the infant shortly after his birth and known only to the close family, thus the actual name that Chiang received at birth was Jiang Ruiyuan (Chinese:è£çå|w="Chiang Jui-yÃ¼an").

In 1903, the 16-year-old Chiang went to Ningbo to be a student, and he chose a "school name" (). This was actually the formal name of a person, used by older people to address him, and the one he would use the most in the first decades of his life (as the person grew older, younger generations would have to use one of the courtesy names instead). Colloquially, the school name is called "big name" (), whereas the "milk name" is known as the "small name" (). The school name that Chiang chose for himself was Zhiqing (Chinese:å¿æ¸|w=Chi-châing, which means "purity of aspirations"). For the next fifteen years or so, Chiang was known as Jiang Zhiqing (Wade-Giles: Chiang Chi-châing). This is the name under which Sun Yat-sen knew him when Chiang joined the republicans in Kwangtung in the 1910s.

In 1912, when Jiang Zhiqing was in Japan, he started to use the name Chiang Kai-shek (Chinese: è£ä»ç³; Wade-Giles: ; Hanyu Pinyin: "JiÇng JiÃ¨shÃ­") as a pen name for the articles that he published in a Chinese magazine he founded: "Voice of the Army" (). "Jieshi" is the Pinyin romanization of this name, based on Mandarin, but the most recognized romanized rendering is "Kai-shek" which is in Cantonese romanization. As the republicans were based in Canton (a Cantonese speaking area, now commonly known as Guangdong), Chiang became known by Westerners under the Cantonese romanization of his courtesy name, while the family name as known in English seems to be the Mandarin pronunciation of his Chinese family name, transliterated in Wade-Giles.

"Kai-shek"/"Jieshi" soon became Chiang's courtesy name (). Some think the name was chosen from the classic Chinese book the "I Ching"; "ä»äºç³", "[he who is] firm as a rock", is the beginning of line 2 of Hexagram 16, "è±«". Others note that the first character of his courtesy name is also the first character of the courtesy name of his brother and other male relatives on the same generation line, while the second character of his courtesy name "shi" (âmeaning "stone") suggests the second character of his "register name" "tai" (âthe famous Mount Tai of China). Courtesy names in China often bore a connection with the personal name of the person. As the courtesy name is the name used by people of the same generation to address the person, Chiang soon became known under this new name.

Sometime in 1917 or 1918, as Chiang became close to Sun Yat-sen, he changed his name from Jiang Zhiqing to Chiang Chung-cheng (Chinese:è£ä¸­æ­£|w=Chiang3 Chung1-cheng4). By adopting the name Chung-cheng ("central uprightness"), he was choosing a name very similar to the name of Sun Yat-sen, who was (and still is) known among Chinese as Zhongshan (âmeaning "central mountain"), thus establishing a link between the two. The meaning of uprightness, rectitude, or orthodoxy, implied by his name, also positioned him as the legitimate heir of Sun Yat-sen and his ideas. It was readily accepted by members of the Chinese Nationalist Party and is the name under which Chiang Kai-shek is still commonly known in Taiwan. However, the name was often rejected by the Chinese Communists, and is not as well known in mainland China. Often the name is shortened to "Chung-cheng" only ("Zhongzheng" in Pinyin). Many public places in Taiwan are named Chungcheng after Chiang. For many years passengers arriving at the Chiang Kai-shek International Airport were greeted by signs in Chinese welcoming them to the "Chung Cheng International Airport". Similarly, the monument erected to Chiang's memory in Taipei, known in English as Chiang Kai-shek Memorial Hall, was literally named "Chung Cheng Memorial Hall" in Chinese. In Singapore, Chung Cheng High School was named after him.

His name is also written in Taiwan as "The Late President Honorable Chiang" (), where the one-character-wide space in front of his name known as nuo tai shows respect. He is often called "Honorable Chiang" () (without the title or space), or his name Chiang Chung-cheng, in Taiwan.

Chiang was born in Xikou, a town in Fenghua, Zhejiang, about west of central Ningbo. He was born into a family of Wu Chinese-speaking people with their ancestral homeâa concept important in Chinese societyâin Heqiao (), a town in Yixing, Jiangsu, about southwest of central Wuxi and from the shores of Lake Tai. He was the third child and second son of his father Chiang Chao-Tsung (1842â1895, ) and the first child of his father's third wife Wang Tsai-yu (1863â1921, ) who were members of a prosperous family of salt merchants. Chiang lost his father when he was eight, and he wrote of his mother as the "embodiment of Confucian virtues". The young Chiang was inspired throughout his youth by the realisation that the reputation of an honored family rested upon his shoulders. He was a mischievous child, at only three years old he thrust a pair of chopsticks down his throat to see how far they would reach. They became stuck and were removed with great difficulty. Even at a young age he was interested in war, and directed mimic campaigns with a wooden sword and spear. As he grew older, Chiang became more aware of the issues that surrounded him and in his speech to the Kuomintang in 1945 said:

Chiang grew up at a time in which military defeats, natural disasters, famines, revolts, unequal treaties and civil wars had left the Manchu-dominated Qing dynasty destabilized and in debt. Successive demands of the Western powers and Japan since the Opium War had left China owing millions of taels of silver. During his first visits to Japan to pursue a military career in 1906, he describes having strong nationalistic feelings with a desire among other things to, 'expel the Manchu Qing and to restore China'. He decided to pursue a military career. He began his military training at the Baoding Military Academy in 1906, the same year Japan left its bimetallic currency standard, devaluing its yen. He left for Tokyo Shinbu Gakko, a preparatory school for the Imperial Japanese Army Academy intended for Chinese students, in 1907. There, he came under the influence of compatriots to support the revolutionary movement to overthrow the Manchu-dominated Qing dynasty and to set up a Han-dominated Chinese republic. He befriended Chen Qimei, and in 1908 Chen brought Chiang into the Tongmenghui, an important revolutionary brotherhood of the era. Finishing his military schooling at Tokyo Shinbu Gakko, Chiang served in the Imperial Japanese Army from 1909 to 1911.

After learning of the Wuchang uprising, Chiang returned to China in 1911, intending to fight as an artillery officer. He served in the revolutionary forces, leading a regiment in Shanghai under his friend and mentor Chen Qimei, as one of Chen's chief lieutenants. In early 1912 a dispute arose between Chen and Tao Chen-chang, an influential member of the Revolutionary Alliance who opposed both Sun Yat-sen and Chen. Tao sought to avoid escalating the quarrel by hiding in a hospital but Chiang discovered him there. Chen dispatched assassins. Chiang may not have taken part in the act, but would later assume responsibility to help Chen avoid trouble. Chen valued Chiang despite Chiang's already legendary temper, regarding such bellicosity as useful in a military leader.

Chiang's friendship with Chen Qimei signaled an association with Shanghai's criminal syndicate (the Green Gang headed by Du Yuesheng and Huang Jinrong). During Chiang's time in Shanghai, the British-administered Shanghai International Settlement police watched him and charged him with various felonies. These charges never resulted in a trial, and Chiang was never jailed.

Chiang became a founding member of the KMT after the success (February 1912) of the 1911 Revolution. After the takeover of the Republican government by Yuan Shikai and the failed Second Revolution in 1913, Chiang, like his KMT comrades, divided his time between exile in Japan and the havens of the Shanghai International Settlement. In Shanghai, Chiang cultivated ties with the city's underworld gangs, which were dominated by the notorious Green Gang and its leader Du Yuesheng. On 18 May 1916, agents of Yuan Shikai assassinated Chen Qimei. Chiang then succeeded Chen as leader of the Chinese Revolutionary Party in Shanghai. Sun Yat-sen's political career reached its lowest point during this time when most of his old Revolutionary Alliance comrades refused to join him in the exiled Chinese Revolutionary Party.

In 1917 Sun Yat-sen moved his base of operations to Canton (now known as Guangzhou), and Chiang joined him in 1918. At this time Sun remained largely sidelined â without arms or money, he was soon expelled from Kwangtung and exiled again to Shanghai. He was restored to Kwangtung with mercenary help in 1920. After his return to Kwangtung, a rift developed between Sun, who sought to militarily unify China under the KMT, and Guangdong Governor Chen Jiongming, who wanted to implement a federalist system with Guangdong as a model province. On 16 June 1922 Ye Ju, a general of Chen's whom Sun had attempted to exile, led an assault on Kwangtung's Presidential Palace. Sun had already fled to the naval yard and boarded the SSÂ "Haiqi", but his wife narrowly evaded shelling and rifle-fire as she fled. They met on the SS "Yongfeng", where Chiang joined them as swiftly as he could return from Shanghai, where he was ritually mourning his mother's death. For about 50 days, Chiang stayed with Sun, protecting and caring for him and earning his lasting trust. They abandoned their attacks on Chen on 9 August, taking a British ship to Hong Kong and traveling to Shanghai by steamer.

Sun regained control of Kwangtung in early 1923, again with the help of mercenaries from Yunnan and of the Comintern. Undertaking a reform of the KMT, he established a revolutionary government aimed at unifying China under the KMT. That same year Sun sent Chiang to spend three months in Moscow studying the Soviet political and military system. During his trip in Russia, Chiang met Leon Trotsky and other Soviet leaders, but quickly came to the conclusion that the Russian model of government was not suitable for China. Chiang later sent his eldest son, Ching-kuo, to study in Russia. After his father's split from the First United Front in 1927, Ching-kuo was forced to stay there, as a hostage, until 1937. Chiang wrote in his diary, "It is not worth it to sacrifice the interest of the country for the sake of my son." Chiang even refused to negotiate a prisoner swap for his son in exchange for the Chinese Communist Party leader. His attitude remained consistent, and he continued to maintain, by 1937, that "I would rather have no offspring than sacrifice our nation's interests." Chiang had absolutely no intention of ceasing the war against the Communists.

Chiang Kai-shek returned to Kwangtung and in 1924 Sun appointed him Commandant of the Whampoa Military Academy. Chiang resigned from the office after one month in disagreement with Sun's extremely close cooperation with the Comintern, but returned at Sun's demand. The early years at Whampoa allowed Chiang to cultivate a cadre of young officers loyal both to the KMT and to himself.

Throughout his rise to power, Chiang also benefited from membership within the nationalist Tiandihui fraternity, to which Sun Yat-sen also belonged, and which remained a source of support during his leadership of the Kuomintang.

Sun Yat-sen died on 12 March 1925, creating a power vacuum in the Kuomintang. A contest ensued among Wang Jingwei, Liao Zhongkai, and Hu Hanmin. In August, Liao was assassinated and Hu arrested for his connections to the murderers. Wang Jingwei, who had succeeded Sun as chairman of the Kwangtung regime, seemed ascendant but was forced into exile by Chiang following the Canton Coup. The , renamed the "Zhongshan" in Sun's honor, had appeared off Changzhouâthe location of the Whampoa Academyâon apparently falsified orders and amid a series of unusual phone calls trying to ascertain Chiang's location. He initially considered fleeing Kwangtung and even booked passage on a Japanese steamer, but then decided to use his military connections to declare martial law on 20 March 1926, and crack down on Communist and Soviet influence over the NRA, the military academy, and the party. The right wing of the party supported him and Stalinâanxious to maintain Soviet influence in the areaâhad his lieutenants agree to Chiang's demands regarding a reduced Communist presence in the KMT leadership in exchange for certain other concessions. The rapid replacement of leadership enabled Chiang to effectively end civilian oversight of the military after 15 May, though his authority was somewhat limited by the army's own regional composition and divided loyalties. On 5 June 1926, he was named commander-in-chief of the National Revolutionary Army and, on 27 July, he finally launched Sun's long-delayed Northern Expedition, aimed at conquering the northern warlords and bringing China together under the KMT.

The NRA branched into three divisions: to the west was the returned Wang Jingwei, who led a column to take Wuhan; Bai Chongxi's column went east to take Shanghai; Chiang himself led in the middle route, planning to take Nanjing before pressing ahead to capture Beijing. However, in January 1927, Wang Jingwei and his KMT leftist allies took the city of Wuhan amid much popular mobilization and fanfare. Allied with a number of Chinese Communists and advised by Soviet agent Mikhail Borodin, Wang declared the National Government as having moved to Wuhan. Having taken Nanjing in March (and briefly visited Shanghai, now under the control of his close ally Bai Chongxi), Chiang halted his campaign and prepared a violent break with Wang's leftist elements, which he believed threatened his control of the KMT.

Now with an established national government in Nanjing, and supported by conservative allies including Hu Hanmin, Chiang's expulsion of the Communists and their Soviet advisers led to the beginning of the Chinese Civil War. Wang Jingwei's National Government was weak militarily, and was soon ended by Chiang with the support of a local warlord (Li Zongren of Guangxi). Eventually, Wang and his leftist party surrendered to Chiang and joined him in Nanjing. In the Central Plains War, Beijing was taken on June 1928, from an alliance of the warlords Feng Yuxiang and Yan Xishan. In December, the Manchurian warlord Zhang Xueliang pledged allegiance to Chiang's government, completing Chiang's nominal unification of China and ending the Warlord Era.

In 1927, when he was setting up the Nationalist government in Nanjing, he was preoccupied with "the elevation of our leader Dr. Sun Yat-sen to the rank of 'Father of our Chinese Republic'. Dr. Sun worked for 40 years to lead our people in the Nationalist cause, and we cannot allow any other personality to usurp this honored position". He asked Chen Guofu to purchase a photograph that had been taken in Japan around 1895 or 1898. It showed members of the Revive China Society with Yeung Kui-wan ( or , pinyin YÃ¡ng QÃºyÃºn) as President, in the place of honor, and Sun, as secretary, on the back row, along with members of the Japanese Chapter of the Revive China Society. When told that it was not for sale, Chiang offered a million dollars to recover the photo and its negative. "The party must have this picture and the negative at any price. They must be destroyed as soon as possible. It would be embarrassing to have our Father of the Chinese Republic shown in a subordinate position". Chiang never obtained either the photo or its negative.

Chiang made great efforts to gain recognition as the official successor of Sun Yat-sen. In a pairing of great political significance, Chiang was Sun's brother-in-law: he had married Soong Mei-ling, the younger sister of Soong Ching-ling, Sun's widow, on 1 December 1927. Originally rebuffed in the early 1920s, Chiang managed to ingratiate himself to some degree with Soong Mei-ling's mother by first divorcing his wife and concubines and promising to sincerely study the precepts of Christianity. He read the copy of the Bible that May-ling had given him twice before making up his mind to become a Christian, and three years after his marriage he was baptized in the Soong's Methodist church. Although some observers felt that he adopted Christianity as a political move, studies of his recently opened diaries suggest that his faith was strong and sincere and that he felt that Christianity reinforced Confucian moral teachings.

Upon reaching Beijing, Chiang paid homage to Sun Yat-sen and had his body moved to the new capital of Nanjing to be enshrined in a grand mausoleum.

In the West and in the Soviet Union, Chiang Kai-shek was known as the "Red General". Movie theaters in the Soviet Union showed newsreels and clips of Chiang. At Moscow, Sun Yat-sen University portraits of Chiang were hung on the walls; and, in the Soviet May Day Parades that year, Chiang's portrait was to be carried along with the portraits of Karl Marx, Friedrich Engels, Vladimir Lenin, Joseph Stalin, Mao Zedong and other Communist leaders. The United States consulate and other Westerners in Shanghai were concerned about the approach of "Red General" Chiang as his army was seizing control of large areas of the country in the Northern Expedition.

On 12 April 1927, Chiang carried out a purge of thousands of suspected Communists and dissidents in Shanghai, and began large-scale massacres across the country collectively known as the "White Terror". During April, more than people were killed in Shanghai. The killings drove most Communists from urban cities and into the rural countryside, where the KMT was less powerful. In the year after April 1927, over 300,000 people died across China in anti-Communist suppression campaigns, executed by the KMT. One of the most famous quotes from Chiang (during that time) was that he would rather mistakenly kill 1,000 innocent people rather than allow one Communist to escape. Some estimates claim the White Terror in China took millions of lives, most of them in the rural areas. No concrete number can be verified. Chiang allowed Soviet agent and advisor Mikhail Borodin and Soviet general Vasily BlÃ¼cher (Galens) "escape" to safety after the purge.

Having gained control of China, Chiang's party remained surrounded by "surrendered" warlords who remained relatively autonomous within their own regions. On 10 October 1928, Chiang was named director of the State Council, the equivalent to President of the country, in addition to his other titles. As with his predecessor Sun Yat-sen, the Western media dubbed him "Generalissimo".

According to Sun Yat-sen's plans, the Kuomintang (KMT) was to rebuild China in three steps: military rule, political tutelage, and constitutional rule. The ultimate goal of the KMT revolution was democracy, which was not considered to be feasible in China's fragmented state. Since the KMT had completed the first step of revolution through seizure of power in 1928, Chiang's rule thus began a period of what his party considered to be "political tutelage" in Sun Yat-sen's name. During this so-called Republican Era, many features of a modern, functional Chinese state emerged and developed.

From 1928 to 1937, a time period known as known as the Nanjing decade, some aspects of foreign imperialism, concessions and privileges in China were moderated through diplomacy. The government acted to modernize the legal and penal systems, attempted to stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics, and augment industrial and agricultural production. Not all of these projects were successfully completed. Efforts were made towards improving education standards, and in an effort to unify Chinese society, the New Life Movement was launched to encourage Confucian moral values and personal discipline. "Guoyu" ("national language") was promoted as a standard tongue, and the establishment of communications facilities (including radio) were used to encourage a sense of Chinese nationalism in a way that was not possible when the nation lacked an effective central government.

Any successes that the Nationalists did make, however, were met with constant political and military upheavals. While much of the urban areas were now under the control of the KMT, much of the countryside remained under the influence of weakened yet undefeated warlords and Communists. Chiang often resolved issues of warlord obstinacy through military action, but such action was costly in terms of men and material. The 1930 Central Plains War alone nearly bankrupted the Nationalist government and caused almost casualties on both sides. In 1931, Hu Hanmin, Chiang's old supporter, publicly voiced a popular concern that Chiang's position as both premier and president flew in the face of the democratic ideals of the Nationalist government. Chiang had Hu put under house arrest, but he was released after national condemnation, after which he left Nanjing and supported a rival government in Canton. The split resulted in a military conflict between Hu's Kwangtung government and Chiang's Nationalist government. Chiang only won the campaign against Hu after a shift in allegiance by Zhang Xueliang, who had previously supported Hu Hanmin.
Throughout his rule, complete eradication of the Communists remained Chiang's dream. After assembling his forces in Jiangxi, Chiang led his armies against the newly established Chinese Soviet Republic. With help from foreign military advisers, Chiang's Fifth Campaign finally surrounded the Chinese Red Army in 1934. The Communists, tipped off that a Nationalist offensive was imminent, retreated in the Long March, during which Mao Zedong rose from a mere military official to the most influential leader of the Communist Party of China.

Chiang, as a nationalist and a Confucianist, was against the iconoclasm of the May Fourth Movement. Motivated by his sense of nationalism, he viewed some Western ideas as foreign, and he believed that the great introduction of Western ideas and literature that the May Fourth Movement promoted was not beneficial to China. He and Dr. Sun criticized the May Fourth intellectuals as corrupting the morals of China's youth.

Contrary to Communist propaganda that he was pro-capitalism, Chiang antagonized the capitalists of Shanghai, often attacking them and confiscating their capital and assets for the use of the government. Chiang confiscated the wealth of capitalists even while he denounced and fought against communists. Chiang crushed pro-communist worker and peasant organizations and rich Shanghai capitalists at the same time. Chiang continued the anti-capitalist ideology of Sun Yat-sen, directing Kuomintang media to openly attack capitalists and capitalism, while demanding government controlled industry instead.

Chiang has often been interpreted as being pro-capitalist, but this conclusion may be problematic. Shanghai capitalists did briefly support him out of fear of communism in 1927, but this support eroded in 1928 when Chiang turned his tactics of intimidation on them. The relationship between Chiang Kai-shek and Chinese capitalists remained poor throughout the period of his administration. Chiang blocked Chinese capitalists from gaining any political power or voice within his regime. Once Chiang Kai-shek was done with his White Terror on pro-communist laborers, he proceeded to turn on the capitalists. Gangster connections allowed Chiang to attack them in the International Settlement, successfully forcing capitalists to back him up with their assets for his military expeditions.

Chiang viewed Japan, the United States, the Soviet Union, France and Britain as all being imperialists with nobody else's interests in mind but their own, seeing them as hypocritical to condemn each other for imperialism which they all practiced. He manipulated America, Nazi Germany, and the Soviet Union to regain lost territories for China as he viewed all the powers as imperialists trying to curtail and suppress China's power and national resurrection.

Some sources attribute Chiang Kai-shek with responsibility for millions of deaths in scattered mass death events caused by the Nationalist Government of China. He has been deemed partially responsible for the man-made 1938 Yellow River flood, which killed hundreds of thousands of Chinese civilians to fend off a Japanese advance. This accusation is usually sourced from Rudolph Rummel who was referring to the Nationalist regime as whole rather than Chiang Kai-Shek in particular. Regardless, the Nationalist government of China has been accused of mass killings by Rummel, he alleged that the Nationalist government of China was responsible for based on various claims between 6 and 18.5Â million deaths.
He attributes this death toll to a few major causes, for example:

In Nanjing, on April 1931, Chiang Kai-shek attended a national leadership conference with Zhang Xueliang and General Ma Fuxiang, in which Chiang and Zhang dauntlessly upheld that Manchuria was part of China in the face of the Japanese invasion. After the Japanese invasion of Manchuria in 1931, Chiang resigned as Chairman of the National Government. He returned shortly afterwards, adopting the slogan "first internal pacification, then external resistance". However, this policy of avoiding a frontal war against the Japanese was widely unpopular. In 1932, while Chiang was seeking first to defeat the Communists, Japan launched an advance on Shanghai and bombarded Nanjing. This disrupted Chiang's offensives against the Communists for a time, although it was the northern factions of Hu Hanmin's Kwangtung government (notably the 19th Route Army) that primarily led the offensive against the Japanese during this skirmish. Brought into the Nationalist army immediately after the battle, the 19th Route Army's career under Chiang would be cut short after it was disbanded for demonstrating socialist tendencies.

In December 1936, Chiang flew to Xi'an to coordinate a major assault on the Red Army and the Communist Republic that had retreated into Yan'an. However, Chiang's allied commander Zhang Xueliang, whose forces were used in his attack and whose homeland of Manchuria had been recently invaded by the Japanese, did not support the attack on the Communists. On 12 December, Zhang and several other Nationalist generals headed by Yang Hucheng of Shaanxi kidnapped Chiang for two weeks in what is known as the Xi'an Incident. They forced Chiang into making a "Second United Front" with the Communists against Japan. After releasing Chiang and returning to Nanjing with him, Zhang was placed under house arrest and the generals who had assisted him were executed. Chiang's commitment to the Second United Front was nominal at best, and it was all but broken up in 1941.
The Second Sino-Japanese War broke out in July 1937, and in August of that year Chiang sent of his best-trained and equipped soldiers to defend Shanghai. With over 200,000 Chinese casualties, Chiang lost the political cream of his Whampoa-trained officers. Although Chiang lost militarily, the battle dispelled Japanese claims that it could conquer China in three months and demonstrated to the Western powers that the Chinese would continue the fight. By December, the capital city of Nanjing had fallen to the Japanese resulting in the Nanking massacre. Chiang moved the government inland, first to Wuhan and later to Chongqing.

Having lost most of China's economic and industrial centers, Chiang withdrew into the hinterlands, stretching the Japanese supply lines and bogging down Japanese soldiers in the vast Chinese interior. As part of a policy of protracted resistance, Chiang authorized the use of scorched earth tactics, resulting in many civilian deaths. During the Nationalists' retreat from Zhengzhou, the dams around the city were deliberately destroyed by the Nationalist army to delay the Japanese advance, killing 500,000 people in the subsequent 1938 Yellow River flood.

After heavy fighting, the Japanese occupied Wuhan in the fall of 1938 and the Nationalists retreated farther inland, to Chongqing. While en route to Chongqing, the Nationalist army intentionally started the "fire of Changsha", as a part of the scorched earth policy. The fire destroyed much of the city, killed twenty thousand civilians, and left hundreds of thousands of people homeless. Due to an organizational error (it was claimed), the fire was begun without any warning to the residents of the city. The Nationalists eventually blamed three local commanders for the fire and executed them. Newspapers across China blamed the fire on (non-KMT) arsonists, but the blaze contributed to a nationwide loss of support for the KMT.

In 1939 Muslim leaders Isa Yusuf Alptekin and Ma Fuliang were sent by Chiang to several Middle Eastern countries, including Egypt, Turkey, and Syria, to gain support for the Chinese War against Japan, and to express his support for Muslims.

The Japanese, controlling the puppet-state of Manchukuo and much of China's eastern seaboard, appointed Wang Jingwei as a Quisling-ruler of the occupied Chinese territories around Nanjing. Wang named himself President of the Executive Yuan and Chairman of the National Government (not the same 'National Government' as Chiang's), and led a surprisingly large minority of anti-Chiang/anti-Communist Chinese against his old comrades. He died in 1944, within a year of the end of World War II.

The Hui Muslim Xidaotang sect pledged allegiance to the Kuomintang after their rise to power and Hui Muslim General Bai Chongxi acquainted Chiang Kaishek with the Xidaotang jiaozhu Ma Mingren in 1941 in Chongqing.

In 1942 Generalissimo Chiang Kai-shek went on tour in northwestern China in Xinjiang, Gansu, Ningxia, Shaanxi, and Qinghai, where he met both Muslim Generals Ma Buqing and Ma Bufang. He also met the Muslim Generals Ma Hongbin and Ma Hongkui separately.

A border crisis erupted with Tibet in 1942. Under orders from Chiang, Ma Bufang repaired Yushu airport to prevent Tibetan separatists from seeking independence. Chiang also ordered Ma Bufang to put his Muslim soldiers on alert for an invasion of Tibet in 1942. Ma Bufang complied and moved several thousand troops to the border with Tibet. Chiang also threatened the Tibetans with aerial bombardment if they worked with the Japanese. Ma Bufang attacked the Tibetan Buddhist Tsang monastery in 1941. He also constantly attacked the Labrang Monastery.

With the attack on Pearl Harbor and the opening of the Pacific War, China became one of the Allied Powers. During and after World War II, Chiang and his American-educated wife Soong Mei-ling, known in the United States as "Madame Chiang", held the support of the China Lobby in the United States, which saw in them the hope of a Christian and democratic China. Chiang was even named the Supreme Commander of Allied forces in the China war zone. He was appointed Knight Grand Cross of the Order of the Bath in 1942.

General Joseph Stilwell, an American military adviser to Chiang during World War II, strongly criticized Chiang and his generals for what he saw as their incompetence and corruption. In 1944, the United States Army Air Corps commenced Operation Matterhorn to bomb Japan's steel industry from bases to be constructed in mainland China. This was meant to fulfill President Roosevelt's promise to Chiang Kai-shek to begin bombing operations against Japan by November 1944. However, Chiang Kai-shek's subordinates refused to take airbase construction seriously until enough capital had been delivered to permit embezzlement on a massive scale. Stilwell estimated that at least half of the $100Â million spent on construction of airbases was embezzled by Nationalist party officials.

Chiang played the Soviets and Americans against each other during the war. He first told the Americans that they would be welcome in talks between the Soviet Union and China, then secretly told the Soviets that the Americans were unimportant and that their opinions would not be considered. Chiang also used American support and military power in China against the ambitions of the Soviet Union to dominate the talks, stopping the Soviets from taking full advantage of the situation in China with the threat of American military action against the Soviets.

U.S. President Franklin D. Roosevelt, through General Stilwell, privately made it clear that they preferred that the French not reacquire French Indochina (modern day Vietnam, Cambodia and Laos) after the war was over. Roosevelt offered Chiang control of all of Indochina. It was said that Chiang replied: "Under no circumstances!"

After the war, 200,000 Chinese troops under General Lu Han were sent by Chiang Kai-shek to northern Indochina (north of the 16th parallel) to accept the surrender of Japanese occupying forces there, and remained in Indochina until 1946, when the French returned. The Chinese used the VNQDD, the Vietnamese branch of the Chinese Kuomintang, to increase their influence in Indochina and to put pressure on their opponents. Chiang Kai-shek threatened the French with war in response to maneuvering by the French and Ho Chi Minh's forces against each other, forcing them to come to a peace agreement. In February 1946 he also forced the French to surrender all of their concessions in China and to renounce their extraterritorial privileges in exchange for the Chinese withdrawing from northern Indochina and allowing French troops to reoccupy the region. Following France's agreement to these demands, the withdrawal of Chinese troops began in March 1946.

During the Cairo Conference in 1943, Chiang said that Roosevelt asked him whether China would like to claim the Ryukyu Islands from Japan in addition to retaking Taiwan, the Pescadores, and Manchuria. Chiang claims that he said he was in favor of an international presence on the islands. However, the U.S. became the sole protector of the Ryukyus in 1945, and reverted it to the Japanese in 1972 while securing US military presence there.

In 1945, when Japan surrendered, Chiang's Chongqing government was ill-equipped and ill-prepared to reassert its authority in formerly Japanese-occupied China, and it asked the Japanese to postpone their surrender until Kuomintang (KMT) authority could arrive to take over. American troops and weapons soon bolstered KMT forces, allowing them to reclaim cities. The countryside, however, remained largely under Communist control.

For over a year after the Japanese surrender, rumors circulated throughout China that the Japanese had entered into a secret agreement with Chiang, in which the Japanese would assist the Nationalists in fighting the Communists in exchange for the protection of Japanese persons and property there. Many top nationalist generals, including Chiang, had studied and trained in Japan before the Nationalists had returned to the mainland in the 1920s, and maintained close personal friendships with top Japanese officers. The Japanese general in charge of all forces in China, General Yasuji Okamura, had personally trained officers who later became generals in Chiang's staff. Reportedly, General Okamura, before surrendering command of all Japanese military forces in Nanjing, offered Chiang control of all 1.5Â million Japanese military and civilian support staff then present in China. Reportedly, Chiang seriously considered accepting this offer, but declined only in the knowledge that the United States would certainly be outraged by the gesture. Even so, armed Japanese troops remained in China well into 1947, with some noncommissioned officers finding their way into the Nationalist officer corps. That the Japanese in China came to regard Chiang as a magnanimous figure to whom many Japanese owed their lives and livelihoods was a fact attested by both Nationalist and Communist sources.

Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang Kai-Shek, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against Japan. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism.

Following the war, the United States encouraged peace talks between Chiang and Communist leader Mao Zedong in Chongqing. Due to concerns about widespread and well-documented corruption in Chiang's government throughout his rule, the U.S. government limited aid to Chiang for much of the period of 1946 to 1948, in the midst of fighting against the People's Liberation Army led by Mao Zedong. Alleged infiltration of the U.S. government by Chinese Communist agents may have also played a role in the suspension of American aid.

Chiang's right-hand man, the secret police Chief Dai Li, was both anti-American and anti-Communist. Dai ordered Kuomintang agents to spy on American officers. Earlier, Dai had been involved with the Blue Shirts Society, a fascist-inspired paramilitary group within the Kuomintang, which wanted to expel Western and Japanese imperialists, crush the Communists, and eliminate feudalism. Dai Li died in a plane crash, which was suspected to be an assassination orchestrated by Chiang.

Although Chiang had achieved status abroad as a world leader, his government deteriorated as the result of corruption and inflation. In his diary on June 1948, Chiang wrote that the KMT had failed, not because of external enemies but because of rot from within. The war had severely weakened the Nationalists, while the Communists were strengthened by their popular land-reform policies, and by a rural population that supported and trusted them. The Nationalists initially had superiority in arms and men, but their lack of popularity, infiltration by Communist agents, low morale, and disorganization soon allowed the Communists to gain the upper hand in the civil war.

A new Constitution was promulgated in 1947, and Chiang was elected by the National Assembly as the first term President of the Republic of China on 20 May 1948. This marked the beginning of what was termed the "democratic constitutional government" period by the KMT political orthodoxy, but the Communists refused to recognize the new Constitution, and its government, as legitimate. Chiang resigned as President on 21 January 1949, as KMT forces suffered terrible losses and defections to the Communists. After Chiang's resignation the vice-president of the ROC, Li Zongren, became China's acting president.

Shortly after Chiang's resignation the Communists halted their advances and attempted to negotiate the virtual surrender of the ROC. Li attempted to negotiate milder terms that would have ended the civil war, but without success. When it became clear that Li was unlikely to accept Mao's terms, the Communists issued an ultimatum in April 1949, warning that they would resume their attacks if Li did not agree within five days. Li refused.

Li's attempts to carry out his policies faced varying degrees of opposition from Chiang's supporters, and were generally unsuccessful. Chiang especially antagonized Li by taking possession of (and moving to Taiwan) US$200Â million of gold and US dollars belonging to the central government that Li desperately needed to cover the government's soaring expenses. When the Communists captured the Nationalist capital of Nanjing in April 1949, Li refused to accompany the central government as it fled to Guangdong, instead expressing his dissatisfaction with Chiang by retiring to Guangxi.
The former warlord Yan Xishan, who had fled to Nanjing only one month before, quickly insinuated himself within the Li-Chiang rivalry, attempting to have Li and Chiang reconcile their differences in the effort to resist the Communists. At Chiang's request Yan visited Li to convince Li not to withdraw from public life. Yan broke down in tears while talking of the loss of his home province of Shanxi to the Communists, and warned Li that the Nationalist cause was doomed unless Li went to Guangdong. Li agreed to return under the condition that Chiang surrender most of the gold and US dollars in his possession that belonged to the central government, and that Chiang stop overriding Li's authority. After Yan communicated these demands and Chiang agreed to comply with them, Li departed for Guangdong.

In Guangdong, Li attempted to create a new government composed of both Chiang supporters and those opposed to Chiang. Li's first choice of premier was Chu Cheng, a veteran member of the Kuomintang who had been virtually driven into exile due to his strong opposition to Chiang. After the Legislative Yuan rejected Chu, Li was obliged to choose Yan Xishan instead. By this time Yan was well known for his adaptability and Chiang welcomed his appointment.

Conflict between Chiang and Li persisted. Although he had agreed to do so as a prerequisite of Li's return, Chiang refused to surrender more than a fraction of the wealth that he had sent to Taiwan. Without being backed by gold or foreign currency, the money issued by Li and Yan quickly declined in value until it became virtually worthless.

Although he did not hold a formal executive position in the government, Chiang continued to issue orders to the army, and many officers continued to obey Chiang rather than Li. The inability of Li to coordinate KMT military forces led him to put into effect a plan of defense that he had contemplated in 1948. Instead of attempting to defend all of southern China, Li ordered what remained of the Nationalist armies to withdraw to Guangxi and Guangdong, hoping that he could concentrate all available defenses on this smaller, and more easily defensible, area. The object of Li's strategy was to maintain a foothold on the Chinese mainland in the hope that the United States would eventually be compelled to enter the war in China on the Nationalist side.

Chiang opposed Li's plan of defense because it would have placed most of the troops still loyal to Chiang under the control of Li and Chiang's other opponents in the central government. To overcome Chiang's intransigence Li began ousting Chiang's supporters within the central government. Yan Xishan continued in his attempts to work with both sides, creating the impression among Li's supporters that he was a "stooge" of Chiang, while those who supported Chiang began to bitterly resent Yan for his willingness to work with Li. Because of the rivalry between Chiang and Li, Chiang refused to allow Nationalist troops loyal to him to aid in the defense of Kwangsi and Canton, with the result that Communist forces occupied Canton in October 1949.

After Canton fell to the Communists, Chiang relocated the government to Chungking, while Li effectively surrendered his powers and flew to New York for treatment of his chronic duodenum illness at the Hospital of Columbia University. Li visited the President of the United States, Harry S. Truman, and denounced Chiang as a dictator and an usurper. Li vowed that he would "return to crush" Chiang once he returned to China. Li remained in exile, and did not return to Taiwan.

In the early morning of 10 December 1949, Communist troops laid siege to Chengtu, the last KMT-controlled city in mainland China, where Chiang Kai-shek and his son Chiang Ching-kuo directed the defense at the Chengtu Central Military Academy. Chiang Kai-shek, father and son, were evacuated to Taiwan on an aircraft called "May-ling" and arrived the same day. Chiang Kai-shek would never return to the mainland.

Chiang did not re-assume the presidency until 1 March 1950. On January 1952, Chiang commanded the Control Yuan, now in Taiwan, to impeach Li in the "Case of Li Zongren's Failure to carry out Duties due to Illegal Conduct" (æå®ä»éæ³å¤±è·æ¡). Chiang relieved Li of the position as vice-president in the National Assembly in March 1954.

Chiang moved the government to Taipei, Taiwan, where he resumed his duties as President of the Republic of China on 1 March 1950. Chiang was reelected by the National Assembly to be the President of the Republic of China (ROC) on 20 May 1954, and again in 1960, 1966, and 1972. He continued to claim sovereignty over all of China, including the territories held by his government and the People's Republic, as well as territory the latter ceded to foreign governments, such as Tuva and Outer Mongolia. In the context of the Cold War, most of the Western world recognized this position and the ROC represented China in the United Nations and other international organizations until the 1970s.
During his presidency on Taiwan, Chiang continued making preparations to take back mainland China. He developed the ROC army to prepare for an invasion of the mainland, and to defend Taiwan in case of an attack by the Communist forces. He also financed armed groups in mainland China, such as Muslim soldiers of the ROC Army left in Yunnan under Li Mi, who continued to fight. It was not until the 1980s that these troops were finally airlifted to Taiwan. He promoted the Uyghur Yulbars Khan to Governor during the Islamic insurgency on the mainland for resisting the Communists, even though the government had already evacuated to Taiwan. He planned an invasion of the mainland in 1962. In the 1950s Chiang's airplanes dropped supplies to Kuomintang Muslim insurgents in Amdo.

Despite the democratic constitution, the government under Chiang was a one-party state, consisting almost completely of mainlanders; the "Temporary Provisions Effective During the Period of Communist Rebellion" greatly enhanced executive powers, and the goal of retaking mainland China allowed the KMT to maintain a monopoly on power and the prohibition of opposition parties. The government's official line for these martial law provisions stemmed from the claim that emergency provisions were necessary, since the Communists and KMT were still in a state of war. Seeking to promote Chinese nationalism, Chiang's government actively ignored and suppressed local cultural expression, even forbidding the use of local languages in mass media broadcasts or during class sessions. As a result of Taiwan's anti-government uprising in 1947, known as the February 28 incident, the KMT-led political repression resulted in the death or disappearance of over 30,000 Taiwanese intellectuals, activists, and people suspected of opposition to the KMT.

The first decades after the Nationalists moved the seat of government to the province of Taiwan are associated with the organized effort to resist Communism known as the "White Terror", during which about 140,000 Taiwanese were imprisoned for their real or perceived opposition to the Kuomintang. Most of those prosecuted were labeled by the Kuomintang as "bandit spies" (åªè«), meaning spies for Chinese Communists, and punished as such.

Under Chiang, the government recognized limited civil liberties, economic freedoms, property rights (personal and intellectual) and other liberties. Despite these restrictions, free debate within the confines of the legislature was permitted. Under the pretext that new elections could not be held in Communist-occupied constituencies, the National Assembly, Legislative Yuan, and Control Yuan members held their posts indefinitely. The Temporary Provisions also allowed Chiang to remain as president beyond the two-term limit in the Constitution. He was reelected by the National Assembly as president four timesâdoing so in 1954, 1960, 1966, and 1972.
Believing that corruption and a lack of morals were key reasons that the KMT lost mainland China to the Communists, Chiang attempted to purge corruption by dismissing members of the KMT accused of graft. Some major figures in the previous mainland Chinese government, such as H. H. Kung and T. V. Soong, exiled themselves to the United States. Although politically authoritarian and, to some extent, dominated by government-owned industries, Chiang's new Taiwanese state also encouraged economic development, especially in the export sector. A popular sweeping Land Reform Act, as well as American foreign aid during the 1950s, laid the foundation for Taiwan's economic success, becoming one of the Four Asian Tigers.

After Chiang's death, the next president, Chiang's son, Chiang Ching-kuo, and Chiang Ching-kuo's successor, Lee Teng-hui a native Taiwanese, would, in the 1980s and 1990s, increase native Taiwanese representation in the government and loosen the many authoritarian controls of the early era of ROC control in Taiwan.

In 1971, the Australian Opposition Leader Gough Whitlam, who became Prime Minister in 1972 and swiftly relocated the Australian mission from Taipei to Beijing, visited Japan. After meeting with the Japanese Prime Minister, Eisaku Sato, Whitlam observed that the reason Japan at that time was hesitant to withdraw recognition from the Nationalist government was "the presence of a treaty between the Japanese government and that of Chiang Kai-shek". Sato explained that the continued recognition of Japan towards the Nationalist government was due largely to the personal relationship that various members of the Japanese government felt towards Chiang. This relationship was rooted largely in the generous and lenient treatment of Japanese prisoners-of-war by the Nationalist government in the years immediately following the Japanese surrender in 1945, and was felt especially strongly as a bond of personal obligation by the most senior members then in power.

Although Japan recognized the People's Republic in 1972, shortly after Kakuei Tanaka succeeded Sato as Prime Minister of Japan, the memory of this relationship was strong enough to be reported by "The New York Times" (15 April 1978) as a significant factor inhibiting trade between Japan and the mainland. There is speculation that a clash between Communist forces and a Japanese warship in 1978 was caused by Chinese anger after Prime Minister Takeo Fukuda attended Chiang's funeral. Historically, Japanese attempts to normalize their relationship with the People's Republic were met with accusations of ingratitude in Taiwan.

Chiang was suspicious that covert operatives of the United States plotted a coup against him. In 1950, Chiang Ching-kuo became director of the secret police (Bureau of Investigation and Statistics), which he remained until 1965. Chiang was also suspicious of politicians who were overly friendly to the United States, and considered them his enemies. In 1953, seven days after surviving an assassination attempt, Wu Kuo-chen lost his position as governor of Taiwan Province to Chiang Ching-kuo. After fleeing to United States the same year, he became a vocal critic of Chiang's family and government.

Chiang Ching-kuo, educated in the Soviet Union, initiated Soviet-style military organization in the Republic of China Military. He reorganized and Sovietized the political officer corps, and propagated Kuomintang ideology throughout the military. Sun Li-jen, who was educated at the American Virginia Military Institute, was opposed to this.

Chiang Ching-kuo orchestrated the controversial court-martial and arrest of General Sun Li-jen in August 1955, for plotting a coup d'Ã©tat with the American Central Intelligence Agency (CIA) against his father Chiang Kai-shek and the Kuomintang. The CIA allegedly wanted to help Sun take control of Taiwan and declare its independence.

In 1975, 26 years after Chiang came to Taiwan, he died in Taipei at the age of 87. He had suffered a heart attack and pneumonia in the foregoing months and died from renal failure aggravated with advanced cardiac failure on 5 April.

A month of mourning was declared. Chinese music composer Hwang Yau-tai wrote the Chiang Kai-shek Memorial Song. In mainland China, however, Chiang's death was met with little apparent mourning and Communist state-run newspapers gave the brief headline "Chiang Kai-shek Has Died." Chiang's body was put in a copper coffin and temporarily interred at his favorite residence in Cihu, Daxi, Taoyuan. When his son Chiang Ching-kuo died in 1988, he was entombed in a separate mausoleum in nearby Touliao (é ­å¯®). The hope was to have both buried at their birthplace in Fenghua if and when it was possible. In 2004, Chiang Fang-liang, the widow of Chiang Ching-kuo, asked that both father and son be buried at Wuzhi Mountain Military Cemetery in Xizhi, Taipei County (now New Taipei City). Chiang's ultimate funeral ceremony became a political battle between the wishes of the state and the wishes of his family.

Chiang was succeeded as President by Vice President Yen Chia-kan and as Kuomintang party ruler by his son Chiang Ching-kuo, who retired Chiang Kai-shek's title of Director-General and instead assumed the position of chairman. Yen's presidency was interim; Chiang Ching-kuo, who was the Premier, became President after Yen's term ended three years later.

Chiang's portrait hung over Tiananmen Square before Mao's portrait was set up in its place. People also put portraits of Chiang in their homes and in public on the streets.

After his death, the Chiang Kai-shek Memorial Song was written in 1988 to commemorate Chiang Kai-shek.

In Cihu, there are several statues of Chiang Kai-shek.
Chiang was popular among many people and dressed in plain, simple clothes, unlike contemporary Chinese warlords who dressed extravagantly.

Quotes from the Quran and Hadith were used by Muslims in the Kuomintang-controlled Muslim publication, the "Yuehua", to justify Chiang Kai-shek's rule over China.

When the Muslim General and Warlord Ma Lin was interviewed, Ma Lin was described as having "high admiration for and unwavering loyalty to Chiang Kai-shek".

In the Philippines, a school was named in his honour in 1939. Today, Chiang Kai-shek College is the largest educational institution for the Chinoy community in the country.

The Kuomintang used traditional Chinese religious ceremonies, and promoted martyrdom in Chinese culture. Kuomintang ideology promoted the view that the souls of Party martyrs who died fighting for the Kuomintang, the revolution, and the party founder Dr. Sun Yat-sen were sent to heaven. Chiang Kai-shek believed that these martyrs witnessed events on earth from heaven.

When the Northern Expedition was complete, Kuomintang Generals led by Chiang Kai-shek paid tribute to Dr. Sun's soul in heaven with a sacrificial ceremony at the Xiangshan Temple in Beijing in July 1928. Among the Kuomintang Generals present were the Muslim Generals Bai Chongxi and Ma Fuxiang.

Chiang Kai-shek considered both the Han Chinese and all the minority peoples of China, the Five Races Under One Union, as descendants of the Yellow Emperor, the semi-mythical founder of the Chinese nation, and belonging to the Chinese Nation Zhonghua Minzu and he introduced this into Kuomintang ideology, which was propagated into the educational system of the Republic of China.

Chiang's legacy has been the target of heated debates because of the different views held about him. For some, Chiang was a national hero who led the victorious Northern Expedition against the Beiyang Warlords in 1927, achieving Chinese unification, and who subsequently led China to ultimate victory against Japan in 1945. Some blamed him for not doing enough against the Japanese forces in the lead-up to, and during, the Second Sino-Japanese War, preferring to withhold his armies for the fight against the Communists, or merely waiting and hoping that the United States would get involved. Some also see him as a champion of anti-Communism, being a key figure during the formative years of the World Anti-Communist League. During the Cold War, he was also seen as the leader who led Free China and the bulwark against a possible Communist invasion. However, Chiang presided over purges, political authoritarianism, and graft during his tenure in mainland China, and ruled throughout a period of imposed martial law. His governments were accused of being corrupt even before he even took power in 1928. He also allied with known criminals like Du Yuesheng for political and financial gains. Some opponents charge that Chiang's efforts in developing Taiwan were mostly to make the island a strong base from which to one day return to mainland China, and that Chiang had little regard for the long-term prosperity and well-being of the Taiwanese people.

Today, Chiang's popularity in Taiwan is divided along political lines, enjoying greater support among Kuomintang (KMT) supporters. He is generally unpopular among Democratic Progressive Party (DPP) voters and supporters who blame him for the thousands killed during the February 28 Incident and criticise his subsequent dictatorial rule. In sharp contrast to his son, Chiang Ching-kuo, and to Sun Yat-sen, his memory is rarely invoked by current political parties, including the Kuomintang. In contrast, his image has been rehabilitated in contemporary Mainland China. Until recently portrayed as a villain who fought against the "liberation" of China by the Communists, since the 2000s, he has been portrayed by the media in a neutral or slightly positive light as a Chinese nationalist who tried to bring about national unification and resisted the Japanese invasion during World War II. This shift is largely in response to current political landscape of Taiwan, in relation to Chiang's commitment to a unified China and his stance against Taiwanese separatism during his rule of the island, along with the recent dÃ©tente between the Communist Party of China (CPC) and Chiang's KMT. In contrast to efforts to remove his public monuments in Taiwan, his ancestral home in Fenghua, Zhejiang on the Mainland has become a commemorative museum and major tourist attraction.

In the United States and Europe, Chiang was often perceived negatively as the one who lost China to the Communists. His constant demands for Western support and funding also earned him the nickname of "General Cash-My-Check". In the West he has been criticized for his poor military skills. He had a record of issuing unrealistic orders and persistently attempting to fight unwinnable battles, leading to the loss of his best troops.

In recent years, there has been an attempt to find a more moderate interpretation of Chiang. Chiang is now increasingly perceived as a man simply overwhelmed by the events in China, having to fight simultaneously Communists, Japanese, and provincial warlords while having to reconstruct and unify the country. His sincere, albeit often unsuccessful attempts to build a more powerful nation have been noted by scholars such as Jonathan Fenby and Rana Mitter. Mitter has observed that, ironically, today's China is closer to Chiang's vision than to Mao Zedong's. He argues that the Communists, since the 1980s, have essentially created the state envisioned by Chiang in the 1930s. Mitter concludes by writing that "one can imagine Chiang Kai-shek's ghost wandering round China today nodding in approval, while Mao's ghost follows behind him, moaning at the destruction of his vision". Liang Shuming opined that Chiang Kai-shek's "greatest contribution was to make the CCP successful. If he had been a bit more trustworthy, if his character was somewhat better, the CCP would have been unable to beat him".

"Formosa Betrayed", one of the few American movies concerning the process of democratization in Taiwan, depicts Chiang Kai-shek as a brutal dictator, responsible for the execution of thousands of native Taiwanese during the days following the February 28 Incident.

In an arranged marriage, Chiang was married to a fellow villager named Mao Fumei. While married to Mao, Chiang adopted two concubines (concubinage was still a common practice for well-to-do, non-Christian males in China): he married Yao Yecheng (å§å¶èª , 1889â1972) in 1912 and Chen Jieru (é³æ½å¦, 1906â1971) in December 1921. While he was still living in Shanghai, Chiang and Yao adopted a son, Wei-kuo. Chen adopted a daughter in 1924, named Yaoguang (ç¤å), who later adopted her mother's surname. Chen's autobiography refuted the idea that she was a concubine. Chen claiming that, by the time she married Chiang, he had already divorced Yao, and that Chen was therefore his wife. Chiang and Mao had a son, Ching-kuo.

According to the memoirs of Chen Jieru, Chiang's second wife, she contracted gonorrhea from Chiang soon after their marriage. He told her that he acquired this disease after separating from his first wife and living with his concubine Yao Yecheng, as well as with many other women he consorted with. His doctor explained to her that Chiang had sex with her before completing his treatment for the disease. As a result, both Chiang and Ch'en Chieh-ju believed they had become sterile, which would explain why he had only one child, by his first wife; however, a purported miscarriage by Soong Mei-ling in August 1928 would, if it actually occurred, cast serious doubt on whether this was true.

The Xikou (Chikow) Chiangs were descended from Chiang Shih-chieh who during the 1600s (17th century) moved there from Fenghua district, whose ancestors in turn came to southeastern China's Zhejiang (Chekiang) province after moving out of Northern China in the 13th century AD. The 12th century BC Duke of Zhou's (Duke of Chou) third son was the ancestors of the Chiangs.

His great grandfather was Chiang Qi-zeng (Jiang Qizeng) èç¥å¢, his grandfather was Chiang Si-qian èæ¯å, his uncle was Chiang Zhao-hai è£èæµ·, and his father was Chiang Zhao-cong (Jiang Zhaocong) è£èè°.

Chiang personally dealt extensively with religions and power figures in China during his regime.

Chiang Kai-shek was born and raised as a Buddhist, but became a Methodist upon his marriage to his fourth wife, Soong Mei-ling. It was previously believed that this was a political move, but studies of his recently opened diaries suggest that his faith was sincere.

Chiang developed relationships with other generals. Chiang became a sworn brother of the Chinese Muslim general Ma Fuxiang and appointed him to high ranking positions. Chiang addressed Ma Fuxiang's son Ma Hongkui as Shao Yun Shixiong Ma Fuxiang attended national leadership conferences with Chiang during battles against Japan. Ma Hongkui was eventually scapegoated for the failure of the Ningxia Campaign against the Communists, so he moved to the US instead of remaining in Taiwan with Chiang.

When Chiang became President of China after the Northern Expedition, he carved out Ningxia and Qinghai out of Gansu province, and appointed Muslim generals as military governors of all three provinces: Ma Hongkui, Ma Hongbin, and Ma Qi. The three Muslim governors, known as Xibei San Ma (lit. "the three Mas of the Northwest"), controlled armies composed entirely of Muslims. Chiang called on the three and their subordinates to wage war against the Soviet peoples, Tibetans, Communists, and the Japanese. Chiang continued to appoint Muslims as governors of the three provinces, including Ma Lin and Ma Fushou. Chiang's appointments, the first time that Muslims had been appointed as governors of Gansu, increased the prestige of Muslim officials in northwestern China. The armies raised by this "Ma Clique", most notably their Muslim cavalry, were incorporated into the KMT army. Chiang appointed a Muslim general, Bai Chongxi, as the Minister of National Defence of the Republic of China, which controlled the ROC military.

Chiang also supported the Muslim General Ma Zhongying, whom he had trained at Whampoa Military Academy during the Kumul Rebellion, in a Jihad against Jin Shuren, Sheng Shicai, and the Soviet Union during the Soviet Invasion of Xinjiang. Chiang designated Ma's Muslim army as the 36th Division (National Revolutionary Army) and gave his troops Kuomintang flags and uniforms. Chiang then supported Muslim General Ma Hushan against Sheng Shicai and the Soviet Union in the Xinjiang War (1937). All Muslim generals commissioned by Chiang in the National Revolutionary Army swore allegiance to him. Several, like Ma Shaowu and Ma Hushan were loyal to Chiang and Kuomintang hardliners.

The Ili Rebellion and Pei-ta-shan Incident plagued relations with the Soviet Union during Chiang's rule and caused trouble with the Uyghurs. During the Ili Rebellion and Peitashan incident, Chiang deployed Hui troops against Uyghur mobs in Turfan, and against Soviet Russian and Mongols at Peitashan.

During Chiang's rule, attacks on foreigners by Kuomintang forces flared up in several incidents. One of these was the Battle of Kashgar (1934) where a Muslim army loyal to the Kuomintang massacred 4,500 Uyghurs, and killed several British at the British consulate in Kashgar. The British were unable to retaliate.

Hu Songshan, a Muslim Imam, backed Chiang Kai-shek's regime and gave prayers for his government. ROC flags were saluted by Muslims in Ningxia during prayer along with exhortations to nationalism during Chiang's rule. Chiang sent Muslim students abroad to study at places like Al-Azhar University and Muslim schools throughout China taught loyalty to his regime.

The Yuehua, a Chinese Muslim publication, quoted the Quran and Hadith to justify submitting to Chiang Kai-shek as the leader of China, and as justification for Jihad in the war against Japan.

The Yihewani (Ikhwan al Muslimun a.k.a. Muslim brotherhood) was the predominant Muslim sect backed by the Chiang government during Chiang's regime. Other Muslim sects, like the Xidaotang and Sufi brotherhoods like Jahriyya and Khuffiya were also supported by his regime. The Chinese Muslim Association, a pro-Kuomintang and anti-Communist organization, was set up by Muslims working in his regime. Salafism attempted to gain a foothold in China during his regime, but the Yihewani and Hanafi Sunni Gedimu denounced the Salafis as radicals, engaged in fights against them, and declared them heretics, forcing the Salafis to form a separate sect. Ma Ching-chiang, a Muslim General, served as an advisor to Chiang Kai-shek. Ma Buqing was another Muslim General who fled to Taiwan along with Chiang. His government donated money to build the Taipei Grand Mosque on Taiwan.

Chiang had uneasy relations with the Tibetans. He fought against them in the Sino-Tibetan War, and he supported the Muslim General Ma Bufang in his war against Tibetan rebels in Qinghai. Chiang ordered Ma Bufang to prepare his Islamic army to invade Tibet several times, to deter Tibetan independence, and threatened them with aerial bombardment. After the war, Chiang appointed Ma Bufang as ambassador to Saudi Arabia.

Chiang incorporated Methodist values into the New Life Movement under the influence of his wife. Dancing and Western music were discouraged. In one incident, several youths splashed acid on people wearing Western clothing, although Chiang was not directly responsible for these incidents. Despite being a Methodist, he made reference to the Buddha in his diary, and encouraged the establishment of a Buddhist political party under Master Taixu.

According to Jehovah's Witnesses some of their members travelled to Chonqqing and spoke to him personally while distributing their literature there during the Second World War.






</doc>
<doc id="6863" url="https://en.wikipedia.org/wiki?curid=6863" title="Compression ratio">
Compression ratio

In a combustion engine, the static compression ratio is calculated based on the relative volumes of the combustion chamber and the cylinder. It is a fundamental specification for combustion engines. The dynamic compression ratio is a more advanced calculation which also takes into account gasses entering and exiting the cylinder during the compression phase.

Most engines used a fixed compression ratio, however a variable compression ratio engine is able to adjust the compression ratio while the engine is in operation. The first production engine with a variable compression ratio was introduced in 2019.

A high compression ratio is desirable because it allows an engine to extract more mechanical energy from a given mass of airâfuel mixture due to its higher thermal efficiency. This occurs because internal combustion engines are heat engines, and higher compression ratios permit the same combustion temperature to be reached with less fuel, while giving a longer expansion cycle, creating more mechanical power output and lowering the exhaust temperature. 

In production gasoline (petrol) engines from the past 20 years, compression ratios are typically between 8â¶1 and 12â¶1. Several production engines have used higher compression ratios, including:

When forced induction (e.g. a turbocharger or supercharger) is used, the compression ratio is often lower than naturally aspirated engines. This is due to the turbocharger/supercharger already having compressed the air before it enters the cylinders. Engines using port fuel-injection typically run lower boost pressures and/or compression ratios than direct injected engines because port fuel injection causes the air/fuel mixture to be heated together, leading to detonation. Conversely, directly injected engines can run higher boost because heated air will not detonate without a fuel being present.

Higher compression ratios can make gasoline (petrol) engines subject to engine knocking (also known as "detonation", "pre-ignition" or "pinging") if lower octane-rated fuel is used. This can reduce efficiency or damage the engine if knock sensors are not present to modify the ignition timing.

Diesel engines use higher compression ratios than petrol engines, because the lack of a spark plug means that the compression ratio must increase the temperature of the air in the cylinder sufficiently to ignite the diesel. Compression ratios are often between 14â¶1 and 23â¶1 for direct injection diesel engines, and between 18â¶1 and 23â¶1 for indirect injection diesel engines.

Since diesel engines operate on the principle of compression ignition, a fuel which resists autoignition will cause late ignition, which can lead to engine knock. Diesel engines have a higher peak combustion temperature than petrol engines, but the greater expansion means they eject less heat in their cooler exhaust.

The compression ratio may be higher in engines running exclusively on LPG (autogas) or compressed natural gas, due to the higher octane rating of these fuels.

Kerosene engines typically use a compression ratio of 6.5 or lower. The petrol-paraffin engine version of the Ferguson TE20 tractor had a compression ratio of 4.5â¶1 for operation on tractor vaporising oil with an octane rating between 55 and 70.

Motorsport engines often run on high octane petrol and can therefore use higher compression ratios. For example, motorcycle racing engines can use compression ratios as high as 14.7â¶1, and it is common to find motorcycles with compression ratios above 12.0â¶1 designed for 86 or 87 octane fuel. F1 engines come closer to 17â¶1, which is critical for maximizing volumetric/fuel efficiency at around 18,000 RPM.

Ethanol and methanol can take significantly higher compression ratios than gasoline. Racing engines burning methanol and ethanol fuel often have a compression ratio of 14â¶1 to 16â¶1.

In a piston engine, the static compression ratio (formula_1) is the ratio between the volume of the cylinder and combustion chamber when the piston is at the bottom of its stroke, and the volume of the combustion chamber when the piston is at the top of its stroke. It is therefore calculated by the formula

Where:

formula_3 can be estimated by the cylinder volume formula

Where:

Because of the complex shape of formula_4 it is usually measured directly. This is often done by filling the cylinder with liquid and then measuring the volume of the used liquid.

Variable compression ratio is a technology to adjust the compression ratio of an internal combustion engine while the engine is in operation. This is done to increase fuel efficiency while under varying loads. Variable compression engines allow the volume above the piston at top dead centre to be changed.

Higher loads require lower ratios to increase power, while lower loads need higher ratios to increase efficiency, i.e. to lower fuel consumption. For automotive use this needs to be done as the engine is running in response to the load and driving demands.

The 2019 Infiniti QX50 is the first commercially available car that uses a variable compression ratio engine.

Based on the assumptions that adiabatic compression is carried out (i.e. that no heat energy is supplied to the gas being compressed, and that any temperature rise is solely due to the compression) and that air is a perfect gas, the relationship between the compression ratio and overall pressure ratio is as follows:
This relationship is derived from the following equation:

However, in most real-life internal combustion engines, the ratio of specific heats changes with temperature and that significant deviations from adiabatic behavior will occur.

The static compression ratio discussed above â calculated solely based on the cylinder and combustion chamber volumes â do not take into account any gasses entering or exiting the cylinder during the compression phase. In most automotive engines, the intake valve closure (which seals the cylinder) takes place during the compression phase (i.e. after bottom dead centre, BDC), which can cause some of the gasses to be pushed back out through the intake valve. On the other hand, intake port tuning and scavenging can cause a greater amount of gas to be trapped in the cylinder than the static volume would suggest. The dynamic compression ratio accounts for these factors.

The dynamic compression ratio is higher with more conservative intake camshaft timing (i.e. soon after BDC), and lower with more radical intake camshaft timing (i.e. later after BDC). Regardless, the dynamic compression ratio is always lower than the static compression ratio.

The absolute cylinder pressure is used to calculate the dynamic compression ratio, using the following formula:

Under ideal (adiabatic) conditions, the ratio of specific heats would be 1.4, but a lower value, generally between 1.2 and 1.3 is used, since the amount of heat lost will vary among engines based on design, size and materials used. For example, if the static compression ratio is 10â¶1, and the dynamic compression ratio is 7.5â¶1, a useful value for cylinder pressure would be 7.5 Ã atmospheric pressure, or 13.7Â bar (relative to atmospheric pressure).

The two corrections for dynamic compression ratio affect cylinder pressure in opposite directions, but not in equal strength. An engine with high static compression ratio and late intake valve closure will have a dynamic compression ratio similar to an engine with lower compression but earlier intake valve closure.



</doc>
<doc id="6865" url="https://en.wikipedia.org/wiki?curid=6865" title="Concordat of Worms">
Concordat of Worms

The Concordat of Worms is the 1122 agreement between Henry V, Holy Roman Emperor, and Pope Callixtus II, which brought to an end the first phase of the power struggle between the papacy and the Holy Roman Emperor, known as the Investiture Controversy. It was signed on 23 September, 1122, near the German city of Worms.

The Concordat of Worms () is sometimes called the Pactum Callixtinum by papal historians, since the term concordat was not in use until Nicolas of Cusa's "De concordantia catholica" of 1434.

The pact has been interpreted as containing within itself the germ of nation-based sovereignty that would one day be confirmed in the Peace of Westphalia (1648). 

In part this was an unforeseen result of strategic maneuvering between the Church and the European sovereigns over political control within their domains. The king was recognized as having the right to invest bishops with secular authority ("by the lance") in the territories they governed, but not with sacred authority ("by ring and staff"). The result was that bishops owed allegiance in worldly matters both to the pope and to the king, for they were obliged to affirm the right of the sovereign to call upon them for military support, under his oath of fealty. Previous Holy Roman emperors had thought it their right, granted by God, to name Church officials within their territories (such as bishops) and to confirm the papal election (and, at times of extraordinary urgency, actually name popes). In fact, the emperors had been heavily relying on bishops for their secular administration, as they were not hereditary or quasi-hereditary nobility with family interests. 

A more immediate result of the investiture struggle identified a proprietary right that adhered to sovereign territory, recognizing the right of kings to income from the territory of a vacant diocese and a basis for justifiable taxation. These rights lay outside feudalism, which defined authority in a hierarchy of personal relations, with only a loose relation to territory. The pope emerged as a figure above and out of the direct control of the Holy Roman Emperor. 

Following efforts by Lamberto Scannabecchi, the future Pope Honorius II, and the 1221 Diet of WÃ¼rzburg, Pope Callixtus II and Emperor Henry V entered into an agreement that effectively ended the Investiture Controversy. By the terms of the agreement, the election of bishops and abbots in Germany was to take place in the emperor's presence as judge between potentially disputing parties, free of bribes, thus retaining to the emperor a crucial role in choosing these great territorial magnates of the Empire. Beyond the borders of Germany, in Burgundy and Italy, the emperor was to forward the symbols of authority within six months. Callixtus' reference to the feudal homage due the emperor on appointment is guarded: "shall do unto thee for these what he rightfully should" was the wording of the "privilegium" granted by Callixtus. The emperor's right to a substantial imbursement on the election of a bishop or abbot was specifically denied.

The emperor renounced the right to invest ecclesiastics with ring and crosier, the symbols of their spiritual power, and guaranteed election by the canons of cathedral or abbey and free consecration. The two ended by granting one another peace.

The Concordat was confirmed by the First Council of the Lateran in 1123.

The Concordat of Worms was a part of the larger reforms put forth by many popes, most notably Pope Gregory VII. These included the reinforcement of celibacy of the clergy, end of simony and autonomy of the Church from secular leaders (lack of autonomy was known as lay investiture).

The most prized and contested rights that attached to benefices were inheritance and security against confiscation. Benefices were lands granted by the Church to faithful lords. In exchange, the Church expected rent or other services, such as military protection. These lands would then be further divided between lesser lords and commoners. This was the nature of European feudalism. Inheritance was an important issue, since land could fall into the hands of those who did not have loyalty to the Church or the great lords. The usual grant was "in precaria", the granting of a life tenure, whereby the tenant stayed on the land only at the pleasure of the lord. The tenant could be expelled from the land at any time. His tenancy was "precarious". Counts' benefices came to be inherited as counties were broken up and as counts assimilated their offices and ex-officio lands to their family property. In central Europe, kings and counts probably were willing to allow the inheritance of small parcels of land to the heirs of those who had offered military or other services in exchange for tenancy. This was contingent on the heirs being reasonably loyal and capable. Churches in Germany, as elsewhere, were willing to allow peasants to inherit their land. This was a source of profit to both churches and lords when the inheritors were charged a fee to inherit the land. Most bishops had a different attitude toward freemen and nobles. To these peasants, grants were made "in precario" or "in beneficio", usually for a specified and limited number of "life tenures". It was not impossible to recover land left to noble families for generations. But the longer the family held church land, the more difficult it was to oust them from the land. Some church officials came to view granting land to noble families amounted to outright alienation. By the twelfth century great churches in Germany, like those elsewhere were finding it difficult to hold out against the accumulation of lay custom and lay objections to temporary inheritance. The Bishop of Worms issued a statement in 1120 indicating the poor and unfree should be allowed to inherit tenancy without payment of fees. It appears to have been something novel. The growing masses of unfree and the marginal were needed for labour, and to bolster the military of both nobility and the church. By the time of Henry IV, bargaining by the peasants for the benefit of the group was the norm.

The Holy Roman Emperors of Ottonian dynasty, when they came to the throne, believed they should have the power to appoint the pope. They also believed they should appoint minor church officials. The result was that, more often than not, bishops, abbots of monasteries, and even the pope were not independent, but resembled lackeys or sycophants of the crown of the Holy Roman Empire. This attitude was bolstered by the general conception that the Holy Roman Emperor and all other European Kings were chosen by God to be leaders.

For temporal secular reasons, the kings did nothing to dispel this attitude. It meant more power for them. A series of popes began to directly challenge this condition. The most vocal and strident was Pope Gregory VII. Reform took a century, but brought greater autonomy for the papacy and the Church in general.

In the period immediately after 1000, two figures appeared to lead Western Christendom, the pope and the Holy Roman Emperor. Antagonism between the two dominated the next century. After the death of Pope Sylvester II in 1003, the papacy fell under the influence of the nobility in Latium, and then after 1046, under the influence of the German emperors. The reality for the west in the Middle Ages was not only the fact that government was split up into small particles but also the fact that vertical and horizontal powers were entangled. People in the Middle Ages did not always know to which of the many lords, the Church and the individual churches, the towns, princes, and kings, they were subordinate. This can be observed in the complexity even at the administrative and judicial level in the jurisdictional conflicts that fill medieval history.

The Church endeavoured to become disengaged from German control. An example of this secular politicisation is seen when Conrad II, Holy Roman Emperor supported Pope Benedict IX, the most corrupt of any of the popes of the era. It took more than a century to end this manipulation, and was never complete. In the process, the whole Church emerged freed from the grip of all lay lords. This was known as the Gregorian Reform, which takes its name from Pope Gregory VII (1073â85). It was merely the latest and most visible of reforms that tended to move the Church back to its roots. It was a question of restoring the autonomy and power of the priestly class in the face of increasing control by the warrior class. The clergy was forced to renew and define itself. There was a battle against simony. The roadmap to celibacy was drawn, if not immediately enacted. Monarchs were excluded from selecting popes. This had been decreed by Pope Nicholas II in 1059. Afterwards, only cardinals could elect the pope. Gregorian Reform reiterated this notion. There was to be no more lay interference in the selection of clergy. The aim was to deprive emperors and their under-lords the right to nominate and invest bishops. The effect was to deprive lay kings power over the Church and increase both spiritual and temporal power in the Vatican and the bishops.

Gregory VII appeared to have succeeded when the emperor Henry IV, Holy Roman Emperor was humiliated at Canossa in 1077. There, Henry begged in the snow to be let back into the good graces of the Church, having been excommunicated the year before by Gregory. The penitent and humbled emperor did not remain in that state. Soon Henry IV took his revenge. He named his own pope Antipope Clement III in the old manner of the Holy Roman Emperors. Pope Urban II, more prudent than Gregory, sidestepped the issue using a Crusade to gather Christian Europe together under his authority. A compromise was reached in Worms in 1122, by which the emperor abandoned investiture "by ring and staff" to the pope, and promised to respect the freedom of elections and consecrations, but kept for himself the right to invest bishops with the temporalities of their sees "by scepter". Though the Emperor retained some power over imperial churches, his power was damaged irreparably because he lost the religious authority that previously belonged to the office of the king. In France, England, and the Christian state in Spain, the king could overcome rebellions of his magnates and establish the power of his royal demesne because he could rely on the Church, which, for several centuries, had given him a mystical authority. From time to time, rebellious and recalcitrant monarchs might run afoul of the Church. These could be excommunicated, and after an appropriate time and public penance, be received back into the communion and good graces of the Church.

In 1075, Gregory VII condemned lay investiture in a document called Dictatus papae. The new doctrine was called libertas ecclesiae ("freedom of the Church"). Henry IV insisted on involvement in clerical appointment. The dispute revolved around the issue of investiture â i.e., whether the Holy Roman Emperor had the right to name bishops as well as popes. Gregory VII excommunicated Henry IV in 1076, releasing all Henry's subjects from obedience to him. This led to a great political struggle, with many barons rising against Henry in open rebellion. Henry made his way to Canossa where the Pope was staying in the castle of Countess Matilda. Henry claimed a wish to repent. The pope was suspicious of Henry's motives, and did not believe he was truly repentant. Henry did penance in the snow outside the castle for three days. Finally, Gregory gave him absolution. The rebellious nobles in Germany who were interested in deposing Henry IV never forgave Pope Gregory VII for what they viewed as treachery.

Henry IV was excommunicated again in 1080, and would not show any indication of repentance. In turn, Henry called a council of bishops who proclaimed Gregory illegitimate. He remained excommunicated for twenty-six years until his death in 1106. It was the consequence of this lengthy episode that a whole generation grew up in Germany and Northern Italy in an atmosphere of war, doubt and scepticism. The papal backers had been busy propounding arguments to show that royal power was not of divine origin. They had been so successful that the moral authority of the Emperor had been undermined in the minds of many of his subjects. Serious divisions existed from this battle over the Investiture Controversy, which fractured large portions of the Holy Roman Empire in Germany and Italy. Davis argues these rifts were so deep and lasting that neither Germany nor Italy were able to form a cohesive nation state until the 19th century. A similar situation arose from the French revolution, which caused fractures in France that still exist. The effect of Henry's excommunication, and his subsequent refusal to repent left a turbulence in central Europe that lasted throughout the Middle Ages. It may have been emblematic of certain German attitudes toward religion in general, and the perceived relevance of the German Emperor in the universal scheme of things.

Henry IV became so filled with hubris over his position that he renounced Gregory VII and named the bishop of Ravenna pope. Perhaps he was only following what had been thought to be the right of kings: to name the pope. Henry had invested the Ravenna bishop, and now he referred to the new pope, Clement III, Antipope Clement III as "our pope". Henry attacked Rome, and on the outskirts of the city gained thirteen cardinals who became loyal to his cause. On Palm Sunday, 1084, Henry IV solemnly enthroned Clement at St. Peter's Basilica and on Easter Day, Clement returned the favour and crowned Henry IV as Emperor of the Holy Roman Empire. Gregory VII was meanwhile still resisting a few hundred yards away from the basilica in the Castel Sant'Angelo, then known as the house of Cencius. Gregory appealed to the Normans for help, and Robert Guiscard responded, entering Rome on 27 May 1084 and rescuing him.

In the process, Rome was pillaged and partially burned. Gregory VII died the next year on 25 May 1085 in exile. He felt all was lost. The last words he uttered were, "I have loved justice and hated iniquity, and therefore I die in exile." Gregory VII must have felt he died in utter failure, and to many of his contemporaries it appeared Henry IV and Antipope Clement III had won. But the underlying current was that Henry had overreached, and his appointment of the antipope was beyond the pale. Upon the death of Gregory, the cardinals elected a new pope, Pope Victor III. He owed his elevation to the influence of the Normans. Antipope Clement III still occupied St. Peter's. When Victor III died, the cardinals elected Pope Urban II (1088â99). He was one of three men Gregory VII suggested as his successor. Urban II preached the First Crusade, which united Western Europe, and more importantly, reconciled the majority of bishops who had abandoned Gregory VII. In the end, Gregorian Reform won out over Henry IV. Preaching the Crusade had one important consequence. The pope was now viewed as the head of the Church. No longer would kings and emperors think themselves equals of the pope, or the head of the Church in their kingdom. This was the situation from 1122 until the Reformation.

Several years later, Henry IV died in a deep gloom, as had Gregory. It remained for his successor, Henry V to agree with Pope Callixtus II in 1122 to a compromise of the conflict over lay investitures known as the Concordat of Worms.

The reign of Henry IV showed the weakness of the German monarchy. The ruler was dependent upon the good will of the great men, the nobility of his land. These were technically royal officials and hereditary princes. He was also dependent on the resources of the churches. Henry IV alienated the Church of Rome and many of the magnates in his own kingdom. Many of these spent years in open or subversive rebellion. Henry failed to create a proper bureaucracy to replace his disobedient vassals. The magnates became increasingly independent, and the Church withdrew support. Henry IV spent the last years of his life desperately grasping to keep his throne. It was a greatly diminished kingdom.

The reign of Henry IV ended with a diminished kingdom and waning power. Many of his underlords had been in constant or desultory revolt for years. Henry IV's insistence that Antipope Clement III was the real pope had initially been popular with some of the nobles, and even many of the bishops of Germany. But as years passed, this support was slowly withdrawn. The idea that the German king could and should name the pope was increasingly discredited and viewed as an anachronism from a by-gone era. The Empire of the Ottos was virtually lost because of Henry IV.

Henry IV's son, Henry V, rebelled and became emperor after his father's abdication. Henry V realised swift action and a change in his father's policy was necessary. Pope Paschal II rebuked Henry V for appointing bishops in Germany. The king crossed the Alps with an army in 1111. The pope, who was weak and had few supporters was forced to suggest a compromise, the abortive Concordat of 1111. Its simple and radical solution of the Investiture Controversy between the prerogatives of "regnum" and "sacredoium" proposed that German churchmen would surrender their lands and secular offices to the emperor and constitute a purely spiritual church. Henry gained greater control over the lands of his kingdom, especially those that had been in the hands of the church, but of contested title. He would not interfere with ecclesiastical affairs and churchmen would avoid secular services. The church would be given autonomy and to Henry V would be restored large parts of his empire that his father had lost. Henry V was crowned by Pope Paschal II as the legitimate Holy Roman Emperor. When the concessions of land were read in St. Peters, the crowd revolted in anger. Henry took the pope and cardinals hostage until the pope granted Henry V the right of investiture. Then he returned to Germany â crowned emperor and apparent victor over the papacy.

The victory was as short-lived as that of his father, Henry IV over Gregory VII. The clergy urged Paschal to rescind his agreement, which he did in 1112. The quarrel followed the predictable course: Henry V rebelled and was excommunicated. Riots broke out in Germany, a new Antipope Gregory VIII was appointed by the German king, nobles loyal to Rome seceded from Henry. The civil war continued, just as under Henry IV. It dragged on for another ten years. Like his father before him, Henry V was faced with waning power. He had no choice but to give up investiture and the old right of naming the pope. The Concordat of Worms was the result. After the Concordat, the German kings never had the same control over the Church as had existed in the time of the Ottonian dynasty.

Henry V died without heirs in 1125, three years after the Concordat. He had designated his nephew, Frederick von Staufen duke of Swabia, also known as Frederick II, Duke of Swabia as his successor. Instead, churchmen elected Lothair II. A long civil war erupted between the Staufen also known as Hohenstaufen supporters and the heirs of Lothar III. The result was the Hohenstaufen Frederick I 1152â1190 who came to power.

The Concordat of Worms was foreshadowed by the Charter of Liberties of Henry I of England. He was the youngest son of William the Conqueror. Through a series of political intrigues, Henry I gained the English throne in 1100. Henry had three problems:

Henry reconciled with the Church and Anselm. He married Edith, the daughter of Malcolm III of Scotland. The Anglo-Saxon population was placated because they viewed Edith as one of their own. Henry signed and issued the Charter of Liberties in 1100 from the Norman Chapel in the Tower of London. This gave concessions to the earls and barons, as well as the Church. The investiture issue was still contentious, but a compromise at Bec Abbey in 1107 was essentially identical to the Concordat of Worms.

The Concordat of London in 1107 was a forerunner of the compromise that was taken up in the Concordat of Worms. In England, as in Germany, the conflict between Church and State was rife. A distinction was being made in the king's chancery between the secular and ecclesiastical powers of the prelates. Bowing to political reality, Henry I of England ceded his right to invest his bishops and abbots and reserved the custom of requiring them to come and do homage. The system of vassalage was not divided among great local lords in England as it was in France, for by right of the Conquest the king was in control.

Henry I of England perceived a danger in placing monastic scholars in his chancery and turned increasingly to secular clerks, some of whom held minor positions in the Church. He often rewarded these men with the titles of bishop and abbot. Henry I expanded the system of scutage to reduce the monarchy's dependence on knights supplied from church lands. Unlike the situation in Germany, Henry I of England used the Investiture Controversy to strengthen the secular power of the king. It would continue to boil under the surface. The controversy would surface in the Thomas Becket affair under Henry II of England, the , the Statutes of Mortmain and the battles over Cestui que use of Henry VII of England, and finally come to a head under Henry VIII of England.

Of the three reforms Gregory VII and his predecessors and successor popes had attempted, they had been most successful in regard to celibacy of the clergy. Simony had been partially checked. Against lay investiture they won only a limited success, and one that seemed less impressive as the years passed. During the time following the Concordat of Worms, the Church gained in both stature and power.

According to the terms of the compromise, the election of bishops and abbots was to follow proper procedure, that is, the canons of the cathedral were to elect the bishop. The monks were to choose the abbot, and only ecclesiastical superiors were to invest the candidate with ring and staff (the traditional insignia of the episcopal office). This was a minimum that the church had demanded (who had far fewer problems with a mere nomination by a layman if the layman did not actually hand over ring and staff). To make up for this and symbolise the "worldly" authority of the bishop which the pope had always recognised to derive from the Emperor, another symbol, the scepter, was invented, which would be handed over by the king (or his legate). The rest is an actual compromise: In "Germany", the Emperor (or his legate) would have the right to be present at elections to resolve any disputes between candidates ("yet without violence"). What this meant, in effect, was that the king would have the bishop he wanted (though over time, the territorial princes would get some "representation" within the chapters, making it less easy to ignore them). The bishop-elect would then by invested by the Emperor (or representative) with the scepter and, sometime afterwards, by his ecclesial superior with ring and staff. As William of Champeaux assured Henry V, he had nothing to lose by surrendering the right of investiture. The king retained substantially what he already possessedâthe power to fill bishoprics with men of his choice. Nevertheless, Gregory VII's dramatisation of the issue produced a significant improvement in the character of men raised to the episcopacy. Kings no longer interfered so frequently in their election, and when they did, they generally nominated more worthy candidates for the office.

This takes only Germany into account, though. As for Burgundy and Italy, elections were to be held without interference by the Emperor, and consecration and investiture by the ecclesial superior would, here, precede the investiture with the scepter which was to follow some time afterwards. Thus, by the Pope accepting the German bishops to be nominated with a large amount of secular influence, his right was recognized to freely choose the bishops in the other parts of the Empire, especially in Imperial Italy at the very doors of the Papal States. To see in the Concordat of Worms mere face-saving of the Church is thus not correct.

The writing in the document was ambiguous, skirted some issues and avoided others all together. This has caused some scholars to conclude that the settlement turned its back on Gregory VII's and Urban II's genuine hopes for reform. The emperor's influence in episcopal was preserved, and he could decide disputed elections. If the compromise was a rebuke to the most radical vision of the liberty of the Church, on at least one point its implication was firm and unmistakable: the king, even an emperor, was a layman, and his power at least morally limited (hence, totalitarianism was unacceptable). According to the opinion of W. Jordan, the divine right of kings was dealt a blow from which it never completely recovered, yet unfettered authority and Caesaropapism was not something the later Mediaevals and Early Moderns understood by the phrase "by the grace of God" (which many of them ardently defended). If anything, a blow was dealt to subconsciously remaining pre-Christian Germanic feelings of "royal hail".

There exists a misconception concerning the power of the pope in the Middle Ages. Tradition affords him more power and authority than he actually possessed. It is likely the pope in modern ages is much more powerful than those in mediaeval times. The most powerful of all mediaeval popes was Innocent III. His pronouncements on doctrinal matters and the judgments of his court were considered definitive and final. Opposing the mediaeval pope was the primary and unyielding authority of the state. The struggle over investiture between Pope Gregory VII and Henry IV, Holy Roman Emperor had dramatised the clash between church and state. The Concordat of Worms had eased the situation for a generation. But in the end, it solved nothing. Practically speaking, the king retained a decisive voice in the selection of the hierarchy. All kings supported King John of England's defiance of Pope Innocent III ninety years after the Concordat of Worms in the matter concerning Stephen Langton. In theory, the pope named his bishops and cardinals. In reality, more often than not, Rome consecrated the clergy once it was notified by the kings who the incumbent would be. Recalcitrance by Rome would lead to problems in the kingdom. For the most part it was a no-win situation for Rome. In this, the Concordat of Worms changed little. The growth of canon law in the Ecclesiastical Courts was based on the underlying Roman law and increased the strength of the Roman Pontiff.

The English Church was left more or less in the power of the English monarchy. This was the result of the Charter of Liberties, 1100, and the agreement at Bec in 1107. The effect of the Concordat of Worms was different. It ended a civil war that had been going on for more than fifty years. There was no going back to the situation that had preceded it. The political and social structure of Germany had forever been altered. The new generation of cardinals regarded German investiture with contempt and as an embarrassing vestige of the past. They were willing to make concessions with Henry V and his successors in order to get along. The belief after the Concordat was that investiture and the era of theocratic kingship was a discredited doctrine. The German kings had a different view of the matter. Henry V and his successors still believed they had the right and ability to name bishops. In practice, this was true, but only in the territories held by their families. Their domain in the religious sphere had been greatly diminished.

The catastrophic political consequences of the struggle between pope and emperor also led to a cultural disaster. Germany lost intellectual leadership in western Europe. In 1050, German monasteries were great centres of learning and art and German schools of theology and canon law were unsurpassed and probably unmatched anywhere in Europe. The long civil war over investiture sapped the energy of both German churchmen and intellectuals. They fell behind advances in philosophy, law, literature and art taking place in France and Italy. In many ways, Germany never caught up during the rest of the Middle Ages. Universities were established in France, Italy, Spain and England by the early 13th century. Notable are University of Bologna, 1088, University of Salamanca, 1134, University of Paris, 1150, Oxford University, 1167 and University of Cambridge, 1207. The first German university, the Heidelberg University was not established until 1386. It was immediately steeped in mediaeval nominalism and early Protestantism.

Kings continued to attempt to control either the direct leadership of the church, or indirectly through political means for centuries. This is seen most clearly in the Avignon Papacy when the popes moved from Rome to Avignon. The conflict in Germany and northern Italy arguably left the culture ripe for various Protestant sects, such as the Cathars, the Waldensians and ultimately Jan Hus and Martin Luther.





</doc>
<doc id="6867" url="https://en.wikipedia.org/wiki?curid=6867" title="Context-free language">
Context-free language

In formal language theory, a context-free language (CFL) is a language generated by a context-free grammar (CFG).

Context-free languages have many applications in programming languages, in particular, most arithmetic expressions are generated by context-free grammars.

Different context-free grammars can generate the same context-free language. Intrinsic properties of the language can be distinguished from extrinsic properties of a particular grammar by comparing multiple grammars that describe the language.

The set of all context-free languages is identical to the set of languages accepted by pushdown automata, which makes these languages amenable to parsing. Further, for a given CFG, there is a direct way to produce a pushdown automaton for the grammar (and thereby the corresponding language), though going the other way (producing a grammar given an automaton) is not as direct.

A model context-free language is formula_1, the language of all non-empty even-length strings, the entire first halves of which are 's, and the entire second halves of which are 's. is generated by the grammar formula_2.
This language is not regular.
It is accepted by the pushdown automaton formula_3 where formula_4 is defined as follows:

Unambiguous CFLs are a proper subset of all CFLs: there are inherently ambiguous CFLs. An example of an inherently ambiguous CFL is the union of formula_6 with formula_7. This set is context-free, since the union of two context-free languages is always context-free. But there is no way to unambiguously parse strings in the (non-context-free) subset formula_8 which is the intersection of these two languages.

The language of all properly matched parentheses is generated by the grammar formula_9.

The context-free nature of the language makes it simple to parse with a pushdown automaton.

Determining an instance of the membership problem; i.e. given a string formula_10, determine whether formula_11 where formula_12 is the language generated by a given grammar formula_13; is also known as "recognition". Context-free recognition for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to boolean matrix multiplication, thus inheriting its complexity upper bound of "O"("n").
Conversely, Lillian Lee has shown "O"("n") boolean matrix multiplication to be reducible to "O"("n") CFG parsing, thus establishing some kind of lower bound for the latter.

Practical uses of context-free languages require also to produce a derivation tree that exhibits the structure that the grammar associates with the given string. The process of producing this tree is called "parsing". Known parsers have a time complexity that is cubic in the size of the string that is parsed.

Formally, the set of all context-free languages is identical to the set of languages accepted by pushdown automata (PDA). Parser algorithms for context-free languages include the CYK algorithm and Earley's Algorithm.

A special subclass of context-free languages are the deterministic context-free languages which are defined as the set of languages accepted by a deterministic pushdown automaton and can be parsed by a LR(k) parser.

See also parsing expression grammar as an alternative approach to grammar and parser.

The class of context-free languages is closed under the following operations. That is, if "L" and "P" are context-free languages, the following languages are context-free as well:

The context-free languages are not closed under intersection. This can be seen by taking the languages formula_22 and formula_23, which are both context-free. Their intersection is formula_24, which can be shown to be non-context-free by the pumping lemma for context-free languages. As a consequence, context-free languages cannot be closed under complementation, as for any languages "A" and "B", their intersection can be expressed by union and complement: formula_25. In particular, context-free language cannot be closed under difference, since complement can be expressed by difference: formula_26. 

However, if "L" is a context-free language and "D" is a regular language then both their intersection formula_27 and their difference formula_28 are context-free languages.

In formal language theory, questions about regular languages are usually decidable, but ones about context-free languages are often not. It is decidable whether such a language is finite, but not whether it contains every possible string, is regular, is unambiguous, or is equivalent to a language with a different grammar.

The following problems are undecidable for arbitrarily given context-free grammars A and B:

The following problems are "decidable" for arbitrary context-free languages:

According to Hopcroft, Motwani, Ullman (2003), 
many of the fundamental closure and (un)decidability properties of context-free languages were shown in the 1961 paper of Bar-Hillel, Perles, and Shamir

The set formula_8 is a context-sensitive language, but there does not exist a context-free grammar generating this language. So there exist context-sensitive languages which are not context-free. To prove that a given language is not context-free, one may employ the pumping lemma for context-free languages or a number of other methods, such as Ogden's lemma or Parikh's theorem.



</doc>
<doc id="6868" url="https://en.wikipedia.org/wiki?curid=6868" title="Caffeine">
Caffeine

Caffeine is a central nervous system (CNS) stimulant of the methylxanthine class. It is the world's most widely consumed psychoactive drug. Unlike many other psychoactive substances, it is legal and unregulated in nearly all parts of the world. There are several known mechanisms of action to explain the effects of caffeine. The most prominent is that it reversibly blocks the action of adenosine on its receptor and consequently prevents the onset of drowsiness induced by adenosine. Caffeine also stimulates certain portions of the autonomic nervous system.
Caffeine is a bitter, white crystalline purine, a methylxanthine alkaloid, and is chemically related to the adenine and guanine bases of deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). It is found in the seeds, nuts, or leaves of a number of plants native to Africa, East Asia and South America, and helps to protect them against predator insects and to prevent germination of nearby seeds. The most well-known source of caffeine is the coffee bean, a misnomer for the seed of "Coffea" plants. Beverages containing caffeine are ingested to relieve or prevent drowsiness and to improve performance. To make these drinks, caffeine is extracted by steeping the plant product in water, a process called infusion. Caffeine-containing drinks, such as coffee, tea, and cola, are very popular; as of 2014, 85% of American adults consumed some form of caffeine daily, consuming 164Â mg on average.
Caffeine can have both positive and negative health effects. It can treat and prevent the premature infant breathing disorders bronchopulmonary dysplasia of prematurity and apnea of prematurity. Caffeine citrate is on the WHO Model List of Essential Medicines. It may confer a modest protective effect against some diseases, including Parkinson's disease. Some people experience sleep disruption or anxiety if they consume caffeine, but others show little disturbance. Evidence of a risk during pregnancy is equivocal; some authorities recommend that pregnant women limit caffeine to the equivalent of two cups of coffee per day or less. Caffeine can produce a mild form of drug dependenceÂ â associated with withdrawal symptoms such as sleepiness, headache, and irritabilityÂ â when an individual stops using caffeine after repeated daily intake. Tolerance to the autonomic effects of increased blood pressure and heart rate, and increased urine output, develops with chronic use (i.e., these symptoms become less pronounced or do not occur following consistent use).
Caffeine is classified by the US Food and Drug Administration as generally recognized as safe (GRAS). Toxic doses, over 10 grams per day for an adult, are much higher than the typical dose of under 500 milligrams per day. A cup of coffee contains 80â175Â mg of caffeine, depending on what "bean" (seed) is used and how it is prepared (e.g., drip, percolation, or espresso). Thus it requires roughly 50â100 ordinary cups of coffee to reach the toxic dose. However, pure powdered caffeine, which is available as a dietary supplement, can be lethal in tablespoon-sized amounts.

Caffeine is used in:

Caffeine is a central nervous system stimulant that reduces fatigue and drowsiness. At normal doses, caffeine has variable effects on learning and memory, but it generally improves reaction time, wakefulness, concentration, and motor coordination. The amount of caffeine needed to produce these effects varies from person to person, depending on body size and degree of tolerance. The desired effects arise approximately one hour after consumption, and the desired effects of a moderate dose usually subside after about three or four hours.

Caffeine can delay or prevent sleep and improves task performance during sleep deprivation. Shift workers who use caffeine make fewer mistakes due to drowsiness.

A systematic review and meta-analysis from 2014 found that concurrent caffeine and -theanine use has synergistic psychoactive effects that promote alertness, attention, and task switching; these effects are most pronounced during the first hour post-dose.

Caffeine is a proven ergogenic aid in humans. Caffeine improves athletic performance in aerobic (especially endurance sports) and anaerobic conditions. Moderate doses of caffeine (around 5Â mg/kg) can improve sprint performance, cycling and running time trial performance, endurance (i.e., it delays the onset of muscle fatigue and central fatigue), and cycling power output. Caffeine increases basal metabolic rate in adults.

Caffeine improves muscular strength and power, and may enhance muscular endurance. Caffeine also enhances performance on anaerobic tests. Caffeine consumption before constant load exercise is associated with reduced perceived exertion. While this effect is not present during exercise-to-exhaustion exercise, performance is significantly enhanced. This is congruent with caffeine reducing perceived exertion, because exercise-to-exhaustion should end at the same point of fatigue. Caffeine also improves power output and reduces time to completion in aerobic time trials, an effect positively (but not exclusively) associated with longer duration exercise.

For the general population of healthy adults, Health Canada advises a daily intake of no more than 400Â mg. This limit was found to be safe by a 2017 systematic review on caffeine toxicology.

In healthy children, moderate caffeine intake under 400Â mg produces effects that are "modest and typically innocuous". Higher doses of caffeine (>400Â mg) can cause physiological, psychological and behavioral harm, particularly for children with psychiatric or cardiac conditions. There is no evidence that coffee stunts a child's growth. For children age 12 and under, Health Canada recommends a maximum daily caffeine intake of no more than 2.5 milligrams per kilogram of body weight. Based on average body weights of children, this translates to the following age-based intake limits: The American Society of Pediatrics recommends that caffeine consumption is not appropriate for children and adolescents and should be avoided. This recommendation is based on a clinical report released by American Society of Pediatrics in 2011 with a review of 45 publications from 1994 to 2011 and includes inputs from various stakeholders (Pediatricians, Committee on nutrition, Canadian Pediatric Society, Center for Disease Control & Prevention, FDA, Sports Medicine & Fitness committee, National Federations of High School Associations).
Health Canada has not developed advice for adolescents because of insufficient data. However, they suggest that daily caffeine intake for this age group be no more than 2.5Â mg/kg body weight. This is because the maximum adult caffeine dose may not be appropriate for light-weight adolescents or for younger adolescents who are still growing. The daily dose of 2.5Â mg/kg body weight would not cause adverse health effects in the majority of adolescent caffeine consumers. This is a conservative suggestion since older and heavier weight adolescents may be able to consume adult doses of caffeine without suffering adverse effects.

Current evidence regarding the effects of caffeine on pregnancy and for breastfeeding are inconclusive. There is limited primary and secondary advice for, or against, caffeine use during pregnancy and its effects on the fetus or newborn.

The UK Food Standards Agency has recommended that pregnant women should limit their caffeine intake, out of prudence, to less than 200Â mg of caffeine a dayÂ â the equivalent of two cups of instant coffee, or one and a half to two cups of fresh coffee. The American Congress of Obstetricians and Gynecologists (ACOG) concluded in 2010 that caffeine consumption is safe up to 200Â mg per day in pregnant women. For women who breastfeed, are pregnant, or may become pregnant, Health Canada recommends a maximum daily caffeine intake of no more than 300Â mg, or a little over two 8Â oz (237Â mL) cups of coffee. A 2017 systematic review on caffeine toxicology found evidence supporting that caffeine consumption up to 300Â mg/day for pregnant women is generally not associated with adverse reproductive or developmental effect.

There are conflicting reports in the scientific literature about caffeine use during pregnancy. A 2011 review found that caffeine during pregnancy does not appear to increase the risk of congenital malformations, miscarriage or growth retardation even when consumed in moderate to high amounts. Other reviews, however, concluded that there is some evidence that higher caffeine intake by pregnant women may be associated with a higher risk of giving birth to a low birth weight baby, and may be associated with a higher risk of pregnancy loss. A systematic review, analyzing the results of observational studies, suggests that women who consume large amounts of caffeine (greater than 300Â mg/day) prior to becoming pregnant may have a higher risk of experiencing pregnancy loss.

Coffee and caffeine can affect gastrointestinal motility and gastric acid secretion. Caffeine in low doses may cause weak bronchodilation for up to four hours in asthmatics. In postmenopausal women, high caffeine consumption can accelerate bone loss.

Doses of caffeine equivalent to the amount normally found in standard servings of tea, coffee and carbonated soft drinks appear to have no diuretic action. However, acute ingestion of caffeine in large doses (at least 250â300Â mg, equivalent to the amount found in 2â3 cups of coffee or 5â8 cups of tea) results in a short-term stimulation of urine output in individuals who have been deprived of caffeine for a period of days or weeks. This increase is due to both a diuresis (increase in water excretion) and a natriuresis (increase in saline excretion); it is mediated via proximal tubular adenosine receptor blockade. The acute increase in urinary output may increase the risk of dehydration. However, chronic users of caffeine develop a tolerance to this effect and experience no increase in urinary output.

Minor undesired symptoms from caffeine ingestion not sufficiently severe to warrant a psychiatric diagnosis are common and include mild anxiety, jitteriness, insomnia, increased sleep latency, and reduced coordination. Caffeine can have negative effects on anxiety disorders. According to a 2011 literature review, caffeine use is positively associated with anxiety and panic disorders. At high doses, typically greater than 300Â mg, caffeine can both cause and worsen anxiety. For some people, discontinuing caffeine use can significantly reduce anxiety. In moderate doses, caffeine has been associated with reduced symptoms of depression and lower suicide risk.

Increased consumption of coffee and caffeine is associated with a decreased risk of depression.

Some textbooks state that caffeine is a mild euphoriant, others state that it is not a euphoriant, and one states that it is and is not a euphoriant.

Caffeine-induced anxiety disorder is a subclass of the DSM-5 diagnosis of substance/medication-induced anxiety disorder.

Whether caffeine can result in an addictive disorder depends on how addiction is defined. Compulsive caffeine consumption under any circumstances has not been observed, and caffeine is therefore not generally considered addictive. However, some diagnostic models, such as the and ICD-10, include a classification of caffeine addiction under a broader diagnostic model. Some state that certain users can become addicted and therefore unable to decrease use even though they know there are negative health effects.

Caffeine does not appear to be a reinforcing stimulus, and some degree of aversion may actually occur, with people preferring placebo over caffeine in a study on drug abuse liability published in an NIDA research monograph. Some state that research does not provide support for an underlying biochemical mechanism for caffeine addiction. Other research states it can affect the reward system.

"Caffeine addiction" was added to the ICDM-9 and ICD-10. However, its addition was contested with claims that this diagnostic model of caffeine addiction is not supported by evidence. The American Psychiatric Association's does not include the diagnosis of a "caffeine addiction" but proposes criteria for the disorder for more study.

Withdrawal can cause mild to clinically significant distress or impairment in daily functioning. The frequency at which this occurs is self-reported at 11%, but in lab tests only half of the people who report withdrawal actually experience it, casting doubt on many claims of dependence. Mild physical dependence and withdrawal symptoms may occur upon abstinence, with greater than 100Â mg caffeine per day, although these symptoms last no longer than a day. Some symptoms associated with psychological dependence may also occur during withdrawal. The diagnostic criteria for caffeine withdrawal require a previous prolonged daily use of caffeine. Following 24 hours of a marked reduction in consumption, a minimum of 3 of these signs or symptoms is required to meet withdrawal criteria: difficulty concentrating, depressed mood/irritability, flu-like symptoms, headache, and fatigue. Additionally, the signs and symptoms must disrupt important areas of functioning and are not associated with effects of another condition

The ICD-11 includes caffeine dependence as a distinct diagnostic category, which closely mirrors the DSM-5âs proposed set of criteria for âcaffeine-use disorderâ.Â  Caffeine use disorder refers to dependence on caffeine characterized by failure to control caffeine consumption despite negative physiological consequences. The APA, which published the DSM-5, acknowledged that there was sufficient evidence in order to create a diagnostic model of caffeine dependence for the DSM-5, but they noted that the clinical significance of the disorder is unclear. Due to this inconclusive evidence on clinical significance, the DSM-5 classifies caffeine-use disorder as a âcondition for further studyâ.

Tolerance to the effects of caffeine occurs for caffeine induced elevations in blood pressure and the subjective feelings of nervousness. Sensitization, the process whereby effects become more prominent with use, occurs for positive effects such as feelings of alertness and well being. Tolerance varies for daily, regular caffeine users and high caffeine users. High doses of caffeine (750 to 1200Â mg/day spread throughout the day) have been shown to produce complete tolerance to some, but not all of the effects of caffeine. Doses as low as 100Â mg/day, such as a 6 oz cup of coffee or two to three 12 oz servings of caffeinated soft-drink, may continue to cause sleep disruption, among other intolerances. Non-regular caffeine users have the least caffeine tolerance for sleep disruption. Some coffee drinkers develop tolerance to its undesired sleep-disrupting effects, but others apparently do not.

A protective effect of caffeine against Alzheimer's disease and dementia is possible but the evidence is inconclusive. It may protect people from liver cirrhosis. Caffeine may lessen the severity of acute mountain sickness if taken a few hours prior to attaining a high altitude. One meta analysis has found that caffeine consumption is associated with a reduced risk of type 2 diabetes. Two meta analyses have reported that caffeine consumption is associated with a linear reduction in risk for Parkinson's disease. Caffeine consumption may be associated with reduced risk of depression, although conflicting results have been reported.

Caffeine increases intraocular pressure in those with glaucoma but does not appear to affect normal individuals.

The DSM-5 also includes other caffeine-induced disorders consisting of caffeine-induced anxiety disorder, caffeine-induced sleep disorder and unspecified caffeine-related disorders. The first two disorders are classified under âAnxiety Disorderâ and âSleep-Wake Disorderâ because they share similar characteristics. Other disorders that present with significant distress and impairment of daily functioning that warrant clinical attention but do not meet the criteria to be diagnosed under any specific disorders are listed under âUnspecified Caffeine-Related Disordersâ.

Consumption of per day is associated with a condition known as "<dfn>caffeinism</dfn>." Caffeinism usually combines caffeine dependency with a wide range of unpleasant symptoms including nervousness, irritability, restlessness, insomnia, headaches, and palpitations after caffeine use.

Caffeine overdose can result in a state of central nervous system over-stimulation known as caffeine intoxication, a clinically significant temporary condition that develops during, or shortly after, the consumption of caffeine. This syndrome typically occurs only after ingestion of large amounts of caffeine, well over the amounts found in typical caffeinated beverages and caffeine tablets (e.g., more than 400â500Â mg at a time). According to the DSM-5, caffeine intoxication may be diagnosed if five (or more) of the following symptoms develop after recent consumption of caffeine: restlessness, nervousness, excitement, insomnia, flushed face, diuresis (increased production of urine), gastrointestinal disturbance, muscle twitching, rambling flow of thought and speech, tachycardia (increased heart rate) or cardiac arrythmia, periods of inexhaustibility, and psychomotor agitation.

According to the International Classification of Diseases (ICD-11), cases of very high doses of caffeine (e.g. > 5 g) may result in caffeine intoxication with symptoms including mania, depression, lapses in judgement, disorientation, disinhibition, delusions, hallucinations or psychosis, and rhabdomyolysis (breakdown of skeletal muscle tissue) can be provoked.

Death from caffeine ingestion appears to be rare, and most commonly caused by an intentional overdose of medications. In 2016, 3702 caffeine related exposure were reported to Poison Control Centers in the United States, of which 846 required an hospitalization and 16 with a major outcome, and several caffeine-related deaths are reported in case studies. The LD of caffeine in humans is dependent on individual sensitivity, but is estimated to be 150â200 milligrams per kilogram (2.2Â lb) of body mass (75â100 cups of coffee for a adult). There are cases where doses as low as 57 milligrams per kilogram being fatal. A number of fatalities have been caused by overdoses of readily available powdered caffeine supplements, for which the estimated lethal amount is less than a tablespoon. The lethal dose is lower in individuals whose ability to metabolize caffeine is impaired due to genetics or chronic liver disease. A death was reported in a man with liver cirrhosis who overdosed on caffeinated mints.

Since there is no antidote nor reversal agent for caffeine intoxication, treatment of mild caffeine intoxication is directed toward symptom relief; severe intoxication may require peritoneal dialysis, hemodialysis, or hemofiltration.

Caffeine is a substrate for CYP1A2, and interacts with many substances through this and other mechanisms.

According to DSST, alcohol provides a reduction in performance and caffeine has a significant improvement in performance. When alcohol and caffeine are consumed jointly, the effects produced by caffeine are affected, but the alcohol effects remain the same. For example, when additional caffeine is added, the drug effect produced by alcohol is not reduced. However, the jitteriness and alertness given by caffeine is decreased when additional alcohol is consumed. Alcohol consumption alone reduces both inhibitory and activational aspects of behavioral control. Caffeine antagonizes the activational aspect of behavioral control, but has no effect on the inhibitory behavioral control. The Dietary Guidelines for Americans recommend avoidance of concomitant consumption of alcohol and caffeine, as this may lead to increased alcohol consumption, with a higher risk of alcohol-associated injury.

Smoking tobacco increases caffeine clearance by 56%.

Birth control pills can extend the half-life of caffeine, requiring greater attention to caffeine consumption.

Caffeine sometimes increases the effectiveness of some medications, such as those for headaches. Caffeine was determined to increase the potency of some over-the-counter analgesic medications by 40%.

The pharmacological effects of adenosine may be blunted in individuals taking large quantities of methylxanthines like caffeine.

In the absence of caffeine and when a person is awake and alert, little adenosine is present in (CNS) neurons. With a continued wakeful state, over time adenosine accumulates in the neuronal synapse, in turn binding to and activating adenosine receptors found on certain CNS neurons; when activated, these receptors produce a cellular response that ultimately increases drowsiness. When caffeine is consumed, it antagonizes adenosine receptors; in other words, caffeine prevents adenosine from activating the receptor by blocking the location on the receptor where adenosine binds to it. As a result, caffeine temporarily prevents or relieves drowsiness, and thus maintains or restores alertness.

Caffeine is an antagonist at all four adenosine receptor subtypes (A, A, A, and A), although with varying potencies. The affinity (K) values of caffeine for the human adenosine receptors are 12Â Î¼M at A, 2.4Â Î¼M at A, 13Â Î¼M at A, and 80Â Î¼M at A. Knockout mouse studies have specifically implicated antagonism of the A receptor as responsible for the wakefulness-promoting effects of caffeine. Antagonism of adenosine receptors by caffeine stimulates the medullary vagal, vasomotor, and respiratory centers, which increases respiratory rate, reduces heart rate, and constricts blood vessels. Adenosine receptor antagonism also promotes neurotransmitter release (e.g., monoamines and acetylcholine), which endows caffeine with its stimulant effects; adenosine acts as an inhibitory neurotransmitter that suppresses activity in the central nervous system. Heart palpitations are caused by blockade of the A receptor.

Because caffeine is both water- and lipid-soluble, it readily crosses the bloodâbrain barrier that separates the bloodstream from the interior of the brain. Once in the brain, the principal mode of action is as a nonselective antagonist of adenosine receptors (in other words, an agent that reduces the effects of adenosine). The caffeine molecule is structurally similar to adenosine, and is capable of binding to adenosine receptors on the surface of cells without activating them, thereby acting as a competitive antagonist.

In addition to its activity at adenosine receptors, caffeine is an inositol trisphosphate receptor 1 antagonist and a voltage-independent activator of the ryanodine receptors (RYR1, RYR2, and RYR3). It is also a competitive antagonist of the ionotropic glycine receptor.

While caffeine does not directly bind to any dopamine receptors, it influences the binding activity of dopamine at its receptors in the striatum by binding to adenosine receptors that have formed GPCR heteromers with dopamine receptors, specifically the AâD receptor heterodimer (this is a receptor complex with 1 adenosine A receptor and 1 dopamine D receptor) and the AâD receptor heterotetramer (this is a receptor complex with 2 adenosine A receptors and 2 dopamine D receptors). The AâD receptor heterotetramer has been identified as a primary pharmacological target of caffeine, primarily because it mediates some of its psychostimulant effects and its pharmacodynamic interactions with dopaminergic psychostimulants.

Caffeine also causes the release of dopamine in the dorsal striatum and nucleus accumbens core (a substructure within the ventral striatum), but not the nucleus accumbens shell, by antagonizing A receptors in the axon terminal of dopamine neurons and AâA heterodimers (a receptor complex composed of 1 adenosine A receptor and 1 adenosine A receptor) in the axon terminal of glutamate neurons. During chronic caffeine use, caffeine-induced dopamine release within the nucleus accumbens core is markedly reduced due to drug tolerance.

Caffeine, like other xanthines, also acts as a phosphodiesterase inhibitor. As a competitive nonselective phosphodiesterase inhibitor, caffeine raises intracellular cAMP, activates protein kinase A, inhibits TNF-alpha and leukotriene synthesis, and reduces inflammation and innate immunity. Caffeine also affects the cholinergic system where it inhibits the enzyme acetylcholinesterase.

Caffeine antagonizes adenosine A2A receptors in the ventrolateral preoptic area (VLPO), thereby reducing inhibitory GABA neurotransmission to the tuberomammillary nucleus, a histaminergic projection nucleus that activation-dependently promotes arousal. Disinhibition of the tuberomammillary nucleus is the chief mechanism by which caffeine produces wakefulness-promoting effects.

Caffeine from coffee or other beverages is absorbed by the small intestine within 45 minutes of ingestion and distributed throughout all bodily tissues. Peak blood concentration is reached within 1â2Â hours. It is eliminated by first-order kinetics. Caffeine can also be absorbed rectally, evidenced by suppositories of ergotamine tartrate and caffeine (for the relief of migraine) and chlorobutanol and caffeine (for the treatment of hyperemesis). However, rectal absorption is less efficient than oral: the maximum concentration (C) and total amount absorbed (AUC) are both about 30% (i.e., 1/3.5) of the oral amounts.

Caffeine's biological half-lifeÂ â the time required for the body to eliminate one-half of a doseÂ â varies widely among individuals according to factors such as pregnancy, other drugs, liver enzyme function level (needed for caffeine metabolism) and age. In healthy adults, caffeine's half-life is between 3 and 7Â hours. Smoking decreases the half-life by 30â50%, while oral contraceptives can double it and pregnancy can raise it to as much as 15Â hours during the third trimester. In newborns the half-life can be 80Â hours or more, dropping very rapidly with age, possibly to less than the adult value by age 6 months. The antidepressant fluvoxamine (Luvox) reduces the clearance of caffeine by more than 90%, and increases its elimination half-life more than tenfold; from 4.9Â hours to 56Â hours.

Caffeine is metabolized in the liver by the cytochrome P450 oxidase enzyme system, in particular, by the CYP1A2 isozyme, into three dimethylxanthines, each of which has its own effects on the body:

1,3,7-Trimethyluric acid is a minor caffeine metabolite. Each of these metabolites is further metabolized and then excreted in the urine. Caffeine can accumulate in individuals with severe liver disease, increasing its half-life.

A 2011 review found that increased caffeine intake was associated with a variation in two genes that increase the rate of caffeine catabolism. Subjects who had this mutation on both chromosomes consumed 40Â mg more caffeine per day than others. This is presumably due to the need for a higher intake to achieve a comparable desired effect, not that the gene led to a disposition for greater incentive of habituation.

Pure anhydrous caffeine is a bitter-tasting, white, odorless powder with a melting point of 235â238Â Â°C. Caffeine is moderately soluble in water at room temperature (2Â g/100 mL), but very soluble in boiling water (66Â g/100 mL). It is also moderately soluble in ethanol (1.5Â g/100 mL). It is weakly basic (pK of conjugate acid = ~0.6) requiring strong acid to protonate it. Caffeine does not contain any stereogenic centers and hence is classified as an achiral molecule.

The xanthine core of caffeine contains two fused rings, a pyrimidinedione and imidazole. The pyrimidinedione in turn contains two amide functional groups that exist predominantly in a zwitterionic resonance the location from which the nitrogen atoms are double bonded to their adjacent amide carbons atoms. Hence all six of the atoms within the pyrimidinedione ring system are sp hybridized and planar. Therefore, the fused 5,6 ring core of caffeine contains a total of ten pi electrons and hence according to HÃ¼ckel's rule is aromatic.

The biosynthesis of caffeine is an example of convergent evolution among different species.

Caffeine may be synthesized in the lab starting with dimethylurea and malonic acid.

Commercial supplies of caffeine are not usually manufactured synthetically because the chemical is readily available as a byproduct of decaffeination.

Extraction of caffeine from coffee, to produce caffeine and decaffeinated coffee, can be performed using a number of solvents. Benzene, chloroform, trichloroethylene, and dichloromethane have all been used over the years but for reasons of safety, environmental impact, cost, and flavor, they have been superseded by the following main methods:

"Decaffeinated" coffees do in fact contain caffeine in many casesÂ â some commercially available decaffeinated coffee products contain considerable levels. One study found that decaffeinated coffee contained 10Â mg of caffeine per cup, compared to approximately 85Â mg of caffeine per cup for regular coffee.

Caffeine can be quantified in blood, plasma, or serum to monitor therapy in neonates, confirm a diagnosis of poisoning, or facilitate a medicolegal death investigation. Plasma caffeine levels are usually in the range of 2â10Â mg/L in coffee drinkers, 12â36Â mg/L in neonates receiving treatment for apnea, and 40â400Â mg/L in victims of acute overdosage. Urinary caffeine concentration is frequently measured in competitive sports programs, for which a level in excess of 15Â mg/L is usually considered to represent abuse.

Some analog substances have been created which mimic caffeine's properties with either function or structure or both. Of the latter group are the xanthines DMPX and 8-chlorotheophylline, which is an ingredient in dramamine. Members of a class of nitrogen substituted xanthines are often proposed as potential alternatives to caffeine. Many other xanthine analogues constituting the adenosine receptor antagonist class have also been elucidated.

Some other caffeine analogs:

Caffeine, as do other alkaloids such as cinchonine, quinine or strychnine, precipitates polyphenols and tannins. This property can be used in a quantitation method.

Around sixty plant species are known to contain caffeine. Common sources are the "beans" (seeds) of the two cultivated coffee plants, "Coffea arabica" and "Coffea canephora" (the quantity varies, but 1.3% is a typical value); in the leaves of the tea plant; and in kola nuts. Other sources include yaupon holly leaves, South American holly yerba mate leaves, seeds from Amazonian maple guarana berries, and Amazonian holly guayusa leaves. Temperate climates around the world have produced unrelated caffeine-containing plants.

Caffeine in plants acts as a natural pesticide: it can paralyze and kill predator insects feeding on the plant. High caffeine levels are found in coffee seedlings when they are developing foliage and lack mechanical protection. In addition, high caffeine levels are found in the surrounding soil of coffee seedlings, which inhibits seed germination of nearby coffee seedlings, thus giving seedlings with the highest caffeine levels fewer competitors for existing resources for survival. Caffeine is stored in tea leaves in two places. Firstly, in the cell vacuoles where it is complexed with polyphenols. This caffeine probably is released into the mouth parts of insects, to discourage herbivory. Secondly, around the vascular bundles, where it probably inhibits pathogenic fungi from entering and colonizing the vascular bundles. Caffeine in nectar may improve the reproductive success of the pollen producing plants by enhancing the reward memory of pollinators such as honey bees.

The differing perceptions in the effects of ingesting beverages made from various plants containing caffeine could be explained by the fact that these beverages also contain varying mixtures of other methylxanthine alkaloids, including the cardiac stimulants theophylline and theobromine, and polyphenols that can form insoluble complexes with caffeine.

Products containing caffeine are coffee, tea, soft drinks ("colas"), energy drinks, other beverages, chocolate, caffeine tablets, other oral products, and inhalation products.

The world's primary source of caffeine is the coffee "bean" (the seed of the coffee plant), from which coffee is brewed. Caffeine content in coffee varies widely depending on the type of coffee bean and the method of preparation used; even beans within a given bush can show variations in concentration. In general, one serving of coffee ranges from 80 to 100 milligrams, for a single shot (30 milliliters) of arabica-variety espresso, to approximately 100â125 milligrams for a cup (120 milliliters) of drip coffee. "Arabica" coffee typically contains half the caffeine of the "robusta" variety.
In general, dark-roast coffee has very slightly less caffeine than lighter roasts because the roasting process reduces caffeine content of the bean by a small amount.

Tea contains more caffeine than coffee by dry weight. A typical serving, however, contains much less, since less of the product is used as compared to an equivalent serving of coffee. Also contributing to caffeine content are growing conditions, processing techniques, and other variables. Thus, teas contain varying amounts of caffeine.

Tea contains small amounts of theobromine and slightly higher levels of theophylline than coffee. Preparation and many other factors have a significant impact on tea, and color is a very poor indicator of caffeine content. Teas like the pale Japanese green tea, "gyokuro", for example, contain far more caffeine than much darker teas like "lapsang souchong", which has very little.

Caffeine is also a common ingredient of soft drinks, such as cola, originally prepared from kola nuts. Soft drinks typically contain 0 to 55 milligrams of caffeine per 12 ounce serving. By contrast, energy drinks, such as Red Bull, can start at 80 milligrams of caffeine per serving. The caffeine in these drinks either originates from the ingredients used or is an additive derived from the product of decaffeination or from chemical synthesis. Guarana, a prime ingredient of energy drinks, contains large amounts of caffeine with small amounts of theobromine and theophylline in a naturally occurring slow-release excipient.


Chocolate derived from cocoa beans contains a small amount of caffeine. The weak stimulant effect of chocolate may be due to a combination of theobromine and theophylline, as well as caffeine. A typical 28-gram serving of a milk chocolate bar has about as much caffeine as a cup of decaffeinated coffee. By weight, dark chocolate has one to two times the amount of caffeine as coffee: 80â160Â mg per 100Â g. Higher percentages of cocoa such as 90% amount to 200Â mg per 100Â g approximately and thus, a 100-gram 85% cocoa chocolate bar contains about 195Â mg caffeine.

Tablets offer several advantages over coffee, tea, and other caffeinated beverages, including convenience, known dosage, and avoidance of concomitant intake of sugar, acids, and fluids. Manufacturers of caffeine tablets claim that using caffeine of pharmaceutical quality improves mental alertness. These tablets are commonly used by students studying for their exams and by people who work or drive for long hours.

One U.S. company is marketing oral dissolvable caffeine strips. Another intake route is SpazzStick, a caffeinated lip balm. Alert Energy Caffeine Gum was introduced in the United States in 2013, but was voluntarily withdrawn after an announcement of an investigation by the FDA of the health effects of added caffeine in foods.

There are several products being marketed that offer inhalers that deliver proprietary blends of supplements, with caffeine being a key ingredient. In 2012, the FDA sent a warning letter to one of the companies marketing these inhalers, expressing concerns for the lack of safety information available about inhaled caffeine.


According to Chinese legend, the Chinese emperor Shennong, reputed to have reigned in about 3000 BCE, inadvertently discovered tea when he noted that when certain leaves fell into boiling water, a fragrant and restorative drink resulted. Shennong is also mentioned in Lu Yu's "Cha Jing", a famous early work on the subject of tea.

The earliest credible evidence of either coffee drinking or knowledge of the coffee plant appears in the middle of the fifteenth century, in the Sufi monasteries of the Yemen in southern Arabia. From Mocha, coffee spread to Egypt and North Africa, and by the 16th century, it had reached the rest of the Middle East, Persia and Turkey. From the Middle East, coffee drinking spread to Italy, then to the rest of Europe, and coffee plants were transported by the Dutch to the East Indies and to the Americas.

Kola nut use appears to have ancient origins. It is chewed in many West African cultures, in both private and social settings, to restore vitality and ease hunger pangs.

The earliest evidence of cocoa bean use comes from residue found in an ancient Mayan pot dated to 600 BCE. Also, chocolate was consumed in a bitter and spicy drink called "xocolatl", often seasoned with vanilla, chile pepper, and achiote. "Xocolatl" was believed to fight fatigue, a belief probably attributable to the theobromine and caffeine content. Chocolate was an important luxury good throughout pre-Columbian Mesoamerica, and cocoa beans were often used as currency.

"Xocolatl" was introduced to Europe by the Spaniards, and became a popular beverage by 1700. The Spaniards also introduced the cacao tree into the West Indies and the Philippines. It was used in alchemical processes, where it was known as "black bean".

The leaves and stems of the yaupon holly ("Ilex vomitoria") were used by Native Americans to brew a tea called "asi" or the "black drink". Archaeologists have found evidence of this use far into antiquity, possibly dating to Late Archaic times.

In 1819, the German chemist Friedlieb Ferdinand Runge isolated relatively pure caffeine for the first time; he called it ""Kaffebase"" (i.e., a base that exists in coffee). According to Runge, he did this at the behest of Johann Wolfgang von Goethe. In 1821, caffeine was isolated both by the French chemist Pierre Jean Robiquet and by another pair of French chemists, Pierre-Joseph Pelletier and Joseph BienaimÃ© Caventou, according to Swedish chemist JÃ¶ns Jacob Berzelius in his yearly journal. Furthermore, Berzelius stated that the French chemists had made their discoveries independently of any knowledge of Runge's or each other's work. However, Berzelius later acknowledged Runge's priority in the extraction of caffeine, stating: "However, at this point, it should not remain unmentioned that Runge (in his "Phytochemical Discoveries", 1820, pages 146â147) specified the same method and described caffeine under the name "Caffeebase" a year earlier than Robiquet, to whom the discovery of this substance is usually attributed, having made the first oral announcement about it at a meeting of the Pharmacy Society in Paris."

Pelletier's article on caffeine was the first to use the term in print (in the French form "CafÃ©ine" from the French word for coffee: "cafÃ©"). It corroborates Berzelius's account:

Robiquet was one of the first to isolate and describe the properties of pure caffeine, whereas Pelletier was the first to perform an elemental analysis.

In 1827, M. Oudry isolated "thÃ©ine" from tea, but in 1838 it was proved by Mulder and by Carl Jobst that theine was actually the same as caffeine.

In 1895, German chemist Hermann Emil Fischer (1852â1919) first synthesized caffeine from its chemical components (i.e. a "total synthesis"), and two years later, he also derived the structural formula of the compound. This was part of the work for which Fischer was awarded the Nobel Prize in 1902.

Because it was recognized that coffee contained some compound that acted as a stimulant, first coffee and later also caffeine has sometimes been subject to regulation. For example, in the 16th century Islamists in Mecca and in the Ottoman Empire made coffee illegal for some classes. Charles II of England tried to ban it in 1676, Frederick II of Prussia banned it in 1777, and coffee was banned in Sweden at various times between 1756 and 1823.

In 1911, caffeine became the focus of one of the earliest documented health scares, when the US government seized 40 barrels and 20 kegs of Coca-Cola syrup in Chattanooga, Tennessee, alleging the caffeine in its drink was "injurious to health". Although the judge ruled in favor of Coca-Cola, two bills were introduced to the U.S. House of Representatives in 1912 to amend the Pure Food and Drug Act, adding caffeine to the list of "habit-forming" and "deleterious" substances, which must be listed on a product's label.

The Food and Drug Administration (FDA) in the United States currently allows only beverages containing less than 0.02% caffeine; but caffeine powder, which is sold as a dietary supplement, is unregulated. It is a regulatory requirement that the label of most prepackaged foods must declare a list of ingredients, including food additives such as caffeine, in descending order of proportion. However, there is no regulatory provision for mandatory quantitative labeling of caffeine, (e.g., milligrams caffeine per stated serving size). There are a number of food ingredients that naturally contain caffeine. These ingredients must appear in food ingredient lists. However, as is the case for "food additive caffeine", there is no requirement to identify the quantitative amount of caffeine in composite foods containing ingredients that are natural sources of caffeine. While coffee or chocolate are broadly recognized as caffeine sources, some ingredients (e.g., guarana, yerba matÃ©) are likely less recognized as caffeine sources. For these natural sources of caffeine, there is no regulatory provision requiring that a food label identify the presence of caffeine nor state the amount of caffeine present in the food.

Global consumption of caffeine has been estimated at 120,000Â tonnes per year, making it the world's most popular psychoactive substance. This amounts to one serving of a caffeinated beverage for every person every day. The consumption of caffeine has remained stable between 1997 and 2015. Coffee, tea and soft drinks are the most important caffeine sources, with energy drinks contributing little to the total caffeine intake across all age groups.

Some Church of God (Restoration) adherents and Christian Scientists do not consume caffeine. Until recently, the Seventh-day Adventist Church asked for its members to "abstain from caffeinated drinks", but has removed this from baptismal vows (while still recommending abstention as policy). Some from these religions believe that one is not supposed to consume a non-medical, psychoactive substance, or believe that one is not supposed to consume a substance that is addictive. The Church of Jesus Christ of Latter-day Saints has said the following with regard to caffeinated beverages: " . . . the Church revelation spelling out health practices (Doctrine and Covenants 89) does not mention the use of caffeine. The Church's health guidelines prohibit alcoholic drinks, smoking or chewing of tobacco, and 'hot drinks' â taught by Church leaders to refer specifically to tea and coffee."

Gaudiya Vaishnavas generally also abstain from caffeine, because they believe it clouds the mind and over-stimulates the senses. To be initiated under a guru, one must have had no caffeine, alcohol, nicotine or other drugs, for at least a year.

Caffeinated beverages are widely consumed by Muslims today. In the 16th century, some Muslim authorities made unsuccessful attempts to ban them as forbidden "intoxicating beverages" under Islamic dietary laws.

Recently discovered bacteria "Pseudomonas putida" CBB5 can live on pure caffeine and can cleave caffeine into carbon dioxide and ammonia.

Caffeine is toxic to birds and to dogs and cats, and has a pronounced adverse effect on mollusks, various insects, and spiders. This is at least partly due to a poor ability to metabolize the compound, causing higher levels for a given dose per unit weight. Caffeine has also been found to enhance the reward memory of honey bees.

Caffeine has been used to double chromosomes in haploid wheat.





</doc>
<doc id="6874" url="https://en.wikipedia.org/wiki?curid=6874" title="Cyc">
Cyc

Cyc (pronounced , ) is a long-living artificial intelligence project that aims to assemble a comprehensive ontology and knowledge base that spans the basic concepts and rules about how the world works. Hoping to capture common sense knowledge, Cyc focuses on implicit knowledge that other AI platforms may take for granted. This is contrasted with facts one might find somewhere on the internet or retrieve via a search engine or Wikipedia. Cyc enables AI applications to perform human-like reasoning and be less "brittle" when confronted with novel situations.

Douglas Lenat began the project in July 1984 at MCC, where he was Principal Scientist 1984â1994, and then, since January 1995, has been under active development by the Cycorp company, where he is the CEO.

The need for a massive symbolic artificial intelligence project of this kind was born in the early 1980s. Early AI researchers had ample experience over the previous 25 years with AI programs that would generate encouraging early results but then fail to "scale up"âmove beyond the 'training set' to tackle a broader range of cases. Douglas Lenat and Alan Kay publicized this need, and they organized a meeting at Stanford in 1983 to address the problem. The back-of-the-envelope calculations by Doug, Alan, and their colleagues (including Marvin Minsky, Allen Newell, Edward Feigenbaum, and John McCarthy) indicated that that effort would require between 1000 and 3000 person-years of effort, far beyond the standard academic project model. However, events within a year of that meeting enabled an effort of that scale to get underway.

The project began in July 1984 as the flagship project of the 400-person Microelectronics and Computer Technology Corporation (MCC), a research consortium started by two dozen large United States based corporations "to counter a then ominous Japanese effort in AI, the so-called "fifth-generation" project." The US Government reacted to the Fifth Generation threat by passing the National Cooperative Research Act of 1984, which for the first time allowed US companies to "collude" on long-term high-risk high-payoff research, and MCC and Sematech sprang up to take advantage of that ten-year opportunity. MCC's first President and CEO was Bobby Ray Inman, former NSA Director and Central Intelligence Agency deputy director.

The objective of the Cyc project was to codify, in machine-usable form, the millions of pieces of knowledge that compose human common sense. This entailed, along the way, (1) developing an adequately expressive representation language, CycL, (2) developing an ontology spanning all human concepts down to some appropriate level of detail, (3) developing a knowledge base on that ontological framework, comprising all human knowledge about those concepts down to some appropriate level of detail, and (4) developing an inference engine exponentially faster than those used in then-conventional expert systems, to be able to infer the same types and depth of conclusions that humans are capable of, given their knowledge of the world.

In slightly more detail:

CycL has a publicly released specification and dozens of HL modules were described in Lenat and Guha's textbook, but the actual Cyc inference engine code, and the full list of 1000+ HL modules, is Cycorp-proprietary.

The name "Cyc" (from "encyclopedia", pronounced , like ""syke"") is a registered trademark owned by Cycorp. Access to Cyc is through paid licenses, but "bona fide" AI research groups are given research-only no-cost licenses (cf. ResearchCyc); as of 2017, over 600 such groups worldwide have these licenses.

Typical pieces of knowledge represented in the Cyc knowledge base are "Every tree is a plant" and "Plants die eventually". When asked whether trees die, the inference engine can draw the obvious conclusion and answer the question correctly.

Most of Cyc's knowledge, outside math, is only true by default. For example, Cyc knows that "as a default" parents love their children, when you're made happy you smile, taking your first step is a big accomplishment, when someone you love has a big accomplishment that makes you happy, and only adults have children. When asked whether a picture captioned "Someone watching his daughter take her first step" contains a smiling adult person, Cyc can logically infer that the answer is "Yes", and "show its work" by presenting the step by step logical argument using those five pieces of knowledge from its knowledge base. These are formulated in the language CycL, which is based on predicate calculus and has a syntax similar to that of the Lisp programming language.

In 2008, Cyc resources were mapped to many Wikipedia articles. Cyc is presently connected to Wikidata. Future plans may connect Cyc to both DBpedia and Freebase.

Much of the current work Cyc continues to be knowledge engineering, representing facts about the world by hand, and implementing efficient inference mechanisms on that knowledge. Increasingly, however, work at Cycorp involves giving the Cyc system the ability to communicate with end users in natural language, and to assist with the ongoing knowledge formation process via machine learning and natural language understanding. Another large effort at Cycorp is building a suite of Cyc-powered ontological engineering tools to lower the bar to entry for individuals to contribute to, edit, browse, and query Cyc.

Like many companies, Cycorp has ambitions to use Cyc's natural language processing to parse the entire internet to extract structured data; unlike all others, it is able to call on the Cyc system itself to act as an inductive bias and as an adjudicator of ambiguity, metaphor, and ellipsis.

The concept names in Cyc are CycL "terms" or "constants". Constants start with an optional "#$" and are case-sensitive. There are constants for:

Two important binary predicates are #$isa and #$genls. The first one describes that one item is an instance of some collection, the second one that one collection is a subcollection of another one. Facts about concepts are asserted using certain CycL "sentences". Predicates are written before their arguments, in parentheses:
"Bill Clinton belongs to the collection of U.S. presidents."
"All trees are plants."
"Paris is the capital of France."

Sentences can also contain variables, strings starting with "?". These sentences are called "rules". One important rule asserted about the #$isa predicate reads:
"If OBJ is an instance of the collection SUBSET and SUBSET is a subcollection of SUPERSET, then OBJ is an instance of the collection SUPERSET". Another typical example is
which means that for every instance of the collection #$ChordataPhylum (i.e. for every chordate), there exists a female animal (instance of #$FemaleAnimal), which is its mother (described by the predicate #$biologicalMother).

The knowledge base is divided into "microtheories" (Mt), collections of concepts and facts typically pertaining to one particular realm of knowledge. Unlike the knowledge base as a whole, each microtheory must be free from "monotonic" contradictions. Each microtheory is a first-class object in the Cyc ontology; it has a name that is a regular constant; microtheory constants contain the string "Mt" by convention. An example is #$MathMt, the microtheory containing mathematical knowledge. The microtheories can inherit from each other and are organized in a hierarchy:
one specialization of #$MathMt is #$GeometryGMt, the microtheory about geometry.

An inference engine is a computer program that tries to derive answers from a knowledge base.
The Cyc inference engine performs general logical deduction (including modus ponens, modus tollens, universal quantification and existential quantification). It also performs inductive reasoning, statistical machine learning and symbolic machine learning, and abductive reasoning (but of course sparingly and using the existing knowledge base as a filter and guide).

The first version of OpenCyc was released in spring 2002 and contained only 6,000 concepts and 60,000 facts. The knowledge base was released under the Apache License. Cycorp stated its intention to release OpenCyc under parallel, unrestricted licences to meet the needs of its users. The CycL and SubL interpreter (the program that allows users to browse and edit the database as well as to draw inferences) was released free of charge, but only as a binary, without source code. It was made available for Linux and Microsoft Windows. The open source Texai project released the RDF-compatible content extracted from OpenCyc. A version of OpenCyc, 4.0, was released in June 2012. OpenCyc 4.0 included much of the Cyc ontology at that time, containing hundreds of thousands of terms, along with millions of assertions relating the terms to each other; however, these are mainly taxonomic assertions, not the complex rules available in Cyc. The OpenCyc 4.0 knowledge base contained 239,000 concepts and 2,093,000 facts.

The main point of releasing OpenCyc was to help AI researchers understand what was "missing" from what they now call ontologies and knowledge graphs. It's useful and important to have properly taxonomized concepts like person, night, sleep, lying down, waking, happy, etc., but what's "missing" from the OpenCyc content about those terms, but present in the Cyc KB content, are the various rules of thumb that most of us share about those terms: that (as a default, in the ModernWesternHumanCultureMt) each person sleeps at night, sleeps lying down, can be woken up, is not happy about being woken up, "and so on." That point does not require continually-updated releases of OpenCyc, so, as of 2017, OpenCyc is no longer available.

In July 2006, Cycorp released the executable of ResearchCyc 1.0, a version of Cyc aimed at the research community, at no charge. (ResearchCyc was in beta stage of development during all of 2004; a beta version was released in February 2005.) In addition to the taxonomic information contained in OpenCyc, ResearchCyc includes significantly more semantic knowledge (i.e., additional facts and rules of thumb) involving the concepts in its knowledge base; it also includes a large lexicon, English parsing and generation tools, and Java based interfaces for knowledge editing and querying. In addition it contains a system for Ontology-based data integration. As of 2017, regular releases of ResearchCyc continued to appear, with 600 research groups utilizing licenses around the world at no cost for noncommercial research purposes. As of December 2019, ResearchCyc is no longer supported. Cycorp expects to improve and overhaul tools for external developers over the coming years.

There have been over 100 successful applications of Cyc; listed here are a few mutually dissimilar instances:

For over a decade, Glaxo has used Cyc to semi-automatically integrate all the large (hundreds of thousands of terms) thesauri of pharmaceutical-industry terms that reflect differing usage across companies, countries, years, and sub-industries. This ontology integration task requires domain knowledge, shallow semantic knowledge, but also arbitrarily deep common sense knowledge and reasoning. Pharma vocabulary varies across countries, (sub-) industries, companies, departments, and decades of time. E.g., whatâs a" gel pak"? Whatâs the âstreet nameâ for "ranitidine hydrochloride"? Each of these "n "controlled vocabularies is an ontology with approximately 300k terms. Glaxo researchers need to issue a query "in their current vocabulary", have it translated into a neutral âtrue meaningâ, and then have that transformed in the opposite direction to find potential matches against documents each of which was written to comply with a particular known vocabulary. They had been using a large staff to do that manually. Cyc is used as the universal interlingua capable of representing the union of all the termsâ âtrue meaningsâ, and capable of representing the 300k transformations between each of those controlled vocabularies and Cyc, thereby converting an "n2" problem into a linear one without introducing the usual sort of âtelephone gameâ attenuation of meaning. Furthermore, creating each of those 300k mappings for each thesaurus is done in a largely automated fashion, by Cyc.

The comprehensive Terrorism Knowledge Base is an application of Cyc in development that will try to ultimately contain all relevant knowledge about "terrorist" groups, their members, leaders, ideology, founders, sponsors, affiliations, facilities, locations, finances, capabilities, intentions, behaviors, tactics, and full descriptions of specific terrorist events. The knowledge is stored as statements in mathematical logic, suitable for computer understanding and reasoning.

The Cleveland Clinic has used Cyc to develop a natural language query interface of biomedical information, spanning decades of information on cardiothoracic surgeries. A query is parsed into a set of CycL (higher-order logic) fragments with open variables (e.g., "this question is talking about a person who developed an endocarditis infection", "this question is talking about a subset of Cleveland Clinic patients who underwent surgery there in 2009", etc.); then various constraints are applied (medical domain knowledge, common sense, discourse pragmatics, syntax) to see how those fragments could possibly fit together into one semantically meaningful formal query; significantly, in most cases, there is exactly "one and only one" such way of incorporating and integrating those fragments. Integrating the fragments involves (i) deciding which open variables in which fragments actually represent the same variable, and (ii) for all the final variables, decide what order and scope of quantification that variable should have, and what type (universal or existential). That logical (CycL) query is then converted into a SPARQL query that is passed to the CCF SemanticDB that is its data lake.

One Cyc application aims to help students doing math at a 6th grade level, helping them much more deeply understand that subject matter. It is based on the experience that we often have "thought" we understood something, but only "really" understood it after we had to explain or teach it to someone else. Unlike almost all other educational software, where the computer plays the role of the teacher, this application of Cyc, called MathCraft, has Cyc play the role of a fellow student who is always slightly more confused than you, the user, are about the subject. The user's role is to observe the Cyc avatar and give it advice, correct its errors, mentor it, get it to see what it's doing wrong, etc. As the user gives good advice, Cyc allows the avatar to make fewer mistakes of that type, hence, from the user's point of view, it seems as though the user has just successfully taught it something. This is a variation of Learning by Teaching.

The Cyc project has been described as "one of the most controversial endeavors of the artificial intelligence history". Catherine Havasi, CEO of Luminoso, says that Cyc is the predecessor project to IBM's Watson. Machine-learning scientist Pedro Domingos refers to the project as a "catastrophic failure" for several reasons, including the unending amount of data required to produce any viable results and the inability for Cyc to evolve on its own.

Robin Hanson, a professor of economics at George Mason University, gives a more balanced analysis:

A similar sentiment was expressed by Marvin Minsky: "Unfortunately, the strategies most popular among AI researchers in the 1980s have come to a dead end," said Minsky. So-called âexpert systems,â which emulated human expertise within tightly defined subject areas like law and medicine, could match usersâ queries to relevant diagnoses, papers and abstracts, yet they could not learn concepts that most children know by the time they are 3 years old. âFor each different kind of problem,â said Minsky, âthe construction of expert systems had to start all over again, because they didnât accumulate common-sense knowledge.â Only one researcher has committed himself to the colossal task of building a comprehensive common-sense reasoning system, according to Minsky. Douglas Lenat, through his Cyc project, has directed the line-by-line entry of more than 1 million rules into a commonsense knowledge base."

Gary Marcus, a professor of psychology and neural science at New York University and the cofounder of an AI company called Geometric Intelligence, says "it represents an approach that is very different from all the deep-learning stuff that has been in the news.â This is consistent with Doug Lenat's position that "Sometimes the "veneer" of intelligence is not enough".

Stephen Wolfram writes:

Every few years since it began publishing (1993), there is a new Wired Magazine article about Cyc, some positive and some negative (including one issue which contained one of each).

This is a list of some of the notable people who work or have worked on Cyc either while it was a project at MCC (where Cyc was first started) or Cycorp.




</doc>
<doc id="6876" url="https://en.wikipedia.org/wiki?curid=6876" title="CE">
CE

CE, Ce or ce may refer to:












</doc>
<doc id="6878" url="https://en.wikipedia.org/wiki?curid=6878" title="Carlos Valderrama">
Carlos Valderrama

Carlos Alberto Valderrama Palacio ( ; born 2 September 1961), also known as "El Pibe" ("The Kid"), is a Colombian former footballer who played as an attacking midfielder. A creative playmaker, he is regarded as one of the best Colombian footballers of all time, and by some as Colombia's greatest player ever. His distinctive hairstyle, as well as his precise passing and technical skills made him one of South America's most recognisable footballers in the late 1980s and early 1990s. He won the South American Footballer of the Year award in 1987 and 1993, and in 1999, he was also named one of the top 100 players of the 20th century by World Soccer. In 2004, he was included in the FIFA 100, a list of the 125 "greatest living footballers" chosen by PelÃ© to celebrate the 100th anniversary of FIFA.

Valderrama was a member of the Colombia national football team from 1985 until 1998. He represented Colombia in 111 full internationals and scored 11 times, making him the most capped player in the country's history. He played a major role during the golden era of Colombian football in the 1990s, representing his national side in three FIFA World Cups and five Copa AmÃ©rica tournaments.

After spending most of his career playing club football in South America and Europe, towards the end of his career Valderrama played in Major League Soccer, joining the league in its first season. One of the most recognisable players in the league at the time of its inception, he helped popularise the league during the second half of the 1990s. To this day, he is an icon and is considered one of the most decorated players to ever play in MLS; in 2005, he was named to the MLS All-Time Best XI.

Born in Santa Marta, Colombia, Valderrama began his career at UniÃ³n Magdalena of the Colombian First Division in 1981. He also later played for Millonarios in 1984. He joined Deportivo Cali in 1985, where he played most of his Colombian football. In 1988, he moved to the French First Division side Montpellier. He struggled to adapt to the less technical and the faster, more physical, and tactical brand of football being played in Europe, losing his place in the squad. However, his passing ability later saw him become the club's main creative force, and he played a decisive role as his side won the Coupe de France in 1990. In 1991, he remained in Europe and joined Spanish side Real Valladolid for a season. He then returned to Colombia in 1992 and went on to play for Independiente MedellÃ­n, and subsequently AtlÃ©tico Junior in 1993, with whom he won the Colombian championship in 1993 and 1995.

Valderrama began his Major League Soccer career with the US side Tampa Bay Mutiny in the league's inaugural year of 1996, and won its ever first Supporters' Shield and Most Valuable Player award, finishing the season with 4 goals and 17 assists. He remained with the club for the 1997 season, and also spent a spell on loan back at Deportivo Cali in Colombia, before moving to another MLS side, Miami Fusion, in 1998, where he also remained for two seasons. He returned to Tampa Bay in 2000, spending two more seasons with the club; while a member of the Mutiny, the team would sell Carlos Valderrama wigs at Tampa Stadium. In the 2000 MLS season, Valderrama recorded the only 20+ assist season in MLS historyâending the season with 26 â a single season assist record that remains intact to this day, and which MLS itself suggested was an "unbreakable" record in a 2012 article. In 2001, Valderrama joined the Colorado Rapids, and remained with the team until 2002, when he retired; his American soccer league career spanned a total of eight years, during which he made 175 appearances. In the MLS, Valderrama scored relatively few goals (16) for a midfielder, but is the league's fourth all-time leader in assists (114) after Brad Davis (123), Steve Ralston (135) - a former teammate, Landon Donovan (145). In 2005, he was named to the MLS All-Time Best XI.

Valderrama was a member of the Colombia national football team from 1985 until 1998; he made 111 international appearances, scoring 11 goals, making him the most capped player in the country's history. He represented and captained his national side in the 1990, 1994, and 
1998 FIFA World Cups, and also took part in the 1987, 1989, 1991, 1993, and 1995 Copa AmÃ©rica tournaments.

Valderrama made his international debut on 27 October 1985, in a 3â0 defeat to Paraguay in a 1986 World Cup qualifying match, at the age of 24. In his first major international tournament, he helped Colombia to a third-place finish at the 1987 Copa AmÃ©rica in Argentina, as his team's captain, where he was named the tournament's best player; during the tournament he scored the opening goal in Colombia's 2â0 over Bolivia on 1 July, their first match of the group stage.
Some of Valderrama's most impressive international performances came during the 1990 FIFA World Cup in Italy, during which he served as Colombia's captain. He helped his team to a 2â0 win against the UAE in Colombia's opening match of the group stage, scoring the second goal of the match with a strike from 20 yards. Colombia lost their second match against Yugoslavia, however, needing at least a draw against the eventual champions West Germany in their final group match in order to advance to the next round of the competition. In the decisive game, German striker Pierre Littbarski scored what appeared to be the winning goal in the 88th minute of the game; however, within the last minute of injury time, Valderrama beat several opposing players and made a crucial left-footed pass to Freddy Rincon, who subsequently equalised, sealing a place for Colombia in the second round of the tournament with a 1â1 draw. Colombia were eliminated in the round of 16, following a 2â1 extra time loss to Cameroon.

On 5 September 1993, Valderrama contributed to Colombia's historic 5â0 victory over South American rivals Argentina at the "Monumental" in Buenos Aires, which allowed them to qualify for the 1994 World Cup. Although much was expected of Valderrama at the World Cup, an injury during a pre-tournament warm-up game put his place in the squad in jeopardy; although he was able to regain match fitness in time for the tournament, Colombia disappointed and suffered a first round elimination following defeats to Romania and the hosts USA, though it has been contributed by the internal problem and threats by cartel groups at the time.

Four years later, Valderrama led his nation to qualify for the 1998 World Cup in France, scoring three goals during the qualifying stages. His impact in the final tournament at the advancing age of 37, however, was less decisive, and, despite defeating Tunisia, Colombia once again suffered a first round exit, following a 2â0 defeat against England, which was Valderrama's final international appearance.

Although Valderrama is often defined as a 'classic number 10 playmaker', due to his creativity and offensive contribution, in reality he was not a classic playmaker in the traditional sense. Although he often wore the number 10 shirt throughout his career and was deployed as an attacking midfielder at times, he played mostly in deeper positions in the centre of the pitch â often operating in a free role as a deep-lying playmaker, rather than in more advanced midfield positions behind the forwards â in order to have a greater influence on the game. A team-player, Valderrama was also known to be an extremely selfless midfielder, who preferred assisting his teammates over going for goal himself; his tactical intelligence, positioning, efficient movement and versatile range of passing enabled him to both set the tempo of his team in midfield with short, first time exchanges, or create chances with long lobbed passes or through balls.

Valderrama's most instantly recognisable physical features were his big afro-blonde hairstyle, jewelry, and moustache, but he was best known for his grace and elegance on the ball, as well as his agility, and quick feet as a footballer. His control, dribbling ability and footwork were similar to those of smaller players, which for a player of Valderrama's size and physical build was fairly uncommon, and he frequently stood out throughout his career for his ability to use his strength, balance, composure, and flamboyant technique to shield the ball from opponents when put under pressure, and retain possession in difficult situations, which made him extremely popular with the fans. Valderrama's mix of physical strength, two-footed ability, unpredictability and flair enabled him to produce key and incisive performances against top tier teams, while his world class vision and exceptional passing and crossing ability with his right foot made him one of the best assist providers of his time; his height, physique and elevation also made him effective in the air, and he was also an accurate free kick taker and striker of the ball, despite not being a particularly prolific goalscorer.

Despite his natural talent and ability as a footballer, Valderrama earned a reputation for having a "languid" playing style, as well as lacking notable pace, being unfit, and for having a poor defensive work-rate on the pitch, in particular after succumbing to the physical effects of ageing in his later career in the MLS. In his first season in France, he also initially struggled to adapt to the faster-paced, more physical and tactically rigorous European brand of football, which saw him play in an unfamiliar position, and gave him less space and time on the ball to dictate attacking passing moves; he was criticised at times for his lack of match fitness and his low defensive contribution, which initially limited his appearances with the club, although he later successfully became a key creative player in his team's starting line-up due to his discipline, skill, and his precise and efficient passing. Despite these claims, earlier in his career, however, Valderrama demonstrated substantial pace, stamina, and defensive competence.

Former French defender Laurent Blanc, who played with Valderrama in Montpellier, voiced one of the most accurate descriptions for Valderrama, "In the fast and furious European game he wasn't always at his ease. He was a natural exponent of 'toque', keeping the ball moving. But he was so gifted that we could give him the ball when we didn't know what else to do with it knowing he wouldn't lose it... and often he would do things that most of us only dream about."

In February 2004, Valderrama ended his 22-year career in a tribute match at the Metropolitan stadium of Barranquilla, with some of the most important football players of South America, such as Diego Maradona, Enzo Francescoli, IvÃ¡n Zamorano, and JosÃ© Luis Chilavert.

In 2006, a 22-foot bronze statue of Valderrama, created by Colombian artist Amilkar Ariza, was erected outside Estadio Eduardo Santos in Valderrama's birthplace of Santa Marta.

Valderrama was the only Colombian to feature in FIFA's 125 Top Living Football Players list in March 2004.

Valderrama appeared on the cover of Konami's "International Superstar Soccer Pro 98". In the Nintendo 64 version of the game, he is referred to by his nickname, "El Pibe".

Valderrama has also appeared in EA Sports' FIFA football video game series; he was named one of the Ultimate Team Legend cards in "FIFA 15".

Since retiring from professional football, Valderrama has become assistant manager of AtlÃ©tico Junior. On 1 November 2007, Valderrama accused a referee of corruption by waving cash in the face of Oscar Julian Ruiz when the official awarded a penalty to AmÃ©rica de Cali. Junior lost the match 4â1, which ended the club's hopes of playoff qualification. He later also served as a coach for a football academy called Clearwater Galactics in Clearwater, Florida.

Valderrama is married and has six children.

"Scores and results lists Colombia's goal tally first."







</doc>
<doc id="6880" url="https://en.wikipedia.org/wiki?curid=6880" title="Caesar salad">
Caesar salad

A Caesar salad (also spelled Cesar and Cesare) is a green salad of romaine lettuce and croutons dressed with lemon juice (or lime juice), olive oil, egg, Worcestershire sauce, anchovies, garlic, Dijon mustard, Parmesan cheese, and black pepper.

In its original form, this salad was prepared and served tableside.

The salad's creation is generally attributed to restaurateur Caesar Cardini, an Italian immigrant who operated restaurants in Mexico and the United States. Cardini was living in San Diego but he was also working in Tijuana where he avoided the restrictions of Prohibition. His daughter Rosa recounted that her father invented the salad at his restaurant Caesar's (at the Hotel Cesar) when a Fourth of July rush in 1924 depleted the kitchen's supplies. Cardini made do with what he had, adding the dramatic flair of the table-side tossing "by the chef." A number of Cardini's staff have said that they invented the dish.

Julia Child said that she had eaten a Caesar salad at Cardini's restaurant when she was a child in the 1920s. In 1946, newspaper columnist Dorothy Kilgallen wrote of a Caesar containing anchovies, differing from Cardini's version:
The big food rage in Hollywoodâthe Caesar saladâwill be introduced to New Yorkers by Gilmore's Steak House. It's an intricate concoction that takes ages to prepare and contains (zowie!) lots of garlic, raw or slightly coddled eggs, croutons, romaine, anchovies, parmeasan ["sic"] cheese, olive oil, vinegar and plenty of black pepper.According to Rosa Cardini, the original Caesar salad (unlike his brother Alex's "Aviator's salad", which was later renamed to Caesar salad) did not contain pieces of anchovy; the slight anchovy flavor comes from the Worcestershire sauce. Cardini was opposed to using anchovies in his salad.

In the 1970s, Cardini's daughter said that the original recipe included whole lettuce leaves, which were meant to be lifted by the stem and eaten with the fingers; coddled eggs; and Italian olive oil.

Although the original recipe does not contain anchovies, modern recipes typically include anchovies as a key ingredient, which frequently is emulsified in bottled versions. Bottled Caesar dressings are now produced and marketed by many companies.

The trademark brands "Cardini's", "Caesar Cardini's" and "The Original Caesar Dressing" are all claimed to date to February 1950, although they were only registered decades later, and more than a dozen varieties of bottled "Cardini's" dressing are available today, with various ingredients.

Common ingredients in many recipes:

There are limitless other common variations, including varying the leaf, adding meat such as grilled chicken or bacon, or omitting ingredients such as anchovies and eggs.

There is inherent risk of infection by salmonella bacteria occasionally found in raw egg from cracked or improperly washed eggshells. However, some countries such as the UK have eliminated this risk through vaccination and tracking strategies. This is a concern with many similar dressings that are emulsified with eggs, though generally the pH level is thought to be acidic enough to kill those bacteria. Nevertheless, later versions of the recipe call at least for briefly cooked coddled eggs or pasteurized eggs. Recipes may omit the egg and produce a "Caesar vinaigrette". Many variations of this salad exist; yogurt is sometimes substituted for the eggs to maintain a creamy texture and others call for using mayonnaise.





</doc>
<doc id="6881" url="https://en.wikipedia.org/wiki?curid=6881" title="Cecilia Beaux">
Cecilia Beaux

Cecilia Beaux (May 1, 1855 â September 17, 1942) was an American society portraitist, in the manner of John Singer Sargent. She was a near-contemporary of American artist Mary Cassatt and also received her training in Philadelphia and France. Her sympathetic renderings of the American ruling class made her one of the most successful portrait painters of her era.

Eliza Cecilia Beaux was born on May 1, 1855 in Philadelphia, Pennsylvania. She was the youngest daughter of French silk manufacturer Jean Adolphe Beaux and teacher Cecilia Kent Leavitt. Her mother was the daughter of prominent businessman John Wheeler Leavitt of New York City and his wife Cecilia Kent of Suffield, Connecticut. Cecilia Kent Leavitt died from puerperal fever 12 days after giving birth at age 33. Cecilia "Leilie" Beaux and her sister Etta were subsequently raised by their maternal grandmother and aunts, primarily in Philadelphia. Her father, unable to bear the grief of his loss, and feeling adrift in a foreign country, returned to his native France for 16 years, with only one visit back to Philadelphia. He returned when Cecilia was two, but left four years later after his business failed. As she confessed later, "We didn't love Papa very much, he was so foreign. We thought him "peculiar"." Her father did have a natural aptitude for drawing and the sisters were charmed by his whimsical sketches of animals. Later, Beaux would discover that her French heritage would serve her well during her pilgrimage and training in France.

In Philadelphia, Beaux's aunt Emily married mining engineer William Foster Biddle, whom Beaux would later describe as "after my grandmother, the strongest and most beneficent influence in my life." For fifty years, he cared for his nieces-in-law with consistent attention and occasional financial support. Her grandmother, on the other hand, provided day-to-day supervision and kindly discipline. Whether with housework, handiwork, or academics, Grandma Leavitt offered a pragmatic framework, stressing that "everything undertaken must be completed, conquered." The Civil War years were particularly challenging, but the extended family survived despite little emotional or financial support from Beaux's father.

After the war, Beaux began to spend some time in the household of "Willie" and Emily, both proficient musicians. Beaux learned to play the piano but preferred singing. The musical atmosphere later proved an advantage for her artistic ambitions. Beaux recalled, "They understood perfectly the spirit and necessities of an artist's life." In her early teens, she had her first major exposure to art during visits with Willie to the nearby Pennsylvania Academy of the Fine Arts, one of America's foremost art schools and museums. Though fascinated by the narrative elements of some of the pictures, particularly the Biblical themes of the massive paintings of Benjamin West, at this point Beaux had no aspirations of becoming an artist.

Her childhood was a sheltered though generally happy one. As a teen she already manifested the traits, as she described, of "both a realist and a perfectionist, pursued by an uncompromising passion for carrying through." She attended the Misses Lyman School and was just an average student, though she did well in French and Natural History. However, she was unable to afford the extra fee for art lessons. At age 16, Beaux began art lessons with a relative, Catherine Ann Drinker, an accomplished artist who had her own studio and a growing clientele. Drinker became Beaux's role model, and she continued lessons with Drinker for a year. She then studied for two years with the painter Francis Adolf Van der Wielen, who offered lessons in perspective and drawing from casts during the time that the new Pennsylvania Academy of the Fine Arts was under construction. Given the bias of the Victorian age, female students were denied direct study in anatomy and could not attend drawing classes with live models (who were often prostitutes) until a decade later.

At 18, Beaux was appointed as a drawing teacher at Miss Sanford's School, taking over Drinker's post. She also gave private art lessons and produced decorative art and small portraits. Her own studies were mostly self-directed. Beaux received her first introduction to lithography doing copy work for Philadelphia printer Thomas Sinclair and she published her first work in "St. Nicholas" magazine in December 1873. Beaux demonstrated accuracy and patience as a scientific illustrator, creating drawings of fossils for Edward Drinker Cope, for a multi-volume report sponsored by the U.S. Geological Survey. However, she did not find technical illustration suitable for a career (the extreme exactitude required gave her pains in the "solar plexus"). At this stage, she did not yet consider herself an artist.

Beaux began attending the Pennsylvania Academy of the Fine Arts in 1876, then under the dynamic influence of Thomas Eakins, whose great work "The Gross Clinic" had "horrified Philadelphia Exhibition-goers as a gory spectacle" at the Centennial Exhibition of 1876. She steered clear of the controversial Eakins, though she much admired his work. His progressive teaching philosophy, focused on anatomy and live study (and allowed the female students to partake in segregated studios), eventually led to his firing as director of the Academy. She did not ally herself with Eakins' ardent student supporters, and later wrote, "A curious instinct of self-preservation kept me outside the magic circle." Instead, she attended costume and portrait painting classes for three years taught by the ailing director Christian Schussele. Beaux won the Mary Smith Prize at the Pennsylvania Academy of the Fine Arts exhibitions in 1885, 1887, 1891, and 1892.

After leaving the Academy, the 24-year-old Beaux decided to try her hand at porcelain painting and she enrolled in a course at the National Art Training School. She was well suited to the precise work but later wrote, "this was the lowest depth I ever reached in commercial art, and although it was a period when youth and romance were in their first attendance on me, I remember it with gloom and record it with shame." She studied privately with William Sartain, a friend of Eakins and a New York artist invited to Philadelphia to teach a group of art students, starting in 1881. Though Beaux admired Eakins more and thought his painting skill superior to Sartain's, she preferred the latter's gentle teaching style which promoted no particular aesthetic approach. Unlike Eakins, however, Sartain believed in phrenology and Beaux adopted a lifelong belief that physical characteristics correlated with behaviors and traits.

Beaux attended Sartain's classes for two years, then rented her own studio and shared it with a group of women artists who hired a live model and continued without an instructor. After the group disbanded, Beaux set in earnest to prove her artistic abilities. She painted a large canvas in 1884, "Les Derniers Jours d'Enfance", a portrait of her sister and nephew whose composition and style revealed a debt to James McNeill Whistler and whose subject matter was akin to Mary Cassatt's mother-and-child paintings. It was awarded a prize for the best painting by a female artist at the Academy, and further exhibited in Philadelphia and New York. Following that seminal painting, she painted over 50 portraits in the next three years with the zeal of a committed professional artist. Her invitation to serve as a juror on the hanging committee of the Academy confirmed her acceptance amongst her peers. In the mid-1880s, she was receiving commissions from notable Philadelphians and earning $500 per portrait, comparable to what Eakins commanded. When her friend Margaret Bush-Brown insisted that "Les Derniers" was good enough to be exhibited at the famed Paris Salon, Beaux relented and sent the painting abroad in the care of her friend, who managed to get the painting into the exhibition.

At 32, despite her clear success in Philadelphia, Beaux decided that she still needed to advance her skills. She left for Paris with cousin May Whitlock, forsaking several suitors and overcoming the objections of her family. There she trained at the AcadÃ©mie Julian, the largest art school in Paris, and at the AcadÃ©mie Colarossi, receiving weekly critiques from established masters like Tony Robert-Fleury and William-Adolphe Bouguereau. She wrote, "Fleury is much less benign than Bouguereau and don't temper his severitiesâ¦he hinted of possibilities before me and as he rose said the nicest thing of all, 'we will do all we can to help you'â¦I want these menâ¦to know me and recognize that I can do something." Though advised regularly of Beaux's progress abroad and to "not be worried about any indiscretions of ours", her Aunt Eliza repeatedly reminded her niece to avoid the temptations of Paris, "Remember you are first of all a Christian â then a woman and last of all an Artist."

When Beaux arrived in Paris, the Impressionists, a group of artists who had begun their own series of independent exhibitions from the official Salon in 1874, were beginning to lose their solidarity. Also known as the "Independents" or "Intransigents", the group which at times included Degas, Monet, Sisley, Caillebotte, Pissarro, Renoir, and Berthe Morisot, had been receiving the wrath of the critics for several years. Their art, though varying in style and technique, was the antithesis of the type of Academic art in which Beaux was trained and of which her teacher William-Adolphe Bouguereau was a leading master. In the summer of 1888, with classes in summer recess, Beaux worked in the fishing village of Concarneau with the American painters Alexander Harrison and Charles Lazar. She tried applying the plein-air painting techniques used by the Impressionists to her own landscapes and portraiture, with little success. Unlike her predecessor Mary Cassatt, who had arrived near the beginning of the Impressionist movement 15 years earlier and who had absorbed it, Beaux's artistic temperament, precise and true to observation, would not align with Impressionism and she remained a realist painter for the rest of her career, even as CÃ©zanne, Matisse, Gauguin, and Picasso were beginning to take art into new directions. Beaux mostly admired classic artists like Titian and Rembrandt. Her European training did influence her palette, however, and she adopted more white and paler coloration in her oil painting, particularly in depicting female subjects, an approach favored by Sargent as well.

Back in America in 1889, Beaux proceeded to paint portraits in the grand manner, taking as her subjects members of her sister's family as well as the elite of Philadelphia. In making her decision to devote herself to art, she also thought it was best not to marry, and in choosing male company she selected men who would not threaten to sidetrack her career. She resumed life with her family, and they supported her fully, acknowledging her chosen path and demanding of her little in the way of household responsibilities, "I was never once asked to do an errand in town, some bit of shoppingâ¦so well did they understand." She developed a structured, professional routine, arriving promptly at her studio, and expected the same from her models.

The five years that followed were highly productive, resulting in over forty portraits. In 1890 she exhibited at the Paris Exposition, obtained in 1893 the gold medal of the Philadelphia Art Club, and also the Dodge prize at the New York National Academy of Design. She exhibited her work at the Palace of Fine Arts and The Woman's Building at the 1893 World's Columbian Exposition in Chicago, Illinois. Her portrait of "The Reverend Matthew Blackburne Grier" was particularly well-received, as was "Sita and Sarita", a portrait of her cousin Charles W. Leavitt's wife Sarah (Allibone) Leavitt in white, with a small black cat perched on her shoulder, both gazing out mysteriously. The mesmerizing effect prompted one critic to point out "the witch-like weirdness of the black kitten" and for many years, the painting solicited questions by the press. But the result was not pre-planned, as Beaux's sister later explained, "Please make no mystery about itâit was only an idea to put the black kitten on her cousin's shoulder. Nothing deeper." Beaux donated "Sita and Sarita" to the MusÃ©e du Luxembourg, but only after making a copy for herself. Another highly regarded portrait from that period is "New England Woman" (1895), a nearly all-white oil painting which was purchased by the Pennsylvania Academy of the Fine Arts.

In 1895 Beaux became the first woman to have a regular teaching position at the Pennsylvania Academy of the Fine Arts, where she instructed in portrait drawing and painting for the next twenty years. That rare type of achievement by a woman prompted one local newspaper to state, "It is a legitimate source of pride to Philadelphia that one of its most cherished institutions has made this innovation." She was a popular instructor. In 1896, Beaux returned to France to see a group of her paintings presented at the Salon. Influential French critic M. Henri Rochefort commented, "I am compelled to admit, not without some chagrin, that not one of our female artistsâ¦is strong enough to compete with the lady who has given us this year the portrait of Dr. Grier. Composition, flesh, texture, sound drawingâeverything is there without affectation, and without seeking for effect."

Cecilia Beaux considered herself a "New Woman", a 19th-century women who explored educational and career opportunities that had generally been denied to women. In the late 19th century Charles Dana Gibson depicted the "New Woman" in his painting, "The Reason Dinner was Late", which is "a sympathetic portrayal of artistic aspiration on the part of young women" as she paints a visiting policeman. This "New Woman" was successful, highly trained, and often did not marry; other such women included Ellen Day Hale, Mary Cassatt, Elizabeth Nourse and Elizabeth Coffin.

Beaux was a member of Philadelphia's The Plastic Club. Other members included Elenore Abbott, Jessie Willcox Smith, Violet Oakley, Emily Sartain, and Elizabeth Shippen Green. Many of the women who founded the organization had been students of Howard Pyle. It was founded to provide a means to encourage one another professionally and create opportunities to sell their works of art.

By 1900 the demand for Beaux's work brought clients from Washington, D.C., to Boston, prompting the artist to move to New York City; it was there she spent the winters, while summering at Green Alley, the home and studio she had built in Gloucester, Massachusetts. Beaux's friendship with Richard Gilder, editor-in-chief of the literary magazine "The Century", helped promote her career and he introduced her to the elite of society. Among her portraits which followed from that association are those of Georges Clemenceau; First Lady Edith Roosevelt and her daughter; and Admiral Sir David Beatty. She also sketched President Teddy Roosevelt during her White House visits in 1902, during which "He sat for two hours, talking most of the time, reciting Kipling, and reading scraps of Browning." Her portraits "Fanny Travis Cochran", "Dorothea and Francesca", and "Ernesta and her Little Brother", are fine examples of her skill in painting children; "Ernesta with Nurse", one of a series of essays in luminous white, was a highly original composition, seemingly without precedent. She became a member of the National Academy of Design in 1902. and won the Logan Medal of the arts at the Art Institute of Chicago in 1921.

By 1906, Beaux began to live year-round at Green Alley, in a comfortable colony of "cottages" belonging to her wealthy friends and neighbors. All three aunts had died and she needed an emotional break from Philadelphia and New York. She managed to find new subjects for portraiture, working in the mornings and enjoying a leisurely life the rest of the time. She carefully regulated her energy and her activities to maintain a productive output, and considered that a key to her success. On why so few women succeeded in art as she did, she stated, "Strength is the stumbling block. They (women) are sometimes unable to stand the hard work of it day in and day out. They become tired and cannot reenergize themselves."

While Beaux stuck to her portraits of the elite, American art was advancing into urban and social subject matter, led by artists such as Robert Henri who espoused a totally different aesthetic, "Work with great speed..Have your energies alert, up and active. Do it all in one sitting if you can. In one minute if you can. There is no use delayingâ¦Stop studying water pitchers and bananas and paint everyday life." He advised his students, among them Edward Hopper and Rockwell Kent, to live with the common man and paint the common man, in total opposition to Cecilia Beaux's artistic methods and subjects. The clash of Henri and William Merritt Chase (representing Beaux and the traditional art establishment) resulted in 1907 in the independent exhibition by the urban realists known as "The Eight" or the Ashcan School. Beaux and her art friends defended the old order, and many thought (and hoped) the new movement to be a passing fad, but it turned out to be a revolutionary turn in American art.

In 1910, her beloved Uncle Willie died. Though devastated by the loss, at fifty-five years of age, Beaux remained highly productive. In the next five years she painted almost 25 percent of her lifetime output and received a steady stream of honors. She had a major exhibition of 35 paintings at the Corcoran Gallery of Art in Washington, D.C., in 1912. Despite her continuing production and accolades, however, Beaux was working against the current of tastes and trends in art. The famed "Armory Show" of 1913 in New York City was a landmark presentation of 1,200 paintings showcasing Modernism. Beaux believed that the public, initially of mixed opinion about the "new" art, would ultimately reject it and return its favor to the Pre-Impressionists.

Beaux was crippled after breaking her hip while walking in Paris in 1924. With her health impaired, her work output dwindled for the remainder of her life. That same year Beaux was asked to produce a self-portrait for the Medici collection in the Uffizi Gallery in Florence. In 1930 she published an autobiography, "Background with Figures". Her later life was filled with honors. In 1930 she was elected a member of the National Institute of Arts and Letters; in 1933 came membership in the American Academy of Arts and Letters, which two years later organized the first major retrospective of her work. Also in 1933 Eleanor Roosevelt honored Beaux as "the American woman who had made the greatest contribution to the culture of the world". In 1942 The National Institute of Arts and Letters awarded her a gold medal for lifetime achievement.

Cecilia Beaux died at the age of 87 on September 17, 1942, in Gloucester, Massachusetts. She was buried at West Laurel Hill Cemetery in Bala Cynwyd, Pennsylvania. In her will she left a Duncan Phyfe rosewood secretaire made for her father to her cherished nephew Cecil Kent Drinker, a Harvard physician whom she had painted as a young boy. 

Beaux was included in the 2018 exhibit "Women in Paris 1850-1900" at the Clark Art Institute.

Though Beaux was an individualist, comparisons to Sargent would prove inevitable, and often favorable. Her strong technique, her perceptive reading of her subjects, and her ability to flatter without falsifying, were traits similar to his.

"The critics are very enthusiastic. (Bernard) Berenson, Mrs. Coates tells me, stood in front of the portraits â Miss Beaux's three â and wagged his head. 'Ah, yes, I see!' Some Sargents. The ordinary ones are signed John Sargent, the best are signed Cecilia Beaux, which is, of course, nonsense in more ways than one, but it is part of the generous chorus of praise." Though overshadowed by Mary Cassatt and relatively unknown to museum-goers today, Beaux's craftsmanship and extraordinary output were highly regarded in her time. While presenting the Carnegie Institute's Gold Medal to Beaux in 1899, William Merritt Chase stated "Miss Beaux is not only the greatest living woman painter, but the best that has ever lived. Miss Beaux has done away entirely with sex [gender] in art."

During her long productive life as an artist, she maintained her personal aesthetic and high standards against all distractions and countervailing forces. She constantly struggled for perfection, "A perfect technique in anything," she stated in an interview, "means that there has been no break in continuity between the conception and the act of performance." She summed up her driving work ethic, "I can say this: When I attempt anything, I have a passionate determination to overcome every obstacleâ¦And I do my own work with a refusal to accept defeat that might almost be called painful."




</doc>
<doc id="6882" url="https://en.wikipedia.org/wiki?curid=6882" title="Chrysler">
Chrysler

Chrysler (; officially FCA US LLC, the first acronym standing for Fiat Chrysler Automobiles) is one of the "Big Three" automobile manufacturers in the United States, headquartered in Auburn Hills, Michigan. The original Chrysler Corporation was founded in 1925 by Walter Chrysler from the remains of the Maxwell Motor Company. In 1998, it was acquired by Daimler-Benz, and the holding company was renamed DaimlerChrysler. After Daimler divested Chrysler in 2007, the company existed as Chrysler LLC (2007â2009) and Chrysler Group LLC (2009â2014) before merging in 2014 with Italian holding company Fiat S.p.A. and becoming a subsidiary of its successor Fiat Chrysler Automobiles. In addition to the Chrysler brand, FCA sells vehicles worldwide under the Dodge, Jeep, and Ram nameplates. Furthermore, the subsidiary includes Mopar, its automotive parts and accessories division, and SRT, its performance automobile division.

After founding the company, Walter Chrysler used the General Motors brand diversification and hierarchy strategy that he had seen working for Buick, and acquired Fargo Trucks and Dodge Brothers, and created the Plymouth and DeSoto brands in 1928. Facing postwar declines in market share, productivity, and profitability, as GM and Ford were growing, Chrysler borrowed $250 million in 1954 from Prudential Insurance to pay for expansion and updated car designs.

Chrysler expanded into Europe by taking control of French, British and Spanish auto companies in the 1960s; Chrysler Europe was sold in 1978 to PSA Peugeot CitroÃ«n for $1. The company struggled to adapt to changing markets, increased U.S. import competition, and safety and environmental regulation in the 1970s. It began an engineering partnership with Mitsubishi Motors, and began selling Mitsubishi vehicles branded as Dodge and Plymouth in North America. On the verge of bankruptcy in the late 1970s, it was saved by $1.5 billion in loan guarantees from the U.S. government. New CEO Lee Iacocca was credited with returning the company to profitability in the 1980s. In 1985, Diamond-Star Motors was created, further expanding the Chrysler-Mitsubishi relationship. In 1987, Chrysler acquired American Motors Corporation (AMC), which brought the profitable Jeep brand under the Chrysler umbrella. In 1998, Chrysler merged with German automaker Daimler-Benz to form DaimlerChrysler AG; the merger proved contentious with investors. As a result, Chrysler was sold to Cerberus Capital Management and renamed Chrysler LLC in 2007.

Like the other Big Three automobile manufacturers, Chrysler was impacted by the automotive industry crisis of 2008â2010. The company remained in business through a combination of negotiations with creditors, filing for Chapter 11 bankruptcy reorganization on April 30, 2009, and participating in a bailout from the U.S. government through the Troubled Asset Relief Program. On June 10, 2009, Chrysler emerged from the bankruptcy proceedings with the United Auto Workers pension fund, Fiat S.p.A., and the U.S. and Canadian governments as principal owners. The bankruptcy resulted in Chrysler defaulting on over $4 billion in debts. By May 24, 2011, Chrysler finished repaying its obligations to the U.S. government five years early, although the cost to the American taxpayer was $1.3 billion. Over the next few years, Fiat gradually acquired the other parties' shares while removing much of the weight of the loans (which carried a 21% interest rate) in a short period.

On January 1, 2014, Fiat S.p.A announced a deal to purchase the rest of Chrysler from the United Auto Workers retiree health trust. The deal was completed on January 21, 2014, making Chrysler Group a subsidiary of Fiat S.p.A. In May 2014, Fiat Chrysler Automobiles was established by merging Fiat S.p.A. into the company. This was completed in August 2014. Chrysler Group LLC remained a subsidiary until December 15, 2014, when it was renamed FCA US LLC, to reflect the Fiat-Chrysler merger.

The Chrysler company was founded by Walter Chrysler on June 6, 1925, when the Maxwell Motor Company (est. 1904) was re-organized into the Chrysler Corporation.

Chrysler had arrived at the ailing Maxwell-Chalmers company in the early 1920s, hired to overhaul the company's troubled operations (after a similar rescue job at the Willys-Overland car company). In late 1923 production of the Chalmers automobile was ended.
In January 1924, Walter Chrysler launched the well-received Chrysler automobile. The 6-cylinder Chrysler was designed to provide customers with an advanced, well-engineered car, was an automobile at an affordable price. Elements of this car are traceable to a prototype which had been under development at Willys during Chrysler's tenure The original 1924 Chrysler included a carburetor air filter, high compression engine, full pressure lubrication, and an oil filter, features absent from most autos at the time. Among the innovations in its early years were the first practical mass-produced four-wheel hydraulic brakes, a system nearly completely engineered by Chrysler with patents assigned to Lockheed, and rubber engine mounts to reduce vibration.

Chrysler also developed a wheel with a ridged rim, designed to keep a deflated tire from flying off the wheel. This wheel was eventually adopted by the auto industry worldwide.

The Maxwell brand was dropped after the 1925 model year, with the new, lower-priced four-cylinder Chryslers introduced for the 1926 year being badge-engineered Maxwells. The advanced engineering and testing that went into Chrysler Corporation cars helped to push the company to the second-place position in U.S. sales by 1936, which it held until 1949.

In 1928, the Chrysler Corporation began dividing its vehicle offerings by price class and function. The Plymouth brand was introduced at the low-priced end of the market (created essentially by once again reworking and rebadging Chrysler's four-cylinder model). At the same time, the DeSoto brand was introduced in the medium-price field. Also in 1928, Chrysler bought the Dodge Brothers automobile and truck company and continued the successful Dodge line of automobiles and Fargo range of trucks. By the mid-1930s, the DeSoto and Dodge divisions would trade places in the corporate hierarchy.
The Imperial name had been used since 1926 but was never a separate make, just the top-of-the-line Chrysler. However, in 1955, the company decided to spin it off as its own make/brand and division to better compete with its rivals, Lincoln and Cadillac.

On April 28, 1955, Chrysler and Philco had announced the development and production of the World's First All-Transistor car radio. The all-transistor car radio, Mopar model 914HR, was developed and produced by Chrysler and Philco, and it was a $150.00 "option" on the 1956 Imperial automobile models. Philco began manufacturing this radio in the fall of 1955 at its Sandusky Ohio plant.

On September 28, 1957, Chrysler had announced the first production electronic fuel injection (EFI), as an option on some of its new 1958 car models (Chrysler 300D, Dodge D500, DeSoto Adventurer, Plymouth Fury). The first attempt to use this system was by American Motors on the 1957 Rambler Rebel. Bendix Corporation's Electrojector used a transistor computer brain modulator box, but teething problems on pre-production cars meant very few cars were made. The EFI system in the Rambler ran fine in warm weather, but suffered hard starting in cooler temperatures and AMC decided not to use this EFI system, on its 1957 Rambler Rebel production cars that were sold to the public. Chrysler also used the Bendix "Electrojector" fuel injection system and only around 35 vehicles were built with this option, on its 1958 production built car models. Owners of EFI Chryslers were so dissatisfied that all but one were retrofitted with carburetors (while that one has been completely restored, with original EFI electronic problems resolved).

Imperial would see new body styles introduced every two to three years, all with V8 engines and automatic transmissions, as well as technologies that would filter down to Chrysler corporation's other models. Imperial was folded back into the Chrysler brand in 1971.

The Valiant was also introduced for 1960 as a distinct brand. In the U.S. market, Valiant was made a model in the Plymouth line for 1961 and the DeSoto make was discontinued in 1961. With those exceptions per applicable year and market, Chrysler's range from lowest to highest price from the 1940s through the 1970s was Valiant, Plymouth, Dodge, DeSoto, Chrysler, and Imperial.
From 1963 through 1969, Chrysler increased its existing stakes to take full control of the French Simca, British Rootes and Spanish Barreiros companies, merging them into Chrysler Europe in 1967. In the 1970s, an engineering partnership was established with Mitsubishi Motors, and Chrysler began selling Mitsubishi vehicles branded as Dodge and Plymouth in North America.

Chrysler struggled to adapt to the changing environment of the 1970s. When consumer tastes shifted to smaller cars in the early 1970s, particularly after the 1973 oil crisis, Chrysler could not meet the demand. Additional burdens came from increased US import competition, and tougher government regulation of car safety, fuel economy, and emissions. As the smallest of the Big 3 US automakers, Chrysler lacked the financial resources to meet all of these challenges. In 1978, Lee Iacocca was brought in to turn the company around, and in 1979 Iacocca sought US government help. Congress later passed the "Loan Guarantee Act" providing $1.5 billion in loan guarantees. The "Loan Guarantee Act" required that Chrysler also obtain $2 billion in concessions or aid from sources outside the federal government, which included interest rate reductions for $650 million of the savings, asset sales of $300 million, local and state tax concessions of $250 million, and wage reductions of about $590 million along with a $50 million stock offering. $180 million was to come from concessions from dealers and suppliers.

After a period of plant closures and salary cuts agreed to by both management and the auto unions, the loans were repaid with interest in 1983. In November 1983, the Dodge Caravan/Plymouth Voyager was introduced, establishing the minivan as a major category, and initiating Chrysler's return to stability.

In 1985, Diamond-Star Motors was created, further expanding the Chrysler-Mitsubishi relationship. In 1987, Chrysler acquired American Motors Corporation (AMC), which brought the profitable Jeep brand under the Chrysler umbrella.

In 1985, Chrysler entered an agreement with AMC to produce Chrysler M platform rear-drive, as well as Dodge Omnis front wheel drive cars, in AMC's Kenosha, Wisconsin plant. In 1987, Chrysler acquired the 47% ownership of AMC that was held by Renault. The remaining outstanding shares of AMC were bought on the NYSE by August 5, 1987, making the deal valued somewhere between US$1.7 billion and US$2 billion, depending on how costs were counted. Chrysler CEO Lee Iacocca wanted the Jeep brand, particularly the Jeep Grand Cherokee (ZJ) that was under development, the new world-class manufacturing plant in Bramalea, Ontario, and AMC's engineering and management talent that became critical for Chrysler's future success. Chrysler established the Jeep/Eagle division as a "specialty" arm to market products distinctly different from the K-car-based products with the Eagle cars targeting import buyers. Former AMC dealers sold Jeep vehicles and various new Eagle models, as well as Chrysler products, strengthening the automaker's retail distribution system.

Eurostar, a joint venture between Chrysler and Steyr-Daimler-Puch, began producing the Chrysler Voyager in Austria for European markets in 1992.

In 1998, Chrysler and its subsidiaries entered into a partnership dubbed a "merger of equals" with German-based Daimler-Benz AG, creating the combined entity DaimlerChrysler AG. To the surprise of many stockholders, Daimler acquired Chrysler in a stock swap before Chrysler CEO Bob Eaton retired. It is widely accepted that the merger was needed because of Eaton's lack of planning for Chrysler in the 1990s, to become their own global automotive company. Under DaimlerChrysler, the company was named DaimlerChrysler Motors Company LLC, with its U.S. operations generally called "DCX". The Eagle brand was retired soon after Chrysler's merger with Daimler-Benz in 1998 Jeep became a stand-alone division, and efforts were made to merge the Chrysler and Jeep brands as one sales unit. In 2001, the Plymouth brand was also discontinued.

Eurostar also built the Chrysler PT Cruiser in 2001 and 2002. The Austrian venture was sold to Magna International in 2002 and became Magna Steyr. The Voyager continued in production until 2007, whereas the Chrysler 300C, Jeep Grand Cherokee and Jeep Commander were also built at the plant from 2005 to 2010.

On May 14, 2007, DaimlerChrysler announced the sale of 80.1% of Chrysler Group to American private equity firm Cerberus Capital Management, L.P., thereafter known as Chrysler LLC, although Daimler (renamed as Daimler AG) continued to hold a 19.9% stake.

The economic collapse of 2007 to 2009 pushed the fragile company to the brink. On April 30, 2009, the automaker filed for Chapter 11 bankruptcy protection to be able to operate as a going concern, while renegotiating its debt structure and other obligations, which resulted in the corporation defaulting on over $4 billion in secured debts. The U.S. government described the company's action as a "prepackaged surgical bankruptcy".

On June 10, 2009, substantially all of Chrysler's assets were sold to "New Chrysler", organized as Chrysler Group LLC. The federal government provided support for the deal with US$8 billion in financing at near 21%. Under CEO Sergio Marchionne, "World Class Manufacturing" or WCM, a system of thorough manufacturing quality, was introduced and several products re-launched with quality and luxury. The 2010 Jeep Grand Cherokee very soon became the most awarded SUV ever. The Ram, Jeep, Dodge, SRT and Chrysler divisions were separated to focus on their own identity and brand, and 11 major model refreshes occurred in 21 months. The PT Cruiser, Nitro, Liberty and Caliber models (created during DCX) were discontinued. On May 24, 2011, Chrysler repaid its $7.6 billion loans to the United States and Canadian governments. The US Treasury, through the Troubled Asset Relief Program (TARP), invested $12.5 billion in Chrysler and recovered $11.2 billion when the company shares were sold in May 2011, resulting in a $1.3 billion loss. On July 21, 2011, Fiat bought the Chrysler shares held by the US Treasury. The purchase made Chrysler foreign-owned again, this time as the luxury division. The Chrysler 300 was badged Lancia Thema in some European markets (with additional engine options), giving Lancia a much needed replacement for its flagship.

On January 21, 2014, Fiat bought the remaining shares of Chrysler owned by the VEBA worth $3.65 billion. Several days later, the intended reorganization of Fiat and Chrysler under a new holding company, Fiat Chrysler Automobiles, together with a new FCA logo were announced. The most challenging launch for this new company came immediately in January 2014 with a completely redesigned Chrysler 200. The vehicle's creation is from the completely integrated company, FCA, executing from a global compact-wide platform.

On December 16, 2014, Chrysler Group LLC announced a name change to FCA US LLC.

On January 12, 2017, FCA shares traded at the New York Stock Exchange lost value after the EPA accused FCA US of using emissions cheating software to evade diesel-emissions tests, however the company countered the accusations, and the chairman and CEO Sergio Marchionne sternly rejected them. The following day, shares rose as investors played down the effect of the accusations. Analysts gave estimates of potential fines from several hundred million dollars to $4 billion, although the likelihood of a hefty fine was low. Senior United States Senator Bill Nelson urged the FTC to look into possible deceptive marketing of the company's diesel-powered SUVs. Shares dropped 2.2% after the announcement.

On July 21, 2018, Sergio Marchionne stepped down as chairman and CEO for health reasons, and was replaced by John Elkann and Michael Manley, respectively.

As a result of ending domestic production of more fuel-efficient passenger automobiles such as the Dodge Dart and Chrysler 200 sedans, FCA US elected to pay $77 million in fines for violating the anti-backsliding provision of fuel economy standards set under the Energy Independence and Security Act of 2007 for its model year 2016 fleet. It was again fined for the 2017 model year for not meeting the minimum domestic passenger car standard. FCA described the $79 million civil penalty as "not expected to have a material impact on its business."

As part of a January 2019 settlement, Fiat Chrysler will recall and repair approximately 100,000 automobiles equipped with a 3.0-liter V6 EcoDiesel engine having a prohibited defeat device, pay $311 million in total civil penalties to US regulators and CARB, pay $72.5 million for state civil penalties, implement corporate governance reforms, and pay $33.5 million to mitigate excess pollution. The company will also pay affected consumers up to $280 million and offer extended warranties on such vehicles worth $105 million. The total value of the settlement is worth about $800 million, though FCA did not admit liability, and it did not resolve an ongoing criminal investigation.

, management positions of FCA US include:



Chrysler is the smallest of the "Big Three" U.S. automakers (FCA US, Ford Motor Company, and General Motors). In 2019, Chrysler sold just over 2.2 million vehicles.

Chrysler is the world's 11th largest vehicle manufacturer as ranked by OICA in 2012. Total Chrysler vehicle production was about 2.37 million that year.

In 2007, Chrysler began to offer vehicle lifetime powertrain warranty for the first registered owner or retail lessee. The deal covered owner or lessee in U.S., Puerto Rico and the Virgin Islands, for 2009 model year vehicles, and 2006, 2007 and 2008 model year vehicles purchased on or after July 26, 2007. Covered vehicles excluded SRT models, Diesel vehicles, Sprinter models, Ram Chassis Cab, Hybrid System components (including transmission), and certain fleet vehicles. The warranty is non-transferable. After Chrysler's restructuring, the warranty program was replaced by five-year/100,000 mile transferable warranty for 2010 or later vehicles.

In 2008, as a response to customer feedback citing the prospect of rising gas prices as a top concern, Chrysler launched the "Let's Refuel America" incentive campaign, which guaranteed new-car buyers a gasoline price of $2.99 for three years. With the U.S. purchase of eligible Chrysler, Jeep, and Dodge vehicles, customers could enroll in the program and receive a gas card that immediately lowers their gas price to $2.99 a gallon, and keeps it there for the three years.

Chrysler plans for Lancia to codevelop products, with some vehicles being shared. Olivier Francois, Lancia's CEO, was appointed to the Chrysler division in October 2009. Francois plans to reestablish the Chrysler brand as an upscale brand.

In October 2009, Dodge's car and truck lines were separated, with the name "Dodge" being used for cars, minivans and crossovers and "Ram" for light- and medium-duty trucks and other commercial-use vehicles.<ref name="autoblog.com/2009"></ref>
In 2011, Chrysler unveiled their "Imported From Detroit" campaign with ads featuring Detroit rapper Eminem, one of which aired during the Super Bowl. The campaign highlighted the rejuvenation of the entire product lineup, which included the new, redesigned and repackaged 2011 200 sedan and 200 convertible, the Chrysler 300 sedan and the Chrysler Town & Country minivan. As part of the campaign, Chrysler sold a line of clothing items featuring the Monument to Joe Louis, with proceeds being funneled to Detroit-area charities, including the Boys and Girls Clubs of Southeast Michigan, Habitat for Humanity Detroit and the Marshall Mathers Foundation. Following the Eminem ad, there was also an ad for Detroit Lions defensive tackle Ndamukong Suh driving a Chrysler 300 to Portland, Oregon, to visit his mother, an ad featuring Detroit-born fashion designer John Varvatos cruising through a shadowy Gotham while Kevin Yon's familiar baritone traces the designer's genesis.

In March 2011, Chrysler Group LLC filed a lawsuit against Moda Group LLC (owner of Pure Detroit clothing retailer) for copying and selling merchandise with the "Imported from Detroit" slogan. Chrysler claimed it had notified defendant of its pending trademark application February 14, but the defendant argued Chrysler had not secured a trademark for the "Imported From Detroit" phrase. On June 18, 2011, U.S. District Judge Arthur Tarnow ruled that Chrysler's request did not show that it would suffer irreparable harm or that it had a strong likelihood of winning its case. Therefore, Pure Detroit's owner, Detroit retailer Moda Group LLC, can continue selling its "Imported from Detroit" products. Tarnow also noted that Chrysler does not have a trademark on "Imported from Detroit" and rejected the automaker's argument that trademark law is not applicable to the case. In March 2012, Chrysler Group LLC and Pure Detroit agreed to a March 27 mediation to try to settle the lawsuit over the clothing company's use of "Imported from Detroit" slogan. Pure Detroit stated that Chrysler has made false claims about the origins of three vehicles - Chrysler 200, Chrysler 300 and Chrysler Town & Country - none of which are built in Detroit. Pure Detroit also said that Chrysler's Imported From Detroit merchandise is not being made in Detroit. In 2012 Chrysler and Pure Detroit came to an undisclosed settlement.

Chrysler's Jefferson North Assembly, which makes the Jeep Grand Cherokee and Dodge Durango, is the only car manufacturing plant of any company remaining entirely in Detroit (General Motors operates a plant which is partly in Detroit and partly in Hamtramck).

In 2011, Eminem settled a lawsuit against Audi alleging the defendant had ripped off the Chrysler 300 Super Bowl commercial in the Audi A6 Avant ad.

Again in 2012, Chrysler advertised during the Super Bowl. Its two-minute February 5, 2012 Super Bowl XLVI advertisement was titled "Half Time in America". The ad drew criticism from several leading U.S. conservatives, who suggested that its messaging implied that President Barack Obama deserved a second term and, as such, was political payback for Obama's support for the federal bailout of the company. Asked about the criticism in a "60 Minutes" interview with Steve Kroft, Sergio Marchionne responded "just to rectify the record I paid back the loans at 19.7% Interest. I don't think I committed to do to a commercial on top of that" and characterized the Republican reaction as "unnecessary and out of place".


In 2014, Chrysler started using a new slogan, "America's Import" in ads introducing their all-new 2015 Chrysler 200, targeting foreign automakers from Germany to Japan with such ads (German performance and Japanese quality), and at the ending of selected ads, the advertisement will say, "We Built This", indicating being built in America, instead of overseas.




First introduced as MyGig, Chrysler Uconnect is a system that brings interactive ability to the in-car radio and telemetric-like controls to car settings. As of mid-2015, it is installed in hundreds of thousands of Fiat Chrysler vehicles. It connects to the Internet via the mobile network of AT&T, providing the car with its own IP address. Internet connectivity using any Chrysler, Dodge, Jeep or Ram vehicle, via a Wi-Fi "hot-spot", is also available via Uconnect Web. According to Chrysler LLC, the hotspot range extends approximately from the vehicle in all directions, and combines both Wi-Fi and Sprint's 3G cellular connectivity. Uconnect is available on several current and was available on several discontinued Chrysler models including the current Dodge Dart, Chrysler 300, Aspen, Sebring, Town and Country, Dodge Avenger, Caliber, Grand Caravan, Challenger, Charger, Journey, Nitro, and Ram.

In July 2015, IT security researchers announced a severe security flaw assumed to affect every Chrysler vehicle with Uconnect produced from late 2013 to early 2015. It allows hackers to gain access to the car over the Internet, and in the case of a Jeep Cherokee was demonstrated to enable an attacker to take control not just of the radio, A/C, and windshield wipers, but also of the car's steering, brakes and transmission. Chrysler published a patch that car owners can download and install via a USB stick, or have a car dealer install for them.


The Chrysler brand has mostly been Chrysler's premium brand competing with brands such as Cadillac, Packard, Cord and Lincoln. After the corporation decided to spin Imperial off as a separate brand in 1955 to better compete with Cadillac and Lincoln, Chrysler became the corporation's number two brand, but still offered luxury and near-luxury products. After the Imperial brand was dropped in 1975, Chrysler once again became the top brand.

The Chrysler Town & Country is a station wagon that was manufactured by Chrysler from 1940 to 1942 and from 1945 to 1988 (there was no production during World War II from 1943 to 1945). The Town & Country was also available in four-door sedan, two-door hardtop (no "B" pillar), and convertible body styles from 1947 to 1950 and from 1983 to 1986. The 1988 model year was the last for the Chrysler Town & Country station wagon, after that and partly during one model year (1989), the Town & Country nameplate was off the market until the 1990 model year run when Chrysler re-introduced the Town & Country nameplate as a rebadged variant Chrysler Town & Country minivan.

Chrysler's Town & Country wagon was reintroduced as a four-door station wagon of all steel construction in 1951. It was offered in both Windsor and New Yorker variants through the end of Windsor model production after the 1960 model year, and then in Newport and New Yorker models through 1965. After that, it was a model in its own right, with trim and features which bridged the gap between the two sedan lines. It was distinguished by luxury features including a carpeted loadfloor trimmed with chrome strips, and from 1968 forward, woodgrain paneling on the body sides and tailgate, a feature also associated with somewhat competitive top-shelf station wagons such as the AMC Ambassador, Buick Estate, Oldsmobile Custom Cruiser, Ford Country Squire, and the Mercury Colony Park. Town and Country, however, stood in a class by itself until the last of the full-sized versions of 1977. From 1978, it was sized down and absorbed into the LeBaron series, with a lesser version lacking the more luxurious features and the woodgrain bodyside decals available for a few years in the early 1980s.

Chrysler re-introduced the Town & Country nameplate in calendar year 1989 as a luxury rebadged variant of the Dodge Grand Caravan/Plymouth Grand Voyager minivan for the 1990 model year and continued to sell this incarnation of the Chrysler Town & Country until the end of the 2016 model year when Chrysler reintroduced the Pacifica nameplate for their minivan in calendar year 2016 for the 2017 model year run.

Chrysler introduced their first overhead-valve, high-compression V8 engine in 1951, Displacing 331 cubic inches, it was rated at 180 bhp, 20 more hoursepower than the new-for-1949 Cadillac V8. It was unique as the only American V8 engine designed with hemispherical combustion chambers. After successfully winning Mexican Road Races, the engine was upgraded to 250 bhp by 1955. Although Chrysler didn't have the capital to build a small sporty car (such as the Chevrolet Corvette and the Ford Thunderbird), they decided to build a unique sporting car based on the New Yorker hardtop coupe, that featured a 300-bhp "Hemi" V8. To add to the car's uniqueness, the car was given a grille from the Imperial, and side trim from the less-adorned Windsor. A PowerFlite 2-speed automatic transmission was the only available gearbox. It was marketed as the Chrysler 300, emphasizing the engine's horsepower.

A 1955 restyle by newly-hired Virgil Exner saw a dramatic rise in Chrysler sales, which rose even more in 1957, when the entire line was dramatically restyled a second time with a sloping front end and high-flying tailfins at the rear. Although well-received at first, it soon became apparent that quality control was compromised to get the new cars to market on an accelerated schedule. Sales therefore plummeted in 1958 and 1959 despite improvements in quality. Throughout the mid- and late-1950s, Chryslers were available in top-line New Yorker, mid-line Saratoga, and base Windsor series. Exner's designs for the Chrysler brand in the early 1960s were overblown versions of the late 1950s, which were unhelpful in sales. Exner left his post by 1962, leaving Elwood Engel, a recent transfer from Ford Motor Co, in charge of Chrysler styling.

Although early 1960s Chrysler cars reflected Virgil Exner's exaggerated styling, ELwood Engel's influence was evident as early as 1963, when a restyled, trimmer, boxier Chrysler was introduced. The Windsor and Saratoga series were replaced with the Newport, while New Yorker continued as the top-of-the-line. The Chrysler 300, officially part of the New York line, continued in production through 1965, adding a different letter of the alphabet for each year of production, starting with the 300-B of 1956, through the 300-L of 1965. 1962 saw a "non-letter" 300 which was lower in price but was equipped with downgraded standard equipment. The '65 Chryslers were again dramatically restyled, with a thoroughly modern unit body and larger engines up to 440 cubic inches. They were trim and boxy, with glass-covered headlamps and a swept-back roofline for 2-door hardtop models. Although Chryslers though the 1960s were well-built, quality cars with man innovative features (such as unit bodies and front torsion bar suspension), sales slumped as American buyers bought record numbers of cars from Ford and GM.

The Cordoba was introduced by Chrysler for the 1975 model year as an upscale personal luxury car, competing with the Oldsmobile Cutlass, Buick Regal, and Mercury Cougar. The Cordoba was originally intended to be a Plymouthâthe names Mirada, Premier, Sebring, and Grand Era were associated with the project; all except Grand Era would be used on later Chrysler, Dodge, and Eagle vehicles, though only the Dodge Mirada would be related to the Cordoba. However, losses from the newly introduced full-size C-body models due to the 1973 oil crisis encouraged Chrysler executives to seek higher profits by marketing the model under the more upscale Chrysler brand.

The car was a success, with over 150,000 examples sold in 1975, a sales year that was otherwise dismal for the company. For the 1976 model year, sales increased slightly to 165,000. The mildly revised 1977 version also sold well, with just under 140,000 cars. The success of using the Chrysler nameplate strategy is contrasted to sales of its similar and somewhat cheaper corporate cousin, the Dodge Charger SE. Interiors were more luxurious than the Dodge Charger SE and much more than the top-line standard intermediates (Plymouth Fury, Dodge Coronet) with a velour cloth notchback bench seat and folding armrest standard. Optionally available were bucket seats upholstered in Corinthian leather with a center armrest and cushion, or at extra cost, a center console with floor shifter and storage compartment.

In 1977, Chrysler brought out a new mid-size line of cars called LeBaron (a name previously used for an Imperial model) which included a coupe, sedan, and station wagon.

For 1982, the LeBaron moved to the front-wheel drive Chrysler K platform, where it was the upscale brand's lowest priced offering. It was initially available in just sedan and coupe versions. In early 1982, it was released in a convertible version, bringing to the market the first factory-built open-topped domestic vehicle since the 1976 Cadillac Eldorado. A station wagon version called the Town and Country was added as well. A special Town and Country convertible was also made from 1983 to 1986 in limited quantities (1,105 total), which like the wagon featured simulated wood paneling that made it resemble the original 1940s Town and Country. This model was part of the well-equipped Mark Cross option package for the latter years.

In 1982 the R-body line was discontinued and the New Yorker nameplate transferred to the smaller M-body line. Up to this point, the Chrysler M-body entry had been sold as LeBaron, but that name was moved to a new K-car based FWD line (refer to the Chrysler LeBaron article for information on the 1977-81 M-bodies). Following the nameplate swap, the M-body line was consolidated and simplified. 360 V8 engines were gone, as were coupes and station wagons (the K-car LeBaron's coupe and wagon replaced them). The Fifth Avenue option was still available as a $1,244 option package. It was adapted from the earlier LeBaron's package, with a distinctive vinyl roof, electro-luminescent opera lamps, and a rear fascia adapted from the Dodge Diplomat. Interiors featured button-tufted, pillow-soft seats covered in either "Kimberley velvet" or "Corinthian leather", choices that would continue unchanged throughout the car's run. In addition, the carpet was thicker than that offered in the base New Yorker, Diplomat and Gran Fury/Caravelle Salon, and the interior had more chrome trim.

1983 was the last year for Chrysler's Cordoba coupe. Also in 1983, Chrysler introduced a new front-wheel drive New Yorker model based on a stretched K-Car platform. Additionally, a less expensive, less equipped version of the new New Yorker was sold as the Chrysler E-Class in 1983 and 1984. More upscale stretched K-Car models were also sold as Chrysler Executive sedans and limousines.

For 1984, the New Yorker Fifth Avenue was now simply called Fifth Avenue, setting the name that would continue for six successful years. All Fifth Avenues from 1984 to 1989 were powered by a 5.2Â L (318Â inÂ³) V8 engine, with either a two barrel carburetor making (in all states except California) or a four barrel rated at (in California), mated to Chrysler's well-known Torqueflite three speed automatic transmission. Fifth Avenue production was moved from Windsor, Ontario to St. Louis, Missouri. Beginning in late 1986 through the 1989 model year, they were manufactured at the American Motors plant in Kenosha, Wisconsin (purchased by Chrysler in 1987). The Fifth Avenue also far outsold its Dodge Diplomat and Plymouth Gran Fury siblings, with a much greater proportion of sales going to private customers, despite its higher price tag. Production peaked at 118,000 cars for 1986 and the Fifth Avenue stood out in a by-now K-car dominated lineup as Chrysler's lone concession to traditional RWD American sedans.

Chrysler introduced a new mid-size five door hatchback model for 1985 under the LeBaron GTS nameplate. It was sold alongside the mid-size LeBaron sedan, coupe, convertible, and station wagon. The LeBaron coupe and convertible were redesigned for 1987. Unlike previous LeBarons, this new coupe and convertible had unique styling instead of being just two-door versions of the sedan. The new design featured hidden headlamps (through 1992) and full width taillights.

The New Yorker was redesigned for the 1988 model year and now included a standard V6 engine. This generation New Yorker also saw the return of hidden headlamps which had not been available on the New Yorker since the 1981 R-body version. In 1989, Chrysler brought out the TC by Maserati luxury roadster as a more affordable alternative to Cadillac's Allante. It was a joint venture model between Chrysler and Maserati.

Chrysler re-introduced the Town & Country nameplate in calendar year 1989 as a luxury rebadged variant of the Dodge Grand Caravan/Plymouth Grand Voyager minivan for the 1990 model year and continued to sell this incarnation of the Chrysler Town & Country until the end of the 2016 model year when Chrysler reintroduced the Pacifica nameplate for their minivan in calendar year 2016 for the 2017 model year run. 1990 saw the previous relationship between New Yorker and Fifth Avenue return, as the Fifth Avenue became a model of the New Yorker. There was some substantive difference, however, as the New Yorker Fifth Avenue used a slightly longer chassis than the standard car. The new New Yorker Fifth Avenue's larger interior volume classified it as a full-size model this time; despite having smaller exterior dimensions than the first generation. For 1990, Chrysler's new 3.3-liter V6 engine was the standard and only choice, teamed with the company's A-604 four-speed electronic automatic transaxle. Beginning in 1991, a larger 3.8-liter V-6 became optional. It delivered the same 147 horsepower as the 3.3, but had more torque.

The New Yorker Fifth Avenue's famous seats, long noted for their button-tufted appearance and sofa-like comfort, continued to be offered with the customer's choice of velour or leather, with the former "Corinthian leather" replaced by that of the Mark Cross company. Leather-equipped cars bore the Mark Cross logo on the seats and, externally, on an emblem attached to the brushed aluminum band ahead of the rear door opera windows. In this form, the New Yorker Fifth Avenue resembled the newly revived Chrysler Imperial, although some much-needed distinction was provided between the cars when the New Yorker Fifth Avenue (along with its New Yorker Salon linemate) received restyled, rounded-off front and rear ends for the 1992 model year, while the Imperial continued in its original crisply-lined form.

The early 1990s saw a revival of the Imperial as a high-end sedan in Chrysler's lineup. Unlike the 1955â1983 Imperial, this car was a model of Chrysler, not its own marque. Based on the Y platform, it represented the top full-size model in Chrysler's lineup; below it was the similar New Yorker Fifth Avenue, and below that was the shorter wheelbase New Yorker. The reintroduction of the Imperial was two years after the Lincoln Continental was changed to a front-wheel drive sedan with a V6 engine. Other domestic competitors in this segment included the Cadillac Sedan de Ville/Fleetwood, Oldsmobile 98 and Buick Electra/Park Avenue. Though closely related, the Imperial differed from the New Yorker Fifth Avenue in many ways. The Imperial's nose was more wedge-shaped, while the New Yorker Fifth Avenue's had a sharper, more angular profile (the New Yorker Fifth Avenue was later restyled with a more rounded front end). The rears of the two cars also differed. Like the front, the New Yorker Fifth Avenue's rear came to stiffer angles while the Imperial's rear-end came to more rounded edges. Also found on the Imperial were full-width taillights which were similar to those of the Chrysler TC, as well as the early 1980s Imperial coupe, while the New Yorker Fifth Avenue came with smaller vertical taillights.

Initially, the 1990 Imperial was powered by the 3.3 L "EGA" V6 engine, which was rated at of torque. For 1991, the 3.3 L V6 was replaced by the larger 3.8 L "EGH" V6. Although horsepower only increased to , with the new larger 3.8 L V6 torque increased to at 2750 rpm. A four-speed automatic transmission was standard with both engines.

Also new for 1990 was a redesigned LeBaron sedan which offered a standard V6 engine. Later models would also be available with 4 cylinder engines.

The Town & Country minivan was restyled for 1991 in conjunction with the restyling of the Dodge and Plymouth minivan models. 1991 would also be the last year for the TC by Maserati, leaving the LeBaron as the brand's sole coupe and convertible options.

The first generation of the Chrysler Concorde debuted at the 1992 North American International Auto Show in Detroit as a 1993 model. It debuted as a single, well-equipped model with a base price of US$18,341. Out of all the LH sedans, the first generation Concorde was most closely related to the Eagle Vision. The Concorde was given a more traditional image than the Vision. The two shared nearly all sheetmetal in common with the main differences limited to their grilles, rear fascias, body side moldings, and wheel choices. The Concorde featured a modern take on Chrysler's signature waterfall grille. It was split into six sections divided by body colored strips with the Chrysler Pentastar logo on the center strip. The Concorde's rear fascia was highlighted by a full-width and full-height lightbar between the taillights, giving the appearance that the taillights stretched across the entire trunk. In keeping with its upscale position, Concorde's body side moldings incorporated bright chrome (later golden colored) work not found on its Dodge or Eagle siblings. On Concordes with gray lower body paint color, the gray came all the way up to the chrome beltline; on Visions the gray lower body paint area was smaller and much more subtle. Wheel styles, which included available aluminum wheels with a Spiralcast design, were also unique to the Chrysler LH sedans (Concorde, LHS, New Yorker); Dodge and Eagle had their own different wheel styles.

Introduced in May 1993 for the 1994 model year, the Chrysler LHS was the top of the line model for the division, as well as the most expensive of the Chrysler LH platform cars. All the LH-series models shared a wheelbase and were developed using Chrysler's new computer drafting system. The car was differentiated from the division's New Yorker sedan by its bucket leather seats (the New Yorker had a bench seat) and standard features such as alloy wheels that were options on the New Yorker. Further differences between the Chrysler LHS and its New Yorker counterpart were a floor console and shifter, five-passenger seating, lack of chrome trim, an upgraded interior and a sportier image. The New Yorker was dropped after the 1996 model year in favor of a six-passenger option on the LHS. The LHS received a minor face change in 1995 when the corporate wide pentastar emblem was replaced with the revived Chrysler brand emblem. Standard features of the LHS included a 3.5 L EGE 24-valve V6 engine, body-colored grille, side mirrors and trim, traction control, aluminum wheels, integrated fog lights, 8-way power adjustable front seats, premium sound systems with amplifiers, and automatic temperature control. Unlike the New Yorker, leather seats were standard.

The final generation of the New Yorker continued with front-wheel drive on an elongated version of the new Chrysler LH platform and was released in May 1993 along with the nearly identical Chrysler LHS as an early 1994 model, eight months after the original LH cars: the Chrysler Concorde, Dodge Intrepid, and Eagle Vision, were introduced. The New Yorker came standard with the 3.5Â L "EGE" which produced . Chrysler gave the New Yorker a more "traditional American" luxury image, and the LHS a more European performance image (as was done with the Eagle Vision). Little separated New Yorker from LHS in appearance, with New Yorker's chrome hood trim, body-color cladding, standard chrome wheel covers and 15" wheels, column shifter and front bench seat, being the only noticeable differences. An option provided for 16" wheels and a firmer suspension type ("touring suspension"). This option eliminated the technical differences between New Yorker and LHS. LHS came with almost all of New Yorker's optional features as standard equipment and featured the firmer tuned suspension, to go with its more European image.

During the 1994 model run, various changes were made to the New Yorker. On the outside, New Yorker was switched to new accent-color body cladding, whereas LHS received body-color cladding. This change aligned New Yorker with the Chrysler Concorde which also had accent-color cladding. Instead of standard 15" and optional 16" wheels, for the sake of enhanced stability 16" wheels became standard and the 15" wheels were dropped. Likewise, the touring suspension option available on early 1994 New Yorker models was discontinued, leaving only "ride-tuned" suspension.

In 1995, the Chrysler Sebring was introduced as a coupe, replacing the LeBaron coupe, and the new JA platform Chrysler Cirrus replaced the outgoing LeBaron sedan. A year later, a convertible version of the Sebring went on the market and replaced the LeBaron convertible. In 1999, Chrysler introduced the new LH platform 300M sedan alongside a redesigned LHS. The 300M was originally designed to be the next generation Eagle Vision but since the Eagle brand had been discontinued in 1998, it instead became a Chrysler sedan.

In 2000, the Voyager and Grand Voyager minivans were repositioned as Chrysler models due to the phasing out of the Plymouth brand. In 2001, a sedan was added to the Sebring model line and served as a replacement for the discontinued Cirrus. That same year, the Chrysler brand added a retro-styled PT Cruiser as well as the Prowler roadster which had previously been a Plymouth model. By 2004, all Chrysler brand minivans were now sold under the Town & Country nameplate.

The 2000s also saw the Chrysler brand move into the fast growing crossover/SUV segment with the introduction of the Chrysler Pacifica crossover in 2004, and the Chrysler Aspen SUV in 2007. The Pacifica would be discontinued in 2008 (the nameplate would return on a new minivan model in 2017) and the Aspen would be discontinued in 2009.

Between 2004 and 2008, Chrysler offered a two-seat coupe and convertible model called Crossfire. This was in addition to Chrysler's five-seat Sebring coupe (through 2005) and four-seat convertible being sold at the time.

In 2005, Chrysler introduced the LX platform Chrysler 300 sedan which replaced both the 300M and Concorde. It was the brand's first rear-wheel drive sedan since the discontinuation of the Chrysler Fifth Avenue in 1989. It was also the first time a Chrysler sedan was available with a V8 engine since 1989.

Following FCA's acquisition of Chrysler, FCA set a long-term goal of reviving the Chrysler brand as a full luxury brand to compete again with Cadillac and other luxury brands. The company stated in October 2009 that future plans for Chrysler brand vehicles include closer cooperation and shared development between Chrysler and Lancia, an upscale Italian automaker within the Fiat Group. In 2011, the brand's winged emblem was modified, eliminating the historic blue ribbon center which dated from the 1930s, replacing it with a blue-backed "Chrysler" nameplate. Also that year, the Chrysler 300 was restyled and the Sebring was rebranded as the Chrysler 200. In May 2014, FCA announced it would make the brand a mainstream brand with premium features. A redesigned Chrysler 200 was introduced for 2015 as a sedan only, but would be discontinued in 2017 as FCA shifted focus more towards SUVs and minivans. For 2017, the Chrysler Pacifica nameplate returned on a new minivan, replacing the long-running Town & Country.

On June 27, 2019, FCA announced that the low-end "L" and "LX" models would be separated from the Pacifica line and sold under the Voyager nameplate starting with the 2020 model year. Additionally, a fleet-only Voyager "LXi" version would be added.

The brand's current lineup consists of the Chrysler 300, Chrysler Pacifica and Chrysler Voyager.

In 2010, Fiat Auto was planning to sell seven of its vehicles in the U.S. by 2014, while Fiat-controlled Chrysler Group was to supply nine models to sell under Fiat brands in the European market, according to a five-year plan rolled out on April 21, 2010 in Turin, Italy, by Fiat and Chrysler CEO Sergio Marchionne. At least five of the Fiat Auto models were expected to be marketed in the U.S. under its Alfa Romeo brand. Showing the level of integration envisioned, a product introduction timeline envisaged Chrysler-built compact and full-size SUVs going on sale in 2012 and 2014, respectively, in both European and North American markets.

Chrysler's quality and customer satisfaction ratings have been below average according to Consumer Reports and JD Powers since the late 1990s. Consumer Reports has consistently reported Chrysler brands at the bottom of their reliability ratings in the past decade as well as their Automotive Brand Report Card. JDP has found similar results over the same time period in both Initial Quality Studies and Customer Service Indexes as has the American Customer Satisfaction Index survey. Chrysler has had a few quality successes during this period. Strategic Vision named Chrysler an overall winner in 2015 noting strong customer appeal and that with the rise in quality of all cars the difference between high and low "problem-counting" ratings are relatively small.

Chrysler produced an experimental electric vehicle in 1979, the company developed Chrysler ETV-1 electric prototype in cooperation with U.S. Department of Energy.

In 1992, Chrysler developed the Dodge EPIC concept minivan. In 1993, Chrysler began to sell a limited-production electric minivan called the TEVan; however only 56 were produced. In 1997, a second generation, called the EPIC, was released. It was discontinued after 1999.

Chrysler once owned the Global Electric Motorcars company, building low-speed neighborhood electric vehicles, but sold GEM to Polaris Industries in 2011.

In September 2007, Chrysler established ENVI, an in-house organization focused on electric-drive vehicles and related technologies which was disbanded by late 2009. In August 2009, Chrysler took US$70 million in grants from the U.S. Department of Energy to develop a test fleet of 220 hybrid pickup trucks and minivans.

The first hybrid models, the Chrysler Aspen hybrid and the Dodge Durango hybrid, were discontinued a few months after production in 2008, sharing their GM-designed hybrid technology with GM, Daimler and BMW.

Chrysler is on the Advisory Council of the PHEV Research Center, and undertook a government sponsored demonstration project with Ram and minivan vehicles.

In 2012, FCA CEO Sergio Marchionne said that Chrysler and Fiat both plan to focus primarily on alternative fuels, such as CNG and Diesel, instead of hybrid and electric drivetrains for their consumer products.

Fiat Chrysler bought 8.2 million megagrams of U.S. greenhouse gas emission credits from competitors including Toyota, Honda, Tesla and Nissan. It had the worst fleet average fuel economy among major manufacturers selling in the US from model years 2012â2017.

The dedicated tank building division of Chrysler, this division was founded as the Chrysler Tank division in 1940, originally with the intention of providing another production line for the M2 Medium Tank, so that the U.S. Army could more rapidly build up its inventory of the type. Its first plant was the Detroit Arsenal Tank Plant. When the M2A1 was unexpectedly declared obsolete in August of the same year, plans were altered (though not without considerable difficulty) to produce the M3 Grant instead, primarily for the British as part of the United States under the counter support for Great Britain against Nazi Germany (the U.S. not yet being formally in the war), with the balance of the revised order going to the U.S. Army as the "Lee". After December 1941 and the United States' entry into the war against the Axis powers, the Tank division rapidly expanded, with new facilities such as the Tank Arsenal Proving Ground at (then) Utica, Michigan. It also quickly widened the range of products it was developing and producing, including the M4 Sherman tank and the Chrysler A57 multibank tank engine.

During World War II, essentially all of Chrysler's facilities were devoted to building military vehicles (the Jeep brand came later, after Chrysler acquired American Motors Corporation). They were also designing V12 and V16 hemi-engines producing for airplanes, but they did not make it into production as jets were developed and were seen as the future for air travel. During the 1950s Cold War period, Chrysler made air raid sirens powered by its Hemi V-8 engines.

When the Radiation Laboratory at MIT was established in 1941 to develop microwave radars, one of the first projects resulted in the SCR-584, the most widely recognized radar system of the war era. This system included a parabolic antenna six feet in diameter that was mechanically aimed in a helical pattern (round and round as well as up and down).

One of Chrysler's most significant contributions to the war effort was not in the field of vehicles but in the radar field. For the final production design of this antenna and its highly complex drive mechanism, the Army's Signal Corps Laboratories turned to Chrysler's Central Engineering Office. There, the parabola was changed from aluminum to steel, allowing production forming using standard automotive presses. To keep weight down, 6,000 equally spaced holes were drilled in the face (this had no effect on the radiation pattern). The drive mechanism was completely redesigned, using technology derived from Chrysler's research in automotive gears and differentials. The changes resulted in improved performance, reduced weight, and easier maintenance. A large portion of the Dodge plant was used in building 1,500 of the SCR-584 antennas as well as the vans used in the systems.


In April 1950, the U.S. Army established the Ordnance Guided Missile Center (OGMC) at Redstone Arsenal, adjacent to Huntsville, Alabama. To form OGMC, over 1,000 civilian and military personnel were transferred from Fort Bliss, Texas. Included was a group of German scientists and engineers led by Wernher von Braun; this group had been brought to America under Project Paperclip. OGMC designed the Army's first short-range ballistic missile, the PGM-11 Redstone, based on the WWII German V-2 missile. Chrysler established the Missile Division to serve as the Redstone prime contractor, setting up an engineering operation in Huntsville and for production obtaining use from the U.S. Navy of a large plant in Sterling Heights, Michigan. The Redstone was in active service from 1958 to 1964; it was also the first missile to test-launch a live nuclear weapon, first detonated in a 1958 test in the South Pacific.

Working together, the Missile Division and von Braun's team greatly increased the capability of the Redstone, resulting in the PGM-19 Jupiter, a medium-range ballistic missile. In May 1959, a Jupiter missile launched two small monkeys into space in a nose cone; this was America's first successful flight and recovery of live space payloads. Responsibility for deploying Jupiter missiles was transferred from the Army to the Air Force; armed with nuclear warheads, they were first deployed in Italy and Turkey during the early 1960s.

In July 1959, NASA chose the Redstone missile as the basis for the Mercury-Redstone Launch Vehicle to be used for suborbital test flights of the Project Mercury spacecraft. Three unmanned MRLV launch attempts were made between November 1960 and March 1961, two of which were successful. The MRLV successfully launched the chimpanzee Ham, and astronauts Alan Shepard and Gus Grissom on three suborbital flights in January, May and July 1961, respectively.

America's more ambitious manned space travel plans included the design of the Saturn series of heavy-lift launch vehicles by a team headed by Wernher von Braun. Chrysler's Huntsville operation, then designated the Space Division, became Marshall Space Flight Center's prime contractor for the first stage of the Saturn I and Saturn IB versions. The design was based on a cluster of Redstone and Jupiter fuel tanks, and Chrysler built it for the Apollo program in the Michoud Assembly Facility in East New Orleans, one of the largest manufacturing plants in the world. Between October 1961 and July 1975, NASA used ten Saturn Is and nine Saturn IBs for suborbital and orbital flights, all of which were successful; Chrysler missiles and boosters never suffered a launch failure. The division was also a subcontractor which modified one of the Mobile Launcher Platforms for use with the Saturn IB rockets using Saturn V infrastructure.





</doc>
<doc id="6883" url="https://en.wikipedia.org/wiki?curid=6883" title="City of London">
City of London

The City of London is a city, county and a local government district that contains the historic centre and the primary central business district (CBD) of London. It constituted most of London from its settlement by the Romans in the 1st century AD to the Middle Ages, but the agglomeration has since grown far beyond the City's borders. The City is now only a tiny part of the metropolis of London, though it remains a notable part of central London. Administratively, it forms one of the 33 local authority districts of Greater London; however, the City of London is not a London borough, a status reserved for the other 32 districts (including London's only other city, the City of Westminster). It is also a separate ceremonial county, being an enclave surrounded by Greater London, and is the smallest county in the United Kingdom.

The City of London is widely referred to simply as the City (differentiated from the phrase "the city of London" by capitalising "City") and is also colloquially known as the Square Mile, as it is in area. Both of these terms are also often used as metonyms for the United Kingdom's trading and financial services industries, which continue a notable history of being largely based in the City. The name "London" is now ordinarily used for a far wider area than just the City. "London" most often denotes the sprawling London metropolis, or the 32 London boroughs, in addition to the City of London itself. This wider usage of "London" is documented as far back as 1888, when the County of London was created.

The local authority for the City, namely the City of London Corporation, is unique in the UK and has some unusual responsibilities for a local council, such as being the police authority. It is also unusual in having responsibilities and ownerships beyond its boundaries. The Corporation is headed by the Lord Mayor of the City of London, an office separate from (and much older than) the Mayor of London. The Lord Mayor, as of November 2019, is William Russel.

The City is a major business and financial centre. Throughout the 19th century, the City was the world's primary business centre, and it continues to be a major meeting point for businesses. London came top in the Worldwide Centres of Commerce Index, published in 2008. The insurance industry is focused around the eastern side of the City, around Lloyd's building. A secondary financial district exists outside the City, at Canary Wharf, to the east.

The City has a resident population of 9,401 (ONS estimate, mid-2016) but over 500,000 are employed there, and some estimates put the number of workers in the city to be over 1 million. About three-quarters of the jobs in the City of London are in the financial, professional, and associated business services sectors. The legal profession forms a major component of the northern and western sides of the City, especially in the Temple and Chancery Lane areas where the Inns of Court are located, of which twoâInner Temple and Middle Templeâfall within the City of London boundary.

Known as "Londinium", the Roman legions established a settlement on the current site of the City of London around AD 43. Its bridge over the River Thames turned the city into a road nexus and major port, serving as a major commercial centre in Roman Britain until its abandonment during the 5th century. Archaeologist Leslie Wallace notes that, because extensive archaeological excavation has not revealed any signs of a significant pre-Roman presence, "arguments for a purely Roman foundation of London are now common and uncontroversial."

At its height, the Roman city had a population of approximately 45,000â60,000 inhabitants. Londinium was an ethnically diverse city, with inhabitants from across the Roman Empire, including natives of Britannia, continental Europe, the Middle East, and North Africa. The Romans built the London Wall some time between AD 190 and 225. The boundaries of the Roman city were similar to those of the City of London today, though the City extends further west than Londonium's Ludgate, and the Thames was undredged and thus wider than it is today, with Londonium's shoreline slightly north of the City's present shoreline. The Romans built a bridge across the river, as early as AD 50, near to today's London Bridge.

By the time the London Wall was constructed, the City's fortunes were in decline, and it faced problems of plague and fire. The Roman Empire entered a long period of instability and decline, including the Carausian Revolt in Britain. In the 3rd and 4th centuries, the city was under attack from Picts, Scots, and Saxon raiders. The decline continued, both for Londinium and the Empire, and in AD 410 the Romans withdrew entirely from Britain. Many of the Roman public buildings in Londinium by this time had fallen into decay and disuse, and gradually after the formal withdrawal the city became almost (if not, at times, entirely) uninhabited. The centre of trade and population moved away from the walled Londinium to Lundenwic ("London market"), a settlement to the west, roughly in the modern day Strand/Aldwych/Covent Garden area.

During the Anglo-Saxon Heptarchy, the London area came in turn under the Kingdoms of Essex, Mercia, and later Wessex, though from the mid 8th century it was frequently under the control or threat of the Vikings.
Bede records that in AD 604 St Augustine consecrated Mellitus as the first bishop to the Anglo-Saxon kingdom of the East Saxons and their king, SÃ¦berht. SÃ¦berht's uncle and overlord, Ãthelberht, king of Kent, built a church dedicated to St Paul in London, as the seat of the new bishop. It is assumed, although unproven, that this first Anglo-Saxon cathedral stood on the same site as the later medieval and the present cathedrals.

Alfred the Great, King of Wessex (and arguably the first king of the "English") occupied and began the resettlement of the old Roman walled area, in 886, and appointed his son-in-law Earl Ãthelred of Mercia over it as part of their reconquest of the Viking occupied parts of England. The refortified Anglo-Saxon settlement was known as Lundenburh ("London Fort", a borough). The historian Asser said that "Alfred, king of the Anglo-Saxons, restored the city of London splendidlyÂ ... and made it habitable once more." Alfred's "restoration" entailed reoccupying and refurbishing the nearly deserted Roman walled city, building quays along the Thames, and laying a new city street plan.

Alfred's taking of London and the rebuilding of the old Roman city was a turning point in history, not only as the permanent establishment of the City of London, but also as part of a unifying moment in early England, with Wessex becoming the dominant English kingdom and the repelling (to some degree) of the Viking occupation and raids. While London, and indeed England, were afterwards subjected to further periods of Viking and Danish raids and occupation, the establishment of the City of London and the Kingdom of England prevailed.

In the 10th century, Athelstan permitted eight mints to be established, compared with six in his capital, Winchester, indicating the wealth of the city. London Bridge, which had fallen into ruin following the Roman evacuation and abandonment of Londinium, was rebuilt by the Saxons, but was periodically destroyed by Viking raids and storms.

As the focus of trade and population was moved back to within the old Roman walls, the older Saxon settlement of Lundenwic was largely abandoned and gained the name of "Ealdwic" (the "old settlement"). The name survives today as Aldwych (the "old market-place"), a name of a street and an area of the City of Westminster between Westminster and the City of London.

Following the Battle of Hastings, William the Conqueror marched on London "(reaching as far as Southwark)", but failed to get across London Bridge or to defeat the Londoners. He eventually crossed the River Thames at Wallingford, pillaging the land as he went. Rather than continuing the war, Edgar the Ãtheling, Edwin of Mercia and Morcar of Northumbria surrendered at Berkhamsted. William granted the citizens of London a charter in 1075; the City was one of a few examples of the English retaining some authority. The City was not covered by the Domesday Book.

William built three castles nearby, to keep Londoners subdued:


About 1130, Henry I granted a sheriff to the people of London, along with control of the county of Middlesex: this meant that the two entities were regarded as one administratively (not that the county was a dependency of the City) until the Local Government Act 1888. By 1141 the whole body of the citizenry was considered to constitute a single community. This 'commune' was the origin of the City of London Corporation and the citizens gained the right to appoint, with the king's consent, a mayor in 1189âand to directly elect the mayor from 1215.

From medieval times, the City has been composed of 25 ancient wards, each headed by an alderman, who chairs Wardmotes, which still take place at least annually. A Folkmoot, for the whole of the City held at the outdoor cross of St Paul's Cathedral, was formerly also held. Many of the medieval offices and traditions continue to the present day, demonstrating the unique nature of the City and its Corporation.

In 1381, the Peasants' Revolt affected London. The rebels took the City and the Tower of London, but the rebellion ended after its leader, Wat Tyler, was killed during a confrontation that included Lord Mayor William Walworth.

The City was burnt severely on a number of occasions, the worst being in 1123 and (more famously) in the Great Fire of London in 1666. Both of these fires were referred to as "the" Great Fire. After the fire of 1666, a number of plans were drawn up to remodel the City and its street pattern into a renaissance-style city with planned urban blocks, squares and boulevards. These plans were almost entirely not taken up, and the medieval street pattern re-emerged almost intact.

By the late 16th century, London increasingly became a major centre for banking, international trade and commerce. The Royal Exchange was founded in 1565 by Sir Thomas Gresham as a centre of commerce for London's merchants, and gained Royal patronage in 1571. Although no longer used for its original purpose, its location at the corner of Cornhill and Threadneedle Street continues to be the geographical centre of the City's core of banking and financial services, with the Bank of England moving to its present site in 1734, opposite the Royal Exchange on Threadneedle Street. Immediately to the south of Cornhill, Lombard Street was the location from 1691 of Lloyd's Coffee House, which became the world-leading insurance market. London's insurance sector continues to be based in the area, particularly in Lime Street.

In 1708, Christopher Wren's masterpiece, St Paul's Cathedral, was completed on his birthday. The first service had been held on 2 December 1697, more than 10 years earlier. It replaced the original St Paul's, which had been completely destroyed in the Great Fire of London, and is considered to be one of the finest cathedrals in Britain and a fine example of Baroque architecture.

The 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire. The urban area expanded beyond the borders of the City of London, most notably during this period towards the West End and Westminster.

Expansion continued and became more rapid by the beginning of the 19th century, with London growing in all directions. To the East the Port of London grew rapidly during the century, with the construction of many docks, needed as the Thames at the City could not cope with the volume of trade. The arrival of the railways and the Tube meant that London could expand over a much greater area. By the mid-19th century, with London still rapidly expanding in population and area, the City had already become only a small part of the wider metropolis.

An attempt was made in 1894 with the Royal Commission on the Amalgamation of the City and County of London to end the distinction between the City and the surrounding County of London, but a change of government at Westminster meant the option was not taken up. The City as a distinct polity survived despite its position within the London conurbation and numerous local government reforms. Supporting this status, the City was a special parliamentary borough that elected four members to the unreformed House of Commons, who were retained after the Reform Act 1832; reduced to two under the Redistribution of Seats Act 1885; and ceased to be a separate constituency under the Representation of the People Act 1948. Since then the City is a minority (in terms of population and area) of the Cities of London and Westminster.
The City's population fell rapidly in the 19th century and through most of the 20th century, as people moved outwards in all directions to London's vast suburbs, and many residential buildings were demolished to make way for office blocks. Like many areas of London and other British cities, the City fell victim to large scale and highly destructive aerial bombing during World War II, especially in the Blitz. Whilst St Paul's Cathedral survived the onslaught, large swathes of the area did not and the particularly heavy raids of late December 1940 led to a firestorm called the Second Great Fire of London.

There was a major rebuilding programme in the decades following the war, in some parts (such as at the Barbican) dramatically altering the urban landscape. But the destruction of the older historic fabric allowed the construction of modern and larger-scale developments, whereas in those parts not so badly affected by bomb damage the City retains its older character of smaller buildings. The street pattern, which is still largely medieval, was altered slightly in places, although there is a more recent trend of reversing some of the post-war modernist changes made, such as at Paternoster Square.

The City suffered terrorist attacks including the 1993 Bishopsgate bombing (IRA) and the 7 July 2005 London bombings (Islamist). In response to the 1993 bombing, a system of road barriers, checkpoints and surveillance cameras referred to as the "ring of steel" has been maintained to control entry points to the City.

The 1970s saw the construction of tall office buildings including the 600-foot (183Â m), 47-storey Natwest Tower, the first skyscraper in the UK. Office space development has intensified especially in the central, northern and eastern parts, with skyscrapers including 30 St. Mary Axe ("the Gherkin"'), Leadenhall Building ("the Cheesegrater"), 20 Fenchurch Street ("the Walkie-Talkie"), the Broadgate Tower and the Heron Tower, the tallest in the City. Another skyscraper, 22 Bishopsgate, is under construction.

The main residential section of the City today is the Barbican Estate, constructed between 1965 and 1976. The Museum of London is based there, as are a number of other services provided by the Corporation.

"For a history of the etymology behind the City's streets see: Street names of the City of London"

The City has a unique political status, a legacy of its uninterrupted integrity as a corporate city since the Anglo-Saxon period and its singular relationship with the Crown. Historically its system of government was not unusual, but it was not reformed by the Municipal Reform Act 1835 and little changed by later reforms.

It is administered by the City of London Corporation, headed by the Lord Mayor of London ("not the same as the more recent Mayor of London"), which is responsible for a number of functions and has interests in land beyond the City's boundaries. Unlike other English local authorities, the Corporation has two council bodies: the (now largely ceremonial) Court of Aldermen and the Court of Common Council. The Court of Aldermen represents the wards, with each ward (irrespective of size) returning one Alderman. The chief executive of the Corporation holds the ancient office of Town Clerk of London.

The City is a ceremonial county which has a Commission of Lieutenancy headed by the Lord Mayor instead of a Lord-Lieutenant and has two Sheriffs instead of a High Sheriff (see list of Sheriffs of London), quasi-judicial offices appointed by the Livery Companies, an ancient political system based on the representation and protection of trades (Guilds). Senior members of the Livery Companies are known as Liverymen and form the Common Hall, which chooses the Lord Mayor, the Sheriffs and certain other officers.

The City is made up of . They are survivors of the medieval government system that allowed a very local area to exist as a self-governing unit within the wider city. They can be described as electoral/political divisions; ceremonial, geographic and administrative entities; sub-divisions of the City. Each ward has an Alderman, who until the mid-1960s held office for life but since put themselves up for re-election at least every 6 years. Wards continue to have a Beadle, an ancient position which is now largely ceremonial whose main remaining function is the running of an annual Wardmote of electors, representatives and officials. At the Wardmote the ward's Alderman appoints at least one Deputy for the year ahead. Each ward also has a Ward Club, which is similar to a residents' association.

The wards are ancient and their number has changed three times since time immemorial

Following boundary changes in 1994, and later reform of the business vote in the City, there was a major boundary and electoral representation revision of the wards in 2003, and they were reviewed again in 2010 for change in 2013, though not to such a dramatic extent. The review was conducted by senior officers of the Corporation and senior judges of the Old Bailey; the wards are reviewed by this process to avoid malapportionment. The procedure of review is unique in the United Kingdom as it is not conducted by the Electoral Commission or a local government boundary commission every 8 to 12 years, which is the case for all other wards in Great Britain. Particular churches, livery company halls and other historic buildings and structures are associated with a ward, such as St Paul's Cathedral with Castle Baynard, and London Bridge with Bridge; boundary changes in 2003 removed some of these historic connections.

Each ward elects an Alderman to the Court of Aldermen, and Commoners (the City equivalent of a Councillor) to the Court of Common Council of the Corporation. Only electors who are Freemen of the City of London are eligible to stand. The number of Commoners a ward sends to the Common Council varies from two to ten, depending on the number of electors in each ward. Since the 2003 review it has been agreed that the four more residential wards: Portsoken, Queenhithe, Aldersgate and Cripplegate together elect 20 of the 100 Commoners, whereas the business-dominated remainder elect the remaining 80 Commoners. 2003 and 2013 boundary changes have increased the residential emphasis of the mentioned four wards.

Census data provides eight nominal rather than 25 real wards, all of varying size and population. Being subject to renaming and definition at any time, these census 'wards' are notable in that four of the eight wards accounted for 67% of the 'square mile' and held 86% of the population, and these were in fact similar to and named after four City of London wards:

The City has a unique electoral system. Most of its voters are representatives of businesses and other bodies that occupy premises in the City. Its ancient wards have very unequal numbers of voters. In elections, both the businesses based in the City and the residents of the City vote.

The City of London Corporation was not reformed by the Municipal Corporations Act 1835, because it had a more extensive electoral franchise than any other borough or city; in fact, it widened this further with its own equivalent legislation allowing one to become a freeman without being a liveryman. In 1801, the City had a population of about 130,000, but increasing development of the City as a central business district led to this falling to below 5,000 after the Second World War. It has risen slightly to around 9,000 since, largely due to the development of the Barbican Estate. In 2009, the business vote was about 24,000, greatly exceeding residential voters. As the City of London Corporation has not been affected by other municipal legislation over the period of time since then, its electoral practice has become increasingly anomalous. Uniquely for city or borough elections, its elections remain independent-dominated.

The business or "non-residential vote" was abolished in other UK local council elections by the Representation of the People Act 1969, but was preserved in the City of London. The principal reason given by successive UK governments for retaining this mechanism for giving businesses representation, is that the City is "primarily a place for doing business". About 330,000 non-residents constitute the day-time population and use most of its services, far outnumbering residents, who number around 7,000 (2011). By contrast, opponents of the retention of the business vote argue that it is a cause of institutional inertia.

The City of London (Ward Elections) Act 2002, a private Act of Parliament, reformed the voting system and greatly increased the business franchise, allowing many more businesses to be represented. Under the new system, the number of non-resident voters has doubled from 16,000 to 32,000. Previously disenfranchised firms (and other organisations) are entitled to nominate voters, in addition to those already represented, and all such bodies are now required to choose their voters in a representative fashion. Bodies employing fewer than ten people may appoint one voter; those employing ten to 50 people one voter for every five employees; those employing more than 50 people ten voters and one additional voter for each 50 employees beyond the first 50. The Act also removed other anomalies which had been unchanged since the 1850s.

Inner Temple and Middle Temple (which neighbour each other) are two of the few remaining liberties, an old name for a geographic division. They are independent extra-parochial areas, historically not governed by the City of London Corporation (and are today regarded as local authorities for most purposes) and equally outside the ecclesiastical jurisdiction of the Bishop of London. They are within the boundaries and liberties of the City, but can be thought of as independent enclaves. They are both part of Farringdon Without.

Within the City, the Corporation owns and runs both Smithfield Market and Leadenhall Market. It owns land beyond its boundaries, including open spaces (parks, forests and commons) in and around Greater London, including most of Epping Forest, Hampstead Heath. The Honourable The Irish Society, a body closely linked with the Corporation, also owns many public spaces in Northern Ireland. The Corporation owns Old Spitalfields Market and Billingsgate Fish Market, in the neighbouring London Borough of Tower Hamlets. It owns and helps fund the Old Bailey, the Central Criminal Court for England and Wales, as a gift to the nation, having begun as the City and Middlesex Sessions.

The City has its own independent police force, the City of London Policeâthe Common Council (the main body of the Corporation) is the police authority. The Corporation also run the Hampstead Heath Constabulary, Epping Forest Keepers and the City of London market constabularies (whose members are no longer attested as constables but retain the historic title). The majority of Greater London is policed by the Metropolitan Police Service, based at New Scotland Yard.

The City has one hospital, St Bartholomew's Hospital, also known as 'Barts'. Founded in 1123, it is located at Smithfield, and is undergoing a long-awaited regeneration after doubts as to its continuing use during the 1990s.

The City is the third largest UK patron of the arts. It oversees the Barbican Centre and subsidises several important performing arts companies.

The London Port Health Authority, which is the responsibility of the Corporation, is responsible for all port health functions on the tidal part of the Thames, including various seaports and London City Airport. The Corporation oversees the running of the Bridge House Trust, which maintains London Bridge, Blackfriars Bridge, Southwark Bridge, Tower Bridge and the Millennium Bridge. The City's flag flies over Tower Bridge, although neither footing is in the City.

The size of the City was constrained by a defensive perimeter wall, known as London Wall, which was built by the Romans in the late 2nd century to protect their strategic port city. However the boundaries of the City of London no longer coincide with the old city wall, as the City expanded its jurisdiction slightly over time. During the medieval era, the City's jurisdiction expanded westwards, crossing the historic western border of the original settlementâthe River Fleetâalong Fleet Street to Temple Bar. The City also took in the other "City bars" which were situated just beyond the old walled area, such as at Holborn, Aldersgate, Bishopsgate and Aldgate. These were the important entrances to the City and their control was vital in maintaining the City's special privileges over certain trades.
Most of the wall has disappeared, but several sections remain visible. A section near the Museum of London was revealed after the devastation of an air raid on 29 December 1940 at the height of the Blitz. Other visible sections are at St Alphage, and there are two sections near the Tower of London. The River Fleet was canalised after the Great Fire of 1666 and then in stages was bricked up and has been since the 18th century one of London's "lost rivers or streams", today underground as a storm drain.

The boundary of the City was unchanged until minor boundary changes on 1 April 1994, when it expanded slightly to the west, north and east, taking small parcels of land from the London Boroughs of Westminster, Camden, Islington, Hackney and Tower Hamlets. The main purpose of these changes was to tidy up the boundary where it had been rendered obsolete by changes in the urban landscape. In this process the City also lost small parcels of land, though there was an overall net gain (the City grew from 1.05 to 1.12 square miles). Most notably, the changes placed the (then recently developed) Broadgate estate entirely in the City.

Southwark, to the south of the City on the other side of the Thames, was within the City between 1550 and 1899 as the "Ward of Bridge Without", a situation connected with the Guildable Manor. The City's administrative responsibility there had in practice disappeared by the mid-Victorian period as various aspects of metropolitan government were extended into the neighbouring areas. Today it is part of the London Borough of Southwark. The Tower of London has always been outside the City and comes under the London Borough of Tower Hamlets.

The Corporation of the City of London has a full achievement of armorial bearings consisting of a shield on which the arms are displayed, a crest displayed on a helm above the shield, supporters on either side and a motto displayed on a scroll beneath the arms.

The coat of arms is "anciently recorded" at the College of Arms. The arms consist of a silver shield bearing a red cross with a red upright sword in the first quarter. They combine the emblems of the patron saints of England and London: the Cross of St George with the symbol of the martyrdom of Saint Paul. The sword is often erroneously supposed to commemorate the killing of Peasants' Revolt leader Wat Tyler by Lord Mayor of London William Walworth. However the arms were in use some months before Tyler's death, and the tradition that Walworth's dagger is depicted may date from the late 17th century.

The Latin motto of the City is ""Domine dirige nos"", which translates as ""Lord, direct (guide) us"". It appears to have been adopted in the 17th century, as the earliest record of it is in 1633.

A banner of the arms (the design on the shield) is flown as a flag.

The City is England's smallest ceremonial county by area and population, and the fourth most densely populated. Of the 326 English districts, it is the second smallest by population, after the Isles of Scilly, and the smallest by area. It is also the smallest English city by population (and in Britain, only two cities in Wales are smaller), and the smallest in the UK by area.

The elevation of the City ranges from sea level at the Thames to at the junction of High Holborn and Chancery Lane. Two small but notable hills are within the historic core, Ludgate Hill to the west and Cornhill to the east. Between them ran the Walbrook, one of the many "lost" rivers or streams of London (another is the Fleet).

Official boundary map, with wards.

Beginning in the west, where the City borders Westminster, the boundary crosses the Victoria Embankment from the Thames, passes to the west of Middle Temple, then turns for a short distance along Strand and then north up Chancery Lane, where it borders Camden. It turns east along Holborn to Holborn Circus, and then goes north east to Charterhouse Street. As it crosses Farringdon Road it becomes the boundary with Islington. It continues to Aldersgate, goes north, and turns east into some back streets soon after Aldersgate becomes Goswell Road, since 1994 embracing all of the Corporation's Golden Lane Estate. Here, at Baltic Street West, is the most northerly extent. The boundary includes all of the Barbican Estate and continues east along Ropemaker Street and its continuation on the other side of Moorgate, becomes South Place. It goes north, reaching the border with Hackney, then east, north, east on back streets, with Worship Street forming a northern boundary, so as to include the Broadgate estate. The boundary then turns south at Norton Folgate and becomes the border with Tower Hamlets. It continues south into Bishopsgate, and takes some backstreets to Middlesex Street (Petticoat Lane) where it continues south-east then south. It then turns south-west, crossing the Minories so as to exclude the Tower of London, and then reaches the river. It then runs up the centre of the Thames, with the exception that Blackfriars Bridge falls within the City; the City controls London Bridge (as part of Bridge ward) but only half of the river underneath it, a feature which is unique in British local administration.

The boundaries are marked by black bollards bearing the City's emblem, and by dragon boundary marks at major entrances, such as Holborn. A more substantial monument marks the boundary at Temple Bar on Fleet Street.

In some places the financial district extends slightly beyond the boundaries, notably to the north and east, into the London Boroughs of Tower Hamlets, Hackney and Islington, and informally these locations are seen as part of the "Square Mile". Since the 1990s the eastern fringe, extending into Hackney and Tower Hamlets, has increasingly been a focus for large office developments due to the availability of large sites compared to within the City.

The City has no sizeable parks within its boundary, but does have a network of a large number of gardens and small open spaces, many of them maintained by the Corporation. These range from formal gardens such as the one in Finsbury Circus, containing a bowling green and bandstand, to churchyards such as St Olave Hart Street, to water features and artwork in courtyards and pedestrianised lanes.

Gardens include:

There are a number of private gardens and open spaces, often within courtyards of the larger commercial developments. Two of the largest are those of the Inner Temple and Middle Temple Inns of Court, in the far southwest.

The Thames and its riverside walks are increasingly being valued as open space and in recent years efforts have been made to increase the ability for pedestrians to access and walk along the river.

The nearest weather station has historically been the London Weather Centre at Kingsway/ Holborn, although observations ceased in 2010. Now St. James Park provides the nearest official readings.

The City has an oceanic climate (KÃ¶ppen "Cfb") modified by the Urban Heat Island in the centre of London. This generally causes higher night-time minima than outlying areas. For example, the August mean minimum of compares to a figure of for Greenwich and Heathrow whereas is at Wisley in the middle of several square miles of Metropolitan Green Belt. All figures refer to the observation period 1971â2000.

Accordingly, the weather station holds the record for the UK's warmest overnight minimum temperature, , recorded on 4 August 1990. The maximum is , set on 10 August 2003. The absolute minimum for the weather station is a mere , compared to readings around towards the edges of London. Unusually, this temperature was during a windy and snowy cold spell (mid-January 1987), rather than a cold clear nightâcold air drainage is arrested due to the vast urban area surrounding the city.

The station holds the record for the highest British mean monthly temperature, (mean maximum , mean minimum during July 2006). However, in terms of daytime maximum temperatures, Cambridge NIAB and Botanical Gardens with a mean maximum of , and Heathrow with all exceeded this.

The City is a police area and has its own police force, the City of London Police, separate from the Metropolitan Police Service covering the majority of Greater London. The City Police have three police stations, at Snow Hill, Wood Street and Bishopsgate, and an administrative headquarters at Guildhall Yard East. The force comprises 735 police officers including 273 detectives. It is the smallest territorial police force in England and Wales, in both geographic area and the number of police officers.

Where the majority of British police forces have silver-coloured badges, those of the City of London Police are black and gold featuring the City crest. The force has rare red and white chequered cap bands and unique red and white striped duty arm bands on the sleeves of the tunics of constables and sergeants (red and white being the colours of the City), which in most other British police forces are black and white. City police sergeants and constables wear crested custodian helmets whilst on foot patrol. These helmets do not feature either St Edward's Crown or the Brunswick Star, which are used on most other police helmets in England and Wales.

The City's position as the United Kingdom's financial centre and a critical part of the country's economy, contributing about 2.5% of the UK's gross national product, has resulted in it becoming a target for political violence. The Provisional IRA exploded several bombs in the early 1990s, including the 1993 Bishopsgate bombing.

The area is also spoken of as a possible target for al-Qaeda. For instance, when in May 2004 the BBC's "Panorama" programme examined the preparedness of Britain's emergency services for a terrorist attack on the scale of the September 11, 2001 attacks, they simulated a chemical explosion on Bishopsgate in the east of the City.

The "Ring of Steel" is a particularly notable measure, established in the wake of the IRA bombings, that has been taken against terrorist threats.

The City has fire risks in many historic buildings, including St Paul's Cathedral, Old Bailey, Mansion House, Smithfield Market, the Guildhall, and also in numerous high-rise buildings. There is one London Fire Brigade station in the City, at Dowgate, with one pumping appliance. The City relies upon stations in the surrounding London boroughs to support it at some incidents. The first fire engine is in attendance in roughly five minutes on average, the second when required in a little over five and a half minutes. There were 1,814 incidents attended in the City in 2006/2007âthe lowest in Greater London. No-one died in an event arising from a fire in the four years prior to 2007.

There is power station located in Charterhouse Street that also provides heat to some of the surrounding buildings

The Office for National Statistics recorded the population in 2011 as 7,375; slightly higher than in the last census, 2001, and estimates the population as at mid-2016 to be 9,401. At the 2001 census the ethnic composition was 84.6% White, 6.8% South Asian, 2.6% Black, 2.3% Mixed, 2.0% Chinese and 1.7% were listed as "other". To the right is a table showing the change in population since 1801, based on decadal censuses. The first half of the 19th century shows a population of between 120,000â140,000, decreasing dramatically from 1851 to 1991, with a small increase between 1991 and 2001. The only notable boundary change since the first census in 1801 occurred in 1994.

The City's full-time working residents have much higher gross weekly pay than in London and Great Britain (England, Wales and Scotland): Â£773.30 compared to Â£598.60 and Â£491.00 respectively. There is a large inequality of income between genders (Â£1,085.90 in men compared to Â£653.50 in women), though this can be explained by job type and length of employment respectively. The 2001 Census showed the City as a unique district amongst 376 districts surveyed in England and Wales. The City had the highest proportional population increase, one-person households, people with qualifications at degree level or higher and the highest indications of overcrowding. It recorded the lowest proportion of households with cars or vans, people who travel to work by car, married couple households and the lowest average household size: just 1.58 people. It also ranked highest within the Greater London area for the percentage of people with no religion and people who are employed.

The City vies with New York City's Downtown Manhattan as the financial capital of the world; many banking and insurance institutions have their headquarters there. The London Stock Exchange (shares and bonds), Lloyd's of London (insurance) and the Bank of England are all based in the City. Over 500 banks have offices in the City, and the City is an established leader in trading in Eurobonds, foreign exchange, energy futures and global insurance. The Alternative Investment Market, a market for trades in equities of smaller firms, is a recent development. In 2009, the City of London accounted for 2.4% of UK GDP.

London is the world's greatest foreign exchange market, with much of the trade conducted in the City of London. Of the $3.98Â trillion daily global turnover, as measured in 2009, trading in London accounted for around $1.85Â trillion, or 46.7% of the total. The pound sterling, the currency of the United Kingdom, is globally the fourth most traded currency and the third most held reserve currency.

Since 1991 Canary Wharf, a few miles east of the City in Tower Hamlets, has become another centre for London's financial services industry which houses many banks and other institutions formerly located in the Square Mile. Although growth has continued in both locations, and there have been relocations in both directions, the Corporation has come to realise that its planning policies may have been causing financial firms to choose Canary Wharf as a location.

Many major global companies have their headquarters in the City, including Aviva, BT Group, Lloyds Banking Group, Old Mutual, Prudential, Schroders, Standard Chartered, and Unilever.

A number of the world's largest law firms are headquartered in the City, including four of the "Magic Circle" law firms (Allen & Overy, Freshfields Bruckhaus Deringer, Linklaters and Slaughter & May), as well as other firms such as DLA Piper, Eversheds Sutherland, Herbert Smith Freehills and Hogan Lovells.

Whilst the financial sector, and related businesses and institutions, continue to dominate, the economy is not limited to that sector. The legal profession has a strong presence, especially in the west and north (i.e., towards the Inns of Court). Retail businesses were once important, but have gradually moved to the West End of London, though it is now Corporation policy to encourage retailing in some locations, for example at Cheapside near St Paul's. The City has a number of visitor attractions, mainly based on its historic heritage as well as the Barbican Centre and adjacent Museum of London, though tourism is not at present a major contributor to the City's economy or character. The City has many pubs, bars and restaurants, and the "night-time" economy does feature in the Bishopsgate area, towards Shoreditch. The meat market at Smithfield, wholly within the City, continues to be one of London's main markets (the only one remaining in central London) and the country's largest meat market. In the east is Leadenhall Market, a fresh food market that is also a visitor attraction.

The trend for purely office development is beginning to reverse as the Corporation encourages residential use, albeit with development occurring when it arises on windfall sites. The City has a target of 90 additional dwellings per year. Some of the extra accommodation is in small pre-World War II listed buildings, which are not suitable for occupation by the large companies which now provide much of the City's employment. Recent residential developments include "the Heron", a high-rise residential building on the Milton Court site adjacent to the Barbican, and the Heron Plaza development on Bishopsgate is also expected to include residential parts.

Since the 1990s, the City has diversified away from near exclusive office use in other ways. For example, several hotels and the first department store opened in the 2000s. A shopping centre was more recently opened at One New Change, Cheapside (near St Paul's Cathedral) in October 2010, which is open seven days a week. However, large sections remain quiet at weekends, especially in the eastern section, and it is quite common to find shops, pubs and cafes closed on these days.

Fire bombing and post-World War II redevelopment have meant that the City, despite its history, has fewer intact historic structures than one might expect. Nonetheless, there remain many dozens of (mostly Victorian and Edwardian) fine buildings, typically in historicist and neoclassical style. They include the Monument to the Great Fire of London ("the Monument"), St Paul's Cathedral, the Guildhall, the Royal Exchange, Dr. Johnson's House, Mansion House and a , many designed by Sir Christopher Wren, who also designed St Paul's. 2 King's Bench Walk and Prince Henry's Room are notable historic survivors of heavy bombing of the Temple area, which has largely been rebuilt to its historic form. Another example of a bomb-damaged place having been restored is Staple Inn on Holborn. A few small sections of the Roman London Wall exist, for example near the Tower of London and in the Barbican area. Among the twentieth-century listed buildings are Bracken House, the first post World War II buildings in the country to be given statutory protection, and the whole of the Barbican and Golden Lane Estate.

The Tower of London is not in the City, but is a notable visitor attraction which brings tourists to the southeast of the City. Other landmark buildings with historical significance include the Bank of England, the Old Bailey, the Custom House, Smithfield Market, Leadenhall Market and St Bartholomew's Hospital. Noteworthy contemporary buildings include a number of modern high-rise buildings (see section below) as well as the Lloyd's building.


A growing number of tall buildings and skyscrapers are principally used by the financial sector. Almost all are situated in the eastern side around Bishopsgate, Leadenhall Street and Fenchurch Street, in the financial core of the City. In the north there is a smaller cluster comprising the Barbican Estate's three tall residential towers and the commercial CityPoint tower. In 2007, the tall Drapers' Gardens building was demolished and replaced by a shorter tower.

The City's buildings of more than in height are:

The timeline of the tallest building in the City is as follows:

Seven of the eleven London Underground lines run through the City, serving eleven stations.

The Docklands Light Railway (DLR) has two stations within the City: Bank and Tower Gateway.

Three longer-distance rail termini are in the City: Liverpool Street (services primarily to Essex and East Anglia including Southend Airport), Fenchurch Street (services to East London and South Essex) and Cannon Street (services to the south east).

Moorgate is the terminus for suburban services from Hertfordshire, and two through-routes operate mostly underground along the main axes:

The Northern line connects to two other main railway termini, Euston) and (Waterloo)); the latter has a direct connection to the City via the Waterloo & City Line.

The City is in Travelcard Zone 1.

The national A1, A10 A3, A4, and A40 road routes begin in the City. The City is in the London congestion charge zone, with the small exception on the eastern boundary of the sections of the A1210/A1211 that are part of the Inner Ring Road. The following bridges, listed west to east (downstream), cross the River Thames: Blackfriars Bridge, Blackfriars Railway Bridge, Millennium Bridge (footbridge), Southwark Bridge, Cannon Street Railway Bridge and London Bridge; Tower Bridge is not in the City. The City, like most of central London, is well served by buses, including night buses. Two bus stations are in the City, at Aldgate on the eastern boundary with Tower Hamlets, and at Liverpool Street by the railway station. There are approximately 28 Santander Cycles docking stations in the City. A number of existing and proposed cycle routes criss-cross the City, as part of the London Cycle Network.

One London River Services pier is on the Thames in the City, Blackfriars Millennium Pier, though the Tower Millennium Pier lies adjacent to the boundary near the Tower of London. One of the Port of London's 25 safeguarded wharves, Walbrook Wharf, is adjacent to Cannon Street station, and is used by the Corporation to transfer waste via the river. Swan Lane Pier, just upstream of London Bridge, is proposed to be replaced and upgraded for regular passenger services, planned to take place in 2012â2015. Before then, Tower Pier is to be extended.

There is a public riverside walk along the river bank, opened in stages over recent years. The only section not running along the river is a short stretch at Queenhithe. The walk along Walbrook Wharf is closed to pedestrians when waste is being transferred onto barges.

According to a survey conducted in March 2011, the methods by which employed residents 16â74 get to work varied widely: 48.4% go on foot; 19.5% via light rail, (i.e. the Underground, DLR, etc.); 9.2% work mainly from home; 5.8% take the train; 5.6% travel by bus, minibus, or coach; and 5.3% go by bicycle; with just 3.4% commuting by car or van, as driver or passenger.

The City has only one directly maintained primary school, Sir John Cass's Foundation Primary School at Aldgate (ages 4 to 11). It is a Voluntary-Aided (VA) Church of England school, maintained by the Education Service of the City of London.

City residents send their children to schools in neighbouring Local Education Authorities, such as Islington, Tower Hamlets, Westminster and Southwark.

The City controls three independent schools, City of London School (a boys' school) and City of London School for Girls in the City, and the City of London Freemen's School (co-educational day and boarding) in Ashtead, Surrey. The City of London School for Girls has its own preparatory department for entrance at age seven. It is the principal sponsor of The City Academy, Hackney, City of London Academy Islington, and City of London Academy, Southwark.

The City is home to the Cass Business School, The London Institute of Banking & Finance, the Guildhall School of Music and Drama and parts of three of the universities in London: the Maughan Library of King's College London on Chancery Lane, the business school of London Metropolitan University, and a campus of the University of Chicago Graduate School of Business. The College of Law has its London campus in Moorgate. Part of Barts and The London School of Medicine and Dentistry is on the Barts hospital site at West Smithfield.

Libraries operated by the Corporation include three lending libraries; Barbican Library, Shoe Lane Library and Artizan Street Library and Community Centre. Membership is open to all â with one official proof of address required to join.

Guildhall Library, and City Business Library are also public reference libraries, specialising in the history of London and business reference resources.

Author and journalist Nicholas Shaxson argued that, in return for the financial institutions based in the City raising loans and finance for the British government, the City "has extracted privileges and freedoms from rules and laws to which the rest of Britain must submit". He further claims that the assistance provided to the institutions based within it, many of which help their rich clients with offshore tax arrangements, mean that the City is "a tax haven in its own right".

The documentary "" asserts the tax haven status that the City provides.



</doc>
