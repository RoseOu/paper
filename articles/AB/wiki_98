<doc id="23628" url="https://en.wikipedia.org/wiki?curid=23628" title="PDP-10">
PDP-10

Digital Equipment Corporation (DEC)'s PDP-10, later marketed as the DECsystem-10, is a mainframe computer family manufactured beginning in 1966 and discontinued in 1983. 1970s models and beyond were marketed under the DECsystem-10 name, especially as the TOPS-10 operating system became widely used.

The PDP-10's architecture is almost identical to that of DEC's earlier PDP-6, sharing the same 36-bit word length and slightly extending the instruction set (but with improved hardware implementation). Some aspects of the instruction set are unusual, most notably the "byte" instructions, which operated on bit fields of any size from 1 to 36 bits inclusive, according to the general definition of a byte as "a contiguous sequence of a fixed number of bits".

The PDP-10 is the machine that made time-sharing common, and this and other features made it a common fixture in many university computing facilities and research labs during the 1970s, the most notable being Harvard University's Aiken Computation Laboratory, MIT's AI Lab and Project MAC, Stanford's SAIL, Computer Center Corporation (CCC), ETH (ZIR), and Carnegie Mellon University. Its main operating systems, TOPS-10 and TENEX, were used to build out the early ARPANET. For these reasons, the PDP-10 looms large in early hacker folklore.

Projects to extend the PDP-10 line were eclipsed by the success of the unrelated VAX superminicomputer, and the cancellation of the PDP-10 line was announced in 1983.

The original PDP-10 processor is the KA10, introduced in 1968. It uses discrete transistors packaged in DEC's Flip-Chip technology, with backplanes wire wrapped via a semi-automated manufacturing process. Its cycle time is 1 μs and its add time 2.1 μs. In 1973, the KA10 was replaced by the KI10, which uses transistor–transistor logic (TTL) SSI. This was joined in 1975 by the higher-performance KL10 (later faster variants), which is built from emitter-coupled logic (ECL), microprogrammed, and has cache memory. The KL10's performance was about 1 megaflops using 36-bit floating point numbers on matrix row reduction. It was slightly faster than the newer VAX-11/750, although more limited in memory.

A smaller, less expensive model, the KS10, was introduced in 1978, using TTL and Am2901 bit-slice components and including the PDP-11 Unibus to connect peripherals. The KS was marketed as the DECsystem-2020, DEC's entry in the distributed processing arena, and it was introduced as "the world's lowest cost mainframe computer system."

The KA10 has a maximum main memory capacity (both virtual and physical) of 256 kilowords (equivalent to 1152 kilobytes). As supplied by DEC, it did not include paging hardware; memory management consisted of two sets of protection and relocation registers, called "base and bounds" registers. This allows each half of a user's address space to be limited to a set section of main memory, designated by the base physical address and size. This allows the model of separate read-only shareable code segment (normally the high segment) and read-write data/stack segment (normally the low segment) used by TOPS-10 and later adopted by Unix. Some KA10 machines, first at MIT, and later at Bolt, Beranek and Newman (BBN), were modified to add virtual memory and support for demand paging, and more physical memory.

The KA10 weighed about .

The 10/50 was the top-of-the-line Uni-processor KA machine at the time when the "PA1050" software package was introduced. Two other KA10 models were the uniprocessor 10/40, and the dual-processor 10/55.

The KI10 introduced support for paged memory management, and also support a larger physical address space of 4 megawords. KI10 models include 1060, 1070 and 1077, the latter incorporating two CPUs.

The original KL10 PDP-10 (also marketed as DECsystem-10) models (1080, 1088, etc.) use the original PDP-10 memory bus, with external memory modules. Module in this context meant a cabinet, dimensions roughly (WxHxD) 30 x 75 x 30 in. with a capacity of 32 to 256 kWords of magnetic core memory (the picture on the right hand side of the introduction shows six of these cabinets). The processors used in the DECSYSTEM-20 (2040, 2050, 2060, 2065), commonly but incorrectly called "KL20", use internal memory, mounted in the same cabinet as the CPU. The 10xx models also have different packaging; they come in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. The differences between the 10xx and 20xx models are more cosmetic than real; some 10xx systems have "20-style" internal memory and I/O, and some 20xx systems have "10-style" external memory and an I/O bus. In particular, all ARPAnet TOPS-20 systems had an I/O bus because the AN20 IMP interface was an I/O bus device. Both could run either TOPS-10 or TOPS-20 microcode and thus the corresponding operating system.

Later, the "Model B" version of the 2060 processors removed the 256 kiloword limit on the virtual address space, by allowing the use of up to 32 "sections" of up to 256 kilowords each, along with substantial changes to the instruction set. "Model A" and "Model B" KL10 processors can be thought of as being different CPUs. The first operating system that took advantage of the Model B's capabilities was TOPS-20 release 3, and user mode extended addressing was offered in TOPS-20 release 4. TOPS-20 versions after release 4.1 would only run on a Model B.

TOPS-10 versions 7.02 and 7.03 also use extended addressing when run on a 1090 (or 1091) Model B processor running TOPS-20 microcode.

The final upgrade to the KL10 was the MCA25 upgrade of a 2060 to 2065 (or a 1091 to 1095), which gave some performance increases for programs which run in multiple sections.

The I/O architecture of the 20xx series KL machines is based on a DEC bus design called the Massbus. While many attributed the success of the PDP-11 to DEC's decision to make the PDP-11 Unibus an open architecture, DEC reverted to prior philosophy with the KL, making Massbus both unique and proprietary. Consequently, there were no aftermarket peripheral manufacturers who made devices for the Massbus, and DEC chose to price their own Massbus devices, notably the RP06 disk drive, at a substantial premium above comparable IBM-compatible devices. CompuServe for one, designed its own alternative disk controller that could operate on the Massbus, but connect to IBM style 3330 disk subsystems.

The KL class machines cannot be started without the assistance of a PDP-11/40 front-end processor installed in every system. The PDP-11 is booted from a dual-ported RP06 disk drive (or alternatively from an 8" floppy disk drive or DECtape), and then commands can be given to the PDP-11 to start the main processor, which is typically booted from the same RP06 disk drive as the PDP-11. The PDP-11 performs watchdog functions once the main processor is running.

Communication with IBM mainframes, including Remote Job Entry (RJE), were accomplished via a DN61 or DN-64 front-end processor, using a PDP-11/40or PDP-11/34a.

The KS10 was a lower-cost PDP-10 built using AMD 2901 bit-slice chips, with an Intel 8080A microprocessor as a control processor. The KS10 design was crippled to be a Model A even though most of the necessary data paths needed to support the Model B architecture were present. This was no doubt intended to segment the market, but it greatly shortened the KS10's product life.

The KS system uses a similar boot procedure to the KL10. The 8080 control processor loads the microcode from an RM03, RM80, or RP06 disk or magnetic tape and then starts the main processor. The 8080 switches modes after the operating system boots and controls the console and remote diagnostic serial ports.

Two models of tape drives were supported by the TM10 Magnetic Tape Control subsystem:
A mix of up to eight of these could be supported, providing seven-track &/or nine-track devices.
The TU20 and TU30 each came in A (9 track) and B (7 track) versions, and all of the aforementioned tape drives could read/write from/to 200 BPI, 556 BPI and 800 BPI IBM-compatible tapes. 

The TM10 Magtape controller was available in two submodels:

From the first PDP-6s to the KL-10 and KS-10, the user-mode instruction set architecture is largely the same. This section covers that architecture. The only major change to the architecture is the addition of multi-section extended addressing in the KL-10; extended addressing, which changes the process of generating the effective address of an instruction, is briefly discussed at the end.

The PDP-10 has 36-bit words and 18-bit word addresses. In supervisor mode, instruction addresses correspond directly to physical memory. In user mode, addresses are translated to physical memory. Earlier models give a user process a "high" and a "low" memory: addresses with a 0 top bit used one base register, and higher addresses used another. Each segment is contiguous. Later architectures have paged memory access, allowing non-contiguous address spaces. The CPU's general-purpose registers can also be addressed as memory locations 0-15.

There are 16 general-purpose, 36-bit registers. The right half of these registers (other than register 0) may be used for indexing. A few instructions operate on pairs of registers. The "PC Word" consists of a 13-bit condition register (plus 5 always zero bits) in the left half and an 18-bit Program Counter in the right half. The condition register, which records extra bits from the results of arithmetic operations ("e.g." overflow), can be accessed by only a few instructions.

In the original KA-10 systems, these registers were simply the first 16 words of main memory. The "fast registers" hardware option implemented them as registers in the CPU, still addressable as the first 16 words of memory. Some software took advantage of this by using the registers as an instruction cache by loading code into the registers and then jumping to the appropriate address; this was used, for example, in Maclisp to implement one version of the garbage collector. Later models all had registers in the CPU.

There are two operational modes, supervisor and user mode. Besides the difference in memory referencing described above, supervisor-mode programs can execute input/output operations.

Communication from user-mode to supervisor-mode is done through Unimplemented User Operations (UUOs): instructions which are not defined by the hardware, and are trapped by the supervisor. This mechanism is also used to emulate operations which may not have hardware implementations in cheaper models.

The major datatypes which are directly supported by the architecture are two's complement 36-bit integer arithmetic (including bitwise operations), 36-bit floating-point, and halfwords. Extended, 72-bit, floating point is supported through special instructions designed to be used in multi-instruction sequences. Byte pointers are supported by special instructions. A word structured as a "count" half and a "pointer" half facilitates the use of bounded regions of memory, notably stacks.

The instruction set is very symmetrical. Every instruction consists of a 9-bit opcode, a 4-bit register code, and a 23-bit effective address field, which consists in turn of a 1-bit indirect bit, a 4-bit register code, and an 18-bit offset. Instruction execution begins by calculating the effective address. It adds the contents of the given register (if not register zero) to the offset; then, if the indirect bit is 1, an "indirect word", containing an indirect bit, register code, and offset in the same positions as in instructions, is fetched at the calculated address and the effective address calculation is repeated using that word, adding the register (if not register zero) to the offset, until an indirect word with a zero indirect bit is reached. The resulting effective address can be used by the instruction either to fetch memory contents, or simply as a constant. Thus, for example, MOVEI A,3(C) adds 3 to the 18 lower bits of register C and puts the result in register A, without touching memory.

There are three main classes of instruction: arithmetic, logical, and move; conditional jump; conditional skip (which may have side effects). There are also several smaller classes.

The arithmetic, logical, and move operations include variants which operate immediate-to-register, memory-to-register, register-to-memory, register-and-memory-to-both or memory-to-memory. Since registers may be addressed as part of memory, register-to-register operations are also defined. (Not all variants are useful, though they are well-defined.) For example, the ADD operation has as variants ADDI (add an 18-bit "I"mmediate constant to a register), ADDM (add register contents to a "M"emory location), ADDB (add to "B"oth, that is, add register contents to memory and also put the result in the register). A more elaborate example is HLROM ("H"alf "L"eft to "R"ight, "O"nes to "M"emory), which takes the Left half of the register contents, places them in the Right half of the memory location, and replaces the left half of the memory location with Ones. Halfword instructions are also used for linked lists: HLRZ is the Lisp CAR operator; HRRZ is CDR.

The conditional jump operations examine register contents and jump to a given location depending on the result of the comparison. The mnemonics for these instructions all start with JUMP, JUMPA meaning "jump always" and JUMP meaning "jump never" – as a consequence of the symmetric design of the instruction set, it contains several no-ops such as JUMP. For example, JUMPN A,LOC jumps to the address LOC if the contents of register A is non-zero. There are also conditional jumps based on the processor's condition register using the JRST instruction. On the KA10 and KI10, JRST is faster than JUMPA, so the standard unconditional jump is JRST.

The conditional skip operations compare register and memory contents and skip the next instruction (which is often an unconditional jump) depending on the result of the comparison. A simple example is CAMN A,LOC which compares the contents of register A with the contents of location LOC and skips the next instruction if they are not equal. A more elaborate example is TLCE A,LOC (read "Test Left Complement, skip if Equal"), which using the contents of LOC as a mask, selects the corresponding bits in the left half of register A. If all those bits are "E"qual to zero, skip the next instruction; and in any case, replace those bits by their boolean complement.

Some smaller instruction classes include the shift/rotate instructions and the procedure call instructions. Particularly notable are the stack instructions PUSH and POP, and the corresponding stack call instructions PUSHJ and POPJ. The byte instructions use a special format of indirect word to extract and store arbitrary-sized bit fields, possibly advancing a pointer to the next unit.

In processors supporting extended addressing, the address space is divided into "sections". An 18-bit address is a "local address", containing an offset within a section, and a "global address" is 30 bits, divided into a 12-bit section number at the bottom of the upper 18 bits and an 18-bit offset within that section in the lower 18 bits. A register can contain either a "local index", with an 18-bit unsigned displacement or local address in the lower 18 bits, or a "global index", with a 30-bit unsigned displacement or global address in the lower 30 bits. An indirect word can either be a "local indirect word", with its uppermost bit set, the next 12 bits reserved, and the remaining bits being an indirect bit, a 4-bit register code, and an 18-bit displacement, or a "global indirect word", with its uppermost bit clear, the next bit being an indirect bit, the next 4 bits being a register code, and the remaining 30 bits being a displacement.

The process of calculating the effective address generates a 12-bit section number and an 18-bit offset within that segment.

The original PDP-10 operating system was simply called "Monitor", but was later renamed TOPS-10. Eventually the PDP-10 system itself was renamed the DECsystem-10. Early versions of Monitor and TOPS-10 formed the basis of Stanford's WAITS operating system and the CompuServe time-sharing system.

Over time, some PDP-10 operators began running operating systems assembled from major components developed outside DEC. For example, the main Scheduler might come from one university, the Disk Service from another, and so on. The commercial timesharing services such as CompuServe, On-Line Systems (OLS), and Rapidata maintained sophisticated inhouse systems programming groups so that they could modify the operating system as needed for their own businesses without being dependent on DEC or others. There are also strong user communities such as DECUS through which users can share software that they have developed.

BBN developed their own alternative operating system, TENEX, which fairly quickly became the de facto standard in the research community. DEC later ported TENEX to the KL10, enhanced it considerably, and named it TOPS-20, forming the DECSYSTEM-20 line. 

MIT, which had developed CTSS, Compatible Time-Sharing System to run on their IBM 709 (and later a modified IBM 7094 system), also developed ITS, Incompatible Timesharing System to run on their PDP-6 (and later a modified PDP-10); the naming was related, since the IBM and the DEC/PDP hardware were different, i.e. "incompatible" (despite each having a 36-bit CPU).

The ITS name, selected by Tom Knight, "was a play on" the CTSS name.

Tymshare developed TYMCOM-X, derived from TOPS-10 but using a page-based file system like TOPS-20.

In 1971 to 1972, researchers at Xerox PARC were frustrated by top company management's refusal to let them buy a PDP-10. Xerox had just bought Scientific Data Systems (SDS) in 1969, and wanted PARC to use an SDS machine. Instead, a group led by Charles P. Thacker designed and constructed two PDP-10 clone systems named MAXC (pronounced as Max, in honour of Max Palevsky, who had sold SDS to Xerox) for their own use. MAXC was also a backronym for Multiple Access Xerox Computer. MAXC ran a modified version of TENEX.

Third-party attempts to sell PDP-10 clones were relatively unsuccessful; see Foonly, Systems Concepts, and XKL.

One of the largest collections of DECsystem-10 architecture systems ever assembled was at CompuServe, which, at its peak, operated over 200 loosely coupled systems in three data centers in Columbus, Ohio. CompuServe used these systems as 'hosts', providing access to commercial applications, and the CompuServe Information Service. While the first such systems were bought from DEC, when DEC abandoned the PDP-10 architecture in favor of the VAX, CompuServe and other PDP-10 customers began buying plug compatible computers from Systems Concepts. As of January 2007, CompuServe was operating a small number of PDP-10 architecture machines to perform some billing and routing functions.

The main power supplies used in the KL-series machines were so inefficient that CompuServe engineers designed a replacement supply that used about half the energy. CompuServe offered to license the design for its KL supply to DEC for free if DEC would promise that any new KL bought by CompuServe would have the more efficient supply installed. DEC declined the offer.
Another modification made to the PDP-10 by CompuServe engineers was replacing the hundreds of incandescent indicator lamps on the KI10 processor cabinet with LED lamp modules. The cost of conversion was easily offset by cost savings in electricity use, reduced heat, and labor needed to replace burned-out lamps. Digital followed this step all over the world. The picture on the right hand side shows the light panel of the MF10 memory which is contemporary with the KI10 CPU. This item is part of a computer museum, and was populated with LEDs in 2008 for demonstration purposes only. There were no similar banks of indicator lamps on KL and KS processors.

The PDP-10 was eventually eclipsed by the VAX superminicomputer machines (descendants of the PDP-11) when DEC recognized that the PDP-10 and VAX product lines were competing with each other and decided to concentrate its software development effort on the more profitable VAX. The PDP-10 product line cancellation was announced in 1983, including cancelling the ongoing Jupiter project to produce a new high-end PDP-10 processor (despite that project being in good shape at the time of the cancellation) and the Minnow project to produce a desktop PDP-10, which may then have been at the prototyping stage.

This event spelled the doom of ITS and the technical cultures that had spawned the original jargon file, but by the 1990s it had become something of a badge of honor among old-time hackers to have cut one's teeth on a PDP-10.

The PDP-10 assembly language instructions LDB and DPB (load/deposit byte) live on as functions in the programming language Common Lisp. See the "References" section on the LISP article. The 36-bit word size of the PDP-6 and PDP-10 was influenced by the programming convenience of having 2 LISP pointers, each 18 bits, in one word.

Will Crowther created "Adventure", the prototypical computer adventure game, for a PDP-10. Don Daglow created the first computer baseball game (1971) and "Dungeon" (1975), the first role-playing video game on a PDP-10. Walter Bright originally created "Empire" for the PDP-10. Roy Trubshaw and Richard Bartle created the first MUD on a PDP-10. "Zork" was written on the PDP-10. Infocom used PDP-10s for game development and testing.

Bill Gates and Paul Allen originally wrote Altair BASIC using an Intel 8080 simulator running on a PDP-10 at Harvard University. Allen had modified the PDP-10 assembler to become a cross assembler for the 8080 chip. They founded Microsoft shortly after.

The software for simulation of historical computers SIMH contains a module to emulate the KS10 CPU on a Windows or Unix-based machine. Copies of DEC's original distribution tapes are available as downloads from the Internet so that a running TOPS-10 or TOPS-20 system may be established. ITS is also available for SIMH.

Ken Harrenstien's KLH10 software for Unix-like systems emulates a KL10B processor with extended addressing and 4 MW of memory or a KS10 processor with 512 KW of memory. The KL10 emulation supports v.442 of the KL10 microcode, which enables it to run the final versions of both TOPS-10 and TOPS-20. The KS10 emulation supports both ITS v.262 microcode for the final version of KS10 ITS and DEC v.130 microcode for the final versions of KS TOPS-10 and TOPS-20.








</doc>
<doc id="23629" url="https://en.wikipedia.org/wiki?curid=23629" title="DECSYSTEM-20">
DECSYSTEM-20

The DECSYSTEM-20 was a 36-bit Digital Equipment Corporation PDP-10 mainframe computer running the TOPS-20 operating system (products introduced in 1977).

PDP-10 computers running the TOPS-10 operating system were labeled "DECsystem-10" as a way of differentiating them from the PDP-11. Later on, those systems running TOPS-20 (on the KL10 PDP-10 processors) were labeled "DECSYSTEM-20" (the block capitals being the result of a lawsuit brought against DEC by Singer, which once made a computer called "The System Ten"). The DECSYSTEM-20 was sometimes called PDP-20, although this designation was never used by DEC.

The following models were produced:


The only significant difference the user could see between a DECsystem-10 and a DECSYSTEM-20 was the operating system and the color of the paint. Most (but not all) machines sold to run TOPS-10 were painted "Blasi Blue", whereas most TOPS-20 machines were painted "Terracotta" (often mistakenly called "Chinese Red" or orange; the actual name of the color on the paint cans was Terra Cotta).

There were some significant internal differences between the earlier KL10 Model A processors, used in the earlier DECsystem-10s running on KL10 processors, and the later KL10 Model Bs, used for the DECSYSTEM-20s. Model As used the original PDP-10 memory bus, with external memory modules. The later Model B processors used in the DECSYSTEM-20 used internal memory, mounted in the same cabinet as the CPU. The Model As also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20.

The last released implementation of DEC's 36-bit architecture was the single cabinet DECSYSTEM-2020, using a KS10 processor.

The DECSYSTEM-20 was primarily designed and used as a small mainframe for timesharing. That is, multiple users would concurrently log on to individual user accounts and share use of the main processor to compile and run applications. Separate disk allocations were maintained for all users by the operating system, and various levels of protection could be maintained by for System, Owner, Group, and World users. A model 2060, for example, could typically host up to 40 to 60 simultaneous users before exhibiting noticeably reduced response time.

The Living Computer Museum of Seattle, Washington maintains a 2065 running TOPS-10, which is available to interested parties via SSH upon registration (at no cost) at their website.




</doc>
<doc id="23630" url="https://en.wikipedia.org/wiki?curid=23630" title="Programmed Data Processor">
Programmed Data Processor

Programmed Data Processor (PDP), referred to by some customers, media and authors as "Programmable Data Processor, is a term used by the Digital Equipment Corporation from 1957 to 1990 for several lines of minicomputers. The name "PDP" intentionally avoids the use of the term "computer" because, at the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines, and the venture capitalists behind Digital (especially Georges Doriot) would not support Digital's attempting to build a "computer"; the word "minicomputer" had not yet been coined. So instead, Digital used their existing line of logic modules to build a "Programmed Data Processor" and aimed it at a market that could not afford the larger computers.

The various PDP machines can generally be grouped into families based on word length.

Members of the PDP series include:










Various sites list documents by Charles Lasner, the creator of the alt.sys.pdp8 discussion group, and related documents by various members of the alt.sys.pdp8 readership with even more authoritative information about the various models, especially detailed focus upon the various members of the PDP-8 "family" of computers both made and not made by DEC.


</doc>
<doc id="23631" url="https://en.wikipedia.org/wiki?curid=23631" title="Primary mirror">
Primary mirror

A primary mirror (or primary) is the principal light-gathering surface (the objective) of a reflecting telescope.

The primary mirror of a reflecting telescope is a spherical or parabolic shaped disks of polished reflective metal (speculum metal up to the mid 19th century), or in later telescopes, glass or other material coated with a reflective layer. One of the first known reflecting telescopes, Newton's reflector of 1668, used a 3.3 cm polished metal primary mirror. The next major change was to use silver on glass rather than metal, in the 19th century such was with the Crossley reflector. This was changed to vacuum deposited aluminum on glass, used on the 200-inch Hale telescope.

Solid primary mirrors have to sustain their own weight and not deform under gravity, which limits the maximum size for a single piece primary mirror.

Segmented mirror configurations are used to get around the size limitation on single primary mirrors. For example, the Giant Magellan Telescope will have seven 8.4 meter primary mirrors, with the resolving power equivalent to a optical aperture.

The largest optical telescope in the world as of 2009 to use a non-segmented single-mirror as its primary mirror is the 8.2 m (26.9 ft) Subaru telescope of the National Astronomical Observatory of Japan, located in Mauna Kea Observatory on Hawaii since 1997; however, this is not the largest diameter single mirror in a telescope, the U.S./German/Italian Large Binocular Telescope has two 8.4 m (27.6 ft) mirrors (which can be used together for interferometric mode). Both of these are smaller than the 10 m segmented primary mirrors on the two Keck telescope. The Hubble Space Telescope has a 2.4 m (7 ft 10 in) primary mirror.

Radio and submillimeter telescopes use much larger dishes or antennae, which do not have to be made as precisely as the mirrors used in optical telescopes. The Arecibo Observatory uses a 305 m dish, which is the world largest single-dish radio telescope fixed to the ground. The Green Bank Telescope has the world's largest steerable single radio dish with 100 m in diameter. There are larger radio arrays, composed of multiple dishes which have better image resolution but less sensitivity.



</doc>
<doc id="23632" url="https://en.wikipedia.org/wiki?curid=23632" title="Platonic idealism">
Platonic idealism

Platonic idealism usually refers to Plato's theory of forms or doctrine of ideas. It holds that only ideas encapsulate the true and essential nature of things, in a way that the physical form cannot. We recognise a tree, for instance, even though its physical form may be most untree-like. The treelike nature of a tree is therefore independent of its physical form. Plato’s idealism evolved from Pythagorean philosophy, which held that mathematical formulas and proofs accurately describe the essential nature of all things, and these truths are eternal. Plato believed that because knowledge is innate and not discovered through experience, we must somehow arrive at the truth through introspection and logical analysis, stripping away false ideas to reveal the truth.

Some commentators hold that Plato argued that truth is an abstraction. In other words, we are urged to believe that Plato's theory of ideals is an abstraction, divorced from the so-called external world, of modern European philosophy, despite the fact Plato taught that ideals are ultimately real, and different from non-ideal things—indeed, he argued for a distinction between the ideal and non-ideal realm.

These commentators speak thus: for example, a particular tree, with a branch or two missing, possibly alive, possibly dead, and with the initials of two lovers carved into its bark, is distinct from the abstract form of Tree-ness. A Tree is the ideal that each of us holds that allows us to identify the imperfect reflections of trees all around us.

Plato gives the divided line as an outline of this theory. At the top of the line, the Form of the Good
is found, directing everything underneath.

Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular).

Platonism is an ancient school of philosophy, founded by Plato; at the beginning, this school had a physical existence at a site just outside the walls of Athens called the Academy, as well as the intellectual unity of a shared approach to philosophizing.

Platonism is usually divided into three periods:


Plato's students used the hypomnemata as the foundation to his philosophical approach to knowledge. The hypomnemata constituted a material memory of things read, heard, or thought, thus offering these as an accumulated treasure for rereading and later meditation. For the Neoplatonist they also formed a raw material for the writing of more systematic treatises in which were given arguments and means by which to struggle against some defect (such as anger, envy, gossip, flattery) or to overcome some difficult circumstance (such as a mourning, an exile, downfall, disgrace).

Platonism is considered to be, in mathematics departments the world over, the predominant philosophy of mathematics, especially regarding the foundations of mathematics.

One statement of this philosophy is the thesis that mathematics is not created but discovered.
A lucid statement of this is found in an essay written by the British mathematician G. H. Hardy in defense of pure mathematics. 

The absence in this thesis of clear distinction between mathematical and non-mathematical "creation" leaves open the inference that it applies to allegedly creative endeavors in art, music, and literature.

It is unknown if Plato's ideas of idealism have some earlier origin, but Plato held Pythagoras in high regard, and Pythagoras as well as his followers in the movement known as Pythagoreanism claimed the world was literally built up from numbers, an abstract, absolute form.




</doc>
<doc id="23633" url="https://en.wikipedia.org/wiki?curid=23633" title="List of physicists">
List of physicists

Following is a list of physicists who are notable for their achievements.






























</doc>
<doc id="23634" url="https://en.wikipedia.org/wiki?curid=23634" title="Protein">
Protein

Proteins are large biomolecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, providing structure to cells, and organisms, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific three-dimensional structure that determines its activity.

A linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20–30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; however, in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Sometimes proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.

Once formed, proteins only exist for a certain period and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.

Like other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for use in the metabolism.

Proteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.

Most proteins consist of linear polymers built from series of up to 20 different -α- amino acids. All proteinogenic amino acids possess common structural features, including an α-carbon to which an amino group, a carboxyl group, and a variable side chain are bonded. Only proline differs from this basic structure as it contains an unusual ring to the N-end amine group, which forces the CO–NH amide moiety into a fixed conformation. The side chains of the standard amino acids, detailed in the list of standard amino acids, have a great variety of chemical structures and properties; it is the combined effect of all of the amino acid side chains in a protein that ultimately determines its three-dimensional structure and its chemical reactivity.
The amino acids in a polypeptide chain are linked by peptide bonds. Once linked in the protein chain, an individual amino acid is called a "residue," and the linked series of carbon, nitrogen, and oxygen atoms are known as the "main chain" or "protein backbone."

The peptide bond has two resonance forms that contribute some double-bond character and inhibit rotation around its axis, so that the alpha carbons are roughly coplanar. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. The end with a free amino group is known as the N-terminus or amino terminus, whereas the end of the protein with a free carboxyl group is known as the C-terminus or carboxy terminus (the sequence of the protein is written from N-terminus to C-terminus, from left to right).

The words "protein", "polypeptide," and "peptide" are a little ambiguous and can overlap in meaning. "Protein" is generally used to refer to the complete biological molecule in a stable conformation, whereas "peptide" is generally reserved for a short amino acid oligomers often lacking a stable three-dimensional structure. However, the boundary between the two is not well defined and usually lies near 20–30 residues. "Polypeptide" can refer to any single linear chain of amino acids, usually regardless of length, but often implies an absence of a defined conformation.

Proteins can interact with many types of molecules, including with other proteins, with lipids, with carboyhydrates, and with DNA.

It has been estimated that average-sized bacteria contain about 2 million proteins per cell (e.g. "E. coli" and "Staphylococcus aureus"). Smaller bacteria, such as "Mycoplasma" or "spirochetes" contain fewer molecules, on the order of 50,000 to 1 million. By contrast, eukaryotic cells are larger and thus contain much more protein. For instance, yeast cells have been estimated to contain about 50 million proteins and human cells on the order of 1 to 3 billion. The concentration of individual protein copies ranges from a few molecules per cell up to 20 million. Not all genes coding proteins are expressed in most cells and their number depends on, for example, cell type and external stimuli. For instance, of the 20,000 or so proteins encoded by the human genome, only 6,000 are detected in lymphoblastoid cells. Moreover, the number of proteins the genome encodes correlates well with the organism complexity. Eukaryotes have 15,000, bacteria have 3,200, archaea have 2,400, and viruses have 42 proteins on average coded in their respective genomes.

Proteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine-uracil-guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (also known as a "primary transcript") using various forms of Post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.

The process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase "charges" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the "nascent chain". Proteins are always biosynthesized from N-terminus to C-terminus.

The size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported in units of "daltons" (synonymous with atomic mass units), or the derivative unit kilodalton (kDa). The average size of a protein increases from Archaea to Bacteria to Eukaryote (283, 311, 438 residues and 31, 34, 49 kDa respecitvely) due to a bigger number of protein domains constituting proteins in higher organisms. For instance, yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost 3,000 kDa and a total length of almost 27,000 amino acids.

Short proteins can also be synthesized chemically by a family of methods known as peptide synthesis, which rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.

Most proteins fold into unique 3-dimensional structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:

Proteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as "conformations", and transitions between them are called "conformational changes." Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution proteins also undergo variation in structure through thermal vibration and the collision with other molecules.

Proteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.

A special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.

Many proteins are composed of several protein domains, i.e. segments of a protein that fold into distinct structural units. Domains usually also have specific functions, such as enzymatic activities (e.g. kinase) or they serve as binding modules (e.g. the SH3 domain binds to proline-rich sequences in other proteins).

Short amino acid sequences within proteins often act as recognition sites for other proteins. For instance, SH3 domains typically bind to short PxxP motifs (i.e. 2 prolines [P], separated by two unspecified amino acids [x], although the surrounding amino acids may determine the exact binding specificity). Many such motifs has been collected in the Eukaryotic Linear Motif (ELM) database.

Proteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an "Escherichia coli" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.

The chief characteristic of proteins that also allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or "pocket" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (<10 M) but does not bind at all to its amphibian homolog onconase (>1 M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.

Proteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein–protein interactions also regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can also bind to, or even be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.
As interactions between proteins are reversible, and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.

The best-known role of proteins in the cell is as enzymes, which catalyse chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalysed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous—as much as 10-fold increase in rate over the uncatalysed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).

The molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction—three to four residues on average—that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.

Dirigent proteins are members of a class of proteins that dictate the stereochemistry of a compound synthesized by other enzymes.

Many proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.

Antibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.

Many ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, but must also release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.

Transmembrane proteins can also serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.

Structural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can also play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.

Other proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They also generate the forces exerted by contracting muscles and play essential roles in intracellular transport.

The activities and structures of proteins may be examined "in vitro," "in vivo, and in silico". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.

To perform "in vitro" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according to their charge using electrofocusing.

For natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a "tag" consisting of a specific amino acid sequence, often a series of histidine residues (a "His-tag"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of different tags have been developed to help researchers purify specific proteins from complex mixtures.

The study of proteins "in vivo" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a "reporter" such as green fluorescent protein (GFP). The fused protein's position within the cell can be cleanly and efficiently visualized using microscopy, as shown in the figure opposite.

Other methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.

Other possibilities exist, as well. For example, immunohistochemistry usually utilizes an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it does increase the likelihood, and is more amenable to large-scale studies.

Finally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique also uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.

Through another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.

The total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of many proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of the various proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein–protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.

A vast array of computational methods have been developed to analyze the structure, function and evolution of proteins.

The development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.

Discovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function and how it can be affected, i.e. in drug design. As proteins are too small to be seen under a light microscope, other methods have to be employed to determine their structure. Common experimental methods include X-ray crystallography and NMR spectroscopy, both of which can produce structural information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal β-sheet / α-helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can also produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.

Many more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins and large protein complexes, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.

Complementary to the field of structural genomics, "protein structure prediction" develops efficient mathematical models of proteins to computationally predict the molecular formations in theory, instead of detecting structures with laboratory observation. The most successful type of structure prediction, known as homology modeling, relies on the existence of a "template" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a "perfect" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein–protein interaction prediction.

Mathematical models to simulate dynamic processes of protein folding and binding involve molecular mechanics, in particular, molecular dynamics. Monte Carlo techniques facilitate the computations, which exploit advances in parallel and distributed computing (for example, the Folding@home project which performs molecular modeling on GPUs). "In silico" simulations discovered the folding of small α-helical protein domains such as the villin headpiece and the HIV accessory protein. Hybrid methods combining standard molecular dynamics with quantum mechanical mathematics explored the electronic states of rhodopsins.

Many proteins (in Eucaryota ~33%) contain large unstructured but biologically functional segments and can be classified as intrinsically disordered proteins. Predicting and analysing protein disorder is, therefore, an important part of protein structure characterisation.

Most microorganisms and plants can biosynthesize all 20 standard amino acids, while animals (including humans) must obtain some of the amino acids from the diet. The amino acids that an organism cannot synthesize on its own are referred to as essential amino acids. Key enzymes that synthesize certain amino acids are not present in animals—such as aspartokinase, which catalyses the first step in the synthesis of lysine, methionine, and threonine from aspartate. If amino acids are present in the environment, microorganisms can conserve energy by taking up the amino acids from their surroundings and downregulating their biosynthetic pathways.

In animals, amino acids are obtained through the consumption of foods containing protein. Ingested proteins are then broken down into amino acids through digestion, which typically involves denaturation of the protein through exposure to acid and hydrolysis by enzymes called proteases. Some ingested amino acids are used for protein biosynthesis, while others are converted to glucose through gluconeogenesis, or fed into the citric acid cycle. This use of protein as a fuel is particularly important under starvation conditions as it allows the body's own proteins to be used to support life, particularly those found in muscle.

In animals such as dogs and cats, protein maintains the health and quality of the skin by promoting hair follicle growth and keratinization, and thus reducing the likelihood of skin problems producing malodours. Poor-quality proteins also have a role regarding gastrointestinal health, increasing the potential for flatulence and odorous compounds in dogs because when proteins reach the colon in an undigested state, they are fermented producing hydrogen sulfide gas, indole, and skatole. Dogs and cats digest animal proteins better than those from plants, but products of low-quality animal origin are poorly digested, including skin, feathers, and connective tissue.

Proteins were recognized as a distinct class of biological molecules in the eighteenth century by Antoine Fourcroy and others, distinguished by the molecules' ability to coagulate or flocculate under treatments with heat or acid. Noted examples at the time included albumin from egg whites, blood serum albumin, fibrin, and wheat gluten.

Proteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, CHNOPS. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term "protein" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος ("proteios"), meaning "primary", "in the lead", or "standing in front", + "-in". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da. Prior to "protein", other names were used, like "albumins" or "albuminous materials" ("Eiweisskörper", in German).

Early nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that "flesh makes flesh." Karl Heinrich Ritthausen extended known protein forms with the identification of glutamic acid. At the Connecticut Agricultural Experiment Station a detailed review of the vegetable proteins was compiled by Thomas Burr Osborne. Working with Lafayette Mendel and applying Liebig's law of the minimum in feeding laboratory rats, the nutritionally essential amino acids were established. The work was continued and communicated by William Cumming Rose. The understanding of proteins as polypeptides came through the work of Franz Hofmeister and Hermann Emil Fischer in 1902. The central role of proteins as enzymes in living organisms was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.

The difficulty in purifying proteins in large quantities made them very difficult for early protein biochemists to study. Hence, early studies focused on proteins that could be purified in large quantities, e.g., those of blood, egg white, various toxins, and digestive/metabolic enzymes obtained from slaughterhouses. In the 1950s, the Armour Hot Dog Co. purified 1 kg of pure bovine pancreatic ribonuclease A and made it freely available to scientists; this gesture helped ribonuclease A become a major target for biochemical study for the following decades.

Linus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.

The first protein to be sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958.

The first protein structures to be solved were hemoglobin and myoglobin, by Max Perutz and Sir John Cowdery Kendrew, respectively, in 1958. , the Protein Data Bank has over 126,060 atomic-resolution structures of proteins. In more recent times, cryo-electron microscopy of large macromolecular assemblies and computational protein structure prediction of small protein domains are two methods approaching atomic resolution.





</doc>
<doc id="23635" url="https://en.wikipedia.org/wiki?curid=23635" title="Physical chemistry">
Physical chemistry

Physical chemistry is the study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of the principles, practices, and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics, analytical dynamics and chemical equilibrium.

Physical chemistry, in contrast to chemical physics, is predominantly (but not always) a macroscopic or supra-molecular science, as the majority of the principles on which it was founded relate to the bulk rather than the molecular/atomic structure alone (for example, chemical equilibrium and colloids).

Some of the relationships that physical chemistry strives to resolve include the effects of:

The key concepts of physical chemistry are the ways in which pure physics is applied to chemical problems.

One of the key concepts in classical chemistry is that all chemical compounds can be described as groups of atoms bonded together and chemical reactions can be described as the making and breaking of those bonds. Predicting the properties of chemical compounds from a description of atoms and how they bond is one of the major goals of physical chemistry. To describe the atoms and bonds precisely, it is necessary to know both where the nuclei of the atoms are, and how electrons are distributed around them.<br>
Quantum chemistry, a subfield of physical chemistry especially concerned with the application of quantum mechanics to chemical problems, provides tools to determine how strong and what shape bonds are, how nuclei move, and how light can be absorbed or emitted by a chemical compound. Spectroscopy is the related sub-discipline of physical chemistry which is specifically concerned with the interaction of electromagnetic radiation with matter.

Another set of important questions in chemistry concerns what kind of reactions can happen spontaneously and which properties are possible for a given chemical mixture. This is studied in chemical thermodynamics, which sets limits on quantities like how far a reaction can proceed, or how much energy can be converted into work in an internal combustion engine, and which provides links between properties like the thermal expansion coefficient and rate of change of entropy with pressure for a gas or a liquid. It can frequently be used to assess whether a reactor or engine design is feasible, or to check the validity of experimental data. To a limited extent, quasi-equilibrium and non-equilibrium thermodynamics can describe irreversible changes. However, classical thermodynamics is mostly concerned with systems in equilibrium and reversible changes and not what actually does happen, or how fast, away from equilibrium.

Which reactions do occur and how fast is the subject of chemical kinetics, another branch of physical chemistry. A key idea in chemical kinetics is that for reactants to react and form products, most chemical species must go through transition states which are higher in energy than either the reactants or the products and serve as a barrier to reaction. In general, the higher the barrier, the slower the reaction. A second is that most chemical reactions occur as a sequence of elementary reactions, each with its own transition state. Key questions in kinetics include how the rate of reaction depends on temperature and on the concentrations of reactants and catalysts in the reaction mixture, as well as how catalysts and reaction conditions can be engineered to optimize the reaction rate.

The fact that how fast reactions occur can often be specified with just a few concentrations and a temperature, instead of needing to know all the positions and speeds of every molecule in a mixture, is a special case of another key concept in physical chemistry, which is that to the extent an engineer needs to know, everything going on in a mixture of very large numbers (perhaps of the order of the Avogadro constant, 6 x 10) of particles can often be described by just a few variables like pressure, temperature, and concentration. The precise reasons for this are described in statistical mechanics, a specialty within physical chemistry which is also shared with physics. Statistical mechanics also provides ways to predict the properties we see in everyday life from molecular properties without relying on empirical correlations based on chemical similarities.

The term "physical chemistry" was coined by Mikhail Lomonosov in 1752, when he presented a lecture course entitled "A Course in True Physical Chemistry" (Russian: «Курс истинной физической химии») before the students of Petersburg University. In the preamble to these lectures he gives the definition: "Physical chemistry is the science that must explain under provisions of physical experiments the reason for what is happening in complex bodies through chemical operations".

Modern physical chemistry originated in the 1860s to 1880s with work on chemical thermodynamics, electrolytes in solutions, chemical kinetics and other subjects. One milestone was the publication in 1876 by Josiah Willard Gibbs of his paper, "On the Equilibrium of Heterogeneous Substances". This paper introduced several of the cornerstones of physical chemistry, such as Gibbs energy, chemical potentials, and Gibbs' phase rule.

The first scientific journal specifically in the field of physical chemistry was the German journal, "Zeitschrift für Physikalische Chemie", founded in 1887 by Wilhelm Ostwald and Jacobus Henricus van 't Hoff. Together with Svante August Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. All three were awarded the Nobel Prize in Chemistry between 1901–1909.

Developments in the following decades include the application of statistical mechanics to chemical systems and work on colloids and surface chemistry, where Irving Langmuir made many contributions. Another important step was the development of quantum mechanics into quantum chemistry from the 1930s, where Linus Pauling was one of the leading names. Theoretical developments have gone hand in hand with developments in experimental methods, where the use of different forms of spectroscopy, such as infrared spectroscopy, microwave spectroscopy, electron paramagnetic resonance and nuclear magnetic resonance spectroscopy, is probably the most important 20th century development.

Further development in physical chemistry may be attributed to discoveries in nuclear chemistry, especially in isotope separation (before and during World War II), more recent discoveries in astrochemistry, as well as the development of calculation algorithms in the field of "additive physicochemical properties" (practically all physicochemical properties, such as boiling point, critical point, surface tension, vapor pressure, etc.—more than 20 in all—can be precisely calculated from chemical structure alone, even if the chemical molecule remains unsynthesized), and herein lies the practical importance of contemporary physical chemistry.

See Group contribution method, Lydersen method, Joback method, Benson group increment theory, quantitative structure–activity relationship

Some journals that deal with physical chemistry include Zeitschrift für Physikalische Chemie (1887); Journal of Physical Chemistry A (from 1896 as "Journal of Physical Chemistry", renamed in 1997); Physical Chemistry Chemical Physics (from 1999, formerly Faraday Transactions with a history dating back to 1905); Macromolecular Chemistry and Physics (1947); Annual Review of Physical Chemistry (1950); Molecular Physics (1957); Journal of Physical Organic Chemistry (1988); Journal of Physical Chemistry B (1997); ChemPhysChem (2000); Journal of Physical Chemistry C (2007); and Journal of Physical Chemistry Letters (from 2010, combined letters previously published in the separate journals)

Historical journals that covered both chemistry and physics include Annales de chimie et de physique (started in 1789, published under the name given here from 1815–1914).




</doc>
<doc id="23636" url="https://en.wikipedia.org/wiki?curid=23636" title="Perimeter">
Perimeter

A perimeter is a path that encompasses/surrounds a two-dimensional shape. The term may be used either for the path, or its length—in one dimension. It can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference. 

Calculating the perimeter has several practical applications. A calculated perimeter is the length of fence required to surround a yard or garden. The perimeter of a wheel/circle (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter; if the length of the string was exact, it would equal the perimeter.

The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated, as any path, with formula_1, where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced with by algebraic forms in order to be practically calculated. If the perimeter is given as a closed piecewise smooth plane curve formula_4 with
then its length formula_2 can be computed as follows:

A generalized notion of perimeter, which includes hypersurfaces bounding volumes in formula_8-dimensional Euclidean spaces, is described by the theory of Caccioppoli sets.

Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.

The perimeter of a polygon equals the sum of the lengths of its sides (edges). In particular, the perimeter of a rectangle of width formula_9 and length formula_10 equals formula_11

An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.

A regular polygon may be characterized by the number of its sides and by its circumradius, that is to say, the constant distance between its centre and each of its vertices. The length of its sides can be calculated using trigonometry. If is a regular polygon's radius and is the number of its sides, then its perimeter is 

A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. The three splitters of a triangle all intersect each other at the Nagel point of the triangle.

A cleaver of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's Spieker center.

The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, (the Greek "p" for perimeter), such that if is the circle's perimeter and its diameter then,

In terms of the radius of the circle, this formula becomes,

To calculate a circle's perimeter, knowledge of its radius or diameter and the number suffices. The problem is that is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of is important in the calculation. The computation of the digits of is relevant to many fields, such as mathematical analysis, algorithmics and computer science.

The perimeter and the area are two main measures of geometric figures. Confusing them is a common error, as well as believing that the greater one of them is, the greater the other must be. Indeed, a commonplace observation is that an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is times the area of the shape on the map. Nevertheless, there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.

Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. However, a field's production is proportional to its area, not to its perimeter, so many naive peasants may have gotten fields with long perimeters but small areas (thus, few crops).

If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, confusion between the perimeter and the convex hull may arise. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. In the animated picture on the left, all the figures have the same convex hull; the big, first hexagon.

The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive; it is the circle. In particular, this can be used to explain why drops of fat on a broth surface are circular.

This problem may seem simple, but its mathematical proof requires some sophisticated theorems. The isoperimetric problem is sometimes simplified by restricting the type of figures to be used. In particular, to find the quadrilateral, or the triangle, or another particular figure, with the largest area amongst those with the same shape having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is any irregular polygon with the same number of sides.

The word comes from the Greek περίμετρος "perimetros" from περί "peri" "around" and μέτρον "metron" "measure".




</doc>
<doc id="23637" url="https://en.wikipedia.org/wiki?curid=23637" title="Phase (matter)">
Phase (matter)

In the physical sciences, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform. Examples of physical properties include density, index of refraction, magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air is a third phase over the ice and water. The glass of the jar is another separate phase. (See )

The term "phase" is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter. Also, the term "phase" is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter, such as a change from liquid to solid or a more subtle change from one crystal structure to another, this latter usage is similar to the use of "phase" as a synonym for state of matter. However, the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.

Distinct phases may be described as different states of matter such as gas, liquid, solid, plasma or Bose–Einstein condensate. Useful mesophases between solid and liquid form other states of matter.

Distinct phases may also exist within a given state of matter. As shown in the diagram for iron alloys, several phases exist for both the solid and liquid states. Phases may also be differentiated based on solubility as in polar (hydrophilic) or non-polar (hydrophobic). A mixture of water (a polar liquid) and oil (a non-polar liquid) will spontaneously separate into two phases. Water has a very low solubility (is insoluble) in oil, and oil has a low solubility in water. Solubility is the maximum amount of a solute that can dissolve in a solvent before the solute ceases to dissolve and remains in a separate phase. A mixture can separate into more than two liquid phases and the concept of phase separation extends to solids, i.e., solids can form solid solutions or crystallize into distinct crystal phases. Metal pairs that are mutually soluble can form alloys, whereas metal pairs that are mutually insoluble cannot.

As many as eight immiscible liquid phases have been observed. Mutually immiscible liquid phases are formed from water (aqueous phase), hydrophobic organic solvents, perfluorocarbons (fluorous phase), silicones, several different metals, and also from molten phosphorus. Not all organic solvents are completely miscible, e.g. a mixture of ethylene glycol and toluene may separate into two distinct organic phases.

Phases do not need to macroscopically separate spontaneously. Emulsions and colloids are examples of immiscible phase pair combinations that do not physically separate.
Left to equilibration, many compositions will form a uniform single phase, but depending on the temperature and pressure even a single substance may separate into two or more distinct phases. Within each phase, the properties are uniform but between the two phases properties differ.

Water in a closed jar with an air space over it forms a two phase system. Most of the water is in the liquid phase, where it is held by the mutual attraction of water molecules. Even at equilibrium molecules are constantly in motion and, once in a while, a molecule in the liquid phase gains enough kinetic energy to break away from the liquid phase and enter the gas phase. Likewise, every once in a while a vapor molecule collides with the liquid surface and condenses into the liquid. At equilibrium, evaporation and condensation processes exactly balance and there is no net change in the volume of either phase.

At room temperature and pressure, the water jar reaches equilibrium when the air over the water has a humidity of about 3%. This percentage increases as the temperature goes up. At 100 °C and atmospheric pressure, equilibrium is not reached until the air is 100% water. If the liquid is heated a little over 100 °C, the transition from liquid to gas will occur not only at the surface, but throughout the liquid volume: the water boils.

For a given composition, only certain phases are possible at a given temperature and pressure. The number and type of phases that will form is hard to predict and is usually determined by experiment. The results of such experiments can be plotted in phase diagrams.

The phase diagram shown here is for a single component system. In this simple system, which phases that are possible depends only on pressure and temperature. The markings show points where two or more phases can co-exist in equilibrium. At temperatures and pressures away from the markings, there will be only one phase at equilibrium.

In the diagram, the blue line marking the boundary between liquid and gas does not continue indefinitely, but terminates at a point called the critical point. As the temperature and pressure approach the critical point, the properties of the liquid and gas become progressively more similar. At the critical point, the liquid and gas become indistinguishable. Above the critical point, there are no longer separate liquid and gas phases: there is only a generic fluid phase referred to as a supercritical fluid. In water, the critical point occurs at around 647 K (374 °C or 705 °F) and 22.064 MPa.

An unusual feature of the water phase diagram is that the solid–liquid phase line (illustrated by the dotted green line) has a negative slope. For most substances, the slope is positive as exemplified by the dark green line. This unusual feature of water is related to ice having a lower density than liquid water. Increasing the pressure drives the water into the higher density phase, which causes melting.

Another interesting though not unusual feature of the phase diagram is the point where the solid–liquid phase line meets the liquid–gas phase line. The intersection is referred to as the triple point. At the triple point, all three phases can coexist.

Experimentally, the phase lines are relatively easy to map due to the interdependence of temperature and pressure that develops when multiple phases forms. See Gibbs' phase rule. Consider a test apparatus consisting of a closed and well insulated cylinder equipped with a piston. By charging the right amount of water and applying heat, the system can be brought to any point in the gas region of the phase diagram. If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed.

Between two phases in equilibrium there is a narrow region where the properties are not that of either phase. Although this region may be very thin, it can have significant and easily observable effects, such as causing a liquid to exhibit surface tension. In mixtures, some components may preferentially move toward the interface. In terms of modeling, describing, or understanding the behavior of a particular system, it may be efficacious to treat the interfacial region as a separate phase.

A single material may have several distinct solid states capable of forming separate phases. Water is a well-known example of such a material. For example, water ice is ordinarily found in the hexagonal form ice I, but can also exist as the cubic ice I, the rhombohedral ice II, and many other forms. Polymorphism is the ability of a solid to exist in more than one crystal form. For pure chemical elements, polymorphism is known as allotropy. For example, diamond, graphite, and fullerenes are different allotropes of carbon.

When a substance undergoes a phase transition (changes from one state of matter to another) it usually either takes up or releases energy. For example, when water evaporates, the increase in kinetic energy as the evaporating molecules escape the attractive forces of the liquid is reflected in a decrease in temperature. The energy required to induce the phase transition is taken from the internal thermal energy of the water, which cools the liquid to a lower temperature; hence evaporation is useful for cooling. See Enthalpy of vaporization. The reverse process, condensation, releases heat. The heat energy, or enthalpy, associated with a solid to liquid transition is the enthalpy of fusion and that associated with a solid to gas transition is the enthalpy of sublimation.

While phases of matter are traditionally defined for systems in thermal equilibrium, work on quantum many-body localized (MBL) systems has provided a framework for defining phases out of equilibrium. MBL phases never reach thermal equilibrium, and can allow for new forms of order disallowed in equilibrium via a phenomenon known as localization protected quantum order. The transitions between different MBL phases and between MBL and thermalizing phases are novel dynamical phase transitions whose properties are active areas of research.



</doc>
<doc id="23638" url="https://en.wikipedia.org/wiki?curid=23638" title="Outline of physical science">
Outline of physical science

Physical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences".

Physical science can be described as all of the following:


History of physical science – history of the branch of natural science that studies non-living systems, in contrast to the life sciences. It in turn has many branches, each referred to as a "physical science", together called the "physical sciences". However, the term "physical" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).


Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:


Astronomy – science of celestial bodies and their interactions in space. Its studies includes the following:


Chemistry – branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.


Earth science – the science of the planet Earth, the only identified life-bearing planet. Its studies include the following:








</doc>
<doc id="23639" url="https://en.wikipedia.org/wiki?curid=23639" title="Gasoline">
Gasoline

Gasoline (American English), or petrol (British English), is a colorless petroleum-derived flammable liquid that is used primarily as a fuel in most spark-ignited internal combustion engines. It consists mostly of organic compounds obtained by the fractional distillation of petroleum, enhanced with a variety of additives. On average, a barrel of crude oil yields about of gasoline (among other refined products) after processing in an oil refinery, though this varies based on the crude oil assay.

The characteristic of a particular gasoline blend to resist igniting too early (which causes knocking and reduces efficiency in reciprocating engines) is measured by its octane rating which is produced in several grades. Tetraethyl lead and other lead compounds are no longer used in most areas to increase octane rating (still used in aviation and auto-racing). Other chemicals are frequently added to gasoline to improve chemical stability and performance characteristics, control corrosiveness and provide fuel system cleaning. Gasoline may contain oxygen-containing chemicals such as ethanol, MTBE or ETBE to improve combustion.

Gasoline used in internal combustion engines can have significant effects on the local environment, and is also a contributor to global human carbon dioxide emissions. Gasoline can also enter the environment uncombusted, both as liquid and as vapor, from leakage and handling during production, transport and delivery (e.g., from storage tanks, from spills, etc.). As an example of efforts to control such leakage, many underground storage tanks are required to have extensive measures in place to detect and prevent such leaks. Gasoline contains benzene and other known carcinogens.

"Gasoline" is an English word that refers to fuel for automobiles. The "Oxford English Dictionary" dates its first recorded use to 1863 when it was spelled "gasolene". The term "gasoline" was first used in North America in 1864. The word is a derivation from the word "gas" and the chemical suffixes "-ol" and "-ine" or "-ene".

However, the term may also have been influenced by the trademark "Cazeline" or "Gazeline". On 27 November 1862, the British publisher, coffee merchant and social campaigner John Cassell placed an advertisement in "The Times" of London:

In most Commonwealth countries, the product is called "petrol", rather than "gasoline". "Petrol" was first used in about 1870, as the name of a refined petroleum product sold by British wholesaler Carless, Capel & Leonard, which marketed it as a solvent. When the product later found a new use as a motor fuel, Frederick Simms, an associate of Gottlieb Daimler, suggested to Carless that they register the trademark "petrol", but by that time the word was already in general use, possibly inspired by the French "pétrole", and the registration was not allowed. Carless registered a number of alternative names for the product, but "petrol" nonetheless became the common term for the fuel in the British Commonwealth.

British refiners originally used "motor spirit" as a generic name for the automotive fuel and "aviation spirit" for aviation gasoline. When Carless was denied a trademark on "petrol" in the 1930s, its competitors switched to the more popular name "petrol". However, "motor spirit" had already made its way into laws and regulations, so the term remains in use as a formal name for petrol. The term is used most widely in Nigeria, where the largest petroleum companies call their product "premium motor spirit". Although "petrol" has made inroads into Nigerian English, "premium motor spirit" remains the formal name that is used in scientific publications, government reports, and newspapers.

The use of the word "gasoline" instead of "petrol" is uncommon outside North America, particularly given the usual shortening of "gasoline" to "gas", because various forms of gaseous products are also used as automotive fuel, such as compressed natural gas (CNG), liquefied natural gas (LNG) and liquefied petroleum gas (LPG). In many languages, the name of the product is derived from benzene, such as "Benzin" in German or "benzina" in Italian. Argentina, Uruguay and Paraguay use the colloquial name "nafta" derived from that of the chemical naphtha.

The first internal combustion engines suitable for use in transportation applications, so-called Otto engines, were developed in Germany during the last quarter of the 19th century. The fuel for these early engines was a relatively volatile hydrocarbon obtained from coal gas. With a boiling point near (octane boils about 40 °C higher), it was well-suited for early carburetors (evaporators). The development of a "spray nozzle" carburetor enabled the use of less volatile fuels. Further improvements in engine efficiency were attempted at higher compression ratios, but early attempts were blocked by the premature explosion of fuel, known as knocking.

In 1891, the Shukhov cracking process became the world's first commercial method to break down heavier hydrocarbons in crude oil to increase the percentage of lighter products compared to simple distillation.

The evolution of gasoline followed the evolution of oil as the dominant source of energy in the industrializing world. Prior to World War One, Britain was the world's greatest industrial power and depended on its navy to protect the shipping of raw materials from its colonies. Germany was also industrializing and, like Britain, lacked many natural resources which had to be shipped to the home country. By the 1890s, Germany began to pursue a policy of global prominence and began building a navy to compete with Britain's. Coal was the fuel that powered their navies. Though both Britain and Germany had natural coal reserves, new developments in oil as a fuel for ships changed the situation. Coal-powered ships were a tactical weakness because the process of loading coal was extremely slow and dirty and left the ship completely vulnerable to attack, and unreliable supplies of coal at international ports made long-distance voyages impractical. The advantages of petroleum oil soon found the navies of the world converting to oil, but Britain and Germany had very few domestic oil reserves. Britain eventually solved its naval oil dependence by securing oil from Royal Dutch Shell and the Anglo-Persian Oil Company and this determined from where and of what quality its gasoline would come.

During the early period of gasoline engine development, aircraft were forced to use motor vehicle gasoline since aviation gasoline did not yet exist. These early fuels were termed "straight-run" gasolines and were byproducts from the distillation of a single crude oil to produce kerosene, which was the principal product sought for burning in kerosene lamps. Gasoline production would not surpass kerosene production until 1916. The earliest straight-run gasolines were the result of distilling eastern crude oils and there was no mixing of distillates from different crudes. The composition of these early fuels was unknown and the quality varied greatly as crude oils from different oil fields emerged in different mixtures of hydrocarbons in different ratios. The engine effects produced by abnormal combustion (engine knocking and pre-ignition) due to inferior fuels had not yet been identified, and as a result there was no rating of gasoline in terms of its resistance to abnormal combustion. The general specification by which early gasolines were measured was that of specific gravity via the Baumé scale and later the volatility (tendency to vaporize) specified in terms of boiling points, which became the primary focuses for gasoline producers. These early eastern crude oil gasolines had relatively high Baumé test results (65 to 80 degrees Baumé) and were called Pennsylvania "High-Test" or simply "High-Test" gasolines. These would often be used in aircraft engines.

By 1910, increased automobile production and the resultant increase in gasoline consumption produced a greater demand for gasoline. Also, the growing electrification of lighting produced a drop in kerosene demand, creating a supply problem. It appeared that the burgeoning oil industry would be trapped into over-producing kerosene and under-producing gasoline since simple distillation could not alter the ratio of the two products from any given crude. The solution appeared in 1911 when the development of the Burton process allowed thermal cracking of crude oils, which increased the percent yield of gasoline from the heavier hydrocarbons. This was combined with expansion of foreign markets for the export of surplus kerosene which domestic markets no longer needed. These new thermally "cracked" gasolines were believed to have no harmful effects and would be added to straight-run gasolines. There also was the practice of mixing heavy and light distillates to achieve a desired Baumé reading and collectively these were called "blended" gasolines.

Gradually, volatility gained favor over the Baumé test, though both would continue to be used in combination to specify a gasoline. As late as June 1917, Standard Oil (the largest refiner of crude oil in the United States at the time) stated that the most important property of a gasoline was its volatility. It is estimated that the rating equivalent of these straight-run gasolines varied from 40 to 60 octane and that the "High-Test", sometimes referred to as "fighting grade", probably averaged 50 to 65 octane.

Prior to the American entry into World War I, the European Allies used fuels derived from crude oils from Borneo, Java and Sumatra, which gave satisfactory performance in their military aircraft. When the United States entered the war in April 1917, the U.S. became the principal supplier of aviation gasoline to the Allies and a decrease in engine performance was noted. Soon it was realized that motor vehicle fuels were unsatisfactory for aviation, and after the loss of a number of combat aircraft, attention turned to the quality of the gasolines being used. Later flight tests conducted in 1937 showed that an octane reduction of 13 points (from 100 down to 87 octane) decreased engine performance by 20 percent and increased take-off distance by 45 percent. If abnormal combustion were to occur, the engine could lose enough power to make getting airborne impossible and a take-off roll became a threat to the pilot and aircraft.

On 2 August 1917, the United States Bureau of Mines arranged to study fuels for aircraft in cooperation with the Aviation Section of the U.S. Army Signal Corps and a general survey concluded that no reliable data existed for the proper fuels for aircraft. As a result, flight tests began at Langley, McCook and Wright fields to determine how different gasolines performed under different conditions. These tests showed that in certain aircraft, motor vehicle gasolines performed as well as "High-Test" but in other types resulted in hot-running engines. It was also found that gasolines from aromatic and naphthenic base crude oils from California, South Texas and Venezuela resulted in smooth-running engines. These tests resulted in the first government specifications for motor gasolines (aviation gasolines used the same specifications as motor gasolines) in late 1917.

Engine designers knew that, according to the Otto cycle, power and efficiency increased with compression ratio, but experience with early gasolines during World War I showed that higher compression ratios increased the risk of abnormal combustion, producing lower power, lower efficiency, hot-running engines and potentially severe engine damage. To compensate for these poor fuels, early engines used low compression ratios, which required relatively large, heavy engines to produce limited power and efficiency. The Wright brothers' first gasoline engine used a compression ratio as low as 4.7-to-1, developed only from and weighed . This was a major concern for aircraft designers and the needs of the aviation industry provoked the search for fuels that could be used in higher-compression engines.

Between 1917 and 1919, the amount of thermally cracked gasoline utilized almost doubled. Also, the use of natural gasoline increased greatly. During this period, many U.S. states established specifications for motor gasoline but none of these agreed and were unsatisfactory from one standpoint or another. Larger oil refiners began to specify unsaturated material percentage (thermally cracked products caused gumming in both use and storage and unsaturated hydrocarbons are more reactive and tend to combine with impurities leading to gumming). In 1922, the U.S. government published the first specifications for aviation gasolines (two grades were designated as "Fighting" and "Domestic" and were governed by boiling points, color, sulphur content and a gum formation test) along with one "Motor" grade for automobiles. The gum test essentially eliminated thermally cracked gasoline from aviation usage and thus aviation gasolines reverted to fractionating straight-run naphthas or blending straight-run and highly treated thermally cracked naphthas. This situation persisted until 1929.

The automobile industry reacted to the increase in thermally cracked gasoline with alarm. Thermal cracking produced large amounts of both mono- and diolefins (unsaturated hydrocarbons), which increased the risk of gumming. Also the volatility was decreasing to the point that fuel did not vaporize and was sticking to spark plugs and fouling them, creating hard starting and rough running in winter and sticking to cylinder walls, bypassing the pistons and rings and going into the crankcase oil. One journal stated, "...on a multi-cylinder engine in a high-priced car we are diluting the oil in the crankcase as much as 40 percent in a 200-mile run, as the analysis of the oil in the oil-pan shows."

Being very unhappy with the consequent reduction in overall gasoline quality, automobile manufacturers suggested imposing a quality standard on the oil suppliers. The oil industry in turn accused the automakers of not doing enough to improve vehicle economy, and the dispute became known within the two industries as "The Fuel Problem". Animosity grew between the industries, each accusing the other of not doing anything to resolve matters, and relationships deteriorated. The situation was only resolved when the American Petroleum Institute (API) initiated a conference to address "The Fuel Problem" and a Cooperative Fuel Research (CFR) Committee was established in 1920 to oversee joint investigative programs and solutions. Apart from representatives of the two industries, the Society of Automotive Engineers (SAE) also played an instrumental role, with the U.S. Bureau of Standards being chosen as an impartial research organization to carry out many of the studies. Initially, all the programs were related to volatility and fuel consumption, ease of starting, crankcase oil dilution and acceleration.

With the increased use of thermally cracked gasolines came an increased concern regarding its effects on abnormal combustion, and this led to research for antiknock additives. In the late 1910s, researchers such as A.H. Gibson, Harry Ricardo, Thomas Midgley Jr. and Thomas Boyd began to investigate abnormal combustion. Beginning in 1916, Charles F. Kettering began investigating additives based on two paths, the "high percentage" solution (where large quantities of ethanol were added) and the "low percentage" solution (where only 2–4 grams per gallon were needed). The "low percentage" solution ultimately led to the discovery of tetraethyllead (TEL) in December 1921, a product of the research of Midgley and Boyd. This innovation started a cycle of improvements in fuel efficiency that coincided with the large-scale development of oil refining to provide more products in the boiling range of gasoline. Ethanol could not be patented but TEL could, so Kettering secured a patent for TEL and began promoting it instead of other options.

The dangers of compounds containing lead were well-established by then and Kettering was directly warned by Robert Wilson of MIT, Reid Hunt of Harvard, Yandell Henderson of Yale, and Charles Kraus of the University of Potsdam in Germany about its use. Kraus had worked on tetraethyllead for many years and called it "a creeping and malicious poison" that had killed a member of his dissertation committee. On 27 October 1924, newspaper articles around the nation told of the workers at the Standard Oil refinery near Elizabeth, New Jersey who were producing TEL and were suffering from lead poisoning. By 30 October, the death toll had reached five. In November, the New Jersey Labor Commission closed the Bayway refinery and a grand jury investigation was started which had resulted in no charges by February 1925. Leaded gasoline sales were banned in New York City, Philadelphia and New Jersey. General Motors, DuPont, and Standard Oil, who were partners in Ethyl Corporation, the company created to produce TEL, began to argue that there were no alternatives to leaded gasoline that would maintain fuel efficiency and still prevent engine knocking. After flawed studies determined that TEL-treated gasoline was not a public health issue, the controversy subsided.

In the five-year period prior to 1929, a great amount of experimentation was conducted on different testing methods for determining fuel resistance to abnormal combustion. It appeared engine knocking was dependent on a wide variety of parameters including compression, cylinder temperature, air-cooled or water-cooled engines, chamber shapes, intake temperatures, lean or rich mixtures and others. This led to a confusing variety of test engines that gave conflicting results, and no standard rating scale existed. By 1929, it was recognized by most aviation gasoline manufacturers and users that some kind of antiknock rating must be included in government specifications. In 1929, the octane rating scale was adopted, and in 1930 the first octane specification for aviation fuels was established. In the same year, the U.S. Army Air Force specified fuels rated at 87 octane for its aircraft as a result of studies it conducted.

During this period, research showed that hydrocarbon structure was extremely important to the antiknocking properties of fuel. Straight-chain paraffins in the boiling range of gasoline had low antiknock qualities while ring-shaped molecules such as aromatic hydrocarbons (an example is benzene) had higher resistance to knocking. This development led to the search for processes that would produce more of these compounds from crude oils than achieved under straight distillation or thermal cracking. Research by the major refiners into conversion processes yielded isomerization, dehydration, and alkylation that could change the cheap and abundant butane into isooctane, which became an important component in aviation fuel blending. To further complicate the situation, as engine performance increased, the altitude that aircraft could reach also increased, which resulted in concerns about the fuel freezing. The average temperature decrease is per increase in altitude, and at , the temperature can approach . Additives like benzene, with a freezing point of , would freeze in the gasoline and plug fuel lines. Substitute aromatics such as toluene, xylene and cumene combined with limited benzene solved the problem.

By 1935, there were seven different aviation grades based on octane rating, two Army grades, four Navy grades and three commercial grades including the introduction of 100-octane aviation gasoline. By 1937 the Army established 100-octane as the standard fuel for combat aircraft and to add to the confusion, the government now recognized 14 different grades, in addition to 11 others in foreign countries. With some companies required to stock 14 grades of aviation fuel, none of which could be interchanged, the effect on the refiners was negative. The refining industry could not concentrate on large capacity conversion processes for so many different grades and a solution had to be found. By 1941, principally through the efforts of the Cooperative Fuel Research Committee, the number of grades for aviation fuels was reduced to three: 73, 91 and 100 octane.

In 1937, Eugene Houdry developed the Houdry process of catalytic cracking, which produced a high-octane base stock of gasoline which was superior to the thermally cracked product since it did not contain the high concentration of olefins. In 1940, there were only 14 Houdry units in operation in the U.S.; by 1943, this had increased to 77, either of the Houdry process or of the Thermofor Catalytic or Fluid Catalyst type.

The search for fuels with octane ratings above 100 led to the extension of the scale by comparing power output. A fuel designated grade 130 would produce 130 percent as much power in an engine as it would running on pure iso-octane. During WW II, fuels above 100-octane were given two ratings, a rich and lean mixture and these would be called 'performance numbers' (PN). 100-octane aviation gasoline would be referred to as 130/100 grade.

Oil and its byproducts, especially high-octane aviation gasoline, would prove to be a driving concern for how Germany conducted the war. As a result of the lessons of World War I, Germany had stockpiled oil and gasoline for its blitzkrieg offensive and had annexed Austria, adding 18,000 barrels per day of oil production, but this was not sufficient to sustain the planned conquest of Europe. Because captured supplies and oil fields would be necessary to fuel the campaign, the German high command created a special squad of oil-field experts drawn from the ranks of domestic oil industries. They were sent in to put out oil-field fires and get production going again as soon as possible. But capturing oil fields remained an obstacle throughout the war. During the Invasion of Poland, German estimates of gasoline consumption turned out to be vastly underestimated. Heinz Guderian and his Panzer divisions consumed nearly of gasoline on the drive to Vienna. When they were engaged in combat across open country, gasoline consumption almost doubled. On the second day of battle, a unit of the XIX Corps was forced to halt when it ran out of gasoline. One of the major objectives of the Polish invasion was their oil fields but the Soviets invaded and captured 70 percent of the Polish production before the Germans could reach it. Through the German-Soviet Commercial Agreement (1940), Stalin agreed in vague terms to supply Germany with additional oil equal to that produced by now Soviet-occupied Polish oil fields at Drohobych and Boryslav in exchange for hard coal and steel tubing.

Even after the Nazis conquered the vast territories of Europe, this did not help the gasoline shortage. This area had never been self-sufficient in oil before the war. In 1938, the area that would become Nazi-occupied would produce 575,000 barrels per day. In 1940, total production under German control amounted to only —a shortfall of 59 percent. By the spring of 1941 and the depletion of German gasoline reserves, Adolf Hitler saw the invasion of Russia to seize the Polish oil fields and the Russian oil in the Caucasus as the solution to the German gasoline shortage. As early as July 1941, following the 22 June start of Operation Barbarossa, certain Luftwaffe squadrons were forced to curtail ground support missions due to shortages of aviation gasoline. On 9 October, the German quartermaster general estimated that army vehicles were barrels short of gasoline requirements.

Virtually all of Germany's aviation gasoline came from synthetic oil plants that hydrogenated coals and coal tars. These processes had been developed during the 1930s as an effort to achieve fuel independence. There were two grades of aviation gasoline produced in volume in Germany, the B-4 or blue grade and the C-3 or green grade, which accounted for about two-thirds of all production. B-4 was equivalent to 89-octane and the C-3 was roughly equal to the U.S. 100-octane, though lean mixture was rated around 95-octane and was poorer than the U.S. Maximum output achieved in 1943 reached 52,200 barrels a day before the Allies decided to target the synthetic fuel plants. Through captured enemy aircraft and analysis of the gasoline found in them, both the Allies and the Axis powers were aware of the quality of the aviation gasoline being produced and this prompted an octane race to achieve the advantage in aircraft performance. Later in the war the C-3 grade was improved to where it was equivalent to the U.S. 150 grade (rich mixture rating).

Japan, like Germany, had almost no domestic oil supply and by the late 1930s produced only 7% of its own oil while importing the rest – 80% from the United States. As Japanese aggression grew in China (USS Panay incident) and news reached the American public of Japanese bombing of civilian centers, especially the bombing of Chungking, public opinion began to support a U.S. embargo. A Gallup poll in June 1939 found that 72 percent of the American public supported an embargo on war materials to Japan. This increased tensions between the U.S. and Japan led to the U.S. placing restrictions on exports and in July 1940 the U.S. issued a proclamation that banned the export of 87 octane or higher aviation gasoline to Japan. This ban did not hinder the Japanese as their aircraft could operate with fuels below 87 octane and if needed they could add TEL to increase the octane. As it turned out, Japan bought 550 percent more sub-87 octane aviation gasoline in the five months after the July 1940 ban on higher octane sales. The possibility of a complete ban of gasoline from America created friction in the Japanese government as to what action to take to secure more supplies from the Dutch East Indies and demanded greater oil exports from the exiled Dutch government after the Battle of the Netherlands. This action prompted the U.S. to move its Pacific fleet from Southern California to Pearl Harbor to help stiffen British resolve to stay in Indochina. With the Japanese invasion of French Indochina in September 1940 came great concerns about the possible Japanese invasion of the Dutch Indies to secure their oil. After the U.S. banned all exports of steel and iron scrap, the next day Japan signed the Tripartite Pact and this led Washington to fear that a complete U.S. oil embargo would prompt the Japanese to invade the Dutch East Indies. On 16 June 1941 Harold Ickes, who was appointed Petroleum Coordinator for National Defense, stopped a shipment of oil from Philadelphia to Japan in light of the oil shortage on the East coast due to increased exports to Allies. He also telegrammed all oil suppliers on the East coast not to ship any oil to Japan without his permission. President Roosevelt countermanded Ickes' orders telling Ickes that the "... I simply have not got enough Navy to go around and every little episode in the Pacific means fewer ships in the Atlantic". On 25 July 1941 the U.S. froze all Japanese financial assets and licenses would be required for each use of the frozen funds including oil purchases that could produce aviation gasoline. On 28 July 1941 Japan invaded southern Indochina.

The debate inside the Japanese government as to its oil and gasoline situation was leading to invasion of the Dutch East Indies but this would mean war with the U.S. whose Pacific fleet was a threat to their flank. This situation led to the decision to attack the U.S. fleet at Pearl Harbor before proceeding with the Dutch East Indies invasion. On 7 December 1941 Japan attacked Pearl Harbor and the next day the Netherlands declared war on Japan which initiated the Dutch East Indies campaign. But the Japanese missed a golden opportunity at Pearl Harbor. "All of the oil for the fleet was in surface tanks at the time of Pearl Harbor," Admiral Chester Nimitz, who became Commander in Chief of the Pacific Fleet, was later to say. "We had about of oil out there and all of it was vulnerable to .50 caliber bullets. Had the Japanese destroyed the oil," he added, "it would have prolonged the war another two years."

Early in 1944, William Boyd, president of the American Petroleum Institute and chairman of the Petroleum Industry War Council said: "The Allies may have floated to victory on a wave of oil in World War I, but in this infinitely greater World War II, we are flying to victory on the wings of petroleum". In December, 1941 the United States had 385,000 oil wells producing 1.4 billion barrels of oil a year and 100-octane aviation gasoline capacity was at 40,000 barrels a day. By 1944 the U.S. was producing over 1.5 billion barrels a year (67 percent of world production) and the petroleum industry had built 122 new plants for the production of 100-octane aviation gasoline and capacity was over 400,000 barrels a day – an increase of more than ten-fold. It was estimated that the U.S. was producing enough 100-octane aviation gasoline to permit the dropping of 20,000 tons of bombs on the enemy every day of the year. The record of gasoline consumption by the Army prior to June, 1943 was uncoordinated as each supply service of the Army purchased its own petroleum products and no centralized system of control nor records existed. On 1 June 1943 the Army created the Fuels and Lubricants Division of the Quartermaster Corps and from their records they tabulated that the Army (excluding fuels and lubricants for aircraft) purchased over 2.4 billion gallons of gasoline for delivery to overseas theaters between 1 June 1943 through August, 1945. That figure does not include gasoline used by the Army inside the United States. Motor fuel production had declined from 701,000,000 barrels in 1941 down to 608,000,000 barrels in 1943. World War II marked the first time in U.S. history that gasoline was rationed and the government imposed price controls to prevent inflation. Gasoline consumption per automobile declined from 755 gallons per year in 1941 down to 540 gallons in 1943 with the goal of preserving rubber for tires since the Japanese had cut the U.S. off from over 90 percent of its rubber supply which had come from the Dutch East Indies and the U.S. synthetic rubber industry was in its infancy. Average gasoline prices went from an all-time record low of $0.1275 per gallon ($0.1841 with taxes) in 1940 to $0.1448 per gallon ($0.2050 with taxes) in 1945.

Even with the world's largest aviation gasoline production, the U.S. military still found that more was needed. Throughout the duration of the war, aviation gasoline supply was always behind requirements and this impacted training and operations. The reason for this shortage developed before the war even began. The free market did not support the expense of producing 100-octane aviation fuel in large volume, especially during the Great Depression. Iso-octane in the early development stage cost $30 a gallon and even by 1934 it was still $2 a gallon compared to $0.18 for motor gasoline when the Army decided to experiment with 100-octane for its combat aircraft. Though only 3 percent of U.S. combat aircraft in 1935 could take full advantage of the higher octane due to low compression ratios, the Army saw the need for increasing performance warranted the expense and purchased 100,000 gallons. By 1937 the Army established 100-octane as the standard fuel for combat aircraft and by 1939 production was only 20,000 barrels a day. In effect, the U.S. military was the only market for 100-octane aviation gasoline and as war broke out in Europe this created a supply problem that persisted throughout the duration.

With the war in Europe in 1939 a reality, all predictions of 100-octane consumption were outrunning all possible production. Neither the Army nor the Navy could contract more than six months in advance for fuel and they could not supply the funds for plant expansion. Without a long term guaranteed market the petroleum industry would not risk its capital to expand production for a product that only the government would buy. The solution to the expansion of storage, transportation, finances and production was the creation of the Defense Supplies Corporation on 19 September 1940. The Defense Supplies Corporation would buy, transport and store all aviation gasoline for the Army and Navy at cost plus a carrying fee.

When the Allied breakout after D-Day found their armies stretching their supply lines to a dangerous point, the makeshift solution was the Red Ball Express. But even this soon was inadequate. The trucks in the convoys had to drive longer distances as the armies advanced and they were consuming a greater percentage of the same gasoline they were trying to deliver. In 1944, General George Patton's Third Army finally stalled just short of the German border after running out of gasoline. The general was so upset at the arrival of a truckload of rations instead of gasoline he was reported to have shouted: "Hell, they send us food, when they know we can fight without food but not without oil." The solution had to wait for the repairing of the railroad lines and bridges so that the more efficient trains could replace the gasoline consuming truck convoys.

The development of jet engines burning kerosene-based fuels during WW II for aircraft produced a superior performing propulsion system than internal combustion engines could offer and the U.S. military forces gradually replaced their piston combat aircraft with jet powered planes. This development would essentially remove the military need for ever increasing octane fuels and eliminated government support for the refining industry to pursue the research and production of such exotic and expensive fuels. Commercial aviation was slower to adapt to jet propulsion and until 1958 when the Boeing 707 first entered commercial service, piston powered airliners still relied on aviation gasoline. But commercial aviation had greater economic concerns than the maximum performance that the military could afford. As octane numbers increased so did the cost of gasoline but the incremental increase in efficiency becomes less as compression ratio goes up. This reality set a practical limit to how high compression ratios could increase relative to how expensive the gasoline would become. Last produced in 1955, the Pratt & Whitney R-4360 Wasp Major was using 115/145 Aviation gasoline and producing 1 horsepower per cubic inch at 6.7 compression ratio (turbo-supercharging would increase this) and 1 pound of engine weight to produce 1.1 horsepower. This compares to the Wright Brothers engine needing almost 17 pounds of engine weight to produce 1 horsepower.

The US automobile industry after WWII could not take advantage of the high octane fuels then available. Automobile compression ratios increased from an average of 5.3-to-1 in 1931 to just 6.7-to-1 in 1946. The average octane number of regular grade motor gasoline increased from 58 to 70 during the same time. Military aircraft were using expensive turbo-supercharged engines that cost at least 10 times as much per horsepower as automobile engines and had to be overhauled every 700 to 1,000 hours. The automobile market could not support such expensive engines. It would not be until 1957 that the first US automobile manufacturer could mass-produce an engine that would produce one horsepower per cubic inch, the Chevrolet 283 hp/283 cubic inch V-8 engine option in the Corvette. At $485 this was an expensive option that few consumers could afford and would only appeal to the performance oriented consumer market willing to pay for the premium fuel required. This engine had an advertised compression ratio of 10.5-to-1 and the 1958 AMA Specifications stated the octane requirement was 96-100 RON. At (1959 with aluminum intake), it took of engine weight to make .

In the 1950s oil refineries started to focus on high octane fuels, and then detergents were added to gasoline to clean the jets in carburetors. The 1970s witnessed greater attention to the environmental consequences of burning gasoline. These considerations led to the phasing out of TEL and its replacement by other antiknock compounds. Subsequently, low-sulfur gasoline was introduced, in part to preserve the catalysts in modern exhaust systems.

Gasoline is produced in oil refineries. Roughly of gasoline is derived from a barrel of crude oil. Material separated from crude oil via distillation, called virgin or straight-run gasoline, does not meet specifications for modern engines (particularly the octane rating; see below), but can be pooled to the gasoline blend.

The bulk of a typical gasoline consists of a homogeneous mixture of small, relatively lightweight hydrocarbons with between 4 and 12 carbon atoms per molecule (commonly referred to as C4–C12). It is a mixture of paraffins (alkanes), olefins (alkenes) and cycloalkanes (naphthenes). The usage of the terms "paraffin" and "olefin" in place of the standard chemical nomenclature "alkane" and "alkene", respectively, is particular to the oil industry. The actual ratio of molecules in any gasoline depends upon:

The various refinery streams blended to make gasoline have different characteristics. Some important streams include:

The terms above are the jargon used in the oil industry and terminology varies.

Currently, many countries set limits on gasoline aromatics in general, benzene in particular, and olefin (alkene) content. Such regulations have led to an increasing preference for high-octane pure paraffin (alkane) components, such as alkylate, and are forcing refineries to add processing units to reduce benzene content. In the European Union, the benzene limit is set at 1% by volume for all grades of automotive gasoline.

Gasoline can also contain other organic compounds, such as organic ethers (deliberately added), plus small levels of contaminants, in particular organosulfur compounds (which are usually removed at the refinery).

The specific gravity of gasoline is from 0.71 to 0.77, with higher densities having a greater volume of aromatics. Finished marketable gasoline is traded (in Europe) with a standard reference of , and its price is escalated or de-escalated according to its actual density. Because of its low density, gasoline floats on water, and so water cannot generally be used to extinguish a gasoline fire unless applied in a fine mist.

Quality gasoline should be stable for six months if stored properly, but as gasoline is a mixture rather than a single compound, it will break down slowly over time due to the separation of the components. Gasoline stored for a year will most likely be able to be burned in an internal combustion engine without too much trouble but the effects of long-term storage will become more noticeable with each passing month until a time comes when the gasoline should be diluted with ever-increasing amounts of freshly made fuel so that the older gasoline may be used up. If left undiluted, improper operation will occur and this may include engine damage from misfiring or the lack of proper action of the fuel within a fuel injection system and from an onboard computer attempting to compensate (if applicable to the vehicle). Gasoline should ideally be stored in an airtight container (to prevent oxidation or water vapor mixing in with the gas) that can withstand the vapor pressure of the gasoline without venting (to prevent the loss of the more volatile fractions) at a stable cool temperature (to reduce the excess pressure from liquid expansion and to reduce the rate of any decomposition reactions). When gasoline is not stored correctly, gums and solids may result, which can corrode system components and accumulate on wetted surfaces, resulting in a condition called "stale fuel". Gasoline containing ethanol is especially subject to absorbing atmospheric moisture, then forming gums, solids or two phases (a hydrocarbon phase floating on top of a water-alcohol phase).

The presence of these degradation products in the fuel tank or fuel lines plus a carburetor or fuel injection components makes it harder to start the engine or causes reduced engine performance. On resumption of regular engine use, the buildup may or may not be eventually cleaned out by the flow of fresh gasoline. The addition of a fuel stabilizer to gasoline can extend the life of fuel that is not or cannot be stored properly, though removal of all fuel from a fuel system is the only real solution to the problem of long-term storage of an engine or a machine or vehicle. Typical fuel stabilizers are proprietary mixtures containing mineral spirits, isopropyl alcohol, 1,2,4-trimethylbenzene or other additives. Fuel stabilizers are commonly used for small engines, such as lawnmower and tractor engines, especially when their use is sporadic or seasonal (little to no use for one or more seasons of the year). Users have been advised to keep gasoline containers more than half full and properly capped to reduce air exposure, to avoid storage at high temperatures, to run an engine for ten minutes to circulate the stabilizer through all components prior to storage, and to run the engine at intervals to purge stale fuel from the carburetor.

Gasoline stability requirements are set by the standard ASTM D4814. This standard describes the various characteristics and requirements of automotive fuels for use over a wide range of operating conditions in ground vehicles equipped with spark-ignition engines.

A gasoline-fueled internal combustion engine obtains energy from the combustion of gasoline's various hydrocarbons with oxygen from the ambient air, yielding carbon dioxide and water as exhaust. The combustion of octane, a representative species, performs the chemical reaction:

<chem>2 C8H18 + 25 O2 -> 16 CO2 + 18 H2O
</chem>

By weight, gasoline contains about or by volume , quoting the lower heating value. Gasoline blends differ, and therefore actual energy content varies according to the season and producer by up to 1.75% more or less than the average. On average, about 74 L (19.5 US gal; 16.3 imp gal) of gasoline are available from a barrel of crude oil (about 46% by volume), varying with the quality of the crude and the grade of the gasoline. The remainder are products ranging from tar to naphtha.

A high-octane-rated fuel, such as liquefied petroleum gas (LPG), has an overall lower power output at the typical 10:1 compression ratio of an engine design optimized for gasoline fuel. An engine tuned for LPG fuel via higher compression ratios (typically 12:1) improves the power output. This is because higher-octane fuels allow for a higher compression ratio without knocking, resulting in a higher cylinder temperature, which improves efficiency. Also, increased mechanical efficiency is created by a higher compression ratio through the concomitant higher expansion ratio on the power stroke, which is by far the greater effect. The higher expansion ratio extracts more work from the high-pressure gas created by the combustion process. An Atkinson cycle engine uses the timing of the valve events to produce the benefits of a high expansion ratio without the disadvantages, chiefly detonation, of a high compression ratio. A high expansion ratio is also one of the two key reasons for the efficiency of diesel engines, along with the elimination of pumping losses due to throttling of the intake air flow.

The lower energy content of LPG by liquid volume in comparison to gasoline is due mainly to its lower density. This lower density is a property of the lower molecular weight of propane (LPG's chief component) compared to gasoline's blend of various hydrocarbon compounds with heavier molecular weights than propane. Conversely, LPG's energy content by weight is higher than gasoline's due to a higher hydrogen-to-carbon ratio.

Molecular weights of the representative octane combustion are CH 114, O 32, CO 44, HO 18; therefore 1 kg of fuel reacts with 3.51 kg of oxygen to produce 3.09 kg of carbon dioxide and 1.42 kg of water.

Spark-ignition engines are designed to burn gasoline in a controlled process called deflagration. However, the unburned mixture may autoignite by pressure and heat alone, rather than igniting from the spark plug at exactly the right time, causing a rapid pressure rise which can damage the engine. This is often referred to as engine knocking or end-gas knock. Knocking can be reduced by increasing the gasoline's resistance to autoignition, which is expressed by its octane rating.

Octane rating is measured relative to a mixture of 2,2,4-trimethylpentane (an isomer of octane) and n-heptane. There are different conventions for expressing octane ratings, so the same physical fuel may have several different octane ratings based on the measure used. One of the best known is the research octane number (RON).

The octane rating of typical commercially available gasoline varies by country. In Finland, Sweden and Norway, 95 RON is the standard for regular unleaded gasoline and 98 RON is also available as a more expensive option.

In the United Kingdom, ordinary regular unleaded gasoline is sold at 95 RON (commonly available), premium unleaded gasoline is always 97 RON, and super-unleaded is usually 97–98 RON. However, both Shell and BP produce fuel at 102 RON for cars with high-performance engines, and in 2006 the supermarket chain Tesco began to sell super-unleaded gasoline rated at 99 RON.

In the United States, octane ratings in unleaded fuels vary between 85 and 87 AKI (91–92 RON) for regular, 89–90 AKI (94–95 RON) for mid-grade (equivalent to European regular), up to 90–94 AKI (95–99 RON) for premium (European premium).

As South Africa's largest city, Johannesburg, is located on the Highveld at above sea level, the Automobile Association of South Africa recommends 95-octane gasoline at low altitude and 93-octane for use in Johannesburg because "The higher the altitude the lower the air pressure, and the lower the need for a high octane fuel as there is no real performance gain".

Octane rating became important as the military sought higher output for aircraft engines in the late 1930s and the 1940s. A higher octane rating allows a higher compression ratio or supercharger boost, and thus higher temperatures and pressures, which translate to higher power output. Some scientists even predicted that a nation with a good supply of high-octane gasoline would have the advantage in air power. In 1943, the Rolls-Royce Merlin aero engine produced 1,320 horsepower (984 kW) using 100 RON fuel from a modest 27-liter displacement. By the time of Operation Overlord, both the RAF and USAAF were conducting some operations in Europe using 150 RON fuel (100/150 avgas), obtained by adding 2.5% aniline to 100-octane avgas. By this time the Rolls-Royce Merlin 66 was developing 2,000 hp using this fuel.

Almost all countries in the world have phased out automotive leaded fuel. In 2011, six countries were still using leaded gasoline: Afghanistan, Myanmar, North Korea, Algeria, Iraq and Yemen. It was expected that by the end of 2013 those countries, too, would ban leaded gasoline, but this target was not met. Algeria replaced leaded with unleaded automotive fuel only in 2015. Different additives have replaced the lead compounds. The most popular additives include aromatic hydrocarbons, ethers and alcohol (usually ethanol or methanol). For technical reasons, the use of leaded additives is still permitted worldwide for the formulation of some grades of aviation gasoline such as 100LL, because the required octane rating would be technically infeasible to reach without the use of leaded additives.

Gasoline, when used in high-compression internal combustion engines, tends to autoignite or "detonate" causing damaging engine knocking (also called "pinging" or "pinking"). To address this problem, tetraethyllead (TEL) was widely adopted as an additive for gasoline in the 1920s. With the discovery of the seriousness of the extent of environmental and health damage caused by lead compounds, however, and the incompatibility of lead with catalytic converters, governments began to mandate reductions in gasoline lead.

In the United States, the Environmental Protection Agency issued regulations to reduce the lead content of leaded gasoline over a series of annual phases, scheduled to begin in 1973 but delayed by court appeals until 1976. By 1995, leaded fuel accounted for only 0.6 percent of total gasoline sales and under 2000 short tons (1814 t) of lead per year. From 1 January 1996, the U.S. Clean Air Act banned the sale of leaded fuel for use in on-road vehicles in the U.S. The use of TEL also necessitated other additives, such as dibromoethane.

European countries began replacing lead-containing additives by the end of the 1980s, and by the end of the 1990s, leaded gasoline was banned within the entire European Union.

Reduction in the average lead content of human blood is believed to be a major cause for falling violent crime rates around the world, including in the United States and South Africa. A statistically significant correlation has been found between the usage rate of leaded gasoline and violent crime: taking into account a 22-year time lag, the violent crime curve virtually tracks the lead exposure curve.

Lead replacement petrol (LRP) was developed for vehicles designed to run on leaded fuels and incompatible with unleaded fuels. Rather than tetraethyllead it contains other metals such as potassium compounds or methylcyclopentadienyl manganese tricarbonyl (MMT); these are purported to buffer soft exhaust valves and seats so that they do not suffer recession due to the use of unleaded fuel.

LRP was marketed during and after the phaseout of leaded motor fuels in the United Kingdom, Australia, South Africa and some other countries. Consumer confusion led to a widespread mistaken preference for LRP rather than unleaded, and LRP was phased out 8 to 10 years after the introduction of unleaded.

Leaded gasoline was withdrawn from sale in Britain after 31 December 1999, seven years after EEC regulations signaled the end of production for cars using leaded gasoline in member states. At this stage, a large percentage of cars from the 1980s and early 1990s which ran on leaded gasoline were still in use, along with cars which could run on unleaded fuel. However, the declining number of such cars on British roads saw many gasoline stations withdrawing LRP from sale by 2003.

Methylcyclopentadienyl manganese tricarbonyl (MMT) is used in Canada and the US to boost octane rating. It also helps old cars designed for leaded fuel run on unleaded fuel without the need for additives to prevent valve problems. Its use in the United States has been restricted by regulations. Its use in the European Union is restricted by Article 8a of the Fuel Quality Directive following its testing under the Protocol for the evaluation of effects of metallic fuel-additives on the emissions performance of vehicles.

Gummy, sticky resin deposits result from oxidative degradation of gasoline during long-term storage. These harmful deposits arise from the oxidation of alkenes and other minor components in gasoline (see drying oils). Improvements in refinery techniques have generally reduced the susceptibility of gasolines to these problems. Previously, catalytically or thermally cracked gasolines were most susceptible to oxidation. The formation of gums is accelerated by copper salts, which can be neutralized by additives called metal deactivators.

This degradation can be prevented through the addition of 5–100 ppm of antioxidants, such as phenylenediamines and other amines. Hydrocarbons with a bromine number of 10 or above can be protected with the combination of unhindered or partially hindered phenols and oil-soluble strong amine bases, such as hindered phenols. "Stale" gasoline can be detected by a colorimetric enzymatic test for organic peroxides produced by oxidation of the gasoline.

Gasolines are also treated with metal deactivators, which are compounds that sequester (deactivate) metal salts that otherwise accelerate the formation of gummy residues. The metal impurities might arise from the engine itself or as contaminants in the fuel.

Gasoline, as delivered at the pump, also contains additives to reduce internal engine carbon buildups, improve combustion and allow easier starting in cold climates. High levels of detergent can be found in Top Tier Detergent Gasolines. The specification for Top Tier Detergent Gasolines was developed by four automakers: GM, Honda, Toyota, and BMW. According to the bulletin, the minimal U.S. EPA requirement is not sufficient to keep engines clean. Typical detergents include alkylamines and alkyl phosphates at the level of 50–100 ppm.

In the EU, 5% ethanol can be added within the common gasoline spec (EN 228). Discussions are ongoing to allow 10% blending of ethanol (available in Finnish, French and German gas stations). In Finland, most gasoline stations sell 95E10, which is 10% ethanol, and 98E5, which is 5% ethanol. Most gasoline sold in Sweden has 5–15% ethanol added. Three different ethanol blends are sold in the Netherlands—E5, E10 and hE15. The last of these differs from standard ethanol–gasoline blends in that it consists of 15% hydrous ethanol (i.e., the ethanol–water azeotrope) instead of the anhydrous ethanol traditionally used for blending with gasoline.

The Brazilian National Agency of Petroleum, Natural Gas and Biofuels (ANP) requires gasoline for automobile use to have 27.5% of ethanol added to its composition. Pure hydrated ethanol is also available as a fuel.

Legislation requires retailers to label fuels containing ethanol on the dispenser, and limits ethanol use to 10% of gasoline in Australia. Such gasoline is commonly called E10 by major brands, and it is cheaper than regular unleaded gasoline.

The federal Renewable Fuel Standard (RFS) effectively requires refiners and blenders to blend renewable biofuels (mostly ethanol) with gasoline, sufficient to meet a growing annual target of total gallons blended. Although the mandate does not require a specific percentage of ethanol, annual increases in the target combined with declining gasoline consumption has caused the typical ethanol content in gasoline to approach 10%. Most fuel pumps display a sticker that states that the fuel may contain up to 10% ethanol, an intentional disparity that reflects the varying actual percentage. Until late 2010, fuel retailers were only authorized to sell fuel containing up to 10 percent ethanol (E10), and most vehicle warranties (except for flexible fuel vehicles) authorize fuels that contain no more than 10 percent ethanol. In parts of the United States, ethanol is sometimes added to gasoline without an indication that it is a component.

In October 2007, the Government of India decided to make 5% ethanol blending (with gasoline) mandatory. Currently, 10% ethanol blended product (E10) is being sold in various parts of the country. Ethanol has been found in at least one study to damage catalytic converters.

Though gasoline is a naturally colorless liquid, many gasolines are dyed in various colors to indicate their composition and acceptable uses. In Australia, the lowest grade of gasoline (RON 91) was dyed a light shade of red/orange and is now the same colour as the medium grade (RON 95) and high octane (RON 98) which are dyed yellow. In the United States, aviation gasoline (avgas) is dyed to identify its octane rating and to distinguish it from kerosene-based jet fuel, which is clear. In Canada, the gasoline for marine and farm use is dyed red and is not subject to sales tax.

Oxygenate blending adds oxygen-bearing compounds such as MTBE, ETBE, TAME, TAEE, ethanol and biobutanol. The presence of these oxygenates reduces the amount of carbon monoxide and unburned fuel in the exhaust. In many areas throughout the U.S., oxygenate blending is mandated by EPA regulations to reduce smog and other airborne pollutants. For example, in Southern California, fuel must contain 2% oxygen by weight, resulting in a mixture of 5.6% ethanol in gasoline. The resulting fuel is often known as reformulated gasoline (RFG) or oxygenated gasoline, or in the case of California, California reformulated gasoline. The federal requirement that RFG contain oxygen was dropped on 6 May 2006 because the industry had developed VOC-controlled RFG that did not need additional oxygen.

MTBE was phased out in the U.S. due to groundwater contamination and the resulting regulations and lawsuits. Ethanol and, to a lesser extent, the ethanol-derived ETBE are common substitutes. A common ethanol-gasoline mix of 10% ethanol mixed with gasoline is called gasohol or E10, and an ethanol-gasoline mix of 85% ethanol mixed with gasoline is called E85. The most extensive use of ethanol takes place in Brazil, where the ethanol is derived from sugarcane. In 2004, over 3.4 billion US gallons (2.8 billion imp gal; 13 million m³) of ethanol was produced in the United States for fuel use, mostly from corn, and E85 is slowly becoming available in much of the United States, though many of the relatively few stations vending E85 are not open to the general public.

The use of bioethanol and bio-methanol, either directly or indirectly by conversion of ethanol to bio-ETBE, or methanol to bio-MTBE is encouraged by the European Union Directive on the Promotion of the use of biofuels and other renewable fuels for transport. Since producing bioethanol from fermented sugars and starches involves distillation, though, ordinary people in much of Europe cannot legally ferment and distill their own bioethanol at present (unlike in the U.S., where getting a BATF distillation permit has been easy since the 1973 oil crisis).

Combustion of of gasoline produces of carbon dioxide (2.3 kg/l), a greenhouse gas.

The main concern with gasoline on the environment, aside from the complications of its extraction and refining, is the effect on the climate through the production of carbon dioxide. Unburnt gasoline and evaporation from the tank, when in the atmosphere, reacts in sunlight to produce photochemical smog. Vapor pressure initially rises with some addition of ethanol to gasoline, but the increase is greatest at 10% by volume. At higher concentrations of ethanol above 10%, the vapor pressure of the blend starts to decrease. At a 10% ethanol by volume, the rise in vapor pressure may potentially increase the problem of photochemical smog. This rise in vapor pressure could be mitigated by increasing or decreasing the percentage of ethanol in the gasoline mixture.

The chief risks of such leaks come not from vehicles, but from gasoline delivery truck accidents and leaks from storage tanks. Because of this risk, most (underground) storage tanks now have extensive measures in place to detect and prevent any such leaks, such as monitoring systems (Veeder-Root, Franklin Fueling).

Production of gasoline consumes 0.63 gallons of water per mile driven.

The safety data sheet for a 2003 Texan unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts, including benzene (up to 5% by volume), toluene (up to 35% by volume), naphthalene (up to 1% by volume), trimethylbenzene (up to 7% by volume), methyl "tert"-butyl ether (MTBE) (up to 18% by volume, in some states) and about ten others. Hydrocarbons in gasoline generally exhibit low acute toxicities, with LD50 of 700–2700 mg/kg for simple aromatic compounds. Benzene and many antiknocking additives are carcinogenic.

People can be exposed to gasoline in the workplace by swallowing it, breathing in vapors, skin contact, and eye contact. Gasoline is toxic. The National Institute for Occupational Safety and Health (NIOSH) has also designated gasoline as a carcinogen. Physical contact, ingestion or inhalation can cause health problems. Since ingesting large amounts of gasoline can cause permanent damage to major organs, a call to a local poison control center or emergency room visit is indicated.

Contrary to common misconception, swallowing gasoline does not generally require special emergency treatment, and inducing vomiting does not help, and can make it worse. According to poison specialist Brad Dahl, "even two mouthfuls wouldn't be that dangerous as long as it goes down to your stomach and stays there or keeps going." The US CDC's Agency for Toxic Substances and Disease Registry says not to induce vomiting, lavage, or administer activated charcoal.

Inhaled (huffed) gasoline vapor is a common intoxicant. Users concentrate and inhale gasoline vapour in a manner not intended by the manufacturer to produce euphoria and intoxication. Gasoline inhalation has become epidemic in some poorer communities and indigenous groups in Australia, Canada, New Zealand, and some Pacific Islands. The practice is thought to cause severe organ damage, including mental retardation.

In Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on a number of occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada.
Australia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by United States servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.

In Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. "Boss", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up. In 2005, the Government of Australia and BP Australia began the usage of Opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.

Like other hydrocarbons, gasoline burns in a limited range of its vapor phase and, coupled with its volatility, this makes leaks highly dangerous when sources of ignition are present. Gasoline has a lower explosive limit of 1.4% by volume and an upper explosive limit of 7.6%. If the concentration is below 1.4%, the air-gasoline mixture is too lean and does not ignite. If the concentration is above 7.6%, the mixture is too rich and also does not ignite. However, gasoline vapor rapidly mixes and spreads with air, making unconstrained gasoline quickly flammable.

The United States accounts for about 44% of the world's gasoline consumption. In 2003, the United States consumed , which equates to of gasoline each day. The United States used about of gasoline in 2006, of which 5.6% was mid-grade and 9.5% was premium grade.

Countries in Europe impose substantially higher taxes on fuels such as gasoline when compared to the United States. The price of gasoline in Europe is typically higher than that in the U.S. due to this difference.

From 1998 to 2004, the price of gasoline fluctuated between US$1 and US$2 per U.S. gallon. After 2004, the price increased until the average gas price reached a high of $4.11 per U.S. gallon in mid-2008, but receded to approximately $2.60 per U.S. gallon by September 2009. More recently, the U.S. experienced an upswing in gasoline prices through 2011, and by 1 March 2012, the national average was $3.74 per gallon.

In the United States, most consumer goods bear pre-tax prices, but gasoline prices are posted with taxes included. Taxes are added by federal, state, and local governments. As of 2009, the federal tax is 18.4¢ per gallon for gasoline and 24.4¢ per gallon for diesel (excluding red diesel). Among individual states, the highest gasoline tax rates, including the federal taxes as of October 2018, are found in Pennsylvania (77.1¢/gal), California (73.93¢/gal), and Washington (67.8¢/gal).

About 9 percent of all gasoline sold in the U.S. in May 2009 was premium grade, according to the Energy Information Administration. "Consumer Reports" magazine says, "If [your owner’s manual] says to use regular fuel, do so—there's no advantage to a higher grade." The "Associated Press" said premium gas—which has a higher octane rating and costs more per gallon than regular unleaded—should be used only if the manufacturer says it is "required". Cars with turbocharged engines and high compression ratios often specify premium gas because higher octane fuels reduce the incidence of "knock", or fuel pre-detonation. The price of gas varies considerably between the summer and winter months.

About of carbon dioxide (CO) are produced from burning of gasoline that does not contain ethanol (2.36 kg/L). About of CO are produced from burning one US gallon of diesel fuel (2.69 kg/l).

The U.S. EIA estimates that U.S. motor gasoline and diesel (distillate) fuel consumption for transportation in 2015 resulted in the emission of about 1,105 million metric tons of CO and 440 million metric tons of CO, respectively, for a total of 1,545 million metric tons of CO. This total was equivalent to 83% of total U.S. transportation-sector CO emissions and equivalent to 29% of total U.S. energy-related CO emissions in 2015.

Most of the retail gasoline now sold in the United States contains about 10% fuel ethanol (or E10) by volume. Burning a gallon of E10 produces about of CO that is emitted from the fossil fuel content. If the CO emissions from ethanol combustion are considered, then about of CO are produced when a gallon of E10 is combusted. About of CO are produced when a gallon of pure ethanol is combusted.

Below is a table of the volumetric and mass energy density of various transportation fuels as compared with gasoline. In the rows with gross and net, they are from the Oak Ridge National Laboratory's Transportation Energy Data Book.



</doc>
<doc id="23640" url="https://en.wikipedia.org/wiki?curid=23640" title="Pentose">
Pentose

A pentose is a monosaccharide with five carbon atoms. Pentoses are organized into two groups: Aldopentoses have an aldehyde functional group at position 1. Ketopentoses have a ketone functional group at position 2 or 3. In the cell, pentoses have a higher metabolic stability than hexoses.

The aldopentoses have three chiral centers; therefore, eight (2) different stereoisomers are possible.
Ribose is a constituent of RNA, and the related molecule, deoxyribose, is a constituent of DNA. Phosphorylated pentoses are important products of the pentose phosphate pathway, most importantly ribose 5-phosphate (R5P), which is used in the synthesis of nucleotides and nucleic acids, and erythrose 4-phosphate (E4P), which is used in the synthesis of aromatic amino acids. 

The 2-ketopentoses have two chiral centers; therefore, four (2) different stereoisomers are possible. The 3-ketopentoses are rare.

The one deoxypentose has two steroisomers, for two total steroisomers.
The aldehyde and ketone functional groups in these carbohydrates react with neighbouring hydroxyl functional groups to form intramolecular hemiacetals and hemiketals, respectively. The resulting ring structure is related to furan, and is termed a furanose. The ring spontaneously opens and closes, allowing rotation to occur about the bond between the carbonyl group and the neighbouring carbon atom — yielding two distinct configurations (α and β). This process is termed mutarotation.

A polymer composed of pentose sugars is called a pentosan.

The most important tests for pentoses rely on converting the pentose to furfural, which then reacts with a chromophore. In Tollens’ test for pentoses (not to be confused with Tollens' silver-mirror test for reducing sugars) the furfural ring reacts with phloroglucinol to produce a colored compound; in the aniline acetate test with aniline acetate; and in Bial's test, with orcinol. In each of these tests, pentoses react much more strongly and quickly than hexoses.


</doc>
<doc id="23643" url="https://en.wikipedia.org/wiki?curid=23643" title="Propane">
Propane

Propane () is a three-carbon alkane with the molecular formula . It is a gas at standard temperature and pressure, but compressible to a transportable liquid. A by-product of natural gas processing and petroleum refining, it is commonly used as a fuel. Discovered in 1857 by the French chemist Marcellin Berthelot, it became commercially available in the US by 1911. Propane is one of a group of liquefied petroleum gases (LP gases). The others include butane, propylene, butadiene, butylene, isobutylene, and mixtures thereof.

Propane gas has become a popular choice for barbecues and portable stoves because its low boiling point makes it vaporize as soon as it is released from its pressurized container. Propane powers buses, forklifts, taxis, outboard boat motors, and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers. Propane is also used in some locomotive diesel engines to improve combustion. Propane cylinders have also been used as improvised explosive devices in attacks against schools such as the Columbine High School massacre, as well as the 2012 Brindisi school bombing, the Discovery Communications headquarters hostage crisis and in car bombs.

Propane is a simple asphyxiant. Unlike natural gas, propane is denser than air. It may accumulate in low spaces and near the floor. When abused as an inhalant, it may cause hypoxia (lack of oxygen), pneumonia, cardiac failure or cardiac arrest. Propane has low toxicity since it is not readily absorbed and is not biologically active.

Propane was discovered by the French chemist Marcellin Berthelot in 1857. It was found dissolved in Pennsylvanian light crude oil by Edmund Ronalds in 1864. Walter O. Snelling of the U.S. Bureau of Mines highlighted it as a volatile component in gasoline in 1910, which was the beginning of the propane industry in the United States. The volatility of these lighter hydrocarbons caused them to be known as "wild" because of the high vapor pressures of unrefined gasoline. On March 31, 1912, "The New York Times" reported on Snelling's work with liquefied gas, saying "a steel bottle will carry enough gas to light an ordinary home for three weeks".

It was during this time that Snelling, in cooperation with Frank P. Peterson, Chester Kerr, and Arthur Kerr, created ways to liquefy the LP gases during the refining of gasoline. Together, they established American Gasol Co., the first commercial marketer of propane. Snelling had produced relatively pure propane by 1911, and on March 25, 1913, his method of processing and producing LP gases was issued patent #1,056,845. A separate method of producing LP gas through compression was created by Frank Peterson and its patent granted on July 2, 1912.

The 1920s saw increased production of LP gas, with the first year of recorded production totaling in 1922. In 1927, annual marketed LP gas production reached , and by 1935, the annual sales of LP gas had reached . Major industry developments in the 1930s included the introduction of railroad tank car transport, gas odorization, and the construction of local bottle-filling plants. The year 1945 marked the first year that annual LP gas sales reached a billion gallons. By 1947, 62% of all U.S. homes had been equipped with either natural gas or propane for cooking.

In 1950, 1,000 propane-fueled buses were ordered by the Chicago Transit Authority, and by 1958, sales in the U.S. had reached annually. In 2004, it was reported to be a growing $8-billion to $10-billion industry with over of propane being used annually in the U.S.

The "prop-" root found in "propane" and names of other compounds with three-carbon chains was derived from "propionic acid", which in turn was named after the Greek words protos (meaning first) and pion (fat).

Propane is produced as a by-product of two other processes, natural gas processing and petroleum refining. The processing of natural gas involves removal of butane, propane, and large amounts of ethane from the raw gas, in order to prevent condensation of these volatiles in natural gas pipelines. Additionally, oil refineries produce some propane as a by-product of cracking petroleum into gasoline or heating oil.

The supply of propane cannot easily be adjusted to meet increased demand, because of the by-product nature of propane production. About 90% of U.S. propane is domestically produced. The United States imports about 10% of the propane consumed each year, with about 70% of that coming from Canada via pipeline and rail. The remaining 30% of imported propane comes to the United States from other sources via ocean transport.

After it is separated from the crude oil, North American propane is stored in huge salt caverns. Examples of these are Fort Saskatchewan, Alberta; Mont Belvieu, Texas; and Conway, Kansas. These salt caverns were hollowed out in the 1940s, and they can store or more of propane. When the propane is needed, much of it is shipped by pipelines to other areas of the United States. The North American standard grade of automotive use propane is rated HD 5. HD 5 grade has a maximum of 5 percent butane, but propane sold in Europe, has a max allowable amount of butane of 30 percent, meaning it's not the same fuel as HD 5. The LPG used as auto fuel and cooking gas in Asia and Australia, also has a very high content of butane. Propane is also shipped by truck, ship, barge, and railway to many U.S. areas.

Propane can also be produced as a biofuel. Biopropane is commercially sold in Europe.

Propane is a color- and odorless gas. At normal pressure it liquifies below its boiling point at −42 °C and solidifies below its melting point at −187.7 °C. Propane crystallizes in the space group P2/n . The low spacefilling of 58.5 % (at 90 K), due to the bad stacking properties of the molecule, is the reason for the particularly low melting point.

Propane undergoes combustion reactions in a similar fashion to other alkanes. In the presence of excess oxygen, propane burns to form water and carbon dioxide.<chem display="block">C3H8 + 5O2 -> 3CO2 + 4H2O + heat </chem> When not enough oxygen or too much oxygen is present for complete combustion, incomplete combustion occurs, allowing carbon monoxide and/or soot (carbon) to be formed as well:
<chem display="block">2C3H8 + 9O2 -> 4CO2 + 2CO + 8H2O + heat </chem><chem display="block">C3H8 + 2O2 -> 3C + 4H2O + heat </chem> Complete combustion of propane produces about 50 MJ/kg of heat.

Propane combustion is much cleaner than that of coal or unleaded gasoline. Propane per BTU production of CO2 is almost as low as that of natural gas. Propane burns hotter than home heating oil or diesel fuel because of the very high hydrogen content. The presence of C–C bonds, plus the multiple bonds of propylene and butylene, create organic exhausts besides carbon dioxide and water vapor during typical combustion. These bonds also cause propane to burn with a visible flame.

The enthalpy of combustion of propane gas where all products return to standard state, for example where water returns to its liquid state at standard temperature (known as higher heating value), is (2219.2 ± 0.5) kJ/mol, or (50.33 ± 0.01) MJ/kg.
The enthalpy of combustion of propane gas where products do not return to standard state, for example where the hot gases including water vapor exit a chimney, (known as lower heating value) is −2043.455 kJ/mol. The lower heat value is the amount of heat available from burning the substance where the combustion products are vented to the atmosphere; for example, the heat from a fireplace when the flue is open.

The density of propane gas at 25 °C (77 °F) is 1.808 kg/m. The density of liquid propane at 25 °C (77 °F) is 0.493 g/cm, which is equivalent to 4.11 pounds per U.S. liquid gallon or 493 g/L. Propane expands at 1.5% per 10 °F. Thus, liquid propane has a density of approximately 4.2 pounds per gallon (504 g/L) at 60 °F (15.6 °C).

Propane is a popular choice for barbecues and portable stoves because the low boiling point of makes it vaporize as soon as it is released from its pressurized container. Therefore, no carburetor or other vaporizing device is required; a simple metering nozzle suffices. Propane powers buses, forklifts, taxis, outboard boat motors, and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers. Propane is also used in some locomotive diesel engines as a fuel added into the turbocharger yielding much better combustion.

Since it can be transported easily, it is a popular fuel for home heat and backup electrical generation in sparsely populated areas that do not have natural gas pipelines. Many heavy-duty highway trucks use propane as a boost, where it is added through the turbocharger, to mix with diesel fuel droplets. Propane droplets' very high hydrogen content helps the diesel fuel to burn hotter and therefore more completely. This provides more torque, more horsepower, and a cleaner exhaust for the trucks. It is normal for a 7-liter medium-duty diesel truck engine to increase fuel economy by 20 to 33 percent when a propane boost system is used. It is cheaper because propane is much cheaper than diesel fuel. The longer distance a cross country trucker can travel on a full load of combined diesel and propane fuel means they can maintain federal hours of work rules with two fewer fuel stops in a cross country trip. Truckers, tractor pulling competitions, and farmers have been using a propane boost system for over forty years in North America. 

International ships can reuse propane from ocean-going ships that transport LPG because as the sun evaporates the propane during the voyage, the international ship catches the evaporating propane gas and feeds it into the air intake system of the ship's diesel engines. This reduces bunker fuel consumption and the pollution created by the ships. There is an international agreement to use either propane or CNG as a mandatory additive to the bunker fuel for all ocean traveling ships beginning in 2020.
Propane is generally stored and transported in steel cylinders as a liquid with a vapor space above the liquid. The vapor pressure in the cylinder is a function of temperature. When gaseous propane is drawn at a high rate, the latent heat of vaporization required to create the gas will cause the bottle to cool. (This is why water often condenses on the sides of the bottle and then freezes). Since lightweight, high-octane propane vaporize before the heavier, low-octane propane, the ignition properties change as the cylinder empties. For these reasons, the liquid is often withdrawn using a dip tube.

Commercially available "propane" fuel, or LPG, is not pure. Typically in the United States and Canada, LPG is primarily propane (at least 90%), while the rest is mostly ethane, propylene, butane, and odorants including ethyl mercaptan. This is the HD-5 standard, (Heavy Duty-5% maximum allowable propylene content, and no more than 5% butanes and ethane) defined by the American Society for Testing and Materials by its Standard 1835 for internal combustion engines. Not all products labeled "LPG" conform to this standard, however. In Mexico, for example, gas labeled "LPG" may consist of 60% propane and 40% butane. "The exact proportion of this combination varies by country, depending on international prices, on the availability of components and, especially, on the climatic conditions that favor LPG with higher butane content in warmer regions and propane in cold areas".

Propane use is growing rapidly in non-industrialized areas of the world. Propane has replaced many older traditional fuel sources.

North American industries using propane include glass makers, brick kilns, poultry farms and other industries that need portable heat.

In rural areas of North America, as well as northern Australia, propane is used to heat livestock facilities, in grain dryers, and other heat-producing appliances. When used for heating or grain drying it is usually stored in a large, permanently-placed cylinder which is recharged by a propane-delivery truck. , 6.2 million American households use propane as their primary heating fuel.

In North America, local delivery trucks with an average cylinder size of , fill up large cylinders that are permanently installed on the property, or other service trucks exchange empty cylinders of propane with filled cylinders. Large tractor-trailer trucks, with an average cylinder size of , transport propane from the pipeline or refinery to the local bulk plant. The bobtail and transport are not unique to the North American market, though the practice is not as common elsewhere, and the vehicles are generally called "tankers". In many countries, propane is delivered to consumers via small or medium-sized individual cylinders, while empty cylinders are removed for refilling at a central location.

Propene (also called propylene) can be a contaminant of commercial propane. Propane containing too much propene is not suited for most vehicle fuels. HD-5 is a specification that establishes a maximum concentration of 5% propene in propane. Propane and other LP gas specifications are established in ASTM D-1835. All propane fuels include an odorant, almost always ethanethiol, so that people can easily smell the gas in case of a leak. Propane as HD-5 was originally intended for use as vehicle fuel. HD-5 is currently being used in all propane applications.

Propane is also instrumental in providing off-the-grid refrigeration, as the energy source for a gas absorption refrigerator and is commonly used for camping and recreational vehicles.
In addition, blends of pure, dry "isopropane" (R-290a) (isobutane/propane mixtures) and isobutane (R-600a) can be used as the circulating refrigerant in suitably constructed compressor-based refrigeration. Compared to fluorocarbons, propane has a negligible ozone depletion potential and very low global warming potential (having a value of only 3.3 times the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems. Because its global warming effect is far less than current refrigerants, propane was chosen as one of five replacement refrigerants approved by the EPA in 2015, for use in systems specially designed to handle its flammability.

Such substitution is widely prohibited or discouraged in motor vehicle air conditioning systems, on the grounds that using flammable hydrocarbons in systems originally designed to carry non-flammable refrigerant presents a significant risk of fire or explosion.

Vendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.

Propane is also being used increasingly for vehicle fuels. In the U.S., over 190,000 on-road vehicles use propane, and over 450,000 forklifts use it for power. It is the third most popular vehicle fuel in the world, behind gasoline and Diesel fuel. In other parts of the world, propane used in vehicles is known as autogas. In 2007, approximately 13 million vehicles worldwide use autogas.

The advantage of propane in cars is its liquid state at a moderate pressure. This allows fast refill times, affordable fuel cylinder construction, and price ranges typically just over half that of gasoline. Meanwhile, it is noticeably cleaner (both in handling, and in combustion), results in less engine wear (due to carbon deposits) without diluting engine oil (often extending oil-change intervals), and until recently was a relative bargain in North America. The octane rating of propane is relatively high at 110. In the United States the propane fueling infrastructure is the most developed of all alternative vehicle fuels. Many converted vehicles have provisions for topping off from "barbecue bottles". Purpose-built vehicles are often in commercially owned fleets, and have private fueling facilities. A further saving for propane fuel vehicle operators, especially in fleets, is that pilferage is much more difficult than with gasoline or diesel fuels.

Propane is also used as fuel for small engines, especially those used indoors or in areas with insufficient fresh air and ventilation to carry away the more toxic exhaust of an engine running on gasoline or Diesel fuel. More recently, there have been lawn care products like string trimmers, lawn mowers and leaf blowers intended for outdoor use, but fueled by propane in order to reduce air pollution.

Propane and propane cylinders have been used as improvised explosive devices in attacks and attempted attacks against schools and terrorist targets such as the Columbine High School massacre, 2012 Brindisi school bombing, the Discovery Communications headquarters hostage crisis and in car bombs.


Propane is a simple asphyxiant. Unlike natural gas, propane is denser than air. It may accumulate in low spaces and near the floor. When abused as an inhalant, it may cause hypoxia (lack of oxygen), pneumonia, cardiac failure or cardiac arrest. Propane has low toxicity since it is not readily absorbed and is not biologically active. Commonly stored under pressure at room temperature, propane and its mixtures will flash evaporate at atmospheric pressure and cool well below the freezing point of water. The cold gas, which appears white due to moisture condensing from the air, may cause frostbite.

Propane is denser than air. If a leak in a propane fuel system occurs, the gas will have a tendency to sink into any enclosed area and thus poses a risk of explosion and fire. The typical scenario is a leaking cylinder stored in a basement; the propane leak drifts across the floor to the pilot light on the furnace or water heater, and results in an explosion or fire. This property makes propane generally unsuitable as a fuel for boats.

One hazard associated with propane storage and transport is known as a BLEVE or boiling liquid expanding vapor explosion. The Kingman Explosion involved a railroad tank car in Kingman, Arizona in 1973 during a propane transfer. The fire and subsequent explosions resulted in twelve fatalities and numerous injuries.

Propane is bought and stored in a liquid form (LPG), and thus fuel energy can be stored in a relatively small space. Compressed natural gas (CNG), largely methane, is another gas used as fuel, but it cannot be liquefied by compression at normal temperatures, as these are well above its critical temperature. As a gas, very high pressure is required to store useful quantities. This poses the hazard that, in an accident, just as with any compressed gas cylinder (such as a CO cylinder used for a soda concession) a CNG cylinder may burst with great force, or leak rapidly enough to become a self-propelled missile. Therefore, CNG is much less efficient to store, due to the large cylinder volume required. An alternative means of storing natural gas is as a cryogenic liquid in an insulated container as liquefied natural gas (LNG). This form of storage is at low pressure and is around 3.5 times as efficient as storing it as CNG. Unlike propane, if a spill occurs, CNG will evaporate and dissipate harmlessly because it is lighter than air. Propane is much more commonly used to fuel vehicles than is natural gas, because the equipment required costs less. Propane requires just of pressure to keep it liquid at .
, the retail cost of propane was approximately $2.37 per gallon, or roughly $25.95 per 1 million BTUs. This means that filling a 500-gallon propane tank, which is what households that use propane as their main source of energy usually require, costs $948 (80% of 500 gallons or 400 gallons), a 7.5% increase on the 2012–2013 winter season average US price. However, propane costs per gallon change significantly from one state to another: the Energy Information Administration (EIA) quotes a $2.995 per gallon average on the East Coast for October 2013, while the figure for the Midwest was $1.860 for the same period.




</doc>
<doc id="23645" url="https://en.wikipedia.org/wiki?curid=23645" title="Precambrian">
Precambrian

The Precambrian (or Pre-Cambrian, sometimes abbreviated pЄ, or Cryptozoic) is the earliest part of Earth's history, set before the current Phanerozoic Eon. The Precambrian is so named because it preceded the Cambrian, the first period of the Phanerozoic eon, which is named after Cambria, the Latinised name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of the Earth's geologic time.

The Precambrian (colored green in the timeline figure) is an informal unit of geologic time, subdivided into three eons (Hadean, Archean, Proterozoic) of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about million years ago (Ma), when hard-shelled creatures first appeared in abundance.

Relatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered from the 1960s onwards. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and fossils from the Precambrian (e.g. stromatolites) are of limited biostratigraphic use. This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.

It is thought that the Earth coalesced from material in orbit around the Sun at roughly 4,543 Ma, and may have been struck by a very large (Mars-sized) planetesimal shortly after it formed, splitting off material that formed the Moon (see Giant impact hypothesis). A stable crust was apparently in place by 4,433 Ma, since zircon crystals from Western Australia have been dated at 4,404 ± 8 Ma.

The term "Precambrian" is recognized by the International Commission on Stratigraphy as the only "supereon" in geologic time; it is so-called because it includes the Hadean (~4.6–4 billion), Archean (4–2.5 billion), and Proterozoic (2.5 billion—541 million) eons. (There is only one other eon: the Phanerozoic, 541 million-present.) "Precambrian" is still used by geologists and paleontologists for general discussions not requiring the more specific eon names. , the United States Geological Survey considers the term informal, lacking a stratigraphic rank.

A specific date for the origin of life has not been determined. Carbon found in 3.8 billion-year-old rocks (Archean eon) from islands off western Greenland may be of organic origin. Well-preserved microscopic fossils of bacteria older than 3.46 billion years have been found in Western Australia. Probable fossils 100 million years older have been found in the same area. However, there is evidence that life could have evolved over 4.280 billion years ago. There is a fairly solid record of bacterial life throughout the remainder (Proterozoic eon) of the Precambrian.

Excluding a few contested reports of much older forms from North America and India, the first complex multicellular life forms seem to have appeared at roughly 1500 Ma, in the Mesoproterozoic era of the Proterozoic eon. Fossil evidence from the later Ediacaran period of such complex life comes from the Lantian formation, at least 580 million years ago. A very diverse collection of soft-bodied forms is found in a variety of locations worldwide and date to between 635 and 542 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span, marking the beginning of the Phanerozoic eon. By the middle of the following Cambrian period, a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The increase in diversity of lifeforms during the early Cambrian is called the Cambrian explosion of life.

While land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.

Tracks from an animal with leg like appendages have been found in what was mud 551 million years ago.

Evidence of the details of plate motions and other tectonic activity in the Precambrian has been poorly preserved. It is generally believed that small proto-continents existed prior to 4280 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1130 Ma. The supercontinent, known as Rodinia, broke up around 750 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2400–2100 Ma. One of the best studied is the Sturtian-Varangian glaciation, around 850–635 Ma, which may have brought glacial conditions all the way to the equator, resulting in a "Snowball Earth".

The atmosphere of the early Earth is not well understood. Most geologists believe it was composed primarily of nitrogen, carbon dioxide, and other relatively inert gases, and was lacking in free oxygen. There is, however, evidence that an oxygen-rich atmosphere existed since the early Archean.

At present, it is still believed that molecular oxygen was not a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from a chemically inert to an oxidizing atmosphere caused an ecological crisis, sometimes called the oxygen catastrophe. At first, oxygen would have quickly combined with other elements in Earth's crust, primarily iron, removing it from the atmosphere. After the supply of oxidizable surfaces ran out, oxygen would have begun to accumulate in the atmosphere, and the modern high-oxygen atmosphere would have developed. Evidence for this lies in older rocks that contain massive banded iron formations that were laid down as iron oxides.

A terminology has evolved covering the early years of the Earth's existence, as radiometric dating has allowed real dates to be assigned to specific formations and features. The Precambrian is divided into three eons: the Hadean (– Ma), Archean (- Ma) and Proterozoic (- Ma). See Timetable of the Precambrian.
It has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five "natural" eons, characterized as follows:

The movement of Earth's plates has caused the formation and break-up of continents over time, including occasional formation of a supercontinent containing most or all of the landmass. The earliest known supercontinent was Vaalbara. It formed from proto-continents and was a supercontinent 3.636 billion years ago. Vaalbara broke up c. 2.845–2.803 Ga ago. The supercontinent Kenorland was formed c. 2.72 Ga ago and then broke sometime after 2.45–2.1 Ga into the proto-continent cratons called Laurentia, Baltica, Yilgarn craton, and Kalahari. The supercontinent Columbia or Nuna formed 2.06–1.82 billion years ago and broke up about 1.5–1.35 billion years ago. The supercontinent Rodinia is thought to have formed about 1.13–1.071 billion years ago, to have embodied most or all of Earth's continents and to have broken up into eight continents around 750–600 million years ago.





</doc>
<doc id="23647" url="https://en.wikipedia.org/wiki?curid=23647" title="Polymerase chain reaction">
Polymerase chain reaction

Polymerase chain reaction (PCR) is a method widely used in molecular biology to rapidly make millions to billions of copies of a specific DNA sample allowing scientists to take a very small sample of DNA and amplify it to a large enough amount to study in detail. PCR was invented in 1983 by Kary Mullis. It is fundamental to much of genetic testing including analysis of ancient samples of DNA and identification of infectious agents. Using PCR, copies of very small amounts of DNA sequences are exponentially amplified in a series or cycles of temperature changes. PCR is now a common and often indispensable technique used in medical laboratory and clinical laboratory research for a broad variety of applications including biomedical research and criminal forensics. 
The vast majority of PCR methods rely on thermal cycling. Thermal cycling exposes reactants to repeated cycles of heating and cooling to permit different temperature-dependent reactions – specifically, DNA melting and enzyme-driven DNA replication. PCR employs two main reagents – primers (which are short single strand DNA fragments known as oligonucleotides that are a complementary sequence to the target DNA region) and a DNA polymerase. In the first step of PCR, the two strands of the DNA double helix are physically separated at a high temperature in a process called Nucleic acid denaturation. In the second step, the temperature is lowered and the primers bind to the complementary sequences of DNA. The two DNA strands then become templates for DNA polymerase to enzymatically assemble a new DNA strand from free nucleotides, the building blocks of DNA. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified.

Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase, an enzyme originally isolated from the thermophilic bacterium "Thermus aquaticus". If the polymerase used was heat-susceptible, it would denature under the high temperatures of the denaturation step. Before the use of Taq polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process.

Applications of the technique include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of hereditary diseases; amplification of ancient DNA; analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases.

PCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods amplify DNA fragments of between 0.1 and 10 kilo base pairs (kbp) in length, although some techniques allow for amplification of fragments up to 40 kbp. The amount of amplified product is determined by the available substrates in the reaction, which become limiting as the reaction progresses.

A basic PCR set-up requires several components and reagents, including a "DNA template" that contains the DNA target region to amplify; a "DNA polymerase"; an enzyme that polymerizes new DNA strands; heat-resistant Taq polymerase is especially common, as it is more likely to remain intact during the high-temperature DNA denaturation process; two DNA "primers" that are complementary to the 3' (three prime) ends of each of the sense and anti-sense strands of the DNA target (DNA polymerase can only bind to and elongate from a double-stranded region of DNA; without primers there is no double-stranded initiation site at which the polymerase can bind); specific primers that are complementary to the DNA target region are selected beforehand, and are often custom-made in a laboratory or purchased from commercial biochemical suppliers; "deoxynucleoside triphosphates", or dNTPs (sometimes called "deoxynucleotide triphosphates"; nucleotides containing triphosphate groups), the building blocks from which the DNA polymerase synthesizes a new DNA strand; a "buffer solution" providing a suitable chemical environment for optimum activity and stability of the DNA polymerase; "bivalent cations", typically magnesium (Mg) or manganese (Mn) ions; Mg is the most common, but Mn can be used for PCR-mediated DNA mutagenesis, as a higher Mn concentration increases the error rate during DNA synthesis;
and "monovalent cations", typically potassium (K) ions

The reaction is commonly carried out in a volume of 10–200 μL in small reaction tubes (0.2–0.5 mL volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of the Peltier effect, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibration. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermal cyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.

Typically, PCR consists of a series of 20–40 repeated temperature changes, called thermal cycles, with each cycle commonly consisting of two or three discrete temperature steps (see figure below). The cycling is often preceded by a single temperature step at a very high temperature (>), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters, including the enzyme used for DNA synthesis, the concentration of bivalent ions and dNTPs in the reaction, and the melting temperature ("T") of the primers. The individual steps common to most PCR methods are as follows:




To check whether the PCR successfully generated the anticipated DNA target region (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis may be employed for size separation of the PCR products. The size(s) of PCR products is determined by comparison with a DNA ladder, a molecular weight marker which contains DNA fragments of known size run on the gel alongside the PCR products.
As with other chemical reactions, the reaction rate and efficiency of PCR are affected by limiting factors. Thus, the entire PCR process can further be divided into three stages based on reaction progress:

In practice, PCR can fail for various reasons, in part due to its sensitivity to contamination causing amplification of spurious DNA products. Because of this, a number of techniques and procedures have been developed for optimizing PCR conditions. Contamination with extraneous DNA is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. This usually involves spatial separation of PCR-setup areas from areas for analysis or purification of PCR products, use of disposable plasticware, and thoroughly cleaning the work surface between reaction setups. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of spurious products, and the usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. Addition of reagents, such as formamide, in buffer systems may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.

PCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments many ways, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.

Other applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies "(such as E. coli)" can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.

Some PCR 'fingerprints' methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e., the 16S rRNA and recA genes of microorganisms).

Because PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ancient DNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.

Quantitative PCR or Real Time PCR (qPCR, not to be confused with RT-PCR) methods allow the estimation of the amount of a given sequence present in a sample—a technique often applied to quantitatively determine levels of gene expression. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification.

qPCR allows the quantification and detection of a specific DNA sequence in real time since it measures concentration while the synthesis process is taking place. There are two methods for simultaneous detection and quantification. The first method consists of using fluorescent dyes that are retained nonspecifically in between the double strands. The second method involves probes that code for specific sequences and are fluorescently labeled. Detection of DNA using these methods can only be seen after the hybridization of probes with its complementary DNA takes place. An interesting technique combination is real-time PCR and reverse transcription. This sophisticated technique, called RT-qPCR, allows for the quantification of a small quantity of RNA. Through this combined technique, mRNA is converted to cDNA, which is further quantified using qPCR. This technique lowers the possibility of error at the end point of PCR, increasing chances for detection of genes associated with genetic diseases such as cancer. Laboratories use RT-qPCR for the purpose of sensitively measuring gene regulation.

Prospective parents can be tested for being genetic carriers, or their children might be tested for actually being affected by a disease. DNA samples for prenatal testing can be obtained by amniocentesis, chorionic villus sampling, or even by the analysis of rare fetal cells circulating in the mother's bloodstream. PCR analysis is also essential to preimplantation genetic diagnosis, where individual cells of a developing embryo are tested for mutations.

PCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.

Characterization and detection of infectious disease organisms have been revolutionized by PCR in the following ways:


The development of PCR-based genetic (or DNA) fingerprinting protocols has seen widespread application in forensics:


PCR has been applied to many areas of research in molecular genetics:


PCR has a number of advantages. It is fairly simple to understand and to use, and produces results rapidly. The technique is highly sensitive with the potential to produce millions to billions of copies of a specific product for sequencing, cloning, and analysis. qRT-PCR shares the same advantages as the PCR, with an added advantage of quantification of the synthesized product. Therefore, it has its uses to analyze alterations of gene expression levels in tumors, microbes, or other disease states.

PCR is a very powerful and practical research tool. The sequencing of unknown etiologies of many diseases are being figured out by the PCR. The technique can help identify the sequence of previously unknown viruses related to those already known and thus give us a better understanding of the disease itself. If the procedure can be further simplified and sensitive non radiometric detection systems can be developed, the PCR will assume a prominent place in the clinical laboratory for years to come.

One major limitation of PCR is that prior information about the target sequence is necessary in order to generate the primers that will allow its selective amplification.<ref name="10.1038/jid.2013.1"></ref> This means that, typically, PCR users must know the precise sequence(s) upstream of the target region on each of the two single-stranded templates in order to ensure that the DNA polymerase properly binds to the primer-template hybrids and subsequently generates the entire target region during DNA synthesis.

Like all enzymes, DNA polymerases are also prone to error, which in turn causes mutations in the PCR fragments that are generated.

Another limitation of PCR is that even the smallest amount of contaminating DNA can be amplified, resulting in misleading or ambiguous results. To minimize the chance of contamination, investigators should reserve separate rooms for reagent preparation, the PCR, and analysis of product. Reagents should be dispensed into single-use aliquots. Pipetters with disposable plungers and extra-long pipette tips should be routinely used.



A 1971 paper in the "Journal of Molecular Biology" by and co-workers in the laboratory of H. Gobind Khorana first described a method of using an enzymatic assay to replicate a short DNA template with primers "in vitro". However, this early manifestation of the basic PCR principle did not receive much attention at the time and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.

When Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies, where he was responsible for synthesizing short chains of DNA. Mullis has written that he first conceived the idea for PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In "Scientific American", Mullis summarized the procedure: "Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat." DNA fingerprinting was first used for paternity testing in 1988.

Mullis was awarded the Nobel Prize in Chemistry in 1993 for his invention, seven years after he and his colleagues at Cetus first put his proposal to practice. Mullis’s 1985 paper with R. K. Saiki and H. A. Erlich, “Enzymatic Amplification of β-globin Genomic Sequences and Restriction Site Analysis for Diagnosis of Sickle Cell Anemia”—the polymerase chain reaction invention (PCR) – was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society in 2017.

Some controversies have remained about the intellectual and practical contributions of other scientists to Mullis' work, and whether he had been the sole inventor of the PCR principle (see below).

At the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of > required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time-consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.

The discovery in 1976 of Taq polymerase—a DNA polymerase purified from the thermophilic bacterium, "Thermus aquaticus", which naturally lives in hot () environments such as hot springs—paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from "T. aquaticus" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.

The PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The "Taq" polymerase enzyme was also covered by patents. There have been several high-profile lawsuits related to the technique, including an unsuccessful lawsuit brought by DuPont. The Swiss pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992 and currently holds those that are still protected.

A related patent battle over the Taq polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and Taq polymerase patents, which expired on March 28, 2005.




</doc>
<doc id="23648" url="https://en.wikipedia.org/wiki?curid=23648" title="Polymerase">
Polymerase

A polymerase is an enzyme (EC 2.7.7.6/7/19/48/49) that synthesizes long chains of polymers or nucleic acids. DNA polymerase and RNA polymerase are used to assemble DNA and RNA molecules, respectively, by copying a DNA template strand using base-pairing interactions or RNA by half ladder replication.

A DNA polymerase from the thermophilic bacterium, "Thermus aquaticus" ("Taq") (PDB 1BGX, EC 2.7.7.7) is used in the polymerase chain reaction, an important technique of molecular biology.


In general, viral single-subunit RNA polymerases/replicases/reverse transcriptase shares a common origin with DNA polymerase. They have a conserved "palm" domain. Multi-subunit RNA polymerase forms an unrelated group. Primases have a more complex story: bacterial primases with the Toprim domain are related to topoisomerase and mitochrondrial helicase, while archaea and eukaryotic primases form an unrelated family, possibly related to the polymerase palm. Both families nevertheless associate to the same bunch of helicases.


</doc>
<doc id="23649" url="https://en.wikipedia.org/wiki?curid=23649" title="Pacific Scandal">
Pacific Scandal

The Pacific Scandal was a political scandal in Canada involving bribes being accepted by 150 members of the Conservative government in the attempts of private interests to influence the bidding for a national rail contract. As part of British Columbia's 1871 agreement to join the Canadian Confederation, the government had agreed to build a transcontinental railway linking the Pacific Province to the eastern provinces.

The scandal led to the resignation of Canada's first Prime Minister, Sir John A. Macdonald, and a transfer of power from his Conservative government to a Liberal government led by Alexander Mackenzie. One of the new government's first measures was to introduce secret ballots in an effort to improve the integrity of future elections. After the scandal broke, the railway plan collapsed and the proposed line was not built. An entirely different operation later built the Canadian Pacific Railway to the Pacific.

For a young and loosely defined nation, the building of a national railway was an active attempt at state-making, as well as an aggressive capitalist venture. Canada, a nascent country with a population of 3.5 million in 1871, lacked the means to exercise meaningful "de facto" control within the "de jure" political boundaries of the recently acquired Rupert's Land; building a transcontinental railway was national policy of high order to change this situation. Moreover, after the American Civil War the American frontier rapidly expanded west with land-hungry settlers, exacerbating talk of annexation. Indeed, sentiments of Manifest Destiny were abuzz in this time: in 1867, year of Confederation, US Secretary of State W. H. Seward surmised that the whole North American continent "shall be, sooner or later, within the magic circle of the American Union". Therefore, preventing American investment into the project was considered as being in Canada's national interest. Thus the federal government favoured an "all Canadian route" through the rugged Canadian Shield of northern Ontario, refusing to consider a less costly route passing south through Wisconsin and Minnesota.

However, a route across the Canadian Shield was highly unpopular with potential investors, not only in the United States but also in Canada and especially Great Britain, the only other viable source of financing. For would-be investors, the objections were not primarily based on politics or nationalism but economics. At the time, national governments lacked the finances needed to undertake such large projects. For the First Transcontinental Railroad, the United States government had made extensive grants of public land to the railway's builders, inducing private financiers to fund the railway on the understanding that they would acquire rich farmland along the route, which could then be sold for a large profit. However, the eastern terminus of the proposed Canadian Pacific route, unlike that of the First Transcontinental, was not in rich Nebraskan farmland, but deep within the Canadian Shield. Copying the American financing model whilst insisting on an all-Canadian route would require the railway's backers to build hundreds of miles of track across rugged shield terrain (with little economic value) at considerable expense before they could expect to access lucrative farmland in Manitoba and the newly created Northwest Territories. Many financiers, who had expected to make a relatively quick profit, were not willing to make this sort of long-term commitment.

Nevertheless, the Montreal capitalist Hugh Allan, with his syndicate Canada Pacific Railway Company, sought the potentially lucrative charter for the project. The problem lay in that Allan and Macdonald highly, and secretly, were in cahoots with American financiers such as George W. McMullen and Jay Cooke, men who were deeply interested in the rival American undertaking, the Northern Pacific Railroad.

Two groups competed for the contract to build the railway, Hugh Allan's Canada Pacific Railway Company and David Lewis Macpherson's Inter-Oceanic Railway Company. On April 2, 1873, Lucius Seth Huntington, a Liberal Member of Parliament, created an uproar in the House of Commons. He announced he had uncovered evidence that Allan and his associates had been granted the Canadian Pacific Railway contract in return for political donations of $360,000.

In 1873, it became known that Allan had contributed a large sum of money to the Conservative government's re-election campaign of 1872; some sources quote a sum over $360,000. Allan had promised to keep American capital out of the railway deal, but had lied to Macdonald over this vital point, and Macdonald later discovered the lie. The Liberal party, at this time the opposition party in Parliament, accused the Conservatives of having made a tacit agreement to give the contract to Hugh Allan in exchange for money.

In making such allegations, the Liberals and their allies in the press (in particular, George Brown's newspaper the Globe) presumed that most of the money had been used to bribe voters in the 1872 election. The secret ballot, then considered a novelty, had not yet been introduced in Canada. Although it was illegal to offer, solicit or accept bribes in exchange for votes, effective enforcement of this prohibition proved impossible.

Despite Macdonald's claims that he was innocent, evidence came to light showing receipts of money from Allan to Macdonald and some of his political colleagues. Perhaps even more damaging to Macdonald was when the Liberals discovered a telegram, through a former employee of Allan, which was thought to have been stolen from the safe of Allan's lawyer, John Abbott.

The scandal proved fatal to Macdonald's government. Macdonald's control of Parliament was already tenuous following the 1872 election. In a time when party discipline was not as strong as it is today, once Macdonald's culpability in the scandal became known he could no longer expect to retain the confidence of the House of Commons.

Macdonald resigned as prime minister on 5 November 1873. He also offered his resignation as the head of the Conservative party, but it was not accepted and he was convinced to stay. Perhaps as a direct result of this scandal, the Conservative party fell in the eyes of the public and was relegated to being the Official Opposition in the federal election of 1874. This election, in which secret ballots were used for the first time, gave Alexander Mackenzie a firm mandate to succeed Macdonald as the new prime minister of Canada.

Despite the short-term defeat, the scandal was not a mortal wound to Macdonald, the Conservative Party, or the Canadian Pacific Railway. An economic depression gripped Canada shortly after Macdonald left office, and although the causes of the depression were largely external to Canada many Canadians nevertheless blamed Mackenzie for the ensuing hard times. Macdonald would return as prime minister in the 1878 election thanks to his National Policy. He would hold the office of prime minister to his death in 1891, and the Canadian Pacific would be completed by 1885 with Macdonald still in office.




</doc>
<doc id="23650" url="https://en.wikipedia.org/wiki?curid=23650" title="Primer (molecular biology)">
Primer (molecular biology)

A primer is a short single-stranded nucleic acid utilized by all living organisms in the initiation of DNA synthesis. The enzymes responsible for DNA replication, DNA polymerases, are only capable of adding nucleotides to the 3’-end of an existing nucleic acid, requiring a primer be bound to the template before DNA polymerase can begin a complementary strand. Living organisms use solely RNA primers, while laboratory techniques in biochemistry and molecular biology that require in vitro DNA synthesis (such as DNA sequencing and polymerase chain reaction) usually use DNA primers, since they are more temperature stable.

RNA primers are used by living organisms in the initiation of synthesizing a strand of DNA. A class of enzymes called primases add a complementary RNA primer to the reading template "de novo" on both the leading and lagging strands. Starting from the free 3’-OH of the primer, known as the primer terminus, a DNA polymerase can extend a newly synthesized strand. The leading strand in DNA replication is synthesized in one continuous piece moving with the replication fork, requiring only an initial RNA primer to begin synthesis. In the lagging strand, the template DNA runs in the 5′→3′ direction. Since DNA polymerase cannot add bases in the 3′→5′ direction complementary to the template strand, DNA is synthesized ‘backward’ in short fragments moving away from the replication fork, known as Okazaki fragments. Unlike in the leading strand, this method results in the repeated starting and stopping of DNA synthesis, requiring multiple RNA primers. Along the DNA template, primase intersperses RNA primers that DNA polymerase uses to synthesize DNA from in the 5′→3′ direction.

Another example of primers being used to enable DNA synthesis is reverse transcription. Reverse transcriptase is an enzyme that uses a template strand of RNA to synthesize a complementary strand of DNA. The DNA polymerase component of reverse transcriptase requires an existing 3' end to begin synthesis.

After the insertion of Okazaki fragments, the RNA primers are removed (the mechanism of removal differs between prokaryotes and eukaryotes) and replaced with new deoxyribonucleotides that fill the gaps where the RNA was present. DNA ligase then joins the fragmented strands together, completing the synthesis of the lagging strand.

In prokaryotes, DNA polymerase I synthesizes the Okazaki fragment until it reaches the previous RNA primer. Then the enzyme simultaneously acts as a 5′→3′ exonuclease, removing primer ribonucleotides in front and adding deoxyribonucleotides behind until the region has been replaced by DNA, leaving a small gap in the DNA backbone between Okazaki fragments which is sealed by DNA ligase.

In eukaryotic primer removal, DNA polymerase δ extends the Okazaki fragment in 5′→3′ direction, and upon encountering the RNA primer from the previous Okazaki fragment, it displaces the 5′ end of the primer into a single-stranded RNA flap, which is removed by nuclease cleavage. Cleavage of the RNA flaps involves either flap structure-specific endonuclease 1 (FEN1) cleavage of short flaps, or coating of long flaps by the single-stranded DNA binding protein replication protein A (RPA) and sequential cleavage by Dna2 nuclease and FEN1.

Synthetic primers are chemically synthesized oligonucleotides, usually of DNA, which can be customized to anneal to a specific site on the template DNA. In solution, the primer spontaneously hybridizes with the template through Watson-Crick base pairing before being extended by DNA polymerase. The ability to create and customize synthetic primers has proven an invaluable tool necessary to a variety of molecular biological approaches involving the analysis of DNA. Both the Sanger chain termination method and the “Next-Gen” method of DNA sequencing require primers to initiate the reaction.

The polymerase chain reaction (PCR) uses a pair of custom primers to direct DNA elongation toward each-other at opposite ends of the sequence being amplified. These primers are typically between 18 and 24 bases in length, and must code for only the specific upstream and downstream sites of the sequence being amplified. A primer that can bind to multiple regions along the DNA will amplify them all, eliminating the purpose of PCR.

A few criteria must be brought into consideration when designing a pair of PCR primers. Pairs of primers should have similar melting temperatures since annealing during PCR occurs for both strands simultaneously, and this shared melting temperature must not be either too much higher or lower than the reaction's annealing temperature. A primer with a "T" (melting temperature) too much higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence. A "T" significantly lower than the annealing temperature may fail to anneal and extend at all.

Additionally, primer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of hybridization to a similar sequence nearby. A commonly used method for selecting a primer site is BLAST search, whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design and BLAST search into one application, as do commercial software products such as ePrime and Beacon Designer. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design by giving melting and annealing temperatures, etc.

Many online tools are freely available for primer design, some of which focus on specific applications of PCR. The popular tools Primer3Plus and PrimerQuest can be used to find primers matching a wide variety of specifications. Highly degenerate primers for targeting a wide variety of DNA templates can be interactively designed using GeneFISHER. Primers with high specificity for a subset of DNA templates in the presence of many similar variants can be designed using DECIPHER. Primer design aims to generate a balance between specificity and efficiency of amplification.

Selecting a specific region of DNA for primer binding requires some additional considerations. Regions high in mononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers should not easily anneal with other primers in the mixture; this phenomenon can lead to the production of 'primer dimer' products contaminating the end solution. Primers should also not anneal strongly to themselves, as internal hairpins and loops could hinder the annealing with the template DNA.

When designing primers, additional nucleotide bases can be added to the back ends of each primer, resulting in a customized cap sequence on each end of the amplified region. One application for this practice is for use in TA cloning, a special subcloning technique similar to PCR, where efficiency can be increased by adding AG tails to the 5′ and the 3′ ends.

Some situations may call for the use of "degenerate primers." These are mixtures of primers that are similar, but not identical. These may be convenient when amplifying the same gene from different organisms, as the sequences are probably similar but not identical. This technique is useful because the genetic code itself is degenerate, meaning several different codons can code for the same amino acid. This allows different organisms to have a significantly different genetic sequence that code for a highly similar protein. For this reason, degenerate primers are also used when primer design is based on protein sequence, as the specific sequence of codons are not known. Therefore, primer sequence corresponding to the amino acid isoleucine might be "ATH", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Degenerate primers may not perfectly hybridize with a target sequence, which can greatly reduce the specificity of the PCR amplification.

"Degenerate primers" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations of the codon sequence.




</doc>
<doc id="23652" url="https://en.wikipedia.org/wiki?curid=23652" title="Purine">
Purine

Purine is a heterocyclic aromatic organic compound that consists of a pyrimidine ring fused to an imidazole ring. It is water-soluble. Purine also gives its name to the wider class of molecules, purines, which include substituted purines and their tautomers. They are the most widely occurring nitrogen-containing heterocycles in nature.

Purines are found in high concentration in meat and meat products, especially internal organs such as liver and kidney. In general, plant-based diets are low in purines. Examples of high-purine sources include: sweetbreads, anchovies, sardines, liver, beef kidneys, brains, meat extracts (e.g., Oxo, Bovril), herring, mackerel, scallops, game meats, beer (from the yeast) and gravy.

A moderate amount of purine is also contained in red meat, beef, pork, poultry, fish and seafood, asparagus, cauliflower, spinach, mushrooms, green peas, lentils, dried peas, beans, oatmeal, wheat bran, wheat germ, and haws.

Purines and pyrimidines make up the two groups of nitrogenous bases, including the two groups of nucleotide bases. Two of the four deoxyribonucleotides (deoxyadenosine and deoxyguanosine) and two of the four ribonucleotides (adenosine, or AMP, and guanosine, or GMP), the respective building blocks of DNA and RNA, are purines. In order to form DNA and RNA, both purines and pyrimidines are needed by the cell in approximately equal quantities. Both purine and pyrimidine are self-inhibiting and activating. When purines are formed, they inhibit the enzymes required for more purine formation. This self-inhibition occurs as they also activate the enzymes needed for pyrimidine formation. Pyrimidine simultaneously self-inhibits and activates purine in similar manner. Because of this, there is nearly an equal amount of both substances in the cell at all times.

Purine is both a very weak acid (pK 2.39) and an even weaker base (pK 8.93). If dissolved in pure water, the pH will be halfway between these two pKa values.

There are many naturally occurring purines. They include the nucleobases adenine (2) and guanine (3). In DNA, these bases form hydrogen bonds with their complementary pyrimidines, thymine and cytosine, respectively. This is called complementary base pairing. In RNA, the complement of adenine is uracil instead of thymine.

Other notable purines are hypoxanthine (4), xanthine (5), theobromine (6), caffeine (7), uric acid (8) and isoguanine (9).

Aside from the crucial roles of purines (adenine and guanine) in DNA and RNA, purines are also significant components in a number of other important biomolecules, such as ATP, GTP, cyclic AMP, NADH, and coenzyme A. Purine (1) itself, has not been found in nature, but it can be produced by organic synthesis.

They may also function directly as neurotransmitters, acting upon purinergic receptors. Adenosine activates adenosine receptors.

The word "purine" ("pure urine") was coined by the German chemist Emil Fischer in 1884. He synthesized it for the first time in 1898. The starting material for the reaction sequence was uric acid (8), which had been isolated from kidney stones by Carl Wilhelm Scheele in 1776. Uric acid (8) was reacted with PCl to give 2,6,8-trichloropurine (10), which was converted with HI and PHI to give 2,6-diiodopurine (11). The product was reduced to purine (1) using zinc dust.

Many organisms have metabolic pathways to synthesize and break down purines.

Purines are biologically synthesized as nucleosides (bases attached to ribose).

Accumulation of modified purine nucleotides is defective to various cellular processes, especially those involving DNA and RNA. To be viable, organisms possess a number of (deoxy)purine phosphohydrolases, which hydrolyze these purine derivatives removing them from the active NTP and dNTP pools. Deamination of purine bases can result in accumulation of such nucleotides as ITP, dITP, XTP and dXTP.

Defects in enzymes that control purine production and breakdown can severely alter a cell’s DNA sequences, which may explain why people who carry certain genetic variants of purine metabolic enzymes have a higher risk for some types of cancer.

Higher levels of meat and seafood consumption are associated with an increased risk of gout, whereas a higher level of consumption of dairy products is associated with a decreased risk. Moderate intake of purine-rich vegetables or protein is not associated with an increased risk of gout. Similar results have been found with the risk of hyperuricemia.

In addition to "in vivo" synthesis of purines in purine metabolism, purine can also be created artificially.

Purine (1) is obtained in good yield when formamide is heated in an open vessel at 170 °C for 28 hours.

This remarkable reaction and others like it have been discussed in the context of the origin of life.

Oro, Orgel and co-workers have shown that four molecules of HCN tetramerize to form diaminomaleodinitrile (12), which can be converted into almost all naturally occurring purines. For example, five molecules of HCN condense in an exothermic reaction to make adenine, especially in the presence of ammonia.

The Traube purine synthesis (1900) is a classic reaction (named after Wilhelm Traube) between an amine-substituted pyrimidine and formic acid.




</doc>
<doc id="23653" url="https://en.wikipedia.org/wiki?curid=23653" title="Pyrimidine">
Pyrimidine

Pyrimidine is an aromatic heterocyclic organic compound similar to pyridine. One of the three diazines (six-membered heterocyclics with two nitrogen atoms in the ring), it has the nitrogen atoms at positions 1 and 3 in the ring. The other diazines are pyrazine (nitrogen atoms at the 1 and 4 positions) and pyridazine (nitrogen atoms at the 1 and 2 positions). In nucleic acids, three types of nucleobases are pyrimidine derivatives: cytosine (C), thymine (T), and uracil (U).

The pyrimidine ring system has wide occurrence in nature
as substituted and ring fused compounds and derivatives, including the nucleotides cytosine, thymine and uracil, thiamine (vitamin B1) and alloxan. It is also found in many synthetic compounds such as barbiturates and the HIV drug, zidovudine. Although pyrimidine derivatives such as uric acid and alloxan were known in the early 19th century, a laboratory synthesis of a pyrimidine was not carried out until 1879, when Grimaux reported the preparation of barbituric acid from urea and malonic acid in the presence of phosphorus oxychloride.
The systematic study of pyrimidines began in 1884 with Pinner,
who synthesized derivatives by condensing ethyl acetoacetate with amidines. Pinner first proposed the name “pyrimidin” in 1885. The parent compound was first prepared by Gabriel and Colman in 1900,

by conversion of barbituric acid to 2,4,6-trichloropyrimidine followed by reduction using zinc dust in hot water.

The nomenclature of pyrimidines is straightforward. However, like other heterocyclics, tautomeric hydroxyl groups yield complications since they exist primarily in the cyclic amide form. For example, 2-hydroxypyrimidine is more properly named 2-pyrimidone. A partial list of trivial names of various pyrimidines exists.

Physical properties are shown in the data box. A more extensive discussion, including spectra, can be found in Brown "et al."

Per the classification by Albert six-membered heterocycles can be described as π-deficient. Substitution by electronegative groups or additional nitrogen atoms in the ring significantly increase the π-deficiency. These effects also decrease the basicity.

Like pyridines, in pyrimidines the π-electron density is decreased to an even greater extent. Therefore, electrophilic aromatic substitution is more difficult while nucleophilic aromatic substitution is facilitated. An example of the last reaction type is the displacement of the amino group in 2-aminopyrimidine by chlorine and its reverse.

Electron lone pair availability (basicity) is decreased compared to pyridine. Compared to pyridine, "N"-alkylation and "N"-oxidation are more difficult. The p"K" value for protonated pyrimidine is 1.23 compared to 5.30 for pyridine. Protonation and other electrophilic additions will occur at only one nitrogen due to further deactivation by the second nitrogen. The 2-, 4-, and 6- positions on the pyrimidine ring are electron deficient analogous to those in pyridine and nitro- and dinitrobenzene. The 5-position is less electron deficient and substituents there are quite stable. However, electrophilic substitution is relatively facile at the 5-position, including nitration and halogenation.

Reduction in resonance stabilization of pyrimidines may lead to addition and ring cleavage reactions rather than substitutions. One such manifestation is observed in the Dimroth rearrangement.

Pyrimidine is also found in meteorites, but scientists still do not know its origin. Pyrimidine also photolytically decomposes into uracil under ultraviolet light.

As is often the case with parent heterocyclic ring systems, the synthesis of pyrimidine is not that common and is usually performed by removing functional groups from derivatives. Primary syntheses in quantity involving formamide have been reported.

As a class, pyrimidines are typically synthesized by the principal synthesis involving cyclization of β-dicarbonyl compounds with N–C–N compounds. Reaction of the former with amidines to give 2-substituted pyrimidines, with urea to give 2-pyrimidinones, and guanidines to give 2-aminopyrimidines are typical.

Pyrimidines can be prepared via the Biginelli reaction. Many other methods rely on condensation of carbonyls with diamines for instance the synthesis of 2-thio-6-methyluracil from thiourea and ethyl acetoacetate or the synthesis of 4-methylpyrimidine with 4,4-dimethoxy-2-butanone and formamide.

A novel method is by reaction of "N"-vinyl and "N"-aryl amides with carbonitriles under electrophilic activation of the amide with 2-chloro-pyridine and trifluoromethanesulfonic anhydride:

Because of the decreased basicity compared to pyridine, electrophilic substitution of pyrimidine is less facile. Protonation or alkylation typically takes place at only one of the ring nitrogen atoms. Mono-"N"-oxidation occurs by reaction with peracids.

Electrophilic "C"-substitution of pyrimidine occurs at the 5-position, the least electron-deficient. Nitration, nitrosation, azo coupling, halogenation, sulfonation, formylation, hydroxymethylation, and aminomethylation have been observed with substituted pyrimidines.

Nucleophilic "C"-substitution should be facilitated at the 2-, 4-, and 6-positions but there are only a few examples. Amination and hydroxylation has been observed for substituted pyrimidines. Reactions with Grignard or alkyllithium reagents yield 4-alkyl- or 4-aryl pyrimidine after aromatization.

Free radical attack has been observed for pyrimidine and photochemical reactions have been observed for substituted pyrimidines. Pyrimidine can be hydrogenated to give tetrahydropyrimidine.

Three nucleobases found in nucleic acids, cytosine (C), thymine (T), and uracil (U), are pyrimidine derivatives:

In DNA and RNA, these bases form hydrogen bonds with their complementary purines. Thus, in DNA, the purines adenine (A) and guanine (G) pair up with the pyrimidines thymine (T) and cytosine (C), respectively.

In RNA, the complement of adenine (A) is uracil (U) instead of thymine (T), so the pairs that form are adenine:uracil and guanine:cytosine.

Very rarely, thymine can appear in RNA, or uracil in DNA, but when the other three major pyrimidine bases are represented, some minor pyrimidine bases can also occur in nucleic acids. These minor pyrimidines are usually methylated versions of major ones and are postulated to have regulatory functions.

These hydrogen bonding modes are for classical Watson–Crick base pairing. Other hydrogen bonding modes ("wobble pairings") are available in both DNA and RNA, although the additional 2′-hydroxyl group of RNA expands the configurations, through which RNA can form hydrogen bonds.

In March 2015, NASA Ames scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds.


</doc>
<doc id="23654" url="https://en.wikipedia.org/wiki?curid=23654" title="Play-by-mail game">
Play-by-mail game

Play-by-mail games, or play-by-post games, are games, of any type, played through postal mail or email.

Correspondence chess has been played by mail for centuries. The boardgame "Diplomacy" has been played by mail since the 1960s, starting with a printed newsletter (a fanzine) written by John Boardman. More complex games, moderated entirely or partially by computer programs, were pioneered by Rick Loomis of Flying Buffalo in 1970. The first such game offered via major email services was "WebWar II" (based on Starweb and licensed from Flying Buffalo) from Neolithic Enterprises who accepted email turns from all of the major email services including CompuServe in 1983.

Play by mail games are often referred to as PBM games, and play by email is sometimes abbreviated PBeM—as opposed to face to face (FTF) or over the board (OTB) games which are played in person. Another variation on the name is play-by-Internet (PBI) or play-by-web (PBW). In all of these examples, player instructions can be either executed by a human moderator, a computer program, or a combination of the two.

In the 1980s, play-by-mail games reached their peak of popularity with the advent of "Gaming Universal", "Paper Mayhem" and "Flagship magazine", the first professional magazines devoted to play-by-mail games. (An earlier fanzine, "The Nuts & Bolts of PBM", was the first publication to exclusively cover the hobby.) Bob McLain, the publisher and editor of Gaming Universal, further popularized the hobby by writing articles that appeared in many of the leading mainstream gaming magazines of the time. Flagship later bought overseas right to Gaming Universal, making it the leading magazine in the field. "Flagship magazine" was founded by Chris Harvey and Nick Palmer. The magazine still exists, under a new editor, but health concerns have led to worries over the publication's long term viability.

In the late 1990s, computer and Internet games marginalized play-by-mail conducted by actual postal mail, but the postal hobby still exists with an estimated 2000–3000 adherents worldwide.

Postal gaming developed as a way for geographically separated gamers to compete with each other. It was especially useful for those living in isolated areas and those whose tastes in games were uncommon.

In the case of a two player game such as chess, players would simply send their moves to each other alternately. In the case of a multi-player game such as "Diplomacy", a central game master would run the game, receiving the moves and publishing adjudications. Such adjudications were often published in postal game zines, some of which contained far more than just games.

The commercial market for play-by-mail games grew to involve computer servers set up to host potentially thousands of players at once. Players would typically be split up into parallel games in order to keep the number of players per game at a reasonable level, with new games starting as old games ended. A typical closed game session might involve one to two dozen players, although some games claimed to have as many as five hundred people simultaneously competing in the same game world. While the central company was responsible for feeding in moves and mailing the processed output back to players, players were also provided with the mailing addresses of others so that direct contact could be made and negotiations performed. With turns being processed every few weeks (a two-week turnaround being standard), more advanced games could last over a year.

Game themes are heavily varied, and may range from those based on historical or real events to those taking place in alternate or fictional worlds.

One of the most successful and longest running PBM games is TribeNet, a strategy game with themes of exploration, trade and warfare. TribeNet was launched by Jeff Perkins as a PBM in 1985 and was transformed by Peter Rzechorzek into a PBeM when he took over the game in 1997. Peter has remained in charge for over twenty years.

The onset of the computer-moderated PBM game (primarily the "Legends" game system) inevitably meant that the human moderated games became "boutique" games with little chance of matching the gross revenues that larger, automated games could produce.

The mechanics of play-by-mail games require that players think and plan carefully before making moves. Because planned actions can typically only be submitted at a fixed maximum frequency (e.g., once every few days or every few weeks), the number of discrete actions is limited compared to real-time games. As a result, players are provided with a variety of resources to assist in turn planning, including game aids, maps, and results from previous turns. Using this material, planning a single turn may take a number of hours.

Actual move/turn submission is traditionally carried out by filling in a "turn card". This card has formatted entry areas where players enter their planned actions (using some form of encoding) for the upcoming turn. Players are limited to some finite number of actions, and in some cases must split their resources between these actions (so that additional actions make each less effective). The way the card is filled in often implies an ordering between each command, so that they are processed in-order, one after another. Once completed, the card is then mailed (or, in more modern times, emailed) to the game master, where it is either processed, or held until the next turn processing window begins.

By gathering turn cards from a number of players and processing them all at the same time, games can provide simultaneous actions for all players. However, for this same reason, co-ordination between players can be difficult to achieve. For example, player A might attempt to move to player B's current location to do something with (or to) player B, while player B might simultaneously attempt to move to player A's current location. As such, the output/results of the turn can differ significantly from the submitted plan. Whatever the results, they are mailed back to the player to be studied and used as the basis for the next turn (often along with a new blank turn card).

While billing is sometimes done using a flat per-game rate (when the length of the game is known and finite), games more typically use a per-turn cost schedule. In such cases, each turn submitted depletes a pool of credit which must periodically be replenished in order to keep playing. Some games have multiple fee schedules, where players can pay more to perform advanced actions, or to take a greater number of actions in a turn.

Some role-playing PBM games also include an element whereby the player may describe actions of their characters in a free text form. The effect and effectiveness of the action is then based on the judgement of the GM who may allow or partially allow the action. This gives the player more flexibility beyond the normal fixed actions at the cost of more complexity and, usually, expense.

With the rise of the Internet, email and websites have largely replaced postal gaming and postal games zines. Play-by-email (PBEM) games differ from popular online multiplayer games in that, for most computerized multiplayer games, the players have to be online at the same time - also known as synchronous play. With a play-by-mail game, the players can play whenever they choose, since responses need not be immediate; this is sometimes referred to as turn-based gaming and is common among browser-based games. Some video games can be played in turn-based mode: one player makes a move, declaring it by email, and then the turn passes to another player who to makes his or her move. Depending on the game, a PBEM Aide may be available as an alternative to describing the move with text. Such an aide generally consists of a picture of the board along with movable playing pieces, cards, etc. A player moves the pieces as desired and then can either save a static picture of the new game state, or a replay sequentially showing all changes that transpired during his turn, and send this to his opponent.

Several non-commercial email games played on the Internet and BITNET predate these. Some cards games like poker can also be played by email using cryptography, such as with FXTOP.

An increasingly popular format for play-by-email games is play-by-web. As with play-by-email games the players are notified by email when it becomes their turn, but they must then return to the game's website to continue playing. The game may be either browser-based or a video game. The main advantage of this is that the players can be presented with a graphical representation of the game and an interactive interface to guide them through their turn. Since the notifications only have to remind the players that it is their turn they can just as easily be sent via instant messaging.

Some sites have extended this gaming style by allowing the players to see each other's actions as they are made. This allows for real time playing while everyone is online and active, or slower progress if not.

Increasingly, this format is being adopted by social and mobile games, often described using the term "asynchronous multiplayer".

Some video games, such as Civilization, are suitable for being played either play-by-web via separate client software or play-by-email sending game save packages from one player to another.




</doc>
<doc id="23658" url="https://en.wikipedia.org/wiki?curid=23658" title="Philip K. Dick Award">
Philip K. Dick Award

The Philip K. Dick Award is an American science fiction award given annually at Norwescon and sponsored by the Philadelphia Science Fiction Society and (since 2005) the Philip K. Dick Trust. Named after science fiction and fantasy writer Philip K. Dick, it has been awarded since 1983, the year after his death. It is awarded to the best original paperback published each year in the US.

The award was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. As of 2016, it is administered by Pat LoBrutto, John Silbersack, and Gordon Van Gelder. Past administrators include Algis Budrys, David G. Hartwell, and David Alexander Smith.

Winners are listed in bold.<br>
Authors of special citation entries are listed in "italics". The year in the table below indicates the year the book was published; winners are announced the following year.



</doc>
<doc id="23659" url="https://en.wikipedia.org/wiki?curid=23659" title="Plug-in (computing)">
Plug-in (computing)

In computing, a plug-in (or plugin, add-in, addin, add-on, or addon) is a software component that adds a specific feature to an existing computer program. When a program supports plug-ins, it enables customization.

Web browsers have historically allowed executables as plug-ins, though they are now mostly deprecated, which are a different type of software module than browser extensions. Two plug-in examples are the Adobe Flash Player for playing Adobe Flash content and a Java virtual machine for running applets.

A theme or skin is a preset package containing additional or changed graphical appearance details, achieved by the use of a graphical user interface (GUI) that can be applied to specific software and websites to suit the purpose, topic, or tastes of different users to customize the look and feel of a piece of computer software or an operating system front-end GUI (and window managers).

Applications support plug-ins for many reasons. Some of the main reasons include:

Types of applications and why they use plug-ins:

The host application provides services which the plug-in can use, including a way for plug-ins to register themselves with the host application and a protocol for the exchange of data with plug-ins. Plug-ins depend on the services provided by the host application and do not usually work by themselves. Conversely, the host application operates independently of the plug-ins, making it possible for end-users to add and update plug-ins dynamically without needing to make changes to the host application.

Programmers typically implement plug-in functionality using shared libraries, which get dynamically loaded at run time, installed in a place prescribed by the host application. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called "stacks") themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plugins by loading a directory of simple script files written in a scripting language like Python or Lua.

In Mozilla Foundation definitions, the words "add-on", "extension" and "plug-in" are not synonyms. "Add-on" can refer to anything that extends the functions of a Mozilla application. Extensions comprise a subtype, albeit the most common and the most powerful one. Mozilla applications come with integrated add-on managers that, similar to package managers, install, update and manage extensions. The term, "Plug-in", however, strictly refers to NPAPI-based web content renderers. Plug-ins are deprecated.

Plug-ins appeared as early as the mid 1970s, when the EDT text editor running on the Unisys VS/9 operating system using the UNIVAC Series 90 mainframe computers provided the ability to run a program from the editor and to allow such a program to access the editor buffer, thus allowing an external program to access an edit session in memory. The plug-in program could make calls to the editor to have it perform text-editing services upon the buffer that the editor shared with the plug-in. The Waterloo Fortran compiler used this feature to allow interactive compilation of Fortran programs edited by EDT.

Very early PC software applications to incorporate plug-in functionality included HyperCard and QuarkXPress on the Macintosh, both released in 1987. In 1988, Silicon Beach Software included plug-in functionality in Digital Darkroom and SuperPaint, and Ed Bomke coined the term "plug-in".



</doc>
<doc id="23660" url="https://en.wikipedia.org/wiki?curid=23660" title="Pierre Teilhard de Chardin">
Pierre Teilhard de Chardin

Pierre Teilhard de Chardin ( (); 1 May 1881 – 10 April 1955) was a French idealist philosopher and Jesuit priest who trained as a paleontologist and geologist and took part in the discovery of the Peking Man. He conceived the vitalist idea of the Omega Point (a maximum level of complexity and consciousness towards which he believed the universe was evolving), and he developed Vladimir Vernadsky's concept of noosphere. Teilhard's ideas had a profound influence on the New Age movement.

In 1962, the Holy Office condemned several of Teilhard's works based on their alleged ambiguities and doctrinal errors. Some eminent Catholic figures, including Cardinal Ratzinger and Pope Francis, made positive comments on some of his ideas. The response to his writings by scientists has been mostly critical.

Pierre Teilhard de Chardin was born in the Château of Sarcenat, Orcines commune, some 4 km north-west of Clermont-Ferrand, Auvergne, France, on 1 May 1881, as the fourth of eleven children of librarian Emmanuel Teilhard de Chardin (1844–1932) and of Berthe-Adèle, née de Dompierre d'Hornoys of Picardy, a great-grandniece of Voltaire. 
He inherited the double surname from his father, who was descended on the "Teilhard" side from an ancient family of magistrates from Auvergne originating in Murat, Cantal, ennobled under Louis XVIII.

His father, an amateur naturalist, collected stones, insects and plants and promoted the observation of nature in the household. Pierre Teilhard's spirituality was awakened by his mother. 
When he was 12, he went to the Jesuit college of Mongré, in Villefranche-sur-Saône, where he completed baccalaureates of philosophy and mathematics. Then, in 1899, he entered the Jesuit novitiate at Aix-en-Provence, where he began a philosophical, theological and spiritual career.

When the Associations Bill of 1901 required congregational associations to submit their properties to state control, some of the Jesuits exiled themselves in the United Kingdom. Young Jesuit students continued their studies in Jersey. In the meantime, Teilhard earned a licentiate in literature in Caen in 1902.

From 1905 to 1908, he taught physics and chemistry in Cairo, Egypt, at the Jesuit College of the Holy Family. He wrote "... it is the dazzling of the East foreseen and drunk greedily ... in its lights, its vegetation, its fauna and its deserts."

Teilhard studied theology in Hastings, in Sussex, from 1908 to 1912. There he synthesized his scientific, philosophical and theological knowledge in the light of evolution. At that time he read "L'Évolution Créatrice" (The Creative Evolution) by Henri Bergson, about which he wrote that "the only effect that brilliant book had upon me was to provide fuel at just the right moment, and very briefly, for a fire that was already consuming my heart and mind." In short, Bergson's ideas helped him to unify his views on matter, life, and energy into a coherent and organic whole.

From 1912 to 1914, Teilhard worked in the paleontology laboratory of the Museum National d'Histoire Naturelle in Paris, studying the mammals of the middle tertiary period. Later he studied elsewhere in Europe. In June 1912 he formed part of the original digging team, with Arthur Smith Woodward and Charles Dawson, at the Piltdown site, after the discovery of the first fragments of the (fraudulent) "Piltdown Man". Some have suggested he participated in the hoax. Marcellin Boule, a specialist in Neanderthal studies, who as early as 1915 had recognized the non-hominid origins of the Piltdown finds, gradually guided Teilhard towards human paleontology. At the museum's Institute of Human Paleontology, he became a friend of Henri Breuil and in 1913 took part with him in excavations at the prehistoric painted Caves of Castillo in northwest Spain.

Mobilized in December 1914, Teilhard served in World War I as a stretcher-bearer in the 8th Moroccan Rifles. For his valor, he received several citations, including the Médaille militaire and the Legion of Honor.

During the war, he developed his reflections in his diaries and in letters to his cousin, Marguerite Teillard-Chambon, who later published a collection of them. He later wrote: "...the war was a meeting ... with the Absolute." In 1916, he wrote his first essay: "La Vie Cosmique" ("Cosmic life"), where his scientific and philosophical thought was revealed just as his mystical life. While on leave from the military he pronounced his solemn vows as a Jesuit in Sainte-Foy-lès-Lyon on 26 May 1918. In August 1919, in Jersey, he wrote "Puissance spirituelle de la Matière" ("The Spiritual Power of Matter").

At the University of Paris, Teilhard pursued three unit degrees of natural science: geology, botany, and zoology. His thesis treated the mammals of the French lower Eocene and their stratigraphy. After 1920, he lectured in geology at the Catholic Institute of Paris and after earning a science doctorate in 1922 became an assistant professor there.

In 1923 he traveled to China with Father Emile Licent, who was in charge of a significant laboratory collaboration between the Natural History Museum in Paris and Marcellin Boule's laboratory in Tientsin. Licent carried out considerable basic work in connection with missionaries who accumulated observations of a scientific nature in their spare time.

Teilhard wrote several essays, including "La Messe sur le Monde" (the "Mass on the World"), in the Ordos Desert. In the following year, he continued lecturing at the Catholic Institute and participated in a cycle of conferences for the students of the Engineers' Schools. Two theological essays on Original Sin were sent to a theologian at his request on a purely personal basis:
The Church required him to give up his lecturing at the Catholic Institute in order to continue his geological research in China.

Teilhard traveled again to China in April 1926. He would remain there for about twenty years, with many voyages throughout the world. He settled until 1932 in Tientsin with Emile Licent, then in Beijing. Teilhard made five geological research expeditions in China between 1926 and 1935. 
They enabled him to establish a general geological map of China. That same year, Teilhard's superiors in the Jesuit Order forbade him to teach any longer.

In 1926–27, after a missed campaign in Gansu, Teilhard traveled in the Sang-Kan-Ho valley near Kalgan (Zhangjiakou) and made a tour in Eastern Mongolia. He wrote "Le Milieu Divin" ("The Divine Milieu"). Teilhard prepared the first pages of his main work "Le Phénomène Humain" ("The Phenomenon of Man"). The Holy See refused the Imprimatur for "Le Milieu Divin" in 1927. 

He joined the ongoing excavations of the Peking Man Site at Zhoukoudian as an advisor in 1926 and continued in the role for the Cenozoic Research Laboratory of the Geological Survey of China following its founding in 1928. Teilhard resided in Manchuria with Emile Licent, staying in Western Shansi (Shanxi) and northern Shensi (Shaanxi) with the Chinese paleontologist C. C. Young and with Davidson Black, Chairman of the Geological Survey of China.

After a tour in Manchuria in the area of Great Khingan with Chinese geologists, Teilhard joined the team of American Expedition Center-Asia in the Gobi Desert, organized in June and July by the American Museum of Natural History with Roy Chapman Andrews. Henri Breuil and Teilhard discovered that the Peking Man, the nearest relative of "Pithecanthropus" from Java, was a "faber" (worker of stones and controller of fire). Teilhard wrote "L'Esprit de la Terre" ("The Spirit of the Earth").

Teilhard took part as a scientist in the Croisière Jaune (Yellow Cruise) financed by André Citroën in Central Asia. Northwest of Beijing in Kalgan, he joined the Chinese group who joined the second part of the team, the Pamir group, in Aksu. He remained with his colleagues for several months in Ürümqi, capital of Sinkiang. The following year the Sino-Japanese War (1937–1945) began.

In 1933, Rome ordered him to give up his post in Paris. Teilhard subsequently undertook several explorations in the south of China. He traveled in the valleys of Yangtze River and Sichuan in 1934, then, the following year, in Kwang-If and Guangdong. The relationship with Marcellin Boule was disrupted; the museum cut its financing on the grounds that Teilhard worked more for the Chinese Geological Service than for the museum.

During all these years, Teilhard contributed considerably to the constitution of an international network of research in human paleontology related to the whole of eastern and southeastern Asia. He would be particularly associated in this task with two friends, the English/Canadian Davidson Black and the Scot George Brown Barbour. Often he would visit France or the United States, only to leave these countries for further expeditions.

From 1927 to 1928, Teilhard based himself in Paris. He journeyed to Leuven, Belgium, and to Cantal and Ariège, France. Between several articles in reviews, he met new people such as Paul Valéry and Bruno de Solages, who were to help him in issues with the Catholic Church.

Answering an invitation from Henry de Monfreid, Teilhard undertook a journey of two months in Obock, in Harrar and in Somalia with his colleague Pierre Lamarre, a geologist, before embarking in Djibouti to return to Tianjin. While in China, Teilhard developed a deep and personal friendship with Lucile Swan.

During 1930–1931, Teilhard stayed in France and in the United States. During a conference in Paris, Teilhard stated: "For the observers of the Future, the greatest event will be the sudden appearance of a collective humane conscience and a human work to make." From 1932–1933, he began to meet people to clarify issues with the Congregation for the Doctrine of the Faith, regarding "Le Milieu divin" and "L'Esprit de la Terre". He met Helmut de Terra, a German geologist in the International Geology Congress in Washington, DC.

Teilhard participated in the 1935 Yale–Cambridge expedition in northern and central India with the geologist Helmut de Terra and Patterson, who verified their assumptions on Indian Paleolithic civilisations in Kashmir and the Salt Range Valley. He then made a short stay in Java, on the invitation of Dutch paleontologist Ralph von Koenigswald to the site of Java man. A second cranium, more complete, was discovered. Professor von Koenigswald had also found a tooth in a Chinese apothecary shop in 1934 that he believed belonged to a three-meter-tall ape, "Gigantopithecus," which lived between one hundred thousand and around a million years ago. Fossilized teeth and bone ("dragon bones") are often ground into powder and used in some branches of traditional Chinese medicine.

In 1937, Teilhard wrote "Le Phénomène spirituel" ("The Phenomenon of the Spirit") on board the boat Empress of Japan, where he met the Raja of Sarawak. The ship conveyed him to the United States. He received the Mendel Medal granted by Villanova University during the Congress of Philadelphia, in recognition of his works on human paleontology. He made a speech about evolution, the origins and the destiny of man. "The New York Times" dated 19 March 1937 presented Teilhard as the Jesuit who held that man descended from monkeys. Some days later, he was to be granted the "Doctor Honoris Causa" distinction from Boston College. Upon arrival in that city, he was told that the award had been cancelled.

Rome banned his work "L’Énergie Humaine" in 1939. By this point Teilhard was based again in France, where he was immobilized by malaria. During his return voyage to Beijing he wrote "L'Energie spirituelle de la Souffrance" ("Spiritual Energy of Suffering") (Complete Works, tome VII).

In 1941, Teilhard submitted to Rome his most important work, "Le Phénomène Humain". By 1947, Rome forbade him to write or teach on philosophical subjects. The next year, Teilhard was called to Rome by the Superior General of the Jesuits who hoped to acquire permission from the Holy See for the publication of "Le Phénomène Humain". However, the prohibition to publish it that was previously issued in 1944 was again renewed. Teilhard was also forbidden to take a teaching post in the Collège de France. Another setback came in 1949, when permission to publish "Le Groupe Zoologique" was refused.

Teilhard was nominated to the French Academy of Sciences in 1950. He was forbidden by his Superiors to attend the International Congress of
Paleontology in 1955. The Supreme Authority of the Holy Office, in a decree dated 15 November 1957, forbade the works of de Chardin to be retained in libraries, including those of religious institutes. His books were not to be sold in Catholic bookshops and were not to be translated into other languages.

Further resistance to Teilhard's work arose elsewhere. In April 1958, all Jesuit publications in Spain ("Razón y Fe", "Sal Terrae","Estudios de Deusto", etc.) carried a notice from the Spanish Provincial of the Jesuits that Teilhard's works had been published in Spanish without previous ecclesiastical examination and in defiance of the decrees of the Holy See. A decree of the Holy Office dated 30 June 1962, under the authority of Pope John XXIII, warned that "... it is obvious that in philosophical and theological matters, the said works [Teilhard's] are replete with ambiguities or rather with serious errors which offend Catholic doctrine. That is why... the Rev. Fathers of the Holy Office urge all Ordinaries, Superiors, and Rectors... to effectively protect, especially the minds of the young, against the dangers of the works of Fr. Teilhard de Chardin and his followers" (AAS, 6 August 1962).

The Diocese of Rome on 30 September 1963 required Catholic booksellers in Rome to withdraw his works as well as those that supported his views.

Teilhard died in New York City, where he was in residence at the Jesuit Church of St. Ignatius Loyola, Park Avenue. On 15 March 1955, at the house of his diplomat cousin Jean de Lagarde, Teilhard told friends he hoped he would die on Easter Sunday. On the evening of Easter Sunday, 10 April 1955, during an animated discussion at the apartment of Rhoda de Terra, his personal assistant since 1949, Teilhard suffered a heart attack and died. He was buried in the cemetery for the New York Province of the Jesuits at the Jesuit novitiate, St. Andrew-on-Hudson, in Hyde Park, New York. With the moving of the novitiate, the property was sold to the Culinary Institute of America in 1970.

Teilhard de Chardin wrote two comprehensive works, "The Phenomenon of Man" and "The Divine Milieu".

His posthumously published book, "The Phenomenon of Man", set forth a sweeping account of the unfolding of the cosmos and the evolution of matter to humanity, to ultimately a reunion with Christ. In the book, Teilhard abandoned literal interpretations of creation in the Book of Genesis in favor of allegorical and theological interpretations. The unfolding of the material cosmos is described from primordial particles to the development of life, human beings and the noosphere, and finally to his vision of the Omega Point in the future, which is "pulling" all creation towards it. He was a leading proponent of orthogenesis, the idea that evolution occurs in a directional, goal-driven way. Teilhard argued in Darwinian terms with respect to biology, and supported the synthetic model of evolution, but argued in Lamarckian terms for the development of culture, primarily through the vehicle of education. Teilhard made a total commitment to the evolutionary process in the 1920s as the core of his spirituality, at a time when other religious thinkers felt evolutionary thinking challenged the structure of conventional Christian faith. He committed himself to what the evidence showed.

Teilhard made sense of the universe by assuming it had a vitalist evolutionary process. He interprets complexity as the axis of evolution of matter into a geosphere, a biosphere, into consciousness (in man), and then to supreme consciousness (the Omega Point).

Teilhard's unique relationship to both paleontology and Catholicism allowed him to develop a highly progressive, cosmic theology which takes into account his evolutionary studies. Teilhard recognized the importance of bringing the Church into the modern world, and approached evolution as a way of providing ontological meaning for Christianity, particularly creation theology. For Teilhard, evolution was "the natural landscape where the history of salvation is situated."

Teilhard's cosmic theology is largely predicated on his interpretation of Pauline scripture, particularly Colossians 1:15-17 (especially verse 1:17b) and 1 Corinthians 15:28. He drew on the Christocentrism of these two Pauline passages to construct a cosmic theology which recognizes the absolute primacy of Christ. He understood creation to be "a teleological process towards union with the Godhead, effected through the incarnation and redemption of Christ, 'in whom all things hold together' (Col. 1:17)." He further posited that creation would not be complete until each "participated being is totally united with God through Christ in the Pleroma, when God will be 'all in all' (1Cor. 15:28)." 
Teilhard's life work was predicated on his conviction that human spiritual development is moved by the same universal laws as material development. He wrote, "...everything is the sum of the past" and "...nothing is comprehensible except through its history. 'Nature' is the equivalent of 'becoming', self-creation: this is the view to which experience irresistibly leads us. ... There is nothing, not even the human soul, the highest spiritual manifestation we know of, that does not come within this universal law." "The Phenomenon of Man" represents Teilhard's attempt at reconciling his religious faith with his academic interests as a paleontologist. One particularly poignant observation in Teilhard's book entails the notion that evolution is becoming an increasingly optional process. Teilhard points to the societal problems of isolation and marginalization as huge inhibitors of evolution, especially since evolution requires a unification of consciousness. He states that "no evolutionary future awaits anyone except in association with everyone else." Teilhard argued that the human condition necessarily leads to the psychic unity of humankind, though he stressed that this unity can only be voluntary; this voluntary psychic unity he termed "unanimization." Teilhard also states that "evolution is an ascent toward consciousness", giving encephalization as an example of early stages, and therefore, signifies a continuous upsurge toward the Omega Point which, for all intents and purposes, is God.

Teilhard also used his perceived correlation between spiritual and material to describe Christ, arguing that Christ not only has a mystical dimension but also takes on a physical dimension as he becomes the organizing principle of the universe—that is, the one who "holds together" the universe (Col. 1:17b). For Teilhard, Christ forms not only the eschatological end toward which his mystical/ecclesial body is oriented, but he also "operates physically in order to regulate all things" becoming "the one from whom all creation receives its stability." In other words, as the one who holds all things together, "Christ exercises a supremacy over the universe which is physical, not simply juridical. He is the unifying center of the universe and its goal. The function of holding all things together indicates that Christ is not only man and God; he also possesses a third aspect—indeed, a third nature—which is cosmic." In this way, the Pauline description of the Body of Christ is not simply a mystical or ecclesial concept for Teilhard; it is cosmic. This cosmic Body of Christ "extend[s] throughout the universe and compris[es] all things that attain their fulfillment in Christ [so that] ... the Body of Christ is the one single thing that is being made in creation." Teilhard describes this cosmic amassing of Christ as "Christogenesis." According to Teilhard, the universe is engaged in Christogenesis as it evolves toward its full realization at Omega, a point which coincides with the fully realized Christ. It is at this point that God will be "all in all" (1Cor. 15:28c).

In 1925, Teilhard was ordered by the Jesuit Superior General Wlodimir Ledóchowski to leave his teaching position in France and to sign a statement withdrawing his controversial statements regarding the doctrine of original sin. Rather than leave the Society of Jesus, Teilhard signed the statement and left for China.

This was the first of a series of condemnations by certain ecclesiastical officials that would continue until after Teilhard's death. The climax of these condemnations was a 1962 "monitum" (warning) of the Holy Office cautioning on Teilhard's works. It said:

The Holy Office did not place any of Teilhard's writings on the "Index Librorum Prohibitorum" (Index of Forbidden Books), which existed during Teilhard's lifetime and at the time of the 1962 decree.

Shortly thereafter, prominent clerics mounted a strong theological defense of Teilhard's works. Henri de Lubac (later a Cardinal) wrote three comprehensive books on the theology of Teilhard de Chardin in the 1960s. While de Lubac mentioned that Teilhard was less than precise in some of his concepts, he affirmed the orthodoxy of Teilhard de Chardin and responded to Teilhard's critics: "We need not concern ourselves with a number of detractors of Teilhard, in whom emotion has blunted intelligence". Later that decade Joseph Ratzinger, a German theologian who became Pope Benedict XVI, spoke glowingly of Teilhard's Christology in Ratzinger's "Introduction to Christianity":

Over the next several decades prominent theologians and prelates, including leading cardinals all wrote approvingly of Teilhard's ideas. In 1981, Cardinal Agostino Casaroli, wrote on the front page of the Vatican newspaper, "l'Osservatore Romano":
On 20 July 1981, the Holy See stated that, after consultation of Cardinal Casaroli and Cardinal Franjo Šeper, the letter did not change the position of the warning issued by the Holy Office on 30 June 1962, which pointed out that Teilhard's work contained ambiguities and grave doctrinal errors.

Cardinal Ratzinger in his book "The Spirit of the Liturgy" incorporates Teilhard's vision as a touchstone of the Catholic Mass:

Cardinal Avery Dulles said in 2004:
Cardinal Christoph Schönborn wrote in 2007:
In July 2009, Vatican spokesman Federico Lombardi said, "By now, no one would dream of saying that [Teilhard] is a heterodox author who shouldn't be studied."

Pope Francis refers to Teilhard's eschatological contribution in his encyclical Laudato si'.

The philosopher Dietrich von Hildebrand criticized severely the work of Teilhard. According to this philosopher, in a conversation after a lecture by Teilhard: "He (Teilhard) ignored completely the decisive difference between nature and supernature. After a lively discussion in which I ventured a criticism of his ideas, I had an opportunity to speak to Teilhard privately. When our talk touched on St. Augustine, he exclaimed violently: 'Don’t mention that unfortunate man; he spoiled everything by introducing the supernatural.'" Von Hildebrand writes that Teilhardism is incompatible with Christianity, substitutes efficiency for sanctity, dehumanizes man, and describes love as merely cosmic energy.

According to Daniel Dennett (1995), "it has become clear to the point of unanimity among scientists that Teilhard offered nothing serious in the way of an alternative to orthodoxy; the ideas that were peculiarly his were confused, and the rest was just bombastic redescription of orthodoxy." 
Steven Rose wrote that "Teilhard is revered as a mystic of genius by some, but amongst most biologists is seen as little more than a charlatan."

In 1961, British immunologist and Nobel laureate Peter Medawar wrote a scornful review of "The Phenomenon Of Man" for the journal "Mind": "the greater part of it, I shall show, is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself"; "In spite of all the obstacles that Teilhard perhaps wisely puts in our way, it is possible to discern a train of thought in "The Phenomenon of Man"". Evolutionary biologist Richard Dawkins called Medawar's review "devastating" and "The Phenomenon of Man" "the quintessence of bad poetic science".

Sir Julian Huxley, the evolutionary biologist, in the preface to the 1955 edition of "The Phenomenon of Man", praised the thought of Teilhard de Chardin for looking at the way in which human development needs to be examined within a larger integrated universal sense of evolution, though admitting he could not follow Teilhard all the way. 
Theodosius Dobzhansky, writing in 1973, drew upon Teilhard's insistence that evolutionary theory provides the core of how man understands his relationship to nature, calling him "one of the great thinkers of our age".

George Gaylord Simpson felt that if Teilhard were right, the lifework "of Huxley, Dobzhansky, and hundreds of others was not only wrong, but meaningless", and was mystified by their public support for him. He considered Teilhard a friend and his work in paleontology extensive and important, but expressed strongly adverse views of his contributions as scientific theorist and philosopher.

In a 2012 interview, evolutionary biologist David Sloan Wilson judged that Teilhard remains "amazingly relevant" even though he has been "largely forgotten as a scientist", and that Teilhard had anticipated Wilson's own work in multilevel selection theory. In his 2019 "This View of Life: Completing the Darwinian Revolution", Wilson praised Teilhard's book "The Phenomenon of Man" as "scientifically prophetic in many ways", and considers his own work as an updated version of it, commenting that:
Modern evolutionary theory shows that what Teilhard meant by the Omega Point is achievable in the foreseeable future.

Brian Swimme wrote "Teilhard was one of the first scientists to realize that the human and the universe are inseparable. The only universe we know about is a universe that brought forth the human."

Pierre Teilhard de Chardin is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on 10 April. George Gaylord Simpson named the most primitive and ancient genus of true primate, the Eocene genus "Teilhardina".

Teilhard and his work continue to influence the arts and culture. Characters based on Teilhard appear in several novels, including Jean Telemond in Morris West's "The Shoes of the Fisherman" (mentioned by name and quoted by Oskar Werner playing Fr. Telemond in the movie version of the novel). In Dan Simmons' 1989–97 "Hyperion Cantos", Teilhard de Chardin has been canonized a saint in the far future. His work inspires the anthropologist priest character, Paul Duré. When Duré becomes Pope, he takes "Teilhard I" as his regnal name. Teilhard appears as a minor character in the play "Fake" by Eric Simonson, staged by Chicago's Steppenwolf Theatre Company in 2009, involving a fictional solution to the infamous Piltdown Man hoax.

References range from occasional quotations—an auto mechanic quotes Teilhard in Philip K. Dick's "A Scanner Darkly"—to serving as the philosophical underpinning of the plot, as Teilhard's work does in Julian May's 1987–94 Galactic Milieu Series. Teilhard also plays a major role in Annie Dillard's 1999 "For the Time Being". Teilhard is mentioned by name and the Omega Point briefly explained in Arthur C. Clarke's and Stephen Baxter's "The Light of Other Days".
The title of the short-story collection "Everything That Rises Must Converge" by Flannery O'Connor is a reference to Teilhard's work. The American novelist Don DeLillo's 2010 novel "Point Omega" borrows its title and some of its ideas from Teilhard de Chardin. Robert Wright, in his book "", compares his own naturalistic thesis that biological and cultural evolution are directional and, possibly, purposeful, with Teilhard's ideas.

Teilhard's work also inspired philosophical ruminations by Italian laureate architect Paolo Soleri, artworks such as French painter Alfred Manessier's "L'Offrande de la terre ou Hommage à Teilhard de Chardin" and American sculptor Frederick Hart's acrylic sculpture "The Divine Milieu: Homage to Teilhard de Chardin". A sculpture of the Omega Point by Henry Setter, with a quote from Teilhard de Chardin, can be found at the entrance to the Roesch Library at the University of Dayton. The Spanish painter Salvador Dali was fascinated by Teilhard de Chardin and the Omega Point theory. His 1959 painting The Ecumenical Council (painting) is said to represent the "interconnectedness" of the Omega Point.

Edmund Rubbra's 1968 Symphony No. 8 is titled "Hommage à Teilhard de Chardin".

"The Embracing Universe" an oratorio for choir and 7 instruments composed by Justin Grounds to a libretto by Fred LaHaye saw its first performance in 2019. It is based on the life and thought of Teilhard de Chardin.

Several college campuses honor Teilhard. A building at the University of Manchester is named after him, as are residence dormitories at Gonzaga University and Seattle University.

"The De Chardin Project", a play celebrating Teilhard's life, ran from 20 November to 14 December 2014 in Toronto, Canada. "The Evolution of Teilhard de Chardin", a documentary film on Teilhard's life, was scheduled for release in 2015.

Founded in 1978, George Addair based much of Omega Vector on Teilhard's work.

The American physicist Frank J. Tipler has further developed Teilhard's Omega Point concept in two controversial books, The Physics of Immortality and the more theologically based Physics of Christianity. While keeping the central premise of Teilhard's Omega Point (i.e. a universe evolving towards a maximum state of complexity and consciousness) Tipler has supplanted some of the more mystical/ theological elements of the OPT with his own scientific and mathematical observations (as well as some elements borrowed from Freeman Dyson's eternal intelligence theory).

In 1972, the Uruguayan priest Juan Luis Segundo, in his five-volume series "A Theology for Artisans of a New Humanity," wrote that Teilhard "noticed the profound analogies existing between the conceptual elements used by the natural sciences — all of them being based on the hypothesis of a general evolution of the universe."

Teilhard has had a profound influence on the New Age movement and has been described as "perhaps the man most responsible for the spiritualization of evolution in a global and cosmic context".

Teilhard’s quote on likening the discovery of the power of love to the second time man will have discovered the power of fire was quoted in the sermon of the Most Reverend Michael Curry, Presiding Bishop of the Episcopal Church, during the wedding of Prince Harry and Meghan Markle on 20 May 2018.

The dates in parentheses are the dates of first publication in French and English. Most of these works were written years earlier, but Teilhard's ecclesiastical order forbade him to publish them because of their controversial nature. The essay collections are organized by subject rather than date, thus each one typically spans many years.






</doc>
<doc id="23661" url="https://en.wikipedia.org/wiki?curid=23661" title="Phutball">
Phutball

Phutball (short for Philosopher's Football) is a two-player abstract strategy board game described in Elwyn Berlekamp, John Horton Conway, and Richard K. Guy's "Winning Ways for your Mathematical Plays".

Phutball is played on the intersections of a 19×15 grid using one white stone and as many black stones as needed. 
In this article the two players are named Ohs (O) and Eks (X).
The board is labeled A through P (omitting I) from left to right and 1 to 19 from bottom to top from Ohs' perspective. 
Rows 0 and 20 represent "off the board" beyond rows 1 and 19 respectively.

As specialized phutball boards are hard to come by, the game is usually played on a 19×19 Go board, with a white stone representing the football and black stones representing the men.

The objective is to score goals by using the men (the black stones) to move the football (the white stone) onto or over the opponent's goal line. Ohs tries to move the football to rows 19 or 20 and Eks to rows 1 or 0. 
At the start of the game the football is placed on the central point, unless one player gives the other a handicap, in which case the ball starts nearer one player's goal.

Players alternate making moves. 
A move is either to add a man to any vacant point on the board or to move the ball. 
There is no difference between men played by Ohs and those played by Eks.

The football is moved by a series of jumps over adjacent men. 
Each jump is to the first vacant point in a straight line horizontally, vertically, or diagonally over one or more men. 
The jumped men are then removed from the board (before any subsequent jump occurs). 
This process repeats for as long as there remain men available to be jumped and the player desires. Jumping is optional: there is no requirement to jump. 
In contrast to checkers, multiple men in a row are jumped and removed as a group.

The diagram on the right illustrates a jump. 

If the football ends the move on or over the opponent's goal line then a goal has been scored. 
If the football passes through a goal line, but ends up elsewhere due to further jumps, the game continues.

The game is sufficiently complex that checking whether there is a win in one (on an m×n board) is NP-complete. It is not known whether any player has a winning strategy or both players have a drawing strategy.
Given an arbitrary board position, with initially a black stone placed in the center, determining whether the current player has a winning strategy is PSPACE-hard.




</doc>
<doc id="23664" url="https://en.wikipedia.org/wiki?curid=23664" title="Papyrus">
Papyrus

Papyrus ( ) is a material similar to thick paper that was used in ancient times as a writing surface. It was made from the pith of the papyrus plant, "Cyperus papyrus", a wetland sedge. "Papyrus" (plural: "papyri") can also refer to a document written on sheets of such material, joined together side by side and rolled up into a scroll, an early form of a book. 
Papyrus is first known to have been used in Egypt (at least as far back as the First Dynasty), as the papyrus plant was once abundant across the Nile Delta. It was also used throughout the Mediterranean region and in the Kingdom of Kush. Apart from a writing material, ancient Egyptians employed papyrus in the construction of other artifacts, such as reed boats, mats, rope, sandals, and baskets.

Papyrus was first manufactured in Egypt as far back as the fourth millennium BCE. The earliest archaeological evidence of papyrus was excavated in 2012 and 2013 at Wadi al-Jarf, an ancient Egyptian harbor located on the Red Sea coast. These documents, the Diary of Merer, date from c. 2560–2550 BCE (end of the reign of Khufu). The papyrus rolls describe the last years of building the Great Pyramid of Giza. In the first centuries BCE and CE, papyrus scrolls gained a rival as a writing surface in the form of parchment, which was prepared from animal skins. Sheets of parchment were folded to form quires from which book-form codices were fashioned. Early Christian writers soon adopted the codex form, and in the Græco-Roman world, it became common to cut sheets from papyrus rolls to form codices.

Codices were an improvement on the papyrus scroll, as the papyrus was not pliable enough to fold without cracking and a long roll, or scroll, was required to create large-volume texts. Papyrus had the advantage of being relatively cheap and easy to produce, but it was fragile and susceptible to both moisture and excessive dryness. Unless the papyrus was of perfect quality, the writing surface was irregular, and the range of media that could be used was also limited.

Papyrus was replaced in Europe by the cheaper, locally produced products parchment and vellum, of significantly higher durability in moist climates, though Henri Pirenne's connection of its disappearance with the Muslim conquest of Egypt is contested. Its last appearance in the Merovingian chancery is with a document of 692, though it was known in Gaul until the middle of the following century. The latest certain dates for the use of papyrus are 1057 for a papal decree (typically conservative, all papal bulls were on papyrus until 1022), under Pope Victor II, and 1087 for an Arabic document. Its use in Egypt continued until it was replaced by more inexpensive paper introduced by the Islamic world who originally learned of it from the Chinese. By the 12th century, parchment and paper were in use in the Byzantine Empire, but papyrus was still an option.

Papyrus was made in several qualities and prices. Pliny the Elder and Isidore of Seville described six variations of papyrus which were sold in the Roman market of the day. These were graded by quality based on how fine, firm, white, and smooth the writing surface was. Grades ranged from the superfine Augustan, which was produced in sheets of 13 digits (10 inches) wide, to the least expensive and most coarse, measuring six digits (four inches) wide. Materials deemed unusable for writing or less than six digits were considered commercial quality and were pasted edge to edge to be used only for wrapping.

Until the middle of the 19th century, only some isolated documents written on papyrus were known, and museums simply displayed them as curiosities. They did not contain literary works. The first modern discovery of papyri rolls was made at Herculaneum in 1752. Until then, the only papyri known had been a few surviving from medieval times. Scholarly investigations began with the Dutch historian Caspar Jacob Christiaan Reuvens (1793–1835). He wrote about the content of the Leyden papyrus, published in 1830. The first publication has been credited to the British scholar Charles Wycliffe Goodwin (1817–1878), who published for the Cambridge Antiquarian Society, one of the Papyri Graecae Magicae V, translated into English with commentary in 1853.

The English word "papyrus" derives, via Latin, from Greek πάπυρος ("papyros"), a loanword of unknown (perhaps Pre-Greek) origin. Greek has a second word for it, βύβλος ("byblos"), said to derive from the name of the Phoenician city of Byblos. The Greek writer Theophrastus, who flourished during the 4th century BCE, uses "papyros" when referring to the plant used as a foodstuff and "byblos" for the same plant when used for nonfood products, such as cordage, basketry, or writing surfaces. The more specific term βίβλος "biblos", which finds its way into English in such words as 'bibliography', 'bibliophile', and 'bible', refers to the inner bark of the papyrus plant. "Papyrus" is also the etymon of 'paper', a similar substance.

In the Egyptian language, papyrus was called "wadj" ("w3ḏ"), "tjufy" ("ṯwfy"), or "djet" ("ḏt").

The word for the material papyrus is also used to designate documents written on sheets of it, often rolled up into scrolls. The plural for such documents is papyri. Historical papyri are given identifying names — generally the name of the discoverer, first owner or institution where they are kept—and numbered, such as "Papyrus Harris I". Often an abbreviated form is used, such as "pHarris I". These documents provide important information on ancient writings; they give us the only extant copy of Menander, the Egyptian Book of the Dead, Egyptian treatises on medicine (the Ebers Papyrus) and on surgery (the Edwin Smith papyrus), Egyptian mathematical treatises (the Rhind papyrus), and Egyptian folk tales (the Westcar papyrus). When, in the 18th century, a library of ancient papyri was found in Herculaneum, ripples of expectation spread among the learned men of the time. However, since these papyri were badly charred, their unscrolling and deciphering is still going on today.

Papyrus is made from the stem of the papyrus plant, "Cyperus papyrus". The outer rind is first removed, and the sticky fibrous inner pith is cut lengthwise into thin strips of about long. The strips are then placed side by side on a hard surface with their edges slightly overlapping, and then another layer of strips is laid on top at a right angle. The strips may have been soaked in water long enough for decomposition to begin, perhaps increasing adhesion, but this is not certain. The two layers possibly were glued together. While still moist, the two layers are hammered together, mashing the layers into a single sheet. The sheet is then dried under pressure. After drying, the sheet is polished with some rounded object, possibly a stone or seashell or round hardwood.

Sheets, or kollema, could be cut to fit the obligatory size or glued together to create a longer roll. The point where the kollema are joined with glue is called the kollesis. A wooden stick would be attached to the last sheet in a roll, making it easier to handle. To form the long strip scrolls required, a number of such sheets were united, placed so all the horizontal fibres parallel with the roll's length were on one side and all the vertical fibres on the other. Normally, texts were first written on the "recto", the lines following the fibres, parallel to the long edges of the scroll. Secondarily, papyrus was often reused, writing across the fibres on the "verso". Pliny the Elder describes the methods of preparing papyrus in his "Naturalis Historia".

In a dry climate, like that of Egypt, papyrus is stable, formed as it is of highly rot-resistant cellulose; but storage in humid conditions can result in molds attacking and destroying the material. Library papyrus rolls were stored in wooden boxes and chests made in the form of statues. Papyrus scrolls were organized according to subject or author, and identified with clay labels that specified their contents without having to unroll the scroll. In European conditions, papyrus seems to have lasted only a matter of decades; a 200-year-old papyrus was considered extraordinary. Imported papyrus once commonplace in Greece and Italy has since deteriorated beyond repair, but papyri are still being found in Egypt; extraordinary examples include the Elephantine papyri and the famous finds at Oxyrhynchus and Nag Hammadi. The Villa of the Papyri at Herculaneum, containing the library of Lucius Calpurnius Piso Caesoninus, Julius Caesar's father-in-law, was preserved by the eruption of Mount Vesuvius, but has only been partially excavated.

Sporadic attempts to revive the manufacture of papyrus have been made since the mid-18th century. Scottish explorer James Bruce experimented in the late 18th century with papyrus plants from the Sudan, for papyrus had become extinct in Egypt. Also in the 18th century, Sicilian Saverio Landolina manufactured papyrus at Syracuse, where papyrus plants had continued to grow in the wild. During the 1920s, when Egyptologist Battiscombe Gunn lived in Maadi, outside Cairo, he experimented with the manufacture of papyrus, growing the plant in his garden. He beat the sliced papyrus stalks between two layers of linen, and produced successful examples of papyrus, one of which was exhibited in the Egyptian Museum in Cairo. The modern technique of papyrus production used in Egypt for the tourist trade was developed in 1962 by the Egyptian engineer Hassan Ragab using plants that had been reintroduced into Egypt in 1872 from France. Both Sicily and Egypt have centres of limited papyrus production.

Papyrus is still used by communities living in the vicinity of swamps, to the extent that rural householders derive up to 75% of their income from swamp goods. Particularly in East and Central Africa, people harvest papyrus, which is used to manufacture items that are sold or used locally. Examples include baskets, hats, fish traps, trays or winnowing mats, and floor mats. Papyrus is also used to make roofs, ceilings, rope and fences. Although alternatives, such as eucalyptus, are increasingly available, papyrus is still used as fuel.



Other ancient writing materials:




</doc>
<doc id="23665" url="https://en.wikipedia.org/wiki?curid=23665" title="Pixel">
Pixel

In digital imaging, a pixel, pel, or picture element is a physical point in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color imaging systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.

In some contexts (such as descriptions of camera sensors), "pixel" refers to a single scalar element of a multi-component representation (called a "photosite" in the camera sensor context, although "sensel" is sometimes used), while in yet other contexts it may refer to the set of component intensities for a spatial position.

The word "pixel" is a portmanteau of "pix" (from "pictures", shortened to "pics") and "el" (for ""element""); similar formations with '"el"' include the words "voxel" and "texel". The word "pix" appeared in "Variety" magazine headlines in 1932, as an abbreviation for the word "pictures", in reference to movies. By 1938, "pix" was being used in reference to still pictures by photojournalists.

The word "pixel" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of scanned images from space probes to the Moon and Mars. Billingsley had learned the word from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who in turn said he did not know where it originated. McFarland said simply it was "in use at the time" (circa 1963).

The concept of a "picture element" dates to the earliest days of television, for example as ""Bildpunkt"" (the German word for "pixel", literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term "picture element" itself was in "Wireless World" magazine in 1927, though it had been used earlier in various U.S. patents filed as early as 1911.

Some authors explain "pixel" as "picture cell," as early as 1972. In graphics and in image and video processing, "pel" is often used instead of "pixel". For example, IBM used it in their Technical Reference for the original PC.

Pixels, abbreviated as "px", are also a unit of measurement commonly used in graphic and web design, equivalent to roughly . This measurement is used to make sure a given element will display as the same size no matter what screen resolution views it.

Pixilation, spelled with a second "i", is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning "possession by spirits (pixies)", the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.

 thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be "printed pixels" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a digital camera (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. "Pixels" can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.

The measures dots per inch (dpi) and pixels per inch (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement. For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer. Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.

The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a "three-megapixel" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a "640 by 480 display", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display) and therefore has a total number of 640 × 480 = 307,200 pixels, or 0.3 megapixels.

The pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a "bitmapped image" or a "raster image". The word "raster" originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.

For convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.

For example:


Computers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. LCD monitors also use pixels to display an image, and have a native resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution. On some CRT monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported.
To produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.

The pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale "s" measured in radians is the ratio of the pixel spacing "p" and focal length "f" of the preceding optics, "s"="p/f". (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)
Because "p" is usually expressed in units of arcseconds per pixel, because 1 radian equals "180/π*3600≈206,265" arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as "s=206p/f".

The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:

For color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue each, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image).

Many display and image-acquisition systems are not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as subpixels. For example, LCDs typically divide each pixel vertically into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as "pixels", as they are the basic addressable elements in a viewpoint of hardware, and hence "pixel circuits" rather than "subpixel circuits" is used.

Most digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as "pixels" just like in the display industry, not "subpixels".

For systems with subpixels, two different approaches can be taken:

This latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.

The concept of subpixels is related to samples.

A megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048 × 1536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have "3.2 megapixels" or "3.4 megapixels", depending on whether the number reported is the "effective" or the "total" pixel count.

Digital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called "pixels", even though they only record 1 channel (only red or green or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called "N-megapixel" camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).

DxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens – as opposed to the MP a manufacturer states for a camera product, which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing up camera sharpness. As of mid-2013, the Sigma 35 mm f/1.4 DG HSM lens mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still wipes off more than one-third of the D800's 36.3 MP sensor. In August 2019, Xiaomi released Redmi Note 8 Pro as the world's first smartphone with 64 MP camera. On December 12, 2019 Samsung released Samsung A71 with also a 64 MP camera.

One new method to add megapixels has been introduced in a Micro Four Thirds System camera, which only uses a 16 MP sensor but can produce a 64 MP RAW (40 MP JPEG) image by making two exposures, shifting the sensor by a half pixel between them. Using a tripod to take level multi-shots within an instance, the multiple 16 MP images are then generated into a unified 64 MP image.



</doc>
<doc id="23666" url="https://en.wikipedia.org/wiki?curid=23666" title="Prime number">
Prime number

A prime number (or a prime) is a natural number greater than 1 that cannot be formed by multiplying two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, or , involve 5 itself.
However, 6 is composite because it is the product of two numbers () that are both smaller than 6. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.

The property of being prime is called primality. A simple but slow method of checking the primality of a given number formula_1, called trial division, tests whether formula_1 is a multiple of any integer between 2 and formula_3. Faster algorithms include the Miller–Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. the largest known prime number has 24,862,048 decimal digits.

There are infinitely many primes, as demonstrated by Euclid around 300 BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says that the probability of a randomly chosen number being prime is inversely proportional to its number of digits, that is, to its logarithm.

Several historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes having just one even number between them. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals.

A natural number (1, 2, 3, 4, 5, 6, etc.) is called a "prime number" (or a "prime") if it is greater than 1 and cannot be written as the product of two smaller natural numbers. The numbers greater than 1 that are not prime are called composite numbers. In other words, formula_1 is prime if formula_1 items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange formula_1 dots into a rectangular grid that is more than one dot wide and more than one dot high.
For example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers, as there are no other numbers that divide them evenly (without a remainder).
1 is not prime, as it is specifically excluded in the definition. and are both composite.
The divisors of a natural number formula_1 are the natural numbers that divide formula_1 evenly.
Every natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This idea leads to a different but equivalent definition of the primes: they are the numbers with exactly two positive divisors, 1 and the number itself.
Yet another way to express the same thing is that a number formula_1 is prime if it is greater than one and if none of the numbers formula_10 divides formula_1 evenly.

The first 25 prime numbers (all the prime numbers less than 100) are:

No even number formula_1 greater than 2 is prime because any such number can be expressed as the product formula_13. Therefore, every prime number other than 2 is an odd number, and is called an "odd prime". Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite:
decimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.

The set of all primes is sometimes denoted by formula_14 (a boldface capital "P") or by formula_15 (a blackboard bold capital P).

The Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers. However, the earliest surviving records of the explicit study of prime numbers come from ancient Greek mathematics. Euclid's "Elements" (c. 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime. Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes.

Around 1000 AD, the Islamic mathematician Ibn al-Haytham (Alhazen) found Wilson's theorem, characterizing the prime numbers as the numbers formula_1 that evenly divide formula_17. He also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it. Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by testing only the divisors up to the square root of the largest number to be tested. Fibonacci brought the innovations from Islamic mathematics back to Europe. His book "Liber Abaci" (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.

In 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also investigated the primality of the Fermat numbers
formula_18, and Marin Mersenne studied the Mersenne primes, prime numbers of the form formula_19 with formula_20 itself a prime. Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler. Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes. He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes formula_21.
At the start of the 19th century, Legendre and Gauss conjectured that as formula_22 tends to infinity, the number of primes up to formula_22 is asymptotic to formula_24, where formula_25 is the natural logarithm of formula_22. Ideas of Bernhard Riemann in his 1859 paper on the zeta-function sketched an outline for proving this. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem. Another important 19th century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.

Many mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include Pépin's test for Fermat numbers (1877), Proth's theorem (c. 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test.

Since 1951 all the largest known primes have been found using these tests on computers. The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects. The idea that prime numbers had few applications outside of pure mathematics was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.

The increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form. The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) that there are arbitrarily long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size.

Most early Greeks did not even consider 1 to be a number, so they could not consider its primality. A few mathematicians from this time also considered the prime numbers to be a subdivision of the odd numbers, so they also did not consider 2 to be prime. However, Euclid and a majority of the other Greek mathematicians considered 2 as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number.
By the Middle Ages and Renaissance mathematicians began treating 1 as a number, and some of them included it as the first prime number. In the mid-18th century Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler; however, Euler himself did not consider 1 to be prime. In the 19th century many mathematicians still considered 1 to be prime, and lists of primes that included 1 continued to be published as recently as 1956.

If the definition of a prime number were changed to call 1 a prime, many statements involving prime numbers would need to be reworded in a more awkward way. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with different numbers of copies of 1. Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number 1. Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for 1. By the early 20th century, mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a "unit".

Writing a number as a product of prime numbers is called a "prime factorization" of the number. For example:
The terms in the product are called "prime factors". The same prime factor may occur more than once; this example has two copies of the prime factor formula_28 When a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for example, in the second way of writing the product above, formula_29 denotes the square or second power of formula_28

The central importance of prime numbers to number theory and mathematics in general stems from the "fundamental theorem of arithmetic". This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly,
this product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes,
although their ordering may differ. So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the "basic building blocks" of the natural numbers.

Some proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If formula_20 is a prime number and formula_20 divides a product formula_33 of integers formula_34 and formula_35 then formula_20 divides formula_34 or formula_20 divides formula_39 (or both). Conversely, if a number formula_20 has the property that when it divides a product it always divides at least one factor of the product, then formula_20 must be prime.

There are infinitely many prime numbers. Another way of saying this is that the sequence
of prime numbers never ends. This statement is referred to as "Euclid's theorem" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.

Euclid's proof shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add formula_42 If the list consists of the primes formula_43 this gives the number
By the fundamental theorem, formula_45 has a prime factorization
with one or more prime factors. formula_45 is evenly divisible by each of these factors, but formula_45 has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of formula_45 can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.

The numbers formed by adding one to the products of the smallest primes are called Euclid numbers. The first five of them are prime, but the sixth,
is a composite number.

There is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes "only" prime values. However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once. There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its "positive" values are prime.

Other examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants formula_51 and formula_52 such that
are prime for any natural number formula_1 in the first formula, and any number of exponents in the second formula. Here formula_55 represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of formula_56 or formula_57

Many conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer formula_1 greater than 2 can be written as a sum of two primes. , this conjecture has been verified for all numbers up to formula_59 Weaker statements than this have been proven, for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime (the product of two primes). Also, any even integer greater than 10 can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.

Another type of problem concerns prime gaps, the differences between consecutive primes.
The existence of arbitrarily large prime gaps can be seen by noting that the sequence formula_60 consists of formula_61 composite numbers, for any natural number formula_62 However, large prime gaps occur much earlier than this argument shows. For example, the first prime gap of length 8 is between the primes 89 and 97, much smaller than formula_63 It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer formula_64 there are infinitely many pairs of consecutive primes that differ by formula_65
Andrica's conjecture, Brocard's conjecture, Legendre's conjecture, and Oppermann's conjecture all suggest that the largest gaps between primes from formula_66 to formula_1 should be at most approximately formula_68 a result that is known to follow from the Riemann hypothesis, while the much stronger Cramér conjecture sets the largest gap size at formula_69 Prime gaps can be generalized to prime formula_70-tuples, patterns in the differences between more than two prime numbers. Their infinitude and density are the subject of the first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem.

Analytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.

This area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.
The problem asked for the value of the infinite sum formula_71
which today can be recognized as the value formula_72 of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that formula_73.
The reciprocal of this number, formula_74, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).

The distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the formula_1-th prime is known.
Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials
with relatively prime integers formula_34 and formula_39 take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same formula_39 have approximately the same proportions of primes.
Although conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often.

Euler's proof that there are infinitely many primes considers the sums of reciprocals of primes,

Euler showed that, for any arbitrary real number formula_22, there exists a prime formula_20 for which this sum is bigger than formula_22. This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every formula_22.
The growth rate of this sum is described more precisely by Mertens' second theorem. For comparison, the sum

does not grow to infinity as formula_1 goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,
although both sets are infinite. Brun's theorem states that the sum of the reciprocals of twin primes,

is finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.

The prime counting function formula_88 is defined as the number of primes not greater than formula_1. For example, formula_90, since there are five primes less than or equal to 11. Methods such as the Meissel–Lehmer algorithm can compute exact values of formula_88 faster than it would be possible to list each prime up to formula_1. The prime number theorem states that formula_88 is asymptotic to formula_94, which is denoted as
and means that the ratio of formula_88 to the right-hand fraction approaches 1 as formula_1 grows to infinity. This implies that the likelihood that a randomly chosen number less than formula_1 is prime is (approximately) inversely proportional to the number of digits in formula_1.
It also implies that the formula_1th prime number is proportional to formula_101
and therefore that the average size of a prime gap is proportional to formula_102.
A more accurate estimate for formula_88 is given by the offset logarithmic integral

An arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference. This difference is called the modulus of the progression. For example,
is an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression
can have more than one prime only when its remainder formula_34 and modulus formula_107 are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.

The Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes.

Euler noted that the function
yields prime numbers for formula_109, although composite numbers appear among its later values. The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem. The Hardy-Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients
in terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.

The Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.

One of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function formula_110 are located.
This function is an analytic function on the complex numbers. For complex numbers formula_111 with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,
This equality between a sum and a product, discovered by Euler, is called an Euler product. The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.
It leads to another proof that there are infinitely many primes: if there were only finitely many,
then the sum-product equality would also be valid at formula_113, but the sum would diverge (it is the harmonic series formula_114) while the product would be finite, a contradiction.

The Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2. The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1, although other more elementary proofs have been found.
The prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term.
In this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the
asymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of formula_22 for intervals near a number formula_22).

Modular arithmetic modifies usual arithmetic by only using the numbers formula_117, for a natural number formula_1 called the modulus.
Any other natural number can be mapped into this system by replacing it by its remainder after division by formula_1.
Modular sums, differences and products are calculated by performing the same replacement by the remainder
on the result of the usual sum, difference, or product of integers. Equality of integers corresponds to "congruence" in modular arithmetic:
formula_22 and formula_121 are congruent (written formula_122 mod formula_1) when they have the same remainder after division by formula_1. However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number formula_125 as modulus, division by formula_126 is possible: formula_127, because clearing denominators by multiplying both sides by formula_126 gives the valid formula formula_129. However, with the composite modulus formula_130, division by formula_126 is impossible. There is no valid solution to formula_132: clearing denominators by multiplying by formula_126 causes the left-hand side to become formula_134 while the right-hand side becomes either formula_135 or formula_126.
In the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.

Several theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if
formula_137 (mod formula_20), then formula_139 (mod formula_20).
Summing this over all choices of formula_34 gives the equation
valid whenever formula_20 is prime.
Giuga's conjecture says that this equation is also a sufficient condition for formula_20 to be prime.
Wilson's theorem says that an integer formula_145 is prime if and only if the factorial formula_146 is congruent to formula_147 mod formula_20. For a composite this cannot hold, since one of its factors divides both and formula_149, and so formula_150 is impossible.

The formula_20-adic order formula_152 of an integer formula_1 is the number of copies of formula_20 in the prime factorization of formula_1. The same concept can be extended from integers to rational numbers by defining the formula_20-adic order of a fraction formula_157 to be formula_158. The formula_20-adic absolute value formula_160 of any rational number formula_107 is then defined as
formula_162. Multiplying an integer by its formula_20-adic absolute value cancels out the factors of formula_20 in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their formula_20-adic distance, the formula_20-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of formula_20. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the formula_20-adic distance can be extended to a different complete field, the formula_20-adic numbers.

This picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms), and places (extensions to complete fields in which the given field is a dense set, also called completions). The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and formula_20-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers. The local-global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory.

A commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, "prime elements" and "irreducible elements". An element formula_20 of a ring formula_172 is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever formula_20 divides the product formula_174 of two elements of formula_172, it also divides at least one of formula_22 or formula_121. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,
In an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.

The fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers formula_179, the ring of complex numbers of the form formula_180 where formula_181 denotes the imaginary unit and formula_34 and formula_39 are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes formula_184 and formula_185. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not. This is a consequence of Fermat's theorem on sums of two squares,
which states that an odd prime formula_20 is expressible as the sum of two squares, formula_187, and therefore factorizable as formula_188, exactly when formula_20 is 1 mod 4.

Not every ring is a unique factorization domain. For instance, in the ring of numbers formula_190 (for integers formula_34 and formula_39) the number formula_193 has two factorizations formula_194, where neither of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.
"Prime ideals", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), ... The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.

The spectrum of a ring is a geometric space whose points are the prime ideals of the ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers.
Early attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers.
The question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case.

In the theory of finite groups the Sylow theorems imply that, if a power of a prime number formula_195 divides the order of a group, then it has a subgroup of order formula_195. By Lagrange's theorem, any group of prime order is a cyclic group,
and by Burnside's theorem any group whose order is divisible by only two primes is solvable.

For a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics other than the use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.

This vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public key cryptography algorithms.
These applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime.
The most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms.
Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.

The most basic method of checking the primality of a given integer formula_1 is called "trial division". This method divides formula_1 by each integer from 2 up to the square root of formula_1. Any such integer dividing formula_1 evenly establishes formula_1 as composite; otherwise it is prime.
Integers larger than the square root do not need to be checked because, whenever formula_202, one of the two factors formula_34 and formula_39 is less than or equal to the square root of formula_1. Another optimization is to check only primes as factors in this range.
For instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to , which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.

Although this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers. However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter.

Before computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed. The oldest method for generating a list of primes is called the sieve of Eratosthenes. The animation shows an optimized variant of this method.
Another more asymptotically efficient sieving method for the same problem is the sieve of Atkin. In advanced mathematics, sieve theory applies similar methods to other problems.

Some of the fastest modern tests for whether an arbitrary given number formula_1 is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer.
For instance the Solovay–Strassen primality test on a given number formula_20 chooses a number formula_34 randomly from formula_134 through formula_210 and uses modular exponentiation to check
whether formula_211 is divisible by formula_20. If so, it answers yes and otherwise it answers no. If formula_20 really is prime, it will always answer yes, but if formula_20 is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2.
If this test is repeated formula_1 times on the same number,
the probability that a composite number could pass the test every time is at most formula_216. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.
A composite number that passes such a test is called a pseudoprime.

In contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite.
For instance, this is true of trial division.
The algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,
and randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.
When the elliptic curve method concludes that a number is prime, it provides primality certificate that can be verified quickly.
The elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice. These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime;
when doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.

The following table lists some of these tests. Their running time is given in terms of formula_1, the number to be tested and, for probabilistic algorithms, the number formula_70 of tests performed. Moreover, formula_219 is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters formula_1 and formula_70.

In addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly.
For example, the Lucas–Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically,
in the same time as a single iteration of the Miller–Rabin test. This is why since 1992 () the largest "known" prime has always been a Mersenne prime.
It is conjectured that there are infinitely many Mersenne primes.

The following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively.

Given a composite integer formula_1, the task of providing one (or all) prime factors is referred to as "factorization" of formula_1. It is significantly more difficult than primality testing, and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of formula_1, and elliptic curve factorization can be effective when formula_1 has factors of moderate size. Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve. the largest number known to have been factored by a general-purpose algorithm is RSA-240, which has 240 decimal digits (795 bits) and is the product of two large primes.

Shor's algorithm can factor any integer in a polynomial number of steps on a quantum computer. However, current technology can only run this algorithm for very small numbers. the largest number that has been factored by a quantum computer running Shor's algorithm is 21.

Several public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common). RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers formula_22 and formula_121 than to calculate formula_22 and formula_121 (assumed coprime) if only the product formula_174 is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing formula_231), while the reverse operation (the discrete logarithm) is thought to be a hard problem.

Prime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to formula_70-independent hashing by using higher-degree polynomials, again modulo large primes. As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.

Some checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits. Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than formula_233.
Prime numbers are also used in pseudorandom number generators including linear congruential generators and the Mersenne Twister.

Prime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area. Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.
The concept of prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, "prime" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. The prime decomposition of 3-manifolds is another example of this type.

Beyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas.

Fermat primes are primes of the form
with formula_70 a natural number. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers – 3, 5, 17, 257, and 65,537 – are prime, but formula_236 is composite and so are all other Fermat numbers that have been verified as of 2017. A regular formula_1-gon is constructible using straightedge and compass if and only if the odd prime factors of formula_1 (if any) are distinct Fermat primes. Likewise, a regular formula_1-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of formula_1 are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form formula_241.

It is possible to partition any convex polygon into formula_1 smaller convex polygons of equal area and equal perimeter, when formula_1 is a power of a prime number, but this is not known for other values of formula_1.

Beginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems. Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures.

The evolutionary strategy used by cicadas of the genus "Magicicada" makes use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles.
In contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations.

Prime numbers have influenced many artists and writers.
The French composer Olivier Messiaen used prime numbers to create ametrical music through "natural phenomena". In works such as "La Nativité du Seigneur" (1935) and "Quatre études de rythme" (1949–50), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, "Neumes rythmiques". According to Messiaen this way of composing was "inspired by the movements of nature, movements of free and unequal durations".

In his science fiction novel "Contact", scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel "The Curious Incident of the Dog in the Night-Time" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel "The Solitude of Prime Numbers", in which they are portrayed as "outsiders" among integers.




</doc>
<doc id="23669" url="https://en.wikipedia.org/wiki?curid=23669" title="Piers Anthony">
Piers Anthony

Piers Anthony Dillingham Jacob (born 6 August 1934 in Oxford, England) is an English-American author in the science fiction and fantasy genres, publishing under the name Piers Anthony. He is most famous for his set in the fictional realm of Xanth.

Many of his books have appeared on "The New York Times" Best Seller list. He has stated that one of his greatest achievements has been to publish a book beginning with every letter of the alphabet, from "Anthonology" to "Zombie Lover".

Anthony's parents, Alfred and Norma Jacob, were Quaker pacifists studying at Oxford University who interrupted their studies in 1936 to undertake relief work on behalf of the Quakers during the Spanish Civil War, establishing a food kitchen for children in Barcelona. Piers and his sister were left in England in the care of their maternal grandparents and a nanny. Alfred Jacob, although a British citizen, had been born in America near Philadelphia, and in 1940, after being forced out of Spain and with the situation in Britain deteriorating, the family sailed to the United States. In 1941 the family settled in a rustic "back to the land" utopian community near Winhall, Vermont, where a young Piers made the acquaintance of radical author Scott Nearing, a neighbor. Both parents resumed their academic studies, and Alfred eventually became a professor of Romance languages, teaching at a number of colleges in the Philadelphia area.

Piers was moved around to a number of schools, eventually enrolling in Goddard College in Vermont where he graduated in 1956. On "This American Life" on 27 July 2012, Anthony revealed that his parents had divorced, he was bullied, and he had poor grades in school. Anthony referred to his high school as a "very fancy private school", and refuses to donate money to the school because as a student, he recalls being part of "the lower crust", and that no one paid attention to or cared about him. He said, "I didn't like being a member of the underclass, of the peons like that". He became a naturalized U.S. citizen while serving in the United States Army in 1958. After completing a two-year stint in military service, he briefly taught school at Admiral Farragut Academy in St. Petersburg, Florida before becoming a full-time writer.

Anthony met his future wife, Carol Marble, while both were attending college. They were married in 1956, the same year he graduated from Goddard College. After a series of odd jobs, Anthony decided to join the U.S. Army in 1957 for a steady source of income and medical coverage for his pregnant wife. He would stay in the Army until 1959 and became a US citizen during this time. While in the army, he became an editor and cartoonist for the battalion newspaper. After leaving the army, he spent a brief stint as a public school teacher before trying his hand at becoming a full-time writer.

Anthony and his wife made a deal: if he could sell a piece of writing within one year, she would continue to work to support him. But if he could not sell anything in that year, then he would forever give up his dream of being a writer. At the end of the year, he managed to get a short story published. He credits his wife as the person who made his writing career possible, and he advises aspiring writers that they need to have a source of income other than their writing in order to get through the early years of a writing career.

Carol Ann Marble Jacob died at home 3 October 2019 due to what is suspected to be heart related complications due to Chronic Inflammatory Demyelating Polyneuropathy (CIDP).

On multiple occasions Anthony has moved from one publisher to another (taking a profitable hit series with him) when he says he felt the editors were unduly tampering with his work. He has sued publishers for accounting malfeasance and won judgments in his favor. Anthony maintains an Internet Publishers Survey in the interest of helping aspiring writers. For this service, he won the 2003 "Friend of EPIC" award for service to the electronic publishing community. His website won the "Special Recognition for Service to Writers" award from Preditors and Editors, an author's guide to publishers and writing services.

Many of his popular novel series have been optioned for movies. His popular series Xanth inspired the DOS video game "Companions of Xanth", by Legend Entertainment. The series also spawned the board game "Xanth" by Mayfair Games.

Anthony's novels usually end with a chapter-long Author's Note, in which he talks about himself, his life, and his experiences as they related to the process of writing the novel. He often discusses correspondence with readers and any real-world issues that influenced the novel.

Since about 2000, Anthony has written his novels in a Linux environment.

Anthony's "Xanth" series was ranked No. 99 in a 2011 NPR readers' poll of best science fiction and fantasy books.

Act One of episode 470 of the radio program "This American Life" is an account of boyhood obsessions with Piers Anthony. The act is written and narrated by writer Logan Hill who, as a 12-year-old, was consumed with reading Anthony's novels. For a decade he felt he must have been Anthony's number one fan, until, when he was 22, he met "Andy" at a wedding and discovered their mutual interest in the writer. Andy is interviewed for the story and explains that, as a teenager, he had used escapist novels in order to cope with his alienating school and home life in Buffalo, New York. In 1987, at age 15, he decided to run away to Florida in order to try to live with Piers Anthony. The story includes Anthony's reflections on these events.

Naomi King, the daughter of writer Stephen King, enjoyed reading books by Piers Anthony, which included things like pixies, imps and fairies. After she told her father, "Dad, I just don't like those to be scared. Would you write something with dragons in it?", he wrote "The Eyes of the Dragon" which was originally published in 1984 and later in 1987 by Viking Press.

Early in Anthony's literary career, there was a dispute surrounding the original publication (1976) of "But What of Earth?". Editor Roger Elwood commissioned the novel for his nascent science-fiction line Laser Books. According to Anthony, he completed "But What of Earth?", and Elwood accepted and purchased it. Elwood then told Anthony that he wished to make several minor changes, and in order not to waste Anthony's time, he had hired copy editor (and author) Robert Coulson to retype the manuscript with the changes. Anthony described Coulson as a friend and was initially open to his contribution.

However, Elwood told Coulson he was to be a full collaborator, free to make revisions to Anthony's text in line with suggestions made by other copy editors. Elwood promised Coulson a 50-50 split with Anthony on all future royalties. According to Anthony, the published novel was very different from his version, with changes to characters and dialog, and with scenes added and removed. Anthony felt the changes worsened the novel. Laser's ultimate publication of "But What of Earth?" listed Anthony and Coulson together as collaborators. Publication rights were reverted to Anthony under threat of legal action. In 1989, Anthony (re)published his original "But What of Earth?" in an annotated edition through Tor Books. This edition contains an introduction and conclusion setting out the story of the novel's permutations and roughly 60 pages of notes by Anthony giving examples of changes to plot and characters, and describing some of the comments made by copy editors on his manuscript.

Anthony currently lives on a tree farm which he owns in Florida. He and his wife (who died in 2019) had two daughters, Penelope "Penny" Carolyn (who died in 2009) and Cheryl, with one grandchild, Logan. Regarding his religious beliefs, Anthony wrote in the October 2004 entry of his personal website, "I'm agnostic, which means I regard the case as unproven, but I'm much closer to the atheist position than to the theist one." In 2017 he stated, "I am more certain about God and the Afterlife: they don't exist."

For autobiographies refer to autobiographical subsection.



</doc>
<doc id="23670" url="https://en.wikipedia.org/wiki?curid=23670" title="Perfect number">
Perfect number

In number theory, a perfect number is a positive integer that is equal to the sum of its positive divisors, excluding the number itself. For instance, 6 has divisors 1, 2 and 3 (excluding itself), and 1 + 2 + 3 = 6, so 6 is a perfect number.

The sum of divisors of a number, excluding the number itself, is called its aliquot sum, so a perfect number is one that is equal to its aliquot sum. Equivalently, a perfect number is a number that is half the sum of all of its positive divisors including itself; in symbols, "σ"("n") = 2"n" where "σ" is the sum-of-divisors function. For instance, 28 is perfect as 1 + 2 + 4 + 7 + 14 + 28 = 56 = 2 × 28.

This definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called ("perfect", "ideal", or "complete number"). Euclid also proved a formation rule (IX.36) whereby formula_1 is an even perfect number whenever formula_2 is a prime of the form formula_3 for prime formula_4—what is now called a Mersenne prime. Two millennia later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.

It is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist. The first few perfect numbers are 6, 28, 496 and 8128 .

In about 300 BC Euclid showed that if 2 − 1 is prime then 2(2 − 1) is perfect.
The first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus noted 8128 as early as around AD 100. Nicomachus states without proof that every perfect number is (putting it in our terms) of the form formula_5 where formula_6 is prime. He seems to be unaware that itself has to be prime. He also said (wrongly) that the perfect numbers end in 6 or 8 alternately. Philo of Alexandria in his first-century book "On the creation" mentions perfect numbers, claiming that the world was created in 6 days and the moon orbits in 28 days because 6 and 28 are perfect. Philo is followed by Origen, and by Didymus the Blind, who adds the observation that there are only four perfect numbers that are less than 10,000. (Commentary on Genesis 1. 14-19). St Augustine defines perfect numbers in City of God (Part XI, Chapter 30) in the early 5th century AD, repeating the claim that God created the world in 6 days because 6 is the smallest perfect number. The Egyptian mathematician Ismail ibn Fallūs (1194–1252) mentioned the next three perfect numbers (33,550,336; 8,589,869,056; and 137,438,691,328) and listed a few more which are now known to be incorrect. The first known European mention of the fifth perfect number is a manuscript written between 1456 and 1461 by an unknown mathematician. In 1588, the Italian mathematician Pietro Cataldi identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers, and also proved that every perfect number obtained from Euclid's rule ends with a 6 or an 8.

Euclid proved that 2(2 − 1) is an even perfect number whenever 2 − 1 is prime (Elements, Prop. IX.36).

For example, the first four perfect numbers are generated by the formula 2(2 − 1), with "p" a prime number, as follows:

Prime numbers of the form 2 − 1 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For 2 − 1 to be prime, it is necessary that "p" itself be prime. However, not all numbers of the form 2 − 1 with a prime "p" are prime; for example, 2 − 1 = 2047 = 23 × 89 is not a prime number. In fact, Mersenne primes are very rare—of the 2,610,944 prime numbers "p" up to 43,112,609, 
2 − 1 is prime for only 47 of them.

Although Nicomachus had stated (without proof) that all perfect numbers were of the form formula_5 where formula_6 is prime (though he stated this somewhat differently), Ibn al-Haytham (Alhazen) circa AD 1000 conjectured only that every even perfect number is of that form. It was not until the 18th century that Leonhard Euler proved that the formula 2(2 − 1) will yield all the even perfect numbers. Thus, there is a one-to-one correspondence between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid–Euler theorem.

An exhaustive search by the GIMPS distributed computing project has shown that the first 47 even perfect numbers are 2(2 − 1) for
Four higher perfect numbers have also been discovered, namely those for which "p" = 57885161, 74207281, 77232917, and 82589933, though there may be others within this range. , 51 Mersenne primes are known, and therefore 51 even perfect numbers (the largest of which is 2 × (2 − 1) with 49,724,095 digits). It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.

As well as having the form 2(2 − 1), each even perfect number is the triangular number (and hence equal to the sum of the integers from 1 to ) and the hexagonal number. Furthermore, each even perfect number except for 6 is the centered nonagonal number and is equal to the sum of the first odd cubes:

Even perfect numbers (except 6) are of the form

with each resulting triangular number T = 28, T = 496, T = 8128 (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with T = 3, T = 55, T = 903, T = 3727815, ... This can be reformulated as follows: adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers 2(2 − 1) with odd prime "p" and, in fact, with all numbers of the form 2(2 − 1) for odd integer (not necessarily prime) "m".

Owing to their form, 2(2 − 1), every even perfect number is represented in binary form as "p" ones followed by "p" − 1  zeros; for example,
and
Thus every even perfect number is a pernicious number.

Every even perfect number is also a practical number (c.f. Related concepts).

It is unknown whether there is any odd perfect number, though various results have been obtained. In 1496, Jacques Lefèvre stated that Euclid's rule gives all perfect numbers, thus implying that no odd perfect number exists. Euler stated: "Whether (...) there are any odd perfect numbers is a most difficult question".<br>
More recently, Carl Pomerance has presented a heuristic argument suggesting that indeed no odd perfect number should exist. All perfect numbers are also Ore's harmonic numbers, and it has been conjectured as well that there are no odd Ore's harmonic numbers other than 1.

Any odd perfect number "N" must satisfy the following conditions:

In 1888, Sylvester stated:
Furthermore, several minor results are known concerning to the exponents
"e", ..., "e" in

All even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:

The sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with "perfect" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.

By definition, a perfect number is a fixed point of the restricted divisor function , and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_27-perfect numbers, or Granville numbers.

A semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.





</doc>
<doc id="23672" url="https://en.wikipedia.org/wiki?curid=23672" title="Parthenon">
Parthenon

The Parthenon (; ; , "Parthenónas") is a former temple on the Athenian Acropolis, Greece, dedicated to the goddess Athena, whom the people of Athens considered their patron. Construction began in 447 BC when the Athenian Empire was at the peak of its power. It was completed in 438 BC, although decoration of the building continued until 432 BC. It is the most important surviving building of Classical Greece, generally considered the zenith of the Doric order. Its decorative sculptures are considered some of the high points of Greek art. The Parthenon is regarded as an enduring symbol of Ancient Greece, Athenian democracy and Western civilization, and one of the world's greatest cultural monuments. To the Athenians who built it, the Parthenon and other Periclean monuments of the Acropolis were seen fundamentally as a celebration of Hellenic victory over the Persian invaders and as a thanksgiving to the gods for that victory.

The Parthenon itself replaced an older temple of Athena, which historians call the Pre-Parthenon or Older Parthenon, that was destroyed in the Persian invasion of 480 BC. Like most Greek temples, the Parthenon served a practical purpose as the city treasury. For a time, it served as the treasury of the Delian League, which later became the Athenian Empire. In the final decade of the 6th century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary.

After the Ottoman conquest, it was turned into a mosque in the early 1460s. On 26 September 1687, an Ottoman ammunition dump inside the building was ignited by Venetian bombardment during a siege of the Acropolis. The resulting explosion severely damaged the Parthenon and its sculptures. From 1800 to 1803, Thomas Bruce, 7th Earl of Elgin removed some of the surviving sculptures, now known as the Elgin Marbles, with the alleged permission of the Turks of the Ottoman Empire.

Since 1975 numerous large-scale restoration projects have been undertaken; the latest is expected to finish in 2020.

The origin of the Parthenon's name is from the Greek word παρθενών ("parthenon"), which referred to the "unmarried women's apartments" in a house and in the Parthenon's case seems to have been used at first only for a particular room of the temple; it is debated which room this is and how the room acquired its name. The Liddell–Scott–Jones "Greek–English Lexicon" states that this room was the western cella of the Parthenon, as does J.B. Bury. Jamauri D. Green holds that the parthenon was the room in which the peplos presented to Athena at the Panathenaic Festival was woven by the "arrephoroi", a group of four young girls chosen to serve Athena each year. Christopher Pelling asserts that Athena Parthenos may have constituted a discrete cult of Athena, intimately connected with, but not identical to, that of Athena Polias. According to this theory, the name of the Parthenon means the "temple of the virgin goddess" and refers to the cult of Athena Parthenos that was associated with the temple. The epithet "parthénos" () meant "maiden, girl", but also "virgin, unmarried woman" and was especially used for Artemis, the goddess of wild animals, the hunt, and vegetation, and for Athena, the goddess of strategy and tactics, handicraft, and practical reason. It has also been suggested that the name of the temple alludes to the maidens ("parthenoi"), whose supreme sacrifice guaranteed the safety of the city. "Parthénos" has also been applied to the Virgin Mary, Parthénos Maria, and the Parthenon had been converted to a Christian church dedicated to the Virgin Mary in the final decade of the 6th century.

The first instance in which "Parthenon" definitely refers to the entire building is found in the writings of the 4th century BC orator Demosthenes. In 5th-century building accounts, the structure is simply called "ho naos" ('the temple'). The architects Iktinos and Callicrates are said to have called the building "Hekatompedos" () ("the hundred footer") in their lost treatise on Athenian architecture, Harpocration writes that the Parthenon used to be called Hekatompedos by some, not due to its size but because of its beauty and fine proportions and, in the 4th century and later, the building was referred to as the "Hekatompedos" or the "Hekatompedon" as well as the Parthenon; the 1st-century-AD writer Plutarch referred to the building as the "Hekatompedos Parthenon".

Because the Parthenon was dedicated to the Greek goddess Athena, it has sometimes been referred to as the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.

Although the Parthenon is architecturally a temple and is usually called so, it is not really one in the conventional sense of the word. A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess, but the Parthenon never hosted the cult of Athena Polias, patron of Athens: the cult image, which was bathed in the sea and to which was presented the "peplos", was an olivewood "xoanon", located at an older altar on the northern side of the Acropolis.

The colossal statue of Athena by Phidias was not related to any cult and is not known to have inspired any religious fervour. It did not seem to have any priestess, altar or cult name.
According to Thucydides, Pericles once referred to the statue as a gold reserve, stressing that it "contained forty talents of pure gold and it was all removable".
The Athenian statesman thus implies that the metal, obtained from contemporary coinage, could be used again without any impiety.
The Parthenon should then be viewed as a grand setting for Phidias' votive statue rather than a cult site. It is said in many writings of the Greeks that there were many treasures stored inside the temple, such as Persian swords and small statue figures made of precious metals.

Archaeologist Joan Breton Connelly has recently argued for the coherency of the Parthenon's sculptural programme in presenting a succession of genealogical narratives that track Athenian identity back through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos. She argues a pedagogical function for the Parthenon's sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity. While some classicists, including Mary Beard, Peter Green, and Garry Wills have doubted or rejected Connelly's thesis, an increasing number of historians, archaeologists, and classical scholars support her work. They include: J.J. Pollitt, Brunilde Ridgway, Nigel Spivey, Caroline Alexander, and A. E. Stallings.

The first endeavour to build a sanctuary for on the site of the present Parthenon was begun shortly after the Battle of Marathon (c. 490–488 BC) upon a solid limestone foundation that extended and levelled the southern part of the Acropolis summit. This building replaced a "hekatompedon" ('hundred-footer') and would have stood beside the archaic temple dedicated to "Athena Polias" ('of the city'). The Older or Pre-Parthenon, as it is frequently referred to, was still under construction when the Persians sacked the city in 480 BC and razed the Acropolis.

The existence of both the proto-Parthenon and its destruction were known from Herodotus, and the drums of its columns were plainly visible built into the curtain wall north of the Erechtheion. Further physical evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885–90. The findings of this dig allowed Wilhelm Dörpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by Dörpfeld, not immediately below the present edifice as had been previously assumed. Dörpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a wholly different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885–90 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.
If the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479 BC declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450. The mundane fact of the cost of reconstructing Athens after the Persian sack is at least as likely a cause. However, the excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468 BC. Hill claimed that the Karrha limestone step Dörpfeld thought was the highest of Parthenon I was in fact the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at .

One difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to discuss and make sense of the potsherds found on the Acropolis came with the two-volume study by Graef and Langlotz published in 1925–33. This inspired American archaeologist William Bell Dinsmoor to attempt to supply limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495 BC, contradicting the early date given by Dörpfeld. Further, Dinsmoor denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what Dörpfeld referred to as Parthenon II. Dinsmoor and Dörpfeld exchanged views in the "American Journal of Archaeology" in 1935.

In the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League, and Athens was the greatest cultural center of its time, Pericles initiated an ambitious building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today — the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike — were erected during this period. The Parthenon was built under the general supervision of the artist Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447 BC, and the building was substantially completed by 432. However, work on the decorations continued until at least 431.
The Parthenon was built primarily by men who knew how to work marble. These quarrymen had exceptional skills and were able to cut the blocks of marble to very specific measurements. The quarrymen also knew how to avoid the faults, which were numerous in the Pentelic marble. If the marble blocks were not up to standard, the architects would reject them. The marble was worked with iron tools -- picks, points, punches, chisels and drills. The quarrymen would hold their tools against the marble block and firmly tap the surface of the rock.

A big project like the Parthenon attracted stonemasons from far and wide who traveled to Athens to assist in the project. Slaves and foreigners worked together with the Athenian citizens in the building of the Parthenon, doing the same jobs for the same pay. Temple building was a very specialized craft, and there were not many men in Greece qualified to build temples like the Parthenon, so these men would travel around and work where they were needed.

Other craftsmen also were necessary in the building of the Parthenon, specifically carpenters and metalworkers. Unskilled laborers also had key roles in the building of the Parthenon. These laborers loaded and unloaded the marble blocks and moved the blocks from place to place. In order to complete a project like the Parthenon, a number of different laborers were needed, and each played a critical role in constructing the final building.

The Parthenon is a peripteral octastyle Doric temple with Ionic architectural features. It stands on a platform or stylobate of three steps. In common with other Greek temples, it is of post and lintel construction and is surrounded by columns ('peripteral') carrying an entablature. There are eight columns at either end ('octastyle') and 17 on the sides. There is a double row of columns at either end. The colonnade surrounds an inner masonry structure, the "cella," which is divided into two compartments. At either end of the building the gable is finished with a triangular pediment originally occupied by sculpted figures. The columns are of the Doric order, with simple capitals, fluted shafts, and no bases. Above the architrave of the entablature is a frieze of carved pictorial panels (metopes), separated by formal architectural triglyphs, typical of the Doric order. Around the cella and across the lintels of the inner columns runs a continuous sculptured frieze in low relief. This element of the architecture is Ionic in style rather than Doric.

Measured at the stylobate, the dimensions of the base of the Parthenon are . The cella was 29.8 metres long by 19.2 metres wide (97.8 × 63.0 ft). On the exterior, the Doric columns measure in diameter and are high. The corner columns are slightly larger in diameter. The Parthenon had 46 outer columns and 23 inner columns in total, each column containing 20 flutes. (A flute is the concave shaft carved into the column form.) The roof was covered with large overlapping marble tiles known as imbrices and tegulae.

The Parthenon is regarded as the finest example of Greek architecture. The temple, wrote John Julius Cooper, "Enjoys the reputation of being the most perfect Doric temple ever built. Even in antiquity, its architectural refinements were legendary, especially the subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the "entasis" of the columns." "Entasis" refers to the slight swelling, of , in the center of the columns to counteract the appearance of columns having a waist, as the swelling makes them look straight from a distance. The stylobate is the platform on which the columns stand. As in many other classical Greek temples, it has a slight parabolic upward curvature intended to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outward, but they actually lean slightly inward so that if they carried on, they would meet almost exactly above the center of the Parthenon. Since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: "All follow the rule of being built to delicate curves", Gorham Stevens observed when pointing out that, in addition, the west front was built at a slightly higher level than that of the east front.

It is not universally agreed what the intended effect of these "optical refinements" was. They may serve as a sort of "reverse optical illusion." As the Greeks may have been aware, two parallel lines appear to bow, or curve outward, when intersected by converging lines. In this case, the ceiling and floor of the temple may seem to bow in the presence of the surrounding angles of the building. Striving for perfection, the designers may have added these curves, compensating for the illusion by creating their own curves, thus negating this effect and allowing the temple to be seen as they intended. It is also suggested that it was to enliven what might have appeared an inert mass in the case of a building without curves. But the comparison ought to be, according to Smithsonian historian Evan Hadingham, with the Parthenon's more obviously curved predecessors than with a notional rectilinear temple.

Some studies of the Acropolis, including of the Parthenon and its façade, have conjectured that many of its proportions approximate the golden ratio. However, such theories have been discredited by more recent studies, which have shown that the proportions of the Parthenon do not match the golden proportion.

The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC. The appearance of this is known from other images. The decorative stonework was originally highly coloured. The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the sculptural decoration of the Doric metopes on the frieze above the exterior colonnade, and of the Ionic frieze around the upper portion of the walls of the cella, had been completed. In the "opisthodomus" (the back room of the cella) were stored the monetary contributions of the Delian League, of which Athens was the leading member.

Only a very small number of the sculptures remain "in situ"; most of the surviving sculptures are today (controversially) in the British Museum in London as the Elgin Marbles, and the Athens Acropolis Museum, but a few pieces are also in the Louvre, National Museum of Denmark, and museums in Rome, Vienna and Palermo.

The frieze of the Parthenon's entablature contained 92 metopes, 14 each on the east and west sides, 32 each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods). According to the building records, the metope sculptures date to the years 446–440 BC. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (the mythical battle between the Olympian gods and the Giants). The metopes of the west end show the Amazonomachy (the mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13–21 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens and various myths. On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.

The mythological figures of the metopes of the East, North, and West sides of the Parthenon were deliberately mutilated by Christian iconoclasts in late antiquity.

The metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.

In March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to "Eleftherotypia" daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2,250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.

The most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior of the cella walls, which is the inside structure of the Parthenon. The bas-relief frieze was carved in situ; it is dated to 442 BC-438 BC.

One interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners were participating to honour the goddess Athena, offering sacrifices and a new peplos (dress woven by selected noble Athenian girls called "ergastines"). The procession is more crowded (appearing to slow in pace) as it nears the gods on the eastern side of the temple.

Joan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temple's sculptural programme which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of King Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the very first Panathenaia set in mythical times, the model on which historic Panathenaic processions was based.

The traveller Pausanias, when he visited the Acropolis at the end of the 2nd century AD, only mentioned briefly the sculptures of the pediments (gable ends) of the temple, reserving the majority of his description for the gold and ivory statue of the goddess inside.

The figures on the corners of the pediment depict the passage of time over the course of a full day. Tethrippa of Helios and Selene are located on the left and right corners of the pediment respectively. The horses of Helios's chariot are shown with livid expressions as they ascend into the sky at the start of the day; whereas the Selene's horses struggle to stay on the pediment scene as the day comes to an end.

The supporters of Athena are extensively illustrated at the back of the left chariot, while the defenders of Poseidon are shown trailing behind the right chariot. It is believed that the corners of the pediment are filled by Athenian water deities, such as Kephisos river, Ilissos river and nymph Callirhoe. This belief merges from the fluid character of the sculptures' body position which represents the effort of the artist to give the impression of a flowing river., Next to the left river god, there are the sculptures of the mythical king of Athens (Kekrops) with his daughters (Aglauros, Pandrosos, Herse). The statue of Poseidon was the largest sculpture in the pediment until it broke into pieces during Francesco Morosini's effort to remove it in 1688. The posterior piece of the torso was found by Lusieri in the groundwork of a Turkish house in 1801 and is currently held in British Museum. The anterior portion was revealed by Ross in 1835 and is now held in the Acropolis Museum of Athens.

Every statue on the west pediment has a fully completed back, which would have been impossible to see when the sculpture was on the temple; this indicates that the sculptors put great effort into accurately portraying the human body.

The only piece of sculpture from the Parthenon known to be from the hand of Phidias was the statue of Athena housed in the "naos". This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions and coins.

A major fire broke out in the Parthenon shortly after the middle of the third century AD which destroyed the Parthenon's roof and much of the sanctuary's interior. Heruli pirates are also credited with sacking Athens in 276, and destroying most of the public buildings there, including the Parthenon. Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate. A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater incline than the original roof and left the building's wings exposed.

The Parthenon survived as a temple dedicated to Athena for nearly 1,000 years until Theodosius II, during the Persecution of pagans in the late Roman Empire, decreed in 435 AD that all pagan temples in the Eastern Roman Empire be closed.
However, it is debated exactly when during the 5th-century that the closure of the Parthenon as a temple was actually put in practice. It is suggested to have occurred in c. 481–484, in the instructions against the remaining temples by order of Emperor Zeno, because the temple had been the focus of Pagan Hellenic opposition against Zeno in Athens in support of Illus, who had promised to restore Hellenic rites to the temples that were still standing.

At some point in the Fifth Century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege and sack of Constantinople during the Fourth Crusade in 1204 AD.

The Parthenon was converted into a Christian church in the final decade of the sixth century AD to become the Church of the Parthenos Maria (Virgin Mary), or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end, and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located. A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, from the rear chamber, the church's narthex. The spaces between the columns of the "opisthodomus" and the peristyle were walled up, though a number of doorways still permitted access. Icons were painted on the walls and many Christian inscriptions were carved into the Parthenon's columns. These renovations inevitably led to the removal and dispersal of some of the sculptures. 

The Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire after Constantinople, Ephesos, and Thessalonica. In 1018, the emperor Basil II went on a pilgrimage to Athens directly after his final victory over the Bulgarians for the sole purpose of worshipping at the Parthenon. In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well known.

At the time of the Latin occupation, it became for about 250 years a Roman Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.

In 1456, Ottoman Turkish forces invaded Athens and laid siege to a Florentine army defending the Acropolis until June 1458, when it surrendered to the Turks. The Turks may have briefly restored the Parthenon to the Greek Orthodox Christians for continued use as a church. Some time before the close of the fifteenth century, the Parthenon became a mosque.

The precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule. The apse became a mihrab, the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret, a minbar was installed, the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.

Despite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact. In 1667 the Turkish traveller Evliya Çelebi expressed marvel at the Parthenon's sculptures and figuratively described the building as "like some impregnable fortress not made by human agency". He composed a poetic supplication that it, as "a work less of human hands than of Heaven itself, should remain standing for all time". The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations. Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers d’Ortières. These depictions, particularly those made by Carrey, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.

In 1687, the Parthenon was extensively damaged in the greatest catastrophe to befall it in its long history. As part of the Morean War (1684–1699), the Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottoman Turks fortified the Acropolis and used the Parthenon as a gunpowder magazine – despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea – and as a shelter for members of the local Turkish community. 
On 26 September a Venetian mortar round, fired from the Hill of Philopappus, blew up the magazine, and the building was partly destroyed. The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble. Greek architect and archaeologist Kornilia Chatziaslani writes that "...three of the sanctuary’s four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs and metopes." About three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders and caused large fires that burned until the following day and consumed many homes.

Accounts written at the time conflict over whether this destruction was deliberate or accidental; one such account, written by the German officer Sobievolski, states that a Turkish deserter revealed to Morosini the use to which the Turks had put the Parthenon; expecting that the Venetians would not target a building of such historic importance. Morosini was said to have responded by directing his artillery to aim at the Parthenon. Subsequently, Morosini sought to loot sculptures from the ruin and caused further damage in the process. Sculptures of Poseidon and Athena's horses fell to the ground and smashed as his soldiers tried to detach them from the building's west pediment.

The following year, the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.
After the Turks had recaptured the Acropolis they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon. For the next century and a half, portions of the remaining structure were looted for building material and any remaining objects of value.

The 18th century was a period of Ottoman stagnation; as a result, many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. What they produced was the first measured drawings of the Parthenon published in 1787 in the second volume of "Antiquities of Athens Measured and Delineated". In 1801, the British Ambassador at Constantinople, the Earl of Elgin, obtained a questionable "firman" (edict) from the Sultan, whose existence or legitimacy has not been proved to this day, to make casts and drawings of the antiquities on the Acropolis, to demolish recent buildings if this was necessary to view the antiquities, and to remove sculptures from them.

When independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact. Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. However, the image of the small mosque within the Parthenon's cella has been preserved in Joly de Lotbinière's photograph, published in Lerebours's "Excursions Daguerriennes" in 1842: the first photograph of the Acropolis. The area became a historical precinct controlled by the Greek government. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.

The dispute centres around the Parthenon Marbles removed by Thomas Bruce, 7th Earl of Elgin, from 1801 to 1803, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, but more than half are in the Acropolis Museum in Athens. A few can still be seen on the building itself. The Greek government has campaigned since 1983 for the British Museum to return the sculptures to Greece. The British Museum has steadfastly refused to return the sculptures, and successive British governments have been unwilling to force the Museum to do so (which would require legislation). Nevertheless, talks between senior representatives from Greek and British cultural ministries and their legal advisors took place in London on 4 May 2007. These were the first serious negotiations for several years, and there were hopes that the two sides may move a step closer to a resolution.

In 1975, the Greek government began a concerted effort to restore the Parthenon and other Acropolis structures. After some delay, a Committee for the Conservation of the Acropolis Monuments was established in 1983. The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artifact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum. A crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-constructions were found to be incorrect. These were dismantled, and a careful process of restoration began. Originally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not so coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble.





</doc>
<doc id="23673" url="https://en.wikipedia.org/wiki?curid=23673" title="Pachomius the Great">
Pachomius the Great

Saint Pachomius (, c. 292–348), also known as Pachome and Pakhomius (), is generally recognized as the founder of Christian cenobitic monasticism. Coptic churches celebrate his feast day on 9 May, and Eastern Orthodox and Roman Catholic churches mark his feast on 15 May or 28 May. In the Lutheran Church, the saint is remembered as a renewer of the church, along with his contemporary (and fellow desert saint), Anthony of Egypt on January 17.

Saint Pachomius was born in 292 in Thebes (Luxor, Egypt) to pagan parents. According to his hagiography, at age 21, Pachomius was swept up against his will in a Roman army recruitment drive, a common occurrence during this period of turmoil and civil war. With several other youths, he was put onto a ship that floated down the Nile and arrived at Thebes in the evening. Here he first encountered local Christians, who customarily brought food and comfort daily to the conscripted troops. This made a lasting impression, and Pachomius vowed to investigate Christianity further when he got out. He was able to leave the army without ever having to fight, was converted and baptized (314).

Pachomius then came into contact with several well known ascetics and decided to pursue that path under the guidance of the hermit named Palaemon (317). One of his devotions, popular at the time, was praying with his arms stretched out in the form of a cross. After studying seven years with Palaemon, Pachomius set out to lead the life of a hermit near St. Anthony of Egypt, whose practices he imitated until Pachomius heard a voice in Tabennisi that told him to build a dwelling for the hermits to come to. An earlier ascetic named Macarius had created a number of proto-monasteries called lavra, or cells, where holy men who were physically or mentally unable to achieve the rigors of Anthony's solitary life would live in a community setting.

Pachomius established his first monastery between 318 and 323 at Tabennisi, Egypt. His elder brother John joined him, and soon more than 100 monks lived nearby. Pachomius set about organizing these cells into a formal organization. Until then, Christian asceticism had been solitary or "eremitic" with male or female monastics living in individual huts or caves and meeting only for occasional worship services. Pachomius created the community or "cenobitic" organization, in which male or female monastics lived together and held their property in common under the leadership of an abbot or abbess. Pachomius realized that some men, acquainted only with the eremitical life, might speedily become disgusted if the distracting cares of the cenobitical life were thrust too abruptly upon them. He therefore allowed them to devote their whole time to spiritual exercises, undertaking all the community's administrative tasks himself. The community hailed Pachomius as "Abba" ("father" in Hebrew), from which "Abbot" derives.
The monastery at Tabennisi, though enlarged several times, soon became too small and a second was founded at Pabau (Faou). After 336, Pachomius spent most of his time at Pabau. Though Pachomius sometimes acted as lector for nearby shepherds, neither he nor any of his monks became priests. St. Athanasius visited and wished to ordain him in 333, but Pachomius fled from him. Athanasius' visit was probably a result of Pachomius' zealous defence of orthodoxy against Arianism. Basil of Caesarea visited, then took many of Pachomius' ideas, which he adapted and implemented in Caesarea. This ascetic rule, or Ascetica, is still used today by the Eastern Orthodox Church, comparable to that of the Rule of St. Benedict in the West.

Pachomius was the first to set down a written rule. The first rule was composed of prayers generally known and in general use, such as the Lord's Prayer. The monks were to pray them every day. As the community developed, the rule were elaborated with precepts taken from the Bible. He drew up a rule which made things easier for the less proficient, but did not check the most extreme asceticism in the more proficient. The Rule sought to balance prayer with work, the communal life with solitude. The day was organised around the liturgy, with time for manual work and devotional reading.

Fasts and work were apportioned according to the individual's strength. Each monk received the same food and clothing. Common meals were provided, but those who wished to absent themselves from them were encouraged to do so, and bread, salt, and water were placed in their cells. In the Pachomian monasteries it was left very much to the individual taste of each monk to fix the order of life for himself. Thus the hours for meals and the extent of his fasting were settled by him alone, he might eat with the others in common or have bread and salt provided in his own cell every day or every second day.

His rule was translated into Latin by Saint Jerome. Honoratus of Lérins followed the Rule of St. Pachomius. Basil the Great and Benedict of Nursia adapted and incorporated parts of it in their rules.

Pachomius continued as abbot to the cenobites for some forty years. During an epidemic (probably plague), Pachomius called the monks, strengthened their faith, and appointed his successor. Pachomius then died on 14 Pashons, 64 A.M. (9 May 348 A.D.).

By the time Pachomius died, eight monasteries and several hundred monks followed his guidance. Within a generation, cenobic practices spread from Egypt to Palestine and the Judean Desert, Syria, North Africa and eventually Western Europe. The number of monks, rather than the number of monasteries, may have reached 7000.

His reputation as a holy man has endured. As mentioned above, several liturgical calendars commemorate Pachomius. Among many miracles attributed to Pachomius, that though he had never learned the Greek or Latin tongues, he sometimes miraculously spoke them. Pachomius is also credited with being the first Christian to use and recommend use of a prayer rope.

Examples of purely Coptic literature are the works of Abba Antonius and Abba Pachomius, who spoke only Coptic, and the sermons and preachings of Abba Shenouda, who chose to write only in Coptic.

The Pachomian system tended to treat religious literature as mere written instructions.

The name of the saint is of Coptic origin: ⲡⲁϧⲱⲙ "pakhōm" from ⲁϧⲱⲙ "akhōm" "eagle or falcon" (ⲡ "p"- at the beginning is the Coptic definite article). Into Greek it was adopted as Παχούμιος and Παχώμιος. By Greek folk etymology it was sometimes interpreted as "broad-shouldered" from παχύς "thick, large" and ὦμος "shoulder".





</doc>
<doc id="23674" url="https://en.wikipedia.org/wiki?curid=23674" title="Philosophical Investigations">
Philosophical Investigations

Philosophical Investigations () is a work by the philosopher Ludwig Wittgenstein. The book was published posthumously in 1953. Wittgenstein discusses numerous problems and puzzles in the fields of semantics, logic, philosophy of mathematics, philosophy of psychology, philosophy of action, and philosophy of mind, putting forth the view that conceptual confusions surrounding language use are at the root of most philosophical problems. Wittgenstein alleges that the problems are traceable to a set of related assumptions about the nature of language, which themselves presuppose a particular conception of the essence of language. This conception is considered and ultimately rejected for being too general; that is, as an essentialist account of the nature of language it is simply too narrow to be able to account for the variety of things we do with language. This view can be seen to contradict or discard much of what he argued in his earlier work "Tractatus Logico-Philosophicus" (1921).

"Philosophical Investigations" is highly influential. Within the analytic tradition, the book is considered by many as being one of the most important philosophical works of the 20th century. The book paved the way for the ordinary language philosophy that dominated Oxford philosophy in the middle of the twentieth century and also influenced pragmatism. The work continues to influence contemporary philosophers working in the philosophy of language and mind.

"Philosophical Investigations" is divided into two parts, consisting of what Wittgenstein calls, in the preface, "Bemerkungen", translated by Anscombe as "remarks". In the first part, the remarks are rarely more than a paragraph long and are numbered sequentially by paragraph. In the preface, Wittgenstein describes his failure to synthesize his points into a unified work. Due to this failure, he says that the book's structure "compels us to travel over a wide field of thought criss-cross in every direction." Wittgenstein then goes on to describe his remarks in the first part as "a number of sketches of landscapes which were made in the course of these long and involved journeyings."

The first part of "Philosophical Investigations" consists of paragraphs § 1 through § 693. Wittgenstein begins by criticizing Augustine’s description of learning a language and explaining language by ostensive definition in "The Confessions". This discussion occupies paragraphs § 1 through § 38. He then discusses rules and rule-following from paragraphs § 138 to § 242. Wittgenstein’s primary discussion of private language starts at § 244 and continues through paragraph § 271. The discussion of seeing and seeing aspects begins at paragraph § 398 and goes until paragraph § 401 of the first part.

The second part of the book consists of fourteen sections; the remarks are longer and numbered using Roman numerals. In the index, remarks from the first part are referenced by their number rather than page; however, references from the second part are cited by page number. The comparatively unusual nature of the second part is due to the fact that it comprises notes that Wittgenstein may have intended to re-incorporate into the first part. After his death, the text was published as a "Part II" in the first, second and third editions. However, in light of continuing uncertainty about Wittgenstein's intentions regarding this material, the fourth edition (2009) re-titles "Part I" as "Philosophical Investigations" proper, and "Part II" as "Philosophy of Psychology – A Fragment."

In standard references, a small letter following a page, section, or proposition number indicates a paragraph.

"Philosophical Investigations" is unique in Wittgenstein's presentation of argument. A typical philosophical text presents a philosophical problem, summarizes and critiques various alternative approaches to solving it, presents its approach, and then argues in favour of that approach. In contrast, Wittgenstein's book treats philosophy as an activity and presents the text as a dialogue similar to Socrates's method of questioning his interlocutors in Plato's dialogues. But unlike Plato's dialogue, where Socrates and his interlocutor are named, Wittgenstein never makes clear whose views are being argued for or who is being addressed. The following is an excerpt from an early entry in the book that exemplifies this method:

...think of the following use of language: I send someone shopping. I give him a slip marked 'five red apples'. He takes the slip to the shopkeeper, who opens the drawer marked 'apples', then he looks up the word 'red' in a table and finds a colour sample opposite it; then he says the series of cardinal numbers—I assume that he knows them by heart—up to the word 'five' and for each number he takes an apple of the same colour as the sample out of the drawer.—It is in this and similar ways that one operates with words—"But how does he know where and how he is to look up the word 'red' and what he is to do with the word 'five'?" Well, I assume that he "acts" as I have described. Explanations come to an end somewhere.—But what is the meaning of the word 'five'? No such thing was in question here, only how the word 'five' is used.

This example is typical of the book's style. David Stern describes Wittgenstein's presentation of topics as a three-stage process. In the first stage, Wittgenstein introduces the topic that he opposes, usually through dialogue. The second stage presents the topic as appropriate in narrow set of circumstances. As an example of this second stage, Stern cites § 2 of the book which reads: the "philosophical concept of meaning has its place in a primitive idea of the way language functions. But one can also say that it is the idea of a language more primitive than ours. Let us imagine a language for which the description given by Augustine is right." Then Wittgenstein provides an example of a builder A and his assistant B where the view that Wittgenstein attributes to Augustine on language makes sense. Finally, in the third stage, Wittgenstein points out that the position he opposes will not apply in a wider set of circumstances. An example of this third stage can be seen in § 3 of the book.

Through this progress, Wittgenstein attempts to get the reader to grapple with certain difficult philosophical topics, but he does not directly argue in favor of theories. Instead, Wittgenstein says his aim is not "to spare other people the trouble of thinking. But, if possible, to stimulate someone to thoughts of his own."

The "Investigations" deals largely with the difficulties of language and meaning. Wittgenstein viewed the tools of language as being fundamentally simple and he believed that philosophers had obscured this simplicity by misusing language and by asking meaningless questions. He attempted in the "Investigations" to make things clear: ""Der Fliege den Ausweg aus dem Fliegenglas zeigen""—to show the fly the way out of the fly bottle.

Wittgenstein claims that the meaning of a word is based on how the word is understood within the language-game. A common summary of his argument is that meaning is use. According to the use theory of meaning, the words are not defined by reference to the objects they designate, nor by the mental representations one might associate with them, but by how they are used. For example, this means there is no need to postulate that there is something called "good" that exists independently of any good deed. Wittgenstein's use theory of meaning contrasts with Platonic realism and with Gottlob Frege's notions of sense and reference. This argument has been labeled by some authors as "anthropological holism".

Section 43 in Wittgenstein's "Philosophical Investigations" reads: "For a large class of cases—though not for all—in which we employ the word "meaning" it can be defined thus: the meaning of a word is its use in the language."

Wittgenstein begins "Philosophical Investigations" with a quote from Augustine's "Confessions", which represents the view that language serves to point out objects in the world.The individual words in language name objects—sentences are combinations of such names. In this picture of language we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.
Wittgenstein rejects a variety of ways of thinking about what the meaning of a word is, or how meanings can be identified. He shows how, in each case, the "meaning" of the word presupposes our ability to use it. He first asks the reader to perform a thought experiment: to come up with a definition of the word "game". While this may at first seem a simple task, he then goes on to lead us through the problems with each of the possible definitions of the word "game". Any definition that focuses on amusement leaves us unsatisfied since the feelings experienced by a world class chess player are very different from those of a circle of children playing Duck Duck Goose. Any definition that focuses on competition will fail to explain the game of catch, or the game of solitaire. And a definition of the word "game" that focuses on rules will fall on similar difficulties.

The essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define "game", but that "even if we don't have a definition, we can still use the word successfully". Everybody understands what we mean when we talk about playing a game, and we can even clearly identify and correct inaccurate uses of the word, all without reference to any definition that consists of necessary and sufficient conditions for the application of the concept of a game. The German word for "game", "Spiele/Spiel", has a different sense than in English; the meaning of "Spiele" also extends to the concept of "play" and "playing." This German sense of the word may help readers better understand Wittgenstein's context in the remarks regarding games.

Wittgenstein argues that definitions emerge from what he termed "forms of life", roughly the culture and society in which they are used. Wittgenstein stresses the social aspects of cognition; to see how language works for most cases, we have to see how it functions in a specific social situation. It is this emphasis on becoming attentive to the social backdrop against which language is rendered intelligible that explains Wittgenstein's elliptical comment that "If a lion could talk, we could not understand him." However, in proposing the thought experiment involving the fictional character, Robinson Crusoe, a captain shipwrecked on a desolate island with no other inhabitant, Wittgenstein shows that language is not in all cases a social phenomenon (although, they are for most cases); instead the criterion for a language is grounded in a set of interrelated normative activities: teaching, explanations, techniques and criteria of correctness. In short, it is essential that a language is shareable, but this does not imply that for a language to function that it is in fact already shared.

Wittgenstein rejects the idea that ostensive definitions can provide us with the meaning of a word. For Wittgenstein, the thing that the word stands for does "not" give the meaning of the word. Wittgenstein argues for this making a series of moves to show that to understand an ostensive definition presupposes an understanding of the way the word being defined is used. So, for instance, there is no difference between pointing to a piece of paper, to its colour, or to its shape; but understanding the difference is crucial to using the paper in an ostensive definition of a shape or of a colour.

Why is it that we are sure a particular activity—e.g. Olympic target shooting—is a game while a similar activity—e.g. military sharp shooting—is not? Wittgenstein's explanation is tied up with an important analogy. How do we recognize that two people we know are related to one another? We may see similar height, weight, eye color, hair, nose, mouth, patterns of speech, social or political views, mannerisms, body structure, last names, etc. If we see enough matches we say we've noticed a family resemblance. It is perhaps important to note that this is not always a conscious process—generally we don't catalog various similarities until we reach a certain threshold, we just intuitively "see" the resemblances. Wittgenstein suggests that the same is true of language. We are all familiar (i.e. socially) with enough things which "are games" and enough things which "are not games" that we can categorize new activities as either games or not.

This brings us back to Wittgenstein's reliance on indirect communication, and his reliance on thought-experiments. Some philosophical confusions come about because we aren't able to "see" family resemblances. We've made a mistake in understanding the vague and intuitive rules that language uses, and have thereby tied ourselves up in philosophical knots. He suggests that an attempt to untangle these knots requires more than simple deductive arguments pointing out the problems with some particular position. Instead, Wittgenstein's larger goal is to try to divert us from our philosophical problems long enough to become aware of our intuitive ability to "see" the family resemblances.

Wittgenstein develops this discussion of games into the key notion of a "language-game". For Wittgenstein, his use of the term language-game "is meant to bring into prominence the fact that the speaking of language is part of an activity, or of a life-form." A central feature of language-games is that language is used in context and that language cannot be understood outside of its context. Wittgenstein lists the following as examples of language-games: “Giving orders, and obeying them”; “[d]escribing the appearance of an object, or giving its measurements”; “[c]onstructing an object from a description (a drawing)”; “[r]eporting an event”; “[s]peculating about an event." The famous example is the meaning of the word "game". We speak of various kinds of games: board games, betting games, sports, "war games". These are all different uses of the word "games". Wittgenstein also gives the example of "Water!", which can be used as an exclamation, an order, a request, or as an answer to a question. The meaning of the word depends on the language-game within which it is being used. Another way Wittgenstein puts the point is that the word "water" has no meaning apart from its use within a language-game. One might use the word as an order to have someone else bring you a glass of water. But it can also be used to warn someone that the water has been poisoned. One might even use the word as code by members of a secret society.

Wittgenstein does not limit the application of his concept of language games to word-meaning. He also applies it to sentence-meaning. For example, the sentence "Moses did not exist" (§79) can mean various things. Wittgenstein argues that independently of use the sentence does not yet 'say' anything. It is 'meaningless' in the sense of not being significant for a particular purpose. It only acquires significance if we fix it within some context of use. Thus, it fails to say anything because the sentence as such does not yet determine some particular use. The sentence is only meaningful when it is used to say something. For instance, it can be used so as to say that no person or historical figure fits the set of descriptions attributed to the person that goes by the name of "Moses". But it can also mean that the leader of the Israelites was not called Moses. Or that there cannot have been anyone who accomplished all that the Bible relates of Moses, etc. What the sentence means thus depends on its context of use.

Wittgenstein's discussion of rules and rule-following ranges from § 138 through § 242. Wittgenstein begins his discussion of rules with the example of one person giving orders to another "to write down a series of signs according to a certain formation rule." The series of signs consists of the natural numbers. Wittgenstein draws a distinction between following orders by copying the numbers following instruction and understanding the construction of the series of numbers. One general characteristic of games that Wittgenstein considers in detail is the way in which they consist in following rules. Rules constitute a family, rather than a class that can be explicitly defined. As a consequence, it is not possible to provide a definitive account of what it is to follow a rule. Indeed, he argues that "any" course of action can be made out to accord with some particular rule, and that therefore a rule cannot be used to explain an action. Rather, that one is following a rule or not is to be decided by looking to see if the actions conform to the expectations in the particular "form of life" in which one is involved. Following a rule is a social activity.

Saul Kripke provides an influential discussion of Wittgenstein's remarks on rules. For Kripke, Wittgenstein's discussion of rules "may be regarded as a new form of philosophical scepticism." He starts his discussion of Wittgenstein by quoting what he describes as Wittgenstein's sceptical paradox: "This was our paradox: no course of action could be determined by a rule, because every course of action can be made out to accord with the rule. The answer was: if everything can be made out to accord with the rule, then it can also be made out to conflict with it. And so there would be neither accord nor conflict here." Kripke argues that the implications of Wittgenstein's discussion of rules is that no person can mean something by the language that he or she uses or correctly follow (or fail to follow) a rule.

Wittgenstein also ponders the possibility of a language that talks about those things that are known only to the user, whose content is inherently private. The usual example is that of a language in which one names one's sensations and other subjective experiences, such that the meaning of the term is decided by the individual alone. For example, the individual names a particular sensation, on some occasion, 'S', and intends to use that word to refer to that sensation. Such a language Wittgenstein calls a "private language".

Wittgenstein presents several perspectives on the topic. One point he makes is that it is incoherent to talk of "knowing" that one is in some particular mental state. Whereas others can learn of my pain, for example, I simply "have" my own pain; it follows that one does not "know" of one's own pain, one simply "has" a pain. For Wittgenstein, this is a grammatical point, part of the way in which the language-game involving the word "pain" is played.

Although Wittgenstein certainly argues that the notion of private language is incoherent, because of the way in which the text is presented the exact nature of the argument is disputed. First, he argues that a private language is not really a language at all. This point is intimately connected with a variety of other themes in his later works, especially his investigations of "meaning". For Wittgenstein, there is no single, coherent "sample" or "object" that we can call "meaning". Rather, the supposition that there are such things is the source of many philosophical confusions. Meaning is a complicated phenomenon that is woven into the fabric of our lives. A good first approximation of Wittgenstein's point is that meaning is a "social" event; meaning happens "between" language users. As a consequence, it makes no sense to talk about a private language, with words that "mean" something in the absence of other users of the language.

Wittgenstein also argues that one couldn't possibly "use" the words of a private language. He invites the reader to consider a case in which someone decides that each time she has a particular sensation she will place a sign S in a diary. Wittgenstein points out that in such a case one could have no criteria for the correctness of one's use of S. Again, several examples are considered. One is that perhaps using S involves mentally consulting a table of sensations, to check that one has associated S correctly; but in this case, how could the mental table be checked for its correctness? It is "[a]s if someone were to buy several copies of the morning paper to assure himself that what it said was true", as Wittgenstein puts it. One common interpretation of the argument is that while one may have direct or privileged access to one's "current" mental states, there is no such infallible access to identifying previous mental states that one had in the past. That is, the only way to check to see if one has applied the symbol S correctly to a certain mental state is to introspect and determine whether the current sensation is identical to the sensation previously associated with S. And while identifying one's current mental state of remembering may be infallible, whether one remembered correctly is not infallible. Thus, for a language to be used at all it must have some public criterion of identity.

Often, what is widely regarded as a deep philosophical problem will vanish, argues Wittgenstein, and eventually be seen as a confusion about the significance of the words that philosophers use to frame such problems and questions. It is only in this way that it is interesting to talk about something like a "private language" — i.e., it is helpful to see how the "problem" results from a misunderstanding.

To sum up: Wittgenstein asserts that, if something is a language, it "cannot" be (logically) private; and if something "is" private, it is not (and cannot be) a language.

Another point that Wittgenstein makes against the possibility of a private language involves the beetle-in-a-box thought experiment. He asks the reader to imagine that each person has a box, inside which is something that everyone intends to refer to with the word "beetle". Further, suppose that no one can look inside another's box, and each claims to know what a "beetle" is only by examining their own box. Wittgenstein suggests that, in such a situation, the word "beetle" could not be the name of a thing, because supposing that each person has something completely different in their boxes (or nothing at all) does not change the meaning of the word; the beetle as a private object "drops out of consideration as irrelevant". Thus, Wittgenstein argues, if we can talk about something, then it is not "private", in the sense considered. And, contrapositively, if we consider something to be indeed private, it follows that we "cannot talk about it".

The discussion of private languages was revitalized in 1982 with the publication of Kripke's book "Wittgenstein on Rules and Private Language". In this work, Kripke uses Wittgenstein's text to develop a particular type of skepticism about rules that stresses the "communal" nature of language-use as grounding meaning. Critics of Kripke's version of Wittgenstein have facetiously referred to it as "Kripkenstein," scholars such as Gordon Baker, Peter Hacker, Colin McGinn, and John McDowell seeing it as a radical misinterpretation of Wittgenstein's text. Other philosophers – such as Martin Kusch – have defended Kripke's views.

Wittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism that posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social; therefore, there is no 'inner' space in which thoughts can occur. Part of Wittgenstein's credo is captured in the following proclamation: "An 'inner process' stands in need of outward criteria." This follows primarily from his conclusions about private languages: similarly, a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.

According to Wittgenstein, those who insist that consciousness (or any other apparently subjective mental state) is conceptually unconnected to the external world are mistaken. Wittgenstein explicitly criticizes so-called conceivability arguments: "Could one imagine a stone's having consciousness? And if anyone can do so—why should that not merely prove that such image-mongery is of no interest to us?" He considers and rejects the following reply as well:

"But if I suppose that someone is in pain, then I am simply supposing that he has just the same as I have so often had." — That gets us no further. It is as if I were to say: "You surely know what 'It is 5 o'clock here' means; so you also know what 'It's 5 o'clock on the sun' means. It means simply that it is just the same there as it is here when it is 5 o'clock." — The explanation by means of "identity" does not work here.

Thus, according to Wittgenstein, mental states are intimately connected to a subject's environment, especially their linguistic environment, and conceivability or imaginability. Arguments that claim otherwise are misguided.

From his remarks on the importance of public, observable behavior (as opposed to private experiences), it may seem that Wittgenstein is simply a behaviorist—one who thinks that mental states are nothing over and above certain behavior. However, Wittgenstein resists such a characterization; he writes (considering what an objector might say):

"Are you not really a behaviourist in disguise? Aren't you at bottom really saying that everything except human behaviour is a fiction?" — If I do speak of a fiction, then it is of a "grammatical" fiction.

Clearly, Wittgenstein did not want to be a behaviorist, nor did he want to be a cognitivist or a phenomenologist. He is, of course, primarily concerned with facts of linguistic usage. However, some argue that Wittgenstein is basically a behaviorist because he considers facts about language use as all there is. Such a claim is controversial, since it is explicitly opposed in the "Investigations".

In addition to ambiguous sentences, Wittgenstein discussed figures that can be seen and understood in two different ways. Often one can see something in a straightforward way — seeing "that" it is a rabbit, perhaps. But, at other times, one notices a particular aspect — seeing it "as" something.

An example Wittgenstein uses is the "duckrabbit", an ambiguous image that can be "seen as" either a duck or a rabbit. When one looks at the duck-rabbit and sees a rabbit, one is not "interpreting" the picture as a rabbit, but rather "reporting" what one sees. One just sees the picture as a rabbit. But what occurs when one sees it first as a duck, then as a rabbit? As the gnomic remarks in the "Investigations" indicate, Wittgenstein isn't sure. However, he is sure that it could not be the case that the external world stays the same while an 'internal' cognitive change takes place.

According to the standard reading, in the "Philosophical Investigations" Wittgenstein repudiates many of his own earlier views, expressed in the "Tractatus Logico-Philosophicus". The "Tractatus", as Bertrand Russell saw it (though Wittgenstein took strong exception to Russell's reading), had been an attempt to set out a logically perfect language, building on Russell's own work. In the years between the two works Wittgenstein came to reject the idea that underpinned logical atomism, that there were ultimate "simples" from which a language should, or even could, be constructed.

In remark #23 of "Philosophical Investigations" he points out that the practice of human language is more complex than the simplified views of language that have been held by those who seek to explain or simulate human language by means of a formal system. It would be a disastrous mistake, according to Wittgenstein, to see language as being in any way analogous to formal logic.

Besides stressing the "Investigations"' opposition to the "Tractatus", there are critical approaches which have argued that there is much more continuity and similarity between the two works than supposed. One of these is the New Wittgenstein approach.

Norman Malcolm credits Piero Sraffa with providing Wittgenstein with the conceptual break that founded the "Philosophical Investigations", by means of a rude gesture on Sraffa's part:

"Wittgenstein was insisting that a proposition and that which it describes must have the same 'logical form', the same 'logical multiplicity', Sraffa made a gesture, familiar to Neapolitans as meaning something like disgust or contempt, of brushing the underneath of his chin with an outward sweep of the finger-tips of one hand. And he asked: 'What is the logical form of that?'"

The preface itself, dated January 1945, credits Sraffa for the "most consequential ideas" of the book.

Bertrand Russell made the following comment on the "Philosophical Investigations" in his book "My Philosophical Development":I have not found in Wittgenstein's Philosophical Investigations anything that seemed to me interesting and I do not understand why a whole school finds important wisdom in its pages. Psychologically this is surprising. The earlier Wittgenstein, whom I knew intimately, was a man addicted to passionately intense thinking, profoundly aware of difficult problems of which I, like him, felt the importance, and possessed (or at least so I thought) of true philosophical genius. The later Wittgenstein, on the contrary, seems to have grown tired of serious thinking and to have invented a doctrine which would make such an activity unnecessary. I do not for one moment believe that the doctrine which has these lazy consequences is true. I realize, however, that I have an overpoweringly strong bias against it, for, if it is true, philosophy is, at best, a slight help to lexicographers, and at worst, an idle tea-table amusement.Ernest Gellner wrote the book "Words and Things", in which he was fiercely critical of the work of Ludwig Wittgenstein, J. L. Austin, Gilbert Ryle, Antony Flew, P. F. Strawson and many others. Ryle refused to have the book reviewed in the philosophical journal "Mind" (which he edited), and Bertrand Russell (who had written an approving foreword) protested in a letter to "The Times". A response from Ryle and a lengthy correspondence ensued.

"Philosophical Investigations" was not ready for publication when Wittgenstein died in 1951. G. E. M. Anscombe translated Wittgenstein's manuscript into English, and it was first published in 1953. There are multiple editions of "Philosophical Investigations" with the popular third edition and 50th anniversary edition having been edited by Anscombe: 






</doc>
<doc id="23677" url="https://en.wikipedia.org/wiki?curid=23677" title="Poul Anderson">
Poul Anderson

Poul William Anderson (November 25, 1926 – July 31, 2001) was an American science fiction author who began his career in the 1940s and continued to write into the 21st century. Anderson authored several works of fantasy, historical novels, and short stories. His awards include seven Hugo Awards and three Nebula Awards.

Poul Anderson was born on November 25, 1926, in Bristol, Pennsylvania, of Scandinavian parents. Shortly after his birth, his father, Anton Anderson, an engineer, moved the family to Texas, where they lived for over ten years. Following Anton Anderson's death, his widow took her children to Denmark. The family returned to the United States after the outbreak of World War II, settling eventually on a Minnesota farm. The frame story of his later novel "Three Hearts and Three Lions", before the fantasy part begins, is partly set in the Denmark which the young Anderson personally experienced.

While he was an undergraduate student at the University of Minnesota, Anderson's first stories were published by John W. Campbell in "Astounding Science Fiction": "Tomorrow's Children" by Anderson and F. N. Waldrop in March 1947 and a sequel, "Chain of Logic" by Anderson alone, in July. He earned his B.A. in physics with honors but made no serious attempt to work as a physicist; instead he became a free-lance writer after his graduation in 1948, and placed his third story in the December "Astounding".. While finding no purely academic application, Anderson's knowledge of physics is evident in the great care given to details of the scientific background – one of the defining characteristics of his writing style.

Anderson married Karen Kruse in 1953 and moved with her to the San Francisco Bay area. Their daughter Astrid (now married to science fiction author Greg Bear) was born in 1954. They made their home in Orinda, California. Over the years Poul gave many readings at The Other Change of Hobbit bookstore in Berkeley, and his wife later donated his typewriter and desk to the store.

In 1965 Algis Budrys said that Anderson "has for some time been science fiction's best storyteller". He was a founding member of the Society for Creative Anachronism (SCA) in 1966 and of the Swordsmen and Sorcerers' Guild of America (SAGA), also in the mid-1960s. The latter was a loose-knit group of Heroic Fantasy authors led by Lin Carter, originally eight in number, with entry by credentials as a fantasy writer alone. Anderson was the sixth President of Science Fiction and Fantasy Writers of America, taking office in 1972.

Robert A. Heinlein dedicated his 1985 novel "The Cat Who Walks Through Walls" to Anderson and eight of the other members of the Citizens' Advisory Council on National Space Policy. The Science Fiction Writers of America made Anderson its 16th SFWA Grand Master in 1998 and the Science Fiction and Fantasy Hall of Fame inducted him in 2000, its fifth class of two deceased and two living writers. He died of cancer on July 31, 2001, after a month in the hospital. A few of his novels were first published posthumously.

Anderson is probably best known for adventure stories in which larger-than-life characters succeed gleefully or fail heroically. His characters were nonetheless thoughtful, often introspective, and well developed. His plot lines frequently involved the application of social and political issues in a speculative manner appropriate to the science fiction genre. He also wrote some quieter works, generally of shorter length, which appeared more often during the latter part of his career.

Much of his science fiction is thoroughly grounded in science (with the addition of unscientific but standard speculations such as faster-than-light travel). A specialty was imagining scientifically plausible non-Earthlike planets. Perhaps the best known was the planet of "The Man Who Counts" in which Anderson adjusted its size and composition so that humans could live in the open air but flying intelligent aliens could evolve, and he explored the consequences of those adjustments.

In many stories, Anderson commented on society and politics. Whatever other vicissitudes his views went through, he firmly retained his belief in the direct and inextricable connection between human liberty and expansion into space, for which reason he strongly cried out against any idea of space exploration being "a waste of money" or "unnecessary luxury".

The connection between space flight and freedom is clearly (as is stated explicitly in some of the stories) an extension of the 19th-century American concept of the Frontier, where malcontents can advance further and claim some new land, and pioneers either bring life to barren asteroids (as in "Tales of the Flying Mountains") or settle on Earth-like planets teeming with life, but not intelligent forms (such as New Europe in "Star Fox").

As he repeatedly expressed in his nonfiction essays, Anderson firmly held that going into space was not an unnecessary luxury but an existential need, and that abandoning space would doom humanity to "a society of brigands ruling over peasants."

That is graphically expressed in the chilling short story "Welcome". In it, humanity has abandoned space and is left with an overcrowded Earth where a small elite not only treats all the rest as chattel slaves, but also regularly practices cannibalism, their chefs preparing "roast suckling coolie" for their banquets.

Conversely, in the bleak Orwellian world of "The High Ones" where the Soviets have won the Third World War and gained control of the whole of Earth, the dissidents still have some hope, precisely because space flight has not been abandoned. By the end of the story, rebels have established themselves at another stellar system—where their descendants, the reader is told, would eventually build a liberating fleet and set out back to Earth.

While horrified by the prospect of the Soviets winning complete rule over the Earth, Anderson was not enthusiastic about having Americans in that role either. Several stories and books describing the aftermath of a total US victory in another world war, such as "Sam Hall" and its loose sequel "Three Worlds to Conquer" as well as "Shield", are scarcely less bleak than the above-mentioned depictions of a Soviet victory. Like Heinlein in "Solution Unsatisfactory", Anderson assumed that the imposition of US military rule over the rest of the world would necessarily entail the destruction of the USA's democracy and the imposition of a harsh tyrannical rule over its own citizens.

Both Anderson's depiction of a Soviet-dominated world and that of a US-dominated one mention a rebellion breaking out in Brazil in the early 21st century, which is in both cases brutally put down by the dominant world power—the Brazilian rebels being characterized as "counter-revolutionaries" in the one case and as "communists" in the other.

In the early years of the Cold War—when he had been, as described by his later, more conservative self, a "flaming liberal"—Anderson pinned his hopes on the United Nations developing into a true world government. This is especially manifest in "Un-man", a future thriller where the 'Good Guys' are agents of the UN Secretary General working to establish a world government while the 'Bad Guys' are nationalists (especially American nationalists) who seek to preserve their respective nations' sovereignty at all costs. (The title has a double meaning: the hero is literally a UN man and has superhuman abilities which make his enemies fear him as an "un-man").

Anderson and his wife were among those who in 1968 signed a pro-Vietnam War advertisement in "Galaxy Science Fiction". By then, Anderson had repudiated world government; a half-humorous remnant is the beginning of "Tau Zero": a future in which the nations of the world entrusted Sweden with overseeing disarmament and found themselves living under the rule of the Swedish Empire. In "The Star Fox", he unfavorably depicts a future peace group called "World Militants for Peace". A more explicit expression of the same appears in the later "The Shield of Time" where a time-traveling young American woman from the 1990s pays a brief visit to a university campus of the 1960s and is not enthusiastic about what she sees there.

Anderson often returned to right-libertarianism and to the business leader as hero, most notably his character Nicholas van Rijn. Van Rijn is different from the archetype of a modern type of business executive, being more reflective of a Dutch Golden Age merchant of the 17th century. If he spends any time in boardrooms or plotting corporate takeovers, the reader remains ignorant of it since nearly all his appearances are in the wilds of a space frontier.

Beginning in the 1970s, Anderson's historically grounded works were influenced by the theories of the historian John K. Hord, who argued that all empires follow the same broad cyclical pattern, into which the Terran Empire of the Dominic Flandry spy stories fit neatly.

The writer Sandra Miesel (1978) has argued that Anderson's overarching theme is the struggle against entropy and the heat death of the universe, a condition of perfect uniformity in which nothing can happen.

In his numerous books and stories depicting conflict in science fiction or fantasy settings, Anderson takes trouble to make both sides' points of view comprehensible. Even if the author's point of view is obvious, the antagonists are usually depicted not as villains but as honorable on their own terms. The reader is given access to antagonists' thoughts and feelings, and they often have a tragic dignity in defeat. Typical examples are "The Winter of the World" and "The People of the Wind".

A common theme in Anderson's works, with obvious origins in the Northern European legends, is that doing the "right" (wisest) thing often involves performing actions that at face value seem dishonorable, illegal, destructive, or downright evil. "The Man who Counts", Nicholas van Rijn is "The Man" because he is prepared to be tyrannical and callously manipulative so that he and his companions can survive. In "High Treason", the protagonist disobeys orders and betrays his subordinates to prevent a war crime that would bring severe retribution upon humanity. In "A Knight of Ghosts and Shadows", Dominic Flandry first (effectively) lobotomizes his own son and then bombards the home planet of the Chereionite race to do his duty and prop up the Terran Empire. Such actions affect their characters in different ways, and dealing with the repercussions of having done the "right" (but unpleasant) thing is often the major focus of his short stories. The general lesson seems to be that guilt is the penalty for action.

In "The Star Fox", a relationship of grudging respect is built up between the hero, space privateer Gunnar Heim, and his enemy Cynbe, an exceptionally gifted Alerione trained from a young age to understand his species' human enemies to the point of being alienated from his own kind. In the final scene, Cynbe challenges Heim to a space battle which only one of them would survive. Heim accepts, whereupon Cynbe says, "I thank you, my brother."

Anderson set much of his work in the past, often with the addition of magic or in alternate or future worlds that resemble past eras. A specialty was his ancestral Scandinavia, as in his novel versions of the legends of Hrólf Kraki ("Hrolf Kraki's Saga") and Haddingus ("The War of the Gods"). Frequently he presented such worlds as superior to the dull, over-civilized present. Notable depictions of this superiority are the prehistoric world of "The Long Remembering", the quasi-medieval society of "No Truce with Kings", and the untamed Jupiter of "Call Me Joe" and "Three Worlds to Conquer". He handled the lure and power of atavism satirically in "Pact", critically in "The Queen of Air and Darkness" and "The Night Face", and tragically in "Goat Song".

His 1965 novel "The Corridors of Time" alternates between the European Stone Age and a repressive future. In this vision of tomorrow, almost everyone is either an agricultural serf or an industrial slave, but the rulers genuinely believe they are creating a better world. Set largely in Denmark, it treats the Neolithic society with knowledge and respect does but not hide its faults. The protagonist, having access to literally all periods of the past and future, finally decides to settle down in that era and finds a happy and satisfying life.

In many stories, a representative of a technologically advanced society underestimates "primitives" and pays a high price for it. In "The High Crusade", aliens who land in medieval England in the expectation of an easy conquest find that they are not immune to swords and arrows. In "The Only Game in Town", a Mongol warrior, while not knowing that the two "magicians" he meets are time travelers from the future, correctly guesses their intentions—and captures them with the help of the "magic" flashlight they had given him in an attempt to impress him. In another time-travel tale, "The Shield of Time", a "time policeman" from the 20th century, equipped with information and technologies from much further in the future, is outwitted by a medieval knight and barely escapes with his life. Yet another story, "The Man Who Came Early", features a 20th-century US Army soldier stationed in Iceland who is transported to the 10th century. Although he is full of ideas, his lack of practical knowledge of how to implement them and his total unfamiliarity with the technology and customs of the period leads to his downfall.

Anderson wrote the short essay "Uncleftish Beholding", an introduction to atomic theory, using only Germanic-rooted words. Fitting his love for olden years, that kind of learned writing has been named Ander-Saxon after him.

The story told in "The Shield of Time" is also an example of a tragic conflict, another common theme in Anderson's writing. The knight tries to do his best in terms of his own society and time, but his actions might bring about a horrible 20th century (even more horrible than that actually occurred). Therefore, the Time Patrol protagonists, who like the young knight and wish him well (the female protagonist comes close to falling in love with him), have no choice but to fight and ultimately kill him.

In "The Sorrow of Odin the Goth", a time-travelling anthropologist is assigned to study the culture of an ancient Gothic tribe by regular visits every few decades. Gradually, he is drawn into close involvement and feels protective towards the Goths (many of them are his own descendants because of a brief and poignant liaison with a Gothic, girl who dies in childbirth). The Goths identify him as the god Odin/Wodan. He finds that he must cruelly betray his beloved Goths since a ballad says that Odin did so; failure to fulfill his prescribed role might change history and bring the whole of the actual 20th century crashing down. In the final scene, he cries out in anguish: "Not even the gods can defy the Norns!", giving a new twist to that central aspect of the Norse religion.

In "The Pirate", the hero is duty-bound to deny a band of people from societies blighted by poverty the chance for a new start on a new planet since their settling the planet would eradicate the remnants of the artistic and articulate beings who lived there earlier. A similar theme but with much higher stakes appears in "Sister Planet": although terraforming Venus would provide new hope to starving people on the overcrowded Earth, it would exterminate Venus's just-discovered intelligent race, and the hero can avert that genocide only by murdering his best friends.

In "Delenda Est" the stakes are the highest imaginable. Time-travelling outlaws have created a new 20th century that us "not better or worse, just completely different". The hero can fight the outlaws and restore his (and our) familiar history but only at the price of totally destroying the vast world that has taken its place. "Risking your neck in order to negate a world full of people like yourself" is how the hero describes what he eventually undertakes.


Philip K. Dick's story "Waterspider" features Poul Anderson as one of the main characters.

In the opening of S.M. Stirling's novel "In the Courts of the Crimson Kings", a group of science fiction authors, including Poul Anderson, watch first contact with the book's Martians while attending an SF convention. Anderson supplies the beer.

 




</doc>
<doc id="23678" url="https://en.wikipedia.org/wiki?curid=23678" title="Panspermia">
Panspermia

Panspermia () is the hypothesis that life exists throughout the Universe, distributed by space dust, meteoroids, asteroids, comets, planetoids, and also by spacecraft carrying unintended contamination by microorganisms. Distribution may have occurred spanning galaxies, and so may not be restricted to the limited scale of solar systems.

Panspermia hypotheses propose (for example) that microscopic life-forms that can survive the effects of space (such as extremophiles) can become trapped in debris ejected into space after collisions between planets and small Solar System bodies that harbor life. Some organisms may travel dormant for an extended amount of time before colliding randomly with other planets or intermingling with protoplanetary disks. Under certain ideal impact circumstances (into a body of water, for example), and ideal conditions on a new planet's surfaces, it is possible that the surviving organisms could become active and begin to colonize their new environment. At least one report finds that endospores from a type of Bacillus bacteria found in Morocco can survive being heated to , making the argument for Panspermia even stronger. Panspermia studies concentrate not on how life began, but on methods that may distribute it in the Universe.

Pseudo-panspermia (sometimes called ""soft panspermia"" or ""molecular panspermia"") argues that the pre-biotic organic building-blocks of life originated in space, became incorporated in the solar nebula from which planets condensed, and were further—and continuously—distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s, it started to become evident that interstellar dust included a large component of organic molecules. Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. The dust plays a critical role in shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.

The chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10 to 17 million years old. Though the presence of life is confirmed only on the Earth, some scientists think that extraterrestrial life is not only plausible, but probable or inevitable. Probes and instruments have started examining other planets and moons in the Solar System and in other planetary systems for evidence of having once supported simple life, and projects such as SETI attempt to detect radio transmissions from possible extraterrestrial civilizations.

The first known mention of the term was in the writings of the 5th-century BC Greek philosopher Anaxagoras. Panspermia began to assume a more scientific form through the proposals of Jöns Jacob Berzelius (1834), Hermann E. Richter (1865), Kelvin (1871), Hermann von Helmholtz (1879) and finally reaching the level of a detailed scientific hypothesis through the efforts of the Swedish chemist Svante Arrhenius (1903).

Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) were influential proponents of panspermia. In 1974 they proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon), which Wickramasinghe later proved to be correct. Hoyle and Wickramasinghe further contended that life forms continue to enter the Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.

In an Origins Symposium presentation on April 7, 2009, physicist Stephen Hawking stated his opinion about what humans may find when venturing into space, such as the possibility of alien life through the theory of panspermia: "Life could spread from planet to planet or from stellar system to stellar system, carried on meteors."

Three series of astrobiology experiments have been conducted outside the International Space Station between 2008 and 2015 (EXPOSE) where a wide variety of biomolecules, microorganisms, and their spores were exposed to the solar flux and vacuum of space for about 1.5 years. Some organisms survived in an inactive state for considerable lengths of time, and those samples sheltered by simulated meteorite material provide experimental evidence for the likelihood of the hypothetical scenario of lithopanspermia.

Several simulations in laboratories and in low Earth orbit suggest that ejection, entry and impact is survivable for some simple organisms. In 2015, remains of biotic material were found in 4.1 billion-year-old rocks in Western Australia, when the young Earth was about 400 million years old. According to one researcher, "If life arose relatively quickly on Earth … then it could be common in the universe."

In April 2018, a Russian team published a paper which disclosed that they found DNA on the exterior of the ISS from land and marine bacteria similar to those previously observed in superficial micro layers at the Barents and Kara seas' coastal zones. They conclude "The presence of the wild land and marine bacteria DNA on the ISS suggests their possible transfer from the stratosphere into the ionosphere with the ascending branch of the global atmospheric electrical circuit. Alternatively, the wild land and marine bacteria as well as the ISS bacteria may all have an ultimate space origin."

In October 2018, Harvard astronomers presented an analytical model that suggests matter—and potentially dormant spores—can be exchanged across the vast distances between galaxies, a process termed 'galactic panspermia', and not be restricted to the limited scale of solar systems. The detection of an extra-solar object named ʻOumuamua crossing the inner Solar System in a
hyperbolic orbit confirms the existence of a continuing material link with exoplanetary systems.

In November 2019, scientists reported detecting, for the first time, sugar molecules, including ribose, in meteorites, suggesting that chemical processes on asteroids can produce some fundamentally essential bio-ingredients important to life, and supporting the notion of an RNA world prior to a DNA-based origin of life on Earth, and possibly, as well, the notion of panspermia.

Panspermia can be said to be either interstellar (between star systems) or interplanetary (between planets in the same star system); its transport mechanisms may include comets, radiation pressure and lithopanspermia (microorganisms embedded in rocks). Interplanetary transfer of nonliving material is well documented, as evidenced by meteorites of Martian origin found on Earth. Space probes may also be a viable transport mechanism for interplanetary cross-pollination in the Solar System or even beyond. However, space agencies have implemented planetary protection procedures to reduce the risk of planetary contamination, although, as recently discovered, some microorganisms, such as Tersicoccus phoenicis, may be resistant to procedures used in spacecraft assembly clean room facilities.

In 2012, mathematician Edward Belbruno and astronomers Amaya Moro-Martín and Renu Malhotra proposed that gravitational low-energy transfer of rocks among the young planets of stars in their birth cluster is commonplace, and not rare in the general galactic stellar population. Deliberate directed panspermia from space to seed Earth or sent from Earth to seed other planetary systems have also been proposed. One twist to the hypothesis by engineer Thomas Dehel (2006), proposes that plasmoid magnetic fields ejected from the magnetosphere may move the few spores lifted from the Earth's atmosphere with sufficient speed to cross interstellar space to other systems before the spores can be destroyed.

In 1903, Svante Arrhenius published in his article "The Distribution of Life in Space", the hypothesis now called radiopanspermia, that microscopic forms of life can be propagated in space, driven by the radiation pressure from stars. Arrhenius argued that particles at a critical size below 1.5 μm would be propagated at high speed by radiation pressure of the Sun. However, because its effectiveness decreases with increasing size of the particle, this mechanism holds for very tiny particles only, such as single bacterial spores.

The main criticism of radiopanspermia hypothesis came from Iosif Shklovsky and Carl Sagan, who pointed out the proofs of the lethal action of space radiations (UV and X-rays) in the cosmos. Regardless of the evidence, Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit.

Then, data gathered by the orbital experiments ERA, BIOPAN, EXOSTACK and EXPOSE, determined that isolated spores, including those of "B. subtilis", were killed if exposed to the full space environment for merely a few seconds, but if shielded against solar UV, the spores were capable of surviving in space for up to six years while embedded in clay or meteorite powder (artificial meteorites).

Minimal protection is required to shelter a spore against UV radiation: Exposure of unprotected DNA to solar UV and cosmic ionizing radiation break it up into its constituent bases. Also, exposing DNA to the ultrahigh vacuum of space alone is sufficient to cause DNA damage, so the transport of unprotected DNA or RNA during interplanetary flights powered solely by light pressure is extremely unlikely.

The feasibility of other means of transport for the more massive shielded spores into the outer Solar System – for example, through gravitational capture by comets – is at this time unknown.

Based on experimental data on radiation effects and DNA stability, it has been concluded that for such long travel times, boulder-sized rocks which are greater than or equal to 1 meter in diameter are required to effectively shield resistant microorganisms, such as bacterial spores against galactic cosmic radiation. These results clearly negate the radiopanspermia hypothesis, which requires single spores accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets, and support the likelihood of interplanetary transfer of microorganisms within asteroids or comets, the so-called lithopanspermia hypothesis.

Lithopanspermia, the transfer of organisms in rocks from one planet to another either through interplanetary or interstellar space, remains speculative. Although there is no evidence that lithopanspermia has occurred in the Solar System, the various stages have become amenable to experimental testing.

Thomas Gold, a professor of astronomy, suggested in 1960 the hypothesis of "Cosmic Garbage", that life on Earth might have originated accidentally from a pile of waste products dumped on Earth long ago by extraterrestrial beings.

Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new planetary systems with life by introduced species of microorganisms on lifeless planets. The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world" Crick noted later that life may have originated on Earth. It has been suggested that 'directed' panspermia was proposed in order to counteract various objections, including the argument that microbes would be inactivated by the space environment and cosmic radiation before they could make a chance encounter with Earth.

Conversely, active directed panspermia has been proposed to secure and expand life in space. This may be motivated by biotic ethics that values, and seeks to propagate, the basic patterns of our organic gene/protein life-form. The panbiotic program would seed new planetary systems nearby, and clusters of new stars in interstellar clouds. These young targets, where local life would not have formed yet, avoid any interference with local life.

For example, microbial payloads launched by solar sails at speeds up to 0.0001 "c" (30,000 m/s) would reach targets at 10 to 100 light-years in 0.1 million to 1 million years. Fleets of microbial capsules can be aimed at clusters of new stars in star-forming clouds, where they may land on planets or be captured by asteroids and comets and later delivered to planets. Payloads may contain extremophiles for diverse environments and cyanobacteria similar to early microorganisms. Hardy multicellular organisms (rotifer cysts) may be included to induce higher evolution.

The probability of hitting the target zone can be calculated from formula_1 where "A"(target) is the cross-section of the target area, "dy" is the positional uncertainty at arrival; "a" – constant (depending on units), "r"(target) is the radius of the target area; "v" the velocity of the probe; (tp) the targeting precision (arcsec/yr); and "d" the distance to the target, guided by high-resolution astrometry of 1×10 arcsec/yr (all units in SIU). These calculations show that relatively near target stars(Alpha PsA, Beta Pictoris) can be seeded by milligrams of launched microbes; while seeding the Rho Ophiochus star-forming cloud requires hundreds of kilograms of dispersed capsules.

Directed panspermia to secure and expand life in space is becoming possible because of developments in solar sails, precise astrometry, extrasolar planets, extremophiles and microbial genetic engineering. After determining the composition of chosen meteorites, astroecologists performed laboratory experiments that suggest that many colonizing microorganisms and some plants could obtain many of their chemical nutrients from asteroid and cometary materials. However, the scientists noted that phosphate (PO) and nitrate (NO–N) critically limit nutrition to many terrestrial lifeforms. With such materials, and energy from long-lived stars, microscopic life planted by directed panspermia could find an immense future in the galaxy.

A number of publications since 1979 have proposed the idea that directed panspermia could be demonstrated to be the origin of all life on Earth if a distinctive 'signature' message were found, deliberately implanted into either the genome or the genetic code of the first microorganisms by our hypothetical progenitor.

In 2013 a team of physicists claimed that they had found mathematical and semiotic patterns in the genetic code which they think is evidence for such a signature. This claim has been challenged by biologist PZ Myers who said, writing in Pharyngula: In a later peer-reviewed article, the authors address the operation of natural law in an extensive statistical test, and draw the same conclusion as in the previous article. In special sections they also discuss methodological concerns raised by PZ Myers and some others.

Pseudo-panspermia (sometimes called soft panspermia, molecular panspermia or quasi-panspermia) proposes that the organic molecules used for life originated in space and were incorporated in the solar nebula, from which the planets condensed and were further —and continuously— distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. The first suggestion came from Chandra Wickramasinghe, who proposed a polymeric composition based on the molecule formaldehyde (CHO).

Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionized, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.

A 2008 analysis of C/C isotopic ratios of organic compounds found in the Murchison meteorite indicates a non-terrestrial origin for these molecules rather than terrestrial contamination. Biologically relevant molecules identified so far include uracil, an RNA nucleobase, and xanthine. These results demonstrate that many organic compounds which are components of life on Earth were already present in the early Solar System and may have played a key role in life's origin.

In August 2009, NASA scientists identified one of the fundamental chemical building-blocks of life (the amino acid glycine) in a comet for the first time.

In August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these complex organic compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."

In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.

In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics – "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."

In 2013, the Atacama Large Millimeter Array (ALMA Project) confirmed that researchers have discovered an important pair of prebiotic molecules in the icy particles in interstellar space (ISM). The chemicals, found in a giant cloud of gas about 25,000 light-years from Earth in ISM, may be a precursor to a key component of DNA and the other may have a role in the formation of an important amino acid. Researchers found a molecule called cyanomethanimine, which produces adenine, one of the four nucleobases that form the "rungs" in the ladder-like structure of DNA.

The other molecule, called ethanamine, is thought to play a role in forming alanine, one of the twenty amino acids in the genetic code. Previously, scientists thought such processes took place in the very tenuous gas between the stars. The new discoveries, however, suggest that the chemical formation sequences for these molecules occurred not in gas, but on the surfaces of ice grains in interstellar space. NASA ALMA scientist Anthony Remijan stated that finding these molecules in an interstellar gas cloud means that important building blocks for DNA and amino acids can 'seed' newly formed planets with the chemical precursors for life.

In March 2013, a simulation experiment indicate that dipeptides (pairs of amino acids) that can be building blocks of proteins, can be created in interstellar dust.

In February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.

In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

In May 2016, the Rosetta Mission team reported the presence of glycine, methylamine and ethylamine in the coma of 67P/Churyumov-Gerasimenko. This, plus the detection of phosphorus, is consistent with the hypothesis that comets played a crucial role in the emergence of life on Earth.

In 2019, the detection of extraterrestrial sugars in meteorites implied the possibility that extraterrestrial sugars may have contributed to forming functional biopolymers like RNA.

The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. Nonetheless, Earth is the only place in the universe known by humans to harbor life. The sheer number of planets in the Milky Way galaxy, however, may make it probable that life has arisen somewhere else in the galaxy and the universe. It is generally agreed that the conditions required for the evolution of intelligent life as we know it are probably exceedingly rare in the universe, while simultaneously noting that simple single-celled microorganisms may be more likely.

The extrasolar planet results from the Kepler mission estimate 100–400 billion exoplanets, with over 3,500 as candidates or confirmed exoplanets. On 4 November 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists.

It is estimated that space travel over cosmic distances would take an incredibly long time to an outside observer, and with vast amounts of energy required. However, some scientists hypothesize that faster-than-light interstellar space travel might be feasible. This has been explored by NASA scientists since at least 1995.

Hoyle and Wickramasinghe have speculated that several outbreaks of illnesses on Earth are of extraterrestrial origins, including the 1918 flu pandemic, and certain outbreaks of polio and mad cow disease. For the 1918 flu pandemic they hypothesized that cometary dust brought the virus to Earth simultaneously at multiple locations—a view almost universally dismissed by experts on this pandemic. Hoyle also speculated that HIV came from outer space.

After Hoyle's death, "The Lancet" published a letter to the editor from Wickramasinghe and two of his colleagues, in which they hypothesized that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin and not originated from chickens. "The Lancet" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter. A 2008 encyclopedia notes that "Like other claims linking terrestrial disease to extraterrestrial pathogens, this proposal was rejected by the greater research community."

In April 2016, Jiangwen Qu of the Department of Infectious Disease Control in China presented a statistical study suggesting that "extremes of sunspot activity to within plus or minus 1  year may precipitate influenza pandemics." He discussed possible mechanisms of epidemic initiation and early spread, including speculation on primary causation by externally derived viral variants from space via cometary dust.


A separate fragment of the Orgueil meteorite (kept in a sealed glass jar since its discovery) was found in 1965 to have a seed capsule embedded in it, whilst the original glassy layer on the outside remained undisturbed. Despite great initial excitement, the seed was found to be that of a European Juncaceae or Rush plant that had been glued into the fragment and camouflaged using coal dust. The outer "fusion layer" was in fact glue. Whilst the perpetrator of this hoax is unknown, it is thought that they sought to influence the 19th century debate on spontaneous generation — rather than panspermia — by demonstrating the transformation of inorganic to biological matter.

Until the 1970s, life was thought to depend on its access to sunlight. Even life in the ocean depths, where sunlight cannot reach, was believed to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible "Alvin", scientists discovered colonies of assorted creatures clustered around undersea volcanic features known as black smokers.

It was soon determined that the basis for this food chain is a form of bacterium that derives its energy from oxidation of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. This chemosynthesis revolutionized the study of biology by revealing that terrestrial life need not be Sun-dependent; it only requires water and an energy gradient in order to exist.

It is now known that extremophiles, microorganisms with extraordinary capability to thrive in the harshest environments on Earth, can specialize to thrive in the deep-sea, ice, boiling water, acid, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. Living bacteria found in ice core samples retrieved from deep at Lake Vostok in Antarctica, have provided data for extrapolations to the likelihood of microorganisms surviving frozen in extraterrestrial habitats or during interplanetary transport. Also, bacteria have been discovered living within warm rock deep in the Earth's crust. Metallosphaera sedula can grow on meteorites in a lab.

In order to test some of these organisms' potential resilience in outer space, plant seeds and spores of bacteria, fungi and ferns have been exposed to the harsh space environment. Spores are produced as part of the normal life cycle of many plants, algae, fungi and some protozoans, and some bacteria produce endospores or cysts during times of stress. These structures may be highly resilient to ultraviolet and gamma radiation, desiccation, lysozyme, temperature, starvation and chemical disinfectants, while metabolically inactive. Spores germinate when favourable conditions are restored after exposure to conditions fatal to the parent organism.

Although computer models suggest that a captured meteoroid would typically take some tens of millions of years before collision with a planet, there are documented viable Earthly bacterial spores that are 40 million years old that are very resistant to radiation, and others able to resume life after being dormant for 25 million years, suggesting that lithopanspermia life-transfers are possible via meteorites exceeding 1 m in size.

The discovery of deep-sea ecosystems, along with advancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles, opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats and possible transport of hardy microbial life through vast distances.

The question of whether certain microorganisms can survive in the harsh environment of outer space has intrigued biologists since the beginning of spaceflight, and opportunities were provided to expose samples to space. The first American tests were made in 1966, during the Gemini IX and XII missions, when samples of bacteriophage T1 and spores of "Penicillium roqueforti" were exposed to outer space for 16.8 h and 6.5 h, respectively. Other basic life sciences research in low Earth orbit started in 1966 with the Soviet biosatellite program Bion and the U.S. Biosatellite program. Thus, the plausibility of panspermia can be evaluated by examining life forms on Earth for their capacity to survive in space. The following experiments carried on low Earth orbit specifically tested some aspects of panspermia or lithopanspermia:

The Exobiology Radiation Assembly (ERA) was a 1992 experiment on board the European Retrievable Carrier (EURECA) on the biological effects of space radiation. EURECA was an unmanned 4.5 tonne satellite with a payload of 15 experiments. It was an astrobiology mission developed by the European Space Agency (ESA). Spores of different strains of "Bacillus subtilis" and the "Escherichia coli" plasmid pUC19 were exposed to selected conditions of space (space vacuum and/or defined wavebands and intensities of solar ultraviolet radiation). After the approximately 11-month mission, their responses were studied in terms of survival, mutagenesis in the "his" ("B. subtilis") or "lac" locus (pUC19), induction of DNA strand breaks, efficiency of DNA repair systems, and the role of external protective agents. The data were compared with those of a simultaneously running ground control experiment:

BIOPAN is a multi-user experimental facility installed on the external surface of the Russian Foton descent capsule. Experiments developed for BIOPAN are designed to investigate the effect of the space environment on biological material after exposure between 13 and 17 days. The experiments in BIOPAN are exposed to solar and cosmic radiation, the space vacuum and weightlessness, or a selection thereof. Of the 6 missions flown so far on BIOPAN between 1992 and 2007, dozens of experiments were conducted, and some analyzed the likelihood of panspermia. Some bacteria, lichens ("Xanthoria elegans", "Rhizocarpon geographicum" and their mycobiont cultures, the black Antarctic microfungi "Cryomyces minteri" and "Cryomyces antarcticus"), spores, and even one animal (tardigrades) were found to have survived the harsh outer space environment and cosmic radiation.

The German EXOSTACK experiment was deployed on 7 April 1984 on board the Long Duration Exposure Facility satellite. 30% of "Bacillus subtilis" spores survived the nearly 6 years exposure when embedded in salt crystals, whereas 80% survived in the presence of glucose, which stabilize the structure of the cellular macromolecules, especially during vacuum-induced dehydration.

If shielded against solar UV, spores of "B. subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.

EXPOSE is a multi-user facility mounted outside the International Space Station dedicated to astrobiology experiments. There have been three EXPOSE experiments flown between 2008 and 2015: EXPOSE-E, EXPOSE-R and EXPOSE-R2. 
Results from the orbital missions, especially the experiments "SEEDS" and "LiFE", concluded that after an 18-month exposure, some seeds and lichens ("Stichococcus sp." and "Acarospora sp"., a lichenized fungal genus) may be capable to survive interplanetary travel if sheltered inside comets or rocks from cosmic radiation and UV radiation. The "LIFE", "SPORES", and "SEEDS" parts of the experiments provided information about the likelihood of lithopanspermia. These studies will provide experimental data to the lithopanspermia hypothesis, and they will provide basic data to planetary protection issues.

The Tanpopo mission is an orbital astrobiology experiment by Japan that is currently investigating the possible interplanetary transfer of life, organic compounds, and possible terrestrial particles in low Earth orbit. The Tanpopo experiment took place at the Exposed Facility located on the exterior of Kibo module of the International Space Station. The mission collected cosmic dusts and other particles for three years by using an ultra-low density silica gel called aerogel. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of life and its precursors. Some of these aerogels were replaced every one or two years through 2018. Sample collection began in May 2015, and the first samples were returned to Earth in mid-2016. Analyses are ongoing.

Panspermia is often criticized because it does not answer the question of the origin of life but merely places it on another celestial body. It was also criticized because it was thought it could not be tested experimentally.

Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit. Then it was found that isolated spores of "B. subtilis" were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds. Though these results may seem to negate the original panspermia hypothesis, the type of microorganism making the long journey is inherently unknown and also its features unknown. It could then be impossible to dismiss the hypothesis based on the hardiness of a few earth-evolved microorganisms. Also, if shielded against solar UV, spores of "Bacillus subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.




</doc>
<doc id="23680" url="https://en.wikipedia.org/wiki?curid=23680" title="There's Plenty of Room at the Bottom">
There's Plenty of Room at the Bottom

"There's Plenty of Room at the Bottom: An Invitation to Enter a New Field of Physics" was a lecture given by physicist Richard Feynman at the annual American Physical Society meeting at Caltech on December 29, 1959. Feynman considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than those used at the time. Although versions of the talk were reprinted in a few popular magazines, it went largely unnoticed and did not inspire the conceptual beginnings of the field. Beginning in the 1980s, nanotechnology advocates cited it to establish the scientific credibility of their work.

Feynman considered some ramifications of a general ability to manipulate matter on an atomic scale. He was particularly interested in the possibilities of denser computer circuitry, and microscopes that could see things much smaller than is possible with scanning electron microscopes. These ideas were later realized by the use of the scanning tunneling microscope, the atomic force microscope and other examples of scanning probe microscopy and storage systems such as Millipede, created by researchers at IBM.

Feynman also suggested that it should be possible, in principle, to make nanoscale machines that "arrange the atoms the way we want", and do chemical synthesis by mechanical manipulation.

He also presented the possibility of "swallowing the doctor", an idea that he credited in the essay to his friend and graduate student Albert Hibbs. This concept involved building a tiny, swallowable surgical robot.

As a thought experiment he proposed developing a set of one-quarter-scale manipulator hands slaved to the operator's hands to build one-quarter scale machine tools analogous to those found in any machine shop. This set of small tools would then be used by the small hands to build and operate ten sets of one-sixteenth-scale hands and tools, and so forth, culminating in perhaps a billion tiny factories to achieve massively parallel operations. He uses the analogy of a pantograph as a way of scaling down items. This idea was anticipated in part, down to the microscale, by science fiction author Robert A. Heinlein in his 1942 story "Waldo".

As the sizes got smaller, one would have to redesign tools, because the relative strength of various forces would change. Gravity would become less important, and Van der Waals forces such as surface tension would become more important. Feynman mentioned these scaling issues during his talk. Nobody has yet attempted to implement this thought experiment; some types of biological enzymes and enzyme complexes (especially ribosomes) function chemically in a way close to Feynman's vision. Feynman also mentioned in his lecture that it might be better eventually to use glass or plastic because their greater uniformity would avoid problems in the very small scale (metals and crystals are separated into domains where the lattice structure prevails). This could be a good reason to make machines and electronics out of glass and plastic. At the present time, there are electronic components made of both materials. In glass, there are optical fiber cables that amplify the light pulses at regular intervals, using glass doped with the rare-earth element erbium. The doped glass is spliced into the fiber and pumped by a laser operating at a different frequency. In plastic, field effect transistors are being made with polythiophene, a polymer invented by Alan J. Heeger et al. that becomes an electrical conductor when oxidized. By 2016, a factor of just 20 in electron mobility separated plastic from silicon.

At the meeting Feynman concluded his talk with two challenges, and offered a prize of $1000 for the first to solve each one. The first challenge involved the construction of a tiny motor, which, to Feynman's surprise, was achieved by November 1960 by Caltech graduate, William McLellan, a meticulous craftsman, using conventional tools. The motor met the conditions, but did not advance the art. The second challenge involved the possibility of scaling down letters small enough so as to be able to fit the entire "Encyclopædia Britannica" on the head of a pin, by writing the information from a book page on a surface 1/25,000 smaller in linear scale. In 1985, Tom Newman, a Stanford graduate student, successfully reduced the first paragraph of "A Tale of Two Cities" by 1/25,000, and collected the second Feynman prize. Newman's thesis adviser, R. Fabian Pease, had read the paper in 1966; but it was another grad student in the lab, Ken Polasko, who had recently read it who suggested attempting the challenge. Newman was looking for an arbitrary random pattern for demonstrating their technology. Newman said, "Text was ideal because it has so many different shapes."

"The New Scientist" reported "the scientific audience was captivated." Feynman had "spun the idea off the top of his mind" without even "notes from beforehand". There were no copies of the speech available. A "foresighted admirer" brought a tape recorder and an edited transcript, without Feynman's jokes, was made for publication by Caltech. In February 1960, Caltech's "Engineering and Science" published the speech. In addition to excerpts in "The New Scientist", versions were printed in "The Saturday Review" and "Popular Science". Newspapers announced the winning of the first challenge. The lecture was included as the final chapter in the 1961 book, "Miniaturization".

K. Eric Drexler later took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves, via computer control instead of control by a human operator, in his 1986 book "Engines of Creation: The Coming Era of Nanotechnology".

After Feynman's death, scholars studying the historical development of nanotechnology have concluded that his role in catalyzing nanotechnology research was not highly rated by many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, has reconstructed the history of the publication and republication of Feynman's talk, along with the record of citations to "Plenty of Room" in the scientific literature.

In Toumey's 2008 article, "Reading Feynman into Nanotechnology", he found 11 versions of the publication of "Plenty of Room", plus two instances of a closely related talk by Feynman, "Infinitesimal Machinery", which Feynman called "Plenty of Room, Revisited" (published under the name "Infinitesimal Machinery"). Also in Toumey's references are videotapes of that second talk. The journal "Nature Nanotechnology" dedicated an issue in 2009 to the subject.

Toumey found that the published versions of Feynman's talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Interest in "Plenty of Room" in the scientific literature greatly increased in the early 1990s. This is probably because the term "nanotechnology" gained serious attention just before that time, following its use by Drexler in his 1986 book, "Engines of Creation: The Coming Era of Nanotechnology", which cited Feynman, and in a cover article headlined "Nanotechnology", published later that year in a mass-circulation science-oriented magazine, "OMNI". The journal "Nanotechnology" was launched in 1989; the famous Eigler-Schweizer experiment, precisely manipulating 35 xenon atoms, was published in "Nature" in April 1990; and "Science" had a special issue on nanotechnology in November 1991. These and other developments hint that the retroactive rediscovery of "Plenty of Room" gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to Richard Feynman.

Toumey's analysis also includes comments from scientists in nanotechnology who say that "Plenty of Room" did not influence their early work, and most of them had not read it until a later date.

Feynman's stature as a Nobel laureate and as an important figure in 20th century science helped advocates of nanotechnology and provided a valuable intellectual link to the past. More concretely, his stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, illustrated by President Clinton's January 2000 speech calling for a Federal program:

The version of the Nanotechnology Research and Development Act that was passed by the House in May 2003 called for a study of the technical feasibility of molecular manufacturing, but this study was removed to safeguard funding of less controversial research before it was passed by the Senate and signed into law by President George W. Bush on December 3, 2003.





</doc>
<doc id="23681" url="https://en.wikipedia.org/wiki?curid=23681" title="Philately">
Philately

Philately (; ) is the study of stamps and postal history and other related items. It also refers to the collection, appreciation and research activities on stamps and other philatelic products. Philately involves more than just stamp collecting, which does not necessarily involve the study of stamps. It is possible to be a philatelist without owning any stamps. For instance, the stamps being studied may be very rare or reside only in museums.

The word "philately" is the English transliteration of the French "philatélie", coined by Georges Herpin in 1864. Herpin stated that stamps had been collected and studied for the previous six or seven years and a better name was required for the new hobby than "timbromanie" (roughly "stamp quest"), which was disliked. He took the Greek root word φιλ(ο)- "phil(o)-", meaning "an attraction or affinity for something", and ἀτέλεια "ateleia", meaning "exempt from duties and taxes" to form "philatelie". The introduction of postage stamps meant that the receipt of letters was now free of charge, whereas before stamps it was normal for postal charges to be paid by the recipient of a letter.

The alternative terms "timbromania", "timbrophily" and "timbrology" gradually fell out of use as "philately" gained acceptance during the 1860s.

As a collection field, philately appeared in the 1840s, after the introduction of the postage stamps in 1840. Some authors believe that the first philatelist appeared on the day of the release of the world's first postage stamp. It is known that on May 6, 1840, the Liverson, Denby and Lavie London law office sent a letter to Scotland franked with ten uncut Penny Blacks. The envelope had a legible postmark “LS.6MY6. 1840." Thus, this is the first thing in the world with the cancellation of the first day of conversion; in 1992 at an auction in Zurich, this envelope was sold for 690 thousand francs.

Already in 1846, cases of collecting stamps in large numbers were known in England, however, without any system to it, for example, stamps would be used for gluing walls. The systematic collection of postage stamps by country, issue, etc., began in the mid-1850s. The first philatelist is considered to be the print-maker Mansen, who lived in Paris, who in 1855 sold his collection, which contained almost all the postage stamps issued by that time, to the stamp merchant and second-hand book dealer Edard de Laplante. Owing to the demand for postage stamps, merchants appeared who began selling stamps for collecting purposes.

Towards the end of the 19th century stamp collecting reached hundreds of thousands of people of all society classes, from children to adults, from poor to millionaires and princes of reigning houses. Even some states had collections of postage stamps (for example, England, Germany, France, Bavaria, Bulgaria). For this purpose, from the 1870s, national postal museums began to be created in a number of countries where state collections of postage stamps were collected and stored (the first ones appeared in Germany, France, Bulgaria). In the first place was the collection of the British Museum, collected by MP Tapling and bequeathed to the Museum in 1891; it cost 800,000 Deutsche marks. The Imperial Postal Museum in Berlin ("German" Museum für Kommunikation Berlin) also had an extensive collection of stamps. The largest and only one of its kind collection belonged to Baron Philipp von Ferrary in Paris and costed him more than 1 million Deutsche marks.

As the number of postage signs increased every year, collecting them became more and more difficult. Therefore, from the beginning of the 1880s, the so-called collector experts appeared, who were limited to any one part of the world or several states, or even only one state.

Philately as one of the most popular types of collecting continued to develop in the 20th century. Along with the "Scott", the "Stanley Gibbons" and the "Yvert et Tellier" catalogs, the "Zumstein" (Switzerland, 1909), the Michel (Germany, 1910) catalogs, which later became world famous, began to be published.

Approximately in 1934 an idea to celebrate a special Postage Stamp Day was suggested by Hans von Rudolphi, a famous Berlin philatelist. The idea was fairly quickly accepted in Germany, and later was picked up by many other countries. Stamp Day is a memorial day established by the postal administration of a country and annually celebrated, which is designed to attract public attention, popularize postal service, expand postal correspondence and contribute to the development of philately. In 1968-04-24 in Cuba a postage stamp dedicated to the Stamp Day with an image of G. Sciltian's "El filatelista" was issued . 

In 1926, the Fédération Internationale de Philatélie (FIP) was founded, under the patronage of which international philatelic exhibitions have been regularly organized since 1929. So, the first World Philatelic Exhibition in Prague was held in August - September 1962. In 1976, FIP already brought together national societies from 57 countries, which held over 100 exhibitions; in 1987, over 60 countries entered the FIP.

Since the middle of the 20th century, philately has become the most widespread field of amateur collecting, which was facilitated by:


Philatelic magazines were published in many countries of the world, including socialist ones: Filatelen pregled (PRB), Filatéliai Szemie (HPR), Sammler Express (GDR), Filatelia Cubana (Cuba), Filatelista (PPR), Filatelie (SRR), Filatelie (CSR); the capitalist ones - The London Philatelist and Philatelic Magazine (Great Britain), the American Philatelist and Linn's Stamp News (USA), Der Sammler-Dienst and Deutsche Zeitung für Briefmarkenkunde (FRG), Austria-Philatelist (Austria), Schweizer Briefmarken Zeitung [de] (Switzerland), L'Écho de la timbrologie and La Philatélie française (France), Il collezionista, La Tribuna del collezionista, Cronaca Filatelica, Francobolli, Il Nuovo Corriere Filatelico (Italy), Dansk Filatelistisk Tidsskrift (Denmark).

In the socialist countries philately was most developed in the USSR, GDR, PPR, CSR and HPR, in the capitalist countries - in Germany, France, Great Britain, the USA, and Austria. The British Library Philatelic Collections (London) and the postal museums in Stockholm, Paris, and Bern had unique state collections of postage stamps at that time; among the famous private collections are the collections of the Royal Philatelic Collection, F. Ferrari (Austria), M. Burrus (Switzerland), A. Lichtenstein, A. Hind and J. Boker (USA), H. Kanai (Japan).

In the mid-1970s, national organizations and other philately associations existed in most countries, and 150-200 million people were involved in philately.

From August 28 to September 1, 2004, the World Stamp Championship was held for the first time in the history of world philately in Singapore.

Traditional philately is the study of the technical aspects of stamp production and stamp identification, including:


Philately uses several tools, including stamp tongs (a specialized form of tweezers) to safely handle the stamps, strong magnifying glass and a perforation gauge (odontometer) to measure the perforation gauge of the stamp.

The identification of watermarks is important and may be done with the naked eye by turning the stamp over or holding it up to the light. If this fails then "watermark fluid" may be used, which "wets" the stamp to reveal the mark.

Other common tools include stamp catalogs, stamp stock books and stamp hinges.

Philatelic organizations sprang up soon after people started collecting and studying stamps. They include local, national and international clubs and societies where collectors come together to share the various aspects of their hobby. One of the most known organizations is the American Philatelic Society.





</doc>
<doc id="23682" url="https://en.wikipedia.org/wiki?curid=23682" title="Puget Sound">
Puget Sound

Puget Sound is a sound of the Pacific Northwest, an inlet of the Pacific Ocean, and part of the Salish Sea. It is located along the northwestern coast of the U.S. state of Washington. It is a complex estuarine system of interconnected marine waterways and basins, with one major and two minor connections to the open Pacific Ocean via the Strait of Juan de Fuca—Admiralty Inlet being the major connection and Deception Pass and Swinomish Channel being the minor.

Water flow through Deception Pass is approximately equal to 2% of the total tidal exchange between Puget Sound and the Strait of Juan de Fuca. Puget Sound extends approximately from Deception Pass in the north to Olympia, Washington in the south. Its average depth is and its maximum depth, off Jefferson Point between Indianola and Kingston, is . The depth of the main basin, between the southern tip of Whidbey Island and Tacoma, Washington, is approximately .

In 2009, the term Salish Sea was established by the United States Board on Geographic Names as the collective waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. Sometimes the terms "Puget Sound" and "Puget Sound and adjacent waters" are used for not only Puget Sound proper but also for waters to the north, such as Bellingham Bay and the San Juan Islands region.

The term "Puget Sound" is used not just for the body of water but also the Puget Sound region centered on the sound. Major cities on the sound include Seattle, Tacoma, Olympia, and Everett, Washington. Puget Sound is also the third-largest estuary in the United States, after the Chesapeake Bay in Maryland and Virginia, and San Francisco Bay in northern California.

In 1792 George Vancouver gave the name "Puget's Sound" to the waters south of the Tacoma Narrows, in honor of Peter Puget, a Huguenot lieutenant accompanying him on the Vancouver Expedition. This name later came to be used for the waters north of Tacoma Narrows as well.

A different term for Puget Sound, used by a number of Native Americans and environmental groups, is "Whulge" (or "Whulj"), an anglicization of the Lushootseed name "", which means "sea, salt water, ocean, or sound".

The USGS defines Puget Sound as all the waters south of three entrances from the Strait of Juan de Fuca. The main entrance at Admiralty Inlet is defined as a line between Point Wilson on the Olympic Peninsula, and Point Partridge on Whidbey Island. The second entrance is at Deception Pass along a line from West Point on Whidbey Island, to Deception Island, then to Rosario Head on Fidalgo Island. The third entrance is at the south end of the Swinomish Channel, which connects Skagit Bay and Padilla Bay. Under this definition, Puget Sound includes the waters of Hood Canal, Admiralty Inlet, Possession Sound, Saratoga Passage, and others. It does not include Bellingham Bay, Padilla Bay, the waters of the San Juan Islands or anything farther north.

Another definition, given by NOAA, subdivides Puget Sound into five basins or regions. Four of these (including South Puget Sound) correspond to areas within the USGS definition, but the fifth one, called "Northern Puget Sound" includes a large additional region. It is defined as bounded to the north by the international boundary with Canada, and to the west by a line running north from the mouth of the Sekiu River on the Olympic Peninsula. Under this definition significant parts of the Strait of Juan de Fuca and the Strait of Georgia are included in Puget Sound, with the international boundary marking an abrupt and hydrologically arbitrary limit.

According to Arthur Kruckeberg, the term "Puget Sound" is sometimes used for waters north of Admiralty Inlet and Deception Pass, especially for areas along the north coast of Washington and the San Juan Islands, essentially equivalent to NOAA's "Northern Puget Sound" subdivision described above. Kruckeberg uses the term "Puget Sound and adjacent waters".

Continental ice sheets have repeatedly advanced and retreated from the Puget Sound region. The most recent glacial period, called the Fraser Glaciation, had three phases, or stades. During the third, or Vashon Glaciation, a lobe of the Cordilleran Ice Sheet, called the Puget Lobe, spread south about 15,000 years ago, covering the Puget Sound region with an ice sheet about thick near Seattle, and nearly at the present Canada-U.S. border. Since each new advance and retreat of ice erodes away much of the evidence of previous ice ages, the most recent Vashon phase has left the clearest imprint on the land. At its maximum extent the Vashon ice sheet extended south of Olympia to near Tenino, and covered the lowlands between the Olympic and Cascade mountain ranges. About 14,000 years ago the ice began to retreat. By 11,000 years ago it survived only north of the Canada–US border.

The melting retreat of the Vashon Glaciation eroded the land, creating a drumlin field of hundreds of aligned drumlin hills. Lake Washington and Lake Sammamish (which are ribbon lakes), Hood Canal, and the main Puget Sound basin were altered by glacial forces. These glacial forces are not specifically "carving", as in cutting into the landscape via the mechanics of ice/glaciers, but rather eroding the landscape from melt water of the Vashon Glacier creating the drumlin field. As the ice retreated, vast amounts of glacial till were deposited throughout the Puget Sound region. The soils of the region, less than ten thousand years old, are still characterized as immature.

As the Vashon glacier receded a series of proglacial lakes formed, filling the main trough of Puget Sound and inundating the southern lowlands. Glacial Lake Russell was the first such large recessional lake. From the vicinity of Seattle in the north the lake extended south to the Black Hills, where it drained south into the Chehalis River. Sediments from Lake Russell form the blue-gray clay identified as the Lawton Clay. The second major recessional lake was Glacial Lake Bretz. It also drained to the Chehalis River until the Chimacum Valley, in the northeast Olympic Peninsula, melted, allowing the lake's water to rapidly drain north into the marine waters of the Strait of Juan de Fuca, which was rising as the ice sheet retreated.

As icebergs calved off the toe of the glacier, their embedded gravels and boulders were deposited in the chaotic mix of unsorted till geologists call "glaciomarine drift." Many beaches about the Sound display glacial erratics, rendered more prominent than those in coastal woodland solely by their exposed position; submerged glacial erratics sometimes cause hazards to navigation. The sheer weight of glacial-age ice depressed the landforms, which experienced post-glacial rebound after the ice sheets had retreated. Because the rate of rebound was not synchronous with the post-ice age rise in sea levels, the bed of what is Puget Sound, filled alternately with fresh and with sea water. The upper level of the lake-sediment Lawton Clay now lies about above sea level.

The Puget Sound system consists of four deep basins connected by shallower sills. The four basins are Hood Canal, west of the Kitsap Peninsula, Whidbey Basin, east of Whidbey Island, South Sound, south of the Tacoma Narrows, and the Main Basin, which is further subdivided into Admiralty Inlet and the Central Basin. Puget Sound's sills, a kind of submarine terminal moraine, separate the basins from one another, and Puget Sound from the Strait of Juan de Fuca. Three sills are particularly significant—the one at Admiralty Inlet which checks the flow of water between the Strait of Juan de Fuca and Puget Sound, the one at the entrance to Hood Canal (about below the surface), and the one at the Tacoma Narrows (about ). Other sills that present less of a barrier include the ones at Blake Island, Agate Pass, Rich Passage, and Hammersley Inlet.

The depth of the basins is a result of the Sound being part of the Cascadia subduction zone, where the terranes accreted at the edge of the Juan de Fuca Plate are being subducted under the North American Plate. There has not been a major subduction zone earthquake here since the magnitude nine Cascadia earthquake; according to Japanese records, it occurred 26 January 1700. Lesser Puget Sound earthquakes with shallow epicenters, caused by the fracturing of stressed oceanic rocks as they are subducted, still cause great damage. The Seattle Fault cuts across Puget Sound, crossing the southern tip of Bainbridge Island and under Elliott Bay. To the south, the existence of a second fault, the Tacoma Fault, has buckled the intervening strata in the Seattle Uplift.

Typical Puget Sound profiles of dense glacial till overlying permeable glacial outwash of gravels above an impermeable bed of silty clay may become unstable after periods of unusually wet weather and slump in landslides.

The United States Geological Survey (USGS) defines Puget Sound as a bay with numerous channels and branches; more specifically, it is a fjord system of flooded glacial valleys. Puget Sound is part of a larger physiographic structure termed the Puget Trough, which is a physiographic section of the larger Pacific Border province, which in turn is part of the larger Pacific Mountain System.

Puget Sound is a large salt water estuary, or system of many estuaries, fed by highly seasonal freshwater from the Olympic and Cascade Mountain watersheds. The mean annual river discharge into Puget Sound is , with a monthly average maximum of about and minimum of about . Puget Sound's shoreline is long, encompassing a water area of and a total volume of at mean high water. The average volume of water flowing in and out of Puget Sound during each tide is . The maximum tidal currents, in the range of 9 to 10 knots, occurs at Deception Pass.
The size of Puget Sound's watershed is . "Northern Puget Sound" is frequently considered part of the Puget Sound watershed, which enlarges its size to . The USGS uses the name "Puget Sound" for its hydrologic unit subregion 1711, which includes areas draining to Puget Sound proper as well as the Strait of Juan de Fuca, the Strait of Georgia, and the Fraser River. Significant rivers that drain to "Northern Puget Sound" include the Nooksack, Dungeness, and Elwha Rivers. The Nooksack empties into Bellingham Bay, the Dungeness and Elwha into the Strait of Juan de Fuca. The Chilliwack River flows north to the Fraser River in Canada.

Tides in Puget Sound are of the mixed type with two high and two low tides each tidal day. These are called Higher High Water (HHW), Lower Low Water (LLW), Lower High Water (LHW), and Higher Low Water (HLW). The configuration of basins, sills, and interconnections cause the tidal range to increase within Puget Sound. The difference in height between the Higher High Water and the Lower Low Water averages about at Port Townsend on Admiralty Inlet, but increases to about at Olympia, the southern end of Puget Sound.

Puget Sound is generally accepted as the start of the Inside Passage.

Important marine flora of Puget Sound include eelgrass ("Zostera marina") and kelp, especially bull kelp ("Nereocystis luetkeana").

Among the marine mammals species found in Puget Sound are harbor seals ("Phoca vitulina"). Orca ("Orcinus orca") are famous throughout the Sound, and are a large tourist attraction. Although orca are sometimes seen in Puget Sound proper they are far more prevalent around the San Juan Islands north of Puget Sound.

Many fish species occur in Puget Sound. The various salmonid species, including salmon, trout, and char are particularly well-known and studied. Salmonid species of Puget Sound include chinook salmon ("Oncorhynchus tshawytscha"), chum salmon ("O. keta"), coho salmon ("O. kisutch"), pink salmon ("O. gorbuscha"), sockeye salmon ("O. nerka"), sea-run coastal cutthroat trout ("O. clarki clarki"), steelhead ("O. mykiss irideus"), sea-run bull trout ("Salvelinus confluentus"), and Dolly Varden trout ("Salvelinus malma malma").

Common forage fishes found in Puget Sound include Pacific herring ("Clupea pallasii"), surf smelt ("Hypomesus pretiosus"), and Pacific sand lance ("Ammodytes hexapterus"). Important benthopelagic fish of Puget Sound include North Pacific hake ("Merluccius productus"), Pacific cod ("Gadus macrocelhalus"), walleye pollock ("Theragra chalcogramma"), and the spiny dogfish ("Squalus acanthias"). There are about 28 species of Sebastidae (rockfish), of many types, found in Puget Sound. Among those of special interest are copper rockfish ("Sebastes caurinus"), quillback rockfish ("S. maliger"), black rockfish ("S. melanops"), yelloweye rockfish ("S. ruberrimus"), bocaccio rockfish ("S. paucispinis"), canary rockfish ("S. pinniger"), and Puget Sound rockfish ("S. emphaeus").

Many other fish species occur in Puget Sound, such as sturgeons, lampreys, various sharks, rays, and skates.

Puget Sound is home to numerous species of marine invertebrates, including sponges, sea anemones, chitons, clams, sea snails, limpets crabs, barnacles starfish, sea urchins, and sand dollars. Dungeness crabs ("Metacarcinus magister") occur throughout Washington waters, including Puget Sound. Many bivalves occur in Puget Sound, such as Pacific oysters ("Crassostrea gigas") and geoduck clams ("Panopea generosa"). The Olympia oyster ("Ostreola conchaphila"), once common in Puget Sound, was depleted by human activities during the 20th century. There are ongoing efforts to restore Olympia oysters in Puget Sound.
There are many seabird species of Puget Sound. Among these are grebes such as the western grebe ("Aechmophorus occidentalis"); loons such as the common loon ("Gavia immer"); auks such as the pigeon guillemot ("Cepphus columba"), rhinoceros auklet ("Cerorhinca monocerata"), common murre ("Uria aalge"), and marbled murrelet ("Brachyramphus marmoratus"); the brant goose ("Branta bernicla"); seaducks such as the long-tailed duck ("Clangula hyemalis"), harlequin duck ("Histrionicus histrionicus"), and surf scoter ("Melanitta perspicillata"); and cormorants such as the double-crested cormorant ("Phalacrocorax auritus"). Puget Sound is home to a non-migratory and marine-oriented subspecies of great blue herons ("Ardea herodias fannini"). Bald eagles ("Haliaeetus leucocephalus") occur in relative high densities in the Puget Sound region.

It is estimated that more than 100 million geoducks (pronounced "gooey ducks") are packed into Puget Sound's sediments. Also known as "king clam", geoducks are considered to be a delicacy in Asian countries.

George Vancouver explored Puget Sound in 1792, and claimed it for Great Britain on 4 June the same year, and naming it for one of his officers, Lieutenant Peter Puget.

After 1818 Britain and the United States, which both claimed the Oregon Country, agreed to "joint occupancy", deferring resolution of the Oregon boundary dispute until the 1846 Oregon Treaty. Puget Sound was part of the disputed region until 1846, after which it became US territory.

American maritime fur traders visited Puget Sound in the early 19th century.

The first European settlement in the Puget Sound area was Fort Nisqually, a fur trade post of the Hudson's Bay Company (HBC) built in 1833. Fort Nisqually was part of the HBC's Columbia District, headquartered at Fort Vancouver. The Puget Sound Agricultural Company, a subsidiary of the HBC, established farms and ranches near Fort Nisqually. British ships such as the "Beaver", exported foodstuffs and provisions from Fort Nisqually.

The first American settlement on Puget Sound was Tumwater. It was founded in 1845 by Americans who had come via the Oregon Trail. The decision to settle north of the Columbia River was made in part because one of the settlers, George Washington Bush, was considered black and the Provisional Government of Oregon banned the residency of mulattoes but did not actively enforce the restriction north of the river.

In 1853 Washington Territory was formed from part of Oregon Territory. In 1888 the Northern Pacific railroad line reached Puget Sound, linking the region to eastern states.

A unique state-run ferry system, the Washington State Ferries, connects the larger islands to the Washington mainland, as well as both sides of the sound, with vessels capable of carrying passengers and automobile traffic. The system carries 24 million passengers annually and is the largest ferry operator in the United States.

In the past 30 years there has been a large recession in the populations of the species which inhabit Puget Sound. The decrease has been seen in the following populations: forage fish, salmonids, bottom fish, marine birds, harbor porpoise and orcas. This decline is attributed to the various environmental issues in Puget Sound. Because of this population decline, there have been changes to the fishery practices, and an increase in petitioning to add species to the Endangered Species Act. There has also been an increase in recovery and management plans for many different area species.
The causes of these environmental issues are toxic contamination, eutrophication (low oxygen due to excess nutrients), and near shore habitat changes.

On May 22, 1978 a valve was mistakenly opened aboard the submarine USS "Puffer" releasing up to 500 US gallons (1,900 l; 420 imp gal) of radioactive water into Puget Sound, during an overhaul in drydock at Bremerton Naval Shipyard.





</doc>
<doc id="23688" url="https://en.wikipedia.org/wiki?curid=23688" title="Perjury">
Perjury

Perjury is the intentional act of swearing a false oath or falsifying an affirmation to tell the truth, whether spoken or in writing, concerning matters material to an official proceeding. In some jurisdictions, contrary to popular misconception, no crime has occurred when a false statement is (intentionally or unintentionally) made while under oath or subject to penalty. Instead, criminal culpability attaches only at the instant the declarant falsely asserts the truth of statements (made or to be made) that are material to the outcome of the proceeding. For example, it is not perjury to lie about one's age except if age is a fact material to influencing the legal result, such as eligibility for old age retirement benefits or whether a person was of an age to have legal capacity.

Perjury is considered a serious offense, as it can be used to usurp the power of the courts, resulting in miscarriages of justice. In the United States, for example, the general perjury statute under federal law classifies perjury as a felony and provides for a prison sentence of up to five years. The California Penal Code allows for perjury to be a capital offense in cases causing wrongful execution. Perjury which caused the wrongful execution of another or in the pursuit of causing the wrongful execution of another is respectively construed as murder or attempted murder, and is normally itself punishable by execution in countries that retain the death penalty. Perjury is considered a felony in most U.S. states as well as most Australian states. In Queensland, under Section 124 of the Queensland Criminal Code Act 1899, perjury is punishable by up to life in prison if it is committed to procure an innocent person for a crime that is punishable by life in prison. However, prosecutions for perjury are rare. In some countries such as France and Italy, suspects cannot be heard under oath or affirmation and so cannot commit perjury, regardless of what they say during their trial.

The rules for perjury also apply when a person has made a statement "under penalty of perjury" even if the person has not been sworn or affirmed as a witness before an appropriate official. An example is the US income tax return, which, by law, must be signed as true and correct under penalty of perjury (see ). Federal tax law provides criminal penalties of up to three years in prison for violation of the tax return perjury statute. See: 

Statements that entail an "interpretation" of fact are not perjury because people often draw inaccurate conclusions unwittingly or make honest mistakes without the intent to deceive. Individuals may have honest but mistaken beliefs about certain facts or their recollection may be inaccurate, or may have a different perception of what is the accurate way to state the truth. Like most other crimes in the common law system, to be convicted of perjury one must have had the "intention" ("mens rea") to commit the act and to have "actually committed" the act ("actus reus"). Further, statements that "are facts" cannot be considered perjury, even if they might arguably constitute an omission, and it is not perjury to lie about matters that are immaterial to the legal proceeding.

In the United States, Kenya, Scotland and several other English-speaking Commonwealth nations, subornation of perjury, which is attempting to induce another person to commit perjury, is itself a crime.

The offence of perjury is codified by section 132 of the Criminal Code. It is defined by section 131, which provides:

As to corroboration, see section 133.

Mode of trial and sentence

Every one who commits perjury is guilty of an indictable offence and liable to imprisonment for a term not exceeding fourteen years.

A person who, before the Court of Justice of the European Union, swears anything which he knows to be false or does not believe to be true is, whatever his nationality, guilty of perjury. Proceedings for this offence may be taken in any place in the State and the offence may for all incidental purposes be treated as having been committed in that place.

Perjury is a statutory offence in England and Wales. It is created by section 1(1) of the Perjury Act 1911. Section 1 of that Act reads:

The words omitted from section 1(1) were repealed by section 1(2) of the Criminal Justice Act 1948.

A person guilty of an offence under section 11(1) of the European Communities Act 1972 (i.e. perjury before the Court of Justice of the European Union) may be proceeded against and punished in England and Wales as for an offence under section 1(1).

Section 1(4) has effect in relation to proceedings in the Court of Justice of the European Union as it has effect in relation to a judicial proceeding in a tribunal of a foreign state.

Section 1(4) applies in relation to proceedings before a relevant convention court under the European Patent Convention as it applies to a judicial proceeding in a tribunal of a foreign state.

A statement made on oath by a witness outside the United Kingdom and given in evidence through a live television link by virtue of section 32 of the Criminal Justice Act 1988 must be treated for the purposes of section 1 as having been made in the proceedings in which it is given in evidence.

Section 1 applies in relation to a person acting as an intermediary as it applies in relation to a person lawfully sworn as an interpreter in a judicial proceeding; and for this purpose, where a person acts as an intermediary in any proceeding which is not a judicial proceeding for the purposes of section 1, that proceeding must be taken to be part of the judicial proceeding in which the witness's evidence is given.

Where any statement made by a person on oath in any proceeding which is not a judicial proceeding for the purposes of section 1 is received in evidence in pursuance of a special measures direction, that proceeding must be taken for the purposes of section 1 to be part of the judicial proceeding in which the statement is so received in evidence.

The definition in section 1(2) is not "comprehensive".

The book "Archbold" says that it appears to be immaterial whether the court before which the statement is made has jurisdiction in the particular cause in which the statement is made, because there is no express requirement in the Act that the court be one of "competent jurisdiction" and because the definition in section 1(2) does not appear to require this by implication either.

The actus reus of perjury might be considered to be the making of a statement, whether true or false, on oath in a judicial proceeding, where the person knows the statement to be false or believes it to be false.

Perjury is a conduct crime.

Perjury is triable only on indictment.

A person convicted of perjury is liable to imprisonment for a term not exceeding seven years, or to a fine, or to both.

The following cases are relevant:

See also the Crown Prosecution Service sentencing manual.

In Anglo-Saxon legal procedure, the offence of perjury could only be committed by both jurors and by compurgators. With time witnesses began to appear in court they were not so treated despite the fact that their functions were akin to that of modern witnesses. This was due to the fact that their role were not yet differentiated from those of the juror and so evidence or perjury by witnesses was not made a crime. Even in the 14th century, when witnesses started appearing before the jury to testify, perjury by them was not made a punishable offence. The maxim then was that every witness's evidence on oath was true. Perjury by witnesses began to be punished before the end of the 15th century by the Star Chamber.

The immunity enjoyed by witnesses began also to be whittled down or interfered with by the Parliament in England in 1540 with subornation of perjury and, in 1562, with perjury proper. The punishment for the offence then was in the nature of monetary penalty, recoverable in a civil action and not by penal sanction. In 1613, the Star Chamber declared perjury by a witness to be a punishable offence at common law.

Prior to the 1911 Act, perjury was governed by section 3 of the Maintenance and Embracery Act 1540 5 Eliz 1 c. 9 (An Act for the Punyshement of suche persones as shall procure or comit any wyllful Perjurye; repealed 1967) and the Perjury Act 1728.

The requirement that the statement be material can be traced back to and has been credited to Edward Coke, who said:

Perjury is a statutory offence in Northern Ireland. It is created by article 3(1) of the Perjury (Northern Ireland) Order 1979 (S.I. 1979/1714 (N.I. 19)). This replaces the Perjury Act (Northern Ireland) 1946 (c. 13) (N.I.).

Perjury operates in American law as an inherited principle of the common law of England, which defined the act as the "willful and corrupt giving, upon a lawful oath, or in any form allowed by law to be substituted for an oath, in a judicial proceeding or course of justice, of a false testimony material to the issue or matter of inquiry."

William Blackstone touched on the subject in his "Commentaries on the Laws of England", establishing perjury as "a crime committed when a lawful oath is administered, in some judicial proceeding, to a person who swears willfully, absolutely, and falsely, in a matter material to the issue or point in question." The punishment for perjury under the common law has varied from death to banishment and has included such grotesque penalties as severing the tongue of the perjurer. The definitional structure of perjury provides an important framework for legal proceedings, as the component parts of this definition have permeated jurisdictional lines, finding a home in American legal constructs. As such, the main tenets of perjury, including mens rea, a lawful oath, occurring during a judicial proceeding, a false testimony have remained necessary pieces of perjury's definition in the United States.

Perjury's current position in the American legal system takes the form of state and federal statutes. Most notably, the United States Code prohibits perjury, which is defined in two senses for federal purposes as someone who:

The above statute provides for a fine and/or up to five years in prison as punishment. Within federal jurisdiction, statements made in two broad categories of judicial proceedings may qualify as perjurious: 1) Federal official proceedings, and 2) Federal Court or Grand Jury proceedings. A third type of perjury entails the procurement of perjurious statements from another person. More generally, the statement must occur in the "course of justice," but this definition leaves room open for interpretation.

One particularly precarious aspect of the phrasing is that it entails knowledge of the accused person's perception of the truthful nature of events and not necessarily the actual truth of those events. It is important to note the distinction here, between giving a false statement under oath and merely misstating a fact accidentally, but the distinction can be especially difficult to discern in court of law.

The development of perjury law in the United States centers on "United States v. Dunnigan", a seminal case that set out the parameters of perjury within United States law. The court uses the Dunnigan-based legal standard to determine if an accused person: "testifying under oath or affirmation violates this section if she gives false testimony concerning a material matter with the willful intent to provide false testimony, rather than as a result of confusion, mistake, or faulty memory." However, a defendant shown to be willfully ignorant may in fact be eligible for perjury prosecution.

"Dunnigan" distinction manifests its importance with regard to the relation between two component parts of perjury's definition: in willfully giving a false statement, a person must understand that she is giving a false statement to be considered a perjurer under the "Dunnigan" framework. Deliberation on the part of the defendant is required for a statement to constitute perjury. Jurisprudential developments in the American law of perjury have revolved around the facilitation of "perjury prosecutions and thereby enhance the reliability of testimony before federal courts and grand juries."

With that goal in mind, Congress has sometimes expanded the grounds on which an individual may be prosecuted for perjury, with section 1623 of the United States Code recognizing the utterance of two mutually incompatible statements as grounds for perjury indictment even if neither can unequivocally be proven false. However, the two statements must be so mutually incompatible that at least one must necessarily be false; it is irrelevant whether the false statement can be specifically identified from among the two. It thus falls on the government to show that a defendant (a) knowingly made a (b) false (c) material statement (d) under oath (e) in a legal proceeding. The proceedings can be ancillary to normal court proceedings, and thus, even such menial interactions as bail hearings can qualify as protected proceedings under this statute.

Wilfulness is an element of the offense. The mere existence of two mutually-exclusive factual statements is not sufficient to prove perjury; the prosecutor nonetheless has the duty to plead and prove that the statement was willfully made. Mere contradiction will not sustain the charge; there must be strong corroborative evidence of the contradiction.

One significant legal distinction lies in the specific realm of knowledge necessarily possessed by a defendant for her statements to be properly called perjury. Though the defendant must knowingly render a false statement in a legal proceeding or under federal jurisdiction, the defendant need not know that they are speaking under such conditions for the statement to constitute perjury. All tenets of perjury qualification persist: the “knowingly” aspect of telling the false statement simply does not apply to the defendant's knowledge about the person whose deception is intended.

The evolution of United States perjury law has experienced the most debate with regards to the materiality requirement. Fundamentally, statements that are literally true cannot provide the basis for a perjury charge (as they do not meet the falsehood requirement) just as answers to truly ambiguous statements cannot constitute perjury. However, such fundamental truths of perjury law become muddled when discerning the materiality of a given statement and the way in which it was material to the given case. In "United States v. Brown", the court defined material statements as those with "a natural tendency to influence, or is capable of influencing, the decision of the decision-making body to be addressed," such as a jury or grand jury.

While courts have specifically made clear certain instances that have succeeded or failed to meet the nebulous threshold for materiality, the topic remains unresolved in large part, except in certain legal areas where intent manifests itself in an abundantly clear fashion, such as with the so-called perjury trap, a specific situation in which a prosecutor calls a person to testify before a grand jury with the intent of drawing a perjurious statement from the person being questioned.

Despite a tendency of US perjury law toward broad prosecutory power under perjury statutes, American perjury law has afforded potential defendants a new form of defense not found in the British Common Law. This defense requires that an individual admit to making a perjurious statement during that same proceeding and recanting the statement. Though this defensive loophole slightly narrows the types of cases which may be prosecuted for perjury, the effect of this statutory defense is to promote a truthful retelling of facts by witnesses, thus helping to ensure the reliability of American court proceedings just as broadened perjury statutes aimed to do.

Subornation of perjury stands as a subset of US perjury laws and prohibits an individual from inducing another to commit perjury. Subornation of perjury entails equivalent possible punishments as perjury on the federal level. The crime requires an extra level of satisfactory proof, as prosecutors must show not only that perjury occurred but also that the defendant positively induced said perjury. Furthermore, the inducing defendant must know that the suborned statement is a false, perjurious statement.


Notable people who have been accused of perjury include:



</doc>
<doc id="23689" url="https://en.wikipedia.org/wiki?curid=23689" title="Phoenix">
Phoenix

Phoenix most often refers to:


Phoenix may also refer to:




































Lists

Individual vessels





</doc>
<doc id="23690" url="https://en.wikipedia.org/wiki?curid=23690" title="Phosphate">
Phosphate

A Phosphate is a chemical derivative of phosphoric acid. The phosphate ion is an inorganic chemical, the conjugate base that can form many different salts. In organic chemistry, a phosphate, or organophosphate, is an ester of phosphoric acid. Of the various phosphoric acids and phosphates, organic phosphates are important in biochemistry and biogeochemistry (and, consequently, in ecology), and inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry. At elevated temperatures in the solid state, phosphates can condense to form pyrophosphates.

In biology, adding phosphates to—and removing them from—proteins in cells are both pivotal in the regulation of metabolic processes. Referred to as phosphorylation and dephosphorylation, respectively, they are important ways that energy is stored and released in living systems.

The phosphate ion is a polyatomic ion with the empirical formula and a molar mass of 94.97 g/mol. It consists of one central phosphorus atom surrounded by four oxygen atoms in a tetrahedral arrangement. The phosphate ion carries a −3 formal charge and is the conjugate base of the hydrogen phosphate ion, , which is the conjugate base of , the dihydrogen phosphate ion, which in turn is the conjugate base of phosphoric acid, . A phosphate salt forms when a positively charged ion attaches to the negatively charged oxygen atoms of the ion, forming an ionic compound.

Many phosphates are not soluble in water at standard temperature and pressure. The sodium, potassium, rubidium, caesium, and ammonium phosphates are all water-soluble. Most other phosphates are only slightly soluble or are insoluble in water. As a rule, the hydrogen and dihydrogen phosphates are slightly more soluble than the corresponding phosphates. The pyrophosphates are mostly water-soluble. Aqueous phosphate exists in four forms:

More precisely, considering these three equilibrium reactions:

the corresponding constants at 25 °C (in mol/L) are (see phosphoric acid):

The speciation diagram obtained using these p"K" values shows three distinct regions. In effect, , and behave as separate weak acids because the successive p"K" values differ by more than 4. For each acid, the pH at half-neutralization is equal to the p"K" value of the acid. The region in which the acid is in equilibrium with its conjugate base is defined by . Thus, the three pH regions are approximately 0–4, 5–9 and 10–14. This is a simplified model, assuming a constant ionic strength. It will not hold in reality at very low and very high pH values.

For a neutral pH, as in the cytosol, pH = 7.0

so that only and ions are present in significant amounts (62% , 38% . Note that in the extracellular fluid (pH = 7.4), this proportion is inverted (61% , 39% ).

Phosphate can form many polymeric ions such as pyrophosphate), , and triphosphate, . The various metaphosphate ions (which are usually long linear polymers) have an empirical formula of and are found in many compounds.

In biological systems, phosphorus is found as a free phosphate ion in solution and is called inorganic phosphate, to distinguish it from phosphates bound in various phosphate esters. Inorganic phosphate is generally denoted P and at physiological (homeostatic) pH primarily consists of a mixture of and ions.

Inorganic phosphate can be created by the hydrolysis of pyrophosphate, denoted PP:

However, phosphates are most commonly found in the form of adenosine phosphates (AMP, ADP, and ATP) and in DNA and RNA. It can be released by the hydrolysis of ATP or ADP. Similar reactions exist for the other nucleoside diphosphates and triphosphates. Phosphoanhydride bonds in ADP and ATP, or other nucleoside diphosphates and triphosphates, can release high amounts of energy when hydrolyzed which give them their vital role in all living organisms. They are generally referred to as high-energy phosphate, as are the phosphagens in muscle tissue. Compounds such as substituted phosphines have uses in organic chemistry, but do not seem to have any natural counterparts.

The addition and removal of phosphate from proteins in all cells is a pivotal strategy in the regulation of metabolic processes. Phosphorylation and dephosphorylation are important ways that energy is stored and released in living systems. Cells use ATP for this.

Phosphate is useful in animal cells as a buffering agent. Phosphate salts that are commonly used for preparing buffer solutions at cell pHs include NaHPO, NaHPO, and the corresponding potassium salts.

An important occurrence of phosphates in biological systems is as the structural material of bone and teeth. These structures are made of crystalline calcium phosphate in the form of hydroxyapatite. The hard dense enamel of mammalian teeth consists of fluoroapatite, a hydroxy calcium phosphate where some of the hydroxyl groups have been replaced by fluoride ions.

Plants take up phosphorus through several pathways: the arbuscular mycorrhizal pathway and the direct uptake pathway.

Phosphates are the naturally occurring form of the element phosphorus, found in many phosphate minerals. In mineralogy and geology, phosphate refers to a rock or ore containing phosphate ions. Inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry.

The largest global producer and exporter of phosphates is Morocco. Within North America, the largest deposits lie in the Bone Valley region of central Florida, the Soda Springs region of southeastern Idaho, and the coast of North Carolina. Smaller deposits are located in Montana, Tennessee, Georgia, and South Carolina. The small island nation of Nauru and its neighbor Banaba Island, which used to have massive phosphate deposits of the best quality, have been mined excessively. Rock phosphate can also be found in Egypt, Israel, Western Sahara, Navassa Island, Tunisia, Togo, and Jordan, countries that have large phosphate-mining industries.

Phosphorite mines are primarily found in:


In 2007, at the current rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists thought that a "peak phosphorus" will occur in 30 years and Dana Cordell from Institute for Sustainable Futures said that at "current rates, reserves will be depleted in the next 50 to 100 years". Reserves refer to the amount assumed recoverable at current market prices, and, in 2012, the USGS estimated 71 billion tons of world reserves, while 0.19 billion tons were mined globally in 2011. Phosphorus comprises 0.1% by mass of the average rock (while, for perspective, its typical concentration in vegetation is 0.03% to 0.2%), and consequently there are quadrillions of tons of phosphorus in Earth's 3 * 10 ton crust, albeit at predominantly lower concentration than the deposits counted as reserves from being inventoried and cheaper to extract; if it is assumed that the phosphate minerals in phosphate rock are hydroxyapatite and fluoroapatite, phosphate minerals contain roughly 18.5% phosphorus by weight and if phosphate rock contains around 20% of these minerals, the average phosphate rock has roughly 3.7% phosphorus by weight.

Some phosphate rock deposits, such as Mulberry in Florida, are notable for their inclusion of significant quantities of radioactive uranium isotopes. This syndrome is noteworthy because radioactivity can be released into surface waters in the process of application of the resultant phosphate fertilizer (e.g. in many tobacco farming operations in the southeast US).

In December 2012, Cominco Resources announced an updated JORC compliant resource of their Hinda project in Congo-Brazzaville of 531 Mt, making it the largest measured and indicated phosphate deposit in the world.

The three principal phosphate producer countries (China, Morocco and the United States) account for about 70% of world production.

In ecological terms, because of its important role in biological systems, phosphate is a highly sought after resource. Once used, it is often a limiting nutrient in environments, and its availability may govern the rate of growth of organisms. This is generally true of freshwater environments, whereas nitrogen is more often the limiting nutrient in marine (seawater) environments. Addition of high levels of phosphate to environments and to micro-environments in which it is typically rare can have significant ecological consequences. For example, blooms in the populations of some organisms at the expense of others, and the collapse of populations deprived of resources such as oxygen (see eutrophication) can occur. In the context of pollution, phosphates are one component of total dissolved solids, a major indicator of water quality, but not all phosphorus is in a molecular form that algae can break down and consume.

Calcium hydroxyapatite and calcite precipitates can be found around bacteria in alluvial topsoil. As clay minerals promote biomineralization, the presence of bacteria and clay minerals resulted in calcium hydroxyapatite and calcite precipitates.

Phosphate deposits can contain significant amounts of naturally occurring heavy metals. Mining operations processing phosphate rock can leave tailings piles containing elevated levels of cadmium, lead, nickel, copper, chromium, and uranium. Unless carefully managed, these waste products can leach heavy metals into groundwater or nearby estuaries. Uptake of these substances by plants and marine life can lead to concentration of toxic heavy metals in food products.



</doc>
<doc id="23692" url="https://en.wikipedia.org/wiki?curid=23692" title="Prime number theorem">
Prime number theorem

In number theory, the prime number theorem (PNT) describes the asymptotic distribution of the prime numbers among the positive integers. It formalizes the intuitive idea that primes become less common as they become larger by precisely quantifying the rate at which this occurs. The theorem was proved independently by Jacques Hadamard and Charles Jean de la Vallée Poussin in 1896 using ideas introduced by Bernhard Riemann (in particular, the Riemann zeta function).

The first such distribution found is , where is the prime-counting function and is the natural logarithm of . This means that for large enough , the probability that a random integer not greater than is prime is very close to . Consequently, a random integer with at most digits (for large enough ) is about half as likely to be prime as a random integer with at most digits. For example, among the positive integers of at most 1000 digits, about one in 2300 is prime (), whereas among positive integers of at most 2000 digits, about one in 4600 is prime (). In other words, the average gap between consecutive prime numbers among the first integers is roughly .

Let be the prime-counting function that gives the number of primes less than or equal to , for any real number . For example, because there are four prime numbers (2, 3, 5 and 7) less than or equal to 10. The prime number theorem then states that is a good approximation to (where log here means the natural logarithm), in the sense that the limit of the "quotient" of the two functions and as increases without bound is 1:

known as the asymptotic law of distribution of prime numbers. Using asymptotic notation this result can be restated as

This notation (and the theorem) does "not" say anything about the limit of the "difference" of the two functions as increases without bound. Instead, the theorem states that approximates in the sense that the relative error of this approximation approaches 0 as increases without bound.

The prime number theorem is equivalent to the statement that the th prime number satisfies

the asymptotic notation meaning, again, that the relative error of this approximation approaches 0 as increases without bound. For example, the th prime number is , and ()log() rounds to , a relative error of about 6.4%.

The prime number theorem is also equivalent to
where and are the first and the second Chebyshev functions respectively.

Based on the tables by Anton Felkel and Jurij Vega, Adrien-Marie Legendre conjectured in 1797 or 1798 that is approximated by the function , where and are unspecified constants. In the second edition of his book on number theory (1808) he then made a more precise conjecture, with and . Carl Friedrich Gauss considered the same question at age 15 or 16 "in the year 1792 or 1793", according to his own recollection in 1849. In 1838 Peter Gustav Lejeune Dirichlet came up with his own approximating function, the logarithmic integral (under the slightly different form of a series, which he communicated to Gauss). Both Legendre's and Dirichlet's formulas imply the same conjectured asymptotic equivalence of and stated above, although it turned out that Dirichlet's approximation is considerably better if one considers the differences instead of quotients.

In two papers from 1848 and 1850, the Russian mathematician Pafnuty Chebyshev attempted to prove the asymptotic law of distribution of prime numbers. His work is notable for the use of the zeta function , for real values of the argument "", as in works of Leonhard Euler, as early as 1737. Chebyshev's papers predated Riemann's celebrated memoir of 1859, and he succeeded in proving a slightly weaker form of the asymptotic law, namely, that if the limit of as goes to infinity exists at all, then it is necessarily equal to one. He was able to prove unconditionally that this ratio is bounded above and below by two explicitly given constants near 1, for all sufficiently large . Although Chebyshev's paper did not prove the Prime Number Theorem, his estimates for were strong enough for him to prove Bertrand's postulate that there exists a prime number between and for any integer .

An important paper concerning the distribution of prime numbers was Riemann's 1859 memoir "On the Number of Primes Less Than a Given Magnitude", the only paper he ever wrote on the subject. Riemann introduced new ideas into the subject, the chief of them being that the distribution of prime numbers is intimately connected with the zeros of the analytically extended Riemann zeta function of a complex variable. In particular, it is in this paper of Riemann that the idea to apply methods of complex analysis to the study of the real function originates. Extending the ideas of Riemann, two proofs of the asymptotic law of the distribution of prime numbers were obtained independently by Jacques Hadamard and Charles Jean de la Vallée Poussin and appeared in the same year (1896). Both proofs used methods from complex analysis, establishing as a main step of the proof that the Riemann zeta function is non-zero for all complex values of the variable that have the form with .

During the 20th century, the theorem of Hadamard and de la Vallée Poussin also became known as the Prime Number Theorem. Several different proofs of it were found, including the "elementary" proofs of Atle Selberg and Paul Erdős (1949). While the original proofs of Hadamard and de la Vallée Poussin are long and elaborate, later proofs introduced various simplifications through the use of Tauberian theorems but remained difficult to digest. A short proof was discovered in 1980 by American mathematician Donald J. Newman. Newman's proof is arguably the simplest known proof of the theorem, although it is non-elementary in the sense that it uses Cauchy's integral theorem from complex analysis.

Here is a sketch of the proof referred to in one of Terence Tao's lectures. Like most proofs of the PNT, it starts out by reformulating the problem in terms of a less intuitive, but better-behaved, prime-counting function. The idea is to count the primes (or a related set such as the set of prime powers) with "weights" to arrive at a function with smoother asymptotic behavior. The most common such generalized counting function is the Chebyshev function , defined by

This is sometimes written as

where is the von Mangoldt function, namely

It is now relatively easy to check that the PNT is equivalent to the claim that
Indeed, this follows from the easy estimates
and (using big notation) for any ,

The next step is to find a useful representation for . Let be the Riemann zeta function. It can be shown that is related to the von Mangoldt function , and hence to , via the relation

A delicate analysis of this equation and related properties of the zeta function, using the Mellin transform and Perron's formula, shows that for non-integer the equation

holds, where the sum is over all zeros (trivial and nontrivial) of the zeta function. This striking formula is one of the so-called explicit formulas of number theory, and is already suggestive of the result we wish to prove, since the term (claimed to be the correct asymptotic order of ) appears on the right-hand side, followed by (presumably) lower-order asymptotic terms.

The next step in the proof involves a study of the zeros of the zeta function. The trivial zeros −2, −4, −6, −8, ... can be handled separately:
which vanishes for a large . The nontrivial zeros, namely those on the critical strip , can potentially be of an asymptotic order comparable to the main term if , so we need to show that all zeros have real part strictly less than 1.

To do this, we take for granted that is meromorphic in the half-plane , and is analytic there except for a simple pole at , and that there is a product formula
for . This product formula follows from the existence of unique prime factorization of integers, and shows that is never zero in this region, so that its logarithm is defined there and

Write ; then

Now observe the identity
so that

for all . Suppose now that . Certainly is not zero, since has a simple pole at . Suppose that and let tend to 1 from above. Since formula_19 has a simple pole at and stays analytic, the left hand side in the previous inequality tends to 0, a contradiction.

Finally, we can conclude that the PNT is heuristically true. To rigorously complete the proof there are still serious technicalities to overcome, due to the fact that the summation over zeta zeros in the explicit formula for does not converge absolutely but only conditionally and in a "principal value" sense. There are several ways around this problem but many of them require rather delicate complex-analytic estimates that are beyond the scope of this paper. Edwards's book provides the details. Another method is to use Ikehara's Tauberian theorem, though this theorem is itself quite hard to prove. D. J. Newman observed that the full strength of Ikehara's theorem is not needed for the prime number theorem, and one can get away with a special case that is much easier to prove.

In a handwritten note on a reprint of his 1838 paper "", which he mailed to Gauss, Dirichlet conjectured (under a slightly different form appealing to a series rather than an integral) that an even better approximation to is given by the offset logarithmic integral function , defined by

Indeed, this integral is strongly suggestive of the notion that the "density" of primes around should be . This function is related to the logarithm by the asymptotic expansion

So, the prime number theorem can also be written as . In fact, in another paper in 1899 de la Vallée Poussin proved that

for some positive constant , where is the big notation. This has been improved to

In 2016, Trudgian proved an explicit upper bound for the difference between formula_25 and formula_26:
for formula_28.

Because of the connection between the Riemann zeta function and , the Riemann hypothesis has considerable importance in number theory: if established, it would yield a far better estimate of the error involved in the prime number theorem than is available today. More specifically, Helge von Koch showed in 1901 that, if and only if the Riemann hypothesis is true, the error term in the above relation can be improved to

The constant involved in the big notation was estimated in 1976 by Lowell Schoenfeld: assuming the Riemann hypothesis,

for all . He also derived a similar bound for the Chebyshev prime-counting function :

for all . This latter bound has been shown to express a variance to mean power law (when regarded as a random function over the integers), noise and to also correspond to the Tweedie compound Poisson distribution. Parenthetically, the Tweedie distributions represent a family of scale invariant distributions that serve as foci of convergence for a generalization of the central limit theorem.

The logarithmic integral is larger than for "small" values of . This is because it is (in some sense) counting not primes, but prime powers, where a power of a prime is counted as of a prime. This suggests that should usually be larger than by roughly , and in particular should always be larger than . However, in 1914, J. E. Littlewood proved that formula_32 changes sign infinitely often.

In the first half of the twentieth century, some mathematicians (notably G. H. Hardy) believed that there exists a hierarchy of proof methods in mathematics depending on what sorts of numbers (integers, reals, complex) a proof requires, and that the prime number theorem (PNT) is a "deep" theorem by virtue of requiring complex analysis. This belief was somewhat shaken by a proof of the PNT based on Wiener's tauberian theorem, though this could be set aside if Wiener's theorem were deemed to have a "depth" equivalent to that of complex variable methods.

In March 1948, Atle Selberg established, by "elementary" means, the asymptotic formula

where

for primes . By July of that year, Selberg and Paul Erdős had each obtained elementary proofs of the PNT, both using Selberg's asymptotic formula as a starting point. These proofs effectively laid to rest the notion that the PNT was "deep", and showed that technically "elementary" methods were more powerful than had been believed to be the case. On the history of the elementary proofs of the PNT, including the Erdős–Selberg priority dispute, see an article by Dorian Goldfeld.

There is some debate about the significance of Erdős and Selberg's result. There is no rigorous and widely accepted definition of the notion of elementary proof in number theory, so it is not clear exactly in what sense their proof is "elementary". Although it does not use complex analysis, it is in fact much more technical than the standard proof of PNT. One possible definition of an "elementary" proof is "one that can be carried out in first order Peano arithmetic." There are number-theoretic statements (for example, the Paris–Harrington theorem) provable using second order but not first order methods, but such theorems are rare to date. Erdős and Selberg's proof can certainly be formalized in Peano arithmetic, and in 1994, Charalambos Cornaros and Costas Dimitracopoulos proved that their proof can be formalized in a very weak fragment of PA, namely , However, this does not address the question of whether or not the standard proof of PNT can be formalized in PA.

In 2005, Avigad "et al." employed the Isabelle theorem prover to devise a computer-verified variant of the Erdős–Selberg proof of the PNT. This was the first machine-verified proof of the PNT. Avigad chose to formalize the Erdős–Selberg proof rather than an analytic one because while Isabelle's library at the time could implement the notions of limit, derivative, and transcendental function, it had almost no theory of integration to speak of.

In 2009, John Harrison employed HOL Light to formalize a proof employing complex analysis. By developing the necessary analytic machinery, including the Cauchy integral formula, Harrison was able to formalize "a direct, modern and elegant proof instead of the more involved 'elementary' Erdős–Selberg argument".

Let denote the number of primes in the arithmetic progression less than . Dirichlet and Legendre conjectured, and de la Vallée Poussin proved, that, if and are coprime, then

where is Euler's totient function. In other words, the primes are distributed evenly among the residue classes modulo with 1. This is stronger than Dirichlet's theorem on arithmetic progressions (which only states that there is an infinity of primes in each class) and can be proved using similar methods used by Newman for his proof of the prime number theorem.

The Siegel–Walfisz theorem gives a good estimate for the distribution of primes in residue classes.

Although we have in particular

empirically the primes congruent to 3 are more numerous and are nearly always ahead in this "prime number race"; the first reversal occurs at . However Littlewood showed in 1914 that there are infinitely many sign changes for the function

so the lead in the race switches back and forth infinitely many times. The phenomenon that is ahead most of the time is called Chebyshev's bias. The prime number race generalizes to other moduli and is the subject of much research; Pál Turán asked whether it is always the case that and change places when and are coprime to . Granville and Martin give a thorough exposition and survey.

The prime number theorem is an "asymptotic" result. It gives an ineffective bound on as a direct consequence of the definition of the limit: for all , there is an such that for all ,

However, better bounds on are known, for instance Pierre Dusart's
The first inequality holds for all and the second one for .

A weaker but sometimes useful bound for is

In Pierre Dusart's thesis there are stronger versions of this type of inequality that are valid for larger . Later in 2010, Dusart proved:

The proof by de la Vallée Poussin implies the following.
For every , there is an such that for all ,

As a consequence of the prime number theorem, one gets an asymptotic expression for the th prime number, denoted by :
A better approximation is
Again considering the th prime number , this gives an estimate of ; the first 5 digits match and relative error is about 0.00005%.

Rosser's theorem states that 
This can be improved by the following pair of bounds:

The table compares exact values of to the two approximations and . The last column, , is the average prime gap below .

The value for was originally computed assuming the Riemann hypothesis; it has since been verified unconditionally.

There is an analogue of the prime number theorem that describes the "distribution" of irreducible polynomials over a finite field; the form it takes is strikingly similar to the case of the classical prime number theorem.

To state it precisely, let be the finite field with elements, for some fixed , and let be the number of monic "irreducible" polynomials over whose degree is equal to . That is, we are looking at polynomials with coefficients chosen from , which cannot be written as products of polynomials of smaller degree. In this setting, these polynomials play the role of the prime numbers, since all other monic polynomials are built up of products of them. One can then prove that
If we make the substitution , then the right hand side is just
which makes the analogy clearer. Since there are precisely monic polynomials of degree (including the reducible ones), this can be rephrased as follows: if a monic polynomial of degree is selected randomly, then the probability of it being irreducible is about .

One can even prove an analogue of the Riemann hypothesis, namely that

The proofs of these statements are far simpler than in the classical case. It involves a short combinatorial argument, summarised as follows: every element of the degree extension of is a root of some irreducible polynomial whose degree divides ; by counting these roots in two different ways one establishes that
where the sum is over all divisors of . Möbius inversion then yields
where is the Möbius function. (This formula was known to Gauss.) The main term occurs for , and it is not difficult to bound the remaining terms. The "Riemann hypothesis" statement depends on the fact that the largest proper divisor of can be no larger than .





</doc>
<doc id="23693" url="https://en.wikipedia.org/wiki?curid=23693" title="Conflict of laws">
Conflict of laws

Conflict of laws (sometimes called private international law) concerns relations across different legal jurisdictions between natural persons, companies, corporations and other legal entities, their legal obligations and the appropriate forum and procedure for resolving disputes between them. Conflict of laws especially affects private international law, but may also affect domestic legal disputes e.g. determination of which state law applies in the United States, or where a contract makes incompatible reference to more than one legal framework.

Courts faced with a choice of law issue have a two-stage process:


In divorce cases, when a court is attempting to distribute marital property, if the divorcing couple is local and the property is local, then the court applies its domestic law "lex fori". The case becomes more complicated if foreign elements are thrown into the mix, such as when the place of marriage is different from the territory where divorce was filed; when the parties' nationalities and residences do not match; when there is property in a foreign jurisdiction; or when the parties have changed residence several times during the marriage.

Whereas commercial agreements or prenuptial agreements generally do not require legal formalities to be observed, when married couples enter a property agreement (agreement for the division of property at the termination of the marriage), stringent requirements are imposed, including notarization, witnesses, special acknowledgment forms. In some countries, these must be filed (or docketed) with a domestic court, and the terms must be "so ordered" by a judge. This is done in order to ensure that no undue influence or oppression has been exerted by one spouse against the other. Upon presenting a property agreement between spouses to a court of divorce, that court will generally assure itself of the following factors: signatures, legal formalities, intent, later intent, free will, lack of oppression, reasonableness and fairness, consideration, performance, reliance, later repudiation in writing or by conduct, and whichever other concepts of contractual bargaining apply in the context.

Many contracts and other forms of legally binding agreement include a jurisdiction or arbitration clause specifying the parties' choice of venue for any litigation (called a forum selection clause). In the EU, this is governed by the Rome I Regulation. Choice of law clauses may specify which laws the court or tribunal should apply to each aspect of the dispute. This matches the substantive policy of freedom of contract and will be determined by the law of the state where the choice of law clause confers its competence. Oxford Professor Adrian Briggs suggests that this is doctrinally problematic as it is emblematic of 'pulling oneself up by the bootstraps'. 

Judges have accepted that the principle of party autonomy allows the parties to select the law most appropriate to their transaction. This judicial acceptance of subjective intent excludes the traditional reliance on objective connecting factors; it also harms consumers as vendors often impose one-sided contractual terms selecting a venue far from the buyer's home or workplace. Contractual clauses relating to consumers, employees, and insurance beneficiaries are regulated under additional terms set out in Rome I, which may modify the contractual terms imposed by vendors.

To apply one national legal system as against another may never be an entirely satisfactory approach. The parties' interests may always be better protected by applying a law conceived with international realities in mind. The Hague Conference on Private International Law is a treaty organization that oversees conventions designed to develop a uniform system. The deliberations of the conference have recently been the subject of controversy over the extent of cross-border jurisdiction on electronic commerce and defamation issues. There is a general recognition that there is a need for an international law of contracts: for example, many nations have ratified the "Vienna Convention on the International Sale of Goods", the "Rome Convention on the Law Applicable to Contractual Obligations" offers less specialized uniformity, and there is support for the "UNIDROIT Principles of International Commercial Contracts", a private restatement, all of which represent continuing efforts to produce international standards as the Internet and other technologies encourage ever more interstate commerce.

Other branches of the law are less well served and the dominant trend remains the role of the forum law rather than a supranational system for conflict purposes. Even the EU, which has institutions capable of creating uniform rules with direct effect, has failed to produce a universal system for the common market. Nevertheless, the Treaty of Amsterdam does confer authority on the community's institutions to legislate by Council Regulation in this area with supranational effect. Article 177 would give the Court of Justice jurisdiction to interpret and apply their principles so, if the political will arises, uniformity may gradually emerge in letter. Whether the domestic courts of the Member States would be consistent in applying those letters is speculative.





</doc>
<doc id="23696" url="https://en.wikipedia.org/wiki?curid=23696" title="Timeline of programming languages">
Timeline of programming languages

This is a record of historically important programming languages, by decade.




</doc>
<doc id="23698" url="https://en.wikipedia.org/wiki?curid=23698" title="International Fixed Calendar">
International Fixed Calendar

The International Fixed Calendar (also known as the Cotsworth plan, the Eastman plan, the 13 Month calendar or the Equal Month calendar) is a solar calendar proposal for calendar reform designed by Moses B. Cotsworth, who presented it in 1902. It divides the solar year into 13 months of 28 days each. It is therefore a perennial calendar, with every date fixed to the same weekday every year. Though it was never officially adopted in any country, entrepreneur George Eastman adopted it for use in his Eastman Kodak Company, where it was used from 1928 to 1989.

The calendar year has 13 months with 28 days each, divided into exactly 4 weeks (13 × 28 = 364). An extra day added as a holiday at the end of the year (after December 28, i.e. equal December 31 Gregorian), sometimes called "Year Day", does not belong to any week and brings the total to 365 days. Each year coincides with the corresponding Gregorian year, so January 1 in the Cotsworth calendar always falls on Gregorian January 1. Twelve months are named and ordered the same as those of the Gregorian calendar, except that the extra month is inserted between June and July, and called "Sol". Situated in mid-summer (from the point of view of its Northern Hemisphere authors) and including the mid-year "solstice", the name of the new month was chosen in homage to the sun.

Leap year in the International Fixed Calendar contains 366 days, and its occurrence follows the Gregorian rule. There is a leap year in every year whose number is divisible by 4, but not if the year number is divisible by 100, unless it is also divisible by 400. So although the year 2000 was a leap year, the years 1700, 1800, and 1900 were common years. The International Fixed Calendar inserts the extra day in leap year as June 29 - between Saturday June 28 and Sunday Sol 1.

Each month begins on a Sunday, and ends on a Saturday; consequently, every year begins on Sunday. Neither Year Day nor Leap Day are considered to be part of any week; they are preceded by a Saturday and are followed by a Sunday.

All the months look like this:
Today on this calendar will be .

The following shows how the 13 months and extra days of the International Fixed Calendar occur in relation to the dates of the Gregorian calendar:

<nowiki>*</nowiki>These Gregorian dates between March and June are a day earlier in a Gregorian leap year. March in the Fixed Calendar always has a fixed number of days (28), and includes the Gregorian February 29 (on Gregorian leap years).

Lunisolar calendars, with fixed weekdays, existed in many ancient cultures, with certain holidays always falling on the same dates of the month and days of the week. 

The simple idea of a 13-month perennial calendar has been around since at least the middle of the 18th century. Versions of the idea differ mainly on how the months are named, and the treatment of the extra day in leap year.

The "Georgian calendar" was proposed in 1745 by the Rev. Hugh Jones, an American colonist from Maryland writing under the pen name Hirossa Ap-Iccim. The author named the plan, and the thirteenth month, after King George II of Great Britain. The 365th day each year was to be set aside as Christmas. The treatment of leap year varied from the Gregorian rule, however; and the year would begin closer to the winter solstice. In a later version of the plan, published in 1753, the 13 months were all renamed for Christian saints.

In 1849 the French philosopher Auguste Comte (1798–1857) proposed the 13-month "Positivist Calendar", naming the months: Moses, Homer, Aristotle, Archimedes, Caesar, St. Paul, Charlemagne, Dante, Gutenberg, Shakespeare, Descartes, Frederic and Bichat. The days of the year were likewise dedicated to "saints" in the Positivist Religion of Humanity. Positivist weeks, months, and years begin with Monday instead of Sunday. Comte also reset the year number, beginning the era of his calendar (year 1) with the Gregorian year 1789. For the extra days of the year not belonging to any week or month, Comte followed the pattern of Ap-Iccim (Jones), ending each year with a festival on the 365th day, followed by a subsequent feast day occurring only in leap years.

Whether Moses Cotsworth was familiar with the 13-month plans that preceded his International Fixed Calendar is not known. He did follow Ap-Iccim (Jones) in designating the 365th day of the year as Christmas. His suggestion was that this last day of the year should be designated a Sunday, and hence, because the following day would be New Year's Day and a Sunday also, he called it a Double Sunday. Since Cotsworth's goal was a simplified, more "rational" calendar for business and industry, he would carry over all the features of the Gregorian calendar consistent with this goal, including the traditional month names, the week beginning on Sunday (still traditionally used in US, but uncommon in most other countries and in the ISO (International Organization for Standardization) week standard, starting their weeks on Monday), and the Gregorian leap-year rule.

To promote Cotsworth's calendar reform the International Fixed Calendar League was founded in 1923, just after the plan was selected by the League of Nations as the best of 130 calendar proposals put forward. Sir Sandford Fleming, the inventor and driving force behind worldwide adoption of standard time, became the first president of the IFCL. The League opened offices in London and later in Rochester, New York. George Eastman, of the Eastman Kodak Company, became a fervent supporter of the IFC, and instituted its use at Kodak. The International Fixed Calendar League ceased operations shortly after the calendar plan failed to win final approval of the League of Nations in 1937.

The several advantages of the International Fixed Calendar are mainly related to its organization.






</doc>
<doc id="23703" url="https://en.wikipedia.org/wiki?curid=23703" title="Potential energy">
Potential energy

In physics, potential energy is the energy held by an object because of its position relative to other objects, stresses within itself, its electric charge, or other factors.

Common types of potential energy include the gravitational potential energy of an object that depends on its mass and its distance from the center of mass of another object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule, which has the symbol J.

The term "potential energy" was introduced by the 19th-century Scottish engineer and physicist William Rankine, although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that the total work done by these forces on the body depends only on the initial and final positions of the body in space. These forces, that are called "conservative forces", can be represented at every point in space by vectors expressed as gradients of a certain scalar function called "potential".

Since the work of potential forces acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, there is a function known as "potential" that can be evaluated at the two positions to determine this work.

There are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of mutual positions of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their mutual positions.

Forces derivable from a potential are also called conservative forces. The work done by a conservative force is
where formula_2 is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy. Common notations for potential energy are "PE", "U", "V", and "E".

Potential energy is the energy by virtue of an object's position relative to other objects. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching a spring or lifting a mass is performed by an external force that works against the force field of the potential. This work is stored in the force field, which is said to be stored as potential energy. If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.

Consider a ball whose mass is m and whose height is h. The acceleration g of free fall is approximately constant, so the weight force of the ball mg is constant. Force × displacement gives the work done, which is equal to the gravitational potential energy, thus

The more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.

Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points (if the work is done by a conservative force), then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
If the work for an applied force is independent of the path, then the work done by the force is evaluated at the start and end of the trajectory of the point of application. This means that there is a function "U"(x), called a "potential," that can be evaluated at the two points x and x to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is 
where "C" is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, "C", from A to B.

The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.

In this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve "C" takes a special form if the force F is related to a scalar field φ(x) so that
In this case, work along the curve is given by
which can be evaluated using the gradient theorem to obtain
This shows that when forces are derivable from a scalar field, the work of those forces along a curve "C" is computed by evaluating the scalar field at the start point "A" and the end point "B" of the curve. This means the work integral does not depend on the path between "A" and "B" and is said to be independent of the path.

Potential energy "U"=-φ(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is

In this case, the application of the del operator to the work function yields,
and the force F is said to be "derivable from a potential." This also necessarily implies that F must be a conservative vector field. The potential "U" defines a force F at every point x in space, so the set of forces is called a force field.

Given a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve γ(t)=r(t) from γ(a)=A to γ(b)=B, and computing,

For the force field F, let v= dr/dt, then the gradient theorem yields,

The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is

Examples of work that can be computed from potential functions are gravity and spring forces.

For small height changes, gravitational potential energy can be computed using

where m is the mass in kg, g is the local gravitational field (9.8 metres per second squared on earth), h is the height above a reference level in metres, and U is the energy in joules.

In classical physics, gravity exerts a constant downward force F=(0, 0, "F") on the center of mass of a body moving near the surface of the Earth. The work of gravity on a body moving along a trajectory r(t) = ("x"(t), "y"(t), "z"(t)), such as the track of a roller coaster is calculated using its velocity, v=("v", "v", "v"), to obtain
where the integral of the vertical component of velocity is the vertical distance. The work of gravity depends only on the vertical movement of the curve r(t).

A horizontal spring exerts a force F = (−"kx", 0, 0) that is proportional to its deformation in the axial or "x" direction. The work of this spring on a body moving along the space curve s("t") = ("x"("t"), "y"("t"), "z"("t")), is calculated using its velocity, v = ("v", "v", "v"), to obtain
For convenience, consider contact with the spring occurs at "t" = 0, then the integral of the product of the distance "x" and the "x"-velocity, "xv", is "x"/2.

The function 
is called the potential energy of a linear spring.

Elastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.

The gravitational potential function, also known as gravitational potential energy, is:

The negative sign follows the convention that work is gained from a loss of potential energy.

The gravitational force between two bodies of mass "M" and "m" separated by a distance "r" is given by Newton's law

where formula_19 is a vector of length 1 pointing from "M" to "m" and "G" is the gravitational constant.

Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r(t) to r(t) is given by
The position and velocity of the mass "m" are given by

where e and e are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,

This calculation uses the fact that

The electrostatic force exerted by a charge "Q" on another charge "q" separated by a distance "r" is given by Coulomb's Law

where formula_19 is a vector of length 1 pointing from "Q" to "q" and "ε" is the vacuum permittivity. This may also be written using Coulomb constant .

The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function

The potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state; it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used; therefore it can be chosen based on convenience.

Typically the potential energy of a system depends on the "relative" positions of its components only, so the reference state can also be expressed in terms of relative positions.

Gravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.

Consider a book placed on top of a table. As the book is raised from the floor to the table, some external force works against the gravitational force. If the book falls back to the floor, the "falling" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation, and sound by the impact.

The factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. "Height" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.

The strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant ("standard gravity"). In this case, a simple expression for gravitational potential energy can be derived using the "W" = "Fd" equation for work, and the equation

The amount of gravitational potential energy held by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember "W = Fd"). The upward force required while moving at a constant velocity is equal to the weight, "mg", of an object, so the work done in lifting it through a height "h" is the product "mgh". Thus, when accounting only for mass, gravity, and altitude, the equation is:
where "U" is the potential energy of the object relative to its being on the Earth's surface, "m" is the mass of the object, "g" is the acceleration due to gravity, and "h" is the altitude of the object. If "m" is expressed in kilograms, "g" in m/s and "h" in metres then "U" will be calculated in joules.

Hence, the potential difference is

However, over large variations in distance, the approximation that "g" is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy, we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance "r" between the two bodies. Using that definition, the gravitational potential energy of a system of masses "m" and "M" at a distance "r" using gravitational constant "G" is

where "K" is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that "K"=0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making "U" negative; for why this is physically reasonable, see below.

Given this formula for "U", the total potential energy of a system of "n" bodies is found by summing, for all formula_31 pairs of two bodies, the potential energy of the system of those two bodies.

Considering the system of bodies as the combined set of small particles the bodies consist of, and applying the previous on the particle level we get the negative gravitational binding energy. This potential energy is more strongly negative than the total potential energy of the system of bodies as such since it also includes the negative gravitational binding energy of each body. The potential energy of the system of bodies as such is the negative of the energy needed to separate the bodies from each other to infinity, while the gravitational binding energy is the energy needed to separate all particles from each other to infinity.
therefore,

As with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite "r" over another, there seem to be only two reasonable choices for the distance at which "U" becomes zero: formula_34 and formula_35. The choice of formula_36 at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.

The singularity at formula_34 in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with formula_36 for formula_34, would result in potential energy being positive, but infinitely large for all nonzero values of "r", and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and "r" is always non-zero in practice, the choice of formula_36 at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.

The negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.

Gravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example, in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.

Gravitational potential energy is also used to power clocks in which falling weights operate the mechanism. It's also used by counterweights for lifting up an elevator, crane, or sash window.
Roller coasters are an entertaining way to utilize potential energy – chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.

Another practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline. In some cases the kinetic energy obtained from the potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).

Chemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.

The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.

An object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).

Electrostatic potential energy between two bodies in space is obtained from the force exerted by a charge "Q" on another charge "q" which is given by

where formula_19 is a vector of length 1 pointing from "Q" to "q" and "ε" is the vacuum permittivity. This may also be written using Coulomb's constant .

If the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects. The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.

The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by
typically given in "J" for Joules. A related quantity called "electric potential" (commonly denoted with a "V" for voltage) is equal to the electric potential energy per unit charge.

The energy of a magnetic moment formula_44 in an externally produced magnetic B-field has potential energy

The magnetization in a field is

where the integral can be over all space or, equivalently, where is nonzero.
Magnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be higher the further they are apart and lower the closer they are. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.

Nuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.

Nuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them can have less mass than if they were individually free, in which case this mass difference can be liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.

Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.

For example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by formula_47 or formula_48, corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass "M" and "m" separated by a distance "r" is
The gravitational potential (specific energy) of the two bodies is
where formula_51 is the reduced mass.

The work done against gravity by moving an infinitesimal mass from point A with formula_52 to point B with formula_53 is formula_54 and the work done going back the other way is formula_55 so that the total work done in moving from A to B and returning to A is
If the potential is redefined at A to be formula_57 and the potential at B to be formula_58, where formula_59 is a constant (i.e. formula_59 can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is
as before.

In practical terms, this means that one can set the zero of formula_62 and formula_47 anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).
A conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.




</doc>
<doc id="23704" url="https://en.wikipedia.org/wiki?curid=23704" title="Pyramid">
Pyramid

A pyramid (from "") is a structure whose outer surfaces are triangular and converge to a single step at the top, making the shape roughly a pyramid in the geometric sense. The base of a pyramid can be trilateral, quadrilateral, or of any polygon shape. As such, a pyramid has at least three outer triangular surfaces (at least four faces including the base). The square pyramid, with a square base and four triangular outer surfaces, is a common version.

A pyramid's design, with the majority of the weight closer to the ground, and with the pyramidion at the apex, means that less material higher up on the pyramid will be pushing down from above. This distribution of weight allowed early civilizations to create stable monumental structures.

Civilizations in many parts of the world have built pyramids. The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. For thousands of years, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt—the latter is the only one of the Seven Wonders of the Ancient World still remaining.

The Mesopotamians built the earliest pyramidal structures, called "ziggurats". In ancient times, these were brightly painted in gold/bronze. Since they were constructed of sun-dried mud-brick, little remains of them. Ziggurats were built by the Sumerians, Babylonians, Elamites, Akkadians, and Assyrians for local religions. Each ziggurat was part of a temple complex which included other buildings. The precursors of the ziggurat were raised platforms that date from the Ubaid period during the fourth millennium BC. The earliest ziggurats began near the end of the Early Dynastic Period. The latest Mesopotamian ziggurats date from the 6th century BC.

Built in receding tiers upon a rectangular, oval, or square platform, the ziggurat was a pyramidal structure with a flat top. Sun-baked bricks made up the core of the ziggurat with facings of fired bricks on the outside. The facings were often glazed in different colors and may have had astrological significance. Kings sometimes had their names engraved on these glazed bricks. The number of tiers ranged from two to seven. It is assumed that they had shrines at the top, but there is no archaeological evidence for this and the only textual evidence is from Herodotus. Access to the shrine would have been by a series of ramps on one side of the ziggurat or by a spiral ramp from base to summit.

The most famous pyramids are the Egyptian — huge structures built of brick or stone, some of which are among the world's largest constructions. They are shaped as a reference to the rays of the sun. Most pyramids had a polished, highly reflective white limestone surface, to give them a shining appearance when viewed from a distance. The capstone was usually made of hard stone – granite or basalt – and could be plated with gold, silver, or electrum and would also be highly reflective. After 2700 BC, the ancient Egyptians began building pyramids, until around 1700 BC. The first pyramid was erected during the Third Dynasty by the Pharaoh Djoser and his architect Imhotep. This step pyramid consisted of six stacked mastabas. The largest Egyptian pyramids are those at the Giza pyramid complex.

The age of the pyramids reached its zenith at Giza in 2575–2150 BC. Ancient Egyptian pyramids were in most cases placed west of the river Nile because the divine pharaoh's soul was meant to join with the sun during its descent before continuing with the sun in its eternal round. As of 2008, some 135 pyramids have been discovered in Egypt. The Great Pyramid of Giza is the largest in Egypt and one of the largest in the world. At 481ft, it was the tallest building in the world until Lincoln Cathedral was finished in 1311 AD. The base is over in area. The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. It is the only one to survive into modern times. The Ancient Egyptians covered the faces of pyramids with polished white limestone, containing great quantities of fossilized seashells. Many of the facing stones have fallen or have been removed and used for construction in Cairo.

Most pyramids are located near Cairo, with only one royal pyramid being located south of Cairo, at the Abydos temple complex. The pyramid at Abydos, Egypt were commissioned by Ahmose I who founded the 18th Dynasty and the New Kingdom. The building of pyramids began in the Third Dynasty with the reign of King Djoser. Early kings such as Snefru built several pyramids, with subsequent kings adding to the number of pyramids until the end of the Middle Kingdom.

The last king to build royal pyramids was Ahmose, with later kings hiding their tombs in the hills, such as those in the Valley of the Kings in Luxor's West Bank. In Medinat Habu, or Deir el-Medina, smaller pyramids were built by individuals. Smaller pyramids with steeper sides were also built by the Nubians who ruled Egypt in the Late Period.

While pyramids are associated with Egypt, the nation of Sudan has 220 extant pyramids, the most numerous in the world.
Nubian pyramids were constructed (roughly 240 of them) at three sites in Sudan to serve as tombs for the kings and queens of Napata and Meroë. The pyramids of Kush, also known as Nubian Pyramids, have different characteristics than the pyramids of Egypt. The Nubian pyramids were constructed at a steeper angle than Egyptian ones. Pyramids were still being built in Sudan as late as 200 AD.

One of the unique structures of Igbo culture was the Nsude Pyramids, at the Nigerian town of Nsude, northern Igboland. Ten pyramidal structures were built of clay/mud. The first base section was 60 ft. in circumference and 3 ft. in height. The next stack was 45 ft. in circumference. Circular stacks continued, till it reached the top. The structures were temples for the god Ala, who was believed to reside at the top. A stick was placed at the top to represent the god's residence. The structures were laid in groups of five parallel to each other. Because it was built of clay/mud like the Deffufa of Nubia, time has taken its toll requiring periodic reconstruction.

Pausanias (2nd century AD) mentions two buildings resembling pyramids, one, 19 kilometres (12 mi) southwest of the still standing structure at Hellenikon, a common tomb for soldiers who died in a legendary struggle for the throne of Argos and another which he was told was the tomb of Argives killed in a battle around 669/8 BC. Neither of these still survive and there is no evidence that they resembled Egyptian pyramids.
There are also at least two surviving pyramid-like structures still available to study, one at Hellenikon and the other at Ligourio/Ligurio, a village near the ancient theatre Epidaurus. These buildings were not constructed in the same manner as the pyramids in Egypt. They do have inwardly sloping walls but other than those there is no obvious resemblance to Egyptian pyramids. They had large central rooms (unlike Egyptian pyramids) and the Hellenikon structure is rectangular rather than square, which means that the sides could not have met at a point. The stone used to build these structures was limestone quarried locally and was cut to fit, not into freestanding blocks like the Great Pyramid of Giza.

The dating of these structures has been made from the pot shards excavated from the floor and on the grounds. The latest dates available from scientific dating have been estimated around the 5th and 4th centuries. Normally this technique is used for dating pottery, but here researchers have used it to try to date stone flakes from the walls of the structures. This has created some debate about whether or not these structures are actually older than Egypt, which is part of the Black Athena controversy.

Mary Lefkowitz has criticised this research. She suggests that some of the research was done not to determine the reliability of the dating method, as was suggested, but to back up an assumption of age and to make certain points about pyramids and Greek civilization. She notes that not only are the results not very precise, but that other structures mentioned in the research are not in fact pyramids, e.g. a tomb alleged to be the tomb of Amphion and Zethus near Thebes, a structure at Stylidha (Thessaly) which is just a long wall, etc. She also notes the possibility that the stones that were dated might have been recycled from earlier constructions. She also notes that earlier research from the 1930s, confirmed in the 1980s by Fracchia was ignored. She argues that they undertook their research using a novel and previously untested methodology in order to confirm a predetermined theory about the age of these structures.

Liritzis responded in a journal article published in 2011, stating that Lefkowitz failed to understand and misinterpreted the methodology.

The Pyramids of Güímar refer to six rectangular pyramid-shaped, terraced structures, built from lava stone without the use of mortar. They are located in the district of Chacona, part of the town of Güímar on the island of Tenerife in the Canary Islands. The structures have been dated to the 19th century and their original function explained as a byproduct of contemporary agricultural techniques.

Autochthonous Guanche traditions as well as surviving images indicate that similar structures (also known as, "Morras", "Majanos", "Molleros", or "Paredones") could once have been found in many locations on the island. However, over time they have been dismantled and used as a cheap building material. In Güímar itself there were nine pyramids, only six of which survive.

There are many square flat-topped mound tombs in China. The First Emperor Qin Shi Huang (circa 221 BC, who unified the 7 pre-Imperial Kingdoms) was buried under a large mound outside modern day Xi'an. In the following centuries about a dozen more Han Dynasty royals were also buried under flat-topped pyramidal earthworks.

A number of Mesoamerican cultures also built pyramid-shaped structures. Mesoamerican pyramids were usually stepped, with temples on top, more similar to the Mesopotamian ziggurat than the Egyptian pyramid.

The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. Constructed from the 3rd century BC to the 9th century AD, this pyramid is considered the largest monument ever constructed anywhere in the world, and is still being excavated. The third largest pyramid in the world, the Pyramid of the Sun, at Teotihuacan is also located in Mexico. There is an unusual pyramid with a circular plan at the site of Cuicuilco, now inside Mexico City and mostly covered with lava from an eruption of the Xitle Volcano in the 1st century BC. There are several circular stepped pyramids called Guachimontones in Teuchitlán, Jalisco as well.

Pyramids in Mexico were often used as places of human sacrifice. For the re-consecration of Great Pyramid of Tenochtitlan in 1487, Where, according to Michael Harner, "one source states 20,000, another 72,344, and several give 80,400".

Many pre-Columbian Native American societies of ancient North America built large pyramidal earth structures known as platform mounds. Among the largest and best-known of these structures is Monks Mound at the site of Cahokia in what became Illinois, completed around 1100 AD, which has a base larger than that of the Great Pyramid at Giza. Many of the mounds underwent multiple episodes of mound construction at periodic intervals, some becoming quite large. They are believed to have played a central role in the mound-building peoples' religious life and documented uses include semi-public chief's house platforms, public temple platforms, mortuary platforms, charnel house platforms, earth lodge/town house platforms, residence platforms, square ground and rotunda platforms, and dance platforms. Cultures who built substructure mounds include the Troyville culture, Coles Creek culture, Plaquemine culture and Mississippian cultures.

The 27-metre-high Pyramid of Cestius was built by the end of the 1st century BC and still exists today, close to the Porta San Paolo. Another one, named "Meta Romuli", standing in the "Ager Vaticanus" (today's Borgo), was destroyed at the end of the 15th century.

Pyramids have occasionally been used in Christian architecture of the feudal era, e.g. as the tower of Oviedo's Gothic Cathedral of San Salvador.

Many giant granite temple pyramids were made in South India during the Chola Empire, many of which are still in religious use today. Examples of such pyramid temples include Brihadisvara Temple at Thanjavur, the Brihadisvara Temple at Gangaikonda Cholapuram and the Airavatesvara Temple at Darasuram. However, temple pyramid the largest area is the Ranganathaswamy Temple in Srirangam, Tamil Nadu. The Thanjavur temple was built by Raja Raja Chola in the 11th century. The Brihadisvara Temple was declared a World Heritage Site by UNESCO in 1987; the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram were added as extensions to the site in 2004.

Next to menhir, stone table, and stone statue; Austronesian megalithic culture in Indonesia also featured earth and stone step pyramid structures called "punden berundak" as discovered in Pangguyangan site near Cisolok and in Cipari near Kuningan. The construction of stone pyramids is based on the native beliefs that mountains and high places are the abode for the spirit of the ancestors.

The step pyramid is the basic design of 8th century Borobudur Buddhist monument in Central Java. However the later temples built in Java were influenced by Indian Hindu architecture, as displayed by the towering spires of Prambanan temple. In the 15th century Java during late Majapahit period saw the revival of Austronesian indigenous elements as displayed by Sukuh temple that somewhat resemble Mesoamerican pyramid, and also stepped pyramids of Mount Penanggungan.

Andean cultures had used pyramids in various architectural structures such as the ones in Caral, Túcume and Chavín de Huantar.


With the Egyptian Revival movement in the nineteenth and early twentieth century, pyramids were becoming more common in funerary architecture. This style was especially popular with tycoons in the US. Hunt's Tomb in Phoenix, Arizona and Schoenhofen Pyramid Mausoleum in Chicago are some of the notable examples but Henry Bergh, Charles Debrille Poston and many others were buried in pyramid shape mausoleums. People in Europe also adopted this style. One of them was Branislav Nušić who was buried in one such tomb. Even today some people build pyramid tombs for themselves. Nicolas Cage bought a pyramid tomb for himself in a famed New Orleans graveyard.



</doc>
<doc id="23705" url="https://en.wikipedia.org/wiki?curid=23705" title="Predestination">
Predestination

Predestination, in theology, is the doctrine that all events have been willed by God, usually with reference to the eventual fate of the individual soul. Explanations of predestination often seek to address the "paradox of free will", whereby God's omniscience seems incompatible with human free will. In this usage, predestination can be regarded as a form of religious determinism; and usually predeterminism, also known as theological determinism.

There is some disagreement among scholars regarding the views on predestination of first-century AD Judaism, out of which Christianity came. Josephus wrote during the first century that the three main Jewish sects differed on this question. He argued that the Essenes and Pharisees argued that God's providence orders all human events, but the Pharisees still maintained that people are able to choose between right and wrong. He wrote that the Sadducees did not have a doctrine of providence.
The biblical scholar N. T. Wright argues that Josephus's portrayal of these groups is incorrect, and that the Jewish debates referenced by Josephus should be seen as having to do with God's work to liberate Israel rather than philosophical questions about predestination. Wright asserts that Essenes were content to wait for God to liberate Israel while Pharisees believed Jews needed to act in cooperation with God. John Barclay responded that Josephus's description was an over-simplification and there were likely to be complex differences between these groups which may have been similar to those described by Josephus. Francis Watson has also argued on the basis of 4 Ezra, a document dated to the first century AD, that Jewish beliefs in predestination are primarily concerned with God's choice to save some individual Jews.

In the New Testament, Romans 8–11 presents a statement on predestination. In Romans 8:28–30, Paul writes, 
Biblical scholars have interpreted this passage in several ways. Many say this only has to do with service, and is not about salvation. The Catholic biblical commentator Brendan Byrne wrote that the predestination mentioned in this passage should be interpreted as applied to the Christian community corporately rather than individuals. Another Catholic commentator, Joseph Fitzmyer, wrote that this passage teaches that God has predestined the salvation of all humans. Douglas Moo, a Protestant biblical interpreter, reads the passage as teaching that God has predestined a certain set of people to salvation, and predestined the remainder of humanity to reprobation (damnation). Similarly, Wright's interpretation is that in this passage Paul teaches that God will save those whom he has chosen, but Wright also emphasizes that Paul does not intend to suggest that God has eliminated human free will or responsibility. Instead, Wright asserts, Paul is saying that God's will works through that of humans to accomplish salvation.

Origen, writing in the third century, taught that God's providence extends to every individual. He believed God's predestination was based on God's foreknowledge of every individual's merits, whether in their current life or a previous life.

Later in the fourth and fifth centuries, Augustine of Hippo (354–430) also taught that God orders all things while preserving human freedom. Prior to 396, Augustine believed that predestination was based on God's foreknowledge of whether individuals would believe, that God's grace was "a reward for human assent". Later, in response to Pelagius, Augustine said that the sin of pride consists in assuming that "we are the ones who choose God or that God chooses us (in his foreknowledge) because of something worthy in us", and argued that it is God's grace that causes the individual act of faith. Scholars are divided over whether Augustine's teaching implies double predestination, or the belief that God chooses some people for damnation as well as some for salvation. Catholic scholars tend to deny that he held such a view while some Protestants and secular scholars affirm that Augustine did believe in double predestination.

Augustine's position raised objections. Julian of Eclanum expressed the view that Augustine was bringing Manichean thoughts into the church. For Vincent of Lérins, this was a disturbing innovation. This new tension eventually became obvious with the confrontation between Augustine and Pelagius culminating in condemnation of Pelagianism (as interpreted by Augustine) at the Council of Ephesus in 431. Pelagius denied Augustine's view of predestination in order to affirm that salvation is achieved by an act of free will.

The Council of Arles in the late fifth century condemned the position "that some have been condemned to death, others have been predestined to life", though this may seem to follow from Augustine's teaching. The Second Council of Orange in 529 also condemned the position that "some have been truly predestined to evil by divine power".

In the eighth century, John of Damascus emphasized the freedom of the human will in his doctrine of predestination, and argued that acts arising from peoples' wills are not part of God's providence at all. Damascene teaches that people's good actions are done in cooperation with God, but are not caused by him.

Gottschalk of Orbais, a ninth-century Saxon monk, argued that God predestines some people to hell as well as predestining some to heaven, a view known as double predestination. He was condemned by several synods, but his views remained popular. Irish theologian John Scottus Eriugena wrote a refutation of Gottschalk. Eriugena abandoned Augustine's teaching on predestination. He wrote that God's predestination should be equated with his foreknowledge of people's choices.

In the twelfth century, Thomas Aquinas taught that God predestines certain people to the beatific vision based solely on his own goodness rather than that of creatures. Aquinas also believed that people are free in their choices, fully cause their own sin, and are solely responsible for it. According to Aquinas, there are several ways in which God wills actions. He directly wills the good, indirectly wills evil consequences of good things, and only permits evil. Aquinas held that in permitting evil, God does not will it to be done or not to be done.

In the thirteenth century, William of Ockham taught that God does not cause human choices and equated predestination with divine foreknowledge. Though Ockham taught that God predestines based on people's foreseen works, he maintained that God's will was not constrained to do this.

John Calvin rejected the idea that God permits rather than actively decrees the damnation of sinners, as well as other evil. Calvin did not believe God to be guilty of sin, but rather he considered God inflicting sin upon his creations to be an unfathomable mystery (it would seem that God was simultaneously willing and not willing sin to befall humans). Though he maintained God's predestination applies to damnation as well as salvation, he taught that the damnation of the damned is caused by their sin, but that the salvation of the saved is solely caused by God. Other Protestant Reformers, including Huldrych Zwingli, also held double predestinarian views.

The Eastern Orthodox view was summarized by Bishop Theophan the Recluse in response to the question, "What is the relationship between the Divine provision and our free will?"

Catholicism teaches the doctrine of predestination, while rejecting the classical Calvinist view known as "double predestination". This means that while it is held that those whom God has elected to eternal life will infallibly attain it, and are therefore said to be predestined to salvation by God, those who perish are not predestined to damnation. According to the Catholic Church, God predestines no one to go to hell, for this, a willful turning away from God (a mortal sin) is necessary, and persistence in it until the end." Catholicism has been generally discouraging to human attempts to guess or predict the Divine Will. The "Catholic Encyclopedia" entry on predestination says:

Pope John Paul II wrote:
The Catholic Catechism says, "To God, all moments of time are present in their immediacy. When therefore he establishes his eternal plan of "predestination", he includes in it each person's free response to his grace." 

Catholics do not believe that any hints or evidence of the predestined status of individuals is available to humans, and predestination generally plays little or no part in Catholic teaching to the faithful, being a topic addressed in a professional theological context only.

Augustine of Hippo laid the foundation for much of the later Catholic teaching on predestination. His teachings on grace and free will were largely adopted by the Second Council of Orange (529), whose decrees were directed against the Semipelagians. Augustine wrote, 
Augustine also teaches that people have free will. For example, in "On Grace and Free Will", (see especially chapters II–IV) Augustine states that "He [God] has revealed to us, through His Holy Scriptures, that there is in man a free choice of will," and that "God's precepts themselves would be of no use to a man unless he had free choice of will, so that by performing them he might obtain the promised rewards." (chap. II)

Thomas Aquinas' views concerning predestination are largely in agreement with Augustine and can be summarized by many of his writings in his "Summa Theologiæ":

This table summarizes the classical views of three different Protestant beliefs.

Lutherans historically hold to unconditional election unto salvation. However, some do not believe that there are certain people that are predestined to salvation, but salvation is predestined for those who seek God. Lutherans believe Christians should be assured that they are among the predestined. However, they disagree with those who make predestination the source of salvation rather than Christ's suffering, death, and resurrection. Unlike some Calvinists, Lutherans do not believe in a predestination to damnation. Instead, Lutherans teach eternal damnation is a result of the unbeliever's sins, rejection of the forgiveness of sins, and unbelief.

Martin Luther's attitude towards predestination is set out in his "On the Bondage of the Will", published in 1525. This publication by Luther was in response to the published treatise by Desiderius Erasmus in 1524 known as "On Free Will". Luther based his views on Ephesians 2:8–10, which says:

The Belgic Confession of 1561 affirmed that God "delivers and preserves" from perdition "all whom he, in his eternal and unchangeable council, of mere goodness hath elected in Christ Jesus our Lord, without respect to their works" (Article XVI).
Calvinists believe that God picked those who he will save and bring with him to Heaven before the world was created. They also believe that those people God does not save will go to Hell. John Calvin thought people who were saved could never lose their salvation and the "elect" (those God saved) would know they were saved because of their actions.

In this common, loose sense of the term, to affirm or to deny predestination has particular reference to the Calvinist doctrine of unconditional election. In the Calvinist interpretation of the Bible, this doctrine normally has only pastoral value related to the assurance of salvation and the absolution of salvation by grace alone. However, the philosophical implications of the doctrine of election and predestination are sometimes discussed beyond these systematic bounds. Under the topic of the doctrine of God (theology proper), the predestinating decision of God cannot be contingent upon anything outside of himself, because all other things are dependent upon him for existence and meaning. Under the topic of the doctrines of salvation (soteriology), the predestinating decision of God is made from God's knowledge of his own will (Romans 9:15), and is therefore not contingent upon human decisions (rather, free human decisions are outworkings of the decision of God, which sets the total reality within which those decisions are made in exhaustive detail: that is, nothing left to chance). Calvinists do not pretend to understand how this works; but they are insistent that the Scriptures teach both the sovereign control of God and the responsibility and freedom of human decisions.

Calvinist groups use the term Hyper-Calvinism to describe Calvinistic systems that assert without qualification that God's intention to destroy some is equal to his intention to save others. Some forms of Hyper-Calvinism have racial implications, as when Dutch Calvinist theologian Franciscus Gomarus however argued that Jews, because of their refusal to worship Jesus Christ, were members of the non-elect, as also argued by John Calvin himself, based on I John 2:22–23 in The New Testament of the Bible. Some Dutch settlers in South Africa argued that black people were sons of Ham, whom Noah had cursed to be slaves, according to Genesis 9:18–19, or drew analogies between them and the Canaanites, suggesting a "chosen people" ideology similar to that espoused by proponents of the Jewish nation. This justified racial hierarchy on earth, as well as racial segregation of congregations, but did not exclude blacks from being part of the elect. Other Calvinists vigorously objected to these arguments (see Afrikaner Calvinism). 

Expressed sympathetically, the Calvinist doctrine is that God has mercy or withholds it, with particular consciousness of who are to be the recipients of mercy in Christ. Therefore, the particular persons are chosen, out of the total number of human beings, who will be rescued from enslavement to sin and the fear of death, and from punishment due to sin, to dwell forever in his presence. Those who are being saved are assured through the gifts of faith, the sacraments, and communion with God through prayer and increase of good works, that their reconciliation with him through Christ is settled by the sovereign determination of God's will. God also has particular consciousness of those who are passed over by his selection, who are without excuse for their rebellion against him, and will be judged for their sins.

Calvinists typically divide on the issue of predestination into infralapsarians (sometimes called 'sublapsarians') and supralapsarians. Infralapsarians interpret the biblical election of God to highlight his love (1 John 4:8; Ephesians 1:4b–5a) and chose his elect considering the situation after the Fall, while supralapsarians interpret biblical election to highlight God's sovereignty (Romans 9:16) and that the Fall was ordained by God's decree of election. In infralapsarianism, election is God's response to the Fall, while in supralapsarianism the Fall is part of God's plan for election. In spite of the division, many Calvinist theologians would consider the debate surrounding the infra- and supralapsarian positions one in which scant Scriptural evidence can be mustered in either direction, and that, at any rate, has little effect on the overall doctrine.

Some Calvinists decline to describe the eternal decree of God in terms of a sequence of events or thoughts, and many caution against the simplifications involved in describing any action of God in speculative terms. Most make distinctions between the positive manner in which God chooses some to be recipients of grace, and the manner in which grace is consciously withheld so that some are destined for everlasting punishments.

Debate concerning predestination according to the common usage concerns the destiny of the damned: whether God is just if that destiny is settled prior to the existence of any actual volition of the individual, and whether the individual is in any meaningful sense responsible for his destiny if it is settled by the eternal action of God.

Arminians hold that God does not predetermine, but instead infallibly knows who will believe and perseveringly be saved. This view is known as conditional election, because it states that election is conditional on the one who wills to have faith in God for salvation. Although God knows from the beginning of the world who will go where, the choice is still with the individual. The Dutch Calvinist theologian Franciscus Gomarus strongly opposed the views of Jacobus Arminius with his doctrine of supralapsarian predestination.

The Church of Jesus Christ of Latter-day Saints (LDS Church) rejects the doctrine of predestination, but does believe in foreordination. Foreordination, an important doctrine of the LDS Church, teaches that during the pre-mortal existence, God selected ("foreordained") particular people to fulfill certain missions ("callings") during their mortal lives. For example, prophets were foreordained to be the Lord's servants (see Jeremiah 1:5), all who receive the priesthood were foreordained to that calling, and Jesus was foreordained to enact the atonement.

The LDS Church teaches the doctrine of moral agency, the ability to choose and act for oneself, and decide whether to accept Christ's atonement.

Conditional election is the belief that God chooses for eternal salvation those whom he foresees will have faith in Christ. This belief emphasizes the importance of a person's free will. The counter-view is known as unconditional election, and is the belief that God chooses whomever he will, based solely on his purposes and apart from an individual's free will. It has long been an issue in Calvinist–Arminian debate. An alternative viewpoint is Corporate election, which distinguishes God's election and predestination for corporate entities such as the community "in Christ," and individuals who can benefit from that community's election and predestination so long as they continue belonging to that community.

Infralapsarianism (also called sublapsarianism) holds that predestination logically coincides with the preordination of Man's fall into sin. That is, God predestined sinful men for salvation. Therefore, according to this view, God is the ultimate cause, but not the proximate source or "author" of sin. Infralapsarians often emphasize a difference between God's decree (which is inviolable and inscrutable), and his revealed will (against which man is disobedient). Proponents also typically emphasize the grace and mercy of God toward all men, although teaching also that only some are predestined for salvation.

In common English parlance, the doctrine of predestination often has particular reference to the doctrines of Calvinism. The version of predestination espoused by John Calvin, after whom Calvinism is named, is sometimes referred to as "double predestination" because in it God predestines some people for salvation (i.e. unconditional election) and some for condemnation (i.e. Reprobation) which results by allowing the individual's own sins to condemn them. Calvin himself defines predestination as "the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. Not all are created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestined to life or to death."

On the spectrum of beliefs concerning predestination, Calvinism is the strongest form among Christians. It teaches that God's predestining decision is based on the knowledge of his own will rather than foreknowledge, concerning every particular person and event; and, God continually acts with entire freedom, in order to bring about his will in completeness, but in such a way that the freedom of the creature is not violated, "but rather, established".

Calvinists who hold the infralapsarian view of predestination usually prefer that term to "sublapsarianism," perhaps with the intent of blocking the inference that they believe predestination is on the basis of foreknowledge ("sublapsarian" meaning, assuming the fall into sin). The different terminology has the benefit of distinguishing the Calvinist double predestination version of infralapsarianism from Lutheranism's view that predestination is a mystery, which forbids the unprofitable intrusion of prying minds since God only reveals partial knowledge to the human race.

Supralapsarianism is the doctrine that God's decree of predestination for salvation and reprobation logically precedes his preordination of the human race's fall into sin. That is, God decided to save, and to damn; he then determined the means by which that would be made possible. It is a matter of controversy whether or not Calvin himself held this view, but most scholars link him with the infralapsarian position. It is known, however, that Calvin's successor in Geneva, Theodore Beza, held to the supralapsarian view.

Double predestination, or the double decree, is the doctrine that God actively reprobates, or decrees damnation of some, as well as salvation for those whom he has elected. Augustine made statements that on their own seem to teach such a doctrine, but in the context of his other writings it is not clear whether he held it. Augustine's doctrine of predestination does seem to imply a double predestinarian view. Gottschalk of Orbais taught it more explicitly in the ninth century, and Gregory of Rimini in the fourteenth. During the Protestant Reformation John Calvin also held double predestinarian views. John Calvin states: "By predestination we mean the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. All are not created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestinated to life or to death."

Open theism advocates the non-traditional Arminian view of election that predestination is corporate. In corporate election, God does not choose which individuals he will save prior to creation, but rather God chooses the church as a whole. Or put differently, God chooses what type of individuals he will save. Another way the New Testament puts this is to say that God chose the church in Christ (Eph. 1:4). In other words, God chose from all eternity to save all those who would be found in Christ, by faith in God. This choosing is not primarily about salvation from eternal destruction either but is about God's chosen agency in the world. Thus individuals have full freedom in terms of whether they become members of the church or not. Corporate election is thus consistent with the open view's position on God's omniscience, which states that God's foreknowledge does not determine the outcomes of individual free will.

Middle Knowledge is a concept that was developed by Jesuit theologian Luis de Molina, and exists under a doctrine called Molinism. It attempts to deal with the topic of predestination by reconciling God's sovereign providence with the notion of libertarian free will. The concept of Middle Knowledge holds that God has a knowledge of true pre-volitional counterfactuals for all free creatures. That is, what any individual creature with a free will (e.g. a human) would do under any given circumstance. God's knowledge of counterfactuals is reasoned to occur logically prior to his divine creative decree (that is, prior to creation), and after his knowledge of necessary truths. Thus, Middle Knowledge holds that before the world was created, God knew what every existing creature capable of libertarian freedom (e.g. every individual human) would freely choose to do in all possible circumstances. It then holds that based on this information, God elected from a number of these possible worlds, the world most consistent with his ultimate will, which is the actual world that we live in.

For example:


Based on this Middle Knowledge, God has the ability to actualise the world in which A is placed in a circumstance that he freely chooses to do what is consistent with Gods ultimate will. If God determined that the world most suited to his purposes is a world in which A would freely choose Y instead of Z, God can actualise a world in which Free Creature A finds himself in Circumstance B.

In this way, Middle Knowledge is thought of by its proponents to be consistent with any theological doctrines that assert God as having divine providence and man having a libertarian freedom (e.g. Calvinism, Catholicism, Lutheranism), and to offer a potential solution to the concerns that God's providence somehow nullifies man from having true liberty in his choices.

"Qadar" (, transliterated "qadar", meaning "fate", "divine fore-ordainment", "predestination") is the concept of divine destiny in Islam. It is one of Sunni Islam's six pillars of faith, along with belief in the Oneness of God, the Revealed Books, the Prophets of Islam, the Day of Resurrection and Angels.

In Islam, "predestination" is the usual English language rendering of a belief that Muslims call "al-qada wa al-qadar" in Arabic. The phrase means "the divine decree and the predestination". In Islam, God has predetermined, known, ordained, and is constantly creating every event that takes place in the world. This is entailed by his being omnipotent and omniscient. Sunni scholars hold that there is no contradiction in people's deeds (and naturally their choices) being created and predetermined by the creator, since they define free will to be the antonym of compulsion and coercion. People – in the Sunni perspective – do acknowledge that they are free, since they do not see anybody or anything forcing them to do whatever they chose to do. This, however, does not contradict that everything they do, including the choices they make, are predestined and predetermined by God. Consequently, people are already predestined to either heaven or hell at birth, as Sunnis believe; however, they will have no argument on the day of judgment since they never knew in advance what their fate would be, and they do acknowledge that they have choice; which is what moral responsibility comes with.

The concept of human will being predetermined by God's will is stated clearly in the Quran:
"Verily this (The Holy Quran) is no less than a Message to (all) the Worlds; (With profit) to whoever among you wills to go straight, "but ye shall not will except as God wills;" the Cherisher of the Worlds."

Predestination is rejected in Shiaism.

In rabbinic literature, there is much discussion as to the apparent contradiction between God's omniscience and free will. The representative view is that "Everything is foreseen; yet free will is given" (Rabbi Akiva, "Pirkei Avoth" 3:15). Based on this understanding, the problem is formally described as a paradox, perhaps beyond our understanding.

Hasdai Crescas resolved this dialectical tension by taking the position that free will doesn't exist. All of a person's actions are predetermined by the moment of their birth, and thus their judgment in the eyes of God (so to speak) is effectively preordained. In this scheme this is not a result of God's predetermining one's fate, but rather that the universe is deterministic. Crescas's views on this topic were rejected by Judaism at large. In later centuries this idea independently developed among some in the Chabad (Lubavitch) movement of Hasidic Judaism. Many individuals within Chabad take this view seriously, and hence effectively deny the existence of free will.

However, many Chabad (Lubavitch) Jews attempt to hold both views. They affirm as infallible their rebbe's teachings that God knows and controls the fate of all, yet at the same time affirm the classical Jewish belief in free will. The inherent contradiction between the two results in their belief that such contradictions are only "apparent", due to man's inherent lack of ability to understand greater truths and due to the fact that Creator and Created exist in different realities. The same idea is strongly repeated by Rambam (Mishneh Torah, Laws of Repentance, Chapter 5).

Many other Jews (Orthodox, Conservative, Reform and secular) affirm that since free will exists, then by definition one's fate is not preordained. It is held as a tenet of faith that whether God is omniscient or not, nothing interferes with mankind's free will. Some Jewish theologians, both during the medieval era and today, have attempted to formulate a philosophy in which free will is preserved, while also affirming that God has knowledge of what decisions people will make in the future. Whether or not these two ideas are mutually compatible, or whether there is a contradiction between the two, is still a matter of great study and interest in philosophy today.

Predestination is rejected in Zoroastrian teaching. Humans bear responsibility for all situations they are in, and in the way they act toward one another. Reward, punishment, happiness, and grief all depend on how individuals live their lives.






</doc>
<doc id="23706" url="https://en.wikipedia.org/wiki?curid=23706" title="Primitive notion">
Primitive notion

In mathematics, logic, philosophy, and formal systems, a primitive notion is a concept that is not defined in terms of previously defined concepts. It is often motivated informally, usually by an appeal to intuition and everyday experience. In an axiomatic theory, relations between primitive notions are restricted by axioms. Some authors refer to the latter as "defining" primitive notions by one or more axioms, but this can be misleading. Formal theories cannot dispense with primitive notions, under pain of infinite regress (per the regress problem).

For example, in contemporary geometry, "point", "line", and "contains" are some primitive notions. Instead of attempting to define them, their interplay is ruled (in Hilbert's axiom system) by axioms like "For every two points there exists a line that contains them both".

Alfred Tarski explained the role of primitive notions as follows:

An inevitable regress to primitive notions in the theory of knowledge was explained by Gilbert de B. Robinson:

The necessity for primitive notions is illustrated in several axiomatic foundations in mathematics:



</doc>
<doc id="23707" url="https://en.wikipedia.org/wiki?curid=23707" title="Priest">
Priest

A priest or priestess is a religious leader authorized to perform the sacred rituals of a religion, especially as a mediatory agent between humans and one or more deities. They also have the authority or power to administer religious rites; in particular, rites of sacrifice to, and propitiation of, a deity or deities. Their office or position is the priesthood, a term which also may apply to such persons collectively.

According to the trifunctional hypothesis of prehistoric Proto-Indo-European society, priests have existed since the earliest of times and in the simplest societies, most likely as a result of agricultural surplus and consequent social stratification. The necessity to read sacred texts and keep temple or church records helped foster literacy in many early societies. Priests exist in many religions today, such as all or some branches of Judaism, Christianity, Buddhism, Shinto and Hinduism. They are generally regarded as having privileged contact with the deity or deities of the religion to which they subscribe, often interpreting the meaning of events and performing the rituals of the religion. There is no common definition of the duties of priesthood between faiths; but generally it includes mediating the relationship between one's congregation, worshippers, and other members of the religious body, and its deity or deities, and administering religious rituals and rites. These often include blessing worshipers with prayers of joy at marriages, after a birth, and at consecrations, teaching the wisdom and dogma of the faith at any regular worship service, and mediating and easing the experience of grief and death at funerals – maintaining a spiritual connection to the afterlife in faiths where such a concept exists. Administering religious building grounds and office affairs and papers, including any religious library or collection of sacred texts, is also commonly a responsibility – for example, the modern term for clerical duties in a secular office refers originally to the duties of a cleric. The question of which religions have a "priest" depends on how the titles of leaders are used or translated into English. In some cases, leaders are more like those that other believers will often turn to for advice on spiritual matters, and less of a "person authorized to perform the sacred rituals." For example, clergy in Roman Catholicism and Eastern Orthodoxy are "priests", but in Protestant Christianity they are typically "minister" and "pastor". The terms "priest" and "priestess" are sufficiently generic that they may be used in an anthropological sense to describe the religious mediators of an unknown or otherwise unspecified religion.

In many religions, being a priest or priestess is a full-time position, ruling out any other career. Many Christian priests and pastors choose or are mandated to dedicate themselves to their churches and receive their living directly from their churches. In other cases it is a part-time role. For example, in the early history of Iceland the chieftains were titled "goði", a word meaning "priest". As seen in the saga of Hrafnkell Freysgoði, however, being a priest consisted merely of offering periodic sacrifices to the Norse gods and goddesses; it was not a full-time role, nor did it involve ordination.

In some religions, being a priest or priestess is by human election or human choice. In Judaism the priesthood is inherited in familial lines. In a theocracy, a society is governed by its priesthood.

The word "priest", is ultimately derived from Greek via Latin "presbyter", the term for "elder", especially elders of Jewish or Christian communities in late antiquity. The Latin "presbyter" ultimately represents Greek "presbúteros", the regular Latin word for "priest" being "sacerdos", corresponding to "hiereús".

It is possible that the Latin word was loaned into Old English, and only from Old English reached other Germanic languages via the Anglo-Saxon mission to the continent, giving Old Icelandic "prestr", Old Swedish "präster", Old High German "priast". Old High German also has the disyllabic "priester, priestar", apparently derived from Latin independently via Old French "presbtre".

Αn alternative theory makes "priest" cognate with Old High German "priast", "prest", from Vulgar Latin *"prevost" "one put over others", from Latin "praepositus" "person placed in charge".

That English should have only the single term "priest" to translate "presbyter" and "sacerdos" came to be seen as a problem in English Bible translations. The "presbyter" is the minister who both presides and instructs a Christian congregation, while the "sacerdos", offerer of sacrifices, or in a Christian context the eucharist, performs "mediatorial offices between God and man".

The feminine English noun, "priestess", was coined in the 17th century, to refer to female priests of the pre-Christian religions of classical antiquity. In the 20th century, the word was used in controversies surrounding the women ordained in the Anglican communion, who are referred to as "priests", irrespective of gender, and the term priestess is generally considered archaic in Christianity.

In historical polytheism, a priest administers the sacrifice to a deity, often in highly elaborate ritual. In the Ancient Near East, the priesthood also acted on behalf of the deities in managing their property.

Priestesses in antiquity often performed sacred prostitution, and in Ancient Greece, some priestesses such as Pythia, priestess at Delphi, acted as oracles.


In ancient Egyptian religion, the right and obligation to interact with the gods belonged to the pharaoh. He delegated this duty to priests, who were effectively bureaucrats authorized to act on his behalf. Priests staffed temples throughout Egypt, giving offerings to the cult images in which the gods were believed to take up residence and performing other rituals for their benefit. Little is known about what training may have been required of priests, and the selection of personnel for positions was affected by a tangled set of traditions, although the pharaoh had the final say. In the New Kingdom of Egypt, when temples owned great estates, the high priests of the most important cult—that of Amun at Karnak—were important political figures.

High-ranking priestly roles were usually held by men. Women were generally relegated to lower positions in the temple hierarchy, although some held specialized and influential positions, especially that of the God's Wife of Amun, whose religious importance overshadowed the High Priests of Amun in the Late Period.

In ancient Rome and throughout Italy, the ancient sanctuaries of Ceres and Proserpina were invariably led by female "sacerdotes", drawn from women of local and Roman elites. It was the only public priesthood attainable by Roman matrons and was held in great honor.

A Roman matron was any mature woman of the upper class, married or unmarried. Females could serve public cult as Vestal Virgins but few were chosen, and then only from young maidens of the upper class.


In ancient Israel the priests were required by the Law of Moses to be of direct patrilineal descent from Aaron, Moses' elder brother. In Exodus 30:22–25 God instructs Moses to make a holy anointing oil to consecrate the priests "for all of eternity." During the times of the two Jewish Temples in Jerusalem, the Aaronic priests were responsible for the daily and special Jewish holiday offerings and sacrifices within the temples, these offerings are known as the "korbanot".

In Hebrew the word "priest" is "kohen" (singular כהן "kohen", plural כּהנִים "kohanim"), hence the family names "Cohen", "Cahn", "Kahn", "Kohn", "Kogan", etc. These families are from the tribe of Levi (Levites) and in twenty-four instances are called by scripture as such (Jerusalem Talmud to Mishnaic tractate Maaser Sheini p. 31a). In Hebrew the word for "priesthood" is "kehunnah".

Since the destruction of the Second Temple, and (therefore) the cessation of the daily and seasonal temple ceremonies and sacrifices, Kohanim in traditional Judaism (Orthodox Judaism and to some extent, Conservative Judaism) continue to perform a number of priestly ceremonies and roles such as the Pidyon HaBen (redemption of a first-born son) ceremony and the Priestly Blessing, and have remained subject, particularly in Orthodox Judaism, to a number of restrictions, such as restrictions on certain marriages and ritual purity (see Kohanic disqualifications).

Orthodox Judaism regard the "kohanim" as being held in reserve for a future restored Temple. In all branches of Judaism, Kohanim do not perform roles of propitiation, sacrifice, or sacrament. Rather, a "kohen"'s principal religious function is to perform the Priestly Blessing, and, provided he is rabbinically qualified, to serve as an authoritative judge ("posek") and expositor of Jewish halakha law.

With the spread of Christianity and the formation of parishes, the Greek word "ἱερεύς" (hiereus), and Latin "sacerdos", which Christians had since the 3rd century applied to bishops and only in a secondary sense to presbyters, began in the 6th century to be used of presbyters, and is today commonly used of presbyters, distinguishing them from bishops.

Today the term "priest" is used in Roman Catholicism, Eastern Orthodoxy, Anglicanism, Oriental Orthodoxy, the Church of the East, and some branches of Lutheranism to refer to those who have been ordained to a ministerial position through receiving the sacrament of Holy Orders, although "presbyter" is also used. Since the Protestant Reformation, non-sacramental denominations are more likely to use the term "elder" to refer to their pastors. The Christian term "Priest" does not have an entry in the Anchor Bible Dictionary, but the dictionary does deal with the above-mentioned terms under the entry for "Sheep, Shepherd.".

The most significant liturgical acts reserved to priests in these traditions are the administration of the Sacraments, including the celebration of the Holy Mass or Divine Liturgy (the terms for the celebration of the Eucharist in the Latin and Byzantine traditions, respectively), and the Sacrament of Reconciliation, also called Confession. The sacraments of Anointing of the Sick (Extreme Unction) and Confirmation are also administered by priests, though in the Western tradition Confirmation is ordinarily celebrated by a bishop. In the East, Chrismation is performed by the priest (using oil specially consecrated by a bishop) immediately after Baptism, and Unction is normally performed by several priests (ideally seven), but may be performed by one if necessary. In the West, Holy Baptism may be celebrated by anyone. The Vatican catechism states that "According to Latin tradition, the spouses as ministers of Christ's grace mutually confer upon each other the sacrament of Matrimony". Thus marriage is a sacrament administered by the couple to themselves, but may be witnessed and blessed by a deacon, or priest (who usually administers the ceremony). In the East, Holy Baptism and Marriage (which is called "Crowning") may be performed only by a priest. If a person is baptized "in extremis" (i.e., when in fear of immediate death), only the actual threefold immersion together with the scriptural words () may be performed by a layperson or deacon. The remainder of the rite, and Chrismation, must still be performed by a priest, if the person survives. The only sacrament which may be celebrated only by a bishop is that of Ordination ("cheirotonia", "Laying-on of Hands"), or Holy Orders.

In these traditions, only men who meet certain requirements may become priests. In Roman Catholicism the canonical minimum age is twenty-five. Bishops may dispense with this rule and ordain men up to one year younger. Dispensations of more than a year are reserved to the Holy See (Can. 1031 §§1, 4.) A Catholic priest must be incardinated by his bishop or his major religious superior in order to engage in public ministry. In Orthodoxy, the normal minimum age is thirty (Can. 9 of Neocaesarea) but a bishop may dispense with this if needed. In neither tradition may priests marry after ordination. In the Roman Catholic Church, priests in the Latin Rite, which covers the vast majority of Roman Catholicism, must be celibate except under special rules for married clergy converting from certain other Christian confessions. Married men may become priests in Eastern Orthodoxy and the Eastern Catholic Churches, but in neither case may they marry after ordination, even if they become widowed. Candidates for bishop are chosen only from among the celibate. Orthodox priests will either wear a clerical collar similar to the above mentioned, or simply a very loose black robe that does not have a collar.

The role of a priest in the Anglican Communion is largely the same as within the Roman Catholic Church and Eastern Christianity, except that canon law in almost every Anglican province restricts the administration of confirmation to the bishop, just as with ordination. Whilst Anglican priests who are members of religious orders must remain celibate (although there are exceptions, such as priests in the Anglican Order of Cistercians), the secular clergy—bishops, priests, and deacons who are not members of religious orders—are permitted to marry before or after ordination (although in most provinces they are not permitted to marry a person of the same sex.) The Anglican churches, unlike the Roman Catholic or Eastern Christian traditions, have allowed the ordination of women as priests (referred to as "priests" not "priestesses") in some provinces since 1971. This practice remains controversial, however; a minority of provinces (10 out of the 38 worldwide) retain an all-male priesthood. Most Continuing Anglican churches do not ordain women to the priesthood.

As Anglicanism represents a broad range of theological opinion, its presbyterate includes priests who consider themselves no different in any respect from those of the Roman Catholic Church, and a minority who prefer to use the title "presbyter" in order to distance themselves from the more sacrificial theological implications which they associate with the word "priest". While "priest" is the official title of a member of the presbyterate in every Anglican province worldwide (retained by the Elizabethan Settlement), the ordination rite of certain provinces (including the Church of England) recognizes the breadth of opinion by adopting the title "The Ordination of Priests (also called Presbyters)." Even though both words mean 'elders' historically the term "priest" has been more associated with the "High Church" or Anglo-Catholic wing, whereas the term "minister" has been more commonly used in "Low Church" or Evangelical circles.

The general priesthood or the priesthood of all believers, is a Christian doctrine derived from several passages of the New Testament. It is a foundational concept of Protestantism. It is this doctrine that Martin Luther adduces in his 1520 "To the Christian Nobility of the German Nation" in order to dismiss the medieval Christian belief that Christians were to be divided into two classes: "spiritual" and "temporal" or non-spiritual.

The conservative reforms of Lutherans are reflected in the theological and practical view of the ministry of the Church. Much of European Lutheranism follows the traditional Catholic governance of deacon, priest and bishop. The Lutheran archbishops of Finland, Sweden, etc. and Baltic countries are the historic national primates and some ancient cathedrals and parishes in the Lutheran church were constructed many centuries before the Reformation. Indeed, ecumenical work within the Anglican Communion and among Scandinavian Lutherans mutually recognize the historic apostolic legitimacy and full communion. Likewise in America, Lutherans have embraced the apostolic succession of bishops in the full communion with Episcopalians and most Lutheran ordinations are performed by a bishop.

In some Lutheran churches, ordained clergy are called priests as in Sweden and Finland, while in others the term pastor is preferred.

Methodist clergy often have the title of pastor, minister, reverend, etc.

In the Latter Day Saint movement, the priesthood is the power and authority of God given to man, including the authority to perform ordinances and to act as a leader in the church. A body of priesthood holders is referred to as a quorum. Priesthood denotes elements of both power and authority. The priesthood includes the power Jesus gave his apostles to perform miracles such as the casting out of devils and the healing of sick (Luke 9:1). Latter Day Saints believe that the Biblical miracles performed by prophets and apostles were performed by the power of the priesthood, including the miracles of Jesus, who holds all of the keys of the priesthood. The priesthood is formally known as the "Priesthood after the Order of the Son of God", but to avoid the too frequent use of the name of deity, the priesthood is referred to as the Melchizedek priesthood (Melchizedek being the high priest to whom Abraham paid tithes). As an authority, the priesthood is the authority by which a bearer may perform ecclesiastical acts of service in the name of God. Latter Day Saints believe that acts (and in particular, ordinances) performed by one with priesthood authority are recognized by God and are binding in heaven, on earth, and in the afterlife.

There is some variation among the Latter Day Saint denomination regarding who can be ordained to the priesthood. In The Church of Jesus Christ of Latter-day Saints (LDS Church), all worthy males above the age of 12 can be ordained to the priesthood. However, prior to a policy change in 1978, the LDS Church did not ordain men or boys who were of black African descent. The LDS Church does not ordain women to any of its priesthood offices. The Reorganized Church of Jesus Christ of Latter Day Saints (now the Community of Christ), the second largest denomination of the movement, began ordaining women to all of its priesthood offices in 1984. This decision was one of the reasons that led to a schism in the church, which prompted the formation of the independent Restoration Branches movement from which other denominations have sprung, including the Remnant Church of Jesus Christ of Latter Day Saints.

Islam has no sacerdotal priesthood. There are, however, a variety of academic and administrative offices which have evolved to assist Muslims with this task; a full discussion can be found at Clergy#Islam.

Hindu priests historically were members of the Brahmin caste. Priests are ordained and trained as well. There are two types of Hindu priests, "pujaris" ("swamis", "yogis", and gurus) and "purohits" ("pundits"). A "pujari" performs rituals in a temple. These rituals include bathing the "murtis" (the statues of the gods/goddesses), performing "puja", a ritualistic offering of various items to the Gods, the waving of a "ghee" or oil lamp also called an offering in light, known in Hinduism as "aarti", before the "murtis". "Pujaris" are often married.

A "purohit", on the other hand, performs rituals and "saṃskāras" (sacraments) outside of the temple. There are special "purohits" who perform only funeral rites.

In many cases, a "purohit" also functions as a "pujari". Both women and men are ordained as "purohits" and "pujaris".

In Zoroastrianism, the priesthood is reserved for men and is a mostly hereditary position. The priests prepare a drink from a sacred plant, which is called the "haoma" ritual. They officiate the "Yasna", pouring libations into the sacred fire to the accompaniment of ritual chants.

The Taoist priests (道士 "master of the Dao" p. 488) act as interpreters of the principles of Yin-Yang 5 elements (fire, water, soil, wood, and metal p. 53) school of ancient Chinese philosophy, as they relate to marriage, death, festival cycles, and so on. The Taoist priest seeks to share the benefits of meditation with his or her community through public ritual and liturgy (p. 326). In the ancient priesthood before the Tang, the priest was called "Jijiu" ("libationer" p. 550), with both male and female practitioners selected by merit. The system gradually changed into a male only hereditary Taoist priesthood until more recent times (p. 550,551).

The Shinto priest is called a ", originally pronounced "kamunushi", sometimes referred to as a . A kannushi is the person responsible for the maintenance of a Shinto shrine, or "jinja", purificatory rites, and for leading worship and veneration of a certain "kami". Additionally, priests are aided by " for many rites as a kind of shaman or medium. The maidens may either be family members in training, apprentices, or local volunteers.

"Saiin" were female relatives of the Japanese emperor (termed "saiō") who served as High Priestesses in Kamo Shrine. "Saiō" also served at Ise Shrine. "Saiin" priestesses usually were elected from royalty. In principle, "Saiin" remained unmarried, but there were exceptions. Some "Saiin" became consorts of the emperor, called "Nyōgo" in Japanese. The "Saiin" order of priestesses existed throughout the Heian and Kamakura periods.

The Yoruba people of western Nigeria practice an indigenous religion with a chiefly hierarchy of priests and priestesses that dates to AD 800–1000. Ifá priests and priestesses bear the titles Babalawo for men and Iyanifa for women. Priests and priestesses of the varied Orisha are titled Babalorisa for men and Iyalorisa for women. Initiates are also given an Orisa or Ifá name that signifies under which deity they are initiated. For example, a Priestess of Osun may be named Osunyemi, and a Priest of Ifá may be named Ifáyemi. This traditional culture continues to this day as initiates from all around the world return to Nigeria for initiation into the priesthood, and varied derivative sects in the New World (such as Cuban Santería and Brazilian Umbanda) use the same titles to refer to their officers as well.

In Brazil, the priests in the Umbanda, Candomblé and Quimbanda religions are called "pai-de-santo" (literally "Father of saint" in English), or "babalorixá" (a word borrowed from Yoruba "bàbálórìsà", meaning "Father of the Orisha"); its female equivalent is the "mãe-de-santo" ("Mother of saint"), also referred to as "ialorixá" (Yoruba: "iyálórìsà").

In the Cuban Santería, a priest is called "Santero", or "Santera" in its feminine equivalent.

According to traditional Wiccan beliefs, every member of the religion is considered a priestess or priest, as it is believed that no person can stand between another and the Divine. However, in response to the growing number of Wiccan temples and churches, several denominations of the religion have begun to develop a core group of ordained priestesses and priests serving a larger laity. This trend is far from widespread, but is gaining acceptance due to increased interest in the religion.

The dress of religious workers in ancient times may be demonstrated in frescoes and artifacts from the cultures. The dress is presumed to be related to the customary clothing of the culture, with some symbol of the deity worn on the head or held by the person. Sometimes special colors, materials, or patterns distinguish celebrants, as the white wool veil draped on the head of the Vestal Virgins.

Occasionally the celebrants at religious ceremonies shed all clothes in a symbolic gesture of purity. This was often the case in ancient times. An example of this is shown to the left on a Kylix dating from c. 500 BC where a priestess is featured. Modern religious groups tend to avoid such symbolism and some may be quite uncomfortable with the concept.

The retention of long skirts and vestments among many ranks of contemporary priests when they officiate may be interpreted to express the ancient traditions of the cultures from which their religious practices arose.

In most Christian traditions, priests wear clerical clothing, a distinctive form of street dress. Even within individual traditions it varies considerably in form, depending on the specific occasion. In Western Christianity, the stiff white clerical collar has become the nearly universal feature of priestly clerical clothing, worn either with a cassock or a clergy shirt. The collar may be either a full collar or a vestigial tab displayed through a square cutout in the shirt collar.

Eastern Christian priests mostly retain the traditional dress of two layers of differently cut cassock: the "rasson" (Greek) or "podriasnik" (Russian) beneath the outer "exorasson" (Greek) or "riasa" (Russian). If a pectoral cross has been awarded it is usually worn with street clothes in the Russian tradition, but not so often in the Greek tradition.

Distinctive clerical clothing is less often worn in modern times than formerly, and in many cases it is rare for a priest to wear it when not acting in a pastoral capacity, especially in countries that view themselves as largely secular in nature. There are frequent exceptions to this however, and many priests rarely if ever go out in public without it, especially in countries where their religion makes up a clear majority of the population. Pope John Paul II often instructed Catholic priests and religious to always wear their distinctive (clerical) clothing, unless wearing it would result in persecution or grave verbal attacks.

Christian traditions that retain the title of priest also retain the tradition of special liturgical vestments worn only during services. Vestments vary widely among the different Christian traditions.

In modern Pagan religions, such as Wicca, there is no one specific form of dress designated for the clergy. If there is, it is a particular of the denomination in question, and not a universal practice. However, there is a traditional form of dress, (usually a floor-length tunic and a knotted cord cincture, known as the "cingulum"), which is often worn by worshipers during religious rites. Among those traditions of Wicca that do dictate a specific form of dress for its clergy, they usually wear the traditional tunic in addition to other articles of clothing (such as an open-fronted robe or a cloak) as a distinctive form of religious dress, similar to a habit.

In many religions there are one or more layers of assistant priests.

In the Ancient Near East, hierodules served in temples as assistants to the priestess.

In ancient Judaism, the Priests (Kohanim) had a whole class of Levites as their assistants in making the sacrifices, in singing psalms and in maintaining the Temple. The Priests and the Levites were in turn served by servants called Nethinim. These lowest level of servants were not priests.

An assistant priest is a priest in the Anglican and Episcopal churches who is not the senior member of clergy of the parish to which they are appointed, but is nonetheless in priests' orders; there is no difference in function or theology, merely in 'grade' or 'rank'. Some assistant priests have a "sector ministry", that is to say that they specialize in a certain area of ministry within the local church, for example youth work, hospital work, or ministry to local light industry. They may also hold some diocesan appointment part-time. In most (though not all) cases an assistant priest has the legal status of assistant curate, although not all assistant curates are priests, as this legal status also applies to many deacons working as assistants in a parochial setting.

The corresponding term in the Catholic Church is "parochial vicar" – an ordained priest assigned to assist the pastor (Latin: "parochus") of a parish in the pastoral care of parishioners. Normally, all pastors are also ordained priests; occasionally an auxiliary bishop will be assigned that role.

In Wicca, the leader of a coven or temple (either a high priestess or high priest) often appoints an assistant. This assistant is often called a 'deputy', but the more traditional terms 'maiden' (when female and assisting a high priestess) and 'summoner' (when male and assisting a high priest) are still used in many denominations.



</doc>
<doc id="23708" url="https://en.wikipedia.org/wiki?curid=23708" title="PL/I">
PL/I

PL/I (Programming Language One, pronounced and sometimes written PL/1) is a procedural, imperative computer programming language developed and published by IBM. It is designed for scientific, engineering, business and system programming. It has been used by academic, commercial and industrial organizations since it was introduced in the 1960s, and is still used.

PL/I's main domains are data processing, numerical computation, scientific computing, and system programming. It supports recursion, structured programming, linked data structure handling, fixed-point, floating-point, complex, character string handling, and bit string handling. The language syntax is English-like and suited for describing complex data formats with a wide set of functions available to verify and manipulate them.

In the 1950s and early 1960s, business and scientific users programmed for different computer hardware using different programming languages. Business users were moving from Autocoders via COMTRAN to COBOL, while scientific users programmed in General Interpretive Programme (GIP), Fortran, ALGOL, GEORGE, and others. The IBM System/360 (announced in 1964 and delivered in 1966) was designed as a common machine architecture for both groups of users, superseding all existing IBM architectures. Similarly, IBM wanted a single programming language for all users. It hoped that Fortran could be extended to include the features needed by commercial programmers. In October 1963 a committee was formed composed originally of three IBMers from New York and three members of SHARE, the IBM
scientific users group, to propose these extensions to Fortran. Given the constraints of Fortran, they were unable to do this and embarked on the design of a "new programming language" based loosely on ALGOL labeled "NPL". This acronym conflicted with that of the UK's National Physical Laboratory and was replaced briefly by MPPL (MultiPurpose Programming Language) and, in 1965, with PL/I (with a Roman numeral "I"). The first definition appeared in April 1964.

IBM took NPL as a starting point and completed the design to a level that the first compiler could be written: the NPL definition was incomplete in scope and in detail. Control of the PL/I language was vested initially in the New York Programming Center and later at the IBM UK Laboratory at Hursley. The SHARE and GUIDE user groups were involved in extending the language and had a role in IBM's process for controlling the language through their PL/I Projects. The experience of defining such a large language showed the need for a formal definition of PL/I. A project was set up in 1967 in IBM Laboratory Vienna to make an unambiguous and complete specification. This led in turn to one of the first large scale Formal Methods for development, VDM.

Fred Brooks is credited with ensuring PL/I had the CHARACTER data type.

The language was first specified in detail in the manual "PL/I Language Specifications. C28-6571" written in New York from 1965 and superseded by "PL/I Language Specifications. GY33-6003" written in Hursley from 1967. IBM continued to develop PL/I in the late sixties and early seventies, publishing it in the GY33-6003 manual. These manuals were used by the Multics group and other early implementers.

The first compiler was delivered in 1966. The Standard for PL/I was approved in 1976.

The goals for PL/I evolved during the early development of the language. Competitiveness with COBOL's record handling and report writing was required. The language's scope of usefulness grew to include system programming and event-driven programming. Additional goals for PL/I were:


To achieve these goals, PL/I borrowed ideas from contemporary languages while adding substantial new capabilities and casting it with a distinctive concise and readable syntax. Many principles and capabilities combined to give the language its character and were important in meeting the language's goals:


The language is designed to be all things to all programmers. The summary is extracted from the ANSI PL/I Standard
and the ANSI PL/I General-Purpose Subset Standard.

A PL/I program consists of a set of procedures, each of which is written as a sequence of statements. The codice_5 construct is used to include text from other sources during program translation. All of the statement types are summarized here in groupings which give an overview of the language (the Standard uses this organization).

Names may be declared to represent data of the following types, either as single values, or as aggregates in the form of arrays, with a lower-bound and upper-bound per dimension, or structures (comprising nested structure, array and scalar variables):
The codice_6 type comprises these attributes:

The base, scale, precision and scale factor of the codice_7 type is encoded within the codice_8. The mode is specified separately, with the codice_9 applied to both the real and the imaginary parts.

Values are computed by expressions written using a specific set of operations and builtin functions, most of which may be applied to aggregates as well as to single values, together with user-defined procedures which, likewise, may operate on and return aggregate as well as single values. The assignment statement assigns values to one or more variables.

There are no reserved words in PL/I. A statement is terminated by a semi-colon. The maximum length of a statement is implementation defined. A comment may appear anywhere in a program where a space is permitted and is preceded by the characters forward slash, asterisk and is terminated by the characters asterisk, forward slash (i.e. ). Statements may have a label-prefix introducing an entry name (codice_10 and codice_11 statements) or label name, and a condition prefix enabling or disabling a computational condition e.g. codice_12). Entry and label names may be single identifiers or identifiers followed by a subscript list of constants (as in codice_13).

A sequence of statements becomes a "group" when preceded by a codice_4 statement and followed by an codice_15 statement. Groups may include nested groups and begin blocks. The codice_16 statement specifies a group or a single statement as the codice_17 part and the codice_18 part (see the sample program). The group is the unit of iteration. The begin "block" (codice_19) may contain declarations for names and internal procedures local to the block. A "procedure" starts with a codice_11 statement and is terminated syntactically by an codice_15 statement. The body of a procedure is a sequence of blocks, groups, and statements and contains declarations for names and procedures local to the procedure or codice_22 to the procedure.

An "ON-unit" is a single statement or block of statements written to be executed when one or more of these "conditions" occur:

a "computational condition",

or an "Input/Output" condition,

or one of the conditions:

A declaration of an identifier may contain one or more of the following attributes (but they need to be mutually consistent):

Current compilers from Kednos, Micro Focus, and particularly that from IBM implement many extensions over the standardized version of the language. The IBM extensions are summarised in the Implementation sub-section for the compiler later. Although there are some extensions common to these compilers the lack of a current standard means that compatibility is not guaranteed.

Language standardization began in April 1966 in Europe with ECMA TC10. In 1969 ANSI established a "Composite Language Development Committee", nicknamed "Kludge", later renamed X3J1 PL/I. Standardization became a joint effort of ECMA TC/10 and ANSI X3J1. A subset of the GY33-6003 document was offered to the joint effort by IBM and became the base document for standardization. The major features omitted from the base document were multitasking and the attributes for program optimization (e.g. codice_27 and codice_28).

Proposals to change the base document were voted upon by both committees. In the event that the committees disagreed, the chairs, initially Michael Marcotty of General Motors and C.A.R. Hoare representing ICL had to resolve the disagreement. In addition to IBM, Honeywell, CDC, Data General, Digital Equipment, Prime Computer, Burroughs, RCA, and Univac served on X3J1 along with major users Eastman Kodak, MITRE, Union Carbide, Bell Laboratories, and various government and university representatives. Further development of the language occurred in the standards bodies, with continuing improvements in structured programming and internal consistency, and with the omission of the more obscure or contentious features.

As language development neared an end, X3J1/TC10 realized that there were a number of problems with a document written in English text. Discussion of a single item might appear in multiple places which might or might not agree. It was difficult to determine if there were omissions as well as inconsistencies. Consequently, David Beech (IBM), Robert Freiburghouse (Honeywell), Milton Barber (CDC), M. Donald MacLaren (Argonne National Laboratory), Craig Franklin (Data General), Lois Frampton (Digital Equipment), and editor, D.J. Andrews of IBM undertook to rewrite the entire document, each producing one or more complete chapters. The standard is couched as a formal definition using a "PL/I Machine" to specify the semantics. It was the first, and possibly the only, programming language standard to be written as a semi-formal definition.

A "PL/I General-Purpose Subset" ("Subset-G") standard was issued by ANSI in 1981 and a revision published in 1987. The General Purpose subset was widely adopted as the kernel for PL/I implementations.

PL/I was first implemented by IBM, at its Hursley Laboratories in the United Kingdom, as part of the development of System/360. The first production PL/I compiler was the PL/I F compiler for the OS/360 Operating System, built by John Nash's team at Hursley in the UK: the runtime library team was managed by I.M. (Nobby) Clarke. The PL/I F compiler was written entirely in System/360 assembly language. Release 1 shipped in 1966. OS/360 is a real-memory environment and the compiler was designed for systems with as little as 64 kilobytes of real storage – F being 64 kB in S/360 parlance. To fit a large compiler into the 44 kilobytes of memory available on a 64-kilobyte machine, the compiler consists of a control phase and a large number of compiler phases (approaching 100). The phases are brought into memory from disk, one at a time, to handle particular language features and aspects of compilation. Each phase makes a single pass over the partially-compiled program, usually held in memory.

Aspects of the language were still being designed as PL/I F was implemented, so some were omitted until later releases. PL/I RECORD I/O was shipped with PL/I F Release 2. The list processing functions Based Variables, Pointers, Areas and Offsets and LOCATE-mode I/O were first shipped in Release 4. In a major attempt to speed up PL/I code to compete with Fortran object code, PL/I F Release 5 does substantial program optimization of DO-loops facilitated by the REORDER option on procedures.

A version of PL/I F was released on the TSS/360 timesharing operating system for the System/360 Model 67, adapted at the IBM Mohansic Lab. The IBM La Gaude Lab in France developed "Language Conversion Programs" to convert Fortran, Cobol, and Algol programs to the PL/I F level of PL/I.

The PL/I D compiler, using 16 kilobytes of memory, was developed by IBM Germany for the DOS/360 low end operating system. It implements a subset of the PL/I language requiring all strings and arrays to have fixed extents, thus simplifying the run-time environment. Reflecting the underlying operating system, it lacks dynamic storage allocation and the "controlled" storage class. It was shipped within a year of PL/I F.

Compilers were implemented by several groups in the early 1960s. The Multics project at MIT, one of the first to develop an operating system in a high-level language, used Early PL/I (EPL), a subset dialect of PL/I, as their implementation language in 1964. EPL was developed at Bell Labs and MIT by Douglas McIlroy, Robert Morris, and others. The influential Multics PL/I compiler was the source of compiler technology used by a number of manufacturers and software groups.

The Honeywell PL/I compiler (for Series 60) is an implementation of the full ANSI X3J1 standard.

The PL/I Optimizer and Checkout compilers produced in Hursley support a common level of PL/I language and aimed to replace the PL/I F compiler. The checkout compiler is a rewrite of PL/I F in BSL, IBM's PL/I-like proprietary implementation language (later PL/S). The performance objectives set for the compilers are shown in an IBM presentation to the BCS. The compilers had to produce identical results the Checkout Compiler is used to debug programs that would then be submitted to the Optimizer. Given that the compilers had entirely different designs and were handling the full PL/I language this goal was challenging: it was achieved.

The PL/I optimizing compiler took over from the PL/I F compiler and was IBM's workhorse compiler from the 1970s to the 1990s. Like PL/I F, it is a multiple pass compiler with a 44 kilobyte design point, but it is an entirely new design. Unlike the F compiler, it has to perform compile time evaluation of constant expressions using the run-time library, reducing the maximum memory for a compiler phase to 28 kilobytes. A second-time around design, it succeeded in eliminating the annoyances of PL/I F such as cascading diagnostics. It was written in S/360 Macro Assembler by a team, led by Tony Burbridge, most of whom had worked on PL/I F. Macros were defined to automate common compiler services and to shield the compiler writers from the task of managing real-mode storage, allowing the compiler to be moved easily to other memory models. The gamut of program optimization techniques developed for the contemporary IBM Fortran H compiler were deployed: the Optimizer equaled Fortran execution speeds in the hands of good programmers. Announced with IBM S/370 in 1970, it shipped first for the DOS/360 operating system in August 1971, and shortly afterward for OS/360, and the first virtual memory IBM operating systems OS/VS1, MVS, and VM/CMS. (The developers were unaware that while they were shoehorning the code into 28 kb sections, IBM Poughkeepsie was finally ready to ship virtual memory support in OS/360). It supported the batch programming environments and, under TSO and CMS, it could be run interactively. This compiler went through many versions covering all mainframe operating systems including the operating systems of the Japanese plug-compatible machines (PCMs).

The compiler has been superseded by "IBM PL/I for OS/2, AIX, Linux, z/OS" below.

The PL/I checkout compiler, (colloquially "The Checker") announced in August 1970 was designed to speed and improve the debugging of PL/I programs. The team was led by Brian Marks. The three-pass design cut the time to compile a program to 25% of that taken by the F Compiler. It can be run from an interactive terminal, converting PL/I programs into an internal format, "H-text". This format is interpreted by the Checkout compiler at run-time, detecting virtually all types of errors. Pointers are represented in 16 bytes, containing the target address and a description of the referenced item, thus permitting "bad" pointer use to be diagnosed. In a conversational environment when an error is detected, control is passed to the user who can inspect any variables, introduce debugging statements and edit the source program. Over time the debugging capability of mainframe programming environments developed most of the functions offered by this compiler and it was withdrawn (in the 1990s?)

Perhaps the most commercially successful implementation aside from IBM's was Digital Equipment's 1988 release of the ANSI PL/I 1987 subset. The implementation is "a strict superset of the ANSI X3.4-1981 PL/I General Purpose Subset and provides most of the features of the new ANSI X3.74-1987 PL/I General Purpose Subset". The front end was designed by Robert Freiburghouse, and the code generator was implemented by Dave Cutler, who managed the design and implementation of VAX/VMS. It runs on VMS on VAX and Alpha and on Tru64. UniPrise Systems, Inc., was responsible for the compiler;

In the late 1960s and early 1970s, many US and Canadian universities were establishing time-sharing services on campus and needed conversational compiler/interpreters for use in teaching science, mathematics, engineering, and computer science. Dartmouth were developing BASIC, but PL/I was a popular choice, as it was concise and easy to teach. As the IBM offerings were unsuitable, a number of schools built their own subsets of PL/I and their own interactive support. Examples are:

A compiler developed at Cornell University for teaching a dialect called PL/C, which had the unusual capability of never failing to compile any program through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements. The language was almost all of PL/I as implemented by IBM. PL/C was a very fast compiler.

SL/1 (Student Language/1, Student Language/One or Subset Language/1) was a PL/I subset that ran interpretively on the IBM 1130; instructional use
was its strong point.

PLAGO, created at the Polytechnic Institute of Brooklyn, used a simplified subset of the PL/I language and focused on good diagnostic error messages and fast compilation times.

The Computer Systems Research Group of the University of Toronto produced the SP/k compilers which supported a sequence of subsets of PL/I called SP/1, SP/2, SP/3, ..., SP/8 for teaching programming. Programs that ran without errors under the SP/k compilers produced the same results under other contemporary PL/I compilers such as IBM's PL/I F compiler, IBM's checkout compiler or Cornell University's PL/C compiler.

Other examples are PL0 by P. Grouse at the University of New South Wales, PLUM by Marvin Zelkowitz at the University of Maryland., and PLUTO from the University of Toronto.

In a major revamp of PL/I, IBM Santa Teresa in California launched an entirely new compiler in 1992. The initial shipment was for OS/2 and included most ANSI-G features and many new PL/I features. Subsequent releases covered additional platforms (MVS, VM, OS/390, AIX and Windows -although support for Windows has since been withdrawn) and continued to add functions to make PL/I fully competitive with other languages offered on the PC (particularly C and C++) in areas where it had been overtaken. The corresponding "IBM Language Environment" supports inter-operation of PL/I programs with Database and Transaction systems, and with programs written in C, C++, and COBOL, the compiler supports all the data types needed for intercommunication with these languages.

The PL/I design principles were retained and withstood this major extension comprising several new data types, new statements and statement options, new exception conditions, and new organisations of program source. The resulting language is a compatible super-set of the PL/I Standard and of the earlier IBM compilers. Major topics added to PL/I were:


The latest series of PL/I compilers for z/OS, called Enterprise PL/I for z/OS, leverage code generation for the latest z/Architecture processors (z14, z13, zEC12, zBC12, z196, z114) via the use of ARCHLVL parm control passed during compilation, and was the second High level language supported by z/OS Language Environment to do so (XL C/C++ being the first, and Enterprise COBOL v5 the last.)

codice_30 is a new computational data type. The ordinal facilities are like those in Pascal,
e.g. codice_39
but in addition the name and internal values are accessible via built-in functions. Built-in functions provide access to an ordinal value's predecessor and successor.

The codice_40 (see below) allows additional codice_33s to be declared composed from PL/I's built-in attributes.

The codice_42 locator data type is similar to the codice_43 data type, but strongly typed to bind only to a particular data structure. The codice_44 operator is used to select a data structure using a handle.

The codice_34 attribute (equivalent to codice_46 in early PL/I specifications) permits several scalar variables, arrays, or structures to share the same storage in a unit that occupies the amount of storage needed for the largest alternative.

These attributes were added:

New string-handling functions were added to centre text, to edit using a picture format, and to trim blanks or selected characters from the head or tail of text, codice_63 to codice_64 from the right. and codice_65 and codice_66 functions.

Compound assignment operators a la C e.g. codice_67, codice_68, codice_69, codice_70 were added. codice_71 is equivalent to codice_72.

Additional parameter descriptors and attributes were added for omitted arguments and variable length argument lists.

The codice_73 attribute declares an identifier as a constant (derived from a specific literal value or restricted expression).

Parameters can have the codice_74 (pass by address) or codice_37 (pass by value) attributes.

The codice_76 and codice_77 attributes prevent unintended assignments.

codice_78 obviates the need for the contrived construct codice_79.

The codice_40 introduces user-specified names (e.g. codice_81) for combinations of built-in attributes (e.g. codice_82). Thus codice_83 creates the codice_33 name codice_81 as an alias for the set of built-in attributes FIXED BINARY(31.0). codice_31 applies to structures and their members; it provides a codice_33 name for a set of structure attributes and corresponding substructure member declarations for use in a structure declaration (a generalisation of the codice_88 attribute).

A codice_89 statement to exit a loop, and an codice_90 to continue with the next iteration of a loop.

codice_56 and codice_57 options on iterative groups.

The package construct consisting of a set of procedures and declarations for use as a unit. Variables declared outside of the procedures are local to the package, and can use codice_93, codice_94 or codice_95 storage. Procedure names used in the package also are local, but can be made external by means of the codice_96 option of the codice_97.

The codice_98 executed in an ON-unit terminates execution of the ON-unit, and raises the condition again in the procedure that called the current one (thus passing control to the corresponding ON-unit for that procedure).

The codice_99 condition handles invalid operation codes detected by the PC processor, as well as illegal arithmetic operations such as subtraction of two infinite values.

The codice_100 condition is provided to intercept conditions for which no specific ON-unit has been provided in the current procedure.

The codice_101 condition is raised when an codice_102 statement is unable to obtain sufficient storage.

A number of vendors produced compilers to compete with IBM PL/I F or Optimizing compiler on mainframes and minicomputers in the 1970s. In the 1980s the target was usually the emerging ANSI-G subset.






PL/I implementations were developed for mainframes from the late 1960s, mini computers in the 1970s, and personal computers in the 1980s and 1990s. Although its main use has been on mainframes, there are PL/I versions for DOS, Microsoft Windows, OS/2, AIX, OpenVMS, and Unix.

It has been widely used in business data processing and for system use for writing operating systems on certain platforms. Very complex and powerful systems have been built with PL/I:

The SAS System was initially written in PL/I; the SAS data step is still modeled on PL/I syntax.

The pioneering online airline reservation system Sabre was originally written for the IBM 7090 in assembler. The S/360 version was largely written using SabreTalk, a purpose built subset PL/I compiler for a dedicated control program.

The Multics operating system was largely written in PL/I.

PL/I was used to write an executable formal definition to interpret IBM's System Network Architecture

PL/I did not fulfill its supporters' hopes that it would displace Fortran and COBOL and become the major player on mainframes. It remained a minority but significant player. There cannot be a definitive explanation for this, but some trends in the 1970s and 1980s militated against its success by progressively reducing the territory on which PL/I enjoyed a competitive advantage.

First, the nature of the mainframe software environment changed. Application subsystems for database and transaction processing (CICS and IMS and Oracle on System 370) and application generators became the focus of mainframe users' application development. Significant parts of the language became irrelevant because of the need to use the corresponding native features of the subsystems (such as tasking and much of input/output). Fortran was not used in these application areas, confining PL/I to COBOL's territory; most users stayed with COBOL. But as the PC became the dominant environment for program development, Fortran, COBOL and PL/I all became minority languages overtaken by C++, Java and the like.

Second, PL/I was overtaken in the system programming field. The IBM system programming community was not ready to use PL/I; instead, IBM developed and adopted a proprietary dialect of PL/I for system programming. – PL/S. With the success of PL/S inside IBM, and of C outside IBM, the unique PL/I strengths for system programming became less valuable.

Third, the development environments grew capabilities for interactive software development that, again, made the unique PL/I interactive and debugging strengths less valuable.

Fourth, COBOL and Fortran added features such as structured programming, character string operations, and object orientation, that further reduced PL/I's relative advantages.

On mainframes there were substantial business issues at stake too. IBM's hardware competitors had little to gain and much to lose from success of PL/I. Compiler development was expensive, and the IBM compiler groups had an in-built competitive advantage. Many IBM users wished to avoid being locked into proprietary solutions. With no early support for PL/I by other vendors it was best to avoid PL/I.

This article uses the PL/I standard as the reference point for language features. But a number of features of significance in the early implementations were not in the Standard; and some were offered by non-IBM compilers. And the de facto language continued to grow after the standard, ultimately driven by developments on the Personal Computer.

"Multi tasking" was implemented by PL/I F, the Optimizer and the newer AIX and Z/OS compilers. It comprised the data types codice_104 and codice_105, the codice_106 on the codice_107 (Fork), the codice_108 (Join), the codice_109, codice_110s on the record I/O statements and the codice_111 statement to unlock locked records on codice_112 files. Event data identify a particular event and indicate whether it is complete ('1'B) or incomplete ('0'B): task data items identify a particular task (or process) and indicate its priority relative to other tasks.

The first IBM "Compile time preprocessor" was built by the IBM Boston Advanced Programming Center located in Cambridge, Mass, and shipped with the PL/I F compiler. The codice_5 statement was in the Standard, but the rest of the features were not. The DEC and Kednos PL/I compilers implemented much the same set of features as IBM, with some additions of their own. IBM has continued to add preprocessor features to its compilers. The preprocessor treats the written source program as a sequence of tokens, copying them to an output source file or acting on them. When a % token is encountered the following compile time statement is executed: when an identifier token is encountered and the identifier has been codice_114d, codice_115d, and assigned a compile time value, the identifier is replaced by this value. Tokens are added to the output stream if they do not require action (e.g. codice_116), as are the values of ACTIVATEd compile time expressions. Thus a compile time variable codice_117 could be declared, activated, and assigned using codice_118. Subsequent occurrences of codice_117 would be replaced by codice_120.

The data type supported are codice_121 integers and codice_122 strings of varying length with no maximum length. The structure statements are:

and the simple statements, which also may have a [label-list:]

The feature allowed programmers to use identifiers for constants e.g. product part numbers or mathematical constants and was superseded in the standard by named constants for computational data. Conditional compiling and iterative generation of source code, possible with compile-time facilities, was not supported by the standard. Several manufacturers implemented these facilities.

Structured programming additions were made to PL/I during standardization but were not accepted into the standard. These features were the codice_133 to exit from an iterative codice_4, the codice_135 and codice_136 added to codice_4, and a case statement of the general form:
codice_138<br>
These features were all included in DEC PL/I (and IBM's PL/I Checkout and Optimizing compilers). 

PL/I F had offered some debug facilities that were not put forward for the standard but were implemented by others notably the CHECK(variable-list) condition prefix, codice_139 on-condition and the codice_140 option. The IBM Optimizing and Checkout compilers added additional features appropriate to the conversational mainframe programming environment (e.g. an codice_141 condition).

Several attempts had been made to design a structure member type that could have one of several datatypes (codice_46 in early IBM). With the growth of classes in programming theory, approaches to this became possible on a PL/I base codice_34, codice_33 etc. have been added by several compilers.

PL/I had been conceived in a single-byte character world. With support for Japanese and Chinese language becoming essential, and the developments on International Code Pages, the character string concept was expanded to accommodate wide non-ASCII/EBCDIC strings.

Time and date handling were overhauled to deal with the millennium problem, with the introduction of the DATETIME function that returned the date and time in one of about 35 different formats. Several other date functions deal with conversions to and from days and seconds.

Though the language is easy to learn and use, implementing a PL/I compiler is difficult and time-consuming. A language as large as PL/I needed subsets that most vendors could produce and most users master. This was not resolved until "ANSI G" was published. The compile time facilities, unique to PL/I, took added implementation effort and additional compiler passes. A PL/I compiler was two to four times as large as comparable Fortran or COBOL compilers, and also that much slower—supposedly offset by gains in programmer productivity. This was anticipated in IBM before the first compilers were written.

Some argue that PL/I is unusually hard to parse. The PL/I "keywords" are not reserved so programmers can use them as variable or procedure names in programs. Because the original PL/I(F) compiler attempts "auto-correction" when it encounters a keyword used in an incorrect context, it often assumes it is a variable name. This leads to "cascading diagnostics", a problem solved by later compilers.

The effort needed to produce good object code was perhaps underestimated during the initial design of the language. Program optimization (needed to compete with the excellent program optimization carried out by available Fortran compilers) is unusually complex owing to side effects and pervasive problems with aliasing of variables. Unpredictable modification can occur asynchronously in exception handlers, which may be provided by "ON statements" in (unseen) callers. Together, these make it difficult to reliably predict when a program's variables might be modified at runtime. In typical use, however, user-written error handlers (the codice_145) often do not make assignments to variables. In spite of the aforementioned difficulties, IBM produced the PL/I Optimizing Compiler in 1971.

PL/I contains many rarely-used features, such as multitasking support (an IBM extension to the language) which add cost and complexity to the compiler, and its co-processing facilities require a multi-programming environment with support for non-blocking multiple threads for processes by the operating system. Compiler writers were free to select whether to implement these features.

An undeclared variable is, by default, declared by first occurrence—thus misspelling might lead to unpredictable results. This "implicit declaration" is no different from FORTRAN programs. For PL/I(F), however, an attribute listing enables the programmer to detect any misspelled or undeclared variable.

Many programmers were slow to move from COBOL or Fortran due to a perceived complexity of the language and immaturity of the PL/I F compiler. Programmers were sharply divided into scientific programmers (who used Fortran) and business programmers (who used COBOL), with significant tension and even dislike between the groups. PL/I syntax borrowed from both COBOL and Fortran syntax. So instead of noticing features that would make their job easier, Fortran programmers of the time noticed COBOL syntax and had the opinion that it was a business language, while COBOL programmers noticed Fortran syntax and looked upon it as a scientific language.

Both COBOL and Fortran programmers viewed it as a "bigger" version of their own language, and both were somewhat intimidated by the language and disinclined to adopt it. Another factor was "pseudo"-similarities to COBOL, Fortran, and ALGOL. These were PL/I elements that looked similar to one of those languages, but worked differently in PL/I. Such frustrations left many experienced programmers with a jaundiced view of PL/I, and often an active dislike for the language. An early UNIX fortune file contained the following tongue-in-cheek description of the language:

Speaking as someone who has delved into the intricacies of PL/I, I am sure that only Real Men could have written such a machine-hogging, cycle-grabbing, all-encompassing monster. Allocate an array and free the middle third? Sure! Why not? Multiply a character string times a bit string and assign the result to a float decimal? Go ahead! Free a controlled variable procedure parameter and reallocate it before passing it back? Overlay three different types of variable on the same memory location? Anything you say! Write a recursive macro? Well, no, but Real Men use rescan. How could a language so obviously designed and written by Real Men not be intended for Real Man use?

On the positive side, full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions PL/I was indeed quite a leap forward compared to the programming languages of its time. However, these were not enough to persuade a majority of programmers or shops to switch to PL/I.

The PL/I F compiler's compile time preprocessor was unusual (outside the Lisp world) in using its target language's syntax and semantics ("e.g." as compared to the C preprocessor's "#" directives).

PL/I provides several 'storage classes' to indicate how the lifetime of variables' storage is to be managed codice_146 and codice_94. The simplest to implement is codice_93, which indicates that memory is allocated and initialized at load-time, as is done in COBOL "working-storage" and early Fortran. This is the default for codice_22 variables.
PL/I's default storage class for codice_150 variables is codice_151, similar to that of other block-structured languages influenced by ALGOL, like the "auto" storage class in the C language, and default storage allocation in Pascal and "local-storage" in IBM COBOL. Storage for codice_151 variables is allocated upon entry into the codice_153, procedure, or ON-unit in which they are declared. The compiler and runtime system allocate memory for a stack frame to contain them and other housekeeping information. If a variable is declared with an codice_154, code to set it to an initial value is executed at this time. Care is required to manage the use of initialization properly. Large amounts of code can be executed to initialize variables every time a scope is entered, especially if the variable is an array or structure. Storage for codice_151 variables is freed at block exit: codice_156 or codice_94 variables are used to retain variables' contents between invocations of a procedure or block. codice_95 storage is also managed using a stack, but the pushing and popping of allocations on the stack is managed by the programmer, using codice_102 and codice_160 statements. Storage for codice_94 variables is managed using codice_162, but instead of a stack these allocations have independent lifetimes and are addressed through codice_163 or codice_43 variables.

The codice_23 attribute is used to declare programmer-defined heaps. Data can be allocated and freed within a specific area, and the area can be deleted, read, and written as a unit.

There are several ways of accessing allocated storage through different data declarations. Some of these are well defined and safe, some can be used safely with careful programming, and some are inherently unsafe and/or machine dependent.

Passing a variable as an argument to a parameter by reference allows the argument's allocated storage to be referenced using the parameter. The codice_166 attribute (e.g. codice_167) allows part or all of a variable's storage to be used with a different, but consistent, declaration. The language definition includes a codice_46 attribute (later renamed codice_34) to allow different definitions of data to share the same storage. This was not supported by many early IBM compilers. These usages are safe and machine independent.

Record I/O and list processing produce situations where the programmer needs to fit a declaration to the storage of the next record or item, before knowing what type of data structure it has. Based variables and pointers are key to such programs. The data structures must be designed appropriately, typically using fields in a data structure to encode information about its type and size. The fields can be held in the preceding structure or, with some constraints, in the current one. Where the encoding is in the preceding structure, the program needs to allocate a based variable with a declaration that matches the current item (using expressions for extents where needed). Where the type and size information are to be kept in the current structure ("self defining structures") the type-defining fields must be ahead of the type dependent items and in the same place in every version of the data structure. The codice_103-option is used for self-defining extents (e.g. string lengths as in codice_171 where codice_172 is used to allocate instances of the data structure. For self-defining structures, any typing and codice_173 fields are placed ahead of the "real" data. If the records in a data set, or the items in a list of data structures, are organised this way they can be handled safely in a machine independent way.

PL/I implementations do not (except for the PL/I Checkout compiler) keep track of the data structure used when storage is first allocated. Any codice_94 declaration can be used with a pointer into the storage to access the storage inherently unsafe and machine dependent. However, this usage has become important for "pointer arithmetic" (typically adding a certain amount to a known address). This has been a contentious subject in computer science. In addition to the problem of wild references and buffer overruns, issues arise due to the alignment and length for data types used with particular machines and compilers. Many cases where pointer arithmetic might be needed involve finding a pointer to an element inside a larger data structure. The codice_175 function computes such pointers, safely and machine independently.

Pointer arithmetic may be accomplished by aliasing a binary variable with a pointer as in 
codice_176 
It relies on pointers being the same length as codice_177 integers and aligned on the same boundaries.

With the prevalence of C and its free and easy attitude to pointer arithmetic, recent IBM PL/I compilers allow pointers to be used with the addition and subtraction operators to giving the simplest syntax (but compiler options can disallow these practices where safety and machine independence are paramount).

When PL/I was designed, programs only ran in batch mode, with no possible intervention from the programmer at a terminal. An exceptional condition such as division by zero would abort the program yielding only a hexadecimal core dump. PL/I exception handling, via ON-units, allowed the program to stay in control in the face of hardware or operating system exceptions and to recover debugging information before closing down more gracefully. As a program became properly debugged, most of the exception handling could be removed or disabled: this level of control became less important when conversational execution became commonplace.

Computational exception handling is enabled and disabled by condition prefixes on statements, blocks(including ON-units) and procedures. – e.g. codice_178. Operating system exceptions for Input/Output and storage management are always enabled.

The ON-unit is a single statement or codice_179-block introduced by an codice_180. Executing the ON statement enables the condition specified, e.g., codice_181. When the exception for this condition occurs and the condition is enabled, the ON-unit for the condition is executed. ON-units are inherited down the call chain. When a block, procedure or ON-unit is activated, the ON-units established by the invoking activation are inherited by the new activation. They may be over-ridden by another codice_180 and can be reestablished by the codice_183. The exception can be simulated using the codice_184 – e.g. to help debug the exception handlers. The dynamic inheritance principle for ON-units allows a routine to handle the exceptions occurring within the subroutines it uses.

If no ON-unit is in effect when a condition is raised a standard system action is taken (often this is to raise the codice_25 condition). The system action can be reestablished using the codice_186 option of the codice_180. With some conditions it is possible to complete executing an ON-unit and return to the point of interrupt (e.g., the codice_188 conditions) and resume normal execution. With other conditions such as codice_189, the codice_25 condition is raised when this is attempted. An ON-unit may be terminated with a codice_191 preventing a return to the point of interrupt, but permitting the program to continue execution elsewhere as determined by the programmer.

An ON-unit needs to be designed to deal with exceptions that occur in the ON-unit itself. The codice_192 statement allows a nested error trap; if an error occurs within an ON-unit, control might pass to the operating system where a system dump might be produced, or, for some computational conditions, continue execution (as mentioned above).

The PL/I codice_193 I/O statements have relatively simple syntax as they do not offer options for the many situations from end-of-file to record transmission errors that can occur when a record is read or written. Instead, these complexities are handled in the ON-units for the various file conditions. The same approach was adopted for codice_23 sub-allocation and the codice_23 condition.

The existence of exception handling ON-units can have an effect on optimization, because variables can be inspected or altered in ON-units. Values of variables that might otherwise be kept in registers between statements, may need to be returned to storage between statements. This is discussed in the section on Implementation Issues above.

PL/I has counterparts for COBOL and FORTRAN's specialized GO TO statements.

Syntax for both COBOL and FORTRAN exist for coding two special two types of GO TO, each of which has a target that is not always the same.


PL/I's GO TO statement has extensions to implement the above:
HERE(-1): PUT LIST ("I O U"); GO TO Lottery; HERE(0): PUT LIST ("No Cash"); GO TO Lottery;HERE(1): PUT LIST ("Dollar Bill"); GO TO Lottery;HERE(2): PUT LIST ("TWO DOLLARS"); GO TO Lottery;







</doc>
<doc id="23711" url="https://en.wikipedia.org/wiki?curid=23711" title="Punctuation">
Punctuation

Punctuation (formerly sometimes called pointing) is the use of spacing, conventional signs, and certain typographical devices as aids to the understanding and correct reading of written text, whether read silently or aloud. Another description is, "It is the practice action or system of inserting points or other small marks into texts in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks."

In written English, punctuation is vital to disambiguate the meaning of sentences. For example: "woman, without her man, is nothing" (emphasizing the importance of men to women), and "woman: without her, man is nothing" (emphasizing the importance of women to men) have very different meanings; as do "eats shoots and leaves" (which means the subject consumes plant growths) and "eats, shoots, and leaves" (which means the subject eats first, then fires a weapon, and then leaves the scene). The sharp differences in meaning are produced by the simple differences in punctuation within the example pairs, especially the latter.

The rules of punctuation vary with language, location, register, and time and are constantly evolving. Certain aspects of punctuation are stylistic and are thus the author's (or editor's) choice, or tachygraphic (shorthand) language forms, such as those used in online chat and text messages.

The first writing systems were either logographic or syllabicfor example, Chinese and Mayan scriptwhich do not necessarily require punctuation, especially spacing. This is because the entire morpheme or word is typically clustered within a single glyph, so spacing does not help as much to distinguish where one word ends and the other starts. Disambiguation and emphasis can easily be communicated without punctuation by employing a separate written form distinct from the spoken form of the language that uses slightly different phraseology. Even today, written English differs subtly from spoken English because not all emphasis and disambiguation is possible to convey in print, even with punctuation.

Ancient Chinese classical texts were transmitted without punctuation. However, many Warring States period bamboo texts contain the symbols and indicating the end of a chapter and full stop, respectively. By the Song dynasty, addition of punctuation to texts by scholars to aid comprehension became common.

The earliest alphabetic writing — Phoenician, Hebrew, and others of the same family — had no capitalization, no spaces, no vowels (see abjad) and few punctuation marks. This worked as long as the subject matter was restricted to a limited range of topics (for example, writing used for recording business transactions). Punctuation is historically an aid to reading aloud.

The oldest known document using punctuation is the Mesha Stele (9th century BC). This employs points between the words and horizontal strokes between the sense section as punctuation.

Most texts were still written in "scriptura continua", that is without any separation between words. However, the Greeks were sporadically using punctuation marks consisting of vertically arranged dots—usually two (dicolon) or three (tricolon)—in around the 5th century BC as an aid in the oral delivery of texts. Greek playwrights such as Euripides and Aristophanes used symbols to distinguish the ends of phrases in written drama: this essentially helped the play's cast to know when to pause. After 200 BC, the Greeks used Aristophanes of Byzantium's system (called ) of a single dot () placed at varying heights to mark up speeches at rhetorical divisions:
In addition, the Greeks used the paragraphos (or gamma) to mark the beginning of sentences, marginal diples to mark quotations, and a koronis to indicate the end of major sections.

The Romans () also occasionally used symbols to indicate pauses, but the Greek —under the name "distinctiones"—prevailed by the AD 4th century as reported by Aelius Donatus and Isidore of Seville (7th century). Also, texts were sometimes laid out , where every sentence had its own separate line. Diples were used, but by the late period these often degenerated into comma-shaped marks.

Punctuation developed dramatically when large numbers of copies of the Bible started to be produced. These were designed to be read aloud, so the copyists began to introduce a range of marks to aid the reader, including indentation, various punctuation marks (diple, , ), and an early version of initial capitals (). Jerome and his colleagues, who made a translation of the Bible into Latin, the "Vulgate" (), employed a layout system based on established practices for teaching the speeches of Demosthenes and Cicero. Under his layout every sense-unit was indented and given its own line. This layout was solely used for biblical manuscripts during the 5th–9th centuries but was abandoned in favor of punctuation.

In the 7th–8th centuries Irish and Anglo-Saxon scribes, whose native languages were not derived from Latin, added more visual cues to render texts more intelligible. Irish scribes introduced the practice of word separation. Likewise, insular scribes adopted the system while adapting it for minuscule script (so as to be more prominent) by using not differing height but rather a differing number of marks—aligned horizontally (or sometimes triangularly)—to signify a pause's value: one mark for a minor pause, two for a medium one, and three for a major. Most common were the , a comma-shaped mark, and a 7-shaped mark (), often used in combination. The same marks could be used in the margin to mark off quotations.

In the late 8th century a different system emerged in France under the Carolingian dynasty. Originally indicating how the voice should be modulated when chanting the liturgy, the migrated into any text meant to be read aloud, and then to all manuscripts. first reached England in the late 10th century probably during the Benedictine reform movement, but was not adopted until after the Norman conquest. The original were the , , , and , but a fifth symbol, the , was added in the 10th century to indicate a pause of a value between the and . In the late 11th/early 12th century the disappeared and was taken over by the simple (now with two distinct values).

The late Middle Ages saw the addition of the (slash or slash with a midpoint dot) which was often used in conjunction with the for different types of pauses. Direct quotations were marked with marginal diples, as in Antiquity, but from at least the 12th century scribes also began entering diples (sometimes double) within the column of text.

The amount of printed material and its readership began to increase after the invention of moveable type in Europe in the 1450s. As explained by writer and editor Lynne Truss, "The rise of printing in the 14th and 15th centuries meant that a standard system of punctuation was urgently required." Printed books, whose letters were uniform, could be read much more rapidly than manuscripts. Rapid reading, or reading aloud, did not allow time to analyze sentence structures. This increased speed led to the greater use and finally standardization of punctuation, which showed the relationships of words with each other: where one sentence ends and another begins, for example.

The introduction of a standard system of punctuation has also been attributed to the Venetian printers Aldus Manutius and his grandson. They have been credited with popularizing the practice of ending sentences with the colon or full stop (period), inventing the semicolon, making occasional use of parentheses, and creating the modern comma by lowering the virgule. By 1566, Aldus Manutius the Younger was able to state that the main object of punctuation was the clarification of syntax.

By the 19th century, punctuation in the western world had evolved "to classify the marks hierarchically, in terms of weight". Cecil Hartley's poem identifies their relative values:
<poem style="margin:1.2em 0 1.5em 3em;">
The stop point out, with truth, the time of pause
A sentence doth require at ev'ry clause.
At ev'ry comma, stop while "one" you count;
At semicolon, "two" is the amount;
A colon doth require the time of "three";
The period "four", as learned men agree.
</poem>
The use of punctuation was not standardised until after the invention of printing. According to the 1885 edition of "The American Printer", the importance of punctuation was noted in various sayings by children such as:
<poem style="margin:1.2em 0 1.5em 3em;">
Charles the First walked and talked
Half an hour after his head was cut off.
</poem>
With a semi-colon and a comma added it reads:
<poem style="margin:1.2em 0 1.5em 3em;">
Charles the First walked and talked;
Half an hour after, his head was cut off.
</poem>
In a 19th-century manual of typography, Thomas MacKellar writes:
The introduction of electrical telegraphy with a limited set of transmission codes and typewriters with a limited set of keys influenced punctuation subtly. For example, curved quotes and apostrophes were all collapsed into two characters (' and "). The hyphen, minus sign, and dashes of various widths were collapsed into a single character (-, sometimes repeated as—to represent a long dash). The spaces of different widths available to professional typesetters were generally replaced by a single full-character width space, with typefaces monospaced. In some cases a typewriter keyboard did not include an exclamation point (!) but this was constructed by the overstrike of an apostrophe and a period; the original Morse code did not have an exclamation point.

These simplifications were carried forward into digital writing, with teleprinters and the ASCII character set essentially supporting the same characters as typewriters. Treatment of whitespace in HTML discouraged the practice (in English prose) of putting two full spaces after a full stop, since a single or double space would appear the same on the screen. (Some style guides now discourage double spaces, and some electronic writing tools, including Wikipedia's software, automatically collapse double spaces to single.) The full traditional set of typesetting tools became available with the advent of desktop publishing and more sophisticated word processors. Despite the widespread adoption of character sets like Unicode that support the punctuation of traditional typesetting, writing forms like text messages tend to use the simplified ASCII style of punctuation, with the addition of new non-text characters like emoji. Informal text speak tends to drop punctuation when not needed, including some ways that would be considered errors in more formal writing.

In the computer era, punctuation characters were recycled for use in programming languages and URLs. Due to its use in email and Twitter handles, the at sign (@) went from an obscure character mostly used by sellers of bulk commodities (10 pounds @$2.00 per pound), to a very common character in common use for both technical routing and an abbreviation for "at". The tilde (~), in moveable type only used in combination with vowels, for mechanical reasons ended up as a separate key on mechanical typewriters, and like @ it has been put to completely new uses.

There are two major styles of punctuation in English: British or American. These two styles differ mainly in the way in which they handle quotation marks, particularly in conjunction with other punctuation marks. In British English, punctuation such as periods and commas are placed outside the closing quotation mark; in American English, however, punctuation is placed inside the closing quotation mark. This rule varies for other punctuation marks; for example, American English follows the British English rule when it comes to semicolons, colons, question marks, and exclamation points. The serial comma is used much more often in the United States than in England.

Other languages of Europe use much the same punctuation as English. The similarity is so strong that the few variations may confuse a native English reader. Quotation marks are particularly variable across European languages. For example, in French and Russian, quotes would appear as: (in French, each "double punctuation", as the guillemet, requires a non-breaking space; in Russian it does not).

In French of France, the signs : ; ? and ! are always preceded by a thin unbreakable space. In Canada, this is only the case for :.

In Greek, the question mark is written as the English semicolon, while the functions of the colon and semicolon are performed by a raised point , known as the ().

In Georgian, three dots, , were formerly used as a sentence or paragraph divider. It is still sometimes used in calligraphy.

Spanish, and no other language, uses an inverted question mark at the beginning of a question and the normal question mark at the end, as well as an inverted exclamation mark at the beginning of an exclamation and the normal exclamation mark at the end.

Armenian uses several punctuation marks of its own. The full stop is represented by a colon, and vice versa; the exclamation mark is represented by a diagonal similar to a tilde , while the question mark resembles an unclosed circle placed after the last vowel of the word.

Arabic, Urdu, and Persian—written from right to left—use a reversed question mark: , and a reversed comma: . This is a modern innovation; pre-modern Arabic did not use punctuation. Hebrew, which is also written from right to left, uses the same characters as in English, and .

Originally, Sanskrit had no punctuation. In the 17th century, Sanskrit and Marathi, both written using Devanagari, started using the vertical bar to end a line of prose and double vertical bars in verse.

Punctuation was not used in Chinese, Japanese, and Korean writing until the adoption of punctuation from the West in the late 19th and early 20th century. In unpunctuated texts, the grammatical structure of sentences in classical writing is inferred from context. Most punctuation marks in modern Chinese, Japanese, and Korean have similar functions to their English counterparts; however, they often look different and have different customary rules.

In the Indian subcontinent, is sometimes used in place of colon or after a subheading. Its origin is unclear, but could be a remnant of the British Raj. Another punctuation common in the Indian Subcontinent for writing monetary amounts is the use of or after the number. For example, Rs. 20/- or Rs. 20/= implies 20 rupees whole.

Thai, Khmer, Lao and Burmese did not use punctuation until the adoption of punctuation from the West in the 20th century. Blank spaces are more frequent than full stops or commas.

"Further information: Armenian punctuation, Chinese punctuation, Hebrew punctuation, Japanese punctuation and Korean punctuation."

In 1966, the French author Hervé Bazin proposed a series of six innovative punctuation marks in his book ("Let's pluck the bird", 1966). These were:

An international patent application was filed, and published in 1992 under World Intellectual Property Organization (WIPO) number WO9219458, for two new punctuation marks: the "question comma" and the "exclamation comma". The "question comma" has a comma instead of the dot at the bottom of a question mark, while the "exclamation comma" has a comma in place of the point at the bottom of an exclamation mark. These were intended for use as question and exclamation marks within a sentence, a function for which normal question and exclamation marks can also be used, but which may be considered obsolescent. The patent application entered into the national phase only in Canada. It was advertised as lapsing in Australia on 27 January 1994 and in Canada on 6 November 1995.

Various sets of characters are referred to as "punctuation" in certain computing situations, many of which are also used to punctuate natural languages. Sometimes non-punctuation in the natural language sense (such as "&" which is not punctuation but is an abbreviation for "and") are included.

General Punctuation and Supplemental Punctuation are blocks of Unicode symbols.

In regular expressions, the character class codice_1 is defined to consist of the following characters (when operating in ASCII mode): codice_2





</doc>
<doc id="23712" url="https://en.wikipedia.org/wiki?curid=23712" title="Pentomino">
Pentomino

A pentomino (or 5-omino) is a polyomino of order 5, that is, a polygon in the plane made of 5 equal-sized squares connected edge-to-edge. When rotations and reflections are not considered to be distinct shapes, there are 12 different "free" pentominoes. When reflections are considered distinct, there are 18 "one-sided" pentominoes. When rotations are also considered distinct, there are 63 "fixed" pentominoes.

Pentomino tiling puzzles and games are popular in recreational mathematics. Usually, video games such as Tetris imitations and Rampart consider mirror reflections to be distinct, and thus use the full set of 18 one-sided pentominoes.

Each of the twelve pentominoes satisfies the Conway criterion; hence every pentomino is capable of tiling the plane. Each chiral pentomino can tile the plane without being reflected.

Pentominoes were formally defined by American professor Solomon W. Golomb starting in 1953 and later in his 1965 book "Polyominoes: Puzzles, Patterns, Problems, and Packings". They were introduced to the general public by Martin Gardner in his October 1965 Mathematical Games column in Scientific American. Golomb coined the term "pentomino" from the Ancient Greek / "pénte", "five", and the -omino of domino, fancifully interpreting the "d-" of "domino" as if it were a form of the Greek prefix "di-" (two). Golomb named the 12 "free" pentominoes after letters of the Latin alphabet that they resemble.

John Horton Conway proposed an alternate labeling scheme for pentominoes, using O instead of I, Q instead of L, R instead of F, and S instead of N. The resemblance to the letters is more strained, especially for the O pentomino, but this scheme has the advantage of using 12 consecutive letters of the alphabet. It is used by convention in discussing Conway's Game of Life, where, for example, one speaks of the R-pentomino instead of the F-pentomino.


The F, L, N, P, Y, and Z pentominoes are chiral; adding their reflections (F′, L′, N′, Q, Y′, Z′) brings the number of "one-sided" pentominoes to 18. If rotations are also considered distinct, then the pentominoes from the first category count eightfold, the ones from the next three categories (T, U, V, W, Z) count fourfold, I counts twice, and X counts only once. This results in 5×8 + 5×4 + 2 + 1 = 63 "fixed" pentominoes.

For example, the eight possible orientations of the L, F, N, P, and Y pentominoes are as follows:

For 2D figures in general there are two more categories:

A standard pentomino puzzle is to tile a rectangular box with the pentominoes, i.e. cover it without overlap and without gaps. Each of the 12 pentominoes has an area of 5 unit squares, so the box must have an area of 60 units. Possible sizes are 6×10, 5×12, 4×15 and 3×20. The avid puzzler can probably solve these problems by hand within a few hours. A more challenging task, typically requiring a computer search, is to count the total number of solutions in each case.

The 6×10 case was first solved in 1960 by Colin Brian and Jenifer Haselgrove. There are exactly 2339 solutions, excluding trivial variations obtained by rotation and reflection of the whole rectangle, but including rotation and reflection of a subset of pentominoes (which sometimes provides an additional solution in a simple way). The 5×12 box has 1010 solutions, the 4×15 box has 368 solutions, and the 3×20 box has just 2 solutions (one is shown in the figure, and the other one can be obtained from the solution shown by rotating, as a whole, the block consisting of the L, N, F, T, W, Y, and Z pentominoes).

A somewhat easier (more symmetrical) puzzle, the 8×8 rectangle with a 2×2 hole in the center, was solved by Dana Scott as far back as 1958. There are 65 solutions. Scott's algorithm was one of the first applications of a backtracking computer program. Variations of this puzzle allow the four holes to be placed in any position. One of the external links uses this rule. Most such patterns are solvable, with the exceptions of placing each pair of holes near two corners of the board in such a way that both corners could only be fitted by a P-pentomino, or forcing a T-pentomino or U-pentomino in a corner such that another hole is created.

Efficient algorithms have been described to solve such problems, for instance by Donald Knuth. Running on modern hardware, these pentomino puzzles can now be solved in mere seconds.

The solution of tiling rectangles of polyominoes with "n" cells exists only for "n" = 0, 1, 2 and 5; the first three are trivial.

A pentacube is a polycube of five cubes. Of the 29 pentacubes, exactly twelve pentacubes are flat (1-layer) and correspond to the twelve pentominoes extruded to a depth of one square.
A pentacube puzzle or 3D pentomino puzzle, amounts to filling a 3-dimensional box with the 12 flat pentacubes, i.e. cover it without overlap and without gaps. Since each pentacube has a volume of 5 unitand 3×4×5 (3940 solutions). Following are one solution of each case.
Alternatively one could also consider combinations of five cubes that are themselves 3D, i.e., are not part of one layer of cubes. However, in addition to the 12 extruded pentominoes, 6 sets of chiral pairs and 5 pieces make total 29 pieces, resulting in 145 cubes, which will not make a 3D box (as 145 can only be 29×5×1, which the non-flat pentominoes cannot fit into).

There are board games of skill based entirely on pentominoes. Such games are often simply called "Pentominoes".

One of the games is played on an 8×8 grid by two or three players. Players take turns in placing pentominoes on the board so that they do not overlap with existing tiles and no tile is used more than once. The objective is to be the last player to place a tile on the board. This version of Pentominoes is called "Golomb's Game".

The two-player version has been weakly solved in 1996 by Hilarie Orman. It was proved to be a first-player win by examining around 22 billion board positions.

Pentominoes, and similar shapes, are also the basis of a number of other tiling games, patterns and puzzles. For example, the French board game "Blokus" is played with 4 colored sets of polyominoes, each consisting of every pentomino (12), tetromino (5), triomino (2) domino (1) and monomino (1). Like the game "Pentominoes", the goal is to use all of your tiles, and a bonus is given if the monomino is played on the very last move. The player with the fewest blocks remaining wins.

The game of "Cathedral" is also based on polyominoes.

Parker Brothers released a multi-player pentomino board game called "Universe" in 1966. Its theme is based on an outtake from the movie in which the astronaut (seen playing chess in the final version) is playing a two-player pentomino game against a computer. The front of the board game box features scenes from the movie as well as a caption describing it as the "game of the future". The game comes with 4 sets of pentominoes in red, yellow, blue, and white. The board has two playable areas: a base 10x10 area for two players with an additional 25 squares (two more rows of 10 and one offset row of 5) on each side for more than two players.

Game manufacturer Lonpos has a number of games that use the same pentominoes, but on different game planes. Their "101 Game" has a 5 x 11 plane. By changing the shape of the plane, thousands of puzzles can be played, although only a relatively small selection of these puzzles are available in print.

The first pentomino problem, written by Henry Dudeney, was published in 1907 in the Canterbury Puzzles.

Pentominoes were featured in a prominent subplot of Arthur C. Clarke's novel "Imperial Earth", published in 1975. Clarke also wrote an essay in which he described the game and how he got hooked on it.

They were also featured in Blue Balliett's "Chasing Vermeer", which was published in 2003 and illustrated by Brett Helquist, as well as its sequels, "The Wright 3" and "The Calder Game".

In the New York Times crossword puzzle for June 27, 2012, the clue for an 11-letter word at 37 across was "Complete set of 12 shapes formed by this puzzle's black squares."

On several occasions, pentominoes had been used as decoration elements for outer walls of Plattenbau buildings, mainly in Eastern Europe. The patterns used were based on solutions of the 6×10 case puzzle.





</doc>
<doc id="23716" url="https://en.wikipedia.org/wiki?curid=23716" title="Programmer">
Programmer

A computer programmer, sometimes called more recently a coder (especially in more informal contexts), is a person who creates computer software. The term "computer programmer" can refer to a specialist in one area of computers, or to a generalist who writes code for many kinds of software.

A programmer's most oft-used computer language (e.g., Assembly, COBOL, C, C++, C#, Java, Lisp, Python) may be prefixed to the term "programmer". Some who work with web programming languages also prefix their titles with "web".

A range of occupations that involve programming also often require a range of other, similar skills, for example: (software) developer, web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, game programmer, game developer and software analyst. The use of the term "programmer" as applied to these positions is sometimes considered an insulting simplification or even derogatory.

British countess and mathematician Ada Lovelace is often considered to be the first computer programmer, as she was the first to publish part of a program (specifically an algorithm) intended for implementation on Charles Babbage's analytical engine, in October 1842. The algorithm was used to calculate Bernoulli numbers. Because Babbage's machine was never completed as a functioning standard in Lovelace's time, she unfortunately never had the opportunity to see the algorithm in action.

The first person to execute a program on a functioning, modern, electronic computer was the renowned computer scientist Konrad Zuse, in 1941.

The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.
International Programmers' Day is celebrated annually on 7 January. In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had already been an "unofficial" holiday before that in many countries.

The word "software" was used as early as 1953, but did not regularly appear in print until the 1960s. Before this time, computers were programmed either by customers or the few commercial computer manufacturers of the time, such as UNIVAC and IBM. The first company founded to specifically provide software products and services was the Computer Usage Company, in 1955.

The software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, governments and businesses created a demand for software. Many of these programs were written in-house by full-time staff programmers; some were distributed freely between users of a particular machine for no charge. And others were developed on a commercial basis. Other firms, such as Computer Sciences Corporation (founded in 1959) also started to grow. The computer/hardware manufacturers soon started bundling operating systems, system software and programming environments with their machines.

The industry expanded greatly with the rise of the personal computer ("PC") in the mid-1970s, which brought computing to the average office worker. In the following years the PC also helped create a constantly-growing market for games, applications and utilities software. CP/M, later replaced by DOS, Microsoft's Windows popular operating system of the time.

In the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS; this was at least the third time this model had been attempted. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition, no client software is loaded onto the end user's PC. By 2014, the role of cloud developer had been defined; in this context, one definition of a "developer" in general was published:

Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.

Programmers work in many settings, including corporate information technology ("IT") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).

Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer's supervision.

Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as "Java programmers", or by the type of function they perform or environment in which they work: for example, "database programmers", "mainframe programmers", or Web developers.

When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.

Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called "maintenance programming". Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.

Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.

A software developer needs to have deep technical expertise with certain aspects of computing. Some positions will require a degree in a relevant field such as computer science, information technology, engineering, programming, or any other IT related post graduate studies. An ideal software developer is a self-motivated professional carrying a dynamic hands-on experience on key languages of programming such as C++, C#, PHP, Java, C, Javascript, Visual Basic, Python, Smalltalk.

According to developer Eric Sink, the differences between system design, software development, and programming are more apparent. Already in the current market place there can be found a segregation between programmers and developers, in that one who implements is not the same as the one who designs the class structure or hierarchy. Even more so that developers become software architects or systems architects, those who design the multi-leveled architecture or component interactions of a large software system.

Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.

In some organizations, particularly small ones, people commonly known as "programmer analysts" are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.

In addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser. Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.

Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.

According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey. The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.

Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.

Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and avoid previously employer paid training using industry specific technologies not covered in most accredited degree programs. Other reasons for employers claiming skill shortages is the result of their own cost saving combining of several disparate skill sets previously held by several specialized programmers into fewer generalized multifaceted positions that are unlikely to have enough "qualified" candidates with the desired experience.

Enrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers. This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages. Most academic institutions have an Institutional research office that keep past statistics of degrees conferred which show several dips and rises in Computer Science degrees over the past 30 years. The overall trend shows a slightly overall decline in growth (especially when compared to other STEM degree growth) since certain peaks of 1986, 1992, 2002, and 2008 showing periods of flat growth or even declines. In addition the U.S. Bureau of Labor Statistics Occupational Outlook 2016-26 is -7% (a decline in their words) for Computer Programmers because Computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.




</doc>
<doc id="23721" url="https://en.wikipedia.org/wiki?curid=23721" title="Peter Singer">
Peter Singer

Peter Albert David Singer (born 6 July 1946) is an Australian moral philosopher. He is the Ira W. DeCamp Professor of Bioethics at Princeton University, and a Laureate Professor at the Centre for Applied Philosophy and Public Ethics at the University of Melbourne. He specialises in applied ethics and approaches ethical issues from a secular, utilitarian perspective. He is known in particular for his book "Animal Liberation" (1975), in which he argues in favour of veganism, and his essay "Famine, Affluence, and Morality", in which he argues in favour of donating to help the global poor. For most of his career, he was a preference utilitarian, but he stated in "The Point of View of the Universe" (2014), coauthored with Katarzyna de Lazari-Radek, that he had become a hedonistic utilitarian.

On two occasions, Singer served as chair of the philosophy department at Monash University, where he founded its Centre for Human Bioethics. In 1996 he stood unsuccessfully as a Greens candidate for the Australian Senate. In 2004 Singer was recognised as the Australian Humanist of the Year by the Council of Australian Humanist Societies. In 2005, the Sydney Morning Herald placed him among Australia's ten most influential public intellectuals. Singer is a cofounder of Animals Australia and the founder of "The Life You Can Save".

Singer's parents were Austrian Jews who immigrated to Australia from Vienna in 1939, after Austria's annexation by Nazi Germany. They settled in Melbourne, where Singer was born. Singer's father imported tea and coffee, while his mother practiced medicine. He has an older sister, Joan (now Joan Dwyer). His grandparents were less fortunate: his paternal grandparents were taken by the Nazis to Łódź, and never heard from again; his maternal grandfather David Ernst Oppenheim (1881–1943), a teacher, died in the Theresienstadt concentration camp. Oppenheim was a member of the Vienna Psychoanalytic Society and wrote a joint article with Sigmund Freud, before joining the Adlerian sect. Singer later wrote a biography of Oppenheim.

Singer is an atheist, and was raised in a prosperous, happy, non-religious family. His family rarely observed Jewish holidays, and Singer declined to have a Bar Mitzvah. Singer attended Preshil and later Scotch College. After leaving school, Singer studied law, history, and philosophy at the University of Melbourne, earning a bachelor's degree in 1967. He has explained that he elected to major in philosophy after his interest was piqued by discussions with his sister's then-boyfriend. He earned a master's degree for a thesis entitled "Why should I be moral?" at the same university in 1969. He was awarded a scholarship to study at the University of Oxford, and obtained from there a BPhil degree in 1971, with a thesis on civil disobedience supervised by R. M. Hare and published as a book in 1973. Singer names Hare and Australian philosopher H. J. McCloskey as his two most important mentors. One day at Balliol College in Oxford, he had what he refers to as "probably the decisive formative experience of my life". He was having a discussion after class with fellow graduate student Richard Keshen, a Canadian (who would later become a professor at Cape Breton University), over lunch. Keshen opted to have a salad after being told that the spaghetti sauce contained meat. Singer had the spaghetti. Singer eventually questioned Keshen about his reason for avoiding meat. Keshen explained his ethical objections. Singer would later state, "I'd never met a vegetarian who gave such a straightforward answer that I could understand and relate to." Keshen later introduced Singer to his vegetarian friends. Singer was able to find one book in which he could read up on the issue ("Animal Machines" by Ruth Harrison) and "within a week or two" he approached his wife saying that he thought they needed to make a change to their diet, and that he did not think they could justify eating meat.

After spending three years as a Radcliffe lecturer at University College, Oxford, he was a visiting professor at New York University for 16 months. He returned to Melbourne in 1977, where he spent most of his career, aside from appointments as visiting faculty abroad, until his move to Princeton in 1999. In June 2011, it was announced he would join the professoriate of New College of the Humanities, a private college in London, in addition to his work at Princeton. He also has been a regular contributor to Project Syndicate since 2001.

According to philosopher Helga Kuhse, Singer is "almost certainly the best-known and most widely read of all contemporary philosophers". Michael Specter wrote that Singer is among the most influential of contemporary philosophers.

Since 1968 he has been married to Renata Singer; they have three children: Ruth, Marion, and Esther. Renata Singer is a novelist and author and has collaborated on publications with her husband.

Singer's "Practical Ethics" (1979) analyzes why and how living beings' interests should be weighed. His principle of equal consideration of interests does not dictate equal treatment of all those with interests, since different interests warrant different treatment. All have an interest in avoiding pain, for instance, but relatively few have an interest in cultivating their abilities. Not only does his principle justify different treatment for different interests, but it allows different treatment for the same interest when diminishing marginal utility is a factor. For example, this approach would privilege a starving person's interest in food over the same interest of someone who is only slightly hungry.

Among the more important human interests are those in avoiding pain, in developing one's abilities, in satisfying basic needs for food and shelter, in enjoying warm personal relationships, in being free to pursue one's projects without interference, "and many others". The fundamental interest that entitles a being to equal consideration is the capacity for "suffering and/or enjoyment or happiness". Singer holds that a being's interests should always be weighed according to that being's concrete properties. The journey model is tolerant of some frustrated desire and explains why persons who have embarked on their journeys are not replaceable. Only a personal interest in continuing to live brings the journey model into play. This model also explains the priority that Singer attaches to "interests" over trivial desires and pleasures.

Ethical conduct is justified by reasons that go beyond prudence to "something bigger than the individual", addressing a larger audience. Singer thinks this going-beyond identifies moral reasons as "somehow universal", specifically in the injunction to 'love thy neighbour as thyself', interpreted by him as demanding that one give the same weight to the interests of others as one gives to one's own interests. This universalising step, which Singer traces from Kant to Hare, is crucial and sets him apart from those moral theorists, from Hobbes to David Gauthier, who tie morality to prudence. Universalisation leads directly to utilitarianism, Singer argues, on the strength of the thought that one's own interests cannot count for more than the interests of others. Taking these into account, one must weigh them up and adopt the course of action that is most likely to maximise the interests of those affected; utilitarianism has been arrived at. Singer's universalising step applies to interests without reference to who has them, whereas a Kantian's applies to the judgments of rational agents (in Kant's kingdom of ends, or Rawls's Original Position, etc.). Singer regards Kantian universalisation as unjust to animals. As for the Hobbesians, Singer attempts a response in the final chapter of "Practical Ethics", arguing that self-interested reasons support adoption of the moral point of view, such as 'the paradox of hedonism', which counsels that happiness is best found by not looking for it, and the need most people feel to relate to something larger than their own concerns.

Singer's ideas have contributed to the rise of effective altruism. He argues that people should not only try to reduce suffering, but reduce it in the most effective manner possible. While Singer has previously written at length about the moral imperative to reduce poverty and eliminate the suffering of nonhuman animals, particularly in the meat industry, he writes about how the effective altruism movement is doing these things more effectively in his 2015 book, "The Most Good You Can Do". He is a board member of Animal Charity Evaluators, a charity evaluator used by many members of the effective altruism community which recommends the most cost-effective animal advocacy charities and interventions.

His own organisation, The Life You Can Save, also recommends a selection of charities deemed by charity evaluators such as GiveWell to be the most effective when it comes to helping those in extreme poverty. TLYCS was founded after Singer released his 2009 eponymous book, in which he argues more generally in favour of giving to charities that help to end global poverty. In particular, he expands upon some of the arguments made in his 1972 essay "Famine, Affluence, and Morality", in which he posits that citizens of rich nations are morally obligated to give at least some of their disposable income to charities that help the global poor. He supports this using the drowning child analogy, which states that most people would rescue a drowning child from a pond, even if it meant that their expensive clothes were ruined, so we clearly value a human life more than the value of our material possessions. As a result, we should take a significant portion of the money that we spend on our possessions and instead donate it to charity.

Published in 1975, "Animal Liberation" has been cited as a formative influence on leaders of the modern animal liberation movement. The central argument of the book is an expansion of the utilitarian concept that "the greatest good of the greatest number" is the only measure of good or ethical behaviour, and Singer believes that there is no reason not to apply this principle to other animals, arguing that the boundary between human and "animal" is completely arbitrary. There are far more differences between a great ape and an oyster, for example, than between a human and a great ape, and yet the former two are lumped together as "animals", whereas we are considered "human" in a way that supposedly differentiates us from all other "animals."

He popularised the term "speciesism", which had been coined by English writer Richard D. Ryder to describe the practice of privileging humans over other animals, and therefore argues in favour of the equal consideration of interests of all sentient beings. In "Animal Liberation", Singer argues in favour of veganism and against animal experimentation. Singer describes himself as a flexible vegan. He writes, "That is, I'm vegan when it's not too difficult to be vegan, but I'm not rigid about this, if I'm traveling for example."

In an article for the online publication Chinadialogue, Singer called Western-style meat production cruel, unhealthy, and damaging to the ecosystem. He rejected the idea that the method was necessary to meet the population's increasing demand, explaining that animals in factory farms have to eat food grown explicitly for them, and they burn up most of the food's energy just to breathe and keep their bodies warm. In a 2010 Guardian article he titled, "Fish: the forgotten victims on our plate," Singer drew attention to the welfare of fish. He quoted (author) Alison Mood's startling statistics from a report she wrote, which was released on fishcount.org.uk just a month before the Guardian article. Singer states that she "has put together what may well be the first-ever systematic estimate of the size of the annual global capture of wild fish. It is, she calculates, in the order of one trillon, although it could be as high as 2.7tn."

Some chapters of "Animal Liberation" are dedicated to criticising testing on animals but, unlike groups such as PETA, Singer is willing to accept such testing when there is a clear benefit for medicine. In November 2006, Singer appeared on the BBC programme "Monkeys, Rats and Me: Animal Testing" and said that he felt that Tipu Aziz's experiments on monkeys for research into treating Parkinson's disease could be justified. Whereas Singer has continued since the publication of "Animal Liberation" to promote vegetarianism and veganism, he has been much less vocal in recent years on the subject of animal experimentation.

Singer has defended some of the actions of the Animal Liberation Front, such as the stealing of footage from Dr. Thomas Gennarelli's laboratory in May 1984 (as shown in the documentary "Unnecessary Fuss"), but he has condemned other actions such as the use of explosives by some animal-rights activists and sees the freeing of captive animals as largely futile when they are easily replaced.

In the past, Singer has not held that objective moral values exist, on the basis that reason could favour both egoism and equal consideration of interests. Singer himself adopted utilitarianism on the basis that people's preferences can be universalised, leading to a situation where one takes the "point of view of the universe" and "an impartial standpoint". But in the Second Edition of "Practical Ethics", he concedes that the question of why we should act morally "cannot be given an answer that will provide everyone with overwhelming reasons for acting morally".

However, when co-authoring "The Point of View of the Universe" (2014), Singer shifted to the position that objective moral values do exist, and defends the 19th century utilitarian philosopher Henry Sidgwick's view that objective morality can be derived from fundamental moral axioms that are knowable by reason. Additionally, he endorses Derek Parfit's view that there are object-given reasons for action. Furthermore, Singer and Katarzyna de Lazari-Radek (the co-author of the book) argue that evolutionary debunking arguments can be used to demonstrate that it is more rational to take the impartial standpoint of "the point of view of the universe", as opposed to egoism—pursuing one's own self-interest—because the existence of egoism is more likely to be the product of evolution by natural selection, rather than because it is correct, whereas taking an impartial standpoint and equally considering the interests of all sentient beings is in conflict with what we would expect from natural selection, meaning that it is more likely that impartiality in ethics is the correct stance to pursue.

Whilst a student in Melbourne, Singer campaigned against the Vietnam War as president of the Melbourne University Campaign Against Conscription. He also spoke publicly for the legalisation of abortion in Australia.
Singer joined the Australian Labor Party in 1974, but resigned after disillusionment with the centrist leadership of Bob Hawke. In 1992, he became a founding member of the Victorian Greens. He has run for political office twice for the Greens: in 1994 he received 28% of the vote in the Kooyong by-election, and in 1996 he received 3% of the vote when running for the Senate (elected by proportional representation). Before the 1996 election, he co-authored a book "The Greens" with Bob Brown.

In "A Darwinian Left", Singer outlines a plan for the political left to adapt to the lessons of evolutionary biology. He says that evolutionary psychology suggests that humans naturally tend to be self-interested. He further argues that the evidence that selfish tendencies are natural must not be taken as evidence that selfishness is "right." He concludes that game theory (the mathematical study of strategy) and experiments in psychology offer hope that self-interested people will make short-term sacrifices for the good of others, if society provides the right conditions. Essentially, Singer claims that although humans possess selfish, competitive tendencies naturally, they have a substantial capacity for cooperation that also has been selected for during human evolution. Singer's writing in "Greater Good" magazine, published by the Greater Good Science Center of the University of California, Berkeley, includes the interpretation of scientific research into the roots of compassion, altruism, and peaceful human relationships.

Singer has criticized the United States for receiving "oil from countries run by dictators ... who pocket most of the" financial gains, thus "keeping the people in poverty." Singer believes that the wealth of these countries "should belong to the people" within them rather than their "de facto government. In paying dictators for their oil, we are in effect buying stolen goods, and helping to keep people in poverty." Singer holds that America "should be doing more to assist people in extreme poverty". He is disappointed in U.S. foreign aid policy, deeming it "a very small proportion of our GDP, less than a quarter of some other affluent nations." Singer maintains that little "private philanthropy from the U.S." is "directed to helping people in extreme poverty, although there are some exceptions, most notably, of course, the Gates Foundation."

Singer describes himself as not anti-capitalist, stating in a 2010 interview with the New Left Project:

Capitalism is very far from a perfect system, but so far we have yet to find anything that clearly does a better job of meeting human needs than a regulated capitalist economy coupled with a welfare and health care system that meets the basic needs of those who do not thrive in the capitalist economy.

He added that "[i]f we ever do find a better system, I'll be happy to call myself an anti-capitalist".

Similarly, in his book "Marx", Singer is sympathetic to Marx's criticism of capitalism, but is skeptical about whether a better system is likely to be created, writing: "Marx saw that capitalism is a wasteful, irrational system, a system which controls us when we should be controlling it. That insight is still valid; but we can now see that the construction of a free and equal society is a more difficult task than Marx realised."

Singer is opposed to the death penalty, claiming that it does not effectively deter the crimes for which it is the punitive measure, and that he cannot see any other justification for it.

In 2010, Singer signed a petition renouncing his right of return to Israel, because it is "a form of racist privilege that abets the colonial oppression of the Palestinians."

In 2016, Singer called on Jill Stein to withdraw from the US presidential election in states that were close between Hillary Clinton and Donald Trump, on the grounds that "The stakes are too high". He argued against the view that there was no significant difference between Clinton and Trump, whilst also saying that he would not advocate such a tactic in Australia's electoral system, which allows for ranking of preferences.

When writing in 2017 on Trump's denial of climate change and plans to withdraw from the Paris accords, Singer advocated a boycott of all consumer goods from the United States to pressure the Trump administration to change its environmental policies.

Singer holds that the right to life is essentially tied to a being's capacity to hold preferences, which in turn is essentially tied to a being's capacity to feel pain and pleasure.

In "Practical Ethics", Singer argues in favour of abortion rights on the grounds that fetuses are neither rational nor self-aware, and can therefore hold no preferences. As a result, he argues that the preference of a mother to have an abortion automatically takes precedence. In sum, Singer argues that a fetus lacks personhood.

Similar to his argument for abortion rights, Singer argues that newborns lack the essential characteristics of personhood—"rationality, autonomy, and self-consciousness"—and therefore "killing a newborn baby is never equivalent to killing a person, that is, a being who wants to go on living". Singer has clarified that his "view of when life begins isn't very different from that of opponents of abortion." He deems it not "unreasonable to hold that an individual human life begins at conception. If it doesn't, then it begins about 14 days later, when it is no longer possible for the embryo to divide into twins or other multiples." Singer disagrees with abortion rights opponents in that he does not "think that the fact that an embryo is a living human being is sufficient to show that it is wrong to kill it." Singer wishes "to see American jurisprudence, and the national abortion debate, take up the question of which capacities a human being needs to have in order for it to be wrong to kill it" as well as "when, in the development of the early human being, these capacities are present."

Singer classifies euthanasia as voluntary, involuntary, or non-voluntary. Voluntary euthanasia is that to which the subject consents. He argues in favour of voluntary euthanasia and some forms of non-voluntary euthanasia, including infanticide in certain instances, but opposes involuntary euthanasia.

Religious critics have argued that Singer's ethic ignores and undermines the traditional notion of the sanctity of life. Singer agrees and believes the notion of the sanctity of life ought to be discarded as outdated, unscientific, and irrelevant to understanding problems in contemporary bioethics. Bioethicists associated with the disability rights and disability studies communities have argued that his epistemology is based on ableist conceptions of disability. Singer's positions have also been criticised by some advocates for disability rights and right-to-life supporters, concerned with what they see as his attacks upon human dignity. Singer has replied that many people judge him based on secondhand summaries and short quotations taken out of context, not his books or articles and, that his aim is to elevate the status of animals, not to lower that of humans. American publisher Steve Forbes ceased his donations to Princeton University in 1999 because of Singer's appointment to a prestigious professorship. Nazi-hunter Simon Wiesenthal wrote to organisers of a Swedish book fair to which Singer was invited that "A professor of morals ... who justifies the right to kill handicapped newborns ... is in my opinion unacceptable for representation at your level." Marc Maurer, President of the National Federation of the Blind, criticised Singer's appointment to the Princeton faculty in a banquet speech at the organisation's national convention in July 2001, claiming that Singer's support for euthanising disabled babies could lead to disabled older children and adults being valued less as well. Conservative psychiatrist Theodore Dalrymple wrote in 2010 that Singerian moral universalism is "preposterous—psychologically, theoretically, and practically".

In 2002, disability rights activist Harriet McBryde Johnson debated Singer, challenging his belief that it is morally permissible to euthanise new-born children with severe disabilities. "Unspeakable Conversations", Johnson's account of her encounters with Singer and the pro-euthanasia movement, was published in the "New York Times Magazine" in 2003. 

Singer has experienced the complexities of some of these questions in his own life. His mother had Alzheimer's disease. He said, "I think this has made me see how the issues of someone with these kinds of problems are really very difficult". In an interview with Ronald Bailey, published in December 2000, he explained that his sister shares the responsibility of making decisions about his mother. He did say that, if he were solely responsible, his mother might not continue to live.

In 1985, Singer wrote a book with the physician Deanne Wells arguing that surrogate motherhood should be allowed and regulated by the state by establishing nonprofit 'State Surrogacy Boards', which would ensure fairness between surrogate mothers and surrogacy-seeking parents. Singer and Wells endorsed both the payment of medical expenses endured by surrogate mothers and an extra "fair fee" to compensate the surrogate mother.

Singer was a speaker at the 2012 Global Atheist Convention. He has debated with Christians such as John Lennox and Dinesh D'Souza. Singer has pointed to the problem of evil as an objection against the Christian conception of God. He stated: "The evidence of our own eyes makes it more plausible to believe that the world was not created by any god at all. If, however, we insist on believing in divine creation, we are forced to admit that the god who made the world cannot be all-powerful and all good. He must be either evil or a bungler." In keeping with his considerations of non-human animals, Singer also takes issue with the original sin reply to the problem of evil, saying that, "animals also suffer from floods, fires, and droughts, and, since they are not descended from Adam and Eve, they cannot have inherited original sin."

In 1989 and 1990, Singer's work was the subject of a number of protests in Germany. A course in ethics led by Dr. Hartmut Kliemt at the University of Duisburg where the main text used was Singer's "Practical Ethics" was, according to Singer, "subjected to organised and repeated disruption by protesters objecting to the use of the book on the grounds that in one of its ten chapters it advocates active euthanasia for severely disabled newborn infants". The protests led to the course being shut down.

When Singer tried to speak during a lecture at Saarbrücken, he was interrupted by a group of protesters including advocates for disability rights. One of the protesters expressed that entering serious discussions would be a tactical error.

The same year, Singer was invited to speak in Marburg at a European symposium on "Bioengineering, Ethics and Mental Disability". The invitation was fiercely attacked by leading intellectuals and organisations in the German media, with an article in "Der Spiegel" comparing Singer's positions to Nazism. Eventually, the symposium was cancelled and Singer's invitation withdrawn.

A lecture at the Zoological Institute of the University of Zurich was interrupted by two groups of protesters. The first group was a group of disabled people who staged a brief protest at the beginning of the lecture. They objected to inviting an advocate of euthanasia to speak. At the end of this protest, when Singer tried to address their concerns, a second group of protesters rose and began chanting ""Singer raus! Singer raus!"" ("Singer out!") When Singer attempted to respond, a protester jumped on stage and grabbed his glasses, and the host ended the lecture. Singer explains "my views are not threatening to anyone, even minimally" and says that some groups play on the anxieties of those who hear only keywords that are understandably worrying (given the constant fears of ever repeating the Holocaust) if taken with any less than the full context of his belief system.

In 1991, Singer was due to speak along with R. M. Hare and Georg Meggle at the 15th International Wittgenstein Symposium in Kirchberg am Wechsel, Austria. Singer has stated that threats were made to Adolf Hübner, then the president of the Austrian Ludwig Wittgenstein Society, that the conference would be disrupted if Singer and Meggle were given a platform. Hübner proposed to the board of the society that Singer's invitation (as well as the invitations of a number of other speakers) be withdrawn. The Society decided to cancel the symposium.

In an article originally published in "The New York Review of Books", Singer argued that the protests dramatically increased the amount of coverage he received: "instead of a few hundred people hearing views at lectures in Marburg and Dortmund, several millions read about them or listened to them on television". Despite this, Singer argues that it has led to a difficult intellectual climate, with professors in Germany unable to teach courses on applied ethics and campaigns demanding the resignation of professors who invited Singer to speak.

Singer was criticized by Nathan J. Robinson, founder of "Current Affairs", for comments in an op-ed defending Anna Stubblefield, a carer and professor who was convicted of aggravated sexual assault against a man with severe physical and intellectual disabilities. The op-ed questioned whether the victim was capable of giving or withholding consent, and stated that "It seems reasonable to assume that the experience was pleasurable to him; for even if he is cognitively impaired, he was capable of struggling to resist." Robinson called the statements "outrageous" and "morally repulsive", and said that they implied that it might be okay to rape or sexually assault disabled people.

Roger Scruton was critical of the consequentialist, utilitarian approach of Peter Singer. Scruton wrote that Singer's works, including "Animal Liberation" (1975), "contain little or no philosophical argument. They derive their radical moral conclusions from a vacuous utilitarianism that counts the pain and pleasure of all living things as equally significant and that ignores just about everything that has been said in our philosophical tradition about the real distinction between persons and animals."

Singer was inducted into the United States Animal Rights Hall of Fame in 2000.

In June 2012, Singer was appointed a Companion of the Order of Australia (AC) for "eminent service to philosophy and bioethics as a leader of public debate and communicator of ideas in the areas of global poverty, animal welfare and the human condition."

Singer received "Philosophy Now"s 2016 Award for Contributions in the Fight Against Stupidity for his efforts "to disturb the comfortable complacency with which many of us habitually ignore the desperate needs of others ... particularly for this work as it relates to the Effective Altruism movement."

In 2018, Singer was noted in the book, "Rescuing Ladybugs" by author and animal advocate Jennifer Skiff as a "hero among heroes in the world," who, in arguing against speciesism "gave the modern world permission to believe what we innately know – that animals are sentient and that we have a moral obligation not to exploit or mistreat them." The book states that Singer's "moral philosophy on animal equality was sparked when he asked a fellow student at Oxford University a simple question about his eating habits." 









</doc>
<doc id="23723" url="https://en.wikipedia.org/wiki?curid=23723" title="Poznań">
Poznań

Poznań ( , , ; ; known also by other historical names) is a city on the Warta River in west-central Poland, in the Greater Poland region and is the fifth-largest city in Poland. It is best known for its renaissance Old Town and Ostrów Tumski Cathedral. Today, Poznań is an important cultural and business centre and one of Poland's most populous regions with many regional customs such as Saint John's Fair ("Jarmark Świętojański"), traditional Saint Martin's croissants and a local dialect.

Poznań is among the oldest and largest cities in Poland. The city's population is 538,633 (2011 census), while the continuous conurbation with Poznań County and several other communities is inhabited by almost 1.1 million people. The Larger Poznań Metropolitan Area (PMA) is inhabited by 1.3–1.4 million people and extends to such satellite towns as Nowy Tomyśl, Gniezno and Września, making it the fourth largest metropolitan area in Poland. It is the historical capital of the Greater Poland region and is currently the administrative capital of the province called Greater Poland Voivodeship.

Poznań is a centre of trade, sports, education, technology and tourism. It is an important academic site, with about 130,000 students and the Adam Mickiewicz University, the third largest Polish university. Poznań is also the seat of the oldest Polish diocese, now being one of the most populous archdioceses in the country. The city also hosts the Poznań International Fair – the biggest industrial fair in Poland and one of the largest fairs in Europe. The city's most renowned landmarks include Poznań Town Hall, the National Museum, Grand Theatre, Fara Church, Poznań Cathedral and the Imperial Castle.

Poznań is classified as a Gamma- global city by Globalization and World Cities Research Network. It has often topped rankings as a city with very high quality of education and a very high standard of living. It also ranks highly in safety and healthcare quality. The city of Poznań has also, many times, won the prize awarded by "Superbrands" for a very high quality city brand. In 2012, the Poznań's Art and Business Center "Stary Browar" won a competition organised by National Geographic Traveler and was given the first prize as one of the seven "New Polish Wonders".

The official patron saints of Poznań are Saint Peter and Paul of Tarsus, the patrons of the cathedral. Martin of Tours – the patron of the main street Święty Marcin is also regarded as one of the patron saints of the city.

The name Poznań probably comes from a personal name, "Poznan" (from the Polish participle – "one who is known/recognized"), and would mean "Poznan's town". It is also possible that the name comes directly from the verb "poznać", which means "to get to know" or "to recognize," so it may simply mean "known town".

The earliest surviving references to the city are found in the chronicles of Thietmar of Merseburg, written between 1012 and 1018: ("bishop of Poznań", in an entry for 970) and "ab urbe Posnani" ("from the city of Poznań", for 1005). The city's name appears in documents in the Latin nominative case as "Posnania" in 1236 and "Poznania" in 1247. The phrase "in Poznan" appears in 1146 and 1244.

The city's full official name is "Stołeczne Miasto Poznań" ("The Capital City of Poznań"), in reference to its role as a centre of political power in the early Polish state. Poznań is known as "Posen" in German, and was officially called "Haupt- und Residenzstadt Posen" ("Capital and Residence City of Poznań") between 20 August 1910 and 28 November 1918. The Latin names of the city are "Posnania" and "Civitas Posnaniensis". Its Yiddish name is , or "Poyzn".

In Polish, the city name has masculine grammatical gender.

For centuries before the Christianization of Poland, Poznań (consisting of a fortified stronghold between the Warta and Cybina rivers, on what is now Ostrów Tumski) was an important cultural and political centre of the Polan tribe. Mieszko I, the first historically recorded ruler of the Polans, and of the early Polish state which they dominated, built one of his main stable headquarters in Poznań. Mieszko's baptism of 966, seen as a defining moment in the Christianization of the Polish state, may have taken place in Poznań.

Following the baptism, construction began of Poznań's cathedral, the first in Poland. Poznań was probably the main seat of the first missionary bishop sent to Poland, Bishop Jordan. The Congress of Gniezno in 1000 led to the country's first permanent archbishopric being established in Gniezno (which is generally regarded as Poland's capital in that period), although Poznań continued to have independent bishops of its own. Poznań's cathedral was the place of burial of the early Piast monarchs (Mieszko I, Boleslaus I, Mieszko II, Casimir I), and later of Przemysł I and King Przemysł II.

The pagan reaction that followed Mieszko II's death (probably in Poznań) in 1034 left the region weak, and in 1038, Duke Bretislaus I of Bohemia sacked and destroyed both Poznań and Gniezno. Poland was reunited under Casimir I the Restorer in 1039, but the capital was moved to Kraków, which had been relatively unaffected by the troubles.
In 1138, by the testament of Bolesław III, Poland was divided into separate duchies under the late king's sons, and Poznań and its surroundings became the domain of Mieszko III the Old, the first of the Dukes of Greater Poland. This period of fragmentation lasted until 1320. Duchies frequently changed hands; control of Poznań, Gniezno and Kalisz sometimes lay with a single duke, but at other times these constituted separate duchies.

In about 1249, Duke Przemysł I began constructing what would become the Royal Castle on a hill on the left bank of the Warta. Then in 1253 Przemysł issued a charter to Thomas of Guben (Gubin) for the founding of a town under Magdeburg law, between the castle and the river. Thomas brought a large number of German settlers to aid in the building and settlement of the city – this is an example of the German eastern migration ("Ostsiedlung") characteristic of that period. The city (covering the area of today's Old Town neighbourhood) was surrounded by a defensive wall, integrated with the castle. According to Walter Kuhn, in 1400, three quarters of the town's population was German-speaking.

In reunited Poland, and later in the Polish–Lithuanian Commonwealth, Poznań was the seat of a voivodeship. The city's importance began to grow in the Jagiellonian period, due to its position on trading routes from Lithuania and Ruthenia to western Europe. It would become a major centre for the fur trade by the late 16th century. Suburban settlements developed around the city walls, on the river islands and on the right bank, with some (Ostrów Tumski, Śródka, Chwaliszewo, Ostrówek) obtaining their own town charters. However the city's development was hampered by regular major fires and floods. On 2 May 1536 a fire destroyed 175 buildings, including the castle, the town hall, the monastery and the suburban settlement called St. Martin. In 1519 the Lubrański Academy had been established in Poznań as an institution of higher education (but without the right to award degrees, which was reserved to Kraków's Jagiellonian University). However a Jesuits' college, founded in the city in 1571 during the Counter-Reformation, had the right to award degrees from 1611 until 1773, when it was combined with the Academy.

In the second half of the 17th century and most of the 18th, Poznań was severely affected by a series of wars (and attendant military occupations, lootings and destruction) – the Second and Third Northern Wars, the War of the Polish Succession, the Seven Years' War and the Bar Confederation rebellion. It was also hit by frequent outbreaks of plague, and by floods, particularly that of 1736, which destroyed most of the suburban buildings. The population of the conurbation declined (from 20,000 around 1600 to 6,000 around 1730), and Bambergian and Dutch settlers ("Bambrzy" and "Olędrzy") were brought in to rebuild the devastated suburbs. In 1778 a "Committee of Good Order" ("Komisja Dobrego Porządku") was established in the city, which oversaw rebuilding efforts and reorganised the city's administration. However, in 1793, in the Second Partition of Poland, Poznań, came under the control of the Kingdom of Prussia, becoming part of (and initially the seat of) the province of South Prussia.

The Prussian authorities expanded the city boundaries, making the walled city and its closest suburbs into a single administrative unit. Left-bank suburbs were incorporated in 1797, and Ostrów Tumski, Chwaliszewo, Śródka, Ostrówek and Łacina (St. Roch) in 1800. The old city walls were taken down in the early 19th century, and major development took place to the west of the old city, with many of the main streets of today's city centre being laid out.

In the Greater Poland Uprising of 1806, Polish soldiers and civilian volunteers assisted the efforts of Napoleon by driving out Prussian forces from the region. The city became a part of the Duchy of Warsaw in 1807, and was the seat of Poznań Department – a unit of administrative division and local government. However, in 1815, following the Congress of Vienna, the region was returned to Prussia, and Poznań became the capital of the semi-autonomous Grand Duchy of Posen.

The city continued to expand, and various projects were funded by Polish philanthropists, such as the Raczyński Library and the Bazar hotel. The city's first railway, running to Stargard, opened in 1848. Due to its strategic location, the Prussian authorities intended to make Poznań into a fortress city, building a ring of defensive fortifications around it. Work began on the citadel (Fort Winiary) in 1828, and in subsequent years the entire set of defences ("Festung Posen") was completed.

A Greater Poland Uprising during the Revolutions of 1848 was ultimately unsuccessful, and the Grand Duchy lost its remaining autonomy, Poznań becoming simply the capital of the Prussian Province of Posen. It would become part of the German Empire with the unification of German states in 1871. Polish patriots continued to form societies (such as the Central Economic Society for the Grand Duchy of Poznań), and a Polish theatre ("Teatr Polski", still functioning) opened in 1875; however the authorities made efforts to Germanize the region, particularly through the Prussian Settlement Commission (founded 1886). Germans accounted for 38% of the city's population in 1867, though this percentage would later decline somewhat, particularly after the region returned to Poland.

Another expansion of "Festung Posen" was planned, with an outer ring of more widely spaced forts around the perimeter of the city. Building of the first nine forts began in 1876, and nine intermediate forts were built from 1887. The inner ring of fortifications was now considered obsolete and came to be mostly taken down by the early 20th century (although the citadel remained in use). This made space for further civilian construction, particularly the Imperial Palace ("Zamek"), completed 1910, and other grand buildings around it (including today's central university buildings and the opera house). The city's boundaries were also significantly extended to take in former suburban villages: Piotrowo and Berdychowo in 1896, Łazarz, Górczyn, Jeżyce and Wilda in 1900, and Sołacz in 1907.
At the end of World War I, the final Greater Poland Uprising (1918–1919) brought Poznań and most of the region back to newly reborn Poland, which was confirmed by the Treaty of Versailles. The local German populace had to acquire Polish citizenship or leave the country. This led to a wide emigration of the ethnic Germans of the town's population. The town's German population decreased from 65,321 in 1910 to 5,980 in 1926 and further to 4,387 in 1934. In the interwar Second Polish Republic, the city again became the capital of Poznań Voivodeship. Poznań's university (today called Adam Mickiewicz University) was founded in 1919, and in 1925 the Poznań International Fairs began. In 1929 the fairs site was the venue for a major National Exhibition ("Powszechna Wystawa Krajowa", popularly "PeWuKa") marking the tenth anniversary of independence; it attracted around 4.5 million visitors. The city's boundaries were again expanded in 1925 (to include Główna, Komandoria, Rataje, Starołęka, Dębiec, Szeląg and Winogrady) and 1933 (Golęcin, Podolany).

During the German occupation of 1939–1945, Poznań was incorporated into the Third Reich as the capital of "Reichsgau Wartheland". Many Polish inhabitants were executed, arrested, expelled to the General Government or used as forced labour; at the same time many Germans and Volksdeutsche were settled in the city. The German population increased from around 5,000 in 1939 (some 2% of the inhabitants) to around 95,000 in 1944. The pre-war Jewish population of about 2,000 were mostly murdered in the Holocaust. A concentration camp was set up in Fort VII, one of the 19th-century perimeter forts. The camp was later moved to Żabikowo south of Poznań. The Nazi authorities significantly expanded Poznań's boundaries to include most of the present-day area of the city; these boundaries were retained after the war. Poznań was captured by the Red Army, assisted by Polish volunteers, on 23 February 1945 following the Battle of Poznań, in which the German army conducted a last-ditch defence in line with Hitler's designation of the city as a "Festung". The Citadel was the last point to be taken, and the fighting left much of the city, particularly the Old Town, in ruins.

Due to the expulsion and flight of German population Poznań's post-war population was almost uniformly Polish. The city again became a voivodeship capital; in 1950 the size of Poznań Voivodeship was reduced, and the city itself was given separate voivodeship status. This status was lost in the 1975 reforms, which also significantly reduced the size of Poznań Voivodeship.

The Poznań 1956 protests are seen as an early instance of discontent with communist rule. In June 1956, a protest by workers at the city's Cegielski locomotive factory developed into a series of strikes and popular protests against the policies of the government. After a protest march on 28 June was fired on, crowds attacked the communist party and secret police headquarters, where they were repulsed by gunfire. Riots continued for two days until being quelled by the army; 67 people were killed according to official figures. A monument to the victims was erected in 1981 at Plac Mickiewicza.

The post-war years had seen much reconstruction work on buildings damaged in the fighting. From the 1960s onwards intensive housing development took place, consisting mainly of pre-fabricated concrete blocks of flats, especially in Rataje and Winogrady, and later (following its incorporation into the city in 1974) Piątkowo. Another infrastructural change (completed in 1968) was the rerouting of the river Warta to follow two straight branches either side of Ostrów Tumski.

The most recent expansion of the city's boundaries took place in 1987, with the addition of new areas mainly to the north, including Morasko, Radojewo and Kiekrz. The first free local elections following the fall of communism took place in 1990. With the Polish local government reforms of 1999, Poznań again became the capital of a larger province (Greater Poland Voivodeship). It also became the seat of a "powiat" ("Poznań County"), with the city itself gaining separate "powiat" status.

Recent infrastructural developments include the opening of the fast tram route ("Poznański Szybki Tramwaj", popularly "Pestka") in 1997, and Poznań's first motorway connection (part of the A2 "autostrada") in 2003. In 2006 Poland's first F-16 Fighting Falcons came to be stationed at the 31st Air Base in Krzesiny in the south-east of the city.

Poznań continues to host regular trade fairs and international events, including the United Nations Climate Change Conference in 2008. It was one of the host cities for UEFA Euro 2012.

Poznań covers an area of , and has coordinates in the range 52°17'34<nowiki>"</nowiki>–52°30'27<nowiki>"</nowiki>N, 16°44'08<nowiki>"</nowiki>–17°04'28<nowiki>"</nowiki>E. Its highest point, with an altitude of , is the summit of "Góra Moraska" (Morasko Hill) within the Morasko meteorite nature reserve in the north of the city. The lowest altitude is , in the Warta valley.
Poznań's main river is the Warta, which flows through the city from south to north. As it approaches the city centre it divides into two branches, flowing west and east of Ostrów Tumski (the cathedral island) and meeting again further north. The smaller Cybina river flows through eastern Poznań to meet the east branch of the Warta (that branch is also called Cybina – its northern section was originally a continuation of that river, while its southern section has been artificially widened to form a main stream of the Warta). Other tributaries of the Warta within Poznań are the Junikowo Stream "(Strumień Junikowski)", which flows through southern Poznań from the west, meeting the Warta just outside the city boundary in Luboń; the Bogdanka and Wierzbak, formerly two separate tributaries flowing from the north-west and along the north side of the city centre, now with their lower sections diverted underground; the Główna, flowing through the neighbourhood of the same name in north-east Poznań; and the Rose Stream "(Strumień Różany)" flowing east from Morasko in the north of the city. The course of the Warta in central Poznań was formerly quite different from today: the main stream ran between Grobla and Chwaliszewo, which were originally both islands. The branch west of Grobla (the "Zgniła Warta" – "rotten Warta") was filled in late in the 19th century, and the former main stream west of Chwaliszewo was diverted and filled in during the 1960s. This was done partly to prevent floods, which did serious damage to Poznań frequently throughout history.
Poznań's largest lake is "Jezioro Kierskie" (Kiekrz Lake) in the extreme north-west of the city (within the city boundaries since 1987). Other large lakes include Malta (an artificial lake on the lower Cybina, formed in 1952), "Jezioro Strzeszyńskie" (Strzeszyn Lake) on the Bogdanka, and Rusałka, an artificial lake further down the Bogdanka, formed in 1943. The latter two are popular bathing places. Kiekrz Lake is much used for sailing, while Malta is a competitive rowing and canoeing venue.

The city centre (including the Old Town, the former islands of Grobla and Chwaliszewo, the main street "Święty Marcin" and many other important buildings and districts) lies on the west side of the Warta. Opposite it between the two branches of the Warta is Ostrów Tumski, containing Poznań Cathedral and other ecclesiastical buildings, as well as housing and industrial facilities. Facing the cathedral on the east bank of the river is the historic district of Śródka. Large areas of apartment blocks, built from the 1960s onwards, include Rataje in the east, and Winogrady and Piątkowo north of the centre. Older residential and commercial districts include those of Wilda, Łazarz and Górczyn to the south, and Jeżyce to the west. There are also significant areas of forest within the city boundaries, particularly in the east adjoining Swarzędz, and around the lakes in the north-west.

For more details on Poznań's geography, see the articles on the five districts: Stare Miasto, Nowe Miasto, Jeżyce, Grunwald and Wilda.
The climate of Poznań is within the transition zone between a humid continental and oceanic climate (Köppen: "Cfb" to "Dfb" although it totally fits in the second in the 0 °C isotherm) and with relatively cold winters and warm summers. Snow is common in winter, when night-time temperatures are typically below zero. In summer temperatures may often reach . Annual rainfall is more than , among the lowest in Poland. The rainiest month is July, mainly due to short but intense cloudbursts and thunderstorms. The number of hours of sunshine are among the highest in the country. Climate in this area has mild differences between highs and lows, and there is adequate rainfall year-round. The Köppen Climate Classification subtype for this climate is "humid continental climate).

Poznań is divided into 42 neighbourhoods ("see" osiedle), each of which has its own elected council with certain decision-making and spending powers. The first uniform elections for these councils covering the whole area of the city were held on 20 March 2011.

For certain administrative purposes, the old division into five districts (dzielnicas) is used – although these ceased to be governmental units in 1990. These were:

Many citizens of Poznań thanks to the strong economy of the city and high salaries started moving to suburbs of the Poznań County (powiat) in the 1990s. Although the number of inhabitants in Poznań itself was decreasing for the past two decades, the suburbs gained almost twice as many inhabitants. Thus, Poznań urban area has been growing steadily over past years and has already reached 1.0 million inhabitants when student population is included, whereas the entire metropolitan zone may have reached 1.5–1.7 million inhabitants when satellite cities and towns (so-called second Poznań ring counties such as Września, Gniezno and Kościan) are included. The complex infrastructure, population density, number of companies and gross product per capita of Poznań suburbs may be only compared to Warsaw suburbs. Many parts of closer suburbs (for example Tarnowo Podgorne, Komorniki, Suchy Las, Dopiewo) produce more in terms of GDP per capita than the city itself.

Poznań has been an important trade centre since the Middle Ages. Starting in the 19th century, local heavy industry began to grow. Several major factories were built, including the Hipolit Cegielski steel mill and railway factory (see H. Cegielski - Poznań S.A.).

Nowadays Poznań is one of the major trade centres in Poland. Poznań is regarded as the second most prosperous city in Poland after Warsaw. The city of Poznań produced PLN 31.8 billion of Poland's gross domestic product in 2006. It boasts a GDP per capita of 200,4% (2008) of Poland's average. Furthermore, Poznań had very low unemployment rate of 2.3% as of May 2009. For comparison, Poland's national unemployment rate was over 10%.

Many Western European companies have established their Polish headquarters in Poznań or in the nearby towns of Tarnowo Podgórne and Swarzędz. Most foreign investors are German and Dutch companies, with a few others. The best known examples of corporation who have their headquarters in Poznań and the surrounding areas are that of GlaxoSmithKline, Raben Group (near Kórnik) and Kuehne + Nagel (near Gądki).

Investors are mostly from the food processing, furniture, automotive and transport and logistics industries. Foreign companies are primarily attracted by low labour costs and by the relatively good road and railway network, good vocational skills of workers and relatively liberal employment laws.

The recently built Stary Browar shopping centre contains many high-end shops and is considered one of the best in Europe. It has won an award for the best shopping centre in the world in the medium-sized commercial buildings category. Other notable shopping centres in the city include Galeria Malta, one of the largest in Central Europe, and the shops at the Hotel Bazar, a historical hotel and commercial centre in the Old Town.

Some of the best-known major corporations founded and still based in Poznań and the city's metropolitan area include Allegro, Poland's biggest online auction site, H. Cegielski-Poznań SA, a historic manufacturer, Solaris Bus & Coach, a modern bus and coach maker based in Bolechowo and Enea S.A., one of the country's biggest energy firms. Poznań is also where the software development leader Netguru was founded, one of the fastest growing companies in Europe. Kompania Piwowarska based in Poznań produces some of Poland's best known beers, and includes not only the local Lech Brewery but also Tyskie from Tychy and Dojlidy Brewery from Białystok among many others.

Poznań has an extensive public transport system, mostly consisting of trams, such as the Poznań Fast Tram, and both urban and suburban buses. The main railway station is Poznań Central Station to the southwest of the city centre; there is also the smaller "Poznań Wschód" and "Poznań Garbary" station northeast of the centre and a number of other stations on the outskirts of the city. The main east-west A2 motorway runs south of the city connecting it with Berlin in the west and Łódż and Warsaw in the east; other main roads run in the direction of Warsaw, Bydgoszcz, Wągrowiec, Oborniki, Katowice, Wrocław, Buk and Berlin.
Poznań has one of the biggest airports in the west of Poland called Poznań-Ławica Airport. In 2016 it handled approximately 1.71 million passengers.

Poznań has many historic buildings and sights, mostly concentrated around the Old Town and other parts of the city centre. Many of these lie on the Royal-Imperial Route in Poznań – a tourist walk leading through the most important parts of the city showing its history, culture and identity. Portions of the city centre are listed as one of Poland's official national Historic Monuments ("Pomnik historii"), as designated 28 November 2008, along with other portions of the city's historic core. Its listing is maintained by the National Heritage Board of Poland.

Results of new extensive archaeological research performed on Poznań's Ostrów Tumski by Prof. dr hab. Hanna Kocka-Krec from Instytut Prahistorii UAM indicate that Poznań indeed was a central site of the early Polish State (recent discovery of first Polish ruler, Mieszko I's Palatium). Thus, the Tumski Island is more important than it was thought previously, and may have been as important as Gniezno in the Poland of first Piasts. Though it is currently under construction, Ostrów Tumski of Poznań should soon have a very rich historical exposition and be a very interesting place for visitors. It promises to include many attractions, such as the above-mentioned Cathedral, Church of St. Mary the Virgin, Lubranski Academy and they opened in 2012 Genius Loci Archeological Park as well as planned to be opened in 2013 Interactive Center of Ostrów Tumski History (ICHOT) that presents a multimedia museum of the Polish State through many different periods. The Palatium in Poznań will be also transformed into a museum, although more funds are needed. When all the expositions are ready, in a couple of years, Ostrów Tumski may be as worth visiting as the Wawel castle of Kraków. There is a very famous sentence illustrating the importance of Ostrów Tumski in Poznań by the Pope John Paul II: ""Poland began here"".

A popular venue is Malta, a park with an artificial lake in its centre. On one bank of the lake there are ski and sleigh slopes (Malta Ski), on the opposite bank a huge complex of swimming pools including an Olympic-size one (Termy Maltanskie).

An important cultural event in Poznań is the annual Malta Festival, which takes place at many city venues, usually in late June and early July. It hosts mainly modern experimental off-theatre performances, often taking place on squares and other public spaces. It also includes cinema, visual, music and dancing events. Malta Theatre Festival gave birth to many off-theater groups, expressing new ideas in an already rich theatrical background of the city. Thus, Poznań with a great deal of off-theaters and their performances has recently become a new Polish off-theater performance centre.

Classical music events include the Henryk Wieniawski Violin Competition (held every 5 years), and classical music concerts by the city's Philharmonic Orchestra held each month in the University "Aula". Especially popular are concerts by the Poznań Nightingales.
Poznań is also home to new forms of music such as rap and hip-hop made by a great deal of bands and performers ("Peja", "Mezo" and others). Poznań is also known for its rock music performers (Muchy, Malgorzata Ostrowska).

Poznań apart from many traditional theatres with a long history ("Teatr Nowy", "Teatr Wielki", "Teatr Polski", "Teatr Muzyczny" and several others) is also home to a growing number of alternative theatre groups, some of them stemming from International Malta Festival: "Teatr Strefa Ciszy", "Teatr Porywcze Cial", "Teatr Usta Usta", "Teatr u Przyjaciol", "Teatr Biuro Podrozy", "Teatr Osmego Dnia" and many others – it is believed that even up to 30 more or less known groups may work in the city.

Every year on 11 November, Poznanians celebrate The Day of St. Martin. A procession of horses, with St. Martin at the head, parades along St Martin Street, in front of The Imperial Castle. Everybody can eat delicious croissants, the regional product of Poznań. 

Poznań hosted the 2009 European Young Adults Meeting of the ecumenical Christian Taizé Community.

Poznań also stages the Ale Kino! International Young Audience Film Festival in December and "Off Cinema" festival of independent films. Other festivals: "Transatlantyk (film music festival by Jan A.P. Kaczmarek started in 2011), Maski Theater Festival, Dance International Workshops by Polish Dance Theater, Made in Chicago (Jazz Festival), Ethno Port, Festival of Ice Sculpture, Animator, Science and Art Festival, Tzadik (Jewish music festival), Meditations Biennale (Modern Art). 

Poznań has several cinemas, including multiplexes and smaller cinemas, an opera house, several other theatres, and museums. The Rozbrat social centre, a squatted former factory in Jeżyce, serves as a home for independent and open-minded culture. It hosts frequent gigs, an anarchistic library, vernissages, exhibitions, annual birthday festival (each October), poetry evenings and graffiti festivals. The city centre has many clubs, pubs and coffee houses, mainly in the area of the Old Town.

The city is also home to one of the oldest zoological gardens in Poland, the Old Zoo in Poznań, which was established in 1874.

Grażyna Kulczyk's effort to build the Museum of Contemporary and Performance Arts in Poznań was rejected.

Poznań is one of the four largest academic centres in Poland. The number of students in the city of Poznań is about 140,000 (fourth/third after Warsaw, Kraków and close to Wrocław student population). Every one of four inhabitants in Poznań is a student.

Since Poznań is smaller than Warsaw or Kraków still having a very large number of students it makes the city even more vibrant and dense academic hub than both former and current capitals of Poland. Poznań, with its almost 30 colleges and universities, has the second richest educational offering in Poland after Warsaw.

The city has many state-owned universities. Adam Mickiewicz University (abbreviated "UAM" in Polish, "AMU" in English) is one of the most influential and biggest universities in Poland:


Adam Mickiewicz University is one of the three best universities in Poland after University of Warsaw and University of Kraków. They all have a very high number of international student and scientist exchange, research grants and top publications.
In northern suburbs of Poznań a very large "Morasko Campus" has been built (Faculty of Biology, Physics, Mathematics, Chemistry, Political Sciences, Geography). The majority of faculties are already open, although a few more facilities will be constructed. The campus infrastructure belongs to the most impressive among Polish universities. Also, there are plans for "Uniwersytecki Park Historii Ziemii" (Earth History Park), one of the reason for the park construction is a "Morasko meteorite nature reserve" situated close by, it is one of the rare sites of Europe where a number of meteorites fell and some traces may be still seen.

There is also a great number of smaller, mostly private-run colleges and institutions of higher education "("Uczelnie")":


Poznań has numerous high schools, each with a different programme focusing on different subjects. Some of the most notable are:



Poznań is famous for its football teams, Warta Poznań, which was one of the most successful clubs in pre-war history, and Lech Poznań, who are currently one of the biggest clubs in the country, frequently playing in European cups and have many fans from all over the region. Lech plays at the Municipal Stadium, which hosted the 2012 European Championship group stages as well as the opening game and the final of the U-19 Euro Championship in June 2006. Warta plays at the small Dębińska Road Stadium; a former training ground for Edmund Szyc Stadium however since the latter fell into disrepair in 1998 and was sold in 2001 it became the teams main ground;
the club does have aims to restore and return to the historical 60 000 capacity stadium.

The city's third professional football team Olimpia Poznań ceased activity in 2004, focusing on other sports, and remains one of the best judo and tennis clubs in the country, the latter hosting the Poznań Open tournament at the Tennis Park. The club is a large sports complex surrounded by Lake Rusałka, and apart from the tennis facilities boasts a large city recreation area: mountain biking facilities including a four-cross track; an athletics stadium (capacity 3000); and a football-speedway stadium (capacity 20 000), which fell into vast disrepair until it was acquired by the city council from the police in 2013 and was renovated. The football-speedway stadium hosts speedway club PSŻ Poznań, rugby union side NKR Chaos, American football team the Poznań Patriots, and football team Poznaniak Poznań.

The city has the largest circuit in Poland, Tor Poznań, located in the suburbs in Przeźmierowo. Lake Malta hosted the World Rowing Championships in 2009 and has previously hosted some regattas in the Rowing World Cup. It also hosted the ICF Canoe Sprint World Championships (sprint canoe) in 1990 and 2001, and again in 2010. Also near the lake the "Malta Ski" year-long skiing complex hosts minor sport competitions, and is also equipped with a toboggan run and a minigolf course. There is also a roller rink with a roller skating club nearby.

Poznań has experience as a host for international sporting events such as the official 2009 EuroBasket.

The city is also considered to be the hotbed of Polish field hockey, with several top teams: Warta Poznań; Grunwald Poznań; which also has shooting, wrestling, handball and tennis sections; Pocztowiec Poznań; and AZS AWF Poznań, the student club which also fields professional teams in women's volleyball and basketball (AZS Poznań).

Other clubs include: Posnania Poznań, one of the best rugby union clubs in the country; Polonia Poznań, formerly a multi-sports club with many successes in rugby, however today only a football section remains; KKS Wiara Lecha, football club formed by the supporters of Lech Poznań; and Odlew Poznań, arguably the most famous amateur club in the country due to their extensive media coverage and humorous exploits. There are also numerous rhythmic gymnastics and synchronised swimming clubs, as well as numerous less notable amateur football teams.

Poznań bid for the 2014 Summer Youth Olympics but lost to Nanjing, with the Chinese city receiving 47 votes over Poznań's 42.

Since the end of the communist era in 1989, Poznań municipality and suburban area have invested heavily in infrastructure, especially public transport and administration. There is massive investment from foreign companies in Poznań as well as in communities west and south of Poznań (namely, Kórnik and Tarnowo Podgórne).

City investments into transportation were mostly into public transport. While the number of cars since 1989 has at least doubled, municipal policy concentrated on improving public transport. Limiting car access to the city centre, building new tram lines (including Poznański Szybki Tramwaj) and investing in new rolling stock (such as modern Combino trams by Siemens and Solaris low-floor buses) actually increased the level of ridership.

Future investments into transportation include the construction of a third bypass of Poznań, and the completion of A2 (E30) motorway towards Berlin. New cycle lanes are being built, linking to existing ones, and an attempt is currently being made to develop a Karlsruhe-style light rail system for commuters. All this is made more complicated (and more expensive) by the heavy neglect of transport infrastructure throughout the Communist era.

Poznań is twinned with:





</doc>
<doc id="23725" url="https://en.wikipedia.org/wiki?curid=23725" title="Peter Falk">
Peter Falk

Peter Michael Falk (September 16, 1927 – June 23, 2011) was an American actor and comedian, known for his role as Lieutenant Columbo in the long-running television series "Columbo" (1968–2003), for which he won four Primetime Emmy Awards (1972, 1975, 1976, 1990) and a Golden Globe Award (1973). He first starred as Columbo in two 90-minute TV pilots; the first with Gene Barry in 1968 and the second with Lee Grant in 1971. The show then aired as part of "The NBC Mystery Movie" series from 1971 to 1978, and again on ABC from 1989 to 2003.

Falk was twice nominated for the Academy Award for Best Supporting Actor, for "Murder, Inc." (1960) and "Pocketful of Miracles" (1961), and won his first Emmy Award in 1962 for "The Dick Powell Theatre". He was the first actor to be nominated for an Academy Award and an Emmy Award in the same year, achieving the feat twice (1961/62). He went on to appear in such films as "It's a Mad, Mad, Mad, Mad World" (1963), "The Great Race" (1965), "Anzio" (1968), "A Woman Under the Influence" (1974), "Murder by Death" (1976), "Mikey and Nicky" (1976), "The Cheap Detective" (1978), "The In-Laws "(1979), "The Princess Bride" (1987), "Wings of Desire" (1987), "The Player" (1992), and "Next" (2007), as well as many television guest roles.

Director William Friedkin said of Falk's role in his film "The Brink's Job" (1978): "Peter has a great range from comedy to drama. He could break your heart or he could make you laugh." In 1996, "TV Guide" ranked Falk No. 21 on its 50 Greatest TV Stars of All Time list. He received posthumously a star on the Hollywood Walk of Fame in 2013.

Born in New York City, Falk was the son of Michael Peter Falk (1898–1981), owner of a clothing and dry goods store, and his wife, Madeline (née Hochhauser; 1904–2001), an accountant and buyer. Both of his parents were Jewish, coming from Poland and Russia on his father's side, and from Hungary and Łabowa, Nowy Sącz County, Poland, on his mother's side. Falk grew up in Ossining, New York.

Falk's right eye was surgically removed when he was three because of a retinoblastoma; he wore an artificial eye for most of his life. The artificial eye was the cause of his trademark squint. Despite this limitation, as a boy he participated in team sports, mainly baseball and basketball. In a 1997 interview in "Cigar Aficionado" magazine with Arthur Marx, Falk said: "I remember once in high school the umpire called me out at third base when I was sure I was safe. I got so mad I took out my glass eye, handed it to him and said, 'Try this.' I got such a laugh you wouldn't believe."
Falk's first stage appearance was at the age of 12 in "The Pirates of Penzance" at Camp High Point in upstate New York, where one of his camp counselors was Ross Martin (they would later act together in "The Great Race" and the "Columbo" episode "Suitable For Framing"). Falk attended Ossining High School in Westchester County, New York, where he was a star athlete and president of his senior class. 

After graduating from high school in 1945, Falk briefly attended Hamilton College in Clinton, New York. He then tried to join the armed services as World War II was drawing to a close. Rejected because of his missing eye, he joined the United States Merchant Marine and served as a cook and mess boy. Falk said of the experience in 1997: "There they don't care if you're blind or not. The only one on a ship who has to see is the captain. And in the case of the "Titanic", he couldn't see very well, either." Falk recalls this period in his autobiography: "A year on the water was enough for me, so I returned to college. I didn't stay long. Too itchy. What to do next? I signed up to go to Israel to fight in the war on its attack on Egypt; I wasn't passionate about Israel, I wasn't passionate about Egypt, I just wanted more excitement… I got assigned a ship and departure date but the war was over before the ship ever sailed."

After a year and a half in the Merchant Marine, Falk returned to Hamilton College and also attended the University of Wisconsin. He transferred to the New School for Social Research in New York City, which awarded him a bachelor's degree in literature and political science in 1951.

Falk then traveled in Europe and worked on a railroad in Yugoslavia for six months. He returned to New York, enrolling at Syracuse University, but he recalled in his 2006 memoir, "Just One More Thing", that he was unsure what he wanted to do with his life for years after leaving high school.

Falk obtained a Master of Public Administration degree at the Maxwell School of Syracuse University in 1953. The program was designed to train civil servants for the federal government, a career that Falk said in his memoir he had "no interest in and no aptitude for". 

He applied for a job with the CIA, but he was rejected because of his membership in the Marine Cooks and Stewards Union while serving in the Merchant Marine, even though he was required to join and was not active in the union (which had been under fire for communist leanings). He then became a management analyst with the Connecticut State Budget Bureau in Hartford. In 1997, Falk characterized his Hartford job as "efficiency expert": "I was such an efficiency expert that the first morning on the job, I couldn't find the building where I was to report for work. Naturally, I was late, which I always was in those days, but ironically it was my tendency never to be on time that got me started as a professional actor."

While working in Hartford, Falk joined a community theater group called the Mark Twain Masquers, where he performed in plays that included "The Caine Mutiny Court-Martial", "The Crucible", and "The Country Girl" by Clifford Odets. Falk also studied with Eva Le Gallienne, who was giving an acting class at the White Barn Theatre in Westport, Connecticut. Falk later recalled how he "lied his way" into the class, which was for professional actors. He drove down to Westport from Hartford every Wednesday, when the classes were held, and was usually late. In his 1997 interview with Arthur Marx in "Cigar Aficionado" Magazine, Falk said of Le Gallienne: "One evening when I arrived late, she looked at me and asked, 'Young man, why are you always late?' and I said, 'I have to drive down from Hartford.'" She looked down her nose and said, "What do you do in Hartford? There's no theater there. How do you make a living acting?" Falk confessed he wasn't a professional actor. According to him Le Gallienne looked at him sternly and said: "Well, you should be." He drove back to Hartford and quit his job. Falk stayed with the Le Gallienne group for a few months more, and obtained a letter of recommendation from Le Galliene to an agent at the William Morris Agency in New York. In 1956, he left his job with the Budget Bureau and moved to Greenwich Village to pursue an acting career.

Falk's first New York stage role was in an Off-Broadway production of Molière's "Don Juan" at the Fourth Street Theatre that closed after its only performance on January 3, 1956. Falk played the second lead, Sganarelle. His next theater role proved far better for his career. In May, he appeared at Circle in the Square in a revival of "The Iceman Cometh" with Jason Robards playing the bartender.

Later in 1956, Falk made his Broadway debut, appearing in Alexander Ostrovsky's "Diary of a Scoundrel". As the year came to an end, he appeared again on Broadway as an English soldier in Shaw's "Saint Joan" with Siobhán McKenna.

In 1972, Falk appeared in Broadway's "The Prisoner of Second Avenue". According to film historian Ephraim Katz: "His characters derive added authenticity from his squinty gaze, the result of the loss of an eye ...".

Despite his stage success, a theatrical agent advised Falk not to expect much film acting work because of his artificial eye. He failed a screen test at Columbia Pictures and was told by studio boss Harry Cohn: "For the same price I can get an actor with two eyes." He also failed to get a role in the film "Marjorie Morningstar", despite a promising interview for the second lead. His first film performances were in small roles in "Wind Across the Everglades" (1958), "The Bloody Brood" (1959) and "Pretty Boy Floyd" (1960). Falk's performance in "Murder, Inc." (1960) was a turning point in his career. He was cast in the supporting role of killer Abe Reles in a film based on the real-life murder gang of that name that terrorized New York in the 1930s. "The New York Times" film critic Bosley Crowther, while dismissing the movie as "an average gangster film", singled out Falk's "amusingly vicious performance". Crowther wrote: 

The film turned out to be Falk's breakout role. In his autobiography, "Just One More Thing" (2006), Falk said his selection for the film from thousands of other Off-Broadway actors was a "miracle" that "made my career" and that without it, he would not have received the other significant movie roles that he later played. Falk, who played Reles again in the 1960 TV series "The Witness", was nominated for a Best Supporting Actor Academy Award for his performance in the film.
In 1961, multiple Academy Award-winning director Frank Capra cast Falk in the comedy "Pocketful of Miracles". The film was Capra's last feature, and although it was not the commercial success he hoped it would be, he "gushed about Falk's performance". Falk was nominated for an Oscar for the role. In his autobiography, Capra wrote about Falk:
For his part, Falk says he "never worked with a director who showed greater enjoyment of actors and the acting craft. There is nothing more important to an actor than to know that the one person who represents the audience to you, the director, is responding well to what you are trying to do." Falk once recalled how Capra reshot a scene even though he yelled "Cut and Print," indicating the scene was finalized. When Falk asked him why he wanted it reshot: "He laughed and said that he loved the scene so much he just wanted to see us do it again. How's that for support!"

For the remainder of the 1960s, Falk had mainly supporting movie roles and TV guest-starring appearances. Falk turned in a gem of a performance as one of two cabbies who falls victim to greed in the epic 1963 star-studded comedy "It's a Mad, Mad, Mad, Mad World", although he only appears in the last fifth of the movie. His other roles included the character of Guy Gisborne in the Rat Pack musical comedy "Robin and the 7 Hoods" (1964), in which he sings one of the film's numbers, and the spoof "The Great Race" (1965) with Jack Lemmon and Tony Curtis.

Falk first appeared on television in 1957, in the dramatic anthology programs that later became known as the "Golden Age of Television". In 1957, he appeared in one episode of "Robert Montgomery Presents." He was also cast in "Studio One," "Kraft Television Theater," "New York Confidential," "Naked City," "The Untouchables", "Have Gun–Will Travel," "The Islanders," and "Decoy" with Beverly Garland cast as the first female police officer in a series lead. On "The Twilight Zone" he portrayed a Castro-type revolutionary complete with beard who, intoxicated with power, kept seeing his would-be assassins in a newly acquired magic mirror. He starred in two of Alfred Hitchcock's television series, as a gangster terrified of death in a 1961 episode of "Alfred Hitchcock Presents" and as a homicidal evangelist in 1962's "The Alfred Hitchcock Hour".

In 1961, Falk was nominated for an "Emmy Award" for his performance in the episode "Cold Turkey" of James Whitmore's short-lived series "The Law and Mr. Jones" on ABC. On September 29, 1961, Falk and Walter Matthau guest-starred in the premiere episode, "The Million Dollar Dump", of ABC's crime drama "", with Stephen McNally and Robert Harland. He won an Emmy for "The Price of Tomatoes", a drama carried in 1962 on "The Dick Powell Show".

In 1963, Falk and Tommy Sands appeared as brothers who disagreed on the route for a railroad in "The Gus Morgan Story" on ABC's "Wagon Train". Falk played the title role of "Gus", and Sands was his younger brother, Ethan Morgan. Ethan accidentally shoots wagonmaster Chris Hale, played by John McIntire, while the brothers are in the mountains looking at possible route options. Gus makes the decision to leave Hale behind- even choking him, believing he is dead. Ethan has been overcome with oxygen deprivation and needs Gus' assistance to reach safety down the mountain. Unknown to the Morgans, Hale crawls down the mountain through snow, determined to obtain revenge against Gus. In time, though, Hale comes to understand the difficult choice Morgan had to make, and the brothers reconcile their own differences. This episode is remembered for its examination of how far a man will persist amid adversity to preserve his own life and that of his brother.

Falk's first television series was in the title role of the drama "The Trials of O'Brien", in which he played a lawyer. The show ran in 1965 and 1966 and was cancelled after 22 episodes. In 1966, he also co-starred in a television production of "Brigadoon" with Robert Goulet.

In 1971, Pierre Cossette produced the first Grammy Awards show on television with some help from Falk. Cossette writes in his autobiography, "What meant the most to me, though, is the fact that Peter Falk saved my ass. I love show business, and I love Peter Falk."

Although Falk appeared in numerous other television roles in the 1960s and 1970s, he is best known as the star of the TV series "Columbo", "everyone's favorite rumpled television detective". His character was a shabby and ostensibly absent-minded police detective lieutenant driving a Peugeot 403, who had first appeared in the 1968 film "Prescription: Murder". Rather than a whodunit, the show typically revealed the murderer from the beginning, then showed how the Los Angeles police detective Columbo went about solving the crime. Falk would describe his role to Fantle:
Television critic Ben Falk (no relation) added that Falk "created an iconic cop… who always got his man (or woman) after a tortuous cat-and-mouse investigation". He also noted the idea for the character was "apparently inspired by Dostoyevsky's dogged police inspector, Porfiry Petrovich, in the novel "Crime and Punishment".

Falk tries to analyze the character and notes the correlation between his own personality and Columbo's:
With "general amazement", Falk notes: "The show is all over the world. I've been to little villages in Africa with maybe one TV set, and little kids will run up to me shouting, 'Columbo, Columbo!'" Singer Johnny Cash recalled acting in one episode, and although he was not an experienced actor, he writes in his autobiography: "Peter Falk was good to me. I wasn't at all confident about handling a dramatic role, and every day he helped me in all kinds of little ways."

The first episode of "Columbo" as a series was directed in 1971 by a 24-year-old Steven Spielberg in one of his earliest directing jobs. Falk recalled the episode to Spielberg biographer Joseph McBride:
The character of Columbo had previously been played by Bert Freed in a single television episode of "The Chevy Mystery Show" in 1960, and by Thomas Mitchell on Broadway. Falk first played Columbo in "Prescription: Murder", a 1968 TV movie, and the 1970 pilot for the series, "Ransom for a Dead Man". From 1971 to 1978, "Columbo" aired regularly on NBC as part of the umbrella series "NBC Mystery Movie". All episodes were of TV movie length, in a 90- or 120-minute slot including commercials. In 1989, the show returned on ABC in the form of a less frequent series of TV movies, still starring Falk, airing until 2003. Falk won four Emmys for his role as Columbo.

"Columbo" was so popular, co-creator William Link wrote a series of short stories published as "The Columbo Collection" (Crippen & Landru, 2010) which includes a drawing by Falk of himself as Columbo, and the cover features a caricature of Falk/Columbo by Al Hirschfeld.

Falk was a close friend of independent film director John Cassavetes and appeared in his films "Husbands", "A Woman Under the Influence", and, in a cameo, at the end of "Opening Night". Cassavetes guest-starred in the "Columbo" episode "Étude in Black" in 1972; Falk, in turn, co-starred with Cassavetes in the 1976 film "Mikey and Nicky". Falk describes his experiences working with Cassavetes, specifically remembering his directing strategies: "Shooting an actor when he might be unaware the camera was running." 

In 1978, Falk appeared on the comedy TV show "The Dean Martin Celebrity Roast", portraying his Columbo character, with Frank Sinatra the evening's victim.
Falk continued to work in films, including his performance as a questionable ex-CIA agent of dubious sanity in the comedy "The In-Laws". Director Arthur Hiller said during an interview that the "film started out because Alan Arkin and Peter Falk wanted to work together. They went to Warner Brother's and said, 'We'd like to do a picture', and Warner said fine ... and out came "The In-laws" ... of all the films I've done, "The In-laws" is the one I get the most comments on." Movie critic Roger Ebert compared the film with a later remake:
Falk appeared in "The Great Muppet Caper", "The Princess Bride", "Murder by Death", "The Cheap Detective", "Vibes", "Made", and in Wim Wenders' 1987 German language film "Wings of Desire" and its 1993 sequel, "Faraway, So Close!." In "Wings of Desire", Falk played a semi-fictionalized version of himself, a famous American actor who had once been an angel, but who had grown disillusioned with only observing life on Earth and had in turn given up his immortality. Falk described the role as "the craziest thing that I've ever been offered", but he earned critical acclaim for his supporting performance in the film.

In 1998, Falk returned to the New York stage to star in an Off-Broadway production of Arthur Miller's "Mr. Peters' Connections". His previous stage work included shady real estate salesman Shelley "the Machine" Levine in the 1986 Boston/Los Angeles production of David Mamet's prizewinning "Glengarry Glen Ross".

Falk starred in a trilogy of holiday television movies – "A Town Without Christmas" (2001), "Finding John Christmas" (2003), and "When Angels Come to Town" (2004) – in which he portrayed Max, a quirky guardian angel who uses disguises and subterfuge to steer his charges onto the right path. In 2005, he starred in "The Thing About My Folks". Although movie critic Roger Ebert was not impressed with most of the other actors, he wrote in his review: "... We discover once again what a warm and engaging actor Peter Falk is. I can't recommend the movie, but I can be grateful that I saw it, for Falk." In 2007, Falk appeared with Nicolas Cage in the thriller "Next".

Falk married Alyce Mayo whom he met when the two were students at Syracuse University, on April 17, 1960. The couple adopted two daughters, Catherine (who was to become a private investigator) and Jackie. Falk and his wife divorced in 1976. On December 7, 1977, he married actress Shera Danese, who guest-starred in more episodes of the "Columbo" series than any other actress.

Falk was an accomplished artist, and in October 2006 he had an exhibition of his artwork at the Butler Institute of American Art. He took classes at the Art Students League of New York for many years.

Falk was a chess aficionado and a spectator at the American Open in Santa Monica, California, in November 1972, and at the U.S. Open in Pasadena, California, in August 1983.

His memoir "Just One More Thing" () was published by Carroll & Graf on August 23, 2006.

In December 2008 it was reported that Falk had been diagnosed with Alzheimer's disease. In June 2009, at a two-day conservatorship trial in Los Angeles, one of Falk's personal physicians, Dr. Stephen Read, reported he had rapidly slipped into dementia after a series of dental operations in 2007. Dr. Read said it was unclear whether Falk's condition had worsened as a result of anesthesia or some other reaction to the operations. Shera Danese Falk was appointed as her husband's conservator.

On the evening of June 23, 2011, Falk died at his longtime home on Roxbury Drive in Beverly Hills at the age of 83. His death was primarily caused by pneumonia, with complications of Alzheimer's disease being a secondary and underlying cause. His daughters said they would remember his "wisdom and humor". Falk's body was buried at Westwood Village Memorial Park Cemetery in Los Angeles, California.

His death was marked by tributes from many film celebrities including Jonah Hill, Roger Ebert, and Stephen Fry. Steven Spielberg said, "I learned more about acting from him at that early stage of my career than I had from anyone else." Rob Reiner said: "He was a completely unique actor", and went on to say that Falk's work with Alan Arkin in "The In-Laws" was "one of the most brilliant comedy pairings we've seen on screen".

Shera Danese was Falk's second wife and conservator, and allegedly, according to his daughter Catherine, stopped some of his family members from visiting him, did not notify them of major changes in his condition, and did not notify them of his death and funeral arrangements. Catherine later encouraged the passage of legislation called Peter Falk's Law, that provides guidelines that guardians and conservators for an incapacitated person must comply with regarding visitation rights and notice of death. As of 2016, more than ten states had enacted such laws.



</doc>
<doc id="23726" url="https://en.wikipedia.org/wiki?curid=23726" title="Pixies (band)">
Pixies (band)

The Pixies are an American alternative rock band formed in 1986 in Boston, Massachusetts. The original lineup comprised Black Francis (vocals, rhythm guitar), Joey Santiago (lead guitar), Kim Deal (bass, backing vocals) and David Lovering (drums). The band disbanded acrimoniously in 1993, but reunited in 2004. After Deal left in 2013, the Pixies hired Kim Shattuck as a touring bassist; she was replaced the same year by Paz Lenchantin, who became a permanent member in 2016.

The Pixies are associated with the 1990s alternative rock boom, and draw on elements including punk rock and surf rock. Their music is known for its dynamic "loud-quiet" shifts and song structures. Francis is the Pixies' primary songwriter; his often surreal lyrics cover offbeat subjects such as extraterrestrials, incest, and biblical violence. They achieved modest popularity in the US, but were more successful in Europe. Their jarring pop sound influenced acts such as Nirvana, Radiohead, the Smashing Pumpkins and Weezer. Their popularity grew in the years after their break-up, leading to sold-out world tours following their reunion in 2004.

Guitarist Joey Santiago and songwriter Black Francis (born Charles Michael Kitteridge Thompson IV) met when they lived next to each other in a suite while attending the University of Massachusetts Amherst. Although Santiago was worried about distractions, he noticed Francis played music and the pair began to jam together. Francis embarked on a student exchange trip to Puerto Rico to study Spanish. After six months, he returned to Amherst and dropped out of the university. Francis and Santiago spent 1984 working in a warehouse, with Francis composing songs on his acoustic guitar and writing lyrics on the subway train.

The pair formed a band in January 1986. Two weeks later, Francis placed an advertisement seeking a bass player who liked both the folk music act Peter, Paul and Mary and the alternative rock band Hüsker Dü. Kim Deal was the only respondent, and arrived at the audition without a bass, as she had never played one before. She was invited to join as she liked the songs Francis showed her. She obtained a bass, and the trio started rehearsing in Deal's apartment.

After recruiting Deal, Kim paid for her sister, Kelley Deal, to fly to Boston and audition as drummer. Though Francis approved, Kelley was not confident in her drumming, and was more interested in playing songs written by Kim; she later joined Kim's band the Breeders. Kim's husband suggested they hire David Lovering, whom Kim had met at her wedding reception. The group arrived at a name after Santiago selected the word "pixies" randomly from a dictionary, liking how it looked and its definition as "mischievous little elves". Pixies moved rehearsals to Lovering's parents' garage in mid-1986 and began to play shows at bars in the Boston area.

While Pixies were playing a concert with Throwing Muses, they were noticed by producer Gary Smith, manager of Fort Apache Studios. He told the band he "could not sleep until you guys are world famous". The band produced a 17-track demo at Fort Apache soon afterwards, known to fans as the "Purple Tape" because of the tape cover's purple background. Funded by Francis' father at the cost of $1000, the recording session was completed in three days. Local promoter Ken Goes became the band's manager, and he passed the demo to Ivo Watts-Russell of the independent record label 4AD. Watts-Russell nearly passed on the band, finding them too normal, "too rock 'n' roll", but signed them at the persuasion of his girlfriend.

Upon signing with 4AD, eight tracks from the Purple Tape were selected for the "Come on Pilgrim" mini-LP, Pixies' first release. Francis drew upon his experiences in Puerto Rico, mostly in the songs "Vamos" and "Isla de Encanta", describing the poverty in Puerto Rico. The religious lyrics in "Come on Pilgrim" and later albums came from his parents' born-again Christian days in the Pentecostal Church. Critic Heather Phares sees themes such as sexual frustration ("I've Been Tired") and incest ("Nimrod's Son" and "The Holiday Song") on the record.

"Come on Pilgrim" was followed by Pixies' first full-length album, "Surfer Rosa". The album was recorded by Steve Albini (who was hired by Watts-Russell on the advice of a 4AD colleague), completed in two weeks, and released in early 1988. "Surfer Rosa" gained Pixies acclaim in Europe; both "Melody Maker" and "Sounds" gave "Surfer Rosa" their "Album of the Year" award. American critical response was also positive yet more muted, a reaction that persisted for much of the band's career. The album was eventually certified Gold in the U.S. in 2005. After the album was released, the band arrived in England to support Throwing Muses on the European "Sex and Death" tour—beginning at the Mean Fiddler in London. The tour also took them to the Netherlands, where Pixies had already received enough media attention to be headlining the tour. The tour became notable for the band's in-jokes, such as playing their entire set list in alphabetical order.

Meanwhile, Pixies signed an American distribution deal with major record label Elektra. Around this time, Pixies struck up a relationship with the British producer Gil Norton. Norton produced their second full album, "Doolittle", which was recorded in the last six weeks of 1988 and seen as a departure from the raw sound of "Come on Pilgrim" and "Surfer Rosa". "Doolittle" had a much cleaner sound, largely due to Norton and the production budget of US$40,000, which was quadruple that of "Surfer Rosa". "Doolittle" featured the single "Here Comes Your Man", which biographers Josh Frank and Caryn Ganz describe as an unusually jaunty and pop-like song for the band. "Monkey Gone to Heaven" was popular on alternative radio in the US, reaching number 10 on the "Billboard" modern rock tracks, and the single entered the Top 100 in the U.K. Like "Surfer Rosa", "Doolittle" was acclaimed by fans and music critics alike.

After "Doolittle", tensions between Deal and Francis came to a head (for example, Francis threw a guitar at Deal during a concert in Stuttgart), and Deal was almost fired from the band when she refused to play at a concert in Frankfurt. Santiago, in an interview with "Mojo", described Deal as being "headstrong and want[ing] to include her own songs, to explore her own world" on the band's albums; eventually she accepted that Francis was the singer and had musical control of the band, but after the Frankfurt incident, "they kinda stopped talking". The band became increasingly tired during the post-"Doolittle" "Fuck or Fight" tour of the United States and fighting among members continued. After the tour's final date in New York City, the band was too exhausted to attend the end-of-tour party the following night and soon announced a hiatus.

During this time, Santiago and Lovering went on vacation while Francis performed a short solo tour, made up of a number of concerts to generate gas money as he traveled across the country. Deal formed a new band, the Breeders, with Tanya Donelly of Throwing Muses and bass player Josephine Wiggs of Perfect Disaster. Their debut album, "Pod", was released in 1990.

In 1990, all members of the group except for Deal moved to Los Angeles. Lovering stated that he, Santiago, and Francis moved there "because the recording studio was there". Unlike previous recordings, the band had little time to practice beforehand, and Black Francis wrote much of the album in the studio. Featuring the singles "Velouria" and "Dig for Fire", "Bossanova" reached number 70 in the United States. In contrast, the album peaked at number three in the United Kingdom. Also in 1990, Pixies released a cover of the Paul Butterfield Blues Band's "Born in Chicago" on the compilation album "".

The band continued to tour and released "Trompe le Monde" in 1991, their final album before their break-up. The album included "U-Mass", which has been described as being about college apathy, and whose guitar riff was written years before at the University of Massachusetts before Francis and Santiago dropped out. The album also featured a cover of "Head On" by The Jesus and Mary Chain. Also that year, the band contributed a cover of "I Can't Forget" to the Leonard Cohen tribute album "I'm Your Fan", and began an international tour on which they played stadiums in Europe and smaller venues in the United States. They then signed to be the support act of U2 on the lucrative US "Zoo TV" Tour in 1992. Tensions rose among band members, and at the end of the year, Pixies went on sabbatical and focused on separate projects.

In early 1993, Francis announced in an interview to BBC Radio 5 that Pixies were finished. He offered no explanation at the time, unbeknownst to the other members of the band. He later called Santiago and notified Deal and Lovering via fax, in January 1993.

After the breakup, the members embarked on separate projects. Black Francis renamed himself Frank Black, and released several solo albums, including a string of releases with Frank Black and the Catholics. Deal returned to the Breeders, who scored a hit with "Cannonball" from their platinum-selling "Last Splash" in 1993, and released two more albums several years later. She also formed the Amps, who released one album.

Santiago played lead guitar on a number of Frank Black albums, as well as on other artists' albums. He wrote music for the television show "Undeclared" and theme music for the film "Crime and Punishment in Suburbia". He formed the Martinis with his wife Linda Mallari, and released an album in 2004. In 2004, he also played lead guitar on the album "Statecraft" by the novelist and musician Charles Douglas. Lovering became a magician and performed as the Scientific Phenomenalist, performing experiments on stage and occasionally opening for Frank Black and the Breeders. Lovering drummed with the band Cracker, as well as on one of Tanya Donelly's solo albums, and on the Martinis' song "Free", which appeared on the "Empire Records" soundtrack.

4AD and Elektra Records continued to release Pixies material: the best-of album "Death to the Pixies" (1997), the Peel-session compilation "Pixies at the BBC" (1998), and the "Complete 'B' Sides" compilation (2001). In 2002, material from the Pixies' original 17-track demo tape was released as an EP, "Pixies", on Cooking Vinyl in the U.K. and SpinArt Records in the U.S.; Black has also used these labels to release solo work and albums with the Catholics.

In the years following the Pixies' breakup, Black dismissed rumors of a reunion, but incorporated an increasing number of Pixies songs in his sets with the Catholics, and occasionally included Santiago in his solo work and Lovering's magic show as an opening act to concerts. In 2003, a series of phone calls among band members resulted in some low-key rehearsals, and soon the decision to reunite. By February 2004, a full tour was announced, and tickets for nearly all the initial tour dates sold out within minutes. The band's four-night run at London's Brixton Academy was the fastest selling in the venue's twenty-year history.

The Pixies played their first reunion concert on April 13, 2004, at the Fine Line Music Cafe in Minneapolis, Minnesota. A warm-up tour through the U.S. and Canada was followed by an appearance at the Coachella Valley Music and Arts Festival. The band then spent much of 2004 touring throughout Brazil, Europe, Japan, and the U.S. The group won the Act-of-the-Year award in the 2004 "Boston Music Awards". The 2004 reunion tour grossed over $14 million in ticket sales.

In June 2004, the band released a new song, "Bam Thwok" exclusively on the iTunes Music Store; it reached number one in the UK Official Download Chart. 4AD released "", along with a companion DVD, "Pixies". The band also contributed a rendition of "Ain't That Pretty at All" to the Warren Zevon tribute album "". "Bam Thwok" and "Ain't That Pretty at All" were both recorded by engineer Ben Mumphrey, the former at Stagg Street Studios in Van Nuys, CA and the latter at Metamorphosis Studio in Vienna, Austria.

In 2005, the Pixies made appearances at festivals including Lollapalooza, "T on the Fringe", and the Newport Folk Festival. They continued to make appearances through 2006 and 2007, culminating in their first shows in Australia. Francis suggested that a new Pixies studio album was possible, or unlikely, the main obstacle being Deal's reluctance.

To celebrate the 20th anniversary of the release of "Doolittle", the Pixies launched a tour in October 2009 where they performed the album track-for-track, including the associated B-sides. The tour began in Europe, continued in the United States in November, with the South American and Australian tour following in March 2010, then New Zealand, and more European dates in spring 2010, and back to North America in fall 2010 and into spring 2011.

On June 14, 2013, the Pixies announced that Deal had left the band. Deal has since released new solo music and the remaining Pixies have invited her to come back as her schedule with Breeders allows. Two weeks later, the band released a new song, "Bagboy", as a free download from the Pixies website. The song features Jeremy Dubs of Bunnies and formerly of the Bennies on vocals in place of Deal.

On July 1, 2013, the Pixies announced the addition of Muffs and Pandoras guitarist and vocalist Kim Shattuck to replace Deal for their 2013 European tour. On September 3, 2013, the Pixies released an EP of new songs titled "EP1". On November 29, 2013, Shattuck announced that she had been dismissed from the band that day. In December 2013, it was announced that The Entrance Band and A Perfect Circle bassist Paz Lenchantin was joining the Pixies for the 2014 tour. More new material surfaced when the Pixies released their second EP, "EP2", on January 3, 2014. The single released to radio was "Blue Eyed Hexe". Another new EP, "EP3", was released on March 24, 2014. All the EPs were only available as downloads and limited edition vinyl. The three EPs were collected in LP format and released as the album "Indie Cindy" in April 2014. The album was the first release from the band in over two decades, the last being "Trompe le Monde" in 1991. In 2015, the Pixies toured in support of Robert Plant for a series of dates across North America.

On July 6, 2016, the Pixies announced that Lenchantin was now a permanent member of the band. Their sixth album, "Head Carrier", was released on September 30, 2016.

On January 23, 2019, the band announced that they would release a seventh studio album produced by Tom Dalgety in September 2019; adding information about the "Past is Prologue, Pixies" podcast, which starts June 27 for 12 episodes. On June 3, it was revealed that the name of the album would be "Beneath the Eyrie", which the single "On Graveyard Hill" from was released on the same day.

"Spin" magazine described the Pixies' musical style as "Surf music-meets-Stooges spikiness and oft-imitated stop/start and quiet/loud dynamics". Their music has also been pictured as "an unorthodox marriage of surf music and punk rock, ... characterized by Black's bristling lyrics and hackle-raising caterwaul, Kim Deal's whispered harmonies and waspy basslines, Joey Santiago's fragile guitar, and the persistent flush of David Lovering's drums." The band's music incorporates extreme dynamic shifts; Francis explained in 1991, "Those are the two basic components of rock music ... the dreamy side and the rockin' side. It's always been either sweaty or laid back and cool. We do try to be dynamic, but it's dumbo dynamics, because we don't know how to do anything else. We can play loud or quiet—that's it".

The Pixies are influenced by a range of artists and genres; each member came from a different musical background. When he first started writing songs for the Pixies, Francis says he was listening to nothing but Hüsker Dü, Captain Beefheart, and Iggy Pop; During the making of "Doolittle" he listened heavily to the Beatles' "White Album". He has cited Buddy Holly as a model for his compressed songwriting. Francis did not discover punk rock until he was 16, saying "it was good I didn't listen to these hip records". As a child, he listened mainly to 1960s songs, religious music and Emerson Lake and Palmer, then Iggy Pop, Husker Du, Captain Beefheart and Talking Heads, who he says "weren't punk either".

Santiago listened to 1970s and 1980s punk including Black Flag, as well as David Bowie and T. Rex. Guitarists who influenced him include Jimi Hendrix, Les Paul, Wes Montgomery, and George Harrison. Deal's musical background was folk music and country; she had formed a country-folk band with her sister in her teenage years, and played covers of artists such as The Everly Brothers and Hank Williams. Other artists Deal listened to included XTC, Gang of Four and Elvis Costello. Lovering is a fan of the band Rush.

Other media such as film has influenced the Pixies; Francis cites surrealist films "Eraserhead" and "Un chien andalou" (as mentioned in "Debaser") as influences. He has commented on these influences, saying he "didn't have the patience to sit around reading Surrealist novels", but found it easier to watch twenty-minute films.

Most of the Pixies' songs are composed and sung by Francis. Critic Stephen Thomas Erlewine has described Francis's writing as containing "bizarre, fragmented lyrics about space, religion, sex, mutilation, and pop culture". Biblical violence is a theme of "Doolittle"s "Dead" and "Gouge Away"; Francis told a "Melody Maker" interviewer, "It's all those characters in the Old Testament. I'm obsessed with them. Why it comes out so much I don't know." He has described "Come on Pilgrims" "Caribou" as being about reincarnation, and extraterrestrial themes appear in a number of songs on "Bossanova".

Deal co-wrote "Doolittle"s "Silver" with Francis, and they share lead harmony vocals on the track. She also co-wrote and sang lead vocals on "Surfer Rosa"s "Gigantic", and is the sole songwriter of the 2004 digital single "Bam Thwok". She was credited as Mrs. John Murphy on the former composition — at the time she was married, and she used this name as an ironic feminist joke. She also sang lead vocals on the song "Into the White" and the Neil Young cover "Winterlong", which were both B-sides. Lovering sang lead vocals on "Doolittle"s "La La Love You" and the B-side "Make Believe". Most recently, Lenchantin made her lead vocal debut on "Head Carrier"s "All I Think About Now." She also provided lead vocals on "Los Surfers Muertos," from 2019's "Beneath the Eyrie".

The Pixies first album "Surfer Rosa" is certified gold, while "Doolittle" hit platinum status, selling over 1 million copies. The band influenced a number of musicians associated with the alternative rock boom of the 1990s. Gary Smith, who produced their "Come on Pilgrim", commented on the band's influence on alternative rock and their legacy in 1997:

I've heard it said about The Velvet Underground that while not a lot of people bought their albums, everyone who did started a band. I think this is largely true about Pixies as well. Charles' secret weapon turned out to be not so secret and, sooner or later, all sorts of bands were exploiting the same strategy of wide dynamics. It became a kind of new pop formula and, within a short while, "Smells Like Teen Spirit" was charging up the charts and even the members of Nirvana said later that it sounded for all the world like a Pixies song.

Sonically, the Pixies are credited with popularizing the extreme dynamics and stop-start timing that would become widespread in alternative rock; the Pixies songs typically feature hushed, restrained verses, and explosive, wailing choruses. Artists including David Bowie, Matt Noveskey, Radiohead, PJ Harvey, U2, Nirvana, The Strokes, Alice in Chains, Weezer, Bush, Arcade Fire, Pavement, Everclear, Kings of Leon and Matthew Good have cited admiration of or influence by the Pixies. Bono of U2 has called the Pixies "one of America's greatest bands ever", and Radiohead's Thom Yorke said that the Pixies "changed my life". Bowie, whose own music had inspired Francis and Santiago while they were at university, has said that the Pixies made "just about the most compelling music of the entire 80s."

One notable citation as an influence was by Kurt Cobain, on influencing Nirvana's "Smells Like Teen Spirit", which he admitted was a conscious attempt to co-opt the Pixies' style. In a January 1994 interview with "Rolling Stone", he said, "I was trying to write the ultimate pop song. I was basically trying to rip off the Pixies. I have to admit it [smiles]. When I heard the Pixies for the first time, I connected with that band so heavily I should have been in that band—or at least in a Pixies cover band. We used their sense of dynamics, being soft and quiet and then loud and hard." Cobain cited "Surfer Rosa" as one of his main musical influences, and particularly admired the album's natural and powerful drum sounds—a result of Steve Albini's influence on the record. Albini later produced Nirvana's 1993 "In Utero" at the request of Cobain.

No music videos were released from "Come on Pilgrim" or "Surfer Rosa", but from "Doolittle" onwards, the following videos were made: "Monkey Gone To Heaven", "Here Comes Your Man", "Velouria", "Dig For Fire", "Allison", "Alec Eiffel", "Head On", and "Debaser"; these were later released on the 2004 DVD "Pixies". Furthermore, a music video accompanied the release of their 2013 song, "Bagboy", as well an alternate video released on a later date. Videos have been made for all the songs in EP1. The videos for "Here Comes Your Man" and "Allison" were also released on "The Complete 'B' Sides".

By "Bossanova", the band had developed a severe aversion to recording music videos, and Francis refused to lip-sync to them. For example, in the "Here Comes Your Man" video, both Black and Deal open their mouths wide instead of mouthing their lyrics. According to the record label, this became one of the reasons that the Pixies never achieved major coverage on MTV.

With "Bossanova"s release, 4AD hoped to get the Pixies chosen to perform their single "Velouria" on the BBC music programme "Top of the Pops". To this end, the band was pressured into producing a video for the song, and they made one cheaply with the band members filmed running down a quarry, shown in slow motion. The group was ultimately not given a spot on the show.

A 90-minute documentary called "loudQUIETloud: a film about the Pixies" directed by Steven Cantor and Matthew Galkin was released in 2006. The film documents their 2004 reunion and tour, and covers the years after the break-up. In addition to "Pixies" and "LoudQUIETloud", four other Pixies' DVDs were released between 2004 and 2006, all featuring concert performances: "Live at the Town and Country Club 1988", "The Pixies—Sell Out", "The Pixies Acoustic: Live in Newport", and "The Pixies Club Date: Live at the Paradise in Boston".

The Pixies were ranked number 81 on VH1's "100 Greatest Artists of Hard Rock".

In 2013, Sean T. Rayburn, founder of PixiesMusic.com and friend to the band, launched a Kickstarter campaign to fund the release of "PIXIES: A Visual History, Volume 1", a limited edition hardcover, coffee-table size book, featuring hundreds of never-before-seen photos of the band. The Kickstarter edition of the book was limited to 3,500 numbered copies, all signed by Rayburn and Pixies singer Black Francis. Approximately one quarter of these were also signed by designer Aaron Tanner. "A Visual History" went on to win several gold and silver publishing awards.








</doc>
<doc id="23731" url="https://en.wikipedia.org/wiki?curid=23731" title="Plasma ashing">
Plasma ashing

In semiconductor manufacturing plasma ashing is the process of removing the photoresist (light sensitive coating) from an etched wafer. Using a plasma source, a monatomic (single atom) substance known as a reactive species is generated. Oxygen or fluorine are the most common reactive species. The reactive species combines with the photoresist to form ash which is removed with a vacuum pump.

Typically, monatomic oxygen plasma is created by exposing oxygen gas at a low pressure (O) to high power radio waves, which ionise it. This process is done under vacuum in order to create a plasma. As the plasma is formed, many free radicals are created which could damage the wafer. Newer, smaller circuitry is increasingly susceptible to these particles. Originally, plasma was generated in the process chamber, but as the need to get rid of free radicals has increased, many machines now use a downstream plasma configuration, where plasma is formed remotely and the desired particles are channeled to the wafer. This allows electrically charged particles time to recombine before they reach the wafer surface, and prevents damage to the wafer surface.

Two forms of plasma ashing are typically performed on wafers. High temperature ashing, or stripping, is performed to remove as much photo resist as possible, while the "descum" process is used to remove residual photo resist in trenches. The main difference between the two processes is the temperature the wafer is exposed to while in an ashing chamber.

Monatomic oxygen is electrically neutral and although it does recombine during the channeling, it does so at a slower rate than the positively or negatively charged free radicals, which attract one another. This means that when all of the free radicals have recombined, there is still a portion of the active species available for process. Because a large portion of the active species is lost to recombination, process times may take longer. To some extent, these longer process times can be mitigated by increasing the temperature of the reaction area.


</doc>
<doc id="23732" url="https://en.wikipedia.org/wiki?curid=23732" title="Psychophysiology">
Psychophysiology

Psychophysiology (from Greek , "psȳkhē", "breath, life, soul"; , "physis", "nature, origin"; and , "-logia") is the branch of psychology that is concerned with the physiological bases of psychological processes. While psychophysiology was a general broad field of research in the 1960s and 1970s, it has now become quite specialized, and has branched into subspecializations such as social psychophysiology, cardiovascular psychophysiology, cognitive psychophysiology, and cognitive neuroscience.

Some people have difficulty distinguishing a psychophysiologist from a physiological psychologist, two very different perspectives. Psychologists are interested in why we may fear spiders and physiologists may be interested in the input/output system of the amygdala. A psychophysiologist will attempt to link the two. Psychophysiologists generally study the psychological/physiological link in intact human subjects. While early psychophysiologists almost always examined the impact of psychological states on physiological system responses, since the 1970s, psychophysiologists also frequently study the impact of physiological states and systems on psychological states and processes. It is this perspective of studying the interface of mind and body that makes psychophysiologists most distinct.

Historically, most psychophysiologists tended to examine the physiological responses and organ systems innervated by the autonomic nervous system. More recently, psychophysiologists have been equally, or potentially more, interested in the central nervous system, exploring cortical brain potentials such as the many types of event-related potentials (ERPs), brain waves, and utilizing advanced technology such as functional magnetic resonance imaging (fMRI), MRI, PET, MEG, and other neuroimagery techniques.

Continuing the comparison between a psychophysiologist and a physiological psychologist, a psychophysiologist may look at how exposure to a stressful situation will produce a result in the cardiovascular system such as a change in heart rate (HR), vasodilation/vasoconstriction, myocardial contractility, or stroke volume. A physiological psychologist may look at how one cardiovascular event may influence another cardiovascular or endocrine event, or how activation of one neural brain structure exerts excitatory activity in another neural structure which then induces an inhibitory effect in some other system. Often, physiological psychologists examine the effects that they study in infrahuman subjects using surgical or invasive techniques and processes.

Psychophysiology is closely related to the field of neuroscience and social neuroscience, which primarily concerns itself with relationships between psychological events and brain responses. Psychophysiology is also related to the medical discipline known as psychosomatics.

While psychophysiology was a discipline off the mainstream of psychological and medical science prior to roughly the 1960 and 1970s, more recently, psychophysiology has found itself positioned at the intersection of psychological and medical science, and its popularity and importance have expanded commensurately with the realization of the inter-relatedness of mind and body.

Psychophysiology measures exist in three domains; reports, readings, and behavior. Evaluative reports involve participant introspection and self-ratings of internal psychological states or physiological sensations, such as self-report of arousal levels on the self-assessment manikin, or measures of interoceptive visceral awareness such as heartbeat detection. Merits to self-report are an emphasis on accurately understand the participants' subjective experience and understanding their perception; however, its pitfalls include the possibility of participants misunderstanding a scale or incorrectly recalling events. Physiological responses also can be measured via instruments that read bodily events such as heart rate change, electrodermal activity (EDA), muscle tension, and cardiac output. Many indices are part of modern psychophysiology, including brain waves (electroencephalography, EEG), fMRI (functional magnetic resonance imaging), electrodermal activity (a standardized term encompassing skin conductance response, SCR, and galvanic skin response, GSR), cardiovascular measures (heart rate, HR; beats per minute, BPM; heart rate variability, HRV; vasomotor activity), muscle activity (electromyography, EMG), electrogastrogram (EGG) changes in pupil diameter with thought and emotion (pupillometry), eye movements, recorded via the electro-oculogram (EOG) and direction-of-gaze methods, and cardiodynamics, recorded via impedance cardiography. These measures are beneficial because they provide accurate and perceiver-independent objective data recorded by machinery. The downsides, however, are that any physical activity or motion can alter responses, and basal levels of arousal and responsiveness can differ among individuals and even between situations.

Finally, one can measure overt action or behavior, which involves the observation and recording actual actions, such as running, freezing, eye movement, and facial expression. These are good response measures and easy to record in animals, but they are not as frequently used in human studies.

Psychophysiological measures are often used to study emotion and attention responses to stimuli, during exertion, and increasingly, to better understand cognitive processes.
Physiological sensors have been used to detect emotions in schools and intelligent tutoring systems.

It has long been recognized that emotional episodes are partly constituted by physiological responses. Early work done linking emotions to psychophysiology started with research on mapping consistent autonomic nervous system (ANS) responses to discrete emotional states. For example, anger might be constituted by a certain set of physiological responses, such as increased cardiac output and high diastolic blood pressure, which would allow us to better understand patterns and predict emotional responses. Some studies were able to detect consistent patterns of ANS responses that corresponded to specific emotions under certain contexts, like an early study by Paul Ekman and colleagues in 1983 "Emotion-specific activity in the autonomic nervous system was generated by constructing facial prototypes of emotion muscle by muscle and by reliving past emotional experiences. The autonomic activity produced distinguished not only between positive and negative emotions, but also among negative emotions". 
However, as more studies were conducted, more variability was found in ANS responses to discrete emotion inductions, not only among individuals but also over time in the same individuals, and greatly between social groups. Some of these differences can be attributed to variables like induction technique, context of the study, or classification of stimuli, which can alter a perceived scenario or emotional response. However it was also found that features of the participant could also alter ANS responses. Factors such as basal level of arousal at the time of experimentation or between test recovery, learned or conditioned responses to certain stimuli, range and maximal level of effect of ANS action, and individual attentiveness can all alter physiological responses in a lab setting. Even supposedly discrete emotional states fail to show specificity. For example, some emotional typologists consider fear to have subtypes, which might involve fleeing or freezing, both of which can have distinct physiological patterns and potentially distinct neural circuitry. As such no definitive correlation can be drawn linking specific autonomic patterns to discrete emotions, causing emotion theorists to rethink classical definitions of emotions.

Physiological computing represents a category of affective computing that incorporates real-time software adaption to the psychophysiological activity of the user. The main goal of this is to build a computer that responds to user emotion, cognition and motivation. The approach is to enable implicit and symmetrical human-computer communication by granting the software access to a representation of the user's psychological status.

There are several possible methods to represent the psychological state of the user (discussed in the affective computing page). The advantages of using psychophysiological indices are that their changes are continuous, measures are covert and implicit, and only available data source when the user interacts with the computer without any explicit communication or input device. These systems rely upon an assumption that the psychophysiological measure is an accurate one-to-one representation of a relevant psychological dimension such as mental effort, task engagement and frustration.

Physiological computing systems all contain an element that may be termed as an adaptive controller that may be used to represent the player. This adaptive controller represents the decision-making process underlying software adaptation. In their simplest form, adaptive controllers are expressed in Boolean statements. Adaptive controllers encompass not only the decision-making rules, but also the psychophysiological inference that is implicit in the quantification of those trigger points used to activate the rules. The representation of the player using an adaptive controller can become very complex and often only one-dimensional. The loop used to describe this process is known as the biocybernetic loop. The biocybernetic loop describes the closed loop system that receives psychophysiological data from the player, transforms that data into a computerized response, which then shapes the future psychophysiological response from the player. A positive control loop tends towards instability as player-software loop strives towards a higher standard of desirable performance. The physiological computer game may wish to incorporate both positive and negative loops into the adaptive controller.


A. Weissman, M. Aranovitch, S. Blazer, and E. Z. Zimmer (2009)
Pediatrics 124, e921-e92
L. P.T. Hua, C. A. Brown, S. J.M. Hains, M. Godwin, and J. L. Parlow (2009)
Biol Res Nurs 11, 129-143



</doc>
<doc id="23733" url="https://en.wikipedia.org/wiki?curid=23733" title="Periodization">
Periodization

Periodization is the process or study of categorizing the past into discrete, quantified named blocks of time. This is usually done in order to facilitate the study and analysis of history, understanding current and historical processes, and causality that might have linked those events.

This results in descriptive abstractions that provide convenient terms for periods of time with relatively stable characteristics. However, determining the precise beginning and ending to any "period" is often arbitrary, since it has changed over time over the course of history.

To the extent that history is continuous and ungeneralizable, all systems of periodization are more or less arbitrary. Yet without named periods, however clumsy or imprecise, past time would be nothing more than scattered events without a framework to help us understand them. Nations, cultures, families, and even individuals, each with their different remembered histories, are constantly engaged in imposing overlapping, often unsystematized, schemes of temporal periodization; periodizing labels are continually challenged and redefined, but once established, a period "brand" is so convenient that many are very hard to change or shake off.

The division of history into ages or periods is very old, and recorded practically as early as the first development of writing.
The Sumerian King List operates with dynastic regnal eras.
The classical division into a Golden Age, Silver Age, Bronze Age, Heroic Age and Iron Age goes back to Hesiod.
One Biblical periodization scheme commonly used in the Middle Ages was Saint Paul's theological division of history into three ages: the first before the age of Moses (under nature); the second under Mosaic law (under law); the third in the age of Christ (under grace). But perhaps the most widely discussed periodization scheme of the Middle Ages was the Six Ages of the World, where every age was a thousand years counting from Adam to the present, with the present time (in the Middle Ages) being the sixth and final stage.

Not only do periodizing blocks inevitably overlap, they often seemingly conflict with or contradict one another. Some have a cultural usage (the "Gilded Age"), others refer to prominent historical events ("the Inter-War years: 1918–1939"), yet others are defined by decimal numbering systems ("the 1960s", "the 17th century"). Other periods are named from influential or talismanic individuals (the "Victorian Era", the "Edwardian Era", the "Napoleonic Era").

Some of these usages will also be geographically specific. This is especially true of periodizing labels derived from individuals or ruling dynasties, such as the Jacksonian Era in America, the Meiji Era in Japan, or the Merovingian Period in France. Cultural terms may also have a limited reach. Thus the concept of the "Romantic period" is largely meaningless outside the Western world of Europe and European-influenced cultures. Likewise, "the 1960s", though technically applicable to anywhere in the world according to Common Era numbering, has a certain set of specific cultural connotations in certain countries. For this reason it may be possible to say such things as "The 1960s never occurred in Spain". This would mean that the sexual revolution, counterculture, youth rebellion and so on never developed during that decade in Spain's conservative Roman Catholic culture and under Francisco Franco's authoritarian regime. Likewise it is very often said, as the historian Arthur Marwick has, that the "1960s" began in the late 1950s and ended in the early 1970s. His reason for saying this is that the cultural and economic conditions that define the meaning of the period covers more than the accidental fact of a 10-year block beginning with the number 6. This extended usage is termed the "long 1960s". This usage derives from other historians who have adopted labels such as "the long 19th century" (1789–1914) to reconcile arbitrary decimal chronology with meaningful cultural and social phases. Similarly, an Eighteenth Century may run 1714–1789. Eric Hobsbawm has also argued for what he calls "the short twentieth century", encompassing the period from the First World War through to the end of the Cold War.

Similar problems attend other labels. Is it possible to use the term "Victorian" outside Britain, and even within, does her reign of 1837–1901 usefully constitute a historical period? It sometimes is used when it is thought that its connotations usefully describe the politics, culture and economic conditions characteristic of the last two-thirds of the nineteenth century. Nevertheless, periodizing terms often have negative or positive connotations that may affect their usage. This includes "Victorian", which often negatively suggests sexual repression and class conflict. Other labels such as "Renaissance" have strongly positive characteristics. As a result, these terms sometimes extend in meaning. Thus the "English Renaissance" is often used for a period largely identical to the "Elizabethan Period" or reign of Elizabeth I, and begins some 200 years later than the Italian Renaissance. However the "Carolingian Renaissance" is said to have occurred during the reign of the Frankish king Charlemagne, and his immediate successors. Other examples, neither of which constituted a "rebirth" in the sense of revival, are the "American Renaissance" of the 1820s–60s, referring mainly to literature, and the "Harlem Renaissance" of the 1920s, referring mainly to literature but also to music and the visual arts.

The conception of a "rebirth" of Classical Latin learning is first credited to the Italian poet Petrarch (1304–1374), the father of Renaissance Humanism, but the conception of a rebirth has been in common use since Petrarch's time. The dominant usage of the word "Renaissance" refers to the cultural changes that occurred in Italy that culminated in the High Renaissance around 1500–1530. This concept applies dominantly to the visual arts, and the work of Michelangelo, Raphael, and Leonardo da Vinci. Secondarily it is applied to other arts, but it is questionable whether it is useful to describe a phase in economic, social and political history. Many professional historians now refer to the historical periods commonly known as the Renaissance and the Reformation as the start of the Early Modern Period, which extends much later. There is a gradual change in the courses taught and books published to correspond to the change in period nomenclature, which in part reflects differences between social history and cultural history. The new nomenclature suggests a broader geographical coverage and a growing attention to the relationships between Europe and the wider world.

The term Middle Ages also derives from Petrarch. He was comparing his own period to the Ancient or Classical world, seeing his time as a time of rebirth after a dark intermediate period, the Middle Ages. The idea that the Middle Ages was a middle phase between two other large scale periodizing concepts, Ancient and Modern, still persists. It can be sub-divided into the Early, High and Late Middle Ages. The term Dark Ages is no longer in common use among modern scholars because of the difficulty of using it neutrally, though some writers have attempted to retain it and divest it of its negative connotations. The term "Middle Ages" and especially the adjective "medieval" can also have a negative ring in colloquial use ("the barbaric treatment of prisoners in such-and-such a prison is almost medieval") but this does not carry over into academic terminology. However, other terms, such as Gothic architecture, used to refer to a style typical of the High Middle Ages have largely lost the negative connotations they initially had, acquiring new meanings over time (see Gothic architecture and Goth subculture).

The Gothic and the Baroque were both named during subsequent stylistic periods when the preceding style was unpopular. The word "Gothic" was applied as a pejorative term to all things Northern European and, hence, barbarian, probably first by Giorgio Vasari. Vasari is also credited with first using the term "Renaissance" ("rinascita") to describe the period during which he was art historian, artist, and architect. Giorgio Vasari coined the term "Gothic" in an effort to describe, particularly architecture, that he found objectionable, supposedly saying "it is as if the Goths built it". The word "baroque"—derived from similar words in Portuguese, Spanish, or French—literally refers to an irregular or misshapen pearl. Its first use outside the field of jewellery manufacture was in the early 18th century, as a criticism of music that was viewed as over-complicated and rough. Later, the term was also used to describe architecture and art. The Baroque period was first designated as such in the 19th century, and is generally considered to have begun around 1600 in all media. Music history places the end of the period in the year 1750 with the death of J. S. Bach, while art historians consider the main period to have ended significantly earlier in most areas.

The usual method for periodization of the distant prehistoric past, in archeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used.

Some events or short periods of change have such a drastic effect on the cultures they affect that they form a natural break in history. These are often marked by the widespread use of both "pre-" and "post-" phrases centred on the event, as in "pre-Reformation" and "post-Reformation", or "pre-colonial" and "post-colonial". Both pre-war and post-war are still understood to refer to World War II, though at some future point the phrases will need to be altered to make that clear.

The Marxist theory of historical materialism claims society as fundamentally determined by the "material conditions" at any given time – in other words, the relationships, which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.

The theory identifies the following stages of history:

The First Stage: is usually called primitive communism. It has the following characteristics.

The Second Stage: may be called slave society, considered to be the beginning of "class society" where private property appears.

The Third Stage: may be called feudalism; it appears after slave society collapses. This was most obvious during the European Middle Ages when society went from slavery to feudalism.

Marx pays special attention to this stage in human development. The bulk of his work is devoted to analysing the mechanisms of capitalism, which, in western society, classically arose "red in tooth and claw" from feudal society in a revolutionary movement. In capitalism, the profit motive rules and people, freed from serfdom, work for the capitalists for wages. The capitalist class are free to spread their laissez-faire practices around the world. In the capitalist-controlled parliament, laws are made to protect wealth.

Capitalism may be considered the Fourth Stage in the sequence. It appears after the bourgeois revolution when the capitalists (or their merchant predecessors) overthrow the feudal system. Capitalism is categorized by the following:

According to Marx, capitalism has critical failings — inner contradictions — which lead to its downfall. The working class, to which the capitalist class gave birth in order to produce commodities and profits, is the "grave digger" of capitalism. The worker is not paid the full value of what he or she produces. The rest is surplus value — the capitalist's profit, which Marx calls the "unpaid labour of the working class." The capitalists are forced by competition to attempt to drive down the wages of the working class to increase their profits, and this creates conflict between the classes, and gives rise to the development of class consciousness in the working class. The working class, through trade union and other struggles, becomes conscious of itself as an exploited class. In the view of classical Marxism, the struggles of the working class against the attacks of the capitalist class will eventually lead the working class to establish its own collective control over production.

After the working class gains class consciousness and mounts a revolution against the capitalists, socialism, which may be considered the Fifth Stage, will be attained, if the workers are successful.

Socialism may be characterised as follows:

Marx explained that, since socialism, the first stage of communism, would be "in every respect, economically, morally, and intellectually, still stamped with the birthmarks of the old society from whose womb it emerges", each worker would naturally expect to be awarded according to the amount of labor he contributes, despite the fact that each worker's ability and family circumstances would differ, so that the results would still be unequal at this stage, although fully supported by social provision.



</doc>
<doc id="23734" url="https://en.wikipedia.org/wiki?curid=23734" title="Petrarch">
Petrarch

Francesco Petrarca (; July 20, 1304 – July 18/19, 1374), commonly anglicized as Petrarch (), was an Italian scholar and poet during the early Italian Renaissance who was one of the earliest humanists. 

Petrarch's rediscovery of Cicero's letters is often credited with initiating the 14th-century Italian Renaissance and the founding of Renaissance humanism. In the 16th century, Pietro Bembo created the model for the modern Italian language based on Petrarch's works, as well as those of Giovanni Boccaccio, and, to a lesser extent, Dante Alighieri. Petrarch would be later endorsed as a model for Italian style by the Accademia della Crusca.

Petrarch's sonnets were admired and imitated throughout Europe during the Renaissance and became a model for lyrical poetry. He is also known for being the first to develop the concept of the "Dark Ages."

Petrarch was born in the Tuscan city of Arezzo July 20 in 1304. He was the son of Ser Petracco and his wife Eletta Canigiani. His given name was "Francesco Petracco." The name was Latinized to "Petrarca." Petrarch's younger brother was born in Incisa in Val d'Arno in 1307. Dante was a friend of his father.

Petrarch spent his early childhood in the village of Incisa, near Florence. He spent much of his early life at Avignon and nearby Carpentras, where his family moved to follow Pope Clement V who moved there in 1309 to begin the Avignon Papacy. He studied law at the University of Montpellier (1316–20) and Bologna (1320–23) with a lifelong friend and schoolmate called Guido Sette. Because his father was in the legal profession (a notary), he insisted that Petrarch and his brother study law also. Petrarch, however, was primarily interested in writing and Latin literature and considered these seven years wasted. Additionally, he proclaimed that through legal manipulation his guardians robbed him of his small property inheritance in Florence, which only reinforced his dislike for the legal system. He protested, "I couldn't face making a merchandise of my mind," as he viewed the legal system as the art of selling justice.

Petrarch was a prolific letter writer and counted Boccaccio among his notable friends to whom he wrote often. After the death of their parents, Petrarch and his brother Gherardo went back to Avignon in 1326, where he worked in numerous clerical offices. This work gave him much time to devote to his writing. With his first large-scale work, "Africa", an epic in Latin about the great Roman general Scipio Africanus, Petrarch emerged as a European celebrity. On April 8, 1341, he became the second poet laureate since antiquity and was crowned by Roman "Senatori" Giordano Orsini and Orso dell'Anguillara on the holy grounds of Rome's Capitol.

He traveled widely in Europe, served as an ambassador, and (because he traveled for pleasure, as with his ascent of Mont Ventoux), has been called "the first tourist".
During his travels, he collected crumbling Latin manuscripts and was a prime mover in the recovery of knowledge from writers of Rome and Greece. He encouraged and advised Leontius Pilatus's translation of Homer from a manuscript purchased by Boccaccio, although he was severely critical of the result. Petrarch had acquired a copy, which he did not entrust to Leontius, but he knew no Greek; Homer, Petrarch said, "was dumb to him, while he was deaf to Homer". In 1345 he personally discovered a collection of Cicero's letters not previously known to have existed, the collection "Epistulae ad Atticum", in the Chapter Library ("Biblioteca Capitolare") of Verona Cathedral.

Disdaining what he believed to be the ignorance of the centuries preceding the era in which he lived, Petrarch is credited or charged with creating the concept of a historical "Dark Ages".

Petrarch recounts that on April 26, 1336, with his brother and two servants, he climbed to the top of Mont Ventoux (, a feat which he undertook for recreation rather than necessity. The exploit is described in a celebrated letter addressed to his friend and confessor, the monk Dionigi di Borgo San Sepolcro, composed some time after the fact. In it, Petrarch claimed to have been inspired by Philip V of Macedon's ascent of Mount Haemo and that an aged peasant had told him that nobody had ascended Ventoux before or after himself, 50 years before, and warned him against attempting to do so. The nineteenth-century Swiss historian Jacob Burckhardt noted that Jean Buridan had climbed the same mountain a few years before, and ascents accomplished during the Middle Ages have been recorded, including that of Anno II, Archbishop of Cologne.

Scholars note that Petrarch's letter to Dionigi displays a strikingly "modern" attitude of aesthetic gratification in the grandeur of the scenery and is still often cited in books and journals devoted to the sport of mountaineering. In Petrarch, this attitude is coupled with an aspiration for a virtuous Christian life, and on reaching the summit, he took from his pocket a volume by his beloved mentor, Saint Augustine, that he always carried with him.
For pleasure alone he climbed Mont Ventoux, which rises to more than six thousand feet, beyond Vaucluse. It was no great feat, of course; but he was the first recorded Alpinist of modern times, the first to climb a mountain merely for the delight of looking from its top. (Or almost the first; for in a high pasture he met an old shepherd, who said that fifty years before he had attained the summit, and had got nothing from it save toil and repentance and torn clothing.) Petrarch was dazed and stirred by the view of the Alps, the mountains around Lyons, the Rhone, the Bay of Marseilles. He took Augustine's "Confessions" from his pocket and reflected that his climb was merely an allegory of aspiration toward a better life.
As the book fell open, Petrarch's eyes were immediately drawn to the following words:
Petrarch's response was to turn from the outer world of nature to the inner world of "soul":

James Hillman argues that this rediscovery of the inner world is the real significance of the Ventoux event. The Renaissance begins not with the ascent of Mont Ventoux but with the subsequent descent—the "return [...] to the valley of soul", as Hillman puts it. Arguing against such a singular and hyperbolic periodization, Paul James suggests a different reading:

Petrarch spent the later part of his life journeying through northern Italy as an international scholar and poet-diplomat. His career in the Church did not allow him to marry, but he is believed to have fathered two children by a woman or women unknown to posterity. A son, Giovanni, was born in 1337, and a daughter, Francesca, was born in 1343. He later legitimized both.
Giovanni died of the plague in 1361. In the same year Petrarch was named canon in Monselice near Padua. Francesca married Francescuolo da Brossano (who was later named executor of Petrarch's will) that same year. In 1362, shortly after the birth of a daughter, Eletta (the same name as Petrarch's mother), they joined Petrarch in Venice to flee the plague then ravaging parts of Europe. A second grandchild, Francesco, was born in 1366, but died before his second birthday. Francesca and her family lived with Petrarch in Venice for five years from 1362 to 1367 at Palazzo Molina; although Petrarch continued to travel in those years. Between 1361 and 1369 the younger Boccaccio paid the older Petrarch two visits. The first was in Venice, the second was in Padua.

About 1368 Petrarch and his daughter Francesca (with her family) moved to the small town of Arquà in the Euganean Hills near Padua, where he passed his remaining years in religious contemplation. He died in his house in Arquà early on July 20, 1374—his seventieth birthday. The house hosts now a permanent exhibition of Petrarchian works and curiosities; among others you find the famous tomb of Petrarch's beloved cat who was embalmed. On the marble slab there is a Latin inscription written by Antonio Quarenghi:
Petrarch's will (dated April 4, 1370) leaves 50 florins to Boccaccio "to buy a warm winter dressing gown"; various legacies (a horse, a silver cup, a lute, a Madonna) to his brother and his friends; his house in Vaucluse to its caretaker; for his soul, and for the poor; and the bulk of his estate to his son-in-law, Francescuolo da Brossano, who is to give half of it to "the person to whom, as he knows, I wish it to go"; presumably his daughter, Francesca, Brossano's wife. The will mentions neither the property in Arquà nor his library; Petrarch's library of notable manuscripts was already promised to Venice, in exchange for the Palazzo Molina. This arrangement was probably cancelled when he moved to Padua, the enemy of Venice, in 1368. The library was seized by the lords of Padua, and his books and manuscripts are now widely scattered over Europe. Nevertheless, the Biblioteca Marciana traditionally claimed this bequest as its founding, although it was in fact founded by Cardinal Bessarion in 1468.

Petrarch is best known for his Italian poetry, notably the "Canzoniere" ("Songbook") and the "Trionfi" ("Triumphs"). However, Petrarch was an enthusiastic Latin scholar and did most of his writing in this language. His Latin writings include scholarly works, introspective essays, letters, and more poetry. Among them are "Secretum Meum" ("My Secret Book"), an intensely personal, guilt-ridden imaginary dialogue with Augustine of Hippo; "De Viris Illustribus" ("On Famous Men"), a series of moral biographies; "Rerum Memorandarum Libri", an incomplete treatise on the cardinal virtues; "De Otio Religiosorum" ("On Religious Leisure") and "De Vita Solitaria" ("On the Solitary Life"), which praise the contemplative life; "De Remediis Utriusque Fortunae" ("Remedies for Fortune Fair and Foul"), a self-help book which remained popular for hundreds of years; "Itinerarium" ("Petrarch's Guide to the Holy Land"); invectives against opponents such as doctors, scholastics, and the French; the "Carmen Bucolicum", a collection of 12 pastoral poems; and the unfinished epic "Africa". He translated seven psalms, a collection known as the "Penitential Psalms".

Petrarch also published many volumes of his letters, including a few written to his long-dead friends from history such as Cicero and Virgil. Cicero, Virgil, and Seneca were his literary models. Most of his Latin writings are difficult to find today, but several of his works are available in English translations. Several of his Latin works are scheduled to appear in the Harvard University Press series "I Tatti". It is difficult to assign any precise dates to his writings because he tended to revise them throughout his life.

Petrarch collected his letters into two major sets of books called "Epistolae familiares" ("Letters on Familiar Matters") and "Seniles" ("Letters of Old Age"), both of which are available in English translation. The plan for his letters was suggested to him by knowledge of Cicero's letters. These were published "without names" to protect the recipients, all of whom had close relationships to Petrarch. The recipients of these letters included Philippe de Cabassoles, bishop of Cavaillon; Ildebrandino Conti, bishop of Padua; Cola di Rienzo, tribune of Rome; Francesco Nelli, priest of the Prior of the Church of the Holy Apostles in Florence; and Niccolò di Capoccia, a cardinal and priest of Saint Vitalis. His "Letter to Posterity" (the last letter in "Seniles") gives an autobiography and a synopsis of his philosophy in life. It was originally written in Latin and was completed in 1371 or 1372—the first such autobiography in a thousand years (since Saint Augustine).

While Petrarch's poetry was set to music frequently after his death, especially by Italian madrigal composers of the Renaissance in the 16th century, only one musical setting composed during Petrarch's lifetime survives. This is "Non al suo amante" by Jacopo da Bologna, written around 1350.

On April 6, 1327, after Petrarch gave up his vocation as a priest, the sight of a woman called "Laura" in the church of Sainte-Claire d'Avignon awoke in him a lasting passion, celebrated in the "Rime sparse" ("Scattered rhymes"). Later, Renaissance poets who copied Petrarch's style named this collection of 366 poems "Il Canzoniere" ("Song Book"). Laura may have been Laura de Noves, the wife of Count Hugues de Sade (an ancestor of the Marquis de Sade). There is little definite information in Petrarch's work concerning Laura, except that she is lovely to look at, fair-haired, with a modest, dignified bearing. Laura and Petrarch had little or no personal contact. According to his "Secretum", she refused him because she was already married. He channeled his feelings into love poems that were exclamatory rather than persuasive, and wrote prose that showed his contempt for men who pursue women. Upon her death in 1348, the poet found that his grief was as difficult to live with as was his former despair. Later in his "Letter to Posterity", Petrarch wrote: "In my younger days I struggled constantly with an overwhelming but pure love affair—my only one, and I would have struggled with it longer had not premature death, bitter but salutary for me, extinguished the cooling flames. I certainly wish I could say that I have always been entirely free from desires of the flesh, but I would be lying if I did".
While it is possible she was an idealized or pseudonymous character—particularly since the name "Laura" has a linguistic connection to the poetic "laurels" Petrarch coveted—Petrarch himself always denied it. His frequent use of "l'aura" is also remarkable: for example, the line "Erano i capei d'oro a "l'aura" sparsi" may both mean "her hair was all over Laura's body", and "the wind ("l'aura") blew through her hair". There is psychological realism in the description of Laura, although Petrarch draws heavily on conventionalised descriptions of love and lovers from troubadour songs and other literature of courtly love. Her presence causes him unspeakable joy, but his unrequited love creates unendurable desires, inner conflicts between the ardent lover and the mystic Christian, making it impossible to reconcile the two. Petrarch's quest for love leads to hopelessness and irreconcilable anguish, as he expresses in the series of paradoxes in Rima 134 "Pace non trovo, et non ò da far guerra;/e temo, et spero; et ardo, et son un ghiaccio": "I find no peace, and yet I make no war:/and fear, and hope: and burn, and I am ice".

Laura is unreachable – the few physical descriptions of her are vague, almost impalpable as the love he pines for, and such is perhaps the power of his verse, which lives off the melodies it evokes against the fading, diaphanous image that is no more consistent than a ghost. Francesco De Sanctis remarks much the same thing in his "Storia della letteratura italiana", and contemporary critics agree on the powerful music of his verse. Perhaps the poet was inspired by a famous singer he met in Veneto around the 1350s. Gianfranco Contini, in a famous essay on Petrarch's language ("Preliminari sulla lingua del Petrarca". Petrarca, Canzoniere. Turin, Einaudi, 1964) has spoken of linguistic indeterminacy—Petrarch never rises above the "bel pié" (her lovely foot): Laura is too holy to be painted; she is an awe-inspiring goddess. Sensuality and passion are suggested rather by the rhythm and music that shape the vague contours of the lady. In addition, some today consider Laura to be a representation of an "ideal Renaissance woman", based on her nature and definitive characteristics.

Petrarch is a world apart from Dante and his "Divina Commedia". In spite of the metaphysical subject, the "Commedia" is deeply rooted in the cultural and social milieu of turn-of-the-century Florence: Dante's rise to power (1300) and exile (1302), his political passions call for a "violent" use of language, where he uses all the registers, from low and trivial to sublime and philosophical. Petrarch confessed to Boccaccio that he had never read the "Commedia", remarks Contini, wondering whether this was true or Petrarch wanted to distance himself from Dante. Dante's language evolves as he grows old, from the courtly love of his early stilnovistic "Rime" and "Vita nuova" to the "Convivio" and "Divina Commedia", where Beatrice is sanctified as the goddess of philosophy—the philosophy announced by the Donna Gentile at the death of Beatrice.

In contrast, Petrarch's thought and style are relatively uniform throughout his life—he spent much of it revising the songs and sonnets of the "Canzoniere" rather than moving to new subjects or poetry. Here, poetry alone provides a consolation for personal grief, much less philosophy or politics (as in Dante), for Petrarch fights within himself (sensuality versus mysticism, profane versus Christian literature), not against anything outside of himself. The strong moral and political convictions which had inspired Dante belong to the Middle Ages and the libertarian spirit of the commune; Petrarch's moral dilemmas, his refusal to take a stand in politics, his reclusive life point to a different direction, or time. The free commune, the place that had made Dante an eminent politician and scholar, was being dismantled: the "signoria" was taking its place. Humanism and its spirit of empirical inquiry, however, were making progress—but the papacy (especially after Avignon) and the empire (Henry VII, the last hope of the white Guelphs, died near Siena in 1313) had lost much of their original prestige.

Petrarch polished and perfected the sonnet form inherited from Giacomo da Lentini and which Dante widely used in his "Vita nuova" to popularise the new courtly love of the "Dolce Stil Novo". The tercet benefits from Dante's terza rima (compare the "Divina Commedia"), the quatrains prefer the ABBA–ABBA to the ABAB–ABAB scheme of the Sicilians. The imperfect rhymes of "u" with closed "o" and "i" with closed "e" (inherited from Guittone's mistaken rendering of Sicilian verse) are excluded, but the rhyme of open and closed "o" is kept. Finally, Petrarch's enjambment creates longer semantic units by connecting one line to the following. The vast majority (317) of Petrarch's 366 poems collected in the "Canzoniere" (dedicated to Laura) were "sonnets", and the Petrarchan sonnet still bears his name.

Petrarch is traditionally called the father of Humanism and considered by many to be the "father of the Renaissance." In his work "Secretum meum" he points out that secular achievements did not necessarily preclude an authentic relationship with God. Petrarch argued instead that God had given humans their vast intellectual and creative potential to be used to their fullest. He inspired humanist philosophy which led to the intellectual flowering of the Renaissance. He believed in the immense moral and practical value of the study of ancient history and literature—that is, the study of human thought and action. Petrarch was a devout Catholic and did not see a conflict between realizing humanity's potential and having religious faith.

A highly introspective man, he shaped the nascent humanist movement a great deal because many of the internal conflicts and musings expressed in his writings were seized upon by Renaissance humanist philosophers and argued continually for the next 200 years. For example, Petrarch struggled with the proper relation between the active and contemplative life, and tended to emphasize the importance of solitude and study. In a clear disagreement with Dante, in 1346 Petrarch argued in his "De vita solitaria" that Pope Celestine V's refusal of the papacy in 1294 was as a virtuous example of solitary life. Later the politician and thinker Leonardo Bruni (1370–1444) argued for the active life, or "civic humanism". As a result, a number of political, military, and religious leaders during the Renaissance were inculcated with the notion that their pursuit of personal fulfillment should be grounded in classical example and philosophical contemplation.

Petrarch's influence is evident in the works of Serafino Ciminelli from Aquila (1466–1500) and in the works of Marin Držić (1508–1567) from Dubrovnik.

The Romantic composer Franz Liszt set three of Petrarch's Sonnets (47, 104, and 123) to music for voice, "Tre sonetti del Petrarca", which he later would transcribe for solo piano for inclusion in the suite "Années de Pèlerinage". Liszt also set a poem by Victor Hugo, " O quand je dors" in which Petrarch and Laura are invoked as the epitome of erotic love.

While in Avignon in 1991, Modernist composer Elliott Carter completed his solo flute piece "Scrivo in Vento" which is in part inspired by and structured by Petrarch's Sonnet 212, "Beato in sogno". It was premiered on Petrarch's 687th birthday.

In November 2003, it was announced that pathological anatomists would be exhuming Petrarch's body from his casket in Arquà Petrarca, in order to verify 19th-century reports that he had stood 1.83 meters (about six feet), which would have been tall for his period. The team from the University of Padua also hoped to reconstruct his cranium in order to generate a computerized image of his features to coincide with his 700th birthday. The tomb had been opened previously in 1873 by Professor Giovanni Canestrini, also of Padua University. When the tomb was opened, the skull was discovered in fragments and a DNA test revealed that the skull was not Petrarch's, prompting calls for the return of Petrarch's skull.

The researchers are fairly certain that the body in the tomb is Petrarch's due to the fact that the skeleton bears evidence of injuries mentioned by Petrarch in his writings, including a kick from a donkey when he was 42.



</doc>
<doc id="23735" url="https://en.wikipedia.org/wiki?curid=23735" title="PLD">
PLD

PLD may refer to:





</doc>
<doc id="23738" url="https://en.wikipedia.org/wiki?curid=23738" title="Propeller">
Propeller

A propeller is a device with a rotating hub and radiating blades that are set at a pitch to form a helical spiral, that when rotated performs an action which is similar to Archimedes' screw. It transforms rotational power into linear thrust by acting upon a working fluid such as water or air. The rotational motion of the blades is converted into thrust by creating a pressure difference between the two surfaces. A given mass of working fluid is accelerated in one direction and the craft moves in the opposite direction. Propeller dynamics, like those of aircraft wings, can be modelled by Bernoulli's principle and Newton's third law. Most marine propellers are screw propellers with helical blades rotating around an approximately horizontal axis or propeller shaft.

The principle employed in using a screw propeller is used in sculling. It is part of the skill of propelling a Venetian gondola but was used in a less refined way in other parts of Europe and probably elsewhere. For example, propelling a canoe with a single paddle using a "pitch stroke" or side slipping a canoe with a "scull" involves a similar technique. In China, sculling, called "lu", was also used by the 3rd century AD.

In sculling, a single blade is moved through an arc, from side to side taking care to keep presenting the blade to the water at the effective angle. The innovation introduced with the screw propeller was the extension of that arc through more than 360° by attaching the blade to a rotating shaft. Propellers can have a single blade, but in practice there are nearly always more than one so as to balance the forces involved.
The origin of the screw propeller starts with Archimedes, who used a screw to lift water for irrigation and bailing boats, so famously that it became known as Archimedes' screw. It was probably an application of spiral movement in space (spirals were a special study of Archimedes) to a hollow segmented water-wheel used for irrigation by Egyptians for centuries. Leonardo da Vinci adopted the principle to drive his theoretical helicopter, sketches of which involved a large canvas screw overhead.

In 1661, Toogood and Hays proposed using screws for waterjet propulsion, though not as a propeller. Robert Hooke in 1681 designed a horizontal watermill which was remarkably similar to the Kirsten-Boeing vertical axis propeller designed almost two and a half centuries later in 1928; two years later Hooke modified the design to provide motive power for ships through water. In 1693 a Frenchman by the name of Du Quet invented a screw propeller which was tried in 1693 but later abandoned. In 1752, the "Academie des Sciences" in Paris granted Burnelli a prize for a design of a propeller-wheel. At about the same time, the French mathematician Alexis-Jean-Pierre Paucton, suggested a water propulsion system based on the Archimedean screw. In 1771, steam-engine inventor James Watt in a private letter suggested using "spiral oars" to propel boats, although he did not use them with his steam engines, or ever implement the idea.

One of the first practical and applied uses of a propeller was on a submarine dubbed which was designed in New Haven, Connecticut, in 1775 by Yale student and inventor David Bushnell, with the help of the clock maker, engraver, and brass foundryman Isaac Doolittle, and with Bushnell's brother Ezra Bushnell and ship's carpenter and clock maker Phineas Pratt constructing the hull in Saybrook, Connecticut. On the night of September 6, 1776, Sergeant Ezra Lee piloted "Turtle" in an attack on HMS "Eagle" in New York Harbor. "Turtle" also has the distinction of being the first submarine used in battle. Bushnell later described the propeller in an October 1787 letter to Thomas Jefferson: "An oar formed upon the principle of the screw was fixed in the forepart of the vessel its axis entered the vessel and being turned one way rowed the vessel forward but being turned the other way rowed it backward. It was made to be turned by the hand or foot." The brass propeller, like all the brass and moving parts on "Turtle", was crafted by the "ingenious mechanic" Issac Doolittle of New Haven.

In 1785, Joseph Bramah in England proposed a propeller solution of a rod going through the underwater aft of a boat attached to a bladed propeller, though he never built it. In 1802, Edward Shorter proposed using a similar propeller attached to a rod angled down temporarily deployed from the deck above the waterline and thus requiring no water seal, and intended only to assist becalmed sailing vessels. He tested it on the transport ship "Doncaster" in Gibraltar and at Malta, achieving a speed of .

In 1802, the American lawyer and inventor John Stevens built a boat with a rotary stem engine coupled to a four-bladed propeller. The craft achieved a speed of , but Stevens abandoned propellers due to the inherent danger in using the high-pressure steam engines. His subsequent vessels were paddle-wheeled boats.

By 1827, Czech-Austrian inventor Josef Ressel had invented a screw propeller which had multiple blades fastened around a conical base. He had tested his propeller in February 1826 on a small ship that was manually driven. He was successful in using his bronze screw propeller on an adapted steamboat (1829). His ship, "Civetta" of 48 gross register tons, reached a speed of about . This was the first ship successfully driven by an Archimedes screw-type propeller. After a new steam engine had an accident (cracked pipe weld) his experiments were banned by the Austro-Hungarian police as dangerous. Josef Ressel was at the time a forestry inspector for the Austrian Empire. But before this he received an Austro-Hungarian patent (license) for his propeller (1827). He died in 1857. This new method of propulsion was an improvement over the paddlewheel as it was not so affected by either ship motions or changes in draft as the vessel burned coal.

John Patch, a mariner in Yarmouth, Nova Scotia developed a two-bladed, fan-shaped propeller in 1832 and publicly demonstrated it in 1833, propelling a row boat across Yarmouth Harbour and a small coastal schooner at Saint John, New Brunswick, but his patent application in the United States was rejected until 1849 because he was not an American citizen. His efficient design drew praise in American scientific circles but by this time there were multiple competing versions of the marine propeller.

Although there was much experimentation with screw propulsion until the 1830s, few of these inventions were pursued to the testing stage, and those that were proved unsatisfactory for one reason or another.

In 1835, two inventors in Britain, John Ericsson and Francis Pettit Smith, began working separately on the problem. Smith was first to take out a screw propeller patent on 31 May, while Ericsson, a gifted Swedish engineer then working in Britain, filed his patent six weeks later. Smith quickly built a small model boat to test his invention, which was demonstrated first on a pond at his Hendon farm, and later at the Royal Adelaide Gallery of Practical Science in London, where it was seen by the Secretary of the Navy, Sir William Barrow. Having secured the patronage of a London banker named Wright, Smith then built a , canal boat of six tons burthen called "Francis Smith", which was fitted with a wooden propeller of his own design and demonstrated on the Paddington Canal from November 1836 to September 1837. By a fortuitous accident, the wooden propeller of two turns was damaged during a voyage in February 1837, and to Smith's surprise the broken propeller, which now consisted of only a single turn, doubled the boat's previous speed, from about four miles an hour to eight. Smith would subsequently file a revised patent in keeping with this accidental discovery.

In the meantime, Ericsson built a screw-propelled steamboat, "Francis B. Ogden" in 1837, and demonstrated his boat on the River Thames to senior members of the British Admiralty, including Surveyor of the Navy Sir William Symonds. In spite of the boat achieving a speed of 10 miles an hour, comparable with that of existing paddle steamers, Symonds and his entourage were unimpressed. The Admiralty maintained the view that screw propulsion would be ineffective in ocean-going service, while Symonds himself believed that screw propelled ships could not be steered efficiently. Following this rejection, Ericsson built a second, larger screw-propelled boat, "Robert F. Stockton", and had her sailed in 1839 to the United States, where he was soon to gain fame as the designer of the U.S. Navy's first screw-propelled warship, .
Apparently aware of the Royal Navy's view that screw propellers would prove unsuitable for seagoing service, Smith determined to prove this assumption wrong. In September 1837, he took his small vessel (now fitted with an iron propeller of a single turn) to sea, steaming from Blackwall, London to Hythe, Kent, with stops at Ramsgate, Dover and Folkestone. On the way back to London on the 25th, Smith's craft was observed making headway in stormy seas by officers of the Royal Navy. The Admiralty's interest in the technology was revived, and Smith was encouraged to build a full size ship to more conclusively demonstrate the technology's effectiveness.

"Archimedes" had considerable influence on ship development, encouraging the adoption of screw propulsion by the Royal Navy, in addition to her influence on commercial vessels. Trials with Smith's "Archimedes" led to the famous tug-of-war competition in 1845 between the screw-driven and the paddle steamer ; the former pulling the latter backward at .

She also had a direct influence on the design of another innovative vessel, Isambard Kingdom Brunel's in 1843, then the world's largest ship and the first screw-propelled steamship to cross the Atlantic Ocean in August 1845.

Screw propeller design stabilized in the 1880s.

Propellers without a central shaft consist of propeller blades attached to a ring which is part of a circle-shaped electric motor. This design is known as a Rim-driven thruster and has been used by some small, self-guided robotic ships. It has been referenced by the ambiguous phrase "screwless propulsion" in the United Kingdom.

The twisted aerofoil shape of modern aircraft propellers was pioneered by the Wright brothers. While some earlier engineers had attempted to model air propellers on marine propellers, the Wrights realized that a propeller is essentially the same as a wing, and were able to use data from their earlier wind tunnel experiments on wings. They also introduced a twist along the length of the blades. This was necessary to ensure the angle of attack of the blades was kept relatively constant along their length. Their original propeller blades were only about 5% less efficient than the modern equivalent, some 100 years later. The understanding of low speed propeller aerodynamics was fairly complete by the 1920s, but later requirements to handle more power in smaller diameter have made the problem more complex.

Alberto Santos Dumont, another early pioneer, applied the knowledge he gained from experiences with airships to make a propeller with a steel shaft and aluminium blades for his 14 bis biplane. Some of his designs used a bent aluminium sheet for blades, thus creating an airfoil shape. They were heavily undercambered, and this plus the absence of lengthwise twist made them less efficient than the Wright propellers. Even so, this was perhaps the first use of aluminium in the construction of an airscrew.

In the second half of the nineteenth century, several theories were developed. The momentum theory or disk actuator theory – a theory describing a mathematical model of an ideal propeller – was developed by W.J.M. Rankine (1865), Alfred George Greenhill (1888) and R.E. Froude (1889). The propeller is modelled as an infinitely thin disc, inducing a constant velocity along the axis of rotation. This disc creates a flow around the propeller. Under certain mathematical premises of the fluid, there can be extracted a mathematical connection between power, radius of the propeller, torque and induced velocity. Friction is not included.

The blade element theory (BET) is a mathematical process originally designed by William Froude (1878), David W. Taylor (1893) and Stefan Drzewiecki to determine the behaviour of propellers. It involves breaking an airfoil down into several small parts then determining the forces on them. These forces are then converted into accelerations, which can be integrated into velocities and positions.

A propeller is the most common propulsor on ships, imparting momentum to a fluid which causes a force to act on the ship. The ideal efficiency of any propulsor is that of an actuator disc in an ideal fluid. This is called the
Froude efﬁciency and is a natural limit which cannot be exceeded by any device, no matter how good it is. Any propulsor
which has virtually zero slip in the water, whether this is a very large propeller or a huge drag device, approaches 100%
Froude efﬁciency. The essence of the actuator-disc theory is that if the slip is deﬁned as the ratio of fluid velocity
increase through the disc to vehicle velocity, the Froude efﬁciency is equal to 1/(slip + 1). Thus a lightly loaded propeller with a large swept area can have a high Froude efﬁciency.

An actual propeller has blades made up of sections of helicoidal surfaces which can be thought to 'screw' through the fluid (hence the common reference to propellers as "screws"). Actually the blades are twisted airfoils or hydrofoils and each section contributes to the total thrust. Two to five blades are most common, although designs which are intended to operate at reduced noise will have more blades and one-bladed ones with a counterweight have also been used. Lightly loaded propellers for light aircraft and human-powered boats mostly have two blades, motor boats mostly have three blades. The blades are attached to a "boss" (hub), which should be as small as the needs of strength allow – with fixed-pitch propellers the blades and boss are usually a single casting.

An alternative design is the controllable-pitch propeller (CPP, or CRP for controllable-reversible pitch), where the blades are rotated normally to the drive shaft by additional machinery – usually hydraulics – at the hub and control linkages running down the shaft. This allows the drive machinery to operate at a constant speed while the propeller loading is changed to match operating conditions. It also eliminates the need for a reversing gear and allows for more rapid change to thrust, as the revolutions are constant. This type of propeller is most common on ships such as tugs where there can be enormous differences in propeller loading when towing compared to running free. The downsides of a CPP/CRP include: the large hub which decreases the torque required to cause cavitation, the mechanical complexity which limits transmission power and the extra blade shaping requirements forced upon the propeller designer.

For smaller motors there are self-pitching propellers. The blades freely move through an entire circle on an axis at right angles to the shaft. This allows hydrodynamic and centrifugal forces to 'set' the angle the blades reach and so the pitch of the propeller.

A propeller that turns clockwise to produce forward thrust, when viewed from aft, is called right-handed. One that turns anticlockwise is said to be left-handed. Larger vessels often have twin screws to reduce "heeling torque", counter-rotating propellers, the starboard screw is usually right-handed and the port left-handed, this is called outward turning. The opposite case is called inward turning. Another possibility is contra-rotating propellers, where two propellers rotate in opposing directions on a single shaft, or on separate shafts on nearly the same axis. Contra-rotating propellers offer increased efficiency by capturing the energy lost in the tangential velocities imparted to the fluid by the forward propeller (known as "propeller swirl"). The flow field behind the aft propeller of a contra-rotating set has very little "swirl", and this reduction in energy loss is seen as an increased efficiency of the aft propeller.

An azimuthing propeller is a propeller that turns around the vertical axis. The individual airfoil-shaped blades turn as the propeller moves so that they are always generating lift in the vessel's direction of movement. This type of propeller can reverse or change its direction of thrust very quickly.

Fixed-wing aircraft are also subject to the P-factor effect, in which a rotating propeller will yaw an aircraft slightly to one side because the relative wind it produces is asymmetrical. It is particularly noticeable when climbing, but is usually simple to compensate for with the aircraft's rudder. A more serious situation can exist if a multi-engine aircraft loses power to one of its engines, in particular the one which is positioned on the side that enhances the P-factor. This power plant is called the critical engine and its loss will require more control compensation by the pilot. Geometric pitch is the distance an element of an airplane propeller would advance in one revolution if it were moving along a helix having an angle equal to that between the chord of the element and a plane perpendicular to the propeller axis.

Cavitation is the formation of vapor bubbles in water near a moving propeller blade in regions of low pressure due to Bernoulli's principle. It can occur if an attempt is made to transmit too much power through the screw, or if the propeller is operating at a very high speed. Cavitation can waste power, create vibration and wear, and cause damage to the propeller. It can occur in many ways on a propeller. The two most common types of propeller cavitation are suction side surface cavitation and tip vortex cavitation.

Suction side surface cavitation forms when the propeller is operating at high rotational speeds or under heavy load (high blade lift coefficient). The pressure on the upstream surface of the blade (the "suction side") can drop below the vapor pressure of the water, resulting in the formation of a vapor pocket. Under such conditions, the change in pressure between the downstream surface of the blade (the "pressure side") and the suction side is limited, and eventually reduced as the extent of cavitation is increased. When most of the blade surface is covered by cavitation, the pressure difference between the pressure side and suction side of the blade drops considerably, as does the thrust produced by the propeller. This condition is called "thrust breakdown". Operating the propeller under these conditions wastes energy, generates considerable noise, and as the vapor bubbles collapse it rapidly erodes the screw's surface due to localized shock waves against the blade surface.

Tip vortex cavitation is caused by the extremely low pressures formed at the core of the tip vortex. The tip vortex is caused by fluid wrapping around the tip of the propeller; from the pressure side to the suction side. This video demonstrates tip vortex cavitation. Tip vortex cavitation typically occurs before suction side surface cavitation and is less damaging to the blade, since this type of cavitation doesn't collapse on the blade, but some distance downstream.

Cavitation can be used as an advantage in design of very high performance propellers, in the form of the supercavitating propeller. In this case, the blade section is designed such that the pressure side stays wetted while the suction side is completely covered by cavitation vapor. Because the suction side is covered with vapor instead of water it encounters very low viscous friction, making the supercavitating (SC) propeller comparably efficient at high speed. The shaping of SC blade sections however, make it inefficient at low speeds, when the suction side of the blade is wetted. (See also fluid dynamics).

A similar, but quite separate issue, is "ventilation," which occurs when a propeller operating near the surface draws air into the blades, causing a similar loss of power and shaft vibration, but without the related potential blade surface damage caused by cavitation. Both effects can be mitigated by increasing the submerged depth of the propeller: cavitation is reduced because the hydrostatic pressure increases the margin to the vapor pressure, and ventilation because it is further from surface waves and other air pockets that might be drawn into the slipstream.

The blade profile of propellers designed to operate in a ventilated condition is often not of an aerofoil section and is a blunt ended taper instead. These are often known as "chopper" type propellers.

The force (F) experienced by a foil is determined by its area (A), fluid density (ρ), velocity (V) and the angle of the foil to the fluid flow, called "angle of attack" (formula_1), where:

The force has two parts – that normal to the direction of flow is "lift" (L) and that in the direction of flow is "drag " (D). Both can be expressed mathematically:

where C and C are lift coefficient and drag coefficient respectively.

Each coefficient is a function of the angle of attack and Reynolds number. As the angle of attack increases lift rises rapidly from the "no lift angle" before slowing its increase and then decreasing, with a sharp drop as the "stall angle" is reached and flow is disrupted. Drag rises slowly at first and as the rate of increase in lift falls and the angle of attack increases drag increases more sharply.

For a given strength of circulation (formula_5), formula_6. The effect of the flow over and the circulation around the foil is to reduce the velocity over the face and increase it over the back of the blade. If the reduction in pressure is too much in relation to the ambient pressure of the fluid, "cavitation" occurs, bubbles form in the low pressure area and are moved towards the blade's trailing edge where they collapse as the pressure increases, this reduces propeller efficiency and increases noise. The forces generated by the bubble collapse can cause permanent damage to the surfaces of the blade.

Taking an arbitrary radial section of a blade at "r", if revolutions are "N" then the rotational velocity is formula_7. If the blade was a complete screw it would advance through a solid at the rate of "NP", where "P" is the pitch of the blade. In water the advance speed is rather lower, formula_8, the difference, or "slip ratio", is:

where formula_10 is the "advance coefficient", and formula_11 is the "pitch ratio".

The forces of lift and drag on the blade, "dA", where force normal to the surface is "dL":

where:

These forces contribute to thrust, "T", on the blade:

where:

formula_15

As formula_16,

From this total thrust can be obtained by integrating this expression along the blade. The transverse force is found in a similar manner:

Substituting for formula_19 and multiplying by "r", gives torque as:

which can be integrated as before.

The total thrust power of the propeller is proportional to formula_21 and the shaft power to formula_22. So efficiency is formula_23. The blade efficiency is in the ratio between thrust and torque:

showing that the blade efficiency is determined by its momentum and its qualities in the form of angles formula_25 and formula_26, where formula_26 is the ratio of the drag and lift coefficients.

This analysis is simplified and ignores a number of significant factors including interference between the blades and the influence of tip vortices.

The thrust, "T", and torque, "Q", depend on the propeller's diameter, "D", revolutions, "N", and rate of advance, formula_28, together with the character of the fluid in which the propeller is operating and gravity. These factors create the following non-dimensional relationship:

where formula_30 is a function of the advance coefficient, formula_31 is a function of the Reynolds' number, and formula_32 is a function of the Froude number. Both formula_31 and formula_32 are likely to be small in comparison to formula_30 under normal operating conditions, so the expression can be reduced to:

For two identical propellers the expression for both will be the same. So with the propellers formula_37, and using the same subscripts to indicate each propeller:

For both Froude number and advance coefficient:

where formula_40 is the ratio of the linear dimensions.

Thrust and velocity, at the same Froude number, give thrust power:

For torque:

When a propeller is added to a ship its performance is altered; there is the mechanical losses in the transmission of power; a general increase in total resistance; and the hull also impedes and renders non-uniform the flow through the propeller. The ratio between a propeller's efficiency attached to a ship (formula_44) and in open water (formula_45) is termed "relative rotative efficiency."

The "overall propulsive efficiency" (an extension of "effective power" (formula_46)) is developed from the "propulsive coefficient" (formula_47), which is derived from the installed shaft power (formula_48) modified by the effective power for the hull with appendages (formula_49), the propeller's thrust power (formula_50), and the relative rotative efficiency.

Producing the following:

The terms contained within the brackets are commonly grouped as the "quasi-propulsive coefficient" (formula_63, formula_64). The formula_63 is produced from small-scale experiments and is modified with a load factor for full size ships.

"Wake" is the interaction between the ship and the water with its own velocity relative to the ship. The wake has three parts: the velocity of the water around the hull; the boundary layer between the water dragged by the hull and the surrounding flow; and the waves created by the movement of the ship. The first two parts will reduce the velocity of water into the propeller, the third will either increase or decrease the velocity depending on whether the waves create a crest or trough at the propeller.

The controllable-pitch propeller has several advantages over the fixed-pitch variety. These include: the least drag depending on the speed used, the ability to move the sea vessel backwards, and the ability to use the "vane"-stance, which gives the least water resistance when not using the propeller (e.g. when the sails are used instead).

An advanced type of propeller used on German Type 212 submarines is called a skewback propeller. As in the scimitar blades used on some aircraft, the blade tips of a skewback propeller are swept back against the direction of rotation. In addition, the blades are tilted rearward along the longitudinal axis, giving the propeller an overall cup-shaped appearance. This design preserves thrust efficiency while reducing cavitation, and thus makes for a quiet, stealthy design.

A small number of ships use propellers with winglets similar to those on some airplanes, reducing tip vortices and improving efficiency.

A modular propeller provides more control over the boat's performance. There is no need to change an entire prop, when there is an opportunity to only change the pitch or the damaged blades. Being able to adjust pitch will allow for boaters to have better performance while in different altitudes, water sports, and/or cruising.

Voith Schneider propellers use four untwisted straight blades turning around a vertical axis instead of helical blades and can provide thrust in any direction at any time, at the cost of higher mechanical complexity.

For smaller engines, such as outboards, where the propeller is exposed to the risk of collision with heavy objects, the propeller often includes a device that is designed to fail when overloaded; the device or the whole propeller is sacrificed so that the more expensive transmission and engine are not damaged.

Typically in smaller (less than ) and older engines, a narrow shear pin through the drive shaft and propeller hub transmits the power of the engine at normal loads. The pin is designed to shear when the propeller is put under a load that could damage the engine. After the pin is sheared the engine is unable to provide propulsive power to the boat until a new shear pin is fitted.

In larger and more modern engines, a rubber bushing transmits the torque of the drive shaft to the propeller's hub. Under a damaging load the friction of the bushing in the hub is overcome and the rotating propeller slips on the shaft, preventing overloading of the engine's components. After such an event the rubber bushing may be damaged. If so, it may continue to transmit reduced power at low revolutions, but may provide no power, due to reduced friction, at high revolutions. Also, the rubber bushing may perish over time leading to its failure under loads below its designed failure load.

Whether a rubber bushing can be replaced or repaired depends upon the propeller; some cannot. Some can, but need special equipment to insert the oversized bushing for an interference fit. Others can be replaced easily. The "special equipment" usually consists of a funnel, a press and rubber lubricant (soap). If one does not have access to a lathe, an improvised funnel can be made from steel tube and car body filler; as the filler is only subject to compressive forces it is able to do a good job. Often, the bushing can be drawn into place with nothing more complex than a couple of nuts, washers and a threaded rod. A more serious problem with this type of propeller is a "frozen-on" spline bushing, which makes propeller removal impossible. In such cases the propeller must be heated in order to deliberately destroy the rubber insert. Once the propeller is removed, the splined tube can be cut away with a grinder and a new spline bushing is then required. To prevent a recurrence of the problem, the splines can be coated with anti-seize anti-corrosion compound.

In some modern propellers, a hard polymer insert called a "drive sleeve" replaces the rubber bushing. The splined or other non-circular cross section of the sleeve inserted between the shaft and propeller hub transmits the engine torque to the propeller, rather than friction. The polymer is weaker than the components of the propeller and engine so it fails before they do when the propeller is overloaded. This fails completely under excessive load, but can easily be replaced.

A cleaver is a type of propeller design especially used for boat racing. Its leading edge is formed round, while the trailing edge is cut straight. It provides little bow lift, so that it can be used on boats that do not need much bow lift, for instance hydroplanes, that naturally have enough hydrodynamic bow lift. To compensate for the lack of bow lift, a hydrofoil may be installed on the lower unit. Hydrofoils reduce bow lift and help to get a boat out of the hole and onto plane.







</doc>
<doc id="23739" url="https://en.wikipedia.org/wiki?curid=23739" title="Peter Duesberg">
Peter Duesberg

Peter H. Duesberg (born December 2, 1936) is a German American molecular biologist and a professor of molecular and cell biology at the University of California, Berkeley. He is known for his early research into genetic aspects of cancer. He played a pivotal role in the AIDS denialism controversy as a proponent of the belief that HIV does not cause AIDS.

Duesberg received acclaim early in his career for research on oncogenes and cancer. With Peter K. Vogt, he reported in 1970 that a cancer-causing virus of birds had extra genetic material compared with non-cancer-causing viruses, hypothesizing that this material contributed to cancer. At the age of 36, Duesberg was awarded tenure at the University of California, Berkeley, and at 49, he was elected to the National Academy of Sciences. He received an Outstanding Investigator Grant from the National Institutes of Health in 1986, and from 1986 to 1987 was a Fogarty scholar-in-residence at the NIH laboratories in Bethesda, Maryland.

Long considered a contrarian by his scientific colleagues, Duesberg began to gain public notoriety with a March 1987 article in "Cancer Research" entitled "Retroviruses as Carcinogens and Pathogens: Expectations and Reality". In this and subsequent writings, Duesberg proposed his hypothesis that AIDS is caused by long-term consumption of recreational drugs or antiretroviral drugs, and that HIV is a harmless passenger virus. In contrast, the scientific consensus is that HIV infection causes AIDS; Duesberg's HIV/AIDS claims have been addressed and rejected as erroneous by the scientific community. Reviews of his opinions in "Nature" and "Science" asserted that they were unpersuasive and based on selective reading of the literature, and that although Duesberg had a right to a dissenting opinion, his failure to fairly review evidence that HIV causes AIDS meant that his opinion lacked credibility.

Duesberg's views are cited as major influences on South African HIV/AIDS policy under the administration of Thabo Mbeki, which embraced AIDS denialism. Duesberg served on an advisory panel to Mbeki convened in 2000. The Mbeki administration's failure to provide antiretroviral drugs in a timely manner, due in part to the influence of AIDS denialism, is thought to be responsible for hundreds of thousands of preventable AIDS deaths and HIV infections in South Africa. Duesberg disputed these findings in an article in the journal "Medical Hypotheses", but the journal's publisher, Elsevier, later retracted Duesberg's article over accuracy and ethics concerns as well as its rejection during peer review. The incident prompted several complaints to Duesberg's institution, the University of California, Berkeley, which began a misconduct investigation of Duesberg in 2009. The investigation was dropped in 2010, with university officials finding "insufficient evidence ... to support a recommendation for disciplinary action."

Duesberg grew up during World War II, raised as a Catholic in Germany. He moved to the US in 1964 to work at the University of California, Berkeley, following completion of a PhD in chemistry at the University of Frankfurt.

In the 1970s, Duesberg won international acclaim for his groundbreaking work on cancer. Duesberg's early work on cancer included being the first to identify the oncogene "v-src" from the genome of Rous sarcoma virus, a chicken virus believed to trigger tumor growth. Duesberg disputes the importance of oncogenes and retroviruses in cancer. He supports the aneuploidy hypothesis of cancer that was first proposed in 1914 by Theodor Heinrich Boveri.

Duesberg rejects the importance of mutations, oncogenes, and anti-oncogenes entirely. Duesberg along with other researchers, in a 1998 paper published in "Proceedings of the National Academy of Sciences", reported a mathematical correlation between chromosome number and the genetic instability of cancer cells, which they dubbed "the ploidy factor," confirming earlier research by other groups that demonstrated an association between degree of aneuploidy and metastasis.

Although unwilling to concur with Duesberg in throwing out a role for cancer genes, many researchers do support exploration of alternative hypotheses. Research and debate on this subject is ongoing. In 2007, "Scientific American" published an article by Duesberg on his aneuploidy cancer theory. In an editorial explaining their decision to publish this article, the editors of "Scientific American" stated: "Thus, as wrong as Duesberg surely is about HIV, there is at least a chance that he is significantly right about cancer."

In his 1996 book, "Inventing the AIDS Virus", and in numerous journal articles and letters to the editor, Duesberg asserts that HIV is harmless and that recreational and pharmaceutical drug use, especially of zidovudine (AZT, a drug used in the treatment of AIDS) are the causes of AIDS outside Africa (the so-called Duesberg hypothesis). He considers AIDS diseases as markers for drug use, e.g., use of poppers (alkyl nitrites) among some homosexuals, asserting a correlation between AIDS and recreational drug use. This correlation hypothesis has been disproven by evidence showing that only HIV infection, not homosexuality nor recreational/pharmaceutical drug use, predicts who will develop AIDS.

Duesberg asserts that AIDS in Africa is misdiagnosed and the epidemic a "myth", claiming incorrectly that the diagnostic criteria for AIDS are different in Africa than elsewhere and that the breakdown of the immune system in African AIDS patients can be explained exclusively by factors such as malnutrition, tainted drinking water, and various infections that he presumes are common to AIDS patients in Africa. Duesberg also argues that retroviruses like HIV must be harmless to survive, and that the normal mode of retroviral propagation is mother-to-child transmission by infection in utero.

Since Duesberg published his first paper on the subject in 1987, scientists have examined and criticized the accuracy of his hypotheses on AIDS causation. Duesberg sustained a long dispute with John Maddox, then-editor of the scientific journal "Nature", demanding the right to rebut articles that HIV caused AIDS. For several years Maddox consented to this demand but ultimately refused to continue to publish Duesberg's criticisms:
A number of scientific criticisms of Duesberg's hypothesis were summarized in a review article in the journal "Science" in 1994, which presented the results of a 3-month scientific investigation into some of Duesberg's claims. In the "Science" article, science writer Jon Cohen interviewed both HIV researchers and AIDS denialists (including Duesberg himself) and examined the AIDS literature in addition to review articles written by Duesberg. The article stated:
The article also stated that although Duesberg and the AIDS denialist movement have garnered support from some prominent scientists, including Nobel Prize winner Kary Mullis, most of this support is related to Duesberg's right to hold a dissenting opinion, rather than support of his specific claim that HIV does not cause AIDS. Duesberg has been described as "the individual who has done the most damage" regarding denialism, due to the apparent scientific legitimacy his scientific credentials give to his statements.

In a 2010 article on conspiracy theories in science, Ted Goertzel highlights Duesberg's opposition to the HIV/AIDS connection as an example in which scientific findings are disputed on irrational grounds, relying on rhetoric, appeal to fairness and the right to a dissenting opinion rather than on evidence. Goertzel stated that Duesberg, along with many other denialists frequently invoke the meme of a "courageous independent scientist resisting orthodoxy", invoking the name of persecuted physicist and astronomer Galileo Galilei. Regarding this comparison, Goertzel stated:
Duesberg's advocacy of AIDS denialism has, by all accounts, effectively made him a pariah to the worldwide scientific community.

In 2000, Duesberg was the most prominent AIDS denialist to sit on a 44-member Presidential Advisory Panel on HIV and AIDS convened by then-president Thabo Mbeki of South Africa. The panel was scheduled to meet concurrently with the 2000 International AIDS Conference in Durban and to convey the impression that Mbeki's doubts about HIV/AIDS science were valid and actively discussed in the scientific community.

The views of the denialists on the panel, aired during the AIDS conference, received renewed attention. Mbeki later suffered substantial political fallout for his support for AIDS denialism and for opposing the treatment of pregnant HIV-positive South African women with antiretroviral medication. Mbeki partly attenuated his ties with denialists in 2002, asking them to stop associating their names with his.

In response to the inclusion of AIDS denialists on Mbeki's panel, the Durban Declaration was drafted and signed by over 5,000 scientists and physicians, describing the evidence that HIV causes AIDS as "clear-cut, exhaustive and unambiguous".

Two independent studies have concluded that the public health policies of Thabo Mbeki's government, shaped in part by Duesberg's writings and advice, were responsible for over 330,000 excess AIDS deaths and many preventable infections, including those of infants.

A 2008 feature story on Duesberg in "Discover" addresses Duesberg's role in anti-HIV drug-preventable deaths in South Africa. Jeanne Linzer interviews prominent HIV/AIDS expert Max Essex, who suggests that,
In 2009, Duesberg and co-authors including David Rasnick published an article in the journal "Medical Hypotheses", which is not peer reviewed. The article had been rejected previously by the journal "JAIDS", and a peer reviewer had warned that the authors could face scientific misconduct charges if the paper were published.

The reviewers claimed that Duesberg and his co-authors cherry-picked data, cited favorable results while ignoring unfavorable results, and quoted statements out of context. Moreover, they claim that Duesberg "[committed] a serious breach of professional ethics" by failing to state a possible conflict of interest: That co-author Rasnick previously worked for Matthias Rath, a vitamin entrepreneur who sold vitamin pills as AIDS remedies. The article was not revised in response to these criticisms.

In the article, Duesberg questioned research reporting that drugs policies implemented by the South African government on the advice of Duesberg, Rasnick and others had led to excess AIDS deaths. Observing that the overall population of South Africa has increased, Duesberg claimed that HIV must be a harmless "passenger virus" that has not caused deaths in South Africa or elsewhere. Duesberg stated that HIV does not replicate in the body and that antiviral drugs, which he calls "inevitably toxic," do not inhibit HIV. In addition, Duesberg wrote that neither he nor his co-authors had financial conflicts of interest.

Scientists expressed concerns to Elsevier, the publisher of "Medical Hypotheses", about unsupported assertions and incorrect statements by Duesberg. After an internal review and with a unanimous recommendation of rejection by five "Lancet" reviewers, Elsevier stated that the article was flawed and of potential danger to global public health. Elsevier permanently withdrew the Duesberg article and another AIDS denialist publication and asked that the editor of the journal implement a peer review process.

Letters of complaint to the University of California, Berkeley, including one from Nathan Geffen of the South African Treatment Action Campaign (TAC), prompted university officials to open an inquiry into possible academic misconduct related to false statements and failure to disclose potential conflicts of interest. The investigation was dropped in 2010, with University officials finding "insufficient evidence...to support a recommendation for disciplinary action." The investigation did not evaluate the merits of the research but found that publishing the article was protected by the principle of academic freedom.




</doc>
<doc id="23740" url="https://en.wikipedia.org/wiki?curid=23740" title="Toxin">
Toxin

A toxin is a poisonous substance produced within living cells or organisms; synthetic toxicants created by artificial processes are thus excluded. The term was first used by organic chemist Ludwig Brieger (1849–1919), derived from the word toxic.

Toxins can be small molecules, peptides, or proteins that are capable of causing disease on contact with or absorption by body tissues interacting with biological macromolecules such as enzymes or cellular receptors. Toxins vary greatly in their toxicity, ranging from usually minor (such as a bee sting) to almost immediately deadly (such as botulinum toxin).

Toxins are often distinguished from other chemical agents by their method of production—the word toxin does not specify method of delivery (compare with venom and the broader meaning of poison—all substances that can also cause disturbances to organisms). It simply means it is a biologically produced poison.

According to an International Committee of the Red Cross review of the Biological Weapons Convention, "Toxins are poisonous products of organisms; unlike biological agents, they are inanimate and not capable of reproducing themselves", and "Since the signing of the Constitution, there have been no disputes among the parties regarding the definition of biological agents or toxins".

According to Title 18 of the United States Code, "... the term "toxin" means the toxic material or product of plants, animals, microorganisms (including, but not limited to, bacteria, viruses, fungi, rickettsiae or protozoa), or infectious substances, or a recombinant or synthesized molecule, whatever their origin and method of production..."

A rather informal terminology of individual toxins relates them to the anatomical location where their effects are most notable:

On a broader scale, toxins may be classified as either exotoxins, being excreted by an organism, or endotoxins, that are released mainly when bacteria are lysed.

The term "biotoxin" is sometimes used to explicitly confirm the biological origin. Biotoxins can be further classified, for example, as fungal biotoxins, microbial toxins, plant biotoxins, or animal biotoxins.

Toxins produced by microorganisms are important virulence determinants responsible for microbial pathogenicity and/or evasion of the host immune response.

Biotoxins vary greatly in purpose and mechanism, and can be highly complex (the venom of the cone snail contains dozens of small proteins, each targeting a specific nerve channel or receptor), or relatively small protein.

Biotoxins in nature have two primary functions:

Some of the more well known types of biotoxins include:

The term "environmental toxin" can sometimes explicitly include synthetic contaminants such as industrial pollutants and other artificially made toxic substances. As this contradicts most formal definitions of the term "toxin", it is important to confirm what the researcher means when encountering the term outside of microbiological contexts.

Environmental toxins from food chains that may be dangerous to human health include:

In general, when scientists determine the amount of a substance that may be hazardous for humans, animals and/or the environment they determine the amount of the substance likely to trigger effects and if possible establish a safe level. In Europe, the European Food Safety Authority produced risk assessments for more than 4,000 substances in over 1,600 scientific opinions and they provide open access summaries of human health, animal health and ecological hazard assessments in their: OpenFoodTox database. The OpenFoodTox database can be used to screen potential new foods for toxicity.

The Toxicology and Environmental Health Information Program (TEHIP) at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to toxins-related resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET), an integrated system of toxicology and environmental health databases that are available free of charge on the web.

TOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs.

One of the bottlenecks in peptide/protein-based therapy is their toxicity. Recently, "in silico" models for predicting toxicity of peptides and proteins, developed by Gajendra Pal Singh Raghava's group, predict toxicity with reasonably good accuracy. The prediction models are based on machine learning technique and quantitative matrix using various properties of peptides. The prediction tool is freely accessible to public in the form of web server.

When used non-technically, the term "toxin" is often applied to any toxic substance, even though the term toxicant would be more appropriate. Toxic substances not directly of biological origin are also termed poisons and many non-technical and lifestyle journalists follow this usage to refer to toxic substances in general.

In the context of quackery and alternative medicine, the term "toxin" is used to refer to any substance alleged to cause ill health. This could range from trace amounts of potentially dangerous pesticides, to supposedly harmful substances produced in the body by intestinal fermentation (auto-intoxication), to food ingredients such as table sugar, monosodium glutamate (MSG), and aspartame.



</doc>
