<doc id="20170" url="https://en.wikipedia.org/wiki?curid=20170" title="MIPS architecture">
MIPS architecture

MIPS (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA) developed by MIPS Computer Systems, now MIPS Technologies, based in the United States.

There are multiple versions of MIPS: including MIPS I, II, III, IV, and V; as well as five releases of MIPS32/64 (for 32- and 64-bit implementations, respectively). The early MIPS architectures were 32-bit only; 64-bit versions were developed later. As of April 2017, the current version of MIPS is MIPS32/64 Release 6. MIPS32/64 primarily differs from MIPS I–V by defining the privileged kernel mode System Control Coprocessor in addition to the user mode architecture.

Computer architecture courses in universities and technical schools often study the MIPS architecture. The architecture greatly influenced later RISC architectures such as Alpha.

As of April 2017, MIPS processors are used in embedded systems such as residential gateways and routers. Originally, MIPS was designed for general-purpose computing. During the 1980s and 1990s, MIPS processors for personal, workstation, and server computers were used by many companies such as Digital Equipment Corporation, MIPS Computer Systems, NEC, Pyramid Technology, SiCortex, Siemens Nixdorf, Silicon Graphics, and Tandem Computers. Historically, video game consoles such as the Nintendo 64, Sony PlayStation, PlayStation 2, and PlayStation Portable used MIPS processors. MIPS processors also used to be popular in supercomputers during the 1990s, but all such systems have dropped off the TOP500 list. These uses were complemented by embedded applications at first, but during the 1990s, MIPS became a major presence in the embedded processor market, and by the 2000s, most MIPS processors were for these applications. In the mid- to late-1990s, it was estimated that one in three RISC microprocessors produced was a MIPS processor.

MIPS is a modular architecture supporting up to four coprocessors (CP0/1/2/3). In MIPS terminology, CP0 is the System Control Coprocessor (an essential part of the processor that is implementation-defined in MIPS I–V), CP1 is an optional floating-point unit (FPU) and CP2/3 are optional implementation-defined coprocessors (MIPS III removed CP3 and reused its opcodes for other purposes). For example, in the PlayStation video game console, CP2 is the Geometry Transformation Engine (GTE), which accelerates the processing of geometry in 3D computer graphics.

The MIPS architecture has several optional extensions. MIPS-3D which is a simple set of floating-point SIMD instructions dedicated to common 3D tasks, MDMX (MaDMaX) which is a more extensive integer SIMD instruction set using the 64-bit floating-point registers, MIPS16e which adds compression to the instruction stream to make programs take up less room, and MIPS MT, which adds multithreading capability.

In December 2018, Wave Computing, the new owner of the MIPS architecture (see MIPS Technologies), announced that MIPS ISA would be open-sourced in a program dubbed the MIPS Open initiative. The program was intended to open up access to the most recent versions of both the 32-bit and 64-bit designs making them available without any licensing or royalty fees as well as granting participants licenses to existing MIPS patents..
In March 2019, one version of the architecture was made available under a royalty-free license, but later that year the program was shut down again .

The first version of the MIPS architecture was designed by MIPS Computer Systems for its R2000 microprocessor, the first MIPS implementation. Both MIPS and the R2000 were introduced together in 1985. When MIPS II was introduced, "MIPS" was renamed "MIPS I" to distinguish it from the new version.

MIPS is a load/store architecture (also known as a "register-register architecture"); except for the load/store instructions used to access memory, all instructions operate on the registers.

MIPS I has thirty-two 32-bit general-purpose registers (GPR). Register $0 is hardwired to zero and writes to it are discarded. Register $31 is the link register. For integer multiplication and division instructions, which run asynchronously from other instructions, a pair of 32-bit registers, "HI" and "LO", are provided. There is a small set of instructions for copying data between the general-purpose registers and the HI/LO registers.

The program counter has 32 bits. The two low-order bits always contain zero since MIPS I instructions are 32 bits long and are aligned to their natural word boundaries.

Instructions are divided into three types: R, I and J. Every instruction starts with a 6-bit opcode. In addition to the opcode, R-type instructions specify three registers, a shift amount field, and a function field; I-type instructions specify two registers and a 16-bit immediate value; J-type instructions follow the opcode with a 26-bit jump target.

The following are the three formats used for the core instruction set:

MIPS I has instructions that load and store 8-bit bytes, 16-bit halfwords, and 32-bit words. Only one addressing mode is supported: base + displacement. Since MIPS I is a 32-bit architecture, loading quantities fewer than 32 bits requires the datum to be either signed- or zero-extended to 32 bits. The load instructions suffixed by "unsigned" perform zero extension; otherwise sign extension is performed. Load instructions source the base from the contents of a GPR (rs) and write the result to another GPR (rt). Store instructions source the base from the contents of a GPR (rs) and the store data from another GPR (rt). All load and store instructions compute the memory address by summing the base with the sign-extended 16-bit immediate. MIPS I requires all memory accesses to be aligned to their natural word boundaries, otherwise an exception is signaled. To support efficient unaligned memory accesses, there are load/store word instructions suffixed by "left" or "right". All load instructions are followed by a load delay slot. The instruction in the load delay slot cannot use the data loaded by the load instruction. The load delay slot can be filled with an instruction that is not dependent on the load; a nop is substituted if such an instruction cannot be found.

MIPS I has instructions to perform addition and subtraction. These instructions source their operands from two GPRs (rs and rt), and write the result to a third GPR (rd). Alternatively, addition can source one of the operands from a 16-bit immediate (which is sign-extended to 32 bits). The instructions for addition and subtraction have two variants: by default, an exception is signaled if the result overflows; instructions with the "unsigned" suffix do not signal an exception. The overflow check interprets the result as a 32-bit two's complement integer.

MIPS I has instructions to perform bitwise logical AND, OR, XOR, and NOR. These instructions source their operands from two GPRs and write the result to a third GPR. The AND, OR, and XOR instructions can alternatively source one of the operands from a 16-bit immediate (which is zero-extended to 32 bits).

The Set on "relation" instructions write one or zero to the destination register if the specified relation is true or false. These instructions source their operands from two GPRs or one GPR and a 16-bit immediate (which is sign-extended to 32 bits), and write the result to a third GPR. By default, the operands are interpreted as signed integers. The variants of these instructions that are suffixed with "unsigned" interpret the operands as unsigned integers (even those that source an operand from the sign-extended 16-bit immediate).

The Load Immediate Upper instruction copies the 16-bit immediate into the high-order 16 bits of a GPR. It is used in conjunction with the Or Immediate instruction to load a 32-bit immediate into a register.

MIPS I has instructions to perform left and right logical shifts and right arithmetic shifts. The operand is obtained from a GPR (rt), and the result is written to another GPR (rd). The shift distance is obtained from either a GPR (rs) or a 5-bit "shift amount" (the "sa" field).

MIPS I has instructions for signed and unsigned integer multiplication and division. These instructions source their operands from two GPRs and write their results to a pair of 32-bit registers called HI and LO, since they may execute separately from (and concurrently with) the other CPU instructions. For multiplication, the high- and low-order halves of the 64-bit product is written to HI and LO (respectively). For division, the quotient is written to LO and the remainder to HI. To access the results, a pair of instructions (Move from HI and Move from LO) is provided to copy the contents of HI or LO to a GPR. These instructions are interlocked: reads of HI and LO do not proceed past an unfinished arithmetic instruction that will write to HI and LO. Another pair of instructions (Move to HI or Move to LO) copies the contents of a GPR to HI and LO. These instructions are used to restore HI and LO to their original state after exception handling. Instructions that read HI or LO must be separated by two instructions that do not write to HI or LO.

All MIPS I control flow instructions are followed by a branch delay slot. Unless the branch delay slot is filled by an instruction performing useful work, an nop is substituted. MIPS I branch instructions compare the contents of a GPR (rs) against zero or another GPR (rt) as signed integers and branch if the specified condition is true. Control is transferred to the address computed by shifting the 16-bit offset left by two bits, sign-extending the 18-bit result, and adding the 32-bit sign-extended result to the sum of the program counter (instruction address) and 8. Jumps have two versions: absolute and register-indirect. Absolute jumps ("Jump" and "Jump and Link") compute the address control is transferred to by shifting the 26-bit instr_index left by two bits and concatenating the 28-bit result with the four high-order bits of the address of the instruction in the branch delay slot. Register-indirect jumps transfer control to the instruction at the address sourced from a GPR (rs). The address sourced from the GPR must be word-aligned, else an exception is signaled after the instruction in the branch delay slot is executed. Branch and jump instructions that link (except for "Jump and Link Register") save the return address to GPR 31. The "Jump and Link Register" instruction permits the return address to be saved to any writable GPR.

MIPS I has two instructions for software to signal an exception: System Call and Breakpoint. System Call is used by user mode software to make kernel calls; and Breakpoint is used to transfer control to a debugger via the kernel's exception handler. Both instructions have a 20-bit Code field that can contain operating environment-specific information for the exception handler.

MIPS has 32 floating-point registers. Two registers are paired for double precision numbers. Odd numbered registers cannot be used for arithmetic or branching, just as part of a double precision register pair, resulting in 16 usable registers for most instructions (moves/copies and loads/stores were not affected).

Single precision is denoted by the .s suffix, while double precision is denoted by the .d suffix.

MIPS II removed the load delay slot and added several sets of instructions. For shared-memory multiprocessing, the "Synchronize Shared Memory", "Load Linked Word", and "Store Conditional Word" instructions were added. A set of Trap-on-Condition instructions were added. These instructions caused an exception if the evaluated condition is true. All existing branch instructions were given "branch-likely" versions that executed the instruction in the branch delay slot only if the branch is taken. These instructions improve performance in certain cases by allowing useful instructions to fill the branch delay slot. Doubleword load and store instructions for COP1–3 were added. Consistent with other memory access instructions, these loads and stores required the doubleword to be naturally aligned.

The instruction set for the floating point coprocessor also had several instructions added to it. An IEEE 754-compliant floating-point square root instruction was added. It supported both single- and double-precision operands. A set of instructions that converted single- and double-precision floating-point numbers to 32-bit words were added. These complemented the existing conversion instructions by allowing the IEEE rounding mode to be specified by the instruction instead of the Floating Point Control and Status Register.

MIPS Computer Systems' R6000 microprocessor (1989) was the first MIPS II implementation. Designed for servers, the R6000 was fabricated and sold by Bipolar Integrated Technology, but was a commercial failure. During the mid-1990s, many new 32-bit MIPS processors for embedded systems were MIPS II implementations because the introduction of the 64-bit MIPS III architecture in 1991 left MIPS II as the newest 32-bit MIPS architecture until MIPS32 was introduced in 1999.A

MIPS III is a backwards-compatible extension of MIPS II that added support for 64-bit memory addressing and integer operations. The 64-bit data type is called a doubleword, and MIPS III extended the general-purpose registers, HI/LO registers, and program counter to 64 bits to support it. New instructions were added to load and store doublewords, to perform integer addition, subtraction, multiplication, division, and shift operations on them, and to move doubleword between the GPRs and HI/LO registers. Existing instructions originally defined to operate on 32-bit words were redefined, where necessary, to sign-extend the 32-bit results to permit words and doublewords to be treated identically by most instructions. Among those instructions redefined was "Load Word". In MIPS III it sign-extends words to 64 bits. To complement "Load Word", a version that zero-extends was added.

The R instruction format's inability to specify the full shift distance for 64-bit shifts (its 5-bit shift amount field is too narrow to specify the shift distance for doublewords) required MIPS III to provide three 64-bit versions of each MIPS I shift instruction. The first version is a 64-bit version of the original shift instructions, used to specify constant shift distances of 0–31 bits. The second version is similar to the first, but adds 32 the shift amount field's value so that constant shift distances of 32–64 bits can be specified. The third version obtains the shift distance from the six low-order bits of a GPR.

MIPS III added a "supervisor" privilege level in between the existing kernel and user privilege levels. This feature only affected the implementation-defined System Control Processor (Coprocessor 0).

MIPS III removed the Coprocessor 3 (CP3) support instructions, and reused its opcodes for the new doubleword instructions. The remaining coprocessors gained instructions to move doublewords between coprocessor registers and the GPRs. The floating general registers (FGRs) were extended to 64 bits and the requirement for instructions to use even-numbered register only was removed. This is incompatible with earlier versions of the architecture; a bit in the floating-point control/status register is used to operate the MIPS III floating-point unit (FPU) in a MIPS I- and II-compatible mode. The floating-point control registers were not extended for compatibility. The only new floating-point instructions added were those to copy doublewords between the CPU and FPU convert single- and double-precision floating-point numbers into doubleword integers and vice versa.

MIPS Computer Systems' R4000 microprocessor (1991) was the first MIPS III implementation. It was designed for use in personal, workstation, and server computers. MIPS Computer Systems aggressively promoted the MIPS architecture and R4000, establishing the Advanced Computing Environment (ACE) consortium to advance its Advanced RISC Computing (ARC) standard, which aimed to establish MIPS as the dominant personal computing platform. ARC found little success in personal computers, but the R4000 (and the R4400 derivative) were widely used in workstation and server computers, especially by its largest user, Silicon Graphics. Other uses of the R4000 included high-end embedded systems and supercomputers.

MIPS III was eventually implemented by a number of embedded microprocessors. Quantum Effect Design's R4600 (1993) and its derivatives was widely used in high-end embedded systems and low-end workstations and servers. MIPS Technologies' R4200 (1994), was designed for embedded systems, laptop, and personal computers. A derivative, the R4300i, fabricated by NEC Electronics, was used in the Nintendo 64 game console. The Nintendo 64, along with the PlayStation, were among the highest volume users of MIPS architecture processors in the mid-1990s.

MIPS IV is the fourth version of the architecture. It is a superset of MIPS III and is compatible with all existing versions of MIPS. MIPS IV was designed to mainly improve floating-point (FP) performance. To improve access to operands, an indexed addressing mode (base + index, both sourced from GPRs) for FP loads and stores was added, as were prefetch instructions for performing memory prefetching and specifying cache hints (these supported both the base + offset and base + index addressing modes).

MIPS IV added several features to improve instruction-level parallelism. To alleviate the bottleneck caused by a single condition bit, seven condition code bits were added to the floating-point control and status register, bringing the total to eight. FP comparison and branch instructions were redefined so they could specify which condition bit was written or read (respectively); and the delay slot in between an FP branch that read the condition bit written to by a prior FP comparison was removed. Support for partial predication was added in the form of conditional move instructions for both GPRs and FPRs; and an implementation could choose between having precise or imprecise exceptions for IEEE 754 traps.

MIPS IV added several new FP arithmetic instructions for both single- and double-precision FPNs: fused-multiply add or subtract, reciprocal, and reciprocal square-root. The FP fused-multiply add or subtract instructions perform either one or two roundings (it is implementation-defined), to exceed or meet IEEE 754 accuracy requirements (respectively). The FP reciprocal and reciprocal square-root instructions do not comply with IEEE 754 accuracy requirements, and produce results that differ from the required accuracy by one or two units of last place (it is implementation defined). These instructions serve applications where instruction latency is more important than accuracy.

The first MIPS IV implementation was the MIPS Technologies R8000 microprocessor chipset (1994). The design of the R8000 began at Silicon Graphics, Inc. and it was only used in high-end workstations and servers for scientific and technical applications where high performance on large floating-point workloads was important. Later implementations were the MIPS Technologies R10000 (1996) and the Quantum Effect Devices R5000 (1996) and RM7000 (1998). The R10000, fabricated and sold by NEC Electronics and Toshiba, and its derivatives were used by NEC, Pyramid Technology, Silicon Graphics, Inc., and Tandem Computers (among others) in workstations, servers, and supercomputers. The R5000 and R7000 found use in high-end embedded systems, personal computers, and low-end workstations and servers. A derivative of the R5000 from Toshiba, the R5900, was used in Sony Computer Entertainment's Emotion Engine, which powered its PlayStation 2 game console.

Announced on October 21, 1996 at the Microprocessor Forum 1996 alongside the MIPS Digital Media Extensions (MDMX) extension, MIPS V was designed to improve the performance of 3D graphics transformations. In the mid-1990s, a major use of non-embedded MIPS microprocessors were graphics workstations from SGI. MIPS V was completed by the integer-only MDMX extension to provide a complete system for improving the performance of 3D graphics applications.

MIPS V implementations were never introduced. On May 12, 1997, SGI announced the "H1" ("Beast") and "H2" ("Capitan") microprocessors. The former was to have been the first MIPS V implementation, and was due to be introduced in the first half of 1999. The "H1" and "H2" projects were later combined and were eventually canceled in 1998. While there have not been any MIPS V implementations, MIPS64 Release 1 (1999) was based on MIPS V and retains all of its features as an optional Coprocessor 1 (FPU) feature called Paired-Single.

MIPS V added a new data type, the Paired Single (PS), which consisted of two single-precision (32-bit) floating-point numbers stored in the existing 64-bit floating-point registers. Variants of existing floating-point instructions for arithmetic, compare and conditional move were added to operate on this data type in a SIMD fashion. New instructions were added for loading, rearranging and converting PS data. It was the first instruction set to exploit floating-point SIMD with existing resources.

When MIPS Technologies was spun-out of Silicon Graphics in 1998, it refocused on the embedded market. Up to MIPS V, each successive version was a strict superset of the previous version, but this property was found to be a problem, and the architecture definition was changed to define a 32-bit and a 64-bit architecture: MIPS32 and MIPS64. Both were introduced in 1999. MIPS32 is based on MIPS II with some additional features from MIPS III, MIPS IV, and MIPS V; MIPS64 is based on MIPS V. NEC, Toshiba and SiByte (later acquired by Broadcom) each obtained licenses for MIPS64 as soon as it was announced. Philips, LSI Logic, IDT, Raza Microelectronics, Inc., Cavium, Loongson Technology and Ingenic Semiconductor have since joined them.

The first release of MIPS32, based on MIPS II, added conditional moves, prefetch instructions, and other features from the R4000 and R5000 families of 64-bit processors. The first release of MIPS64 adds a MIPS32 mode to run 32-bit code. The MUL and MADD (multiply-add) instructions, previously available in some implementations, were added to the MIPS32 and MIPS64 specifications, as were cache control instructions.

Announced on December 6, 2012. Release 4 was skipped because the number four is perceived as unlucky in many Asian cultures.

MIPS32/MIPS64 Release 6 in 2014 added the following:


Removed infrequently used instructions:


Reorganized the instruction encoding, freeing space for future expansions.

The microMIPS32/64 architectures are supersets of the MIPS32 and MIPS64 architectures (respectively) designed to replace the MIPS16e ASE. A disadvantage of MIPS16e is that it requires a mode switch before any of its 16-bit instructions can be processed. microMIPS adds versions of the most-frequently used 32-bit instructions that are encoded as 16-bit instructions. This allows programs to intermix 16- and 32-bit instructions without having to switch modes. microMIPS was introduced alongside of MIPS32/64 Release 3, and each subsequent release of MIPS32/64 has a corresponding microMIPS32/64 version. A processor may implement microMIPS32/64 or both microMIPS32/64 and its corresponding MIPS32/64 subset. Starting with MIPS32/64 Release 6, support for MIPS16e ended, and microMIPS is the only form of code compression in MIPS.

The base MIPS32 and MIPS64 architectures can be supplemented with a number of optional architectural extensions, which are collectively referred to as "application-specific extensions" (ASEs). These ASEs provide features that improve the efficiency and performance of certain workloads, such as digital signal processing.

Enhancements for microcontroller applications. The MCU ASE (application-specific extension) has been developed to extend the interrupt controller support, reduce the interrupt latency and enhance the I/O peripheral control function typically required in microcontroller system designs.


MIPS16 is an Application-Specific Extension for MIPS I through to V designed by LSI Logic and MIPS Technologies, announced on October 21, 1996 alongside its first implementation, the LSI Logic TinyRISC processor. MIPS16 was subsequently licensed by NEC Electronics, Philips Semiconductors, and Toshiba (among others); and implemented as an extension to the MIPS I, II, an III architectures. MIPS16 decreases the size of application by up to 40% by using 16-bit instructions instead of 32-bit instructions' and also improves power efficiency, the instruction cache hit rate, and is equivalent in performance to its base architecture. It is supported by hardware and software development tools from MIPS Technologies and other providers.

MIPS16e is an improved version of MIPS16 first supported by MIPS32 and MIPS64 Release 1.

MIPS16e2 is an improved version of MIPS16 that is supported by MIPS32 and MIPS64 (up to Release 5). Release 6 replaced it with microMIPS.

The DSP ASE is an optional extension to the MIPS32/MIPS64 Release 2 and newer instruction sets which can be used to accelerate a large range of "media" computations—particularly audio and video. The DSP module comprises a set of instructions and state in the integer pipeline and requires minimal additional logic to implement in MIPS processor cores. Revision 2 of the ASE was introduced in the second half of 2006. This revision adds extra instructions to the original ASE, but is otherwise backwards-compatible with it.

Unlike the bulk of the MIPS architecture, it's a fairly irregular set of operations, many chosen for a particular relevance to some key algorithm.

Its main novel features (vs original MIPS32):


To make use of MIPS DSP ASE, you may:

Linux 2.6.12-rc5 starting 2005-05-31 adds support for the DSP ASE. Note that to actually make use of the DSP ASE a toolchain which support this is required. GCC already has support for DSP and DSPr2.

Instruction set extensions designed to accelerate multimedia.


Hardware supported virtualization technology.

Each multi-threaded MIPS core can support up to two VPEs (Virtual Processing Elements) which share a single pipeline as well as other hardware resources. However, since each VPE includes a complete copy of the processor state as seen by the software system, each VPE appears as a complete standalone processor to an SMP Linux operating system. For more fine-grained thread processing applications, each VPE is capable of supporting up to nine TCs allocated across two VPEs. The TCs share a common execution unit but each has its own program counter and core register files so that each can handle a thread from the software.
The MIPS MT architecture also allows the allocation of processor cycles to threads, and sets the relative thread priorities with an optional Quality of Service (QoS) manager block. This enables two prioritization mechanisms that determine the flow of information across the bus. The first mechanism allows the user to prioritize one thread over another. The second mechanism is used to allocate a specified ratio of the cycles to specific threads over time. The combined use of both mechanisms allows effective allocation of bandwidth to the set of threads, and better control of latencies. In real-time systems, system-level determinism is very critical, and the QoS block facilitates improvement of the predictability of a system. Hardware designers of advanced systems may replace the standard QoS block provided by MIPS Technologies with one that is specifically tuned for their application.

Single-threaded microprocessors today waste many cycles while waiting to access memory, considerably limiting system performance. The use of multi-threading masks the effect of memory latency by increasing processor utilization. As one thread stalls, additional threads are instantly fed into the pipeline and executed, resulting in a significant gain in application throughput. Users can allocate dedicated processing bandwidth to real-time tasks resulting in a guaranteed Quality of Service (QoS). MIPS’ MT technology constantly monitors the progress of threads and dynamically takes corrective actions to meet or exceed the real-time requirements. A processor pipeline can achieve 80-90% utilization by switching threads during data-dependent stalls or cache misses. All of this leads to an improved mobile device user experience, as responsiveness is greatly increased. 

SmartMIPS is an Application-Specific Extension (ASE) designed by Gemplus International and MIPS Technologies to improve performance and reduce memory consumption for smart card software. It is supported by MIPS32 only, since smart cards do not require the capabilities of MIPS64 processors. Few smart cards use SmartMIPS.

Open Virtual Platforms (OVP) includes the freely available for non-commercial use simulator OVPsim, a library of models of processors, peripherals and platforms, and APIs which enable users to develop their own models. The models in the library are open source, written in C, and include the MIPS 4K, 24K, 34K, 74K, 1004K, 1074K, M14K, microAptiv, interAptiv, proAptiv 32-bit cores and the MIPS 64-bit 5K range of cores. These models are created and maintained by Imperas and in partnership with MIPS Technologies have been tested and assigned the MIPS-Verified (tm) mark. Sample MIPS-based platforms include both bare metal environments and platforms for booting unmodified Linux binary images. These platforms–emulators are available as source or binaries and are fast, free for non-commercial usage, and are easy to use. OVPsim is developed and maintained by Imperas and is very fast (hundreds of million of instructions per second), and built to handle multicore homogeneous and heterogeneous architectures and systems.

There is a freely available MIPS32 simulator (earlier versions simulated only the R2000/R3000) called SPIM for use in education. EduMIPS64 is a GPL graphical cross-platform MIPS64 CPU simulator, written in Java/Swing. It supports a wide subset of the MIPS64 ISA and allows the user to graphically see what happens in the pipeline when an assembly program is run by the CPU.

MARS is another GUI-based MIPS emulator designed for use in education, specifically for use with Hennessy's "Computer Organization and Design".

WebMIPS is a browser-based MIPS simulator with visual representation of a generic, pipelined processor. This simulator is quite useful for register tracking during step by step execution.

More advanced free emulators are available from the GXemul (formerly known as the mips64emul project) and QEMU projects. These emulate the various MIPS III and IV microprocessors in addition to entire computer systems which use them.

Commercial simulators are available especially for the embedded use of MIPS processors, for example Wind River Simics (MIPS 4Kc and 5Kc, PMC RM9000, QED RM7000, Broadcom/Netlogic ec4400, Cavium Octeon I), Imperas (all MIPS32 and MIPS64 cores), VaST Systems (R3000, R4000), and CoWare (the MIPS4KE, MIPS24K, MIPS25Kf and MIPS34K).

WepSIM is a browser-based simulator where a subset of MIPS instructions are micro-programmed. This simulator is very useful in order to learn how a CPU works (microprogramming, MIPS routines, traps, interruptions, system calls, etc.).





</doc>
<doc id="20171" url="https://en.wikipedia.org/wiki?curid=20171" title="Murder">
Murder

Murder is the unlawful killing of another human without justification or valid excuse, especially the unlawful killing of another human with malice aforethought. This state of mind may, depending upon the jurisdiction, distinguish murder from other forms of unlawful homicide, such as manslaughter. Manslaughter is a killing committed in the absence of "malice", brought about by reasonable provocation, or diminished capacity. "Involuntary" manslaughter, where it is recognized, is a killing that lacks all but the most attenuated guilty intent, recklessness.

Most societies consider murder to be an extremely serious crime, and thus that a person convicted of murder should receive harsh punishments for the purposes of retribution, deterrence, rehabilitation, or incapacitation. In most countries, a person convicted of murder generally faces a long-term prison sentence, possibly a life sentence; and in a few, the death penalty may be imposed.

The modern English word "murder" descends from the Proto-Indo-European "mrtró" which meant "to die". The Middle English "mordre" is a noun from Anglo-Saxon "morðor" and Old French "murdre". Middle English "mordre" is a verb from Anglo-Saxon "myrdrian" and the Middle English noun.

In many countries, in news reports, out of concern for being accused of defamation, journalists are generally careful not to identify a suspect as a murderer until the suspect is convicted of homicide. After arrest, for example, journalists may instead write that the person was "arrested on suspicion of murder", or, after a prosecutor files charges, as an "accused murderer".

The eighteenth-century English jurist William Blackstone (citing Edward Coke), in his "Commentaries on the Laws of England" set out the common law definition of murder, which by this definition occurs
The elements of common law murder are:


The four states of mind recognized as constituting "malice" are:

Under state of mind (i), intent to kill, the "deadly weapon rule" applies. Thus, if the defendant intentionally uses a deadly weapon or instrument against the victim, such use authorizes a permissive inference of intent to kill. In other words, "intent follows the bullet". Examples of deadly weapons and instruments include but are not limited to guns, knives, deadly toxins or chemicals or gases and even vehicles when intentionally used to harm one or more victims.

Under state of mind (iii), an "abandoned and malignant heart", the killing must result from the defendant's conduct involving a reckless indifference to human life and a conscious disregard of an unreasonable risk of death or serious bodily injury. In Australian jurisdictions, the unreasonable risk must amount to a foreseen probability of death (or grievous bodily harm in most states), as opposed to possibility.

Under state of mind (iv), the felony-murder doctrine, the felony committed must be an inherently dangerous felony, such as burglary, arson, rape, robbery or kidnapping. Importantly, the underlying felony "cannot" be a lesser included offense such as assault, otherwise all criminal homicides would be murder as all are felonies.

In Spanish criminal law, murder takes place when any of these requirements concur: Treachery (the use of means to avoid risk for the aggressor or to ensure that the crime goes unpunished), price or reward (financial gain) or viciousness (deliberately increasing the pain of the victim). After the last reform of the Spanish Criminal Code, in force since July 1, 2015, another circumstance that turns homicide into murder is the desire to facilitate the commission of another crime or to prevent it from being discovered.

As with most legal terms, the precise definition of murder varies between jurisdictions and is usually codified in some form of legislation. Even when the legal distinction between murder and manslaughter is clear, it is not unknown for a jury to find a murder defendant guilty of the lesser offense. The jury might sympathize with the defendant (e.g. in a crime of passion, or in the case of a bullied victim who kills their tormentor), and the jury may wish to protect the defendant from a sentence of life imprisonment or execution.
Many jurisdictions divide murder by degrees. The distinction between first- and second-degree murder exists, for example, in Canadian murder law and U.S. murder law.

The most common division is between first- and second-degree murder. Generally, second-degree murder is common law murder, and first-degree is an aggravated form. The aggravating factors of first-degree murder depend on the jurisdiction, but may include a specific intent to kill, premeditation, or deliberation. In some, murder committed by acts such as strangulation, poisoning, or lying in wait are also treated as first-degree murder. A few states in the U.S. further distinguish third-degree murder, but they differ significantly in which kinds of murders they classify as second-degree versus third-degree. For example, Minnesota defines third-degree murder as depraved-heart murder, whereas Florida defines third-degree murder as felony murder (except when the underlying felony is specifically listed in the definition of first-degree murder).

Some jurisdictions also distinguish premeditated murder. This is the crime of wrongfully and intentionally causing the death of another human being (also known as murder) after rationally considering the timing or method of doing so, in order to either increase the likelihood of success, or to evade detection or apprehension. State laws in the United States vary as to definitions of "premeditation". In some states, premeditation may be construed as taking place mere seconds before the murder. Premeditated murder is one of the most serious forms of homicide, and is punished more severely than manslaughter or other types of homicide, often with a life sentence without the possibility of parole, or in some countries, the death penalty. In the U.S, federal law () criminalizes premeditated murder, felony murder and second-degree murder committed under situations where federal jurisdiction applies. In Canada, the Criminal Code classifies murder as either 1st- or 2nd-degree. The former type of murder is often called premeditated murder, although premeditation is not the only way murder can be classified as first-degree.

According to Blackstone, English common law identified murder as a "public wrong". According to common law, murder is considered to be "malum in se", that is an act which is evil within itself. An act such as murder is wrong or evil by its very nature. And it is the very nature of the act which does not require any specific detailing or definition in the law to consider murder a crime.

Some jurisdictions still take a common law view of murder. In such jurisdictions, what is considered to be murder is defined by precedent case law or previous decisions of the courts of law. However, although the common law is by nature flexible and adaptable, in the interests both of certainty and of securing convictions, most common law jurisdictions have codified their criminal law and now have statutory definitions of murder.

Although laws vary by country, there are circumstances of exclusion that are common in many legal systems.


All jurisdictions require that the victim be a natural person; that is, a human being who was still alive before being murdered. In other words, under the law one cannot murder a corpse, a corporation, a non-human animal, or any other non-human organism such as a plant or bacterium.

California's murder statute, Penal Code Section 187, was interpreted by the Supreme Court of California in 1994 as not requiring any proof of the viability of the fetus as a prerequisite to a murder conviction. This holding has two implications. The first is a defendant in California can be convicted of murder for killing a fetus which the mother herself could have terminated without committing a crime. The second, as stated by Justice Stanley Mosk in his dissent, is that because women carrying nonviable fetuses may not be visibly pregnant, it may be possible for a defendant to be convicted of intentionally murdering a person they did not know existed.

Some countries allow conditions that "affect the balance of the mind" to be regarded as mitigating circumstances. This means that a person may be found guilty of "manslaughter" on the basis of "diminished responsibility" rather than being found guilty of murder, if it can be proved that the killer was suffering from a condition that affected their judgment at the time. Depression, post-traumatic stress disorder and medication side-effects are examples of conditions that may be taken into account when assessing responsibility.

Mental disorder may apply to a wide range of disorders including psychosis caused by schizophrenia and dementia, and excuse the person from the need to undergo the stress of a trial as to liability. Usually, sociopathy and other personality disorders are not legally considered insanity, because of the belief they are the result of free will in many societies. In some jurisdictions, following the pre-trial hearing to determine the extent of the disorder, the defense of "not guilty by reason of insanity" may be used to get a not guilty verdict. This defense has two elements:

Under New York law, for example:
Under the French Penal Code:
Those who successfully argue a defense based on a mental disorder are usually referred to mandatory clinical treatment until they are certified safe to be released back into the community, rather than prison. A criminal defendant is often presented with the option of pleading "not guilty by reason of insanity". Thus, a finding of insanity results in a not-guilty verdict, although the defendant is placed in a state treatment facility where they could be kept for years or even decades.

Postpartum depression (also known as post-natal depression) is recognized in some countries as a mitigating factor in cases of infanticide. According to Dr. Susan Friedman, "Two dozen nations have infanticide laws that decrease the penalty for mothers who kill their children of up to one year of age. The United States does not have such a law, but mentally ill mothers may plead not guilty by reason of insanity." In the law of the Republic of Ireland, infanticide was made a separate crime from murder in 1949, applicable for the mother of a baby under one year old where "the balance of her mind was disturbed by reason of her not having fully recovered from the effect of giving birth to the child or by reason of the effect of lactation consequent upon the birth of the child". Since independence, death sentences for murder in such cases had always been commuted; the new act was intended "to eliminate all the terrible ritual of the black cap and the solemn words of the judge pronouncing sentence of death in those cases ... where it is clear to the Court and to everybody, except perhaps the unfortunate accused, that the sentence will never be carried out." In Russia, murder of a newborn child by the mother has been separate crime since 1996.

For a killing to be considered murder in nine out of fifty states in the US, there normally needs to be an element of intent. A defendant may argue that they took precautions not to kill, that the death could not have been anticipated, or was unavoidable. As a general rule, manslaughter constitutes reckless killing, but manslaughter also includes criminally negligent (i.e. grossly negligent) homicide. Unintentional killing that results from an involuntary action generally cannot constitute murder. After examining the evidence, a judge or jury (depending on the jurisdiction) would determine whether the killing was intentional or unintentional.

In those jurisdictions using the Uniform Penal Code, such as California, diminished capacity may be a defense. For example, Dan White used this defence to obtain a manslaughter conviction, instead of murder, in the assassination of Mayor George Moscone and Supervisor Harvey Milk. Afterward, California amended its penal code to provide "As a matter of public policy there shall be no defense of diminished capacity, diminished responsibility, or irresistible impulse in a criminal action..."

Murder with specified aggravating circumstances is often punished more harshly. Depending on the jurisdiction, such circumstances may include:

In the United States and Canada, these murders are referred to as first-degree or aggravated murders. Murder, under English criminal law, always carries a mandatory life sentence, but is not classified into degrees. Penalties for murder committed under aggravating circumstances are often higher, under English law, than the 15-year minimum non-parole period that otherwise serves as a starting point for a murder committed by an adult.

A legal doctrine in some common law jurisdictions broadens the crime of murder: when an offender kills in the commission of a dangerous crime, (regardless of intent), he/she is guilty of murder. The felony murder rule is often justified by its supporters as a means of deterring dangerous felonies, but the case of Ryan Holle shows it can be used very widely.

In some common law jurisdictions, a defendant accused of murder is not guilty if the victim survives for longer than one year and one day after the attack. This reflects the likelihood that if the victim dies, other factors will have contributed to the cause of death, breaking the chain of causation; and also means that the responsible person does not have a charge of murder "hanging over their head indefinitely". Subject to any statute of limitations, the accused could still be charged with an offense reflecting the seriousness of the initial assault.

With advances in modern medicine, most countries have abandoned a fixed time period and test causation on the facts of the case. This is known as "delayed death" and cases where this was applied or was attempted to be applied go back to at least 1966.

In England and Wales, the "year-and-a-day rule" was abolished by the Law Reform (Year and a Day Rule) Act 1996. However, if death occurs three years or more after the original attack then prosecution can take place only with the Attorney-General's approval.

In the United States, many jurisdictions have abolished the rule as well. Abolition of the rule has been accomplished by enactment of statutory criminal codes, which had the effect of displacing the common-law definitions of crimes and corresponding defenses. In 2001 the Supreme Court of the United States held that retroactive application of a state supreme court decision abolishing the year-and-a-day rule did not violate the Ex Post Facto Clause of Article I of the United States Constitution.

The potential effect of fully abolishing the rule can be seen in the case of 74-year-old William Barnes, charged with the murder of a Philadelphia police officer Walter T. Barclay Jr., who he had shot nearly 41 years previously. Barnes had served 16 years in prison for attempting to murder Barkley, but when the policeman died on August 19, 2007, this was alleged to be from complications of the wounds suffered from the shooting – and Barnes was charged with his murder. He was acquitted on May 24, 2010.

Martin Daly and Margo Wilson of McMaster University have claimed that several aspects of homicides, including the genetic relations or proximity between murderers and their victims, (as in the Cinderella effect), can often be explained by the evolution theory or evolutionary psychology.

In the Abrahamic religions, the first ever murder was committed by Cain against his brother Abel out of jealousy. In the past, certain types of homicide were lawful and justified. Georg Oesterdiekhoff wrote:

In many such societies the redress was not via a legal system, but by blood revenge, although there might also be a form of payment that could be made instead—such as the weregild which in early Germanic society could be paid to the victim's family in lieu of their right of revenge.

One of the oldest-known prohibitions against murder appears in the Sumerian Code of Ur-Nammu written sometime between 2100 and 2050 BC. The code states, "If a man commits a murder, that man must be killed."

In Judeo-Christian traditions, the prohibition against murder is one of the Ten Commandments given by God to Moses in (Exodus: 20v13) and (Deuteronomy 5v17). The Vulgate and subsequent early English translations of the Bible used the term "secretly killeth his neighbour" or "smiteth his neighbour secretly" rather than "murder" for the Latin "clam percusserit proximum". Later editions such as Young's Literal Translation and the World English Bible have translated the Latin "occides" simply as "murder" rather than the alternatives of "kill", "assassinate", "fall upon", or "slay".

In Islam according to the Qur'an, one of the greatest sins is to kill a human being who has committed no fault. "For that cause We decreed for the Children of Israel that whosoever killeth a human being for other than manslaughter or corruption in the earth, it shall be as if he had killed all mankind, and whoso saveth the life of one, it shall be as if he had saved the life of all mankind." "And those who cry not unto any other god along with Allah, nor take the life which Allah hath forbidden save in (course of) justice, nor commit adultery – and whoso doeth this shall pay the penalty."

The term "assassin" derives from Hashshashin, a militant Ismaili Shi'ite sect, active from the 8th to 14th centuries. This mystic secret society killed members of the Abbasid, Fatimid, Seljuq and Crusader elite for political and religious reasons. The Thuggee cult that plagued India was devoted to Kali, the goddess of death and destruction. According to some estimates the Thuggees murdered 1 million people between 1740 and 1840. The Aztecs believed that without regular offerings of blood the sun god Huitzilopochtli would withdraw his support for them and destroy the world as they knew it. According to Ross Hassig, author of "Aztec Warfare", "between 10,000 and 80,400 persons" were sacrificed in the 1487 re-consecration of the Great Pyramid of Tenochtitlan.

Southern slave codes did make willful killing of a slave illegal in most cases. For example, the 1860 Mississippi case of "Oliver v. State" charged the defendant with murdering his own slave. In 1811, the wealthy white planter Arthur Hodge was hanged for murdering several of his slaves on his plantation in the British West Indies.

In Corsica, vendetta was a social code that required Corsicans to kill anyone who wronged their family honor. Between 1821 and 1852, no fewer than 4,300 murders were perpetrated in Corsica.

The World Health Organization reported in October 2002 that a person is murdered every 60 seconds. An estimated 520,000 people were murdered in 2000 around the globe. Another study estimated the worldwide murder rate at 456,300 in 2010 with a 35% increase since 1990. Two-fifths of them were young people between the ages of 10 and 29 who were killed by other young people. Because murder is the least likely crime to go unreported, statistics of murder are seen as a bellwether of overall crime rates.

Murder rates vary greatly among countries and societies around the world. In the Western world, murder rates in most countries have declined significantly during the 20th century and are now between 1 and 4 cases per 100,000 people per year. Latin America and the Caribbean, the region with the highest murder rate in the world, experienced more than 2.5 million murders between 2000 and 2017.

Murder rates in jurisdictions such as Japan, Singapore, Hong Kong, Iceland, Switzerland, Italy, Spain and Germany are among the lowest in the world, around 0.3–1 cases per 100,000 people per year; the rate of the United States is among the highest of developed countries, around 4.5 in 2014, with rates in larger cities sometimes over 40 per 100,000. The top ten highest murder rates are in Honduras (91.6 per 100,000), El Salvador, Ivory Coast, Venezuela, Belize, Jamaica, U.S. Virgin Islands, Guatemala, Saint Kitts and Nevis and Zambia. (UNODC, 2011 – ).

The following absolute murder counts per-country are not comparable because they are not adjusted by each country's total population. Nonetheless, they are included here for reference, with 2010 used as the base year (they may or may not include justifiable homicide, depending on the jurisdiction). There were 52,260 murders in Brazil, consecutively elevating the record set in 2009. Over half a million people were shot to death in Brazil between 1979 and 2003. 33,335 murder cases were registered across India, about 19,000 murders committed in Russia, approximately 17,000 murders in Colombia (the murder rate was 38 per 100,000 people, in 2008 murders went down to 15,000), approximately 16,000 murders in South Africa, approximately 15,000 murders in the United States, approximately 26,000 murders in Mexico, approximately 13,000 murders in Venezuela, approximately 4,000 murders in El Salvador, approximately 1,400 murders in Jamaica, approximately 550 murders in Canada and approximately 470 murders in Trinidad and Tobago. Pakistan reported 12,580 murders.

In the United States, 666,160 people were killed between 1960 and 1996. Approximately 90% of murders in the US are committed by males. Between 1976 and 2005, 23.5% of all murder victims and 64.8% of victims murdered by intimate partners were female. For women in the US, homicide is the leading cause of death in the workplace.

In the US, murder is the leading cause of death for African American males aged 15 to 34. Between 1976 and 2008, African Americans were victims of 329,825 homicides. In 2006, Federal Bureau of Investigation's Supplementary Homicide Report indicated that nearly half of the 14,990 murder victims that year were Black (7421). In the year 2007, there were 3,221 black victims and 3,587 white victims of non-negligent homicides. While 2,905 of the black victims were killed by a black offender, 2,918 of the white victims were killed by white offenders. There were 566 white victims of black offenders and 245 black victims of white offenders. The "white" category in the Uniform Crime Reports (UCR) includes non-black Hispanics. In London in 2006, 75% of the victims of gun crime and 79% of the suspects were "from the African/Caribbean community".
Murder demographics are affected by the improvement of trauma care, which has resulted in reduced lethality of violent assaults – thus the murder rate may not necessarily indicate the overall level of social violence.

Workplace homicide, which tripled during the 1980s, is the fastest growing category of murder in America.

Development of murder rates over time in different countries is often used by both supporters and opponents of capital punishment and gun control. Using properly filtered data, it is possible to make the case for or against either of these issues. For example, one could look at murder rates in the United States from 1950 to 2000, and notice that those rates went up sharply shortly after a moratorium on death sentences was effectively imposed in the late 1960s. This fact has been used to argue that capital punishment serves as a deterrent and, as such, it is morally justified. Capital punishment opponents frequently counter that the United States has much higher murder rates than Canada and most European Union countries, although all those countries have abolished the death penalty. Overall, the global pattern is too complex, and on average, the influence of both these factors may not be significant and could be more social, economic, and cultural.

Despite the immense improvements in forensics in the past few decades, the fraction of murders solved has decreased in the United States, from 90% in 1960 to 61% in 2007. Solved murder rates in major U.S. cities varied in 2007 from 36% in Boston, Massachusetts to 76% in San Jose, California. Major factors affecting the arrest rate include witness cooperation and the number of people assigned to investigate the case.

According to scholar Pieter Spierenburg homicide rates per 100,000 in Europe have fallen over the centuries, from 35 per 100,000 in medieval times, to 20 in 1500 AD, 5 in 1700, to below two per 100,000 in 1900.

In the United States, murder rates have been higher and have fluctuated. They fell below 2 per 100,000 by 1900, rose during the first half of the century, dropped in the years following World War II, and bottomed out at 4.0 in 1957 before rising again. The rate stayed in 9 to 10 range most of the period from 1972 to 1994, before falling to 5 in present times. The increase since 1957 would have been even greater if not for the significant improvements in medical techniques and emergency response times, which mean that more and more attempted homicide victims survive. According to one estimate, if the lethality levels of criminal assaults of 1964 still applied in 1993, the country would have seen the murder rate of around 26 per 100,000, almost triple the actually observed rate of 9.5 per 100,000.
A similar, but less pronounced pattern has been seen in major European countries as well. The murder rate in the United Kingdom fell to 1 per 100,000 by the beginning of the 20th century and as low as 0.62 per 100,000 in 1960, and was at 1.28 per 100,000 . The murder rate in France (excluding Corsica) bottomed out after World War II at less than 0.4 per 100,000, quadrupling to 1.6 per 100,000 since then.

The specific factors driving this dynamics in murder rates are complex and not universally agreed upon. Much of the raise in the U.S. murder rate during the first half of the 20th century is generally thought to be attributed to gang violence associated with Prohibition. Since most murders are committed by young males, the near simultaneous low in the murder rates of major developed countries circa 1960 can be attributed to low birth rates during the Great Depression and World War II. Causes of further moves are more controversial. Some of the more exotic factors claimed to affect murder rates include the availability of abortion and the likelihood of chronic exposure to lead during childhood (due to the use of leaded paint in houses and tetraethyllead as a gasoline additive in internal combustion engines).

The success rate of criminal investigations into murders (the clearance rate) tends to be relatively high for murder compared to other crimes, due to its seriousness. In the United States, the clearance rate was 62.6% in 2004.





</doc>
<doc id="20174" url="https://en.wikipedia.org/wiki?curid=20174" title="Mariner program">
Mariner program

The Mariner program was a 10-mission program conducted by the American space agency NASA in conjunction with Jet Propulsion Laboratory (JPL). The program launched a series of robotic interplanetary probes, from 1962 to 1973, designed to investigate Mars, Venus and Mercury. The program included a number of firsts, including the first planetary flyby, the first planetary orbiter, and the first gravity assist maneuver.

Of the ten vehicles in the Mariner series, seven were successful, forming the starting point for many subsequent NASA/JPL space probe programs. The planned Mariner Jupiter-Saturn vehicles were adapted into the Voyager program, while the Viking program orbiters were enlarged versions of the Mariner 9 spacecraft. Later Mariner-based spacecraft include the Magellan probe and the Galileo probe, while the second-generation Mariner Mark II series evolved into the Cassini–Huygens probe.

The total cost of the Mariner program was approximately $554 million.

The name of the Mariner program was decided in "May 1960-at the suggestion of Edgar M. Cortright" to have the "planetary mission probes ... patterned after nautical terms, to convey 'the impression of travel to great distances and remote lands.'" That "decision was the basis for naming Mariner, Ranger, Surveyor, and Viking probes."

All Mariner spacecraft were based on a hexagonal or octagonal "bus", which housed all of the electronics, and to which all components were attached, such as antennae, cameras, propulsion, and power sources. Mariner 2 was based on the Ranger Lunar probe. All of the Mariners launched after Mariner 2 had four solar panels for power, except for Mariner 10, which had two. Additionally, all except Mariner 1, Mariner 2 and Mariner 5 had TV cameras.

The first five Mariners were launched on Atlas-Agena rockets, while the last five used the Atlas-Centaur. All Mariner-based probes after Mariner 10 used the Titan IIIE, Titan IV unmanned rockets or the Space Shuttle with a solid-fueled Inertial Upper Stage and multiple planetary flybys.

Mariner 1 (P-37) and Mariner 2 (P-38) were two deep-space probes making up NASA's Mariner-R project. The primary goal of the project was to develop and launch two spacecraft sequentially to the near vicinity of Venus, receive communications from the spacecraft and to perform radiometric temperature measurements of the planet. A secondary objective was to make interplanetary magnetic field and/or particle measurements on the way to, and in the vicinity of, Venus. Mariner 1 (designated Mariner R-1) was launched on July 22, 1962, but was destroyed approximately 5 minutes after liftoff by the Air Force Range Safety Officer when its malfunctioning Atlas-Agena rocket went off course. Mariner 2 (designated Mariner R-2) was launched on August 27, 1962, sending it on a 3½-month flight to Venus. The mission was a success, and Mariner 2 became the first spacecraft to have flown by another planet.
Status:

 Sisterships Mariner 3 and Mariner 4 were Mars flyby missions.
Mariner 3 was lost when the launch vehicle's nose fairing failed to jettison.

Mariner 4, launched on November 28, 1964, was the first successful flyby of the planet Mars and gave the first glimpse of Mars at close range.
Status:

The Mariner 5 spacecraft was launched to Venus on June 14, 1967 and arrived in the vicinity of the planet in October 1967. It carried a complement of experiments to probe Venus' atmosphere with radio waves, scan its brightness in ultraviolet light, and sample the solar particles and magnetic field fluctuations above the planet.
Status:
Mariner 5 – Defunct. Now in Heliocentric orbit.

Mariners 6 and 7 were identical teammates in a two-spacecraft mission to Mars. Mariner 6 was launched on February 24, 1969, followed by Mariner 7 on March 21, 1969. They flew over the equator and southern hemisphere of the planet Mars. 

Status: Both Mariner 6 and Mariner 7 are now defunct and are in a Heliocentric orbit.

Mariner 8 and Mariner 9 were identical sister craft designed to map the Martian surface simultaneously, but Mariner 8 was lost in a launch vehicle failure. Mariner 9 was launched in May 1971 and became the first artificial satellite of Mars. It entered Martian orbit in November 1971 and began photographing the surface and analyzing the atmosphere with its infrared and ultraviolet instruments. 
Status:

The Mariner 10 spacecraft launched on November 3, 1973 and was the first to use a gravity assist trajectory, accelerating as it entered the gravitational influence of Venus, then being flung by the planet's gravity onto a slightly different course to reach Mercury. It was also the first spacecraft to encounter two planets at close range, and for 33 years the only spacecraft to photograph Mercury in closeup.
Status: Mariner 10 – Defunct. Now in a Heliocentric orbit.

Mariner Jupiter-Saturn was approved in 1972 after the cancellation of the Grand Tour program, which proposed visiting all the outer planets with multiple spacecraft. The Mariner Jupiter-Saturn program proposed two Mariner-derived probes that would perform a scaled back mission involving flybys of only the two gas giants, though designers at JPL built the craft with the intention that further encounters past Saturn would be an option. Trajectories were chosen to allow one probe to visit Jupiter and Saturn first, and perform a flyby of Saturn's moon Titan to gather information about the moon's substantial atmosphere. The other probe would arrive at Jupiter and Saturn later, and its trajectory would enable it to continue on to Uranus and Neptune assuming the first probe accomplished all its objectives, or be redirected to perform a Titan flyby if necessary. The program's name was changed to Voyager just before launch in 1977, and after Voyager 1 successfully completed its Titan encounter, Voyager 2 went on to visit the two ice giants.



</doc>
<doc id="20175" url="https://en.wikipedia.org/wiki?curid=20175" title="Mariner 4">
Mariner 4

Mariner 4 (together with Mariner 3 known as Mariner–Mars 1964) was the fourth in a series of spacecraft intended for planetary exploration in a flyby mode. It was designed to conduct closeup scientific observations of Mars and to transmit these observations to Earth. Launched on November 28, 1964, Mariner 4 performed the first successful flyby of the planet Mars, returning the first close-up pictures of the Martian surface. It captured the first images of another planet ever returned from deep space; their depiction of a cratered, seemingly dead world, largely changed the scientific community's view of life on Mars. Other mission objectives were to perform field and particle measurements in interplanetary space in the vicinity of Mars and to provide experience in and knowledge of the engineering capabilities for interplanetary flights of long duration. On December 21, 1967, communications with Mariner 4 were terminated.

The Mariner 4 spacecraft consisted of an octagonal magnesium frame, 127 cm across a diagonal and 45.7 cm high. Four solar panels were attached to the top of the frame with an end-to-end span of 6.88 meters, including solar pressure vanes which extended from the ends. A 116.8 cm diameter high-gain parabolic antenna was mounted at the top of the frame as well. An omnidirectional low-gain antenna was mounted on a seven-foot, four inch (223.5 cm) tall mast next to the high-gain antenna. The overall height of the spacecraft was 2.89 meters. The octagonal frame housed the electronic equipment, cabling, midcourse propulsion system, and attitude control gas supplies and regulators.

The scientific instruments included:
The electric power for the instruments and the radio transmitter of Mariner 4 was supplied by 28,224 solar cells contained in the four 176 x 90 cm solar panels, which could provide 310 watts at the distance of Mars. A rechargeable 1200 W·h silver-zinc battery was also used for maneuvers and backup. Monopropellant hydrazine was used for propulsion, via a four-jet vane vector control motor, with thrust, installed on one of the sides of the octagonal structure. The space probe's attitude control was provided by 12 cold nitrogen gas jets mounted on the ends of the solar panels and three gyros. Solar pressure vanes, each with an area of 0.65 square meter (seven ft²), were attached to the tips of the solar panels. Positional information was provided by four Sun sensors, and a sensor for either the Earth, Mars, or the star Canopus, depending on the time in its spaceflight. Mariner 4 was the first space probe that needed a star for a navigational reference object, since earlier missions, which remained near either the Earth, the Moon, or the planet Venus, had sighted onto either the bright face of the home planet or the brightly lit target. During this flight, both the Earth and Mars would be too dim to lock onto. Another bright source at a wide angle away from the Sun was needed and Canopus filled this requirement. Subsequently, Canopus was used as a reference point in many following missions.

The telecommunications equipment on Mariner 4 consisted of dual S-band transmitters (with either a seven-watt triode cavity amplifier or a ten watt traveling-wave tube amplifier) and a single radio receiver which together could send and receive data via the low- and high-gain antennas at 8⅓ or 33⅓ bits per second. Data could also be stored onto a magnetic tape recorder with a capacity of 5.24 million bits for later transmission. All electronic operations were controlled by a command subsystem which could process any of 29 direct command words or three quantitative word commands for mid-course maneuvers. The central computer and sequencer operated stored time-sequence commands using a 38.4 kHz synchronization frequency as a time reference. Temperature control was achieved through the use of adjustable louvers mounted on six of the electronics assemblies, plus multilayer insulating blankets, polished aluminum shields, and surface treatments. Other measurements that could be made included:

After Mariner 3 was a total loss due to failure of the payload shroud to jettison, JPL engineers suggested that there had been a malfunction caused during separation of the metal fairing exterior from the fiberglass inner lining due to pressure differences between the inner and outer part of the shroud and that this could have caused the spring-loaded separation mechanism to become tangled and fail to detach properly.

Testing at JPL confirmed this failure mode and an effort was made to develop a new, all-metal fairing. The downside of this was that the new fairing would be significantly heavier and reduce the Atlas-Agena's lift capacity. Convair and Lockheed-Martin had to make several performance enhancements to the booster to wring more power out of it. Despite fears that the work could not be completed before the 1964 Mars window closed, the new shroud was ready by November.

After launch from Cape Canaveral Air Force Station Launch Complex 12, the protective shroud covering Mariner 4 was jettisoned and the Agena-D/Mariner 4 combination separated from the Atlas-D booster at 14:27:23 UTC on November 28, 1964. The Agena's first burn took place from 14:28:14 to 14:30:38. The initial burn put the spacecraft into an Earth parking orbit and the second burn from 15:02:53 to 15:04:28 injected the craft into a Mars transfer orbit. Mariner 4 separated from the Agena at 15:07:09 and began cruise mode operations. The solar panels deployed and the scan platform was unlatched at 15:15:00. Sun acquisition occurred 16 minutes later.

After Sun acquisition, the Canopus star tracker went searching for Canopus. The star tracker was set to respond to any object more than one-eighth as, and less than eight times as bright as Canopus. Including Canopus, there were seven such objects visible to the sensor. It took more than a day of "star-hopping" to find Canopus, as the sensor locked on to other stars instead: a stray light pattern from the near Earth, Alderamin, Regulus, Naos, and Gamma Velorum were acquired before Canopus.

A consistent problem that plagued the spacecraft during the early portion of its mission was that roll error signal transients would occur frequently and on occasion would cause loss of the Canopus star lock. The first attempt at a midcourse maneuver was aborted by a loss of lock shortly after the gyros began spinup. Canopus lock was lost six times within a period of less than three weeks after launch and each time a sequence of radio commands would be required to reacquire the star. After a study of the problem, the investigators concluded that the behavior was due to small dust particles that were being released from the spacecraft by some means and were drifting through the star sensor field-of-view. Sunlight scattered from the particles then appeared as illumination equivalent to that from a bright star. This would cause a roll error transient as the object passed through the field-of-view while the sensor was locked onto Canopus. When the object was bright enough that it exceeded the high gate limits at eight times the Canopus intensity, the spacecraft would automatically disacquire Canopus and initiate a roll search for a new star. Finally, a radio command was sent on December 17, 1964, that removed the high gate limit. There was no further loss of Canopus lock, although roll transients occurred 38 more times before encounter with Mars.

The 7½ month flight of Mariner 4 involved one midcourse maneuver on December 5, 1964. The maneuver was initially scheduled for December 4, but due to a loss of lock with Canopus, it was postponed. The maneuver was successfully completed on December 5; it consisted of a negative pitch turn of 39.16 degrees, a positive roll turn of 156.08 degrees, and a thrusting time of 20.07 seconds. The turns aimed the motor of the spacecraft back in the general direction of Earth, as the motor was initially pointed along the direction of flight. Both the pitch and roll changes were completed with better than 1% accuracy, the velocity change with about 2.5% accuracy. After the maneuver, Mariner 4 was on course for Mars as planned.

On January 5, 1965, 36 days after launch and 10,261,173 km from Earth, Mariner 4 reduced its rate of transmission of scientific data from 33 1/3 to 8 1/2 bits per second. This was the first autonomous action the spacecraft had taken since the midcourse maneuver. 

The Mariner 4 spacecraft flew by Mars on July 14 and July 15, 1965. Its closest approach was 9,846 km from the Martian surface at 01:00:57 UT July 15, 1965 (8:00:57 p.m. EST July 14), its distance to Earth was 216  million km, its speed was 7 km/s relative to Mars, 1.7 km/s relative to Earth.

Planetary science mode was turned on at 15:41:49 UT on July 14. The camera sequence started at 00:18:36 UT on July 15 (7:18:49 p.m. EST on July 14) and 21 pictures using alternate red and green filters, plus 21 lines of a 22nd picture were taken. The images covered a discontinuous swath of Mars starting near 40° N, 170° E, down to about 35° S, 200° E, and then across to the terminator at 50° S, 255° E, representing about 1% of the planet's surface. The images taken during the flyby were stored in the on-board tape recorder. At 02:19:11 UT Mariner 4 passed behind Mars as seen from Earth and the radio signal ceased. The signal was reacquired at 03:13:04 UT when the spacecraft reappeared. Cruise mode was then re-established. Transmission of the taped images to Earth began about 8.5 hours after signal reacquisition and continued until August 3. All images were transmitted twice to ensure no data was missing or corrupt. Each individual photograph took approximately six hours to be transmitted back to Earth.

The spacecraft performed all programmed activities successfully and returned useful data from launch until 22:05:07 UT on October 1, 1965, when the long distance to Earth (309.2 million km) and the imprecise antenna orientation led to a temporary loss of communication with the spacecraft until 1967.

The on-board tape recorder used on Mariner 4 was a spare, not originally intended for the Mariner 4 flight. Between the failure of Mariner 3, the fact that the Mariner 4 recorder was a spare, and some error readings suggesting an issue with the tape-recorder, it was determined that the team would test the camera function definitively. This eventually led to the first digital image being hand-drawn. While waiting for the image data to be computer processed, the team used a pastel set from an art supply store to hand-color (paint-by-numbers style) a numerical printout of the raw pixels. The resulting image provided early verification that the camera was functioning. The hand-drawn image compared favorably with the processed image when it became available.
Data acquisition resumed in late 1967. The cosmic dust detector registered 17 hits in a 15-minute span on September 15, part of an apparent micrometeoroid shower that temporarily changed the spacecraft attitude and probably slightly damaged its thermal shield. Later it was speculated that Mariner 4 passed through debris of D/1895 Q1 (D/Swift), and even made a flyby of that comet's possibly shattered nucleus at 20 million kilometers.

On December 7 the gas supply in the attitude control system was exhausted, and between December 10 and 11, a total of 83 micrometeoroid hits were recorded which caused perturbation of the spacecraft's attitude and degradation of the signal strength. On December 21, 1967, communications with Mariner 4 were terminated. The spacecraft is now derelict in an exterior heliocentric orbit.

The total data returned by the mission was 5.2 million bits (about 634 kB). All instruments operated successfully with the exception of a part of the ionization chamber, namely the Geiger–Müller tube, which failed in February 1965. In addition, the plasma probe had its performance degraded by a resistor failure on December 8, 1964, but experimenters were able to recalibrate the instrument and still interpret the data. The images returned showed a Moon-like cratered terrain, which scientists did not expect, although amateur astronomer Donald Cyr had predicted craters. Later missions showed that the craters were not typical for Mars, but only for the more ancient region imaged by Mariner 4. A surface atmospheric pressure of 4.1 to 7.0 millibars (410 to 700 pascals) and daytime temperatures of −100°C were estimated. No magnetic field or Martian radiation belts or, again surprisingly, surface water was detected.

Bruce C. Murray used photographs from Mariner 4 to elucidate Mars' geologic history.

Images of craters and measurements of a thin atmosphere—much thinner than expected—indicating a relatively inactive planet exposed to the harshness of space, generally dissipated hopes of finding intelligent life on Mars. Life on Mars had been the subject of speculation and science fiction for centuries. If there was life on Mars, after Mariner 4 most concluded it would probably be smaller, simpler forms. Others concluded that a search for life on Earth at kilometer resolution, using several thousand photographs, did not reveal a sign of life on the vast majority of these photographs; thus, based on the 22 photographs taken by Mariner 4, one could not conclude there was no intelligent life on Mars.
The solar wind was measured, and compared with simultaneous records from Mariner 5 which went to Venus.

The total cost of the Mariner 4 mission is estimated at $83.2 million. Total research, development, launch, and support costs for the Mariner series of spacecraft (Mariners 1 through 10) was approximately $554 million.




</doc>
<doc id="20178" url="https://en.wikipedia.org/wiki?curid=20178" title="MOO (programming language)">
MOO (programming language)

The MOO programming language is a relatively simple programming language used to support the MOO Server. It is dynamically typed and uses a prototype-based object-oriented system, with syntax roughly derived from the ADA school of programming languages.

Stephen White authored the first MOO Server and language in 1990 using C. Over the course of the year, Pavel Curtis joined the project, releasing the first version of the LambdaMOO Server. LambdaMOO is run and maintained entirely on a volunteer basis, and now has its own SourceForge project. Although the last packaged release was in 2000, development is still active in the project's CVS. 

White describes MOO as "a mishmash of c-like operators and ada-like control structures, combined with prototype-style single-inheritance."

The language has explicit exception handling control flow, as well as traditional looping constructs. A verb and property hierarchy provides default values to prototype objects, with over-riding values lower in the hierarchy. This hierarchy of objects is maintained through delegation to an object's "parent" property, resulting in a form of single inheritance. Special security-related attributes of objects, verbs, and properties include ownership, and read, write and execute flags. MOO programs are byte-code compiled, with implicit decompilation when editing, providing a canonical form of programs.

MOO programs are orthogonally persistent through periodic checkpoints. Objects are identified by a unique integer identifier. Unused program data is eliminated through automatic garbage collection (implemented by reference counting). However, MOO objects themselves are not garbage collected and are manually deleted by their owners or superusers (aka wizards) through a process called 'recycling.'

MOO is explicitly a multi-user system and programs (verbs) are contributed by any number of connected users. A distinction is made between the 'driver' (runtime) and 'core' (programs written in the MOO language.) The vast majority of the functionality of a running MOO is handled 'in-core.'

The runtime supports multi-tasking using a retribution based time slicing method. Verbs run with exclusive access to the database, so no explicit locking is necessary to maintain synchronization. Simple TCP/IP messaging (telnet compatible) is used to communicate with client sockets, each of which is identified with a 'player' in the Virtual reality representation system.

The language supports weak references to objects by number, and to properties and verbs through strings. Built-in functions to retrieve lists of properties and verbs exist, giving the language runtime facilities for reflection. The server also contains support for wildcard verb matching, so the same code can easily be used to handle multiple commands with similar names and functions.

Available sequence types in MOO are lists and strings. Both support random access, as well as head and tail operations similar to those available in Lisp. All operations on lists and strings are non-destructive, and all non-object datatypes are immutable. Built-in functions and libraries allow lists to also be used as associative arrays and ordered and unordered sets.

MOO has a very basic set of control structures, with for-in-list being the only "fancy" feature.

The classic Hello World Program can be written in MOO as:

A more interesting example:




</doc>
<doc id="20180" url="https://en.wikipedia.org/wiki?curid=20180" title="Musical ensemble">
Musical ensemble

A musical ensemble, also known as a music group or musical group, is a group of people who perform instrumental or vocal music, with the ensemble typically known by a distinct name. Some music ensembles consist solely of instruments, such as the jazz quartet or the orchestra. Some music ensembles consist solely of singers, such as choirs and doo wop groups. In both popular music and classical music, there are ensembles in which both instrumentalists and singers perform, such as the rock band or the Baroque chamber group for basso continuo (harpsichord and cello) and one or more singers. In classical music, trios or quartets either blend the sounds of musical instrument families (such as piano, strings, and wind instruments) or group together instruments from the same instrument family, such as string ensembles (e.g., string quartet) or wind ensembles (e.g., wind quintet). Some ensembles blend the sounds of a variety of instrument families, such as the orchestra, which uses a string section, brass instruments, woodwinds and percussion instruments, or the concert band, which uses brass, woodwinds and percussion. 

In jazz ensembles or combos, the instruments typically include wind instruments (one or more saxophones, trumpets, etc.), one or two chordal "comping" instruments (electric guitar, piano, or Hammond organ), a bass instrument (bass guitar or double bass), and a drummer or percussionist. Jazz ensembles may be solely instrumental, or they may consist of a group of instruments accompanying one or more singers. In rock and pop ensembles, usually called rock bands or pop bands, there are usually guitars and keyboards (piano, electric piano, Hammond organ, synthesizer, etc.), one or more singers, and a rhythm section made up of a bass guitar and drum kit.

Music ensembles typically have a leader. In jazz bands, rock and pop groups and similar ensembles, this is the band leader. In classical music, orchestras, concert bands and choirs are led by a conductor. In orchestra, the concertmaster (principal first violin player) is the instrumentalist leader of the orchestra. In orchestras, the individual sections also have leaders, typically called the "principal" of the section (e.g., the leader of the viola section is called the "principal viola"). Conductors are also used in jazz big bands and in some very large rock or pop ensembles (e.g., a rock concert that includes a string section, a horn section and a choir which are accompanying a rock band's performance).

In Western classical music, smaller ensembles are called chamber music ensembles. The terms duet, trio, quartet, quintet, sextet, septet, octet, nonet and decet describe groups of two up to ten musicians, respectively. A group of eleven musicians, such as found in "The Carnival of the Animals", is called either a "hendecet" or an "undecet", and a group of twelve is called a "duodecet" (see Latin numerical prefixes). A soloist playing unaccompanied (e.g., a pianist playing a solo piano piece or a cellist playing a Bach suite for unaccompanied cello) is not an ensemble because it only contains one musician.

A string quartet consists of two violins, a viola and a cello. There is a vast body of music written for string quartets, as it is seen as an important genre in classical music.

A woodwind quartet usually features a flute, an oboe, a clarinet and a bassoon. A brass quartet features two trumpets, a trombone and a tuba. A saxophone quartet consists of a soprano saxophone, an alto saxophone, a tenor saxophone, and a baritone saxophone.

The string "quintet" is a common type of group. It is similar to the string quartet, but with an additional viola, cello, or more rarely, the addition of a double bass. Terms such as "piano quintet" or "clarinet quintet" frequently refer to a string quartet "plus" a fifth instrument. Mozart's Clarinet Quintet is similarly a piece written for an ensemble consisting of two violins, a viola, a cello and a clarinet, the last being the exceptional addition to a "normal" string quartet.

Some other quintets in classical music are the wind quintet, usually consisting of flute, oboe, clarinet, bassoon and horn; the brass quintet, consisting of two trumpets, one horn, a trombone and a tuba; and the reed quintet, consisting of an oboe, a soprano clarinet, a saxophone, a bass clarinet, and a bassoon.

Classical chamber ensembles of six (sextet), seven (septet), or eight musicians (octet) are fairly common; use of latinate terms for larger groups is rare, except for the nonet (nine musicians). In most cases, a larger classical group is referred to as an orchestra of some type or a concert band. A small orchestra with fifteen to thirty members (violins, violas, four cellos, two or three double basses, and several woodwind or brass instruments) is called a chamber orchestra. A sinfonietta usually denotes a somewhat smaller orchestra (though still not a chamber orchestra). Larger orchestras are called symphony orchestras (see below) or philharmonic orchestras.

A pops orchestra is an orchestra that mainly performs light classical music (often in abbreviated, simplified arrangements) and orchestral arrangements and medleys of popular jazz, music theater, or pop music songs. A string orchestra has only string instruments, i.e., violins, violas, cellos and double basses.

A symphony orchestra is an ensemble usually comprising at least thirty musicians; the number of players is typically between fifty and ninety-five and may exceed one hundred. A symphony orchestra is divided into families of instruments. In the string family, there are sections of violins (I and II), violas, cellos (often eight), and basses (often from six to eight). The standard woodwind section consists of flutes (one doubling piccolo), oboes (one doubling English horn), soprano clarinets (one doubling bass clarinet), and bassoons (one doubling contrabassoon). The standard brass section consists of horns, trumpets, trombones, and tuba. The percussion section includes the timpani, bass drum, snare drum, and any other percussion instruments called for in a score (e.g., triangle, glockenspiel, chimes, cymbals, wood blocks, etc.). In Baroque music (1600–1750) and music from the early Classical period music (1750–1820), the percussion parts in orchestral works may only include timpani.

A concert band is a large classical ensemble generally made up of between 40 and 70 musicians from the woodwind, brass, and percussion families, along with the double bass. The concert band has a larger number and variety of wind instruments than the symphony orchestra, but does not have a string section (although a single double bass is common in concert bands). The woodwind section of a concert band consists of piccolo, flutes, oboes (one doubling English horn), bassoons (one doubling contrabassoon), soprano clarinets (one doubling E clarinet, one doubling alto clarinet), bass clarinets (one doubling contrabass clarinet or contra-alto clarinet), alto saxophones (one doubling soprano saxophone), tenor saxophone, and baritone saxophone. The brass section consists of horns, trumpets or cornets, trombones, euphoniums, and tubas. The percussion section consists of the timpani, bass drum, snare drum, and any other percussion instruments called for in a score (e.g., triangle, glockenspiel, chimes, cymbals, wood blocks, etc.).

When orchestras perform baroque music (from the 17th century and early 18th century), they may also use a harpsichord or pipe organ, playing the continuo part. When orchestras perform Romantic-era music (from the 19th century), they may also use harps or unusual instruments such as the wind machine or cannons. When orchestras perform music from the 20th century or the 21st century, occasionally instruments such as electric guitar, theremin, or even an electronic synthesizer may be used.

In jazz, there are several types of trios. One type of jazz trio is formed with a piano player, a bass player and a drummer. Another type of jazz trio that became popular in the 1950s and 1960s is the organ trio, which is composed of a Hammond organ player, a drummer, and a third instrumentalist (either a saxophone player or an electric jazz guitarist). In organ trios, the Hammond organ player performs the bass line on the organ bass pedals while simultaneously playing chords or lead lines on the keyboard manuals. Other types of trios include the "drummer-less" trio, which consists of a piano player, a double bassist, and a horn (saxophone or trumpet) or guitar player; and the jazz trio with a horn player (saxophone or trumpet), double bass player, and a drummer. In the latter type of trio, the lack of a chordal instrument means that the horn player and the bassist have to imply the changing harmonies with their improvised lines.

Jazz quartets typically add a "horn" (the generic jazz name for saxophones, trombones, trumpets, or any other wind or brass instrument commonly associated with jazz) to one of the jazz trios described above. Slightly larger jazz ensembles, such as quintets (five instruments) or sextets (six instruments) typically add other soloing instruments to the basic quartet formation, such as different types of saxophones (e.g., alto saxophone, tenor saxophone, etc.) or an additional chordal instrument.

The lineup of larger jazz ensembles can vary considerably, depending on the style of jazz being performed. In a 1920s-style dixieland jazz band, a larger ensemble would be formed by adding a banjo player, woodwind instruments, as with the clarinet, or additional horns (saxophones, trumpets, trombones) to one of the smaller groups. In a 1930s-style Swing big band, a larger ensemble is formed by adding "sections" of like instruments, such as a saxophone section, a trumpet section and a trombone section, which perform arranged "horn lines" to accompany the ensemble. Some Swing bands also added a string section for a lush sound. In a 1970s-style jazz fusion ensemble, a larger ensemble is often formed by adding additional percussionists or sometimes a saxophone player, who would "double" or "triple" (meaning that they would also be proficient at the clarinet, flute or both). Larger jazz ensembles are also formed by the addition of other soloing instruments.

Two-member rock and pop bands are relatively rare, because of the difficulty in providing all of the musical elements which are part of the rock or pop sound (vocals, chords, bass lines, and percussion or drumming). Two-member rock and pop bands typically omit one of these musical elements. In many cases, two-member bands will omit a drummer, since guitars, bass guitars, and keyboards can all be used to provide a rhythmic pulse.

Examples of two-member bands are The Carpenters, The Summer Obsession, Japandroids, They Might Be Giants, Local H, Pet Shop Boys, Hella, Flight of the Conchords, Death from Above 1979, Francis Xavier, I Set My Friends on Fire, Middle Class Rut, The Pity Party, Little Fish, The White Stripes, Big Business, Two Gallants, Lightning Bolt, The Ting Tings, The Black Box Revelation, Satyricon, The Black Keys, Twenty One Pilots, Tenacious D, Simon and Garfunkel, Hall & Oates, Johnossi, The Pack A.D., Air Supply and Royal Blood.

When electronic sequencers became widely available in the 1980s, this made it easier for two-member bands to add in musical elements that the two band members were not able to perform. Sequencers allowed bands to pre-program some elements of their performance, such as an electronic drum part and a synth-bass line. Two-member pop music bands such as Soft Cell, Blancmange, Yazoo and Erasure used pre-programmed sequencers.

W.A.S.P. guitarist Doug Blair is also known for his work in the two-piece progressive rock band signal2noise, where he acts as the lead guitarist and bassist at the same time, thanks to a special custom instrument he invented (an electric guitar with five regular guitar strings paired with three bass guitar strings). Heisenflei of Los Angeles duo The Pity Party plays drums, keyboards, and sings simultaneously. Providence-based Lightning Bolt is a two-member band. Bassist Brian Gibson augments his playing with delay pedals, pitch shifters, looping devices and other pedals, occasionally creating harmony. Local H, Blood Red Shoes, PS I Love You, The Redmond Barry's and Warship are other prominent two-person experimental rock bands.

The smallest ensemble that is commonly used in rock music is the trio format. In a hard rock or blues-rock band, or heavy metal rock group, a "power trio" format is often used, which consists of an electric guitar player, an electric bass guitar player and a drummer, and typically one or more of these musicians also sing (sometimes all three members will sing, e.g. Bee Gees or Alkaline Trio). Some well-known power trios with the guitarist on lead vocals are The Jimi Hendrix Experience, Stevie Ray Vaughan and Double Trouble, Nirvana, Violent Femmes, Gov't Mule, Green Day, The Minutemen, Triumph, Shellac, Sublime, Chevelle, Muse, The Jam, Short Stack, and ZZ Top.

A handful of others with the bassist on vocals include Primus, Motörhead, The Police, The Melvins, MxPx, Blue Cheer, Rush, The Presidents of the United States of America, Venom, and Cream. Some power trios feature two lead vocalists. For example, in the band blink-182 vocals are split between bassist Mark Hoppus and guitarist Tom DeLonge, or in the band Dinosaur Jr., guitarist J. Mascis is the primary songwriter and vocalist, but bassist Lou Barlow writes some songs and sings as well.

An alternative to the power trio are organ trios formed with an electric guitarist, a drummer and a keyboardist. Although organ trios are most commonly associated with 1950s and 1960s jazz organ trio groups such as those led by organist Jimmy Smith, there are also organ trios in rock-oriented styles, such as jazz-rock fusion and Grateful Dead-influenced jam bands such as Medeski Martin & Wood. In organ trios, the keyboard player typically plays a Hammond organ or similar instrument, which permits the keyboard player to perform bass lines, chords, and lead lines, one example being hard rock band Zebra. A variant of the organ trio are trios formed with an electric bassist, a drummer and an electronic keyboardist (playing synthesizers) such as the progressive rock band Emerson, Lake & Palmer, Triumvirat, and Atomic Rooster. Another variation is to have a vocalist, a guitarist and a drummer, an example being Yeah Yeah Yeahs. Another variation is two guitars, a bassist, and a drum machine, examples including Magic Wands and Big Black.

A power trio with the guitarist on lead vocals is a popular record company lineup, as the guitarist and singer will usually be the songwriter. Therefore, the label only has to present one "face" to the public. The backing band may or may not be featured in publicity. If the backup band is not marketed as an integral part of the group, this gives the record company more flexibility to replace band members or use substitute musicians. This lineup often leads to songs that are fairly simple and accessible, as the frontman (or frontwoman) will have to sing and play guitar at the same time.

The four-piece band is the most common configuration in rock and pop music.

Another common formation was a vocalist, electric guitarist, bass guitarist, and a drummer (e.g. The Who, The Monkees, Led Zeppelin, Queen, Ramones, Sex Pistols, Red Hot Chili Peppers, R.E.M., Blur, The Smiths, Echo and the Bunnymen, The Stone Roses, Creed, Black Sabbath, Van Halen, Rage Against the Machine, Gym Class Heroes, The Stooges, Joy Division, and U2.) Instrumentally, these bands can be considered as trios. This format is popular with new bands, as there are only two instruments that need tuning, the melody and chords formula prevalent with their material is easy to learn, four members are commonplace to work with, the roles are clearly defined and generally are: instrumental melody line, rhythm section which plays the chords or countermelody, and vocals on top.

In some early rock bands, keyboardists were used, performing on piano (e.g. The Seeds and The Doors) with a guitarist, singer, drummer and keyboardist. Some bands will have a guitarist, bassist, drummer, and keyboard player (for example, Talking Heads, Gerry and the Pacemakers, Small Faces, King Crimson, The Guess Who, Pink Floyd, Queen, Coldplay, The Killers and Blind Faith).

Some bands will have the bassist on lead vocals, such as Thin Lizzy, The Chameleons, Skillet, Pink Floyd, Motörhead, NOFX, +44, Slayer, The All-American Rejects or even the lead guitarist, such as Death, Dire Straits, Megadeth and Creedence Clearwater Revival. Some bands, such as The Beatles, Dire Straits and Metallica have a lead guitarist, a rhythm guitarist and a bassist that all sing lead and backing vocals, that also play keyboards regularly, as well as a drummer.

Five-piece bands have existed in rock music since the development of the genre. The Beach Boys, The Rolling Stones (until 1993), Aerosmith, Def Leppard, AC/DC, Oasis, Pearl Jam, Guns N' Roses, Radiohead, The Strokes, The Yardbirds, 311 and The Hives are examples of the common vocalist, lead guitar, rhythm guitar, bass, and drums lineup whilst other bands such as Judas Priest have two guitarists who equally share lead and rhythm parts. An alternative to the five-member lineup replaces the rhythm guitarist with a keyboard–synthesizer player (examples being the bands Journey, Elbow, Dream Theater, Genesis, Jethro Tull, The Zombies, The Animals, Bon Jovi, Yes, Fleetwood Mac, Marilyn Manson and Deep Purple, all of which consist of a vocalist, guitarist, bassist, keyboardist, and a drummer) or with a turntablist such as Deftones, Hed PE, Incubus or Limp Bizkit.

Alternatives include a keyboardist, guitarist, drummer, bassist, and saxophonist, such as The Sonics, The Dave Clark 5, and Sam the Sham and the Pharaohs. Another alternative is three guitarists, a bassist and a drummer, such as Foo Fighters, Radiohead, and The Byrds. Some five-person bands feature two guitarists, a keyboardist, a bassist and a drummer, with one or more of these musicians (typically one of the guitarists) handling lead vocals on top of their instrument (examples being Children of Bodom, Styx, Sturm und Drang, Relient K, Ensiferum and the current line up of Status Quo). In some cases, typically in cover bands, one musician plays either rhythm guitar or keyboards, depending on the song (one notable band being Firewind, with Bob Katsionis handling this particular role).

Other times, the vocalist will bring another musical "voice" to the table, most commonly a harmonica or percussion; Mick Jagger, for example, played harmonica and percussion instruments like maracas and tambourine whilst singing at the same time. Keith Relf of the Yardbirds played harmonica frequently, though not often while also singing. Ozzy Osbourne was also known to play the harmonica on some occasions (i.e. "The Wizard" by Black Sabbath). Vocalist Robert Brown of lesser known steampunk band Abney Park plays harmonica, accordion, and darbuka in addition to mandolin. Flutes are also commonly used by vocalists, most notably Ian Anderson of Jethro Tull and Ray Thomas of the Moody Blues, though these are difficult to play while singing at the same time.

A less common lineup is to have lead vocals, two guitarists of varying types and two drummers, e.g. Adam and the Ants.

Although they are quite uncommon, larger bands have long been a part of rock and pop music, in part due to the influence of the "singer accompanied with orchestra" model inherited from popular big-band jazz and swing and popularized by Frank Sinatra and Ella Fitzgerald.To create larger ensembles, rock bands often add an additional guitarist, an additional keyboardist, additional percussionists or second drummer, an entire horn section, and even a flautist. An example of a six-member rock band is Toto with a lead vocalist, guitarist, bassist, two keyboard players, and drummer. The American heavy metal band Slipknot is composed of nine members, with a vocalist, two guitarists, a drummer, a bassist, two custom percussionists/backing vocalists, a turntablist, and a sampler/keyboardist. 

In larger groups (such as The Band), instrumentalists could play multiple instruments, which enabled the ensemble to create a wider variety of instrument combinations. More modern examples of such a band are Arcade Fire and Edward Sharpe and the Magnetic Zeros.
More rarely, rock or pop groups will be accompanied in concerts by a full or partial symphony orchestra, where lush string-orchestra arrangements are used to flesh out the sound of slow ballads.

Rhys Chatham and Glenn Branca started doing performances in the late 1970s with orchestras consisting of ten to hundred (Branca) and even four hundred guitars.

Some groups have a large number of members that all play the same instrument, such as guitar, keyboard, horns or strings.

Electronic music groups typically use electronic musical instruments such as synthesizers, sequencers, samplers and electronic drums to produce music. The production technique of music programming is also widely used in electronic music. Examples include Kraftwerk, Daft Punk, The Chemical Brothers, Faithless and Apollo 440.

Electronic dance music groups usually consist of two to three members, and are mainly producers, DJs and remixers, whose work is solely produced in a studio or with the use of a digital audio workstation. Examples include Basement Jaxx, Flip & Fill, Tin Tin Out, The Chainsmokers, Cheat Codes, Cash Cash and Major Lazer.

Women have a high prominence in many popular music styles as singers. However, professional women instrumentalists are uncommon in popular music, especially in rock genres such as heavy metal. "[P]laying in a band is largely a male homosocial activity, that is, learning to play in a band is largely a peer-based... experience, shaped by existing sex-segregated friendship networks. As well, rock music "...is often defined as a form of male rebellion vis-à-vis female bedroom culture." In popular music, there has been a gendered "distinction between public (male) and private (female) participation" in music. "[S]everal scholars have argued that men exclude women from bands or from the bands' rehearsals, recordings, performances, and other social activities." "Women are mainly regarded as passive and private consumers of allegedly slick, prefabricated – hence, inferior – pop music..., excluding them from participating as high status rock musicians." One of the reasons that there are rarely mixed gender bands is that "bands operate as tight-knit units in which homosocial solidarity – social bonds between people of the same sex... – plays a crucial role." In the 1960s pop music scene, "[s]inging was sometimes an acceptable pastime for a girl, but playing an instrument...simply wasn't done."

"The rebellion of rock music was largely a male rebellion; the women—often, in the 1950s and '60s, girls in their teens—in rock usually sang songs as personæ utterly dependent on their macho boyfriends...". Philip Auslander says that "Although there were many women in rock by the late 1960s, most performed only as singers, a traditionally feminine position in popular music". Though some women played instruments in American all-female garage rock bands, none of these bands achieved more than regional success. So they "did not provide viable templates for women's on-going participation in rock". In relation to the gender composition of heavy metal bands, it has been said that "[h]eavy metal performers are almost exclusively male" "...[a]t least until the mid-1980s" apart from "...exceptions such as Girlschool." However, "...now [in the 2010s] maybe more than ever–strong metal women have put up their dukes and got down to it", "carv[ing] out a considerable place for [them]selves."
When Suzi Quatro emerged in 1973, "no other prominent female musician worked in rock simultaneously as a singer, instrumentalist, songwriter, and bandleader". According to Auslander, she was "kicking down the male door in rock and roll and proving that a female "musician" ... and this is a point I am extremely concerned about ... could play as well if not better than the boys".

Sung dramas such as operas and musicals usually have numbers where several of the principals are singing together, either on their own or with the chorus. Such numbers ("duets", "trios", etc.) are also referred to as 'ensembles'.

A choir is a group of voices. By analogy, sometimes a group of similar instruments in a symphony orchestra are referred to as a choir. For example, the woodwind instruments of a symphony orchestra could be called the woodwind choir.

A group that plays popular music or military music is usually called a band; a drum and bugle corps is a type of the latter. These bands perform a wide range of music, ranging from arrangements of jazz orchestral, or popular music to military-style marches. Drum corps perform on brass and percussion instruments only. Drum and Bugle Corps incorporate costumes, hats, and pageantry in their performances.

Other band types include:




</doc>
<doc id="20181" url="https://en.wikipedia.org/wiki?curid=20181" title="Marienburg">
Marienburg

Marienburg may refer to:






</doc>
<doc id="20182" url="https://en.wikipedia.org/wiki?curid=20182" title="Afghan Armed Forces">
Afghan Armed Forces

The Afghan Armed Forces are the military forces of the Islamic Republic of Afghanistan. They consist of the Afghan National Army and the Afghan Air Force. The President of Afghanistan is the Commander-in-Chief of the Afghan Armed Forces, which is administratively controlled through the Ministry of Defense. The National Military Command Center in Kabul serves as the headquarters of the Afghan Armed Forces. The Afghan Armed Forces currently has approximately 300,000 active duty soldiers and airmen, which are expected to reach 360,000 soldiers and airmen in the coming year.

The current Afghan military originates in 1709 when the Hotaki dynasty was established in Kandahar followed by the Durrani Empire. The Afghan military fought many wars with the Safavid dynasty and Maratha Empire from the 18th to the 19th century. It was re-organized with help from the British in 1880, when the country was ruled by Amir Abdur Rahman Khan. It was modernized during King Amanullah Khan's rule in the early 20th century, and upgraded during King Zahir Shah's forty-year rule. From 1978 to 1992, the Soviet-backed Afghan Armed Force fought with multi-national mujahideen groups who were being backed by the United States, Saudi Arabia, and Pakistan. After President Najibullah's resignation in 1992 and the end of Soviet support, the military dissolved into portions controlled by different warlord factions and the mujahideen took control over the government. This era was followed by the rise of the Pakistan-backed Taliban regime, who established a military force on the basis of Islamic sharia law.

After the removal of the Taliban and the formation of the Transitional Islamic State of Afghanistan in late 2001 and 2002, respectively, the Afghan Armed Forces was gradually rebuilt by NATO forces in the country, primarily by the United States Armed Forces. Despite early problems with recruitment and training, it is becoming effective in fighting against the Taliban insurgency. As of 2014, it is becoming able to operate independently from the NATO International Security Assistance Force. As a major non-NATO ally of the United States, Afghanistan continues to receive billions of dollars in military assistance.

Afghans have served in the militaries of the Ghaznavids (963–1187), Ghurids (1148–1215), Delhi Sultanate (1206–1527), Mughals (1526–1858) and the Persian army. The current Afghan military traces its origin to the early 18th century when the Hotaki dynasty rose to power in Kandahar and defeated the Persian Safavid Empire at the Battle of Gulnabad in 1722.
When Ahmad Shah Durrani formed the Durrani Empire in 1747, his Afghan army fought a number of wars in the Punjab region of Hindustan during the 18th to the 19th century. One of the famous battles was the 1761 Battle of Panipat in which the Afghans invaded and decisively defeated the Hindu Maratha Empire. The Afghans then engaged in wars with the Punjabi Sikh Empire of Ranjit Singh, which included the Battle of Jamrud in which Hari Singh Nalwa was killed by Prince Akbar Khan. During the First Anglo-Afghan War, British India invaded Afghanistan in 1838 but withdraw in 1842. During the three years a number of battles took place in different parts of Afghanistan.

The first organized army of Afghanistan (in the modern sense) was established after the Second Anglo-Afghan War in 1880 when the nation was ruled by Emir Abdur Rahman Khan. Traditionally, Afghan governments relied on three military institutions: the regular army, tribal levies, and community militias. The regular army was sustained by the state and commanded by government leaders. The tribal or regional levies - irregular forces - had part-time soldiers provided by tribal or regional chieftains. The chiefs received tax breaks, land ownership, cash payments, or other privileges in return. The community militia included all available able-bodied members of the community, mobilized to fight, probably only in exceptional circumstances, for common causes under community leaders. Combining these three institutions created a formidable force whose components supplemented each other's strengths and minimized their weaknesses.

After the Third Anglo-Afghan War ended, the reforming King Amanullah did not see the need for a large army, instead deciding to rely on Afghanistan's historical martial qualities. This resulted in neglect, cutbacks, recruitment problems, and finally an army unable to quell the 1929 up-rising that cost him his throne. However, under his reign, the Afghan Air Force was formed in 1924. The Afghan Armed Forces were expanded during King Zahir Shah's reign, reaching a strength of 70,000 in 1933.

Following World War II, Afghanistan briefly received continued military support from the British government under the Lancaster Plan from 1945 to 1947, until the partition of India realigned British priorities in the region. Afghanistan declined to join the 1955 United States-sponsored Baghdad Pact; this rebuff did not stop the United States from continuing its low-level aid program, but it was reluctant to provide Afghanistan with military assistance, so Daoud turned to the Soviet Union and its allies for military aid, and in 1955 he received approximately US$25 million of military aid. In addition, the Soviet bloc also began construction of military airfields in Bagram, Mazar-e-Sharif, and Shindand. By the 1960s, Soviet assistance started to improve the structure, armament, training, and command and control arrangements for the military. The Afghan Armed Forces reached a strength of 98,000 (90,000 soldiers and 8,000 airmen) by this period.

After the exile of King Zahir Shah in 1973, President Daud Khan forged stronger ties with the Soviets by signing two highly controversial military aid packages for his nation in 1973 and 1975. For three years, the Afghan Armed Forces and police officers received advanced Soviet weapons, as well as training by the KGB and Soviet Armed Forces. Due to problems with local political parties in his country, President Daud Khan decided to distance himself from the Soviets in 1976. He made Afghanistan's ties closer to the Greater Middle East and the United States instead.

From 1977 to 1978, the Afghan Armed Forces conducted joint military training with the Egyptian Armed Forces. In April 1978 there was a coup, known as the Saur Revolution, orchestrated by members of the government loyal to the People's Democratic Party of Afghanistan (PDPA). This led to a full-scale Soviet invasion in December 1979, led by the 40th Army and the Airborne Forces. In 1981 the total strength of the Army was around 85,000 troops according to The New York Times. The Army had around 35-40,000 soldiers, who was mostly conscripts, the Air Force had around 7,000 airmen and if put together all military personnel in 1984, the total strength of the Afghan Armed Forces was around 87,000 in 1984. Throughout the 1980s, the Afghan Armed Forces was heavily involved in fighting against the multi-national Mujahiddin rebel groups who were largely backed by the United States and trained by the Pakistani Armed Forces. The rebel groups were fighting to force the Soviet Union to withdraw from Afghanistan as well as to remove the Soviet-backed government of President Mohammad Najibullah. Due to large number of defectors, the Afghan Armed Forces in 1985 was reduced to around 47,000. The Air Force had over 150 combat aircraft with about 7,000 officers who were supported by an estimated 5,000 Cuban Revolutionary Air and Air Defense Force and Czechoslovak Air Force advisers.

Weapons supplies were made available to the Mujahideen through numerous countries; the United States purchased all of Israel's captured Soviet weapons clandestinely, and then funnelled the weapons to the Mujahideen, while Egypt upgraded their own Army's weapons, and sent the older weapons to the militants, Turkey sold their World War II stockpiles to the warlords, and the British and Swiss provided Blowpipe missiles and Oerlikon anti-aircraft guns respectively, after they were found to be poor models for their own forces. China provided the most relevant weapons, likely due to their own experience with guerrilla warfare, and kept meticulous record of all the shipments.

Following the Soviet withdrawal in 1989 the Democratic Republic of Afghanistan continued to deal with attacks from the Mujahiddin. For several years the Afghan Armed Forces had actually increased their effectiveness past levels ever achieved during the Soviet military presence. But the government was dealt a major blow when Abdul Rashid Dostum, a leading general, switched allegiances to the Mujahideen in 1992 and together they captured the city of Kabul. By 1992 the Army fragmented into regional militias under local warlords because of the fall of the Soviet Union which stopped supplying the Afghan Armed Forces and later in 1992 when the Afghan government lost power and the country went into a state of anarchy.

After the fall of Najibullah's regime in 1992, private militias were formed and the nation began to witness a Civil War between the various warlords, including Ahmad Shah Massoud, Gulbuddin Hekmatyar, Abdul Rashid Dostum, Abdul Ali Mazari, Ismail Khan, and many others. They received logistics support from foreign powers including Russia, Pakistan, India, Iran, China, France, Canada and the United States. When the Taliban took power in 1996, the warlords fled Kabul to the north or neighboring countries. With the backing and support of Pakistan, the Taliban began creating a new military force purely based on Islam's Sharia law.

The Taliban maintained a military during their period of control. The Taliban Army possessed over 400 T-54/55 and T-62 tanks and more than 200 Armoured personnel carriers.
The Afghan Air Force under the Taliban maintained five supersonic MIG-21MFs and 10 Sukhoi-22 fighter-bombers. In 1995, during the 1995 Airstan incident, a Taliban fighter plane captured a Russian transport. They also held six Mil Mi-8 helicopters, five Mi-35s, five L-39Cs, six An-12s, 25 An-26s, a dozen An-24/32s, an IL-18, and a Yakovlev.

After the formation of the Karzai administration in late 2001, the Afghan Armed Forces was gradually reestablished by the United States and its allies. Initially, a new land force, the Afghan National Army (ANA), was created, along with an air arm, the Afghan National Army Air Corps, as an integral part of the Army. The ANA Air Corps later split off to become an independent branch, the Afghan Air Force (AAF). Commandos and Special Forces were also trained and formed as a part of the Afghan National Army. Training was managed initially by the U.S. Office of Military Cooperation, followed by other U.S. organisations and then Combined Security Transition Command-Afghanistan, and is now being run by the NATO Training Mission-Afghanistan.

The Afghan Air Force was relatively capable before and during the 1980s but by late 2001, the number of operational aircraft available was minimal. The United States and its allies quickly eliminated the remaining strength and ability of the Taliban to operate aircraft in the opening stages of their invasion. With the occupation of airbases by American forces it became clear how destitute the Air Force had become since the withdrawal of the Soviet Union. Most aircraft were only remnants rusting away for a decade or more. Many others were relocated to neighboring countries for storage purposes or sold cheaply. The AAF was reduced to a very small force while the country was torn by civil war. It is currently being rebuilt and modernized by the NATO-led multinational Combined Air Power Transition Force of the international Combined Security Transition Command - Afghanistan (CSTC-A).
There has been significant progress toward revitalization of the Afghan Armed Forces in the last decade, with two service branches established. The ANA and AAF are under the Afghan Ministry of Defense, which forms the basic military force. By 2006, more than 60,000 former militiamen from around the country have been disarmed. Most heavy weapons from Panjshir, Balkh, Nangarhar and other areas were seized by the Afghan government. In 2007, it was reported that the DDR programmes had dismantled 274 paramilitary organizations, reintegrated over 62,000 militia members into civilian life, and recovered more than 84,000 weapons, including heavy weapons. But "The New York Times" reported in October 2007 this information in the context of a reported rise in the number of hoarded weapons in the face of what has been seen as a growing Taliban threat, even in the north of the country.
The ANA Commando Battalion was established in 2007. The Afghan National Development Strategy of 2008 explained that the aim of DIAG (Disbandment of Illegal Armed Groups) was to ban all illegal armed groups in all provinces of the country. Approximately 2,000 such groups have been identified and most of them have surrendered to the Afghan government or joined the nation's military.

The NATO-trained Afghan National Army is organized into 31 Kandaks, or Battalions, 28 of which are considered combat ready. Seven regional corps headquarters exist. The National Military Academy of Afghanistan was built to provide future officers, it is modeled after the United States Military Academy and United States Air Force Academy. The Afghan Defense University (ADU) is located in Kabul province and consists of a headquarters building, classrooms, dining facility, library, and medical clinic. In addition to this, an $80 million central command center was built next to the Hamid Karzai International Airport. In 2012, Afghanistan became a Major non-NATO ally of the United States.

Sizable numbers of Afghan officers are sent to be trained in India either at the Indian Military Academy in Dehradun, the National Defence Academy near Pune or the Officers Training Academy in Chennai. The Indian Military Academy which has been in existence since 1932, provides a 4-year degree to army officers, while the National Defence Academy is a tri-service college provides a 3-year degree after which officers undergo a 1-year specialization in their respective service colleges. The Officers Training Academy on the other hand provides a 49-week course to Graduate officer candidates. In 2014 the number of Afghan officers in training in India was nearly 1,100. A Total of 1,200 Afghan officers have been trained up to 2013.

The total manpower of the Afghan Armed Forces was around 164,000 in May 2011. By September 2014 it has reached 195,000. Its Air Force has about 100 refurbished aircraft, which includes A-29 Super Tucano attack aircraft, Lockheed C-130 Hercules and Pilatus PC-12s military transport aircraft, as well as Mil Mi-17 and Mi-24 helicopters. It also includes trainers such as Aero L-39 Albatros and Cessna 182. The manpower of the Afghan Air Force is around 3600 airmen, including 450 pilots. It also has small number of female pilots.


Large numbers of military bases are found all cross the country, including major ones in Kabul, Kandahar, Herat, Balkh, Nangarhar, Khost, Paktia, Paktika, Maidan Wardak, Ghazni, Farah, and many other provinces. Some of these were built by the United States Army Corps of Engineers (USACE) while others by ISAF and Afghans. It was reported in 2010 that there were at least 700 military bases inside Afghanistan but more were expected to be built in the coming years. About 400 of these were used by Americans and ISAF forces with the remaining 300 or so by Afghan National Security Forces.

During the 1950s and 1960s, Afghanistan purchased moderate quantities of Soviet weapons to keep the military up to date. It was mainly Sukhoi Su-7, MiG-21 fighter jets, T-34 and Iosif Stalin tanks, SU-76 self-propelled guns, GAZ-69 4x4 light trucks of jeep class (in many versions), ZIL-157 military trucks, Katyusha multiple rocket launchers, and BTR-40 and BTR-152 armored personnel carriers. Also included were PPSh-41 and RPK machine guns. After King Zahir Shah's exile in 1973, President Daoud Khan made attempts to create a strong Afghan military in the Greater Middle East-South Asia region. Between 1973 and 1978, Afghanistan obtained more sophisticated Soviet weapons such as Mi-4 and Mi-8 helicopters, Su-22 and Il-28 jets. In addition to that the nation possessed great many T-55, T-62, and PT-76 tanks along with huge amounts of AKM assault rifles ordered. Armored vehicles delivered in the 1970s also included: ZIL-135s, BMP-1s, BRDM-1s, BTR-60s, UAZ-469, and GAZ-66 as well as large quantities of small arms and artillery.

Under the Democratic Republic of Afghanistan (1978–1992), weapon deliveries by the Soviets were increased and included Mi-24 helicopters, MiG-23 fighter aircraft, ZSU-23-4 "Shilka" and ZSU-57-2 anti-aircraft self-propelled mounts, MT-LB armored personnel carriers, BM-27 "Uragan" and BM-21 "Grad" multiple-launch rocket systems and FROG-7 and Scud launchers. Some of the weapons that were not damaged during the decades of wars are still being used today, while the remainder have probably been sold on the black market.

The United States has provided billions of dollars in military aid. One package included 2,500 Humvees, tens of thousands of M16 assault rifles and body armoured-jackets. It also included the building of a national military command center as well as training compounds in several provinces of the country. Canadian Forces supplied some ANA soldiers surplus C7 assault rifles but the Afghans returned the Canadian-made C7 in favor of the American-made M16 rifle, reason being that parts between the two rifles, despite being similar, are not fully interchangeable.

Besides NATO, Afghanistan has been increasingly turning to India and Russia for assistance. Both countries have supported the Northern Alliance, with funding, training, supplies and medical treatment of wounded fighters, against the Taliban prior to 2002. India has been helping with several billion dollars invested in infrastructure development projects in Afghanistan besides the training of Afghan officers in India, but has been reluctant to provide military aid due to fears of antagonizing its regional rival Pakistan. In 2013, after years of subtle reminders, the Afghan government sent a wish list of heavy weapons to India. The list includes as many as 150 battle tanks T-72, 120 (105 mm) field guns, a large number of 82 mm mortars, one medium lift transport aircraft AN-32, two squadrons of medium lift Mi-17 and attack helicopters Mi-35, and a large number of trucks. In 2014, India signed a deal with Russia and Afghanistan where it would pay Russia for all the heavy equipment requested by Afghanistan instead of directly supplying them. The deal also includes the refurbishment of heavy weapons left behind since the Soviet war.

The United States has also been largely responsible for the growth of the Afghan Air Force, as part of the Combined Air Power Transition Force, from four aircraft at the end of 2001 to about 100 as of 2011. Types include Lockheed C-130 Hercules and Pilatus PC-12 transport aircraft, A-29 Super Tucano attack aircraft, as well as Mi-17 troop-carrying helicopters and Mi-35 attack helicopters. The aircrew are being trained by an American team. The American intention is to spend around $5 billion by 2016 to increase the force to around 120 aircraft.

As the size of Afghan Armed Force is growing rapidly so is the need for more aircraft and vehicles. It was announced in 2011 that the Afghan Armed Forces would be provided with 145 multi-type aircraft, 21 helicopters and 23,000 various type vehicles. As a Major non-NATO ally of the United States, Afghanistan is able to purchase and receive weapons from the United States without restrictions. In the meantime, the Afghan Air Force began seeking fighter aircraft and other advanced weapons. Defense Minister Wardak explained that "what we are asking to acquire is just the ability to defend ourselves, and also to be relevant in the future so that our friends and allies can count on us to participate in peacekeeping and other operations of mutual interest."



</doc>
<doc id="20185" url="https://en.wikipedia.org/wiki?curid=20185" title="Motorcycle sport">
Motorcycle sport

Motorcycle sport is a broad field that encompasses all sporting aspects of motorcycling. The disciplines are not all races or timed-speed events, as several disciplines test a competitor's various riding skills.

Motorcycle racing (also known as moto racing and motorbike racing) is a motorcycle sport involving racing motorcycles. Motorcycle racing can be divided into two categories, tarmac-based road disciplines and off-road.

Track racing is a motorcycle sport where teams or individuals race opponents around an oval track. There are differing variants, with each variant racing on a different surface type.

A road rally is a navigation event on public roads whereby competitors must visit a number of checkpoints in diverse geographical locations while still obeying road traffic laws (not to be confused with car rallies such as WRC).

Speedway is a motorcycle sport in which the motorcycles have one gear and no brakes.

Land speed is where a single rider accelerates over a 1 to long straight track (usually on dry lake beds) and is timed for top speed through a trap at the end of the run. The rider must exceed the previous top speed record for that class or type of bike for their name to be placed on the record books. See— for an example.

Enduro is not exactly racing, because the main objective is to traverse a series of checkpoints, arriving exactly "on time" in accordance with your beginning time and the time it is supposed to take to arrive at each checkpoint. The courses are usually run over thick wooded terrain, sometimes with large obstacles such as logs, ditches, and sudden drops.

A competition based upon points for acrobatic ability on an MX bike over jumps. This activity evolved from Motocross a continuing popular form of racing at both the Amateur and Professional levels.

Known in the US as Observed Trials, it is not racing, but a sport nevertheless. Trials is a test of skill on a motorcycle whereby the rider attempts to traverse an observed section without placing a foot on the ground (and traditionally, although not always, without ceasing forward motion). The winner is the rider with the least penalty points.

Time and observation trials are trials with a time limit. The person who completes the route the quickest sets the "standard time" and all other competitors must finish within a certain amount of time of the standard time to be counted as a finisher (they received penalty points for every minute after the quickest finisher). This is combined with the penalty points accrued from the observed sections to arrive at a winner, who is not alway the quickest rider or the rider who lost the less marks on observation but the rider who balanced these competing demands the best. One of the most famous time and observation trials is the "Scott" trial held annually in North Yorkshire.

Indoor trials held in stadiums (not necessarily with a roof) which by their very nature use man-made artificial sections in contrast to outdoor trials which rely heavily on the natural terrain.

Long Distance Trials (often shortened to 'LDT') in the UK are events for road-registered motorcycles. A course of typically 80 to 120 miles is plotted by the organiser, taking in roads, lanes and Byways Open to All Traffic (known as BOATs). The event is not a race and riders are required to follow the course by using a RoadBook compiled by the organiser.

Similar to car Autocross, Motorcycle Gymkhana is a motorcycle time trial sport round cones on a paved area. The winner is the competitor who completes the course in the shortest time. Time penalties are incurred by putting a foot down, hitting a cone, or going outside the designated area.

Similar to football, but all players (except goalkeepers) are riding motorcycles, and the ball is much bigger. Motorcycle Polo first began as an officially organized sport in the mid-1930s. In France, there are organized motoball competitions, and the sport was included in the inaugural Goodwill Games.

In the United States the completions are usually held on off-road courses, where one competitor at a time attempts to ride up a very steep hill, often 45 degrees or more. In some cases, few riders actually complete the course and results are judged on the distance that they manage to achieve. Of those that do complete the course, the rider to reach the top with the shortest elapsed time wins. The motorcycle of choice in the early decades was the Harley-Davidson 45 cubic inch model due to its high torque at low rpms, similar to farm engines. For years the national competitions was held at Mount Garfield near Muskegon, Michigan.

In other countries, notably the United Kingdom, completions mostly take place on tarmac courses, occasionally closed public roads, with the machines used for competition being similar to those used for other road disciplines.



</doc>
<doc id="20187" url="https://en.wikipedia.org/wiki?curid=20187" title="Marina Tsvetaeva">
Marina Tsvetaeva

Marina Ivanovna Tsvetaeva (; 31 August 1941) was a Russian and Soviet poet. Her work is considered among some of the greatest in twentieth century Russian literature. She lived through and wrote of the Russian Revolution of 1917 and the Moscow famine that followed it. In an attempt to save her daughter Irina from starvation, she placed her in a state orphanage in 1919, where she died of hunger. Tsvetaeva left Russia in 1922 and lived with her family in increasing poverty in Paris, Berlin and Prague before returning to Moscow in 1939. Her husband Sergei Efron and her daughter Ariadna Èfron (Alya) were arrested on espionage charges in 1941; her husband was executed. Tsvetaeva committed suicide in 1941. As a lyrical poet, her passion and daring linguistic experimentation mark her as a striking chronicler of her times and the depths of the human condition.

Marina Tsvetaeva was born in Moscow, the daughter of Ivan Vladimirovich Tsvetaev, a professor of Fine Art at the University of Moscow, who later founded the Alexander III Museum of Fine Arts (known from 1937 as the Pushkin Museum). (The Tsvetaev family name evokes association with flowers – the Russian word цвет ("tsvet") means "color" or "flower".) Tsvetaeva's mother, , Ivan's second wife, was a concert pianist, highly literate, with German and Polish ancestry. Growing up in considerable material comfort, Tsvetaeva would later come to identify herself with the Polish aristocracy.

Tsvetaeva's two half-siblings, Valeria and Andrei, were the children of Ivan's deceased first wife, Varvara Dmitrievna Ilovaiskaya, daughter of the historian Dmitry Ilovaisky. Tsvetaeva's only full sister, Anastasia, was born in 1894. The children quarrelled frequently and occasionally violently. There was considerable tension between Tsvetaeva's mother and Varvara's children, and Tsvetaeva's father maintained close contact with Varvara's family. Tsvetaeva's father was kind, but deeply wrapped up in his studies and distant from his family. He was also still deeply in love with his first wife; he would never get over her. Maria Tsvetaeva had had a love affair before her marriage, from which she never recovered. Maria Tsvetaeva disapproved of Marina's poetic inclination; she wanted her daughter to become a pianist, holding the opinion that her poetry was poor.

In 1902 Tsvetaeva's mother contracted tuberculosis. A change in climate was believed to help cure the disease, and so the family travelled abroad until shortly before her death in 1906, when Tsvetaeva was 14. They lived for a while by the sea at Nervi, near Genoa. There, away from the rigid constraints of a bourgeois Muscovite life, Tsvetaeva was able for the first time to run free, climb cliffs, and vent her imagination in childhood games. There were many Russian "émigré" revolutionaries residing at that time in Nervi, who may have had some influence on the young Tsvetaeva.

In June 1904 Tsvetaeva was sent to school in Lausanne. Changes in the Tsvetaev residence led to several changes in school, and during the course of her travels she acquired the Italian, French, and German languages. She gave up the strict musical studies that her mother had imposed and turned to poetry. She wrote "With a mother like her, I had only one choice: to become a poet".

In 1908, aged 16, Tsvetaeva studied literary history at the Sorbonne. During this time, a major revolutionary change was occurring within Russian poetry: the flowering of the Russian symbolist movement, and this movement was to colour most of her later work. It was not the theory which was to attract her, but the poetry and the gravity which writers such as Andrei Bely and Alexander Blok were capable of generating. Her own first collection of poems, "Vecherny Albom" ("Evening Album"), self-published in 1910, promoted her considerable reputation as a poet. It was well received, although her early poetry was held to be insipid compared to her later work. It attracted the attention of the poet and critic Maximilian Voloshin, whom Tsvetaeva described after his death in "A Living Word About a Living Man". Voloshin came to see Tsvetaeva and soon became her friend and mentor.

She began spending time at Voloshin's home in the Black Sea resort of Koktebel ("Blue Height"), which was a well-known haven for writers, poets and artists. She became enamoured of the work of Alexander Blok and Anna Akhmatova, although she never met Blok and did not meet Akhmatova until the 1940s. Describing the Koktebel community, the "émigré" Viktoria Schweitzer wrote: "Here inspiration was born." At Koktebel, Tsvetaeva met Sergei Yakovlevich Efron, a 17-year-old cadet in the Officers' Academy. She was 19, he 18: they fell in love and were married in 1912, the same year as her father's project, the Alexander III Museum of Fine Arts, was ceremonially opened, an event attended by Tsar Nicholas II. Tsvetaeva's love for Efron was intense; however, this did not preclude her from having affairs, including one with Osip Mandelstam, which she celebrated in a collection of poems called "Mileposts". At around the same time, she became involved in an affair with the poet Sophia Parnok, who was 7 years older than Tsvetaeva, an affair that caused her husband great grief. The two women fell deeply in love, and the relationship profoundly affected both women's writings. She deals with the ambiguous and tempestuous nature of this relationship in a cycle of poems which at times she called "The Girlfriend", and at other times "The Mistake". Tsvetaeva and her husband spent summers in the Crimea until the revolution, and had two daughters: Ariadna, or Alya (born 1912) and Irina (born 1917).

In 1914, Efron volunteered for the front and by 1917 he was an officer stationed in Moscow with the 56th Reserve. Tsvetaeva was a close witness of the Russian Revolution, which she rejected. On trains, she came into contact with ordinary Russian people and was shocked by the mood of anger and violence. She wrote in her journal: "In the air of the compartment hung only three axe-like words: bourgeois, Junkers, leeches." After the 1917 Revolution, Efron joined the White Army, and Marina returned to Moscow hoping to be reunited with her husband. She was trapped in Moscow for five years, where there was a terrible famine.

She wrote six plays in verse and narrative poems. Between 1917 and 1922 she wrote the epic verse cycle "Lebedinyi stan" ('‘The Encampment of the Swans’') about the civil war, glorifying those who fought against the communists. The cycle of poems in the style of a diary or journal begins on the day of Tsar Nicholas II's abdication in March 1917, and ends late in 1920, when the anti-communist White Army was finally defeated. The 'swans' of the title refers to the volunteers in the White Army, in which her husband was fighting as an officer. In 1922 she published a long pro-imperial verse fairy tale, "Tsar-devitsa" ("Tsar-Maiden").

The Moscow famine was to exact a toll on Tsvetaeva. With no immediate family to turn to, she had no way to support herself or her daughters. In 1919, she placed both her daughters in a state orphanage, mistakenly believing that they would be better fed there. Alya became ill, and Tsvetaeva removed her, but Irina died there of starvation in 1920. The child's death caused Tsvetaeva great grief and regret. In one letter, she wrote, "God punished me." 

During these years, Tsvetaeva maintained a close and intense friendship with the actress Sofia Evgenievna Holliday, for whom she wrote a number of plays. Many years later, she would write the novella "Povest o Sonechke" about her relationship with Holliday.

In May 1922, Tsvetaeva and Ariadna left Soviet Russia and were reunited with Efron in Berlin, whom she had thought had been killed by the Bolsheviks. There she published the collections "Separation", "Poems to Blok", and the poem "The Tsar Maiden", much of her poetry appeared in Moscow and Berlin, consolidating her reputation. In August 1922, the family moved to Prague. Living in unremitting poverty, unable to afford living accommodation in Prague itself, with Efron studying politics and sociology at the Charles University and living in hostels, Tsvetaeva and Ariadna found rooms in a village outside the city. She writes "we are devoured by coal, gas, the milkman, the baker...the only meat we eat is horsemeat". When offered an opportunity to earn money by reading her poetry, she describes having to beg a simple dress from a friend to replace the one she had been living in.

Tsvetaeva began a passionate affair with , a former military officer, a liaison which became widely known throughout émigré circles. Efron was devastated. Her break-up with Rodziewicz in 1923 was almost certainly the inspiration for her "The Poem of the End" and "The Poem of the Mountain". At about the same time, Tsvetaeva began correspondence with poet Rainer Maria Rilke and novelist Boris Pasternak. Tsvetaeva and Pasternak were not to meet for nearly twenty years, but maintained friendship until Tsvetaeva's return to Russia.

In summer 1924, Efron and Tsvetaeva left Prague for the suburbs, living for a while in Jíloviště, before moving on to Všenory, where Tsvetaeva completed "The Poem of the End", and was to conceive their son, Georgy, whom she was to later nickname 'Mur'. Tsvetaeva wanted to name him Boris (after Pasternak); Efron insisted on Georgy. He was to be a most difficult child but Tsvetaeva loved him obsessively. With Efron now rarely free from tuberculosis, their daughter Ariadna was relegated to the role of mother's helper and confidante, and consequently felt robbed of much of her childhood. In Berlin before settling in Paris, Tsvetaeva wrote some of her greatest verse, including "Remeslo" ("Craft", 1923) and "Posle Rossii" ("After Russia", 1928). Reflecting a life in poverty and exiled, the work holds great nostalgia for Russia and its folk history, while experimenting with verse forms.

In 1925, the family settled in Paris, where they would live for the next 14 years. At about this time Tsvetaeva contracted tuberculosis. Tsvetaeva received a small stipend from the Czechoslovak government, which gave financial support to artists and writers who had lived in Czechoslovakia. In addition, she tried to make whatever she could from readings and sales of her work. She turned more and more to writing prose because she found it made more money than poetry. Tsvetaeva did not feel at all at home in Paris's predominantly ex-bourgeois circle of Russian émigré writers. Although she had written passionately pro-'White' poems during the Revolution, her fellow émigrés thought that she was insufficiently anti-Soviet, and that her criticism of the Soviet régime was altogether too nebulous. She was particularly criticised for writing an admiring letter to the Soviet poet Vladimir Mayakovsky. In the wake of this letter, the émigré paper "Posledniye Novosti", to which Tsvetaeva had been a frequent contributor, refused point-blank to publish any more of her work. She found solace in her correspondence with other writers, including Boris Pasternak, Rainer Maria Rilke, the Czech poet Anna Tesková, the critics D. S. Mirsky and Aleksandr Bakhrakh, and the Georgian émigré princess Salomea Andronikova, who became her main source of financial support. Her poetry and critical prose of the time, including her autobiographical prose works of 1934–7, is of lasting literary importance. "Consumed by the daily round", resenting the domesticity that left her no time for solitude or writing, her émigré milieu regarded Tsvetaeva as a crude sort who ignored social graces. Describing her misery, she wrote to Tesková "In Paris, with rare personal exceptions, everyone hates me, they write all sorts of nasty things, leave me out in all sorts of nasty ways, and so on". To Pasternak she complained "They don't like poetry and what am I apart from that, not poetry but that from which it is made. [I am] an inhospitable hostess. A young woman in an old dress." She began to look back at even the Prague times with nostalgia and resent her exiled state more deeply.

Meanwhile, Tsvetaeva's husband was developing Soviet sympathies and was homesick for Russia. Eventually, he began working for the NKVD, the forerunner of the KGB. Alya shared his views, and increasingly turned against her mother. In 1937, she returned to the Soviet Union. Later that year, Efron too had to return to the USSR. The French police had implicated him in the murder of the former Soviet defector Ignace Reiss in September 1937, on a country lane near Lausanne, Switzerland. After Efron's escape, the police interrogated Tsvetaeva, but she seemed confused by their questions and ended up reading them some French translations of her poetry. The police concluded that she was deranged and knew nothing of the murder. Later it was learned that Efron possibly had also taken part in the assassination of Trotsky's son in 1936. Tsvetaeva does not seem to have known that her husband was a spy, nor the extent to which he was compromised. However, she was held responsible for his actions and was ostracised in Paris because of the implication that he was involved with the NKVD. World War II had made Europe as unsafe and hostile as the USSR. In 1939, she became lonely and alarmed by the rise of fascism, which she attacked in "Stikhi k Chekhii" ("Verses to Czechia" 1938–39).

In 1939, she and her son returned to Moscow, unaware of the reception she would receive. In Stalin's USSR, anyone who had lived abroad was suspect, as was anyone who had been among the intelligentsia before the Revolution. Tsvetaeva's sister had been arrested before Tsvetaeva's return; although Anastasia survived the Stalin years, the sisters never saw each other again. Tsvetaeva found that all doors had closed to her. She got bits of work translating poetry, but otherwise the established Soviet writers refused to help her, and chose to ignore her plight; Nikolai Aseev, who she had hoped would assist, shied away, fearful for his life and position.

Efron and Alya were arrested for espionage. Alya's fiancé was actually an NKVD agent who had been assigned to spy on the family. Efron was shot in 1941; Alya served over eight years in prison. Both were exonerated after Stalin's death. In 1941, Tsvetaeva and her son were evacuated to Yelabuga (Elabuga), while most families of the Union of Soviet Writers were evacuated to Chistopol. Tsvetaeva had no means of support in Yelabuga, and on 24 August 1941 she left for Chistopol desperately seeking a job. On 26 August, Marina Tsvetaeva and poet Valentin Parnakh applied to the Soviet of Literature Fund asking for a job at the LitFund's canteen. Parnakh was accepted as a doorman, while Tsvetaeva's application for a permission to live in Chistopol was turned down and she had to return to Yelabuga on 28 August.

On 31 August 1941, while living in Yelabuga, Tsvetaeva hanged herself. She left a note for her son Mur: "Forgive me, but to go on would be worse. I am gravely ill, this is not me anymore. I love you passionately. Do understand that I could not live anymore. Tell Papa and Alya, if you ever see them, that I loved them to the last moment and explain to them that I found myself in a trap." According to book "The Death of a Poet: The Last Days of Marina Tsvetaeva", the local NKVD department tried to force Tsvetaeva to start working as their informant, which left her no choice other than to commit suicide.

Tsvetaeva was buried in Yelabuga cemetery on 2 September 1941, but the exact location of her grave remains unknown.

Her son Georgy volunteered for the Eastern Front of World War II and died in battle in 1944. Her daughter Ariadna spent 16 years in Soviet prison camps and exile and was released in 1955. Ariadna wrote a memoir of her family; an English-language edition was published in 2009. She died in 1975.

In the town of Yelabuga, the Tsvetaeva house is now a museum and a monument stands to her. Much of her poetry was republished in the Soviet Union after 1961, and her passionate, articulate and precise work, with its daring linguistic experimentation, brought her increasing recognition as a major poet.

A minor planet, 3511 Tsvetaeva, discovered in 1982 by Soviet astronomer Lyudmila Karachkina, is named after her.

In 1989 in Gdynia, Poland, a special-purpose ship was built for the Russian Academy of Sciences and named Marina Tsvetaeva in her honor. From 2007, the ship served as a tourist vessel to the polar regions for Aurora Expeditions. In 2011 she was renamed and is currently operated by Oceanwide Expeditions as a tourist vessel in the polar regions.

Tsvetaeva's poetry was admired by poets such as Valery Bryusov, Maximilian Voloshin, Osip Mandelstam, Boris Pasternak, Rainer Maria Rilke, and Anna Akhmatova. Later, that recognition was also expressed by the poet Joseph Brodsky, pre-eminent among Tsvetaeva's champions. Tsvetaeva was primarily a lyrical poet, and her lyrical voice remains clearly audible in her narrative poetry. Brodsky said of her work: "Represented on a graph, Tsvetaeva's work would exhibit a curve – or rather, a straight line – rising at almost a right angle because of her constant effort to raise the pitch a note higher, an idea higher (or, more precisely, an octave and a faith higher.) She always carried everything she has to say to its conceivable and expressible end. In both her poetry and her prose, nothing remains hanging or leaves a feeling of ambivalence. Tsvetaeva is the unique case in which the paramount spiritual experience of an epoch (for us, the sense of ambivalence, of contradictoriness in the nature of human existence) served not as the object of expression but as its means, by which it was transformed into the material of art." Critic Annie Finch describes the engaging, heart-felt nature of the work. "Tsvetaeva is such a warm poet, so unbridled in her passion, so completely vulnerable in her love poetry, whether to her female lover Sofie Parnak, to Boris Pasternak. [...] Tsvetaeva throws her poetic brilliance on the altar of her heart’s experience with the faith of a true romantic, a priestess of lived emotion. And she stayed true to that faith to the tragic end of her life.

Tsvetaeva's lyric poems fill ten collections; the uncollected lyrics would add at least another volume. Her first two collections indicate their subject matter in their titles: "Evening Album" (Vecherniy albom, 1910) and "The Magic Lantern" (Volshebnyi fonar, 1912). The poems are vignettes of a tranquil childhood and youth in a professorial, middle-class home in Moscow, and display considerable grasp of the formal elements of style. The full range of Tsvetaeva's talent developed quickly, and was undoubtedly influenced by the contacts she had made at Koktebel, and was made evident in two new collections: "Mileposts" (Versty, 1921) and "Mileposts: Book One" (Versty, Vypusk I, 1922).

Three elements of Tsvetaeva's mature style emerge in the "Mileposts" collections. First, Tsvetaeva dates her poems and publishes them chronologically. The poems in "Mileposts: Book One", for example, were written in 1916 and resolve themselves as a versified journal. Secondly, there are cycles of poems which fall into a regular chronological sequence among the single poems, evidence that certain themes demanded further expression and development. One cycle announces the theme of "Mileposts: Book One" as a whole: the "Poems of Moscow." Two other cycles are dedicated to poets, the "Poems to Akhmatova" and the "Poems to Blok", which again reappear in a separate volume, Poems to Blok ("Stikhi k Bloku", 1922). Thirdly, the "Mileposts" collections demonstrate the dramatic quality of Tsvetaeva's work, and her ability to assume the guise of multiple "dramatis personae" within them.

The collection "Separation" (Razluka, 1922) was to contain Tsvetaeva's first long verse narrative, "On a Red Steed" ("Na krasnom kone"). The poem is a prologue to three more verse-narratives written between 1920 and 1922. All four narrative poems draw on folkloric plots. Tsvetaeva acknowledges her sources in the titles of the very long works, "The Maiden Tsar: A Fairy-tale Poem" ("Tsar-devitsa: Poema-skazka", 1922) and "The Swain", subtitled "A Fairytale" ("Molodets: skazka", 1924). The fourth folklore-style poem is "Byways" ("Pereulochki", published in 1923 in the collection "Remeslo"), and it is the first poem which may be deemed incomprehensible in that it is fundamentally a soundscape of language. The collection "Psyche" ("Psikheya", 1923) contains one of Tsvetaeva's best-known cycles "Insomnia" (Bessonnitsa) and the poem The Swans' Encampment (Lebedinyi stan, Stikhi 1917–1921, published in 1957) which celebrates the White Army.

Tsvetaeva was so infatuated by the subject that she was looking for the topic in other poets writings and even used their lines as a base for her narrative, for example:

Subsequently, as an émigré, Tsvetaeva's last two collections of lyrics were published by émigré presses, "Craft" ("Remeslo", 1923) in Berlin and "After Russia" ("Posle Rossii", 1928) in Paris. There then followed the twenty-three lyrical "Berlin" poems, the pantheistic "Trees" ("Derev'ya"), "Wires" ("Provoda") and "Pairs" ("Dvoe"), and the tragic "Poets" ("Poety"). "After Russia" contains the poem "In Praise of the Rich", in which Tsvetaeva's oppositional tone is merged with her proclivity for ruthless satire.

In 1924, Tsvetaeva wrote "Poem of the End", which details a walk around Prague and across its bridges; the walk is about the final walk she will take with her lover Konstantin Rodzevich. In it everything is foretold: in the first few lines (translated by Elaine Feinstein) the future is already written:

Again, further poems foretell future developments. Principal among these is the voice of the classically oriented Tsvetaeva heard in cycles "The Sibyl", "Phaedra", and "Ariadne". Tsvetaeva's beloved, ill-starred heroines recur in two verse plays, "Theseus-Ariadne" (Tezei-Ariadna, 1927) and "Phaedra" (Fedra, 1928). These plays form the first two parts of an incomplete trilogy "Aphrodite's Rage".
The satirist in Tsvetaeva plays second fiddle only to the poet-lyricist. Several satirical poems, moreover, are among Tsvetaeva's best-known works: "The Train of Life" ("Poezd zhizni") and "The Floorcleaners' Song" ("Poloterskaya"), both included in After Russia, and The Ratcatcher (Krysolov, 1925–1926), a long, folkloric narrative. The target of Tsvetaeva's satire is everything petty and petty bourgeois. Unleashed against such dull creature comforts is the vengeful, unearthly energy of workers both manual and creative. In her notebook, Tsvetaeva writes of "The Floorcleaners' Song": "Overall movement: the floorcleaners ferret out a house's hidden things, they scrub a fire into the door... What do they flush out? Coziness, warmth, tidiness, order... Smells: incense, piety. Bygones. Yesterday... The growing force of their threat is far stronger than the climax." "The Ratcatcher" poem, which Tsvetaeva describes as a "lyrical satire", is loosely based on the legend of the Pied Piper of Hamelin. The Ratcatcher, which is also known as The Pied Piper, is considered by some to be the finest of Tsvetaeva's work. It was also partially an act of "homage" to Heinrich Heine's poem "Die Wanderratten". The Ratcatcher appeared initially, in serial format, in the émigré journal "" in 1925–1926 whilst still being written. It was not to appear in the Soviet Union until after the death of Joseph Stalin in 1956. Its hero is the Pied Piper of Hamelin who saves a town from hordes of rats and then leads the town's children away too, in retribution for the citizens' ingratitude. As in the other folkloric narratives, The Ratcatcher's story line emerges indirectly through numerous speaking voices which shift from invective, to extended lyrical flights, to pathos.
Tsvetaeva's last ten years of exile, from 1928 when "After Russia" appeared until her return in 1939 to the Soviet Union, were principally a "prose decade", though this would almost certainly be by dint of economic necessity rather than one of choice.

Translators of Tsvetaeva's work into English include Elaine Feinstein and David McDuff. Nina Kossman translated many of Tsvetaeva's long (narrative) poems, as well as her lyrical poems; they are collected in two books, "Poem of the End" and "In the Inmost Hour of the Soul". J. Marin King translated a great deal of Tsvetaeva's prose into English, compiled in a book called "A Captive Spirit". Tsvetaeva scholar Angela Livingstone has translated a number of Tsvetaeva's essays on art and writing, compiled in a book called "Art in the Light of Conscience". Livingstone's translation of Tsvetaeva's "The Ratcatcher" was published as a separate book. Mary Jane White has translated the early cycle "Miles" in a book called "Starry Sky to Starry Sky", as well as Tsvetaeva's elegy for Rilke, "New Year's", (Adastra Press 16 Reservation Road, Easthampton, MA 01027 USA) and "Poem of the End" (The Hudson Review, Winter 2009; and in the anthology Poets Translate Poets, Syracuse U. Press 2013) and "Poem of the Hill", (New England Review, Summer 2008) and Tsvetaeva's 1914–1915 cycle of love poems to Sophia Parnok. In 2002, Yale University Press published Jamey Gambrell's translation of post-revolutionary prose, entitled "Earthly Signs: Moscow Diaries, 1917–1922", with notes on poetic and linguistic aspects of Tsvetaeva's prose, and endnotes for the text itself.


The Soviet composer Dmitri Shostakovich set six of Tsvetaeva's poems to music. Later the Russian-Tatar composer Sofia Gubaidulina wrote an "Hommage à Marina Tsvetayeva" featuring her poems. Her poem "Mne Nravitsya..." ("I like that..."), was performed by Alla Pugacheva in the film "The Irony of Fate". In 2003, the opera "Marina: A Captive Spirit", based on Tsvetaeva's life and work, premiered from American Opera Projects in New York with music by Deborah Drattell and libretto by poet Annie Finch. The production was directed by Anne Bogart and the part of Tsvetaeva was sung by Lauren Flanigan. The poetry by Tsvetaeva was set to music and frequently performed as songs by Elena Frolova, Larisa Novoseltseva, Zlata Razdolina and other Russian bards.

On 8 October 2015, Google Doodle commemorated her 123rd birthday.





</doc>
<doc id="20188" url="https://en.wikipedia.org/wiki?curid=20188" title="Matilda of Tuscany">
Matilda of Tuscany

Matilda of Tuscany (Italian: "Matilde di Canossa" , Latin: "Matilda", "Mathilda"; 1046 – 24 July 1115) was a powerful feudal Margravine of Tuscany, ruler in northern Italy and the chief Italian supporter of Pope Gregory VII during the Investiture Controversy; in addition, she was one of the few medieval women to be remembered for her military accomplishments, thanks to which she was able to dominate all the territories north of the Papal States.

In 1076 she came into possession of a substantial territory that included present-day Lombardy, Emilia, the Romagna and Tuscany, and made the castle of Canossa, in the Apennines south of Reggio, the centre of her domains. Between 6 and 11 May 1111 she was crowned Imperial Vicar and Vice-Queen of Italy by Henry V, Holy Roman Emperor at the Castle of Bianello (Quattro Castella, Reggio Emilia).

Sometimes called la Gran Contessa ("the Great Countess") or Matilda of Canossa after her ancestral castle of Canossa, Matilda was one of the most important figures of the Italian Middle Ages. She lived in a period of constant battles, intrigues and excommunications, and was able to demonstrate an extraordinary force, even enduring great pain and humiliation, showing an innate leadership ability.

In a miniature in the early twelfth-century "Vita Mathildis" by the monk Donizo (or, in Italian, Donizone), Matilda is referred to as 'Resplendent Matilda' ("Mathildis Lucens"). Since the Latin word "lucens" is similar to "lucensis" (of/from Lucca), this may also be a reference to Matilda's origins. She was descended from the nobleman Sigifred of Lucca,
and was the youngest of the three children of Margrave Boniface III of Tuscany, ruler of a substantial territory in Northern Italy and one of the most powerful vassals of the Holy Roman Emperor Henry III. Matilda's mother, Beatrice of Lorraine, was the Emperor's first cousin and closely connected to the imperial household. Renowned for her learning, Matilda was literate in Latin, as well as reputed to speak German and French. The extent of Matilda's education in military matters is debated. It has been asserted that she was taught strategy, tactics, riding and wielding weapons, but recent scholarship challenges these claims.

Following the death of their father in 1052, Matilda's brother, Frederick, inherited the family lands and titles under the regency of their mother. Matilda's sister, Beatrice, died the next year, making Matilda heir presumptive to Frederick's personal holdings. In 1054, determined to safeguard the interests of her children as well as her own, her mother married Godfrey the Bearded, a distant kinsman who had been stripped of the Duchy of Upper Lorraine after openly rebelling against Emperor Henry III.

Henry was enraged by Beatrice of Lorraine's unauthorised union with his most vigorous adversary and took the opportunity to have her arrested, along with Matilda, when he marched south to attend a synod in Florence on Pentecost in 1055. Frederick's rather suspicious death soon thereafter made Matilda the last member of the House of Canossa. Mother and daughter were taken to Germany, but Godfrey successfully avoided capture. Unable to defeat him, Henry sought a rapproachment. The Emperor's death in October 1056, which brought to throne the underage Henry IV, seems to have accelerated the negotiations. Godfrey was reconciled with the crown and recognized as Margrave of Tuscany in December, while Beatrice and Matilda were released. By the time she and her mother returned to Italy, in the company of Pope Victor II, Matilda was formally acknowledged as heir to the greatest territorial lordship in the southern part of the Empire.

Matilda's mother and stepfather became heavily involved in the series of disputed papal elections during their regency, supporting the Gregorian Reforms. Godfrey's brother Frederick became Pope Stephen IX, while both of the following two popes, Nicholas II and Alexander II, had been Tuscan bishops. Matilda made her first journey to Rome with her family in the entourage of Nicholas in 1059. Godfrey and Beatrice actively assisted them in dealing with antipopes, while the adolescent Matilda's role remains unclear. A contemporary account of her stepfather's 1067 expedition against Prince Richard I of Capua on behalf of the papacy mentions Matilda's participation in the campaign, describing it as the "first service that the most excellent daughter of Boniface offered to the blessed prince of the apostles."

In 1069, as Godfrey the Bearded lay dying in Verdun, Beatrice and Matilda hastened to reach Lorraine, anxious to ensure a smooth transition of power. Matilda was present at her stepfather's deathbed, and on that occasion she is for the first time clearly mentioned as the wife of her stepbrother, Godfrey the Hunchback, to whom she had been betrothed since childhood. The marriage proved a failure; the death of their only child (a daughter called Beatrice) shortly after birth in August 1071 and Godfrey's physical deformity may have helped fuel deep animosity between the spouses.

By the end of 1071, Matilda had left her husband and returned to Tuscany. Matilda's bold decision to repudiate her husband came at a cost, but ensured her independence. Beatrice started preparing Matilda for rule by holding court jointly with her and, eventually, encouraging her to issue charters on her own as countess ("comitissa") and duchess ("ducatrix").

Godfrey fiercely protested the separation and demanded that Matilda come back to him, which she repeatedly refused. The Duke descended into Italy in 1072, determined to enforce the marriage. He sought the help of both Matilda's mother and her ally, the newly elected Pope Gregory VII, promising military aid to the latter. Matilda's resolution was unshakable, and Godfrey returned to Lorraine alone, losing all hope by 1074. Rather than supporting the Pope as promised, Godfrey turned his attention to imperial affairs. Meanwhile, the conflict later known as the Investiture Controversy was brewing between Gregory and Henry, with both men claiming the right to appoint bishops and abbots within the Empire. Matilda and Godfrey soon found themselves on opposing sides of the dispute, leading to a further detoriation of their difficult relationship. German chroniclers, writing of the synod held at Worms in January 1076, even suggested that Godfrey inspired Henry's allegation of a licentious affair between Gregory and Matilda.

Matilda became a widow on 26 February 1076. Godfrey the Hunchback was assassinated in Flanders while "answering the call of nature". Having been accused of adultery with the Pope the previous month, Matilda was suspected of ordering her estranged husband's death. She could not have known about the proceedings at the Synod of Worms at the time, however, since the news took three months to reach the Pope himself, and it is more likely that Godfrey was killed at the instigation of an enemy nearer to him. Within two months, Beatrice was dead as well. Matilda's power was considerably augmented by these deaths; she was now the undisputed heir of all her parents' allodial lands. Her inheritance would have been threatened had Godfrey survived her mother, but she now enjoyed the privileged status of a widow. It seemed unlikely, however, that Henry would formally invest her with the margraviate.

Between 1076 and 1080, Matilda travelled to Lorraine to lay claim to her husband's estate in Verdun, which he had willed (along with the rest of his patrimony) to his sister Ida's son, Godfrey of Bouillon. Godfrey of Bouillon also disputed her right to Stenay and Mosay, which her mother had received as dowry. The quarrel between aunt and nephew over the episcopal county of Verdun was eventually settled by Theoderic, Bishop of Verdun, who enjoyed the right to nominate the counts. He easily found in favor of Margravine Matilda, as such verdict happened to please both Pope Gregory and King Henry. Matilda then proceeded to enfeoff Verdun to her husband's pro-reform cousin, Albert III of Namur. The deep animosity between Matilda and her nephew is thought to have prevented her from travelling to Jerusalem during the First Crusade, led by him in the late 1090s.

The disagreement between Pope Gregory VII and King Henry IV culminated in the aftermath of the Synod of Worms in February 1076. Gregory declared Henry excommunicated, releasing all his subjects from allegiance to him and providing the perfect reason for rebellion against his rule. Insubordinate southern German princes gathered in Trebur, awaiting the Pope. Matilda's first military endeavor, as well as the first major task altogether as ruler, turned out to be protecting the Pope during his perilous journey north. Gregory could rely on nobody else; as the sole heir to the Attonid patrimony, Matilda controlled all the Apennine passes and nearly all the rest that connected central Italy to the north. The Lombard bishops, who were also excommunicated for taking part in the synod and whose sees bordered Matilda's domain, were keen to capture Gregory. Gregory was aware of the danger, and recorded that all his advisors except Matilda counselled him against travelling to Trebur.

Henry had other plans, however. He decided to descend into Italy and intercept Gregory, who was thus delayed. The German dukes held a council by themselves and informed the King that he had to submit to the Pope or be replaced. Henry's predecessors dealt easily with troublesome pontiffs - they simply deposed them, and the excommunicated Lombard bishops rejoiced at this prospect. When Matilda heard about Henry's approach, she urged Gregory to take refuge in the Castle of Canossa, her family's eponymous stronghold. Gregory took her advice. It soon became clear that the intention behind Henry's walk to Canossa was to show penance. By 25 January 1077, the King stood barefoot in the snow before the gates of Matilda's castle, accompanied by his mother-in-law, Margravine Adelaide of Susa. He remained there, humbled, until 28 January, when Matilda convinced the Pope to see him. Matilda and Adelaide brokered a deal between the men. Henry was taken back into the Church, with the margravines acting as sponsors and formally swearing to the agreement.

In 1079, Matilda gave the Pope all her domains, in open defiance of Henry IV's claims both as the overlord of some of those domains, and as her close relative. Two years later the fortunes of Papacy and Empire turned again: in 1080 Henry IV summoned a council in Brixen, which deposed Gregory VII. The following year the Emperor decided to travel again to Italy to reinstate his overlordship over his territories. He also declared Matilda, on account of her 1079 donation to the Church, forfeit and be banned from the Empire; although this wasn't enough to eliminate her as a source of trouble, for she retained substantial allodial holdings. On 15 October 1080 near Volta Mantovana the Imperial troops (with Guibert of Ravenna as the newly elected Antipope Clement III) defeated the troops loyal to Gregory VII and controlled by Matilda. This was the first serious military defeat of Matilda (Battle of Volta Mantovana).

Matilda, however, didn't surrender. While Gregory VII was forced into exile, she, retaining control over all the western passes in the Apennines, could force Henry IV to approach Rome via Ravenna; even with this route open, the Emperor would find it hard to besiege Rome with a hostile territory at his back. In December 1080 the citizens of Lucca, then the capital of Tuscany, had revolted and driven out her ally Bishop Anselm. She is believed to have commissioned the renowned Ponte della Maddalena where the Via Francigena crosses the river Serchio at Borgo a Mozzano just north of Lucca.

Matilda remained Pope Gregory VII's chief intermediary for communication with northern Europe even as he lost control of Rome and was holed up in the Castel Sant'Angelo. After Henry caught hold of the Pope's seal, Matilda wrote to supporters in Germany only to trust papal messages that came through her.

Henry IV's control of Rome enabled him to enthrone Antipope Clement III, who, in turn, crowned him Emperor. After this, Henry IV returned to Germany, leaving it to his allies to attempt Matilda's dispossession. These attempts floundered after Matilda (with help of the city of Bologna) defeated them at Sorbara near Modena on 2 July 1084.

Gregory VII died in 1085, and Matilda's forces, with those of Prince Jordan I of Capua (her off and on again enemy), took to the field in support of a new pope, Victor III. In 1087, Matilda led an expedition to Rome in an attempt to install Victor, but the strength of the imperial counterattack soon convinced the pope to withdraw from the city.

In 1088 Matilda was facing a new attempt at invasion by Henry IV, and decided to pre-empt it by means of a political marriage. In 1089 Matilda (in her early forties) married Welf V, who was probably fifteen to seventeen years old. Welf was heir to the Duchy of Bavaria. He was also a member of the Welf dynasty: the Welfs/Guelphs were important papal supporters from the eleventh to the fifteenth centuries in their conflict with the German emperors (see Guelphs and Ghibellines). Matilda and Welf's wedding was part of a network of alliances approved by the new pope, Urban II, in order to effectively counter Henry IV.

Cosmas of Prague (writing in the early twelfth century), included a letter in his "Chronica Boemorum", which he claimed that Matilda sent to her future husband, but which is now thought to be spurious:

After this, Matilda sent an army of thousands to the border of Lombardy to escort her bridegroom, welcomed him with honors, and after the marriage (mid-1089), she organized 120 days of wedding festivities, with such splendor that any other medieval ruler's pale in comparison.

Cosmas also reports that for two nights after the wedding, Welf V, fearing witchcraft, refused to share the marital bed. The third day, Matilda appeared naked on a table especially prepared on sawhorses, and told him that "everything is in front of you and there is no hidden malice". But the Duke was dumbfounded; Matilda, furious, slapped him and spat in his face, taunting him: "Get out of here, monster, you don't deserve our kingdom, you vile thing, viler than a worm or a rotten seaweed, don't let me see you again, or you'll die a miserable death...". Matilda and her young husband separated a few years later (1095); they had no children.

Later Matilda allied with the two sons of Henry IV, Conrad and Henry, who rebelled against their father. This forced Henry to return to Italy, where he chased Matilda into the mountains. He was humbled before Canossa, this time in a military defeat in October 1092, from which his influence in Italy never recovered.

After several victories, including one against the Saxons, Henry IV prepared in 1090 his third descent to Italy, in order to inflict the final defeat on the Church. His route was the usual one, Brenner and Verona, along the border of Matilda's possessions, which began outside the cities' gates. The opposing armies would meet near Mantua. Matilda secured the loyalty of the townspeople by exempting them from some taxes, such as "teloneo" and "ripatico", and with the promise of Lombard franchise, entailing the rights to hunt, fish and cut wood on both banks of the Tartaro river.

The Mantua people stood by Matilda until the so-called "Holy Thursday betrayal", when the townspeople, won over by additional concessions from Henry, who had meanwhile besieged the city, sided with him. In 1092 Matilda escaped to the Reggiano Apennines and her most inexpugnable strongholds. Since the times of Adalbert Atto the power of the Canossa family had been based on a network of castles, fortresses and fortified villages in the Val d'Enza, forming a complex polygonal defense that had always resisted all attack from the Apennines. After several bloody battles with mutual defeats, the powerful imperial army was surrounded.

In spite of its fearful power, the Imperial army was defeated by Matilda's liegemen. Among them were small landowners and holders of fortified villages, which remained completely loyal to the Canossas even against the Holy Roman Emperor. Their familiarity with the territory, their quick communications and maneuvering to all the high places of the Val d'Enza gave them victory over Henry's might. It seems that Matilda personally participated, with a handful of chosen faithful men, to the battle, galvanizing the allies with the cry of Just War. The Imperial army was taken as in a vice in the meandering mountain creek. The overall import of Henry's rout was more than a military defeat. The Emperor realized it was impossible to penetrate those places, wholly different from the plains of the Po Valley or of Saxe. There he faced not boundaries drawn by the rivers of Central Europe, but steep trails, ravines, inaccessible places protecting Matilda's fortresses, and high tower houses, whence the defenders could unload on anyone approaching missiles of all kinds: spears, arrows, perhaps even boiling oil, javelins, stones.

After Matilda's victory several cities, such as Milan, Cremona, Lodi and Piacenza, sided with her to free themselves of Imperial rule. In 1093 the Emperor's eldest son, Conrad, supported by the Pope, Matilda and a group of Lombard cities, was crowned King of Italy. Matilda freed and even gave refuge to Henry IV's wife, Eupraxia of Kiev, who, at the urging of Pope Urban II, made a public confession before the church Council of Piacenza. She accused her husband of imprisoning her in Verona after forcing her to participate in orgies, and, according to some later accounts, of attempting a black mass on her naked body. Thanks to these scandals and division within the Imperial family, the prestige and power of Henry IV was increasingly weakened.

In 1095, Henry attempted to reverse his fortunes by seizing Matilda's castle of Nogara, but the countess's arrival at the head of an army forced him to retreat. In 1097, Henry withdrew from Italy altogether, after which Matilda reigned virtually unchallenged, although she did continue to launch military operations to restore her authority and regain control of the towns that had remained loyal to the emperor. With the assistance of the French armies heading off to the First Crusade, she was finally able to restore Urban to Rome. She ordered or led successful expeditions against Ferrara (1101), Parma (1104), Prato (1107) and Mantua (1114).

Henry IV died now defeated in 1106; and after the deposition and death of Conrad (1101), his second son and new Holy Roman Emperor, Henry V, began to turn the fight against the Church and Italy. This time the attitude of Matilda against the imperial house had to change and she accepted the will of the Emperor. In 1111, on his way back to Germany, Henry V met her at the Castle of Bianello, near Reggio Emilia. Matilda confirmed him the inheritance rights over the fiefs that Henry IV disputed her, thus ending a fight that had lasted over twenty years. Henry V gave Matilda a new title: between 6 and 11 May 1111, the Emperor crowned Matilda as Imperial Vicar and Vice-Queen of Italy. This episode was the decisive step towards the Concordat of Worms.

By legend Matilda of Canossa is said to have founded one hundred churches. Documents and local legend identify well over one hundred churches, monasteries, hospices, and bridges built or restored between the Alps and Rome by Matilda and her mother, Beatrice. Today, churches and monasteries in the regions of Lombardy, Reggio Emilia, Tuscany, and even the Veneto attribute their foundation to her. Built originally with hospices for travelers attached, these churches created a network that united the supporters of the Gregorian reform of the Roman Church which Matilda supported. This network also provided protection for pilgrims, merchants and travelers assisting the Renaissance in culture that occurred in the centuries after Matilda's death.

Most of these churches continue today to be vital centers of their communities. They include rural churches located along the Po and Arno rivers, and their tributaries; churches built along the Apennine mountain passes which Matilda's family controlled and those along the ancient highways of the via Emilia, the via Cassia, the via Aurelia and the via Francigena. Among these are monuments listed by UNESCO as among the heritage of our world, including churches in Florence, Ferrara, Lucca, Mantua, Modena, Pisa, Verona and Volterra. Her cultural legacy is enormous throughout Northern Italy.

Some churches traditionally said to have been founded by Matilda include:


It seems that even the foundation of the Church of San Salvaro in Legnago (Verona) was made by Matilda.

Matilda's death from gout in 1115 at Bondeno di Roncore marked the end of an era in Italian politics. It is widely reported that she bequeathed her allodial property to the Pope. Unaccountably, however, this donation was never officially recognized in Rome and no record exists of it. Henry V had promised some of the cities in her territory that he would appoint no successor after he deposed her. In her place the leading citizens of these cities took control, and the era of the city-states in northern Italy began.

Matilda was at first buried in the Abbey of San Benedetto in Polirone, located in the town of San Benedetto Po; then, in 1633, at the behest of Pope Urban VIII, her body was moved to Rome and placed in Castel Sant'Angelo. Finally, in 1645 her remains were definitely deposited in the Vatican, where they now lie in St. Peter's Basilica. She is one of only six women who have the honor of being buried in the Basilica, the others being Queen Christina of Sweden, Maria Clementina Sobieska (wife of James Francis Edward Stuart), St. Petronilla, Queen Charlotte of Cyprus and Agnesina Colonna Caetani.

A memorial tomb for Matilda, commissioned by Pope Urban VIII and designed by Gianlorenzo Bernini, marks her burial place in St Peter's and is often called the "Honor and Glory of Italy".

After her death, an aura of legend came to surround Matilda. Church historians gave her the character of a semi-nun, solely dedicated to contemplation and faith. Some argue, instead, that she was a woman of strong passions of both spiritual and carnal nature (indicated by her supposed affairs with Popes Gregory VII and Urban II).

She has been posited by some critics as the origin of the mysterious "Matilda" who appears to Dante gathering flowers in the earthly paradise in Dante's "Purgatorio".

The story of Matilda and Henry IV is the main plot device in Luigi Pirandello's play "Enrico IV". She is the main historical character in Kathleen McGowan's novel "The Book of Love" (Simon & Schuster, 2009).





</doc>
<doc id="20189" url="https://en.wikipedia.org/wiki?curid=20189" title="Mesopotamia">
Mesopotamia

Mesopotamia (, "") is a historical region of Western Asia situated within the Tigris–Euphrates river system, in the northern part of the Fertile Crescent, in modern days roughly corresponding to most of Iraq, Kuwait, the eastern parts of Syria, Southeastern Turkey, and regions along the Turkish–Syrian and Iran–Iraq borders.

The Sumerians and Akkadians (including Assyrians and Babylonians) dominated Mesopotamia from the beginning of written history (c. 3100 BC) to the fall of Babylon in 539 BC, when it was conquered by the Achaemenid Empire. It fell to Alexander the Great in 332 BC, and after his death, it became part of the Greek Seleucid Empire.

Around 150 BC, Mesopotamia was under the control of the Parthian Empire. Mesopotamia became a battleground between the Romans and Parthians, with western parts of Mesopotamia coming under ephemeral Roman control. In AD 226, the eastern regions of Mesopotamia fell to the Sassanid Persians. The division of Mesopotamia between Roman (Byzantine from AD 395) and Sassanid Empires lasted until the 7th century Muslim conquest of Persia of the Sasanian Empire and Muslim conquest of the Levant from Byzantines. A number of primarily neo-Assyrian and Christian native Mesopotamian states existed between the 1st century BC and 3rd century AD, including Adiabene, Osroene, and Hatra.

Mesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BC. It has been identified as having "inspired some of the most important developments in human history, including the invention of the wheel, the planting of the first cereal crops and the development of cursive script, mathematics, astronomy and agriculture".

The regional toponym "Mesopotamia" (, "[land] between rivers"; ' or '; "miyân rudân"; "Beth Nahrain" "land of rivers") comes from the ancient Greek root words μέσος ("mesos") "middle" and ποταμός ("potamos") "river" and translates to "(land) between rivers". It is used throughout the Greek Septuagint (c. 250 BC) to translate the Hebrew and Aramaic equivalent "Naharaim". An even earlier Greek usage of the name "Mesopotamia" is evident from "The Anabasis of Alexander", which was written in the late 2nd century AD, but specifically refers to sources from the time of Alexander the Great. In the "Anabasis", Mesopotamia was used to designate the land east of the Euphrates in north Syria.

The Aramaic term "biritum/birit narim" corresponded to a similar geographical concept. Later, the term Mesopotamia was more generally applied to all the lands between the Euphrates and the Tigris, thereby incorporating not only parts of Syria but also almost all of Iraq and southeastern Turkey. The neighbouring steppes to the west of the Euphrates and the western part of the Zagros Mountains are also often included under the wider term Mesopotamia.

A further distinction is usually made between Northern or Upper Mesopotamia and Southern or Lower Mesopotamia. Upper Mesopotamia, also known as the "Jazira", is the area between the Euphrates and the Tigris from their sources down to Baghdad. Lower Mesopotamia is the area from Baghdad to the Persian Gulf and includes Kuwait and parts of western Iran.

In modern academic usage, the term "Mesopotamia" often also has a chronological connotation. It is usually used to designate the area until the Muslim conquests, with names like "Syria", "Jazira", and "Iraq" being used to describe the region after that date. It has been argued that these later euphemisms are Eurocentric terms attributed to the region in the midst of various 19th-century Western encroachments.

Mesopotamia encompasses the land between the Euphrates and Tigris rivers, both of which have their headwaters in the Taurus Mountains. Both rivers are fed by numerous tributaries, and the entire river system drains a vast mountainous region. Overland routes in Mesopotamia usually follow the Euphrates because the banks of the Tigris are frequently steep and difficult. The climate of the region is semi-arid with a vast desert expanse in the north which gives way to a region of marshes, lagoons, mud flats, and reed banks in the south. In the extreme south, the Euphrates and the Tigris unite and empty into the Persian Gulf.

The arid environment which ranges from the northern areas of rain-fed agriculture to the south where irrigation of agriculture is essential if a surplus energy returned on energy invested (EROEI) is to be obtained. This irrigation is aided by a high water table and by melting snows from the high peaks of the northern Zagros Mountains and from the Armenian Highlands, the source of the Tigris and Euphrates Rivers that give the region its name. The usefulness of irrigation depends upon the ability to mobilize sufficient labor for the construction and maintenance of canals, and this, from the earliest period, has assisted the development of urban settlements and centralized systems of political authority.

Agriculture throughout the region has been supplemented by nomadic pastoralism, where tent-dwelling nomads herded sheep and goats (and later camels) from the river pastures in the dry summer months, out into seasonal grazing lands on the desert fringe in the wet winter season. The area is generally lacking in building stone, precious metals and timber, and so historically has relied upon long-distance trade of agricultural products to secure these items from outlying areas. In the marshlands to the south of the area, a complex water-borne fishing culture has existed since prehistoric times, and has added to the cultural mix.

Periodic breakdowns in the cultural system have occurred for a number of reasons. The demands for labor has from time to time led to population increases that push the limits of the ecological carrying capacity, and should a period of climatic instability ensue, collapsing central government and declining populations can occur. Alternatively, military vulnerability to invasion from marginal hill tribes or nomadic pastoralists has led to periods of trade collapse and neglect of irrigation systems. Equally, centripetal tendencies amongst city states has meant that central authority over the whole region, when imposed, has tended to be ephemeral, and localism has fragmented power into tribal or smaller regional units. These trends have continued to the present day in Iraq.

The pre-history of the Ancient Near East begins in the Lower Paleolithic period. Therein, writing emerged with a pictographic script in the Uruk IV period (c. 4th millennium BC), and the documented record of actual historical events — and the ancient history of lower Mesopotamia — commenced in the mid-third millennium BC with cuneiform records of early dynastic kings. This entire history ends with either the arrival of the Achaemenid Empire in the late 6th century BC, or with the Muslim conquest and the establishment of the Caliphate in the late 7th century AD, from which point the region came to be known as Iraq. In the long span of this period, Mesopotamia housed some of the world's most ancient highly developed and socially complex states.

The region was one of the four riverine civilizations where writing was invented, along with the Nile valley in Ancient Egypt, the Indus Valley Civilization in the Indian subcontinent, and the Yellow River in Ancient China. Mesopotamia housed historically important cities such as Uruk, Nippur, Nineveh, Assur and Babylon, as well as major territorial states such as the city of Eridu, the Akkadian kingdoms, the Third Dynasty of Ur, and the various Assyrian empires. Some of the important historical Mesopotamian leaders were Ur-Nammu (king of Ur), Sargon of Akkad (who established the Akkadian Empire), Hammurabi (who established the Old Babylonian state), Ashur-uballit II and Tiglath-Pileser I (who established the Assyrian Empire).

Scientists analysed DNA from the 8,000-year-old remains of early farmers found at an ancient graveyard in Germany. They compared the genetic signatures to those of modern populations and found similarities with the DNA of people living in today's Turkey and Iraq.


The earliest language written in Mesopotamia was Sumerian, an agglutinative language isolate. Along with Sumerian, Semitic languages were also spoken in early Mesopotamia. Subartuan a language of the Zagros, perhaps related to the Hurro-Urartuan language family is attested in personal names, rivers and mountains and in various crafts. Akkadian came to be the dominant language during the Akkadian Empire and the Assyrian empires, but Sumerian was retained for administrative, religious, literary and scientific purposes. Different varieties of Akkadian were used until the end of the Neo-Babylonian period. Old Aramaic, which had already become common in Mesopotamia, then became the official provincial administration language of first the Neo-Assyrian Empire, and then the Achaemenid Empire: the official lect is called Imperial Aramaic. Akkadian fell into disuse, but both it and Sumerian were still used in temples for some centuries. The last Akkadian texts date from the late 1st century AD.

Early in Mesopotamia's history (around the mid-4th millennium BC) cuneiform was invented for the Sumerian language. Cuneiform literally means "wedge-shaped", due to the triangular tip of the stylus used for impressing signs on wet clay. The standardized form of each cuneiform sign appears to have been developed from pictograms. The earliest texts (7 archaic tablets) come from the É, a temple dedicated to the goddess Inanna at Uruk, from a building labeled as Temple C by its excavators.

The early logographic system of cuneiform script took many years to master. Thus, only a limited number of individuals were hired as scribes to be trained in its use. It was not until the widespread use of a syllabic script was adopted under Sargon's rule that significant portions of the Mesopotamian population became literate. Massive archives of texts were recovered from the archaeological contexts of Old Babylonian scribal schools, through which literacy was disseminated.

During the third millennium BC, there developed a very intimate cultural symbiosis between the Sumerian and the Akkadian language users, which included widespread bilingualism. The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence. This has prompted scholars to refer to Sumerian and Akkadian in the third millennium as a sprachbund. Akkadian gradually replaced Sumerian as the spoken language of Mesopotamia somewhere around the turn of the 3rd and the 2nd millennium BC (the exact dating being a matter of debate), but Sumerian continued to be used as a sacred, ceremonial, literary, and scientific language in Mesopotamia until the 1st century AD.

Libraries were extant in towns and temples during the Babylonian Empire. An old Sumerian proverb averred that "he who would excel in the school of the scribes must rise with the dawn." Women as well as men learned to read and write, and for the Semitic Babylonians, this involved knowledge of the extinct Sumerian language, and a complicated and extensive syllabary.

A considerable amount of Babylonian literature was translated from Sumerian originals, and the language of religion and law long continued to be the old agglutinative language of Sumer. Vocabularies, grammars, and interlinear translations were compiled for the use of students, as well as commentaries on the older texts and explanations of obscure words and phrases. The characters of the syllabary were all arranged and named, and elaborate lists were drawn up.

Many Babylonian literary works are still studied today. One of the most famous of these was the Epic of Gilgamesh, in twelve books, translated from the original Sumerian by a certain Sîn-lēqi-unninni, and arranged upon an astronomical principle. Each division contains the story of a single adventure in the career of Gilgamesh. The whole story is a composite product, although it is probable that some of the stories are artificially attached to the central figure.

Mesopotamian mathematics and science was based on a sexagesimal (base 60) numeral system. This is the source of the 60-minute hour, the 24-hour day, and the 360-degree circle. The Sumerian calendar was based on the seven-day week. This form of mathematics was instrumental in early map-making. The Babylonians also had theorems on how to measure the area of several shapes and solids. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if were fixed at 3. The volume of a cylinder was taken as the product of the area of the base and the height; however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. Also, there was a recent discovery in which a tablet used as 25/8 (3.125 instead of 3.14159~). The Babylonians are also known for the Babylonian mile, which was a measure of distance equal to about seven modern miles (11 km). This measurement for distances eventually was converted to a time-mile used for measuring the travel of the Sun, therefore, representing time.

From Sumerian times, temple priesthoods had attempted to associate current events with certain positions of the planets and stars. This continued to Assyrian times, when Limmu lists were created as a year by year association of events with planetary positions, which, when they have survived to the present day, allow accurate associations of relative with absolute dating for establishing the history of Mesopotamia.

The Babylonian astronomers were very adept at mathematics and could predict eclipses and solstices. Scholars thought that everything had some purpose in astronomy. Most of these related to religion and omens. Mesopotamian astronomers worked out a 12-month calendar based on the cycles of the moon. They divided the year into two seasons: summer and winter. The origins of astronomy as well as astrology date from this time.

During the 8th and 7th centuries BC, Babylonian astronomers developed a new approach to astronomy. They began studying philosophy dealing with the ideal nature of the early universe and began employing an internal logic within their predictive planetary systems. This was an important contribution to astronomy and the philosophy of science and some scholars have thus referred to this new approach as the first scientific revolution. This new approach to astronomy was adopted and further developed in Greek and Hellenistic astronomy.

In Seleucid and Parthian times, the astronomical reports were thoroughly scientific; how much earlier their advanced knowledge and methods were developed is uncertain. The Babylonian development of methods for predicting the motions of the planets is considered to be a major episode in the history of astronomy.

The only Greek-Babylonian astronomer known to have supported a heliocentric model of planetary motion was Seleucus of Seleucia (b. 190 BC). Seleucus is known from the writings of Plutarch. He supported Aristarchus of Samos' heliocentric theory where the Earth rotated around its own axis which in turn revolved around the Sun. According to Plutarch, Seleucus even proved the heliocentric system, but it is not known what arguments he used (except that he correctly theorized on tides as a result of Moon's attraction).

Babylonian astronomy served as the basis for much of Greek, classical Indian, Sassanian, Byzantine, Syrian, medieval Islamic, Central Asian, and Western European astronomy.

The oldest Babylonian texts on medicine date back to the Old Babylonian period in the first half of the 2nd millennium BC. The most extensive Babylonian medical text, however, is the "Diagnostic Handbook" written by the "ummânū", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069-1046 BC).

Along with contemporary Egyptian medicine, the Babylonians introduced the concepts of diagnosis, prognosis, physical examination, enemas, and prescriptions. In addition, the "Diagnostic Handbook" introduced the methods of therapy and aetiology and the use of empiricism, logic, and rationality in diagnosis, prognosis and therapy. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.

The symptoms and diseases of a patient were treated through therapeutic means such as bandages, creams and pills. If a patient could not be cured physically, the Babylonian physicians often relied on exorcism to cleanse the patient from any curses. Esagil-kin-apli's "Diagnostic Handbook" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its aetiology, its future development, and the chances of the patient's recovery.

Esagil-kin-apli discovered a variety of illnesses and diseases and described their symptoms in his "Diagnostic Handbook". These include the symptoms for many varieties of epilepsy and related ailments along with their diagnosis and prognosis.

Mesopotamian people invented many technologies including metal and copper-working, glass and lamp making, textile weaving, flood control, water storage, and irrigation. They were also one of the first Bronze Age societies in the world. They developed from copper, bronze, and gold on to iron. Palaces were decorated with hundreds of kilograms of these very expensive metals. Also, copper, bronze, and iron were used for armor as well as for different weapons such as swords, daggers, spears, and maces.

According to a recent hypothesis, the Archimedes' screw may have been used by Sennacherib, King of Assyria, for the water systems at the Hanging Gardens of Babylon and Nineveh in the 7th century BC, although mainstream scholarship holds it to be a Greek invention of later times. Later, during the Parthian or Sasanian periods, the Baghdad Battery, which may have been the world's first battery, was created in Mesopotamia.

Ancient Mesopotamian religion was the first recorded. Mesopotamians believed that the world was a flat disc, surrounded by a huge, holed space, and above that, heaven. They also believed that water was everywhere, the top, bottom and sides, and that the universe was born from this enormous sea. In addition, Mesopotamian religion was polytheistic. Although the beliefs described above were held in common among Mesopotamians, there were also regional variations. The Sumerian word for universe is an-ki, which refers to the god An and the goddess Ki. Their son was Enlil, the air god. They believed that Enlil was the most powerful god. He was the chief god of the pantheon. The Sumerians also posed philosophical questions, such as: Who are we?, Where are we?, How did we get here?. They attributed answers to these questions to explanations provided by their gods.

The numerous civilizations of the area influenced the Abrahamic religions, especially the Hebrew Bible; its cultural values and literary influence are especially evident in the Book of Genesis.

Giorgio Buccellati believes that the origins of philosophy can be traced back to early Mesopotamian wisdom, which embodied certain philosophies of life, particularly ethics, in the forms of dialectic, dialogues, epic poetry, folklore, hymns, lyrics, prose works, and proverbs. Babylonian reason and rationality developed beyond empirical observation.

The earliest form of logic was developed by the Babylonians, notably in the rigorous nonergodic nature of their social systems. Babylonian thought was axiomatic and is comparable to the "ordinary logic" described by John Maynard Keynes. Babylonian thought was also based on an open-systems ontology which is compatible with ergodic axioms. Logic was employed to some extent in Babylonian astronomy and medicine.

Babylonian thought had a considerable influence on early Ancient Greek and Hellenistic philosophy. In particular, the Babylonian text "Dialogue of Pessimism" contains similarities to the agonistic thought of the Sophists, the Heraclitean doctrine of dialectic, and the dialogs of Plato, as well as a precursor to the Socratic method. The Ionian philosopher Thales was influenced by Babylonian cosmological ideas.

Ancient Mesopotamians had ceremonies each month. The theme of the rituals and festivals for each month was determined by at least six important factors:


Some songs were written for the gods but many were written to describe important events. Although music and songs amused kings, they were also enjoyed by ordinary people who liked to sing and dance in their homes or in the marketplaces. Songs were sung to children who passed them on to their children. Thus songs were passed on through many generations as an oral tradition until writing was more universal. These songs provided a means of passing on through the centuries highly important information about historical events.

The Oud (Arabic:العود) is a small, stringed musical instrument used by the Mesopotamians. The oldest pictorial record of the Oud dates back to the Uruk period in Southern Mesopotamia over 5000 years ago. It is on a cylinder seal currently housed at the British Museum and acquired by Dr. Dominique Collon. The image depicts a female crouching with her instruments upon a boat, playing right-handed. This instrument appears hundreds of times throughout Mesopotamian history and again in ancient Egypt from the 18th dynasty onwards in long- and short-neck varieties. The oud is regarded as a precursor to the European lute. Its name is derived from the Arabic word العود al-‘ūd 'the wood', which is probably the name of the tree from which the oud was made. (The Arabic name, with the definite article, is the source of the word 'lute'.)

Hunting was popular among Assyrian kings. Boxing and wrestling feature frequently in art, and some form of polo was probably popular, with men sitting on the shoulders of other men rather than on horses. They also played "majore", a game similar to the sport rugby, but played with a ball made of wood. They also played a board game similar to senet and backgammon, now known as the "Royal Game of Ur".

Mesopotamia, as shown by successive law codes, those of Urukagina, Lipit Ishtar and Hammurabi, across its history became more and more a patriarchal society, one in which the men were far more powerful than the women. For example, during the earliest Sumerian period, the ""en"", or high priest of male gods was originally a woman, that of female goddesses, a man. Thorkild Jacobsen, as well as many others, has suggested that early Mesopotamian society was ruled by a "council of elders" in which men and women were equally represented, but that over time, as the status of women fell, that of men increased. As for schooling, only royal offspring and sons of the rich and professionals, such as scribes, physicians, temple administrators, went to school. Most boys were taught their father's trade or were apprenticed out to learn a trade. Girls had to stay home with their mothers to learn housekeeping and cooking, and to look after the younger children. Some children would help with crushing grain or cleaning birds. Unusually for that time in history, women in Mesopotamia had rights. They could own property and, if they had good reason, get a divorce.

Hundreds of graves have been excavated in parts of Mesopotamia, revealing information about Mesopotamian burial habits. In the city of Ur, most people were buried in family graves under their houses, along with some possessions. A few have been found wrapped in mats and carpets. Deceased children were put in big "jars" which were placed in the family chapel. Other remains have been found buried in common city graveyards. 17 graves have been found with very precious objects in them. It is assumed that these were royal graves. Rich of various periods, have been discovered to have sought burial in Bahrein, identified with Sumerian Dilmun.

Irrigated agriculture spread southwards from the Zagros foothills with the Samara and Hadji Muhammed culture, from about 5,000 BC. Sumerian temples functioned as banks and developed the first large-scale system of loans and credit, but the Babylonians developed the earliest system of commercial banking. It was comparable in some ways to modern post-Keynesian economics, but with a more "anything goes" approach.

In the early period down to Ur III temples owned up to one third of the available land, declining over time as royal and other private holdings increased in frequency. The word Ensi was used to describe the official who organized the work of all facets of temple agriculture. Villeins are known to have worked most frequently within agriculture, especially in the grounds of temples or palaces.

The geography of southern Mesopotamia is such that agriculture is possible only with irrigation and good drainage, a fact which has had a profound effect on the evolution of early Mesopotamian civilization. The need for irrigation led the Sumerians, and later the Akkadians, to build their cities along the Tigris and Euphrates and the branches of these rivers. Major cities, such as Ur and Uruk, took root on tributaries of the Euphrates, while others, notably Lagash, were built on branches of the Tigris. The rivers provided the further benefits of fish (used both for food and fertilizer), reeds, and clay (for building materials). With irrigation, the food supply in Mesopotamia was comparable to the Canadian prairies.

The Tigris and Euphrates River valleys form the northeastern portion of the Fertile Crescent, which also included the Jordan River valley and that of the Nile. Although land nearer to the rivers was fertile and good for crops, portions of land farther from the water were dry and largely uninhabitable. This is why the development of irrigation was very important for settlers of Mesopotamia. Other Mesopotamian innovations include the control of water by dams and the use of aqueducts. Early settlers of fertile land in Mesopotamia used wooden plows to soften the soil before planting crops such as barley, onions, grapes, turnips, and apples. Mesopotamian settlers were some of the first people to make beer and wine. As a result of the skill involved in farming in the Mesopotamian, farmers did not depend on slaves to complete farm work for them, but there were some exceptions. There were too many risks involved to make slavery practical (i.e. the escape/mutiny of the slave). Although the rivers sustained life, they also destroyed it by frequent floods that ravaged entire cities. The unpredictable Mesopotamian weather was often hard on farmers; crops were often ruined so backup sources of food such as cows and lambs were also kept. Over time the southernmost parts of Sumerian Mesopotamia suffered from increased salinity of the soils, leading to a slow urban decline and a centring of power in Akkad, further north.

The geography of Mesopotamia had a profound impact on the political development of the region. Among the rivers and streams, the Sumerian people built the first cities along with irrigation canals which were separated by vast stretches of open desert or swamp where nomadic tribes roamed. Communication among the isolated cities was difficult and, at times, dangerous. Thus, each Sumerian city became a city-state, independent of the others and protective of its independence. At times one city would try to conquer and unify the region, but such efforts were resisted and failed for centuries. As a result, the political history of Sumer is one of almost constant warfare. Eventually Sumer was unified by Eannatum, but the unification was tenuous and failed to last as the Akkadians conquered Sumeria in 2331 BC only a generation later. The Akkadian Empire was the first successful empire to last beyond a generation and see the peaceful succession of kings. The empire was relatively short-lived, as the Babylonians conquered them within only a few generations.

The Mesopotamians believed their kings and queens were descended from the City of Gods, but, unlike the ancient Egyptians, they never believed their kings were real gods. Most kings named themselves “king of the universe” or “great king”. Another common name was “shepherd”, as kings had to look after their people.

When Assyria grew into an empire, it was divided into smaller parts, called provinces. Each of these were named after their main cities, like Nineveh, Samaria, Damascus, and Arpad. They all had their own governor who had to make sure everyone paid their taxes. Governors also had to call up soldiers to war and supply workers when a temple was built. He was also responsible for enforcing the laws. In this way, it was easier to keep control of a large empire. Although Babylon was quite a small state in the Sumerian, it grew tremendously throughout the time of Hammurabi's rule. He was known as "the lawmaker", and soon Babylon became one of the main cities in Mesopotamia. It was later called Babylonia, which meant "the gateway of the gods." It also became one of history's greatest centers of learning.

With the end of the Uruk phase, walled cities grew and many isolated Ubaid villages were abandoned indicating a rise in communal violence. An early king Lugalbanda was supposed to have built the white walls around the city. As city-states began to grow, their spheres of influence overlapped, creating arguments between other city-states, especially over land and canals. These arguments were recorded in tablets several hundreds of years before any major war—the first recording of a war occurred around 3200 BC but was not common until about 2500 BC. An Early Dynastic II king (Ensi) of Uruk in Sumer, Gilgamesh (c. 2600 BC), was commended for military exploits against Humbaba guardian of the Cedar Mountain, and was later celebrated in many later poems and songs in which he was claimed to be two-thirds god and only one-third human. The later Stele of the Vultures at the end of the Early Dynastic III period (2600–2350 BC), commemorating the victory of Eannatum of Lagash over the neighbouring rival city of Umma is the oldest monument in the world that celebrates a massacre. From this point forwards, warfare was incorporated into the Mesopotamian political system. At times a neutral city may act as an arbitrator for the two rival cities. This helped to form unions between cities, leading to regional states. When empires were created, they went to war more with foreign countries. King Sargon, for example, conquered all the cities of Sumer, some cities in Mari, and then went to war with northern Syria. Many Assyrian and Babylonian palace walls were decorated with the pictures of the successful fights and the enemy either desperately escaping or hiding amongst reeds.

City-states of Mesopotamia created the first law codes, drawn from legal precedence and decisions made by kings. The codes of Urukagina and Lipit Ishtar have been found. The most renowned of these was that of Hammurabi, as mentioned above, who was posthumously famous for his set of laws, the Code of Hammurabi (created c. 1780 BC), which is one of the earliest sets of laws found and one of the best preserved examples of this type of document from ancient Mesopotamia. He codified over 200 laws for Mesopotamia. Examination of the laws show a progressive weakening of the rights of women, and increasing severity in the treatment of slaves

The art of Mesopotamia rivalled that of Ancient Egypt as the most grand, sophisticated and elaborate in western Eurasia from the 4th millennium BC until the Persian Achaemenid Empire conquered the region in the 6th century BC. The main emphasis was on various, very durable, forms of sculpture in stone and clay; little painting has survived, but what has suggests that painting was mainly used for geometrical and plant-based decorative schemes, though most sculpture was also painted.

The Protoliterate period, dominated by Uruk, saw the production of sophisticated works like the Warka Vase and cylinder seals. The Guennol Lioness is an outstanding small limestone figure from Elam of about 3000–2800 BC, part man and part lion. A little later there are a number of figures of large-eyed priests and worshippers, mostly in alabaster and up to a foot high, who attended temple cult images of the deity, but very few of these have survived. Sculptures from the Sumerian and Akkadian period generally had large, staring eyes, and long beards on the men. Many masterpieces have also been found at the Royal Cemetery at Ur (c. 2650 BC), including the two figures of a "Ram in a Thicket", the "Copper Bull" and a bull's head on one of the Lyres of Ur.

From the many subsequent periods before the ascendency of the Neo-Assyrian Empire Mesopotamian art survives in a number of forms: cylinder seals, relatively small figures in the round, and reliefs of various sizes, including cheap plaques of moulded pottery for the home, some religious and some apparently not. The Burney Relief is an unusual elaborate and relatively large (20 x 15 inches) terracotta plaque of a naked winged goddess with the feet of a bird of prey, and attendant owls and lions. It comes from the 18th or 19th centuries BC, and may also be moulded. Stone stelae, votive offerings, or ones probably commemorating victories and showing feasts, are also found from temples, which unlike more official ones lack inscriptions that would explain them; the fragmentary Stele of the Vultures is an early example of the inscribed type, and the Assyrian Black Obelisk of Shalmaneser III a large and solid late one.

The conquest of the whole of Mesopotamia and much surrounding territory by the Assyrians created a larger and wealthier state than the region had known before, and very grandiose art in palaces and public places, no doubt partly intended to match the splendour of the art of the neighbouring Egyptian empire. The Assyrians developed a style of extremely large schemes of very finely detailed narrative low reliefs in stone for palaces, with scenes of war or hunting; the British Museum has an outstanding collection. They produced very little sculpture in the round, except for colossal guardian figures, often the human-headed lamassu, which are sculpted in high relief on two sides of a rectangular block, with the heads effectively in the round (and also five legs, so that both views seem complete). Even before dominating the region they had continued the cylinder seal tradition with designs which are often exceptionally energetic and refined.

The study of ancient Mesopotamian architecture is based on available archaeological evidence, pictorial representation of buildings, and texts on building practices. Scholarly literature usually concentrates on temples, palaces, city walls and gates, and other monumental buildings, but occasionally one finds works on residential architecture as well. Archaeological surface surveys also allowed for the study of urban form in early Mesopotamian cities.

Brick is the dominant material, as the material was freely available locally, whereas building stone had to be brought a considerable distance to most cities. The ziggurat is the most distinctive form, and cities often had large gateways, of which the Ishtar Gate from Neo-Babylonian Babylon, decorated with beasts in polychrome brick, is the most famous, now largely in the Pergamon Museum in Berlin.

The most notable architectural remains from early Mesopotamia are the temple complexes at Uruk from the 4th millennium BC, temples and palaces from the Early Dynastic period sites in the Diyala River valley such as Khafajah and Tell Asmar, the Third Dynasty of Ur remains at Nippur (Sanctuary of Enlil) and Ur (Sanctuary of Nanna), Middle Bronze Age remains at Syrian-Turkish sites of Ebla, Mari, Alalakh, Aleppo and Kultepe, Late Bronze Age palaces at Bogazkoy (Hattusha), Ugarit, Ashur and Nuzi, Iron Age palaces and temples at Assyrian (Kalhu/Nimrud, Khorsabad, Nineveh), Babylonian (Babylon), Urartian (Tushpa/Van, Kalesi, Cavustepe, Ayanis, Armavir, Erebuni, Bastam) and Neo-Hittite sites (Karkamis, Tell Halaf, Karatepe). Houses are mostly known from Old Babylonian remains at Nippur and Ur. Among the textual sources on building construction and associated rituals are Gudea's cylinders from the late 3rd millennium are notable, as well as the Assyrian and Babylonian royal inscriptions from the Iron Age.





</doc>
<doc id="20192" url="https://en.wikipedia.org/wiki?curid=20192" title="Miranda Richardson">
Miranda Richardson

Miranda Jane Richardson (born 3 March 1958) is an English actress and comedian. She made her film debut playing Ruth Ellis in "Dance with a Stranger" (1985) and went on to receive Academy Award nominations for "Damage" (1992) and "Tom & Viv" (1994). A seven-time BAFTA Award nominee, she won the BAFTA Award for Best Actress in a Supporting Role for "Damage". She has also been nominated for seven Golden Globe Awards, winning twice for "Enchanted April" (1992) and the TV film "Fatherland" (1994).

Richardson began her career in 1979 and made her West End debut in the 1981 play "Moving", before being nominated for the 1987 Olivier Award for Best Actress for "A Lie of the Mind". Her television credits include "Blackadder" (1986–89), "A Dance to the Music of Time" (1997), "Merlin" (1998), "The Lost Prince" (2003), "Gideon's Daughter" (2006), the sitcom "The Life and Times of Vivienne Vyle" (2007), and "Rubicon" (2010). She was nominated for the 2015 Primetime Emmy Award for Outstanding Narrator for "Operation Orangutan".

Her other films include "Empire of the Sun" (1987), "The Crying Game" (1992), "The Apostle" (1997), "Sleepy Hollow" (1999), "Chicken Run" (2000), "The Hours" (2002), "Spider" (2002), "Harry Potter and the Goblet of Fire" (2005), "The Young Victoria" (2009), "Made in Dagenham" (2010), "Belle" (2013), and "Stronger" (2017).

Richardson was born in Southport, England, to Marian Georgina (née Townsend), a housewife, and William Alan Richardson, a marketing executive, and was their second daughter.

Richardson enrolled at the Bristol Old Vic Theatre School, where she studied alongside Daniel Day-Lewis and Jenny Seagrove, having started out with juvenile performances in "Cinderella" and "Lord Arthur Savile's Crime" at the Southport Dramatic Club.

Richardson has enjoyed a successful and extensive theatre career, first joining Manchester Library Theatre in 1979 as an assistant stage manager, followed by a number of appearances in repertory theatre. Her London stage debut was in "Moving" at the Queen's Theatre in 1981. She found recognition in the West End for a series of stage performances, ultimately receiving an Olivier Award nomination for her performance in "A Lie of the Mind", and, in 1996, one critic asserted that she is "the greatest actress of our time in any medium" after she appeared in "Orlando" at the Edinburgh Festival. She returned to the London stage in May 2009 to play the lead role in Wallace Shawn's new play, "Grasses of a Thousand Colours" at the Royal Court Theatre. Richardson has said that she prefers new works rather than the classics because of the history which goes with them.

In 1985, Richardson made her film debut as Ruth Ellis, the last woman to be hanged in the United Kingdom, in the biographical drama "Dance with a Stranger". Around the same time, Richardson played a comedic Queen Elizabeth I, aka Queenie, in the British television comedy "Blackadder II".

Following "Dance with a Stranger", Richardson turned down numerous parts in which her character was unstable or disreputable, including the Glenn Close role in "Fatal Attraction". In this period, she appeared in "Empire of the Sun" (1987). In an episode of the TV series "The Storyteller" ("The Three Ravens", 1988), she played a witch. Meanwhile, she had returned in guest roles in one episode each in "Blackadder the Third" (1987) and "Blackadder Goes Forth" (1989). She returned to play Queenie in the Christmas special "Blackadder's Christmas Carol" (1988) and, later, a special edition for the millennium "".

Her portrayal of a troubled theatre goer in "Secret Friends" (BBC 2, 1990) was described as "a miniature tour de force... Miranda Richardson's finest hour, all in ten minutes" ("The Sunday Times"). Other television roles include Pamela Flitton in "A Dance to the Music of Time" (1997), Miss Gilchrist in "St. Ives" (1998), Bettina the interior decorator in "Absolutely Fabulous", Queen Elspeth, Snow White's stepmother, in "" (2001), and Queen Mary in "The Lost Prince" (2003).
Richardson has appeared in a number of high-profile supporting roles in film, including Vanessa Bell in "The Hours", Lady Van Tassel in "Sleepy Hollow" and Patsy Carpenter in "The Evening Star". She also won acclaim for her performances in "The Crying Game" and "Enchanted April", for which she won a Golden Globe. She received Academy Award nominations for her performances in "Damage" and "Tom & Viv".

Her film credits also include "Kansas City" (1996), "The Apostle" (1997) and "Wah-Wah" (2005). In 2002, she performed a triple-role in the thriller "Spider".

Richardson also appeared as Queen Rosalind of Denmark in "The Prince and Me" and as the ballet mistress Madame Giry in the film version of the Andrew Lloyd Webber musical "The Phantom of the Opera" (2004). In 2005, she appeared in the role of Rita Skeeter, the toxic "Daily Prophet" journalist in "Harry Potter and the Goblet of Fire". She also did the voice for Corky in "The Adventures of Bottle Top Bill and His Best Friend Corky" (2005), an Australian animated series for children. In 2006, she appeared in "Gideon's Daughter". She played Mrs. Claus in the film "Fred Claus" (2007).

Richardson appeared in the BBC sitcom, "The Life and Times of Vivienne Vyle". She appeared as a guest in "A Taste of My Life".

In 2008, Richardson was cast in a leading role in original AMC pilot, "Rubicon". She plays Katherine Rhumor, a New York socialite who finds herself drawn into the central intrigue of a think tank after the death of her husband.

Additionally, she played Labour politician Barbara Castle in the British film "Made in Dagenham".

Richardson was cast as Queen Ulla in "Maleficent", where she was to play the titular character's aunt, but her role was cut from the film during post-production. In 2015, she played Sybil Birling in Helen Edmundson's BBC One adaptation of J. B. Priestley's "An Inspector Calls".

Richardson has never married and has no children. She is interested in falconry.




</doc>
<doc id="20193" url="https://en.wikipedia.org/wiki?curid=20193" title="Mecklenburg">
Mecklenburg

Mecklenburg (, Low German: "Mękel(n)borg" ) is a historical region in northern Germany comprising the western and larger part of the federal-state Mecklenburg-Vorpommern. The largest cities of the region are Rostock, Schwerin, Neubrandenburg, Wismar and Güstrow.

The name Mecklenburg derives from a castle named "Mikilenburg" (Old Saxon: "big castle", hence its translation into New Latin and ), located between the cities of Schwerin and Wismar. In Slavic language it was known as "Veligrad", which also means "big castle". It was the ancestral seat of the House of Mecklenburg; for a time the area was divided into Mecklenburg-Schwerin and Mecklenburg-Strelitz among the same dynasty.

Linguistically Mecklenburgers retain and use many features of Low German vocabulary or phonology.

The adjective for the region is "Mecklenburgian" (); inhabitants are called Mecklenburgians ().

Mecklenburg is known for its mostly flat countryside. Much of the terrain is boggy, with ponds, marshes and fields as common features, with small forests interspersed. The terrain changes as one moves north towards the Baltic Sea.

Under the peat of Mecklenburg are sometimes found deposits of ancient lava flows. Traditionally, at least in the countryside, the stone from these flows is cut and used in the construction of homes, often in joint use with cement, brick and wood, forming a unique look to the exterior of country houses.

Mecklenburg has productive farming, but the land is most suitable for grazing for livestock.

Mecklenburg is the site of many prehistoric dolmen tombs. Its earliest organised inhabitants may have had Celtic origins. By no later than 100 BC the area had been populated by pre-Christian Germanic peoples.

The traditional symbol of Mecklenburg, the grinning steer's head (Low German: "Ossenkopp", lit.: 'oxen's head', with "osse" being a synonym for steer and bull in Middle Low German), with an attached hide, and a crown above, may have originated from this period. It represents what early peoples would have worn, i.e. a steers's head as a helmet, with the hide hanging down the back to protect the neck from the sun, and overall as a way to instill fear in the enemy.

From the 7th through the 12th centuries, Germanic Mecklenburg was ruled by Western Slavic overlords, newly arrived from the steppes. Among them were the Obotrites and other tribes that Frankish sources referred to as "Wends". The 11th century founder of the Mecklenburger dynasty of Dukes and later Grand Dukes, which lasted until 1918, was Nyklot of the Obotrites.

In the late 12th century, Henry the Lion, Duke of the Saxons, reconquered the region, took oaths from its local lords, and Christianized its people, in a precursor to the Northern Crusades. From 12th to 14th century, large numbers of Germans and Flemings settled the area (Ostsiedlung), importing German law and improved agricultural techniques. The Wends who survived all warfare and devastation of the centuries before, including invasions of and expeditions into Saxony, Denmark and Liutizic areas as well as internal conflicts, were assimilated in the centuries thereafter. However, elements of certain names and words used in Mecklenburg speak to the lingering Slavic influence. An example would be the city of Schwerin, which was originally called "Zuarin" in Slavic. Another example is the town of Bresegard, the 'gard' portion of the town name deriving from the Slavic word 'grad', meaning city or town.

Since the 12th century, the territory remained stable and relatively independent of its neighbours; one of the few German territories for which this is true. During the reformation the Duke in Schwerin would convert to Protestantism and so would follow the Duchy of Mecklenburg in 1549.

Like many German territories, Mecklenburg was sometimes partitioned and re-partitioned among different members of the ruling dynasty. In 1621 it was divided into the two duchies of Mecklenburg-Schwerin and Mecklenburg-Güstrow. With the extinction of the Güstrow line in 1701, the Güstrow lands were redivided, part going to the Duke of Mecklenburg-Schwerin, and part going to the new line of Mecklenburg-Strelitz.

In 1815, the two Mecklenburgian duchies were raised to Grand Duchies, the Grand Duchy of Mecklenburg-Schwerin and the Grand Duchy of Mecklenburg-Strelitz, and subsequently existed separately as such in Germany under enlightened but absolute rule (constitutions being granted on the eve of World War I) until the revolution of 1918. Life in Mecklenburg could be quite harsh. Practices such as having to ask for permission from the Grand Duke to get married, or having to apply for permission to emigrate, would linger late into the history of Mecklenburg (i.e. 1918), long after such practices had been abandoned in other German areas. Even as late as the later half of the 19th century the Grand Duke personally owned half of the countryside. The last Duke abdicated in 1918, as monarchies fell throughout Europe. The Duke's ruling house reigned in Mecklenburg uninterrupted (except for two years) from its incorporation into the Holy Roman Empire until 1918. From 1918 to 1933, the duchies were free states in the Weimar Republic.

Traditionally Mecklenburg has always been one of the poorer German areas, and later the poorer of the provinces, or "Länder", within a unified Germany. The reasons for this may be varied, but one factor stands out: agriculturally the land is poor and can not produce at the same level as other parts of Germany. The two Mecklenburgs made attempts at being independent states after 1918, but eventually this failed as their dependence on the rest of the German lands became apparent.

After three centuries of partition, Mecklenburg was united on 1 January 1934 by the Nazi government. The Wehrmacht assigned Mecklenburg and Pomerania to Wehrkreis II under the command of "General der Infanterie" Werner Kienitz, with the headquarters at Stettin. Mecklenburg was assigned to an Area headquartered at Schwerin, which was responsible for military units in Schwerin; Rostock; Parchim; and Neustrelitz.

After World War II, the Soviet government occupying eastern Germany merged Mecklenburg with the smaller neighbouring region of Western Pomerania (German "Vorpommern") to form the state of Mecklenburg-Vorpommern. Mecklenburg contributed about two-thirds of the geographical size of the new state and the majority of its population. Also, the new state became temporary or permanent home for lots of refugees expelled from former German territories seized by the Soviet Union and Poland after the war. The Soviets changed the name from "Mecklenburg-Western Pomerania" to "Mecklenburg" in 1947.

In 1952, the East German government ended the independent existence of Mecklenburg, creating 3 districts ("Bezirke") out of its territory: Rostock, Schwerin and Neubrandenburg.

During German reunification in 1990, the state of Mecklenburg-Vorpommern was revived, and is now one of the 16 states of the Federal Republic of Germany.

The House of Mecklenburg was founded by Niklot, prince of the Obotrites, Chizzini and Circipani on the Baltic Sea, who died in 1160. His Christian progeny was recognized as prince of the Holy Roman Empire 1170 and Duke of Mecklenburg 8 July 1348. On 27 February 1658 the ducal house divided in two branches: Mecklenburg-Schwerin and Mecklenburg-Strelitz.

The flag of both Mecklenburg duchies is traditionally made of the colours blue, yellow and red. The sequence however changed more than once in the past 300 years. In 1813 the duchies used yellow-red-blue. 23 December 1863 for Schwerin and 4 January 1864 for Strelitz blue-yellow-red was ordered. Mecklenburg-Schwerin however used white instead of yellow for flags on sea by law of 24 March 1855.

Siebmachers Wappenbuch gives therefore (?) blue-white-red for Schwerin and blue-yellow-red for Strelitz.
According to this source, the grand ducal house of Schwerin used a flag of 3.75 to 5.625 M with the middle arms on a white quadrant (1.75 M) in the middle.

The middle arms show the shield of Mecklenburg as arranged in the 17th century. The county of Schwerin in the middle and in the quartering Mecklenburg (bull's head with hide), Rostock (griffin), principality of Schwerin (griffin surmounting green rectangle), Ratzeburg (cross surmounted by crown), Stargard (arm with hand holding ring) and Wenden (bull's head). The shield is supported by a bull and a griffin and surmounted by a royal crown.

The dukes of Strelitz used according to Siebmachers the blue-yellow-red flag with just the (oval) shield of Mecklenburg in the yellow band.

Ströhl in 1897 and Bulgaria, show another arrangement: The grand-duke of Mecklenburg-Schwerin flows a flag (4:5) with the arms of the figures from the shield of arms.

The former Schwerin standard with the white quadrant is now ascribed to the grand dukes of Strelitz.
Ströhl mentions a flag for the grand ducal house by law of 23 December 1863 with the middle arms in the yellow band. And he mentions a special sea flag, the same but with a white middle band. 
'Berühmte Fahnen' shows furthermore a standard for grand duchess Alexandra of Mecklenburg-Schwerin, princess of Hannover (1882–1963), showing her shield and that of Mecklenburg joined by the order of the Wendic Crown in a white oval. On sea the yellow band in her flag was of course white. 
The princes (dukes) of Mecklenburg-Schwerin had according to this source their own standard, showing the griffin of Rostock.

Mecklenburg faces a huge increase in tourism since German reunification in 1990, particularly with its beaches and seaside resorts at the Baltic Sea ("German Riviera", Warnemünde, Boltenhagen, Heiligendamm, Kühlungsborn, Rerik and others), the Mecklenburg Lakeland ("Mecklenburgische Seenplatte") and the Mecklenburg Switzerland ("Mecklenburgische Schweiz") with their pristine nature, the old Hanseatic towns of Rostock, Greifswald, Stralsund and Wismar (the latter two being World Heritage) well known for their medieval Brick Gothic buildings, and the former royal residences of Schwerin, Güstrow, Ludwigslust and Neustrelitz.



</doc>
<doc id="20196" url="https://en.wikipedia.org/wiki?curid=20196" title="March 10">
March 10






</doc>
<doc id="20197" url="https://en.wikipedia.org/wiki?curid=20197" title="March 12">
March 12





</doc>
<doc id="20199" url="https://en.wikipedia.org/wiki?curid=20199" title="March 14">
March 14





</doc>
<doc id="20200" url="https://en.wikipedia.org/wiki?curid=20200" title="Management science">
Management science

Management science (MS) is the broad interdisciplinary study of problem solving and decision making in human organizations, with strong links to management, economics, business, engineering, management consulting, and other fields. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and accurate management decisions by arriving at optimal or near optimal solutions to complex decision problems. Management science helps businesses to achieve goals using various scientific methods. 

The field was initially an outgrowth of applied mathematics, where early challenges were problems relating to the optimization of systems which could be modeled linearly, i.e., determining the optima (maximum value of profit, assembly line performance, crop yield, bandwidth, etc. or minimum of loss, risk, costs, etc.) of some objective function. Today, management science encompasses any organizational activity for which the problem can be structured as a functional system so as to obtain a solution set with identifiable characteristics.

Management science is concerned with a number of different areas of study: One is developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems. The models used can often be represented mathematically, but sometimes computer-based, visual or verbal representations are used as well or instead. Another area is designing and developing new and better models of organizational excellence. 

Management science research can be done on three levels:

The management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. The techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.

Its origins can be traced to operations research, which became influential during World War II when the Allied forces recruited scientists of various disciplines to assist with military operations. In these early applications, the scientists used simple mathematical models to make efficient use of limited technologies and resources. The application of these models to the corporate sector became known as management science. 

In 1967 Stafford Beer characterized the field of management science as "the business use of operations research".

Some of the fields that management science involves include: 


as well as many others.

Applications of management science are abundant in industries such as airlines, manufacturing companies, service organizations, military branches, and in government. Management science has contributed insights and solutions to a vast range of problems and issues, including:

Management science is also concerned with so-called "soft-operational analysis", which concerns methods for strategic planning, strategic decision support, and problem structuring methods (PSM). At this level of abstraction, mathematical modeling and simulation will not suffice. Therefore, since the late 20th century, new "non-quantified" modelling methods have been developed, including morphological analysis and various forms of influence diagrams.



</doc>
<doc id="20201" url="https://en.wikipedia.org/wiki?curid=20201" title="Musical notation">
Musical notation

Musical notation is any system used to visually represent aurally perceived music played with instruments or sung by the human voice through the use of written, printed, or otherwise-produced symbols.

Types and methods of notation have varied between cultures and throughout history, and much information about ancient music notation is fragmentary. Even in the same time period, such as in the 2010s, different styles of music and different cultures use different music notation methods; for example, for professional classical music performers, sheet music using staves and noteheads is the most common way of notating music, but for professional country music session musicians, the Nashville Number System is the main method.

The symbols used include ancient symbols and modern symbols made upon any media such as symbols cut into stone, made in clay tablets, made using a pen on papyrus or parchment or manuscript paper; printed using a printing press (c. 1400s), a computer printer (c. 1980s) or other printing or modern copying technology.

Although many ancient cultures used symbols to represent melodies and rhythms, none of them were particularly comprehensive, and this has limited today's understanding of their music. The seeds of what would eventually become modern western notation were sown in medieval Europe, starting with the Catholic Church's goal for ecclesiastical uniformity. The church began notating plainchant melodies so that the same chants could be used throughout the church. Music notation developed further in the Renaissance and Baroque music eras. In the classical period (1750–1820) and the Romantic music era (1820–1900), notation continued to develop as new musical instrument technologies were developed. In the contemporary classical music of the 20th and 21st century, music notation has continued to develop, with the introduction of graphical notation by some modern composers and the use, since the 1980s, of computer-based score writer programs for notating music. Music notation has been adapted to many kinds of music, including classical music, popular music, and traditional music.

The earliest form of musical notation can be found in a cuneiform tablet that was created at Nippur, in Babylonia (today's Iraq), in about 1400 BC. The tablet represents fragmentary instructions for performing music, that the music was composed in harmonies of thirds, and that it was written using a diatonic scale. A tablet from about 1250 BC shows a more developed form of notation. Although the interpretation of the notation system is still controversial, it is clear that the notation indicates the names of strings on a lyre, the tuning of which is described in other tablets. Although they are fragmentary, these tablets represent the earliest notated melodies found anywhere in the world.

Ancient Greek musical notation was in use from at least the 6th century BC until approximately the 4th century AD; several complete compositions and fragments of compositions using this notation survive. The notation consists of symbols placed above text syllables. An example of a complete composition is the Seikilos epitaph, which has been variously dated between the 2nd century BC to the 2nd century AD.

Three hymns by Mesomedes of Crete exist in manuscript. The Delphic Hymns, dated to the 2nd century BC, also use this notation, but they are not completely preserved. Ancient Greek notation appears to have fallen out of use around the time of the Decline of the Western Roman Empire.

Byzantine music once included music for court ceremonies, but has only survived as vocal church music within various Orthodox traditions of monodic (monophonic) chant written down in Byzantine round notation (see Macarie's "anastasimatarion" with the Greek text translated into Romanian and transliterated into Cyrillic script). 

Since the 6th century Greek theoretical categories (melos, genos, harmonia, systema) played a key role to understand and transmit Byzantine music, especially the tradition of Damascus had a strong impact on the pre-Islamic Near East comparable to the impact coming from Persian music. The earliest evidence are papyrus fragments of Greek tropologia. These fragments just present the hymn text following a modal signature or key (like “ΠΛ Α” for “echos plagios protos” or “Β” for “echos devteros”).

Unlike Western notation Byzantine neumes used since the 10th century were always related to modal steps (same modal degree, one degree lower, two degrees higher etc.) in relation to such a clef or modal key (modal signatures). Originally this key or the incipit of a common melody was enough to indicate a certain melodic model given within the echos. Next to ekphonetic notation, only used in lectionaries to indicate formulas used during scriptural lessons, melodic notation developed not earlier than between the 9th and the 10th century, when a theta (θ), oxeia (/) or diple (//) were written under a certain syllable of the text, whenever a longer melisma was expected. This primitive form was called “theta” or “diple notation”.

Today, one can study the evolution of this notation within Greek monastic chant books like those of the sticherarion and the heirmologion (Chartres notation was rather used on Mount Athos and Constantinople, Coislin notation within the Patriarchates of Jerusalem and Alexandria), while there was another gestic notation originally used for the asmatikon (choir book) and kontakarion (book of the soloist or monophonaris) of the Constantinopolitan cathedral rite. The earliest books which have survived, are “kondakars” in Slavonic translation which already show an own notation system known as Kondakarian notation. Like the Greek alphabet notational signs are ordered left to right (though the direction could be adapted like in certain Syriac manuscripts). The question of rhythm was entirely based on cheironomia (the interpretation of so-called great signs which derived from different chant books). Basically those great signs (μεγάλα σῃμάδια) indicated well-known melodical phrases given by gestures of the choirleaders of the cathedral rite. They existed once as part of an oral tradition, developed Kondakarian notation and became during the 13th century integrated into Byzantine round notation as a kind of universal notation system.

Today the main difference between Western and Eastern neumes is that Eastern notation symbols are "differential" rather than absolute, i.e. they indicate pitch steps (rising, falling or at the same step), and the musicians know to deduce correctly, from the score and the note they are singing presently, which correct interval is meant. These step symbols themselves, or better "phonic neumes", resemble brush strokes and are colloquially called "gántzoi" ("hooks") in Modern Greek.

Notes as pitch classes or modal keys (usually memorised by modal signatures) are represented in written form only between these neumes (in manuscripts usually written in red ink). In modern notation they simply serve as an optional reminder and modal and tempo directions have been added, if necessary. In Papadic notation medial signatures usually meant a temporary change into another echos.

The so-called "great signs" were once related to cheironomic signs; according to modern interpretations they are understood as embellishments and microtonal attractions (pitch changes smaller than a semitone), both essential in Byzantine chant.

Since Chrysanthos of Madytos there are seven standard note names used for "solfège" ("parallagē") "pá, vú, gá, dē, ké, zō, nē", while the older practice still used the four enechemata or intonation formulas of the four echoi given by the modal signatures, the authentic or "kyrioi" in ascending direction, and the plagal or "plagioi" in descending direction (Papadic Octoechos). With exception of "vú and zō" they do roughly correspond to Western solmization syllables as "re, mi, fa, sol, la, si, do". Byzantine music uses the eight natural, non-tempered scales whose elements were identified by "Ēkhoi", "sounds", exclusively, and therefore the absolute pitch of each note may slightly vary each time, depending on the particular "Ēkhos" used. Byzantine notation is still used in many Orthodox Churches. Sometimes cantors also use transcriptions into Western or Kievan staff notation while adding non-notatable embellishment material from memory and "sliding" into the natural scales from experience, but even concerning modern neume editions since the reform of Chrysanthos a lot of details are only known from an oral tradition related to traditional masters and their experience.

In 1252, Safi al-Din al-Urmawi developed a form of musical notation, where rhythms were represented by geometric representation. Many subsequent scholars of rhythm have sought to develop graphical geometrical notations. For example, a similar geometric system was published in 1987 by Kjell Gustafson, whose method represents a rhythm as a two-dimensional graph.

The scholar and music theorist Isidore of Seville, while writing in the early 7th century, considered that "unless sounds are held by the memory of man, they perish, because they cannot be written down." By the middle of the 9th century, however, a form of neumatic notation began to develop in monasteries in Europe as a mnemonic device for Gregorian chant, using symbols known as neumes; the earliest surviving musical notation of this type is in the "Musica disciplina" of Aurelian of Réôme, from about 850. There are scattered survivals from the Iberian Peninsula before this time, of a type of notation known as Visigothic neumes, but its few surviving fragments have not yet been deciphered. The problem with this notation was that it only showed melodic contours and consequently the music could not be read by someone who did not know the music already.

Notation had developed far enough to notate melody, but there was still no system for notating rhythm. A mid-13th-century treatise, "De Mensurabili Musica", explains a set of six rhythmic modes that were in use at the time, although it is not clear how they were formed. These rhythmic modes were all in triple time and rather limited rhythm in chant to six different repeating patterns. This was a flaw seen by German music theorist Franco of Cologne and summarised as part of his treatise "Ars cantus mensurabilis" (the art of measured chant, or mensural notation). He suggested that individual notes could have their own rhythms represented by the shape of the note. Not until the 14th century did something like the present system of fixed note lengths arise. The use of regular measures (bars) became commonplace by the end of the 17th century.

The founder of what is now considered the standard music staff was Guido d'Arezzo, an Italian Benedictine monk who lived from about 991 until after 1033. He taught the use of solmization syllables based on a hymn to Saint John the Baptist, which begins Ut Queant Laxis and was written by the Lombard historian Paul the Deacon. The first stanza is:


Guido used the first syllable of each line, Ut, Re, Mi, Fa, Sol, La, and Si, to read notated music in terms of hexachords; they were not note names, and each could, depending on context, be applied to any note. In the 17th century, Ut was changed in most countries except France to the easily singable, open syllable Do, said to have been taken from the name of the Italian theorist Giovanni Battista Doni,
but rather Do have been taken from the word "Dominus" in Latin with the meaning "the Lord".

Catholic monks developed the first forms of modern European musical notation in order to standardize liturgy throughout the worldwide Church, and an enormous body of religious music has been composed for it through the ages. This led directly to the emergence and development of European classical music, and its many derivatives. The Baroque style, which encompassed music, art, and architecture, was particularly encouraged by the post-Reformation Catholic Church as such forms offered a means of religious expression that was stirring and emotional, intended to stimulate religious fervor.

Modern music notation is used by musicians of many different genres throughout the world. The staff acts as a framework upon which pitches are indicated by placing oval noteheads on the staff lines or between the lines. The pitch of the oval musical noteheads can be modified by accidentals. The duration (note length) is shown with different note values, which can be indicated by the notehead being a stemless hollow oval (a whole note or semibreve), a hollow rectangle or stemless hollow oval with one or two vertical lines on either side (double whole note or breve), a stemmed hollow oval (a half note or minim), or solid oval using stems to indicate quarter notes (crotchets) and stems with added flags or beams to indicate smaller subdivisions, and additional symbols such as dots and ties which lengthen the duration of a note. Notation is read from left to right, which makes setting music for right-to-left scripts difficult.

A staff (or stave, in British English) of written music generally begins with a clef, which indicates the position of one particular note on the staff. The treble clef or G clef was originally a letter G and it identifies the second line up on the five line staff as the note G above middle C. The bass clef or F clef shows the position of the note F below middle C. While the treble and bass clef are the most widely used clefs, other clefs are used, such as the alto clef (used for viola and alto trombone music) and the tenor clef (used for some cello, tenor trombone, and double bass music). Notes representing a pitch outside of the scope of the five line staff can be represented using ledger lines, which provide a single note with additional lines and spaces. Some instruments use mainly one clef, such as violin and flute, which use treble clef and double bass and tuba, which use bass clef. Some instruments regularly use both clefs, such as piano and pipe organ.

Following the clef, the key signature on a staff indicates the key of the piece or song by specifying that certain notes are flat or sharp throughout the piece, unless otherwise indicated with accidentals added before certain notes. When a sharp is placed before a note, this makes that note one semitone higher. When a flat is placed before a note, this makes that note one semitone lower. Double sharps and double flats are less common, but they are used. A double sharp is placed before a note to make it two semitones higher. A double flat is placed before a note to make it two semitones lower. A natural sign placed before a note renders that note in its "natural" form, which means that any sharps or flats applying to that note from the key signature or from accidentals are cancelled. Sometimes a courtesy accidental is used in music where it is not technically required, to remind the musician of what pitch the key signature requires.

Following the key signature is the time signature. The time signature typically consists of two numbers, with one of the most common being . The top "4" indicates that there are four beats per measure (also called bar). The bottom "4" indicates that each of those beats are quarter notes. Measures divide the piece into groups of beats, and the time signatures specify those groupings. is used so often that it is also called "common time", and it may be indicated with rather than numbers. Other common time signatures are (three beats per bar, with each beat being a quarter note); (two beats per bar, with each beat being a quarter note); (six beats per bar, with each beat being an eighth note) and (twelve beats per bar, with each beat being an eighth note; in practice, the eighth notes are typically put into four groups of three eighth notes. is a compound time type of time signature). Many other time signatures exist, such as , , , , , and so on.

Many short classical music pieces from the classical era and songs from traditional music and popular music are in one time signature for much or all of the piece. Music from the Romantic music era and later, particularly contemporary classical music and rock music genres such as progressive rock and the hardcore punk subgenre mathcore, may use mixed meter; songs or pieces change from one meter to another, for example alternating between bars of and .

Directions to the player regarding matters such as tempo (e.g., Allegro, Andante, Largo, Vif, Lent, Modérément, Presto, etc.), dynamics (pianississimo, pianissimo, piano, mezzopiano, mezzoforte, forte, fortissimo, fortississimo, etc.) appear above or below the staff. Terms indicating the musical expression or "feel" to a song or piece are indicated at the beginning of the piece and at any points where the mood changes (e.g., "Slow March", "Fast Swing", "Medium Blues", "Fougueux", "Feierlich", "Gelassen", "Piacevole", "Con slancio", "Majestic", "Hostile" etc.) For vocal music, lyrics are written near the pitches of the melody. For short pauses (breaths), retakes (retakes are indicated with a ' mark) are added.

In music for ensembles, a "score" shows music for all players together, with the staves for the different instruments and/or voices stacked vertically. The conductor uses the score while leading an orchestra, concert band, choir or other large ensemble. Individual performers in an ensemble play from "parts" which contain only the music played by an individual musician. A score can be constructed from a complete set of parts and vice versa. The process was laborious and time consuming when parts were hand-copied from the score, but since the development of scorewriter computer software in the 1980s, a score stored electronically can have parts automatically prepared by the program and quickly and inexpensively printed out using a computer printer. 

A ♭ in music lowers a pitch down one semitone. A ♯ in music raises a note one semitone. For example, a sharp on B would raise it to B♯ while a flat would lower it to B♭.


"Jeongganbo" is a unique traditional musical notation system created during the time of Sejong the Great that was the first East Asian system to represent rhythm, pitch, and time. Among various kinds of Korean traditional music, Jeong-gan-bo targets a particular genre, Jeong-ak (정악, 正樂).

Jeong-gan-bo tells the pitch by writing the pitch's name down in a box called 'jeong-gan' (this is where the name comes from). One jeong-gan is one beat each, and it can be split into two, three or more to hold half beats and quarter beats, and more. This makes it easy for the reader to figure out the beat.

Also, there are lots of markings indicating things such as ornaments. Most of these were later created by Ki-su Kim.

The Indian scholar and musical theorist Pingala (c. 200 BC), in his "Chanda Sutra", used marks indicating long and short syllables to indicate meters in Sanskrit poetry.

A rock inscription from circa 7th–8th century CE at Kudumiyanmalai, Tamil Nadu contains an early example of a musical notation. It was first identified and published by archaeologist/epigraphist D. R. Bhandarkar . Written in the Pallava-grantha script of the 7th century, it contains 38 horizontal lines of notations inscribed on a rectangular rock face (dimension of around 13 by 14 feet). Each line of the notation contains 64 characters (characters representing musical notes), written in groups of four notes. The basic characters for the seven notes, 'sa ri ga ma pa dha ni', are seen to be suffixed with the vowels a, i, u ,e. For example, in the place of 'sa', any one of 'sa', 'si', 'su' or 'se' is used. Similarly, in place of ri, any one of 'ra', 'ri', 'ru' or 're' is used. Horizontal lines divide the notation into 7 sections. Each section contains 4 to 7 lines of notation, with a title indicating its musical 'mode'. These modes may have been popular atleast from the 6th century CE and were incorporated into the Indian 'raga' system that developed later. But some of the unusual features seen in this notation have been given several non-conclusive interpretations by scholars. .
In the notation of Indian rāga, a solfege-like system called sargam is used. As in Western solfege, there are names for the seven basic pitches of a major scale (Shadja, Rishabha, Gandhara, Madhyama, Panchama, Dhaivata and Nishada, usually shortened to Sa Re Ga Ma Pa Dha Ni). The tonic of any scale is named Sa, and the dominant Pa. Sa is fixed in any scale, and Pa is fixed at a fifth above it (a Pythagorean fifth rather than an equal-tempered fifth). These two notes are known as achala swar ('fixed notes').

Each of the other five notes, Re, Ga, Ma, Dha and Ni, can take a 'regular' (shuddha) pitch, which is equivalent to its pitch in a standard major scale (thus, shuddha Re, the second degree of the scale, is a whole-step higher than Sa), or an altered pitch, either a half-step above or half-step below the shuddha pitch. Re, Ga, Dha and Ni all have altered partners that are a half-step lower (Komal-"flat") (thus, komal Re is a half-step higher than Sa).

Ma has an altered partner that is a half-step higher (teevra-"sharp") (thus, tivra Ma is an augmented fourth above Sa). Re, Ga, Ma, Dha and Ni are called vikrut swar ('movable notes'). In the written system of Indian notation devised by Ravi Shankar, the pitches are represented by Western letters. Capital letters are used for the achala swar, and for the higher variety of all the vikrut swar. Lowercase letters are used for the lower variety of the vikrut swar.

Other systems exist for non-twelve-tone equal temperament and non-Western music, such as the Indian "Swaralipi".

 
Znamenny Chant is a singing tradition used in the Russian Orthodox Church which uses a "hook and banner" notation. Znamenny Chant is unison, melismatic liturgical singing that has its own specific notation, called the "stolp" notation. The symbols used in the stolp notation are called ' (, 'hooks') or ' (, 'signs'). Often the names of the signs are used to refer to the stolp notation. Znamenny melodies are part of a system, consisting of Eight Modes (intonation structures; called glasy); the melodies are characterized by fluency and well-balancedness. There exist several types of Znamenny Chant: the so-called "Stolpovoy", "Malyj" (Little) and "Bolshoy" (Great) Znamenny Chant. Ruthenian Chant (Prostopinije) is sometimes considered a sub-division of the Znamenny Chant tradition, with the Muscovite Chant (Znamenny Chant proper) being the second branch of the same musical continuum.

Znamenny Chants are not written with notes (the so-called linear notation), but with special signs, called "Znamëna" (Russian for "marks", "banners") or "Kryuki" ("hooks"), as some shapes of these signs resemble hooks. Each sign may include the following components: a large black hook or a black stroke, several smaller black 'points' and 'commas' and lines near the hook or crossing the hook. Some signs may mean only one note, some 2 to 4 notes, and some a whole melody of more than 10 notes with a complicated rhythmic structure. The stolp notation was developed in Kievan Rus' as an East Slavic refinement of the Byzantine neumatic musical notation.

The most notable feature of this notation system is that it records transitions of the melody, rather than notes. The signs also represent a mood and a gradation of how this part of melody is to be sung (tempo, strength, devotion, meekness, etc.) Every sign has its own name and also features as a spiritual symbol. For example, there is a specific sign, called "little dove" (Russian: голубчик "(golubchik)"), which represents two rising sounds, but which is also a symbol of the Holy Ghost. Gradually the system became more and more complicated. This system was also ambiguous, so that almost no one, except the most trained and educated singers, could sing an unknown melody at sight. The signs only helped to reproduce the melody, not coding it in an unambiguous way.

The earliest known examples of text referring to music in China are inscriptions on musical instruments found in the Tomb of Marquis Yi of Zeng (d. 433 B.C.). Sets of 41 chimestones and 65 bells bore lengthy inscriptions concerning pitches, scales, and transposition. The bells still sound the pitches that their inscriptions refer to. Although no notated musical compositions were found, the inscriptions indicate that the system was sufficiently advanced to allow for musical notation. Two systems of pitch nomenclature existed, one for relative pitch and one for absolute pitch. For relative pitch, a solmization system was used.

Gongche notation used Chinese characters for the names of the scale.

Japanese music is highly diversified, and therefore requires various systems of notation. In Japanese shakuhachi music, for example, glissandos and timbres are often more significant than distinct pitches, whereas taiko notation focuses on discrete strokes.

Ryukyuan sanshin music uses kunkunshi, a notation system of kanji with each character corresponding to a finger position on a particular string.

Notation plays a relatively minor role in the oral traditions of Indonesia. However, in Java and Bali, several systems were devised beginning at the end of the 19th century, initially for archival purposes. Today the most widespread are cipher notations ("not angka" in the broadest sense) in which the pitches are represented with some subset of the numbers 1 to 7, with 1 corresponding to either highest note of a particular octave, as in Sundanese gamelan, or lowest, as in the kepatihan notation of Javanese gamelan.

Notes in the ranges outside the central octave are represented with one or more dots above or below the each number. For the most part, these cipher notations are mainly used to notate the skeletal melody (the balungan) and vocal parts (gerongan), although transcriptions of the elaborating instrument variations are sometimes used for analysis and teaching. Drum parts are notated with a system of symbols largely based on letters representing the vocables used to learn and remember drumming patterns; these symbols are typically laid out in a grid underneath the skeletal melody for a specific or generic piece.

The symbols used for drum notation (as well as the vocables represented) are highly variable from place to place and performer to performer. In addition to these current systems, two older notations used a kind of staff: the Solonese script could capture the flexible rhythms of the pesinden with a squiggle on a horizontal staff, while in Yogyakarta a ladder-like vertical staff allowed notation of the balungan by dots and also included important drum strokes. In Bali, there are a few books published of Gamelan gender wayang pieces, employing alphabetical notation in the old Balinese script.

Composers and scholars both Indonesian and foreign have also mapped the slendro and pelog tuning systems of gamelan onto the western staff, with and without various symbols for microtones. The Dutch composer Ton de Leeuw also invented a three line staff for his composition "Gending". However, these systems do not enjoy widespread use.

In the second half of the twentieth century, Indonesian musicians and scholars extended cipher notation to other oral traditions, and a diatonic scale cipher notation has become common for notating western-related genres (church hymns, popular songs, and so forth). Unlike the cipher notation for gamelan music, which uses a "fixed Do" (that is, 1 always corresponds to the same pitch, within the natural variability of gamelan tuning), Indonesian diatonic cipher notation is "moveable-Do" notation, so scores must indicate which pitch corresponds to the number 1 (for example, "1=C").

In pitch bracket notation music is written with melody lines and pitch brackets. Melody lines are like staff lines except they can change pitch by writing pitch brackets on them. Pitch brackets add or subtract scale steps to the melody line. The shape of the bracket (i.e. angle bracket), determines the number of scale steps to add. The direction of the bracket, opening or closing, determines whether to add or subtract scale steps. As a result of the mathematical nature of pitch bracket notation, arithmetic and algebra can be directly applied to the notation. Musical variations can be mathematically generated from their themes.

Cipher notation systems assigning Arabic numerals to the major scale degrees have been used at least since the Iberian organ tablatures of the 16th-century and include such exotic adaptations as "Siffernotskrift". The one most widely in use today is the Chinese "Jianpu", discussed in the main article. Numerals can of course also be assigned to different scale systems, as in the Javanese "kepatihan" notation described above.

Solfège is a way of assigning syllables to names of the musical scale. In order, they are today: "Do Re Mi Fa Sol La Ti Do" (for the octave). The classic variation is: "Do Re Mi Fa Sol La Si Do". The first Western system of functional names for the musical notes was introduced by Guido of Arezzo (c. 991 – after 1033), using the beginning syllables of the first six musical lines of the Latin hymn Ut queant laxis. The original sequence was "Ut Re Mi Fa Sol La", where each verse started a scale note higher. "Ut" later became "Do". The equivalent syllables used in Indian music are: "Sa Re Ga Ma Pa Dha Ni". See also: solfège, sargam, Kodály hand signs.

Tonic sol-fa is a type of notation using the initial letters of solfège.

The notes of the 12-tone scale can be written by their letter names A–G, possibly with a trailing sharp or flat symbol, such as A or B.

Tablature was first used in the Middle Ages for organ music and later in the Renaissance for lute music. In most lute tablatures, a staff is used, but instead of pitch values, the lines of the staff represent the strings of the instrument. The frets to finger are written on each line, indicated by letters or numbers. Rhythm is written separately with one or another variation of standard note values indicating the duration of the fastest moving part. Few seem to have remarked on the fact that tablature combines in one notation system both the physical and technical requirements of play (the lines and symbols on them and in relation to each other representing the actual performance actions) with the unfolding of the music itself (the lines of tablature taken horizontally represent the actual temporal unfolding of the music). In later periods, lute and guitar music was written with standard notation. Tablature caught interest again in the late 20th century for popular guitar music and other fretted instruments, being easy to transcribe and share over the internet in ASCII format. Websites like OLGA have archives of text-based popular music tablature.

Klavarskribo (sometimes shortened to klavar) is a music notation system that was introduced in 1931 by the Dutchman Cornelis Pot. The name means "keyboard writing" in Esperanto. It differs from conventional music notation in a number of ways and is intended to be easily readable. Many klavar readers are from the Netherlands.

Some chromatic systems have been created taking advantage of the layout of black and white keys of the standard piano keyboard. The "staff" is most widely referred to as "piano roll", created by extending the black and white piano keys.

Over the past three centuries, hundreds of music notation systems have been proposed as alternatives to traditional western music notation. Many of these systems seek to improve upon traditional notation by using a "chromatic staff" in which each of the 12 pitch classes has its own unique place on the staff. Examples are the "Ailler-Brennink" notation, Jacques-Daniel Rochat's Dodeka music notation, Tom Reed's "Twinline" notation, Russell Ambrose's "Ambrose Piano Tabs", Paul Morris' "Clairnote", John Keller's "Express Stave", and José A. Sotorrio's "Bilinear Music Notation". These notation systems do not require the use of standard key signatures, accidentals, or clef signs. They also represent interval relationships more consistently and accurately than traditional notation. The Music Notation Project (formerly known as the Music Notation Modernization Association) has a website with information on many of these notation systems.

The term 'graphic notation' refers to the contemporary use of non-traditional symbols and text to convey information about the performance of a piece of music. Practitioners include Christian Wolff, Earle Brown, Anthony Braxton, John Cage, Morton Feldman, Krzysztof Penderecki, Cornelius Cardew, and Roger Reynolds. See "Notations", edited by John Cage and Alison Knowles, .

Simplified Music Notation is an alternative form of musical notation designed to make sight-reading easier. It is based on classical staff notation, but incorporates sharps and flats into the shape of the note heads. Notes such as double sharps and double flats are written at the pitch they are actually played at, but preceded by symbols called "history signs" that show they have been transposed.

Modified Stave Notation (MSN) is an alternative way of notating music for people who cannot easily read ordinary musical notation even if it is enlarged.

Parsons code is used to encode music so that it can be easily searched.

Braille music is a complete, well developed, and internationally accepted musical notation system that has symbols and notational conventions quite independent of print music notation. It is linear in nature, similar to a printed language and different from the two-dimensional nature of standard printed music notation. To a degree Braille music resembles musical markup languages such as MusicXML or NIFF.

In integer notation, or the integer model of pitch, all pitch classes and intervals between pitch classes are designated using the numbers 0 through 11.

The standard form of rap notation is the "flow diagram", where rappers line up their lyrics underneath "beat numbers". Hip-hop scholars also make use of the same flow diagrams that rappers use: the books "How to Rap" and "How to Rap 2" extensively use the diagrams to explain rap's triplets, flams, rests, rhyme schemes, runs of rhyme, and breaking rhyme patterns, among other techniques. Similar systems are used by musicologists Adam Krims in his book "Rap Music and the Poetics of Identity" and Kyle Adams in his work on rap's flow. As rap revolves around a strong 4/4 beat, with certain syllables aligned to the beat, all the notational systems have a similar structure: they all have four beat numbers at the top of the diagram, so that syllables can be written in-line with the beat.

Many computer programs have been developed for creating music notation (called "scorewriters" or "music notation software"). Music may also be stored in various digital file formats for purposes other than graphic notation output.

According to Philip Tagg and Richard Middleton, musicology and to a degree European-influenced musical practice suffer from a 'notational centricity', a methodology slanted by the characteristics of notation. A variety of 20th- and 21st-century composers have dealt with this problem, either by adapting standard Western musical notation or by using graphic notation. These include George Crumb, Luciano Berio, Krzystof Penderecki, Earl Brown, John Cage, Witold Lutoslawski, and others.






</doc>
<doc id="20202" url="https://en.wikipedia.org/wiki?curid=20202" title="Meir Kahane">
Meir Kahane

Meir David HaKohen Kahane (; ; August 1, 1932 – November 5, 1990) was an Israeli-American ordained Orthodox rabbi, writer, and ultra-nationalist politician who served one term in Israel's Knesset. His legacy continues to influence militant and far-right political groups active today in Israel.

Kahane publicized his "Kahanism" ideology, which he claimed was simply Torah Judaism based on "halakha" (Jewish law), through published works, weekly articles, speeches, debates on college campuses and in synagogues throughout the United States, and appearances on various televised programs and radio shows. He was an intense advocate for Jewish causes, such as organizing defense squads and patrols in Jewish neighborhoods and demanding for the Soviet Union to "release its oppressed Jews". He supported violence against those he regarded as enemies of the Jewish people, called for immediate Jewish mass migration to Israel to avoid a potential "Holocaust" in the United States, supported the restriction of Israel's democracy to its Jewish citizens, hoped that Israel would eventually adopt "halakha", and endorsed the annexation of the West Bank and Gaza Strip.

Kahane proposed enforcing "halakha" as codified by Maimonides. Non-Jews wishing to dwell in Israel would have three options: remain as "resident strangers" with limited rights, leave Israel and receive compensation for their property, or be forcibly removed without compensation. While serving in the Knesset in the mid-1980s Kahane proposed numerous laws, none of which passed, to emphasize Judaism in public schools, reduce Israel's bureaucracy, forbid sexual relations between non-Jews and Jews, and end cultural meetings between Jewish and Arab students.

In 1968, Kahane was one of the co-founders of the Jewish Defense League (JDL) in the United States. In 1971, he co-founded Kach ("Thus"), a new political party in Israel. The same year, he was convicted in New York for conspiracy to manufacture explosives and received a suspended sentence of five years. In 1984, he became a member of the Knesset, when Kach gained its only-ever seat in parliamentary elections. In 1988, after polls showed Kach gaining popularity, the Israeli government banned Kach for being "racist" and "anti-democratic" under the terms of a law that it had just passed.

Kahane was assassinated in a Manhattan hotel by an Egyptian-born U.S. citizen in November 1990.

Martin David Kahane was born in Brooklyn, New York, in 1932 to an Orthodox Jewish family. His father, Yechezkel (Charles) Kahane, the author of the "Torah Yesharah", studied at Polish and Czech yeshivas, was involved in the Revisionist Zionist movement, and was a close friend of Ze'ev Jabotinsky.

As a teenager, Kahane became an ardent admirer of Jabotinsky and Peter Bergson, who were frequent guests in his parents' home. He joined the Betar (Brit Trumpeldor) youth wing of Revisionist Zionism. He was active in protests against Ernest Bevin, the British Foreign Secretary who maintained restrictions on the immigration of Jews, even Nazi death camp survivors, to Palestine after the end of the Second World War. In 1947, Kahane was arrested for throwing eggs and tomatoes at Bevin, who was disembarking at Pier 84 on a visit to New York. A photo of the arrest appeared in the "New York Daily News". In 1954, he became the Mazkir (Secretary) of Greater New York City's 16 Bnei Akiva chapters.

Kahane's formal education included elementary school at the Yeshiva of Flatbush, and he attended high school at both Abraham Lincoln High School and the Brooklyn Talmudical Academy. Kahane received his rabbinical ordination from the Mir Yeshiva, in Brooklyn, where he was especially admired by the head Rabbi Abraham Kalmanowitz, and he began going by his Hebrew name, Meir. He was fully conversant in the Tanakh (Jewish Bible), the Talmud, the Midrash and Jewish law. Subsequently, Kahane earned a B.A. in Political Science from Brooklyn College in 1954, a Bachelor of Law - LL.B. from New York Law School, and an M.A. in International Relations from New York University.

In 1956, Kahane married Libby Blum, with whom he had four children: Tzipporah, Tova, Baruch, and Binyamin.

Journalists Michael T. Kaufman and Robert I. Friedman have separately said that Kahane, under an alias and while already married, proposed to 21-year old model Gloria Jean D'Argenio (who used the stage name Estelle Donna Evans) in 1966. Kahane allegedly sent a letter to D'Argenio in which he unilaterally ended their relationship. In response, D'Argenio jumped off the Queensboro Bridge and died of her injuries the next day. Afterwards, Kahane started a charitable foundation in her name. In 2008, Kahane's wife dismissed the incident as lacking proof.

In 1958, Kahane became the rabbi of the Howard Beach Jewish Center in Queens, New York City. Although the synagogue was originally Conservative, rather than strictly Orthodox, the board of directors agreed to Kahane's conditions, which included resigning from the Conservative movement's United Synagogue of America, installing a partition separating men and women during prayer, instituting traditional prayers, and maintaining a kosher kitchen. At the Jewish Center, Kahane influenced many of the synagogue's youngsters to adopt a more observant lifestyle, which often troubled parents. He trained Arlo Guthrie for his bar mitzvah. When his contract was not renewed, he soon published an article entitled "End of the Miracle of Howard Beach". That was Kahane's first article in "The Jewish Press", an American Orthodox Jewish weekly for which he would continue to write for the rest of his life. Kahane also used the pen name David Sinai, and the pseudonyms Michael King, David Borac, and Martin Keene.

In the late 1950s and the early 1960s, Kahane's life of secrecy and his strong anticommunism landed him a position as a consultant with the FBI. According to his wife, Libby, his assignment was to infiltrate the anticommunist John Birch Society and report his findings to the FBI. 

At some time in the late 1950s, Kahane assumed the persona of a Gentile, along with the pseudonym Michael King. Kahane began openly expressing his anticommunism. He and Joseph Churba created the July Fourth Movement, which was formed to counteract widespread opposition towards U.S. involvement in the Vietnam War. Subsequently, they coauthored the book "The Jewish Stake in Vietnam", an attempt to convince American Jews of the "evil of Communism". The introduction states that, "All Americans have a stake in this grim war against Communism... It is vital that Jews realize the threat to their very survival [should Communism succeed]." Churba had a major falling out with Kahane over the use of paramilitary activities, and they parted ways permanently. Churba went on to pursue his own career, joining the U.S. Air Force, writing many books on the Middle East, and eventually becoming one of Ronald Reagan's consultants. Kahane chose to fight for Jewish rights, and was willing to use extreme measures. He even attempted to acquire and grow biological weapons to use on a Soviet military installation. He began using the phrase "Never Again" and conceived the Jewish Star and fist insignia, a symbol resembling that of the Black Panther Party. However, Kahane himself opposed the Black Panthers because they had supported anti-Jewish riots in Massachusetts and had left-wing views.

Kahane founded the Jewish Defense League (JDL) in New York City in 1968. Its self-described purpose was to protect Jews from local manifestations of anti-Semitism. The JDL said it was committed to five fundamental principles:


The JDL favored civil rights for blacks, but opposed black anti-Semites and racism of any form. In 1971, the JDL formed an alliance with a black rights group in what Kahane termed "a turning point in Black-Jewish relations". The Anti-Defamation League claimed that Kahane "preached a radical form of Jewish nationalism which reflected racism, violence and political extremism" that was replicated by Irv Rubin, the JDL's successor to Kahane.

A number of the JDL's members and leaders, including Kahane, were convicted of acts related to domestic terrorism. In 1971, Kahane was sentenced to a suspended five-year prison sentence for conspiring to manufacture explosives. In 1975, Kahane was arrested for leading the attack on the Soviet United Nations mission and injuring two officers, but he was released after being given summonses for disorderly conduct. Later the same year, Kahane was accused of conspiring to kidnap a Soviet diplomat, bomb the Iraqi embassy in Washington, and ship arms abroad from Israel. He was convicted of violating his probation for the 1971 bombing conviction and was sentenced to one year in prison. However, he served most of it in a hotel, with frequent unsupervised absences, because of a concession over the provision of kosher food.

In a 1984 interview with "Washington Post" correspondent Carla Hall, Kahane admitted that the JDL "bombed the Russian [Soviet] mission in New York, the Russian cultural mission here [Washington] in 1971, the Soviet trade offices".

In 1971, Kahane moved to Israel. At the time, he declared that he would focus on Jewish education. He later began gathering lists of Arab citizens of the State of Israel who were willing to emigrate for compensation, and eventually, he initiated protests that advocated the expulsion of Arabs from that country, and Israeli-occupied territories. In 1972, Jewish Defense League leaflets were distributed in Hebron, calling for the mayor to stand trial for the 1929 Hebron massacre. Kahane was arrested dozens of times. In 1971, he founded Kach, a political party that ran for the Knesset, the Israeli Parliament, during the 1973 general elections under the name "The League List". It won 12,811 votes (0.82%), just 2,857 (0.18%) short of the electoral threshold at the time (1%) for winning a Knesset seat. The party was even less successful in the 1977 elections, winning only 4,836 votes.

In 1980, Kahane was arrested for the 62nd time since his emigration, and he was jailed for six months after a detention order that was based on allegations of him planning armed attacks against Palestinians in response to the killings of Jewish settlers. Kahane was held in prison in Ramla, where he wrote the book "They Must Go". Kahane was banned from entering the United Kingdom in 1981.

In 1981, Kahane's party again ran for the Knesset during the 1981 elections, but it did not win a seat and received only 5,128 votes. In 1984, the Israeli Central Elections Committee banned him from being a candidate on the grounds that Kach was a racist party, but the Supreme Court of Israel overturned the ban on the grounds that the committee was not authorized to ban Kahane's candidacy. The Supreme Court suggested that the Knesset pass a law excluding racist parties from future elections. The Knesset responded in 1985 by amending the "Basic Law:Knesset" to include a prohibition (paragraph 7a) against registration of parties that explicitly or implicitly incite racism.

In the 1984 legislative elections, Kahane's Kach party received 25,907 votes, gaining one seat in the Knesset, which was taken by Kahane. He refused to take the standard oath of office and insisted on adding a Biblical verse from Psalms to indicate that national laws were overruled by the Torah if they conflict. Kahane's legislative proposals focused on Jewish education, an open economy, transferring the Arab population out of the Land of Israel, revoking Israeli citizenship from non-Jews, and banning Jewish-Gentile marriages and sexual relations. It was based on the Code of Jewish Law compiled by Maimonides in the Mishneh Torah.

While his popularity in Israel grew, Kahane was boycotted in the Knesset, where his speeches were often made to an assembly that was empty except for the duty chairman and the transcriptionist. Kahane's legislative proposals and motions of no-confidence against the government were ignored or rejected. Kahane often pejoratively called other Knesset members "Hellenists, a reference to Jews who assimilated into Greek culture after Judea's occupation by Alexander the Great. In 1987, Kahane opened a yeshiva ("HaRaayon HaYehudi") with funding from US supporters to teach "the Authentic Jewish Idea". Despite the boycott, his popularity grew among the Israeli public, especially for working-class Sephardi Jews. Polls showed that Kach would have likely received anywhere from four to twelve seats in the coming November 1988 elections.

In 1985, the Knesset passed an amendment to the Basic Law of Israel, barring political parties that incited to racism. The Central Elections Committee banned Kahane a second time, and he appealed to the Israeli Supreme Court. However, the Supreme Court this time ruled in favor of the committee, disqualifying Kach from running in the 1988 legislative elections. Kahane was thus the first candidate in Israel to be barred from election for racism. The move was criticized as being anti-democratic by Alan M. Dershowitz.

Also in 1985, the Knesset passed a law declaring that a Knesset member could only be an Israeli citizen. As a result of this legislation, Kahane renounced his United States citizenship. Following his banning from the Knesset, he tried to get his U.S. citizenship reinstated on the basis of the fact that he was compelled to relinquish it by the Knesset. It was not reinstated, but he was permitted to continue traveling to the United States.

In November 1990, Kahane gave a speech to an audience of mostly Orthodox Jews from Brooklyn, where he warned American Jews to immigrate to Israel before it was "too late". As a crowd gathered around Kahane in the second-floor lecture hall in Midtown Manhattan's New York Marriott East Side, Kahane was assassinated by El Sayyid Nosair, an Egyptian-born U.S. citizen who had trained in Pakistan. He was initially charged and acquitted of the murder. Nosair was later convicted of the murder in U.S. District Court for his involvement in the 1993 World Trade Center bombing. Prosecutors were able to try Nosair again for the murder because the federal indictment included the killing as part of the alleged terrorist conspiracy. He was sentenced to life imprisonment and later made a confession to federal agents.

Some researchers, such as Peter Lance, consider Kahane one of the first, if not the first, American victims of the new group Al Qaeda, since his killer is believed to have links to Osama bin Laden's network. The cell that Kahane's assassin belonged to had been training in the New York Metro system since mid-1989.

Kahane was buried on Har HaMenuchot, in Jerusalem. His funeral was one of the largest in Israel's history, and approximately 150,000 participated. He was eulogized by a number of prominent supporters in both the U.S. and in Israel, including Rabbi Moshe Tendler and the Sephardic Chief Rabbi of Israel, Mordechai Eliyahu, who spoke of how little the people understood of Kahane's "true value".

Kahane argued that there was a glory in Jewish destiny, which came through the observance of the Torah and "halakha" (Jewish law). He also noted, "Democracy and Judaism are not the same thing." Kahane also stressed the view that a Jewish state and a Western democracy were incompatible, since Western democracy is religion-blind, and a Jewish state is religion-oriented by its very name. He also warned of the danger of non-Jewish citizens becoming a majority and voting against the Jewish character of the state: "The question is as follows: if the Arabs settle among us and make enough children to become a majority, will Israel continue to be a Jewish state? Do we have to accept that the Arab majority will decide?" "Western democracy has to be ruled out. For me, that's cut and dried: There's no question of setting up democracy in Israel, because democracy means equal rights for all, irrespective of racial or religious origins."

Kahane proposed an "exchange of populations" that would continue the Jewish exodus from Arab lands: "A total of some 750,000 Jews fled Arab lands since 1948. Surely it is time for Jews, worried over the huge growth of Arabs in Israel, to consider finishing the exchange of populations that began 35 years ago." Kahane proposed a $40,000 compensation plan for Arabs who would leave voluntarily, and forcible expulsion for those who "don't want to leave". He encouraged retaliatory violence against Arabs who attacked Jews: "I approve of anybody who commits such acts of violence. Really, I don't think that we can sit back and watch Arabs throwing rocks at buses whenever they feel like it. They must understand that a bomb thrown at a Jewish bus is going to mean a bomb thrown at an Arab bus."

In some of his writings, Kahane argued that Israel should never start a war for territory but that if a war were launched against Israel, Biblical territory should be annexed. However, in an interview, he defined Israel's "minimal borders" as follows: "The southern boundary goes up to El Arish, which takes in all of northern Sinai, including Yamit. To the east, the frontier runs along the western part of the East Bank of the Jordan River, hence part of what is now Jordan. Eretz Yisrael also includes part of Lebanon and certain parts of Syria, and part of Iraq, all the way to the Euphrates River." When critics suggested that following Kahane's plans would mean a perpetual war between Jews and Arabs, Kahane responded, "There will be a perpetual war. With or without Kahane."


Following Kahane's death, no leader emerged to replace him in the movement although the idea of transferring populations, which was attributed mainly to Kahane, was subsequently incorporated into the political platform of various political parties in Israel, such as Moledet (applying to Arab non-citizen residents of the West Bank) and Yisrael Beiteinu (in the form of population exchange). Two small Kahanist factions later emerged; one under the name "Kach", and the other under the name "Kahane chai" (Hebrew: כהנא חי, literally "Kahane lives [on]"), the second one being led by his younger son, Binyamin Ze'ev Kahane. Neither one was permitted to participate in the Knesset elections by the Central Elections Committee.

In 1994, following the Cave of the Patriarchs massacre of Palestinian Muslim worshippers in Hebron by Kach supporter Baruch Goldstein, in which 29 Muslim worshipers were killed, the Israeli government declared both parties to be terrorist organizations. The US State Department also added Kach and Kahane Chai to its list of Foreign Terrorist Organizations.

In the 2003 Knesset elections, Herut, which had split off from the National Union list, ran with Michael Kleiner and former Kach activist Baruch Marzel taking the top two spots on the list. The joint effort narrowly missed the 1.5% barrier. In the following 2006 elections, the Jewish National Front, led by Baruch Marzel, fared better, but it also failed to pass the minimum threshold. A follower of Kahane who was involved with Kach for many years, Michael Ben-Ari, was elected to the Knesset in the 2009 elections on renewed National Union list. He stood again in the 2013 elections as the second candidate on the list of Otzma LeYisrael, but the party failed to pass the minimum threshold.

In 2007, the FBI released over a thousand documents relating to its daily surveillance of Kahane since the early 1960s.

In 2015, Kahane's grandson, Meir Ettinger, was detained by Israeli law enforcement. He was the alleged leader of the radical Jewish group "The Revolt". In an online "manifesto" echoing some of his grandfather's teachings, Ettinger promotes the "dispossession of gentiles" who live in Israel and the establishment of a new "kingdom of Israel", a theocracy ruled by Halacha. Ettinger's writings condemned Israel's government, mainstream rabbis, and the IDF, and also have denounced Christian churches as "idolatry".

In 2016, Kahane's widow claimed that modern Jewish extremists in Israel are not following the ideology of her late husband, Rabbi Meir Kahane. She justified that claim by arguing that unlike modern Jewish extremists, Rabbi Kahane had a more mature approach that did not encourage illegal activities.

In 2017, "The Forward" reported that some of Kahane's followers were aligning themselves with white nationalists and the alt-right. Other Kahanists declared that such moves did not reflect Kahane's teachings, and they supported that declaration by arguing that Kahane worked together with African Americans.





</doc>
<doc id="20203" url="https://en.wikipedia.org/wiki?curid=20203" title="Marietta Alboni">
Marietta Alboni

Marietta Alboni (6 March 1826 – 23 June 1894) was a renowned Italian contralto opera singer. She is considered as 'one of the greatest contraltos in operatic history'.

Alboni was born at Città di Castello, in Umbria. She became a pupil of of Cesena, Emilia–Romagna, and later of the composer Gioachino Rossini, when he was 'perpetual honorary adviser' in (and then the principal of) the Liceo Musicale, now Conservatorio Giovanni Battista Martini, in Bologna. Rossini tested the humble thirteen-year-old girl himself, had her admitted to the school with special treatment, and even procured her an early engagement to tour his "Stabat Mater" around Northern Italy, so that she could pay for her studies. After she achieved her diploma and made a modest debut in Bologna, in 1842, as "Climene" in Pacini's "Saffo", she obtained a triennial engagement thanks to Rossini's influence on the impresario Bartolomeo Merelli, Intendant at both Milan's Teatro alla Scala and Vienna's Imperial Kärntnertortheater. The favourable contract was signed by Rossini himself, "on behalf of Eustachio Alboni", father of Marietta, who was still a minor. The singer remained, throughout her life, deeply grateful to her ancient "maestro", nearly a second father to her.

Her debut at Teatro alla Scala took place in December 1842 as "Neocle" in the Italian version of "Le siège de Corinthe", which was followed by roles in operas by Marliani, Donizetti (as "Maffio Orsini" and "Leonora" in the Scala premiere of an Italian version of "La favorite"), Salvi and Pacini. In the season 1844–1845 she was engaged in the Saint Petersburg Imperial Bolshoi Kamenny Theatre; later, in 1846–47, she toured the principal cities of Central Europe, finally reaching London and Paris, where she settled permanently. In London, "she appeared in leading roles by Rossini and Donizetti (where she outshone Giulia Grisi and Jenny Lind) and also sang Cherubino (performing with Henriette Sontag)". For the 1848 London run of "Les Huguenots", Meyerbeer transposed the role of the page "Urbain" 'from soprano to contralto and composed the aria "Non! – non, non, non, non, non! Vous n'avez jamais, je gage" in Act 2' for her. On 28 August 1848, she sang at a concert in Manchester's Concert Hall, sharing the stage with Lorenzo Salvi and Frédéric Chopin. She toured the United States in 1852–53, appearing there with Camilla Urso.
In 1853 she wed a nobleman, Count Carlo Pepoli, of the Papal States, but she kept her maiden name for the stage. In 1863 she had to retire the first time on account of her husband's serious mental illness. He died in 1867. A year later, in 1868, Alboni would take part in the funeral of her beloved master and friend, Rossini, in the Église de la Sainte-Trinité. There she sang, alongside Adelina Patti, the leading soprano of the time, a stanza of "Dies irae", "Liber scriptum", adjusted to the music of the duet "Quis est Homo" from Rossini's own "Stabat Mater". Out of deference to her master, she also accepted to resume her singing career mainly in order to tour the orchestral version of the "Petite messe solennelle" around Europe. Rossini had once expressed his hope that she would take upon herself to perform it when he was dead. He had said that he had composed it, and especially the new section "O salutaris", just having her voice in mind.

In 1872 she permanently retired from the stage with four performances of "Fidalma" in Cimarosa's "Il matrimonio segreto", at the Paris Théâtre des Italiens but, in fact, she never gave up singing in private and in benefit concerts. When in 1887 the French and Italian Governments agreed upon moving the mortal remains of Rossini into the Basilica di Santa Croce in Florence, Alboni, then a sixty-one-year-old lady living in seclusion, wrote to the Italian Foreign Minister, Di Robilant, proposing that the "Petite Messe Solennelle", "the last musical composition by Rossini", be performed in Santa Croce the day of the funeral, and "demanding the honour, as an Italian and a pupil of the immortal Maestro," of singing it herself in her "dear and beloved homeland". Her wish, however, never came true and she was just given the chance of being present at the exhumation ceremony in Paris. The Paris correspondent of the Rome newspaper "Il Fanfulla" wrote on the occasion: "photographers snapped in the same shot the greatest performer of "Cenerentola" and "Semiramide", and what is left of the man who wrote these masterpieces".

In 1877 she had remarried—to a French military officer named Charles Zieger. She died at Ville-d'Avray, near Paris, in her "Villa La Cenerentola", and was buried at Père Lachaise Cemetery. Always engaged in charity (often in memory of Maestro Rossini), she left nearly all her estate to the poor of Paris. In her will she wrote that by singing she had earned all her fortune, and on singing she would pass away, with the sweet thought that she had employed it to encourage and to console.
Alboni's voice, an exceptionally fine contralto with a seamless compass of two and one-half octaves, extending as high as the soprano range was said to possess at once power, sweetness, fullness, and extraordinary flexibility. She had no peers in passages requiring a sensitive delivery and semi-religious calmness, owing to the moving quality of her velvety tone. She possessed vivacity, grace, and charm as an actress of the "comédienne" type; but she was not a natural "tragédienne", and her attempt at the strongly dramatic part of Norma was sometimes reported to have turned out a failure. Nevertheless, she scored a real triumph in 1850, when she made her operatic debut at the Paris Opéra performing the tragic role of "Fidès" in Meyerbeer's "Le prophète", which had been created the year before by no less than Pauline Viardot. Furthermore, she was able to cope with such dramatic roles as "Azucena" and "Ulrica" in Verdi's "Il trovatore" and "Un ballo in maschera", and even with the baritone role of "Don Carlo" in "Ernani" (London, 1847).

The following list of the roles performed by Marietta Alboni was drawn up by Arthur Pougin and published in his biography of the singer. It is reported here with the addition of further works and characters according to the sources stated in footnotes. 

Notes
Sources



</doc>
<doc id="20204" url="https://en.wikipedia.org/wiki?curid=20204" title="Manatee">
Manatee

Manatees (family Trichechidae, genus Trichechus) are large, fully aquatic, mostly herbivorous marine mammals sometimes known as sea cows. There are three accepted living species of Trichechidae, representing three of the four living species in the order Sirenia: the Amazonian manatee ("Trichechus inunguis"), the West Indian manatee ("Trichechus manatus"), and the West African manatee ("Trichechus senegalensis"). They measure up to long, weigh as much as , and have paddle-like flippers. The etymology of the name is dubious, with connections having been made to Latin "manus" (hand), and to a word sometimes cited as "manati" used by the Taíno, a pre-Columbian people of the Caribbean, meaning "breast". Manatees are occasionally called sea cows, as they are slow plant-eaters, peaceful and similar to cows on land. They often graze on water plants in tropical seas.

Manatees are three of the four living species in the order Sirenia. The fourth is the Eastern Hemisphere's dugong. The Sirenia are thought to have evolved from four-legged land mammals more than 60 million years ago, with the closest living relatives being the Proboscidea (elephants) and Hyracoidea (hyraxes).

The Amazonian's hair color is brownish gray, and it has thick wrinkled skin, often with coarse hair, or "whiskers". Photos are rare; although very little is known about this species, scientists think it is similar to West Indian manatee.

Manatees weigh , and average in length, sometimes growing to and (the females tend to be larger and heavier). At birth, baby manatees weigh about each. The manatee has a large, flexible, prehensile upper lip, used to gather food and eat and for social interaction and communication. Manatees have shorter snouts than their fellow sirenians, the dugongs. The lids of manatees' small, widely spaced eyes close in a circular manner. The adults have no incisor or canine teeth, just a set of cheek teeth, which are not clearly differentiated into molars and premolars. These teeth are repeatedly replaced throughout life, with new teeth growing at the rear as older teeth fall out from farther forward in the mouth, somewhat as elephants' teeth do. At any time, a manatee typically has no more than six teeth in each jaw of its mouth. Its tail is paddle-shaped, and is the clearest visible difference between manatees and dugongs; a dugong tail is fluked, similar in shape to that of a whale. The female manatee has two teats, one under each flipper, a characteristic that was used to make early links between the manatee and elephants.

The manatee is unusual among mammals in having just six cervical vertebrae, a number that may be due to mutations in the homeotic genes. All other mammals have seven cervical vertebrae, other than the two-toed and three-toed sloths.

Like the horse, the manatee has a simple stomach, but a large cecum, in which it can digest tough plant matter. Generally, the intestines are about 45 meters, unusually long for an animal of the manatee's size.

Apart from mothers with their young, or males following a receptive female, manatees are generally solitary animals. Manatees spend approximately 50% of the day sleeping submerged, surfacing for air regularly at intervals of less than 20 minutes. The remainder of the time is mostly spent grazing in shallow waters at depths of . The Florida subspecies ("T. m. latirostris") has been known to live up to 60 years.

Generally, manatees swim at about . However, they have been known to swim at up to in short bursts.

Manatees are capable of understanding discrimination tasks and show signs of complex associative learning. They also have good long-term memory. They demonstrate discrimination and task-learning abilities similar to dolphins and pinnipeds in acoustic and visual studies.

Manatees typically breed once every two years; generally only a single calf is born. Gestation lasts about 12 months and to wean the calf takes a further 12 to 18 months, although females may have more than one estrous cycle per year.

Manatees emit a wide range of sounds used in communication, especially between cows and their calves. Their ears are large internally but the external openings are small, and they are located four inches behind each eye. Adults communicate to maintain contact and during sexual and play behaviors. Taste and smell, in addition to sight, sound, and touch, may also be forms of communication.

Manatees are herbivores and eat over 60 different freshwater (e.g., floating hyacinth, pickerel weed, alligator weed, water lettuce, hydrilla, water celery, musk grass, mangrove leaves) and saltwater plants (e.g., sea grasses, shoal grass, manatee grass, turtle grass, widgeon grass, sea clover, and marine algae). Using their divided upper lip, an adult manatee will commonly eat up to 10%–15% of their body weight (about 50 kg) per day. Consuming such an amount requires the manatee to graze for up to seven hours a day. To be able to cope with the high levels of cellulose in their plant based diet, manatees utilize hindgut fermentation to help with the digestion process. Manatees have been known to eat small numbers of fish from nets.

Manatees use their flippers to "walk" along the bottom whilst they dig for plants and roots in the substrate. When plants are detected, the flippers are used to scoop the vegetation toward the manatee's lips. The manatee has prehensile lips; the upper lip pad is split into left and right sides which can move independently. The lips use seven muscles to manipulate and tear at plants. Manatees use their lips and front flippers to move the plants into the mouth. The manatee does not have front teeth, however, behind the lips, on the roof of the mouth, there are dense, ridged pads. These horny ridges, and the manatee's lower jaw, tear through ingested plant material.

Manatees have four rows of teeth. There are 6 to 8 high-crowned, open-rooted molars located along each side of the upper and lower jaw giving a total of 24 to 32 flat, rough-textured teeth. Eating gritty vegetation abrades the teeth, particularly the enamel crown; however, research indicates that the enamel structure in manatee molars is weak. To compensate for this, manatee teeth are continually replaced. When anterior molars wear down, they are shed. Posterior molars erupt at the back of the row and slowly move forward to replace these like enamel crowns on a conveyor belt, similarly to elephants. This process continues throughout the manatee's lifetime. The rate at which the teeth migrate forward depends on how quickly the anterior teeth abrade. Some studies indicate that the rate is about 1 cm/month although other studies indicate 0.1 cm/month.

Manatees inhabit the shallow, marshy coastal areas and rivers of the Caribbean Sea and the Gulf of Mexico ("T. manatus", West Indian manatee), the Amazon basin ("T. inunguis", Amazonian manatee), and West Africa ("T. senegalensis", West African manatee).

West Indian manatees prefer warmer temperatures and are known to congregate in shallow waters. They frequently migrate through brackish water estuaries to freshwater springs. They cannot survive below 15 °C (60 °F). Their natural source for warmth during winter is warm, spring-fed rivers.

The coast of the state of Georgia is usually the northernmost range of the West Indian manatees because their low metabolic rate does not protect them in cold water. Prolonged exposure to water below 20 °C (68 °F) can cause "cold stress syndrome" and death.

Florida manatees can move freely between fresh water and salt water.

Manatees have been seen as far north as Cape Cod, and in 1995 and again in 2006, one was seen in New York City and Rhode Island's Narragansett Bay. A manatee was spotted in the Wolf River harbor near the Mississippi River in downtown Memphis in 2006, and was later found dead 10 miles downriver in McKellar Lake.

The West Indian manatee migrates into Florida rivers—such as the Crystal, the Homosassa, and the Chassahowitzka rivers, whose headsprings are 22 °C (72 °F) all year. Between November and March each year, about 600 West Indian manatees gather in the rivers in Citrus County, Florida such as the Crystal River National Wildlife Refuge.

In winter, manatees often gather near the warm-water outflows of power plants along the Florida coast, instead of migrating south as they once did. Some conservationists are concerned that these manatees have become too reliant on these artificially warmed areas. The U.S. Fish and Wildlife Service is trying to find a new way to heat the water for manatees that depended on plants that have closed. The main water treatment plant in Guyana has four manatees that keep storage canals clear of weeds; there are also some in the ponds of the national park in Georgetown, Guyana.

Studies suggest that Florida manatees need access to fresh water for proper regulation of water and salts in their bodies.

Accurate population estimates of the Florida manatee ("T. manatus") are difficult. They have been called scientifically weak because they vary widely from year to year, some areas showing increases, others decreases, and little strong evidence of increases except in two areas. Manatee counts are highly variable without an accurate way to estimate numbers: In Florida in 1996, a winter survey found 2,639 manatees; in 1997, a January survey found 2,229, and a February survey found 1,706. A statewide synoptic survey in January 2010 found 5,067 manatees living in Florida, the highest number recorded to that time.

As of January 2016, the USFWS estimates the range-wide manatee population to be at least 13,000; as of January, 2018, at least 6,100 are estimated to be in Florida.

Population viability studies conducted in 1997 found that decreasing adult survival and eventual extinction were a probable future outcome for Florida manatees unless they got more protection. The U.S. Fish and Wildlife Service proposed downgrading the manatee's status from endangered to threatened in January 2016 after more than 40 years of the manatee's being classified as on the endangered.

Fossil remains of manatee ancestors - also known as sirenians - date back to the Early Eocene.

The freshwater Amazonian manatee ("T. inunguis") inhabits the Amazon River and its tributaries, and never ventures into salt water.

They are found in coastal marine and estuarine habitats, and in freshwater river systems along the west coast of Africa from the Senegal River south to the Cuanza River in Angola. They live as far upriver on the Niger River as Koulikoro in Mali, 2,000 km from the coast.

In relation to the threat posed by humans, predation does not present a significant threat to manatees. When threatened, the manatee's response is to dive as deeply as it can, suggesting that threats have most frequently come from land dwellers such as humans rather than from other water-dwelling creatures such as caimans or sharks. 

The main causes of death for manatees are human-related issues, such as habitat destruction and human objects. Natural causes of death include adverse temperatures, predation by crocodiles on young, and disease.

Their slow-moving, curious nature, coupled with dense coastal development, has led to many violent collisions with propeller-driven boats and ships, leading frequently to maiming, disfigurement, and even death. As a result, a large proportion of manatees exhibit spiral cutting propeller scars on their backs, usually caused by larger vessels that do not have skegs in front of the propellers like the smaller outboard and inboard-outboard recreational boats have. They are now even identified by humans based on their scar patterns. Many manatees have been cut in two by large vessels like ships and tug boats, even in the highly populated lower St. Johns River's narrow channels. Some are concerned that the current situation is inhumane, with upwards of 50 scars and disfigurements from vessel strikes on a single manatee. Often, the lacerations lead to infections, which can prove fatal. Internal injuries stemming from being trapped between hulls and docks and impacts have also been fatal. Recent testing shows that manatees may be able to hear speed boats and other watercraft approaching, due to the frequency the boat makes. However, a manatee may not be able to hear the approaching boats when they are performing day-to-day activities or distractions. The manatee has a tested frequency range of 8 kilohertz to 32 kilohertz.

Manatees hear on a higher frequency than would be expected for such large marine mammals. Many large boats emit very low frequencies, which confuse the manatee and explain their lack of awareness around boats. The Lloyd's mirror effect results in low frequency propeller sounds not being discernible near the surface, where most accidents occur. Research indicates that when a boat has a higher frequency the manatees rapidly swim away from danger.

In 2003, a population model was released by the United States Geological Survey that predicted an extremely grave situation confronting the manatee in both the Southwest and Atlantic regions where the vast majority of manatees are found. It states,
According to marine mammal veterinarians:
These veterinarians go on to state:
In 2009, of the 429 Florida manatees recorded dead, 97 were killed by commercial and recreational vessels, which broke the earlier record number of 95 set in 2002.

Another cause of manatee deaths are red tides, a term used for the proliferation, or "blooms", of the microscopic marine algae, "Karenia brevis". This dinoflagellate produces brevetoxins that can have toxic effects on the central nervous system of animals.

In 1996, a red tide was responsible for 151 manatee deaths. The bloom was present from early March to the end of April and killed approximately 15% of the known population of manatees along South Florida's western coast.
Other blooms in 1982 and 2005 resulted in 37 and 44 deaths, respectively.

Manatees can also be crushed and isolated in water control structures (navigation locks, floodgates, etc.) and are occasionally killed by entanglement in fishing gear, such as crab pot float lines, box traps, and shark nets.

While humans are allowed to swim with manatees in one area of Florida, there have been numerous charges of people harassing and disturbing the manatees. According to the United States Fish and Wildlife Service, approximately 99 manatee deaths each year are related to human activities. In January 2016, there were 43 manatee deaths in Florida alone.

All three species of manatee are listed by the World Conservation Union as vulnerable to extinction. However, The U.S. Fish and Wildlife Service (FWS) does not consider the West Indian manatee to be "endangered" anymore, having downgraded its status to "threatened" as of March 2017. They cite improvements to habitat conditions, population growth and reductions of threats as reasoning for the change. The new classification will not affect current federal protections. They were originally classified as endangered with the 1967 class of endangered species.

As of February 2016, 6,250 manatees were reported swimming in Florida's springs. It is illegal under federal and Florida law to injure or harm a manatee.

The MV "Freedom Star" and MV "Liberty Star", ships used by NASA to tow space shuttle solid rocket boosters back to Kennedy Space Center, were propelled only by water jets to protect the endangered manatee population that inhabits regions of the Banana River where the ships are based.

Brazil outlawed hunting in 1973 in an effort to preserve the species. Deaths by boat strikes are still common.

The oldest manatee in captivity was Snooty, at the South Florida Museum's Parker Manatee Aquarium in Bradenton, Florida. Born at the Miami Aquarium and Tackle Company on July 21, 1948, Snooty was one of the first recorded captive manatee births. Raised entirely in captivity, Snooty was never to be released into the wild. As such he was the only manatee at the aquarium, and one of only a few captive manatees in the United States that was allowed to interact with human handlers. That made him uniquely suitable for manatee research and education.

Snooty died suddenly two days after his 69th birthday, July 23, 2017, when he was found in an underwater area only used to access plumbing for the exhibit life support system. The South Florida Museum's initial press release stated, “Early indications are that an access panel door that is normally bolted shut had somehow been knocked loose and that Snooty was able to swim in.”

There are a number of manatee rehabilitation centers in the United States. These include three government-run critical care facilities in Florida at Lowry Park Zoo, Miami Seaquarium, and SeaWorld Orlando. After initial treatment at these facilities, the manatees are transferred to rehabilitation facilities before release. These include the Cincinnati Zoo and Botanical Garden, Columbus Zoo and Aquarium, Epcot's The Seas, South Florida Museum, and Homosassa Springs Wildlife State Park.

The Columbus Zoo was a founding member of the Manatee Rehabilitation Partnership in 2001. Since 1999, the zoo's Manatee Bay facility has helped rehabilitate 20 manatees. The Cincinnati Zoo has rehabilitated and released more than a dozen manatees since 1999.

Manatees can also be viewed in a number of European zoos, such as the Tierpark Berlin, the Nuremberg Zoo, in ZooParc de Beauval in France and in the Aquarium of Genoa in Italy. The River Safari at Singapore features seven of them.

The manatee has been linked to folklore on mermaids. In West African folklore, they were considered sacred and thought to have been once human. Killing one was taboo and required penance.

In the novel "Moby-Dick", Herman Melville distinguishes manatees ("Lamatins") from small whales; stating, "I am aware that down to the present time, the fish styled Lamatins and Dugongs (Pig-fish and Sow-fish of the Coffins of Nantucket) are included by many naturalists among the whales. But as these pig-fish are a noisy, contemptible set, mostly lurking in the mouths of rivers, and feeding on wet hay, and especially as they do not spout, I deny their credentials as whales; and have presented them with their passports to quit the Kingdom of Cetology."




</doc>
<doc id="20205" url="https://en.wikipedia.org/wiki?curid=20205" title="Marsupial">
Marsupial

Marsupials are any members of the mammalian infraclass Marsupialia. All extant marsupials are endemic to Australasia and the Americas. A distinctive characteristic common to these species is that most of the young are carried in a pouch. Well-known marsupials include kangaroos, wallabies, koalas, phalangeriformes, opossums, wombats, and Tasmanian devils. Some lesser-known marsupials are the dunnarts, potoroos, cuscuses and the extinct thylacine.

Marsupials represent the clade originating from the last common ancestor of extant metatherians. Like other mammals in the Metatheria, they give birth to relatively undeveloped young that often reside in a pouch located on their mothers' abdomen for a certain amount of time. Close to 70% of the 334 extant species occur on the Australian continent (the mainland, Tasmania, New Guinea and nearby islands). The remaining 30% are found in the Americas — primarily in South America, but thirteen in Central America, and one in North America, north of Mexico.

The word "marsupial" comes from "marsupium", the technical term for the abdominal pouch. It, in turn, is borrowed from Latin and ultimately from the ancient Greek , meaning "pouch".

Marsupials are taxonomically identified as members of mammalian infraclass Marsupialia, first described as a family under the order Pollicata by German zoologist Johann Karl Wilhelm Illiger in his 1811 work "Prodromus Systematis Mammalium et Avium". However, James Rennie, author of "The Natural History of Monkeys, Opossums and Lemurs" (1838), pointed out that the placement of five different groups of mammals – monkeys, lemurs, tarsiers, aye-ayes and marsupials (with the exception of kangaroos, that were placed under the order Salientia) – under a single order (Pollicata) did not appear to have a strong justification. In 1816, French zoologist George Cuvier classified all marsupials under the order Marsupialia. In 1997, researcher J. A. W. Kirsch and others accorded infraclass rank to Marsupialia. There are two primary divisions: American marsupials (Ameridelphia) and Australian marsupials (Australidelphia).

Marsupialia is further divided as follows:

† – Extinct


Comprising over 300 extant species, several attempts have been made to accurately interpret the phylogenetic relationships among the different marsupial orders. Studies differ on whether Didelphimorphia or Paucituberculata is the sister group to all other marsupials. Though the order Microbiotheria (which has only one species, the monito del monte) is found in South America, morphological similarities suggest it is closely related to Australian marsupials. Molecular analyses in 2010 and 2011 identified Microbiotheria as the sister group to all Australian marsupials. However, the relations among the four Australidelphid orders are not as well understood. The cladogram below, depicting the relationships among the various marsupial orders, is based on a 2015 phylogenetic study.

DNA evidence supports a South American origin for marsupials, with Australian marsupials arising from a single Gondwanan migration of marsupials from South America to Australia. There are many small arboreal species in each group. The term "opossum" is used to refer to American species (though "possum" is a common abbreviation), while similar Australian species are properly called "possums".

Marsupials have the typical characteristics of mammals—e.g., mammary glands, three middle ear bones, and true hair. There are, however, striking differences as well as a number of anatomical features that separate them from Eutherians.

In addition to the front pouch, which contains multiple teats for the sustenance of their young, marsupials have other common structural features. Ossified patellae are absent in most modern marsupials (though a small number of exceptions are reported) and epipubic bones are present. Marsupials (and monotremes) also lack a gross communication (corpus callosum) between the right and left brain hemispheres.

The skull has peculiarities in comparison to placental mammals. In general, the skull is relatively small and tight. Holes ("foramen lacrimale") are located in the front of the orbit. The cheekbone is enlarged and extends further to the rear. The angular extension ("processus angularis") of the lower jaw is bent toward the center. Another feature is the hard palate which, in contrast to the placental mammals' foramina, always have more openings. The teeth differ from that of placental mammals, so that all taxa except wombats have a different number of incisors in the upper and lower jaws. The early marsupials had a dental formula from 5/4 – 1/1 – 3/3 – 4/4, that is, per pine half; they have five maxilla or four mandibular incisors, one canine, three premolars and four molars, for a total of 50 teeth. Some taxa, such as the opossum, have the original number of teeth. In other groups the number of teeth is reduced. The dental formula for Macropodidae (kangaroos and wallabies etc.) is 3/1 – (0 or 1)/0 – 2/2 – 4/4. Marsupials in many cases have 40 to 50 teeth, significantly more than placental mammals. The upper jaw has a high number of incisors, up to ten, and they have more molars than premolars. The second set of teeth grows in only at the 3rd premolar: all remaining teeth are already created as permanent teeth.

Few general characteristics describe their skeleton. In addition to details in the construction of the ankle, bones ("Ossa epubica") are characteristic, two from the pubic bone of the pelvis, which is a forwardly projecting bone. Since these are present in males and pouchless species, it is believed that they originally had nothing to do with reproduction, but served in the muscular approach to the movement of the hind limbs. This could be explained by an original feature of mammals, as these epipubic bones are also found in monotremes. Marsupial reproductive organs differ from the placental mammals. For them, the reproductive tract is doubled. The females have two uteri and two vaginas, and before birth, a birth canal forms between them, the median vagina. The males have a split or double penis lying in front of the scrotum.

A pouch is present in most, but not all, species. Many marsupials have a permanent bag, whereas in others the pouch develops during gestation, as with the shrew opossum, where the young are hidden only by skin folds or in the fur of the mother. The arrangement of the pouch is variable to allow the offspring to receive maximum protection. Locomotive kangaroos have a pouch opening at the front, while many others that walk or climb on all fours have the opening in the back. Usually, only females have a pouch, but the male water opossum has a pouch that is used to accommodate his genitalia while swimming or running.

Marsupials have adapted to many habitats, reflected in the wide variety in their build. The largest living marsupial, the red kangaroo, grows up to 1.8m (5'11") in height and in weight, but extinct genera, such as "Diprotodon", were significantly larger and heavier. The smallest members of this group are the marsupial mice, which often reach only in body length.

Some species resemble placental mammals and are examples of convergent evolution. The extinct "Thylacine" strongly resembled the placental wolf, hence its nickname "Tasmanian wolf". Flying and the associated ability to glide occurred both with marsupials (as with sugar gliders) and some placental mammals (as with flying squirrels), which developed independently. Other groups such as the kangaroo, however, do not have clear placental counterparts, though they share similarities in lifestyle and ecological niches with ruminants.

Marsupials' reproductive systems differ markedly from those of placental mammals. During embryonic development, a choriovitelline placenta forms in all marsupials. In bandicoots, an additional chorioallantoic placenta forms, although it lacks the chorionic villi found in eutherian placentas.

The evolution of reproduction in marsupials, and speculation about the ancestral state of mammalian reproduction, have engaged discussion since the end of the 19th century. Both sexes possess a cloaca, which is connected to a urogenital sac used to store waste before expulsion. The bladder of marsupials functions as a site to concentrate urine and empties into the common urogenital sinus in both females and males.

Most male marsupials, except for macropods and marsupial moles, have a bifurcated penis, separated into two columns, so that the penis has two ends corresponding to the females' two vaginas. The penis is used only during copulation, and is separate from the urinary tract. It curves forward when erect, and when not erect, it is retracted into the body in an S-shaped curve. Neither marsupials nor monotremes possess a baculum. The shape of the glans penis varies among marsupial species.

The male thylacine had a pouch that acted as a protective sheath, covering his external reproductive organs while he ran through thick brush.

The shape of the urethral grooves of the males' genitalia is used to distinguish between "Monodelphis brevicaudata", "Monodelphis domestica", and "Monodelphis americana". The grooves form 2 separate channels that form the ventral and dorsal folds of the erectile tissue. Several species of dasyurid marsupials can also be distinguished by their penis morphology.

The only accessory sex glands marsupials possess are the prostate and bulbourethral glands. There are no ampullae, seminal vesicles or coagulating glands. The prostate is proportionally larger in marsupials than in placental mammals. During the breeding season, the male tammar wallaby's prostate and bulbourethral gland enlarge. However, there does not appear to be any seasonal difference in the weight of the testes.

Female marsupials have two lateral vaginas, which lead to separate uteri, but both open externally through the same orifice. A third canal, the median vagina, is used for birth. This canal can be transitory or permanent. Some marsupial species are able to store sperm in the oviduct after mating.

Marsupials give birth at a very early stage of development; after birth, newborn marsupials crawl up the bodies of their mothers and attach themselves to a teat, which is located on the underside of the mother, either inside a pouch called the marsupium, or open to the environment. There they remain for a number of weeks, attached to the teat. The offspring are eventually able to leave the marsupium for short periods, returning to it for warmth, protection, and nourishment.

Pre-natal development differs between marsupials and placental mammals. Key aspects of the first stages of placental mammal embryo development, such as the inner cell mass and the process of compaction, are not found in marsupials. The cleavage stages of marsupial development are very variable between groups and aspects of marsupial early development are not yet fully understood.

An early birth removes a developing marsupial from its mother's body much sooner than in placental mammals, thus marsupials have not developed a complex placenta to protect the embryo from its mother's immune system. Though early birth puts the tiny newborn marsupial at a greater environmental risk, it significantly reduces the dangers associated with long pregnancies, as there is no need to carry a large fetus to full term in bad seasons. Marsupials are extremely altricial animals, needing to be intensely cared for immediately following birth (cf. precocial).

Because newborn marsupials must climb up to their mother's teats, their front limbs and facial structures are much more developed than the rest of their bodies at the time of birth. This requirement has been argued to have resulted in the limited range of locomotor adaptations in marsupials compared to placentals. Marsupials must develop grasping forepaws during their early youth, making the evolutive transition from these limbs into hooves, wings, or flippers, as some groups of placental mammals have done, more difficult. However, several marsupials do possess atypical forelimb morphologies, such as the hooved forelimbs of the pig-footed bandicoot, suggesting that the range of forelimb specialization is not as limited as assumed.

An infant marsupial is known as a joey. Marsupials have a very short gestation period—usually around four to five weeks, but as low as 12 days for some species—and the joey is born in an essentially fetal state. The blind, furless, miniature newborn, the size of a jelly bean, crawls across its mother's fur to make its way into the pouch, where it latches onto a teat for food. It will not re-emerge for several months, during which time it develops fully. After this period, the joey begins to spend increasing lengths of time out of the pouch, feeding and learning survival skills. However, it returns to the pouch to sleep, and if danger threatens, it will seek refuge in its mother's pouch for safety.

Joeys stay in the pouch for up to a year in some species, or until the next joey is born. A marsupial joey is unable to regulate its own body temperature and relies upon an external heat source. Until the joey is well-furred and old enough to leave the pouch, a pouch temperature of must be constantly maintained.

Joeys are born with "oral shields". In species without pouches or with rudimentary pouches these are more developed than in forms with well-developed pouches, implying a role in maintaining the young attached to the mother's teat.

The first American marsupial the Europeans encountered was the common opossum. Vicente Yáñez Pinzón, commander of the "Niña" on Christopher Columbus' first voyage in the late 1400s, collected a female opossum with young in her pouch off the Brazilian coast. He presented them to the Spanish monarchs, though by then the young were lost and the female had died. The animal was noted for its strange pouch or "second belly", and how the offspring reached the pouch was a mystery.

On the other hand, it was the Portuguese who first described Australian marsupials. António Galvão, a Portuguese administrator in Ternate (1536–40), wrote a detailed account of the northern common cuscus ("Phalanger orientalis"):

From the start of the 17th century more accounts of marsupials arrived. For instance, a 1606 record of an animal, killed on the southern coast of New Guinea, described it as "in the shape of a dog, smaller than a greyhound", with a snakelike "bare scaly tail" and hanging testicles. The meat tasted like venison, and the stomach contained ginger leaves. This description appears to closely resemble the dusky pademelon ("Thylogale brunii"), in which case this would be the earliest European record of a member of the kangaroo family (Macropodidae).

The relationships among the three extant divisions of mammals (monotremes, marsupials, and placentals) were long a matter of debate among taxonomists. Most morphological evidence comparing traits such as number and arrangement of teeth and structure of the reproductive and waste elimination systems as well as most genetic and molecular evidence favors a closer evolutionary relationship between the marsupials and placental mammals than either has with the monotremes.
The ancestors of marsupials, part of a larger group called metatherians, probably split from those of placental mammals (eutherians) during the mid-Jurassic period, though no fossil evidence of metatherians themselves are known from this time. From DNA and protein analyses, the time of divergence of the two lineages has been estimated to be around 100 to 120 mya. Fossil metatherians are distinguished from eutherians by the form of their teeth; metatherians possess four pairs of molar teeth in each jaw, whereas eutherian mammals (including true placentals) never have more than three pairs. Using this criterion, the earliest known metatherian is "Sinodelphys szalayi", which lived in China around 125 mya. This makes it a contemporary to some early eutherian species which have been found in the same area. While placental fossils dominate in Asia, marsupial fossils occur in larger numbers in North America.

The oldest metatherian fossils are found in present-day China. About 100 mya, the supercontinent Pangaea was in the process of splitting into the northern continent Laurasia and the southern continent Gondwana, with what would become China and Australia already separated by the Tethys Ocean. From there, metatherians spread westward into modern North America (still attached to Eurasia), where the earliest true marsupials are found. Marsupials are difficult to distinguish from other fossils, as they are characterized by aspects of the reproductive system which do not normally fossilize (including pouches) and by subtle changes in the bone and tooth structure that show a metatherian is part of the marsupial crown group (the most exclusive group that contains all living marsupials). The earliest definite marsupial fossil belongs to the species "Peradectes minor", from the Paleocene of Montana, dated to about 65 million years ago. From their point of origin in Laurasia, marsupials spread to South America, which was possibly connected to North America at around 65 mya through a ridge that has since moved on to become the Caribbean Archipelago. Laurasian marsupials eventually died off, for not entirely clear reasons; convention has it that they disappeared due to competition with placentals, but this is no longer accepted to be the primary reason.

Marsupials, "Peradectes" and the related Herpetotheriidae are nested within a clade of metatherians that also included a variety of Cretaceous North American taxa.

In South America, the opossums evolved and developed a strong presence, and the Paleogene also saw the evolution of shrew opossums (Paucituberculata) alongside non-marsupial metatherian predators such as the borhyaenids and the saber-toothed "Thylacosmilus". South American niches for mammalian carnivores were dominated by these marsupial and sparassodont metatherians, which seem to have competitively excluded South American placentals from evolving carnivory. While placental predators were absent, the metatherians did have to contend with avian (terror bird) and terrestrial crocodylomorph competition. Marsupials were excluded in turn from large herbivore niches in South America by the presence of native placental ungulates (now extinct) and xenarthrans (whose largest forms are also extinct). South America and Antarctica remained connected until 35 mya, as shown by the unique fossils found there. North and South America were disconnected until about three million years ago, when the Isthmus of Panama formed. This led to the Great American Interchange. Sparassodonts disappeared for unclear reasons – again, this has classically assumed as competition from carnivoran placentals, but the last sparassodonts co-existed with a few small carnivorans like procyonids and canines, and disappeared long before the arrival of macropredatory forms like felines, while didelphimorphs (opossums) invaded Central America, with the Virginia opossum reaching as far north as Canada.

Marsupials reached Australia via Antarctica about 50 mya, shortly after Australia had split off. This suggests a single dispersion event of just one species, most likely a relative to South America's monito del monte (a microbiothere, the only New World australidelphian). This progenitor may have rafted across the widening, but still narrow, gap between Australia and Antarctica. The journey must not have been easy; South American ungulate and xenarthran remains have been found in Antarctica, but these groups did not reach Australia.

In Australia, marsupials radiated into the wide variety seen today, including not only omnivorous and carnivorous forms such as were present in South America, but also into large herbivores. Modern marsupials appear to have reached the islands of New Guinea and Sulawesi relatively recently via Australia. A 2010 analysis of retroposon insertion sites in the nuclear DNA of a variety of marsupials has confirmed all living marsupials have South American ancestors. The branching sequence of marsupial orders indicated by the study puts Didelphimorphia in the most basal position, followed by Paucituberculata, then Microbiotheria, and ending with the radiation of Australian marsupials. This indicates that Australidelphia arose in South America, and reached Australia after Microbiotheria split off.

In Australia, terrestrial placental mammals disappeared early in the Cenozoic (their most recent known fossils being 55 million-year-old teeth resembling those of condylarths) for reasons that are not clear, allowing marsupials to dominate the Australian ecosystem. Extant native Australian terrestrial placental mammals (such as hopping mice) are relatively recent immigrants, arriving via island hopping from Southeast Asia.

Genetic analysis suggests a divergence date between the marsupials and the placentals at . The ancestral number of chromosomes has been estimated to be 2n = 14.

A new hypothesis suggests that South American microbiotheres resulted from a back-dispersal from eastern Gondwana due to new cranial and post-cranial marsupial fossils from the "Djarthia murgonensis" from the early Eocene Tingamarra Local Fauna in Australia that indicate the "Djarthia murgonensis" is the most plesiomorphic, the oldest unequivocal australidelphian, and may be the ancestral morphotype of the Australian marsupial radiation.




</doc>
<doc id="20206" url="https://en.wikipedia.org/wiki?curid=20206" title="Manchester">
Manchester

Manchester () is a city and metropolitan borough in Greater Manchester, England, with a population of 547,627 as of 2018 (5th most populous English district). It lies within the United Kingdom's second-most populous urban area, with a population of 2.9 million, and second-most populous metropolitan area, with a population of 3.3 million. It is fringed by the Cheshire Plain to the south, the Pennines to the north and east, and an arc of towns with which it forms a continuous conurbation. The local authority for the city is Manchester City Council.

The recorded history of Manchester began with the civilian settlement associated with the Roman fort of "Mamucium" or "Mancunium", which was established in about AD 79 on a sandstone bluff near the confluence of the rivers Medlock and Irwell. It is historically a part of Lancashire, although areas of Cheshire south of the River Mersey were incorporated in the 20th century. The first to be included, Wythenshawe, was added to the city in 1931. Throughout the Middle Ages Manchester remained a manorial township, but began to expand "at an astonishing rate" around the turn of the 19th century. Manchester's unplanned urbanisation was brought on by a boom in textile manufacture during the Industrial Revolution, and resulted in it becoming the world's first industrialised city. Manchester achieved city status in 1853. The Manchester Ship Canal opened in 1894, creating the Port of Manchester and directly linking the city to the Irish Sea, to the west. Its fortune declined after the Second World War, owing to deindustrialisation, but the IRA bombing in 1996 led to extensive investment and regeneration. Following successful redevelopment after the IRA bombing, Manchester was the host city for the 2002 Commonwealth Games.

Manchester is the second most visited city in the UK, after London. It is notable for its architecture, culture, musical exports, media links, scientific and engineering output, social impact, sports clubs and transport connections.

Manchester is a city of notable firsts, Manchester Liverpool Road railway station was the world's first inter-city passenger railway station. The city has also excelled in scientific advancement, as it was at The University of Manchester in 1917 that scientist Ernest Rutherford first split the atom, in 1948 Frederic C. Williams, Tom Kilburn, and Geoff Tootill developed and built the world's first stored-program computer, and in 2004 Andre Geim and Konstantin Novoselov successfully isolated and characterised the first graphene.

The name "Manchester" originates from the Latin name "Mamucium" or its variant "Mancunio" and the citizens are still referred to as Mancunians (). These names are generally thought to represent a Latinisation of an original Brittonic name. The generally accepted etymology of this name is that it comes from Brittonic *"mamm-" ("breast", in reference to a "breast-like hill"). However, more recent work suggests that it could come from *"mamma" ("mother", in reference to a local river goddess). Both usages are preserved in Insular Celtic languages, such as "mam" meaning "breast" in Irish and "mother" in Welsh. The suffix "-chester" is from Old English "ceaster" ("Roman fortification", itself a loanword from Latin "castra", "fort; fortified town").

The Brigantes were the major Celtic tribe in what is now known as Northern England; they had a stronghold in the locality at a sandstone outcrop on which Manchester Cathedral now stands, opposite the bank River Irwell. Their territory extended across the fertile lowland of what is now Salford and Stretford. Following the Roman conquest of Britain in the 1st century, General Agricola ordered the construction of a fort named Mamucium in the year 79 to ensure that Roman interests in Deva Victrix (Chester) and Eboracum (York) were protected from the Brigantes. Central Manchester has been permanently settled since this time. A stabilised fragment of foundations of the final version of the Roman fort is visible in Castlefield. The Roman habitation of Manchester probably ended around the 3rd century; its civilian settlement appears to have been abandoned by the mid-3rd century, although the fort may have supported a small garrison until the late 3rd or early 4th century. After the Roman withdrawal and Saxon conquest, the focus of settlement shifted to the confluence of the Irwell and Irk sometime before the arrival of the Normans after 1066. Much of the wider area was laid waste in the subsequent Harrying of the North.

In the doomsday book Manchester is recorded as within the hundred of Salford and held as tenant in chief by a Norman named Roger of Poitou, later being held by the family of De Gresle, lord of the manor and residents of Manchester Castle until 1215 before a Manor House was built. By 1421 Thomas de la Warre founded and constructed a collegiate church for the parish, now Manchester Cathedral; the domestic premises of the college house Chetham's School of Music and Chetham's Library. The library, which opened in 1653 and is still open to the public today, is the oldest free public reference library in the United Kingdom.

Manchester is mentioned as having a market in 1282. Around the 14th century, Manchester received an influx of Flemish weavers, sometimes credited as the foundation of the region's textile industry. Manchester became an important centre for the manufacture and trade of woollens and linen, and by about 1540, had expanded to become, in John Leland's words, "The fairest, best builded, quickest, and most populous town of all Lancashire." The cathedral and Chetham's buildings are the only significant survivors of Leland's Manchester.

During the English Civil War Manchester strongly favoured the Parliamentary interest. Although not long-lasting, Cromwell granted it the right to elect its own MP. Charles Worsley, who sat for the city for only a year, was later appointed Major General for Lancashire, Cheshire and Staffordshire during the Rule of the Major Generals. He was a diligent puritan, turning out ale houses and banning the celebration of Christmas; he died in 1656.

Significant quantities of cotton began to be used after about 1600, firstly in linen/cotton fustians, but by around 1750 pure cotton fabrics were being produced and cotton had overtaken wool in importance. The Irwell and Mersey were made navigable by 1736, opening a route from Manchester to the sea docks on the Mersey. The Bridgewater Canal, Britain's first wholly artificial waterway, was opened in 1761, bringing coal from mines at Worsley to central Manchester. The canal was extended to the Mersey at Runcorn by 1776. The combination of competition and improved efficiency halved the cost of coal and halved the transport cost of raw cotton. Manchester became the dominant marketplace for textiles produced in the surrounding towns. A commodities exchange, opened in 1729, and numerous large warehouses, aided commerce. In 1780, Richard Arkwright began construction of Manchester's first cotton mill. In the early 1800s, John Dalton formulated his atomic theory in Manchester.

Manchester's history is concerned with textile manufacture during the Industrial Revolution. The great majority of cotton spinning took place in the towns of south Lancashire and north Cheshire, and Manchester was for a time the most productive centre of cotton processing. 

Manchester became known as the world's largest marketplace for cotton goods and was dubbed "Cottonopolis" and "Warehouse City" during the Victorian era. In Australia, New Zealand and South Africa, the term "manchester" is still used for household linen: sheets, pillow cases, towels, etc. The industrial revolution brought about huge change in Manchester and was key to the increase in Manchester's population.

Manchester began expanding "at an astonishing rate" around the turn of the 19th century as people flocked to the city for work from Scotland, Wales, Ireland and other areas of England as part of a process of unplanned urbanisation brought on by the Industrial Revolution. It developed a wide range of industries, so that by 1835 "Manchester was without challenge the first and greatest industrial city in the world." Engineering firms initially made machines for the cotton trade, but diversified into general manufacture. Similarly, the chemical industry started by producing bleaches and dyes, but expanded into other areas. Commerce was supported by financial service industries such as banking and insurance. Trade, and feeding the growing population, required a large transport and distribution infrastructure: the canal system was extended, and Manchester became one end of the world's first intercity passenger railway—the Liverpool and Manchester Railway. Competition between the various forms of transport kept costs down. In 1878 the GPO (the forerunner of British Telecom) provided its first telephones to a firm in Manchester.

The Manchester Ship Canal was built between 1888 and 1894, in some sections by canalisation of the Rivers Irwell and Mersey, running from Salford to Eastham Locks on the tidal Mersey. This enabled oceangoing ships to sail right into the Port of Manchester. On the canal's banks, just outside the borough, the world's first industrial estate was created at Trafford Park. Large quantities of machinery, including cotton processing plant, were exported around the world.

A centre of capitalism, Manchester was once the scene of bread and labour riots, as well as calls for greater political recognition by the city's working and non-titled classes. One such gathering ended with the Peterloo Massacre of 16 August 1819. The economic school of Manchester capitalism developed there, and Manchester was the centre of the Anti-Corn Law League from 1838 onward.

Manchester has a notable place in the history of Marxism and left-wing politics; being the subject of Friedrich Engels' work "The Condition of the Working Class in England in 1844"; Engels spent much of his life in and around Manchester, and when Karl Marx visited Manchester, they met at Chetham's Library. The economics books Marx was reading at the time can be seen in the library, as can the window seat where Marx and Engels would meet. The first Trades Union Congress was held in Manchester (at the Mechanics' Institute, David Street), from 2 to 6 June 1868. Manchester was an important cradle of the Labour Party and the Suffragette Movement.

At that time, it seemed a place in which anything could happen—new industrial processes, new ways of thinking (the Manchester School, promoting free trade and "laissez-faire"), new classes or groups in society, new religious sects, and new forms of labour organisation. It attracted educated visitors from all parts of Britain and Europe. A saying capturing this sense of innovation survives today: "What Manchester does today, the rest of the world does tomorrow." Manchester's golden age was perhaps the last quarter of the 19th century. Many of the great public buildings (including Manchester Town Hall) date from then. The city's cosmopolitan atmosphere contributed to a vibrant culture, which included the Hallé Orchestra. In 1889, when county councils were created in England, the municipal borough became a county borough with even greater autonomy.

Although the Industrial Revolution brought wealth to the city, it also brought poverty and squalor to a large part of the population. Historian Simon Schama noted that "Manchester was the very best and the very worst taken to terrifying extremes, a new kind of city in the world; the chimneys of industrial suburbs greeting you with columns of smoke". An American visitor taken to Manchester's blackspots saw "wretched, defrauded, oppressed, crushed human nature, lying and bleeding fragments".

The number of cotton mills in Manchester itself reached a peak of 108 in 1853. Thereafter the number began to decline and Manchester was surpassed as the largest centre of cotton spinning by Bolton in the 1850s and Oldham in the 1860s. However, this period of decline coincided with the rise of the city as the financial centre of the region. Manchester continued to process cotton, and in 1913, 65% of the world's cotton was processed in the area. The First World War interrupted access to the export markets. Cotton processing in other parts of the world increased, often on machines produced in Manchester. Manchester suffered greatly from the Great Depression and the underlying structural changes that began to supplant the old industries, including textile manufacture.

Like most of the UK, the Manchester area was mobilised extensively during the Second World War. For example, casting and machining expertise at Beyer, Peacock and Company's locomotive works in Gorton was switched to bomb making; Dunlop's rubber works in Chorlton-on-Medlock made barrage balloons; and just outside the city in Trafford Park, engineers Metropolitan-Vickers made Avro Manchester and Avro Lancaster bombers and Ford built the Rolls-Royce Merlin engines to power them. Manchester was thus the target of bombing by the Luftwaffe, and by late 1940 air raids were taking place against non-military targets. The biggest took place during the "Christmas Blitz" on the nights of 22/23 and 24 December 1940, when an estimated of high explosives plus over 37,000 incendiary bombs were dropped. A large part of the historic city centre was destroyed, including 165 warehouses, 200 business premises, and 150 offices. 376 were killed and 30,000 houses were damaged. Manchester Cathedral was among the buildings seriously damaged; its restoration took 20 years.

Cotton processing and trading continued to fall in peacetime, and the exchange closed in 1968. By 1963 the port of Manchester was the UK's third largest, and employed over 3,000 men, but the canal was unable to handle the increasingly large container ships. Traffic declined, and the port closed in 1982. Heavy industry suffered a downturn from the 1960s and was greatly reduced under the economic policies followed by Margaret Thatcher's government after 1979. Manchester lost 150,000 jobs in manufacturing between 1961 and 1983.
Regeneration began in the late 1980s, with initiatives such as the Metrolink, the Bridgewater Concert Hall, the Manchester Arena, and (in Salford) the rebranding of the port as Salford Quays. Two bids to host the Olympic Games were part of a process to raise the international profile of the city.
Manchester has a history of attacks attributed to Irish Republicans, including the Manchester Martyrs of 1867, arson in 1920, a series of explosions in 1939, and two bombs in 1992.
On Saturday 15 June 1996, the Provisional Irish Republican Army (IRA) carried out the 1996 Manchester bombing, the detonation of a large bomb next to a department store in the city centre. The largest to be detonated on British soil, the bomb injured over 200 people, heavily damaged nearby buildings, and broke windows away. The cost of the immediate damage was initially estimated at £50 million, but this was quickly revised upwards. The final insurance payout was over £400 million; many affected businesses never recovered from the loss of trade.

Spurred by the investment after the 1996 bomb and aided by the XVII Commonwealth Games, the city centre has undergone extensive regeneration. New and renovated complexes such as The Printworks and Corn Exchange have become popular shopping, eating and entertainment areas. Manchester Arndale is the UK's largest city-centre shopping centre.

Large city sections from the 1960s have been demolished, re-developed or modernised with the use of glass and steel. Old mills have been converted into apartments. Hulme has undergone extensive regeneration, with million-pound loft-house apartments being developed. The 47-storey, Beetham Tower was the tallest UK building outside London and the highest residential accommodation in Europe when completed in 2006. It was surpassed in 2018 by the South Tower of the Deansgate Square project, also in Manchester. In January 2007, the independent Casino Advisory Panel licensed Manchester to build the UK's only supercasino, but plans were abandoned in February 2008.

On 22 May 2017, an Islamic terrorist carried out a bombing at an Ariana Grande concert in the Manchester Arena. The bomb killed 23, including the attacker, and injured over 800. It was the deadliest terrorist attack and first suicide bombing in Britain since the 7 July 2005 London bombings. It caused worldwide condemnation and changed the UK's threat level to "critical" for the first time since 2007.

Since around the turn of the 21st century, Manchester has been regarded as one of the candidates for the unofficial title of second city of the United Kingdom alongside Birmingham by sections of the international press, British public, and government ministers. The BBC reports that redevelopment of recent years has heightened claims that Manchester is the second city of the UK. Manchester and Birmingham traditionally compete as front runners for this unofficial title.

The City of Manchester is governed by the Manchester City Council. The Greater Manchester Combined Authority, with a directly elected mayor, has responsibilities for economic strategy and transport, amongst other areas, on a Greater Manchester-wide basis. Manchester has been a member of the English Core Cities Group since its inception in 1995.

The town of Manchester was granted a charter by Thomas Grelley in 1301, but lost its borough status in a court case of 1359. Until the 19th century local government was largely in the hands of manorial courts, the last of which was dissolved in 1846.

From a very early time, the township of Manchester lay within the historic or ceremonial county boundaries of Lancashire. Pevsner wrote "That [neighbouring] Stretford and Salford are not administratively one with Manchester is one of the most curious anomalies of England". A stroke of a Norman baron's pen is said to have divorced Manchester and Salford, though it was not Salford that became separated from Manchester, it was Manchester, with its humbler line of lords, that was separated from Salford. It was this separation that resulted in Salford becoming the judicial seat of Salfordshire, which included the ancient parish of Manchester. Manchester later formed its own Poor Law Union using the name "Manchester". In 1792, Commissioners—usually known as "Police Commissioners"—were established for the social improvement of Manchester. Manchester regained its borough status in 1838, and comprised the townships of Beswick, Cheetham Hill, Chorlton upon Medlock and Hulme. By 1846, with increasing population and greater industrialisation, the Borough Council had taken over the powers of the "Police Commissioners". In 1853, Manchester was granted "city status" in the United Kingdom.

In 1885, Bradford, Harpurhey, Rusholme and parts of Moss Side and Withington townships became part of the City of Manchester. In 1889, the city became a county borough as did many larger Lancashire towns, and therefore not governed by Lancashire County Council. Between 1890 and 1933, more areas were added to the city which had been administered by Lancashire County Council, including former villages such as Burnage, Chorlton-cum-Hardy, Didsbury, Fallowfield, Levenshulme, Longsight, and Withington. In 1931, the Cheshire civil parishes of Baguley, Northenden and Northen Etchells from the south of the River Mersey were added. In 1974, by way of the Local Government Act 1972, the City of Manchester became a metropolitan district of the metropolitan county of Greater Manchester. That year, Ringway, the village where the Manchester Airport is located, was added to the City.

In November 2014, it was announced that Greater Manchester would receive a new directly elected Mayor. The Mayor would have fiscal control over health, transport, housing and police in the area. Andy Burnham was elected as the first Mayor of Greater Manchester in 2017.

At , northwest of London, Manchester lies in a bowl-shaped land area bordered to the north and east by the Pennines, an upland chain that runs the length of northern England, and to the south by the Cheshire Plain. Manchester is north-east of Liverpool and north-west of Sheffield, making the city the halfway point between the two. The city centre is on the east bank of the River Irwell, near its confluences with the Rivers Medlock and Irk, and is relatively low-lying, being between above sea level. The River Mersey flows through the south of Manchester. Much of the inner city, especially in the south, is flat, offering extensive views from many highrise buildings in the city of the foothills and moors of the Pennines, which can often be capped with snow in the winter months. Manchester's geographic features were highly influential in its early development as the world's first industrial city. These features are its climate, its proximity to a seaport at Liverpool, the availability of water power from its rivers, and its nearby coal reserves.

The name Manchester, though officially applied only to the metropolitan district within Greater Manchester, has been applied to other, wider divisions of land, particularly across much of the Greater Manchester county and urban area. The "Manchester City Zone", "Manchester post town" and the "Manchester Congestion Charge" are all examples of this.

For purposes of the Office for National Statistics, Manchester forms the most populous settlement within the Greater Manchester Urban Area, the United Kingdom's third-largest conurbation. There is a mix of high-density urban and suburban locations. The largest open space in the city, at around , is Heaton Park. Manchester is contiguous on all sides with several large settlements, except for a small section along its southern boundary with Cheshire. The M60 and M56 motorways pass through Northenden and Wythenshawe respectively in the south of Manchester. Heavy rail lines enter the city from all directions, the principal destination being Manchester Piccadilly station.

Manchester experiences a temperate Oceanic climate (Köppen: "Cfb"), like much of the British Isles, with mild summers and cool winters. Summer daytime temperatures regularly top 20 Celsius, typically reaching 25 Celsius on sunny days throughout July and August in particular. In more recent years, temperatures have reached over 30 Celsius on occasions. There is regular but generally light precipitation throughout the year. The city's average annual rainfall is compared to a UK average of , and its mean rain days are 140.4 per annum, compared to the UK average of 154.4. Manchester has a relatively high humidity level, and this, along with abundant soft water, was one factor that led to advancement of the textile industry in the area. Snowfalls are not common in the city because of the urban warming effect but the West Pennine Moors to the north-west, South Pennines to the north-east and Peak District to the east receive more snow, which can close roads leading out of the city. They include the A62 via Oldham and Standedge, the A57, Snake Pass, towards Sheffield, and the Pennine section of the M62. The lowest temperature ever recorded in Manchester was on 7 January 2010.

Manchester lies at the centre of a green belt region extending into the wider surrounding counties. This reduces urban sprawl, prevents towns in the conurbation from further convergence, protects the identity of outlying communities, and preserves nearby countryside. It is achieved by restricting inappropriate development within the designated areas and imposing stricter conditions on permitted building.

Due to being already highly urban, the city contains limited portions of protected green-belt area within greenfield throughout the borough, with minimal development opportunities, at Clayton Vale, Heaton Park, Chorlton Water Park along with the Chorlton Ees & Ivy Green nature reserve and the floodplain surrounding the River Mersey, as well as the southern area around Manchester Airport. The green belt was first drawn up in 1961.

Below are the 10 largest immigrant groups of Manchester in 2011.

Historically the population of Manchester began to increase rapidly during the Victorian era, estimated at 354,930 for Manchester and 110,833 for Salford in 1865, and peaking at 766,311 in 1931. From then the population began to decrease rapidly, due to slum clearance and the increased building of social housing overspill estates by Manchester City Council after the Second World War such as Hattersley and Langley.

The 2012 mid-year estimate for the population of Manchester was 510,700. This was an increase of 7,900, or 1.6 per cent, since the 2011 estimate. Since 2001, the population has grown by 87,900, or 20.8 per cent, making Manchester the third fastest-growing area in the 2011 census. The city experienced the greatest percentage population growth outside London, with an increase of 19 per cent to over 500,000. Manchester's population is projected to reach 532,200 by 2021, an increase of 5.8 per cent from 2011. This represents a slower rate of growth than the previous decade.

The Greater Manchester Built-up Area in 2011 had an estimated population of 2,553,400. In 2012 an estimated 2,702,200 people lived in Greater Manchester. An 6,547,000 people were estimated in 2012 to live within of Manchester and 11,694,000 within .

Between the beginning of July 2011 and end of June 2012 (mid-year estimate date), births exceeded deaths by 4,800. Migration (internal and international) and other changes accounted for a net increase of 3,100 people between July 2011 and June 2012. Compared with Greater Manchester and with England, Manchester has a younger population, with a particularly large 20–35 age group.

There were 76,095 undergraduate and postgraduate students at Manchester Metropolitan University, the University of Manchester and Royal Northern College of Music in the 2011/2012 academic year.

Since the 2001 census, the proportion of Christians in Manchester has fallen by 22 per cent from 62.4 per cent to 48.7 per cent. The proportion of those with no religious affiliation rose by 58.1 per cent from 16 per cent to 25.3 per cent, whilst the proportion of Muslims increased by 73.6 per cent from 9.1 per cent to 15.8 per cent. The size of the Jewish population in Greater Manchester is the largest in Britain outside London.
Of all households in Manchester, 0.23 per cent were Same-Sex Civil Partnership households, compared with an English national average of 0.16 per cent in 2011.

In terms of ethnic composition, the City of Manchester has the highest non-white proportion of any district in Greater Manchester. Statistics from the 2011 census showed that 66.7 per cent of the population was White (59.3 per cent White British, 2.4 per cent White Irish, 0.1 per cent Gypsy or Irish Traveller, 4.9 per cent Other White – although the size of mixed European and British ethnic groups is unclear, there are reportedly over 25,000 Mancunians of at least partial Italian descent alone, which represents 5.5 per cent of its population). 4.7 per cent were mixed race (1.8 per cent White and Black Caribbean, 0.9 per cent White and Black African, 1.0 per cent White and Asian, 1.0 per cent other mixed), 17.1 per cent Asian (2.3 per cent Indian, 8.5 per cent Pakistani, 1.3 per cent Bangladeshi, 2.7 per cent Chinese, 2.3 per cent other Asian), 8.6 per cent Black (5.1 per cent African, 1.6 per cent other Black), 1.9 per cent Arab and 1.2 per cent of other ethnic heritage.

Kidd identifies Moss Side, Longsight, Cheetham Hill, Rusholme, as centres of population for ethnic minorities. Manchester's Irish Festival, including a St Patrick's Day parade, is one of Europe's largest. There is also a well-established Chinatown in the city with a substantial number of oriental restaurants and Chinese supermarkets. The area also attracts large numbers of Chinese students to the city who, in attending the local universities, contribute to Manchester having the third-largest Chinese population in Europe.

The Manchester Larger Urban Zone, a Eurostat measure of the functional city-region approximated to local government districts, had a population of 2,539,100 in 2004. In addition to Manchester itself, the LUZ includes the remainder of the county of Greater Manchester. The Manchester LUZ is the second largest within the United Kingdom, behind that of London.

The Office for National Statistics does not produce economic data for the City of Manchester alone, but includes four other metropolitan boroughs, Salford, Stockport, Tameside, Trafford, in an area named Greater Manchester South, which had a GVA of £34.8 billion. The economy grew relatively strongly between 2002 and 2012, when growth was 2.3 per cent above the national average. With a GDP of $102.3bn (2015 estimate, PPP) the wider metropolitan economy is the third largest in the United Kingdom. It is ranked as a beta world city by the Globalization and World Cities Research Network.

As the UK economy continues to recover from its 2008–2010 downturn, Manchester compares favourably according to recent figures. In 2012 it showed the strongest annual growth in business stock (5 per cent) of all core cities. The city had a relatively sharp increase in the number of business deaths, the largest increase in all the core cities, but this was offset by strong growth in new businesses, resulting in strong net growth.

Manchester's civic leadership has a reputation for business acumen. It owns two of the country's four busiest airports and uses its earnings to fund local projects. Meanwhile, KPMG's competitive alternative report found that in 2012 Manchester had the 9th lowest tax cost of any industrialised city in the world, and fiscal devolution has come earlier to Manchester than to any other British city: it can keep half the extra taxes it gets from transport investment.

KPMG's competitive alternative report also found that Manchester was Europe's most affordable city featured, ranking slightly better than the Dutch cities of Rotterdam and Amsterdam, which all have a cost-of-living index of less than 95.

Manchester is a city of contrast, where some of the country's most deprived and most affluent neighbourhoods can be found. According to 2010 Indices of Multiple Deprivation, Manchester is the 4th most deprived local council in England. Unemployment throughout 2012–2013 averaged 11.9 per cent, which was above national average, but lower than some of the country's comparable large cities. On the other hand, Greater Manchester is home to more multi-millionaires than anywhere outside London, with the City of Manchester taking up most of the tally. In 2013 Manchester was ranked 6th in the UK for quality of life, according to a rating of the UK's 12 largest cities.

Women fare better in Manchester than the rest of the country in comparative pay with men. The per hours-worked gender pay gap is 3.3 per cent compared with 11.1 per cent for Britain. 37 per cent of the working-age population in Manchester have degree-level qualifications, as opposed to an average of 33 per cent across other core cities, although its schools under-perform slightly compared with the national average.

Manchester has the largest UK office market outside London, according to GVA Grimley, with a quarterly office uptake (averaged over 2010–2014) of some 250,000 square ft – equivalent to the quarterly office uptake of Leeds, Liverpool and Newcastle combined and 90,000 square feet more than the nearest rival, Birmingham. The strong office market in Manchester has been partly attributed to "northshoring", (from offshoring) which entails the relocation or alternative creation of jobs away from the overheated South to areas where office space is possibly cheaper and the workforce market less saturated.

According to 2019 property investment research, Manchester is rated as No. 2 location for "Best Places To Invest in Property in the UK". This was attributed to a 5.6 per cent increase in house prices and local investment in infrastructure and in Manchester Airport.

Manchester's buildings display a variety of architectural styles, ranging from Victorian to contemporary architecture. The widespread use of red brick characterises the city, much of the architecture of which harks back to its days as a global centre for the cotton trade. Just outside the immediate city centre are a large number of former cotton mills, some of which have been left virtually untouched since their closure, while many have been redeveloped as apartment buildings and office space. Manchester Town Hall, in Albert Square, was built in the Gothic revival style and is seen as one of the most important Victorian buildings in England.

Manchester also has a number of skyscrapers built in the 1960s and 1970s, the tallest being the CIS Tower near Manchester Victoria station until the Beetham Tower was completed in 2006. The latter exemplifies a new surge in high-rise building. It includes a Hilton hotel, a restaurant and apartments. The Green Building, opposite Oxford Road station, is a pioneering eco-friendly housing project, while the recently completed One Angel Square, is one of the most sustainable large buildings in the world.

The award-winning Heaton Park in the north of the city borough is one of the largest municipal parks in Europe, covering of parkland. The city has 135 parks, gardens, and open spaces.

Two large squares hold many of Manchester's public monuments. Albert Square has monuments to Prince Albert, Bishop James Fraser, Oliver Heywood, William Ewart Gladstone and John Bright. Piccadilly Gardens has monuments dedicated to Queen Victoria, Robert Peel, James Watt and the Duke of Wellington. The cenotaph in St Peter's Square is Manchester's main memorial to its war dead. Designed by Edwin Lutyens, it echoes the original on Whitehall in London. The Alan Turing Memorial in Sackville Park commemorates his role as the father of modern computing. A larger-than-life statue of Abraham Lincoln by George Gray Barnard in the eponymous Lincoln Square (having stood for many years in Platt Fields) was presented to the city by Mr and Mrs Charles Phelps Taft of Cincinnati, Ohio, to mark the part Lancashire played in the cotton famine and American Civil War of 1861–1865. A Concorde is on display near Manchester Airport.

Manchester has six designated local nature reserves: Chorlton Water Park, Blackley Forest, Clayton Vale and Chorlton Ees, Ivy Green, Boggart Hole Clough and Highfield Country Park.

Manchester Liverpool Road was the world's first purpose-built passenger and goods railway station, and served as the Manchester terminus on the Liverpool and Manchester Railway – the world's first inter-city passenger railway. Today the city is well served by its rail network, and is at the centre of an extensive county-wide railway network, including the West Coast Main Line, with two mainline stations: Manchester Piccadilly and Manchester Victoria. The Manchester station group – comprising Manchester Piccadilly, Manchester Victoria, Manchester Oxford Road and Deansgate – is the third busiest in the United Kingdom, with 44.9 million passengers recorded in 2017/2018. The High Speed 2 link to Birmingham and London is also planned, which if built will include a tunnel under Manchester on the final approach into an upgraded Piccadilly station.

Recent improvements in Manchester as part of the Northern Hub in the 2010s have been numerous electrification schemes into and through Manchester, redevelopment of Victoria station and construction of the Ordsall Chord directly linking Victoria and Piccadilly. Work on two new through platforms at Piccadilly and an extensive upgrade at Oxford Road had not commenced as of 2019. Manchester city centre suffers from constrained rail capacity that frequently leads to delays and cancellations – a 2018 report found that all three major Manchester stations are among the top ten worst stations in the United Kingdom for punctuality, with Oxford Road deemed the worst in the country.

Manchester became the first city in the UK to acquire a modern light rail tram system when the Manchester Metrolink opened in 1992. In 2016–2017, 37.8 million passenger journeys were made on the system.<ref name="16/17DfTstats"></ref> The present system mostly runs on former commuter rail lines converted for light rail use, and crosses the city centre via on-street tram lines. The network consists of seven lines with 93 stops. A new line to the Trafford Centre is due to open by 2020.
Manchester city centre is also serviced by over a dozen heavy and light rail-based park and ride sites.

The city has one of the most extensive bus networks outside London, with over 50 bus companies operating in the Greater Manchester region radiating from the city. In 2011, 80 per cent of public transport journeys in Greater Manchester were made by bus, amounting to 220 million passenger journeys each year. After deregulation in 1986, the bus system was taken over by GM Buses, which after privatisation was split into GM Buses North and GM Buses South. Later these were taken over by First Greater Manchester and Stagecoach Manchester. Much of the First Greater Manchester business was sold to Diamond Bus North West and Go North West in 2019. Go North West operate a two-route zero-fare bus service, called "free bus around the city", which carries 2.8 million commuters a year around Manchester's business districts. Stagecoach Manchester is the Stagecoach Group's largest subsidiary and operates around 690 buses.
 Manchester, Northern England and North Wales are served by it. The airport is the third busiest in the United Kingdom and the largest outside the London region. Services cover many destinations in Europe, North America, the Caribbean, Africa, the Middle East, and Asia (with more destinations from Manchester than any other airport in Britain). A second runway was opened in 2001 and there have been continued terminal improvements. The airport has the highest rating available: ""Category 10"", encompassing an elite group of airports able to handle ""Code F"" aircraft, including the Airbus A380 and Boeing 747-8. From September 2010 the airport became one of only 17 airports in the world and the only UK airport other than Heathrow Airport and Gatwick Airport to operate the Airbus A380.

A smaller City Airport Manchester exists to the west of Manchester city centre. It was Manchester's first municipal airport and became the site of the first air traffic control tower in the UK, and the first municipal airfield in the UK to be licensed by the Air Ministry. Today, private charter flights and general aviation use City. It also has a flight school, and both the Greater Manchester Police Air Support Unit and the North West Air Ambulance have helicopters based there.

An extensive canal network, including the Manchester Ship Canal, was built to carry freight from the Industrial Revolution onward; the canals are still maintained, though now largely repurposed for leisure use. In 2012, plans were approved to introduce a water taxi service between Manchester city centre and MediaCityUK at Salford Quays. This ceased to operate in June 2018, citing poor infrastructure.

Bands that have emerged from the Manchester music scene include Van der Graaf Generator, Oasis, The Smiths, Joy Division and its successor group New Order, Buzzcocks, The Stone Roses, The Fall, The Durutti Column, 10cc, Godley & Creme, The Verve, Elbow, Doves, The Charlatans, M People, The 1975, Simply Red, Take That, Dutch Uncles, Everything Everything, Pale Waves and The Outfield. Manchester was credited as the main driving force behind British indie music of the 1980s led by The Smiths, later including The Stone Roses, Happy Mondays, Inspiral Carpets, and James. The later groups came from what became known as the "Madchester" scene that also centred on The Haçienda nightclub developed by the founder of Factory Records, Tony Wilson. Although from southern England, The Chemical Brothers subsequently formed in Manchester. Former Smiths frontman Morrissey, whose lyrics often refer to Manchester locations and culture, later found international success as a solo artist. Previously, notable Manchester acts of the 1960s include The Hollies, Herman's Hermits, and Davy Jones of the Monkees (famed in the mid-1960s for their albums and their American TV show), and the earlier Bee Gees, who grew up in Chorlton. Another notable contemporary band from near Manchester is The Courteeners consisting of Liam Fray and four close friends. Singer-songwriter Ren Harvieu is also from Greater Manchester.

Manchester has two symphony orchestras, the Hallé and the BBC Philharmonic, and a chamber orchestra, the Manchester Camerata. In the 1950s, the city was home to a so-called "Manchester School" of classical composers, which was composed of Harrison Birtwistle, Peter Maxwell Davies, David Ellis and Alexander Goehr. Manchester is a centre for musical education: the Royal Northern College of Music and Chetham's School of Music. Forerunners of the RNCM were the Northern School of Music (founded 1920) and the Royal Manchester College of Music (founded 1893), which merged in 1973. One of the earliest instructors and classical music pianists/conductors at the RMCM, shortly after its founding, was the Russian-born Arthur Friedheim, (1859–1932), who later had the music library at the famed Peabody Institute conservatory of music in Baltimore, Maryland, named after him. The main classical music venue was the Free Trade Hall on Peter Street until the opening in 1996 of the 2,500 seat Bridgewater Hall.

Brass band music, a tradition in the north of England, is important to Manchester's musical heritage; some of the UK's leading bands, such as the CWS Manchester Band and the Fairey Band, are from Manchester and surrounding areas, and the Whit Friday brass-band contest takes place annually in the neighbouring areas of Saddleworth and Tameside.

Manchester has a thriving theatre, opera and dance scene, with a number of large performance venues, including Manchester Opera House, which feature large-scale touring shows and West End productions; the Palace Theatre; and the Royal Exchange Theatre in Manchester's former cotton exchange, which is the largest theatre in the round in the UK.

Smaller venues include the Contact Theatre and Z-arts in Hulme. The Dancehouse on Oxford Road is dedicated to dance productions. In 2014, HOME, a new custom-built arts complex opened. Housing two theatre spaces, five cinemas and an art exhibition space, it replaced the Cornerhouse and The Library Theatre.

Since 2007 the city has hosted the Manchester International Festival, a biennial international arts festival with a focus on original work, which has included major new commissions by artists, including Bjork. A government statement in 2014 announced a £78 million grant for a new "large-scale, ultra-flexible arts space" for the city. Later the council stated it had secured a further £32 million. The £110 million venue was confirmed in July 2016. The theatre, to be called The Factory, after Manchester's Factory Records, will provide a permanent home for the Manchester International Festival. It is due to open at the end of 2019.

Manchester's museums celebrate Manchester's Roman history, rich industrial heritage and its role in the Industrial Revolution, the textile industry, the Trade Union movement, women's suffrage and football. A reconstructed part of the Roman fort of Mamucium is open to the public in Castlefield. The Museum of Science and Industry, housed in the former Liverpool Road railway station, has a large collection of steam locomotives, industrial machinery, aircraft and a replica of the world's first stored computer program (known as the Manchester Baby). The Museum of Transport displays a collection of historic buses and trams. Trafford Park in the neighbouring borough of Trafford is home to Imperial War Museum North. The Manchester Museum opened to the public in the 1880s, has notable Egyptology and natural history collections.
The municipally owned Manchester Art Gallery in Mosley Street houses a permanent collection of European painting and one of Britain's main collections of Pre-Raphaelite paintings.

In the south of the city, the Whitworth Art Gallery displays modern art, sculpture and textiles and was voted Museum of the Year in 2015. Other exhibition spaces and museums in Manchester include Islington Mill in Salford, the National Football Museum at Urbis, Castlefield Gallery, the Manchester Costume Gallery at Platt Fields Park, the People's History Museum and the Manchester Jewish Museum.

The work of Stretford-born painter , known for "matchstick" paintings of industrial Manchester and Salford, can be seen in the City and Whitworth Manchester galleries, and at the Lowry art centre in Salford Quays (in the neighbouring borough of Salford), which devotes a large permanent exhibition to his works.

Manchester is a UNESCO City of Literature known for a "radical literary history". Manchester in the 19th century featured in works highlighting the changes that industrialisation had brought. They include Elizabeth Gaskell's novel "Mary Barton: A Tale of Manchester Life" (1848), and studies such as "The Condition of the Working Class in England in 1844" by Friedrich Engels, while living and working here. Manchester was the meeting place of Engels and Karl Marx. The two began writing "The Communist Manifesto" in Chetham's Library – founded in 1653 and claiming to be the oldest public library in the English-speaking world. Elsewhere in the city, the John Rylands Library holds an extensive collection of early printing. The Rylands Library Papyrus P52, believed to be the earliest extant New Testament text, is on permanent display there.
Letitia Landon's poem "Manchester" in Fisher's Drawing Room Scrap Book, 1835, records the rapid growth of the city and its cultural importance.

Charles Dickens is reputed to have set his novel "Hard Times" in the city, and though partly modelled on Preston, it shows the influence of his friend Mrs Gaskell. Gaskell penned all her novels but "Mary Barton" at her home in 84 Plymouth Grove. Often her house played host to influential authors: Dickens, Charlotte Brontë, Harriet Beecher Stowe and Charles Eliot Norton, for example. It is now open as a literary museum.

Charlotte Brontë began writing her novel "Jane Eyre" in 1846, while staying at lodgings in Hulme. She was accompanying her father Patrick, who was convalescing in the city after cataract surgery. She probably envisioned Manchester Cathedral churchyard as the burial place for Jane's parents and the birthplace of Jane herself. Also associated with the city is the Victorian poet and novelist Isabella Banks, famed for her 1876 novel "The Manchester Man". Anglo-American author Frances Hodgson Burnett was born in the city's Cheetham Hill district in 1849, and wrote much of her classic children's novel "The Secret Garden" while visiting nearby Salford's Buile Hill Park.

Anthony Burgess is among the 20th-century writers who made Manchester their home. He wrote here the dystopian satire "A Clockwork Orange" in 1962. Dame Carol Ann Duffy, Poet Laureate from 2009 to 2019, moved to the city in 1996 and lives in West Didsbury.

The night-time economy of Manchester has expanded significantly since about 1993, with investment from breweries in bars, public houses and clubs, along with active support from the local authorities. The more than 500 licensed premises in the city centre have a capacity to deal with more than visitors, with 110,000–130,000 people visiting on a typical weekend night, making Manchester the most popular city for events at 79 per thousand people. The night-time economy has a value of about £100 million. and supports 12,000 jobs.

The Madchester scene of the 1980s, from which groups including New Order, The Smiths, The Stone Roses, the Happy Mondays, Inspiral Carpets, 808 State, James and The Charlatans emerged, was based on clubs such as The Haçienda. The period was the subject of the film "24 Hour Party People". Many of the big clubs suffered problems with organised crime at that time; Haslam describes one where staff were so completely intimidated that free admission and drinks were demanded (and given) and drugs were openly dealt. After a series of violent drug-related incidents, The Hacienda closed in 1998. In 1988, Manchester was often referred to as Madchester for its rave scene. Owned by Tony Wilson's Factory Records, it was given the catalogue number FAC51 and official club name, FAC51 The Hacienda. Known for developing many talented 1980s influential acts, it also influenced the graphic design industry via Factory artists such as Peter Saville (PSA), Octavo (8vo), Central Design Station, etc. The memorabilia from this club holds a high value among collectors and fans of these artists and the club. Peter Saville was most notable for his minimalism, which still influences contemporary graphic design.
Public houses in the Canal Street area have had an LGBTQ+ clientele since at least 1940, and now form the centre of Manchester's LGBTQ+ community. Since the opening of new bars and clubs, the area attracts 20,000 visitors each weekend and has hosted a popular festival, Manchester Pride, each August since 2003.

There are three universities in the City of Manchester. The University of Manchester, Manchester Metropolitan University and Royal Northern College of Music. The University of Manchester is the largest full-time non-collegiate university in the United Kingdom, created in 2004 by the merger of Victoria University of Manchester, founded in 1904, and UMIST, founded in 1956, though the university's logo appears to claim it was established in 1824. It includes the Manchester Business School, which offered the first MBA course in the UK in 1965.

Manchester Metropolitan University was formed as Manchester Polytechnic on the merger of three colleges in 1970. It gained university status in 1992, and in the same year absorbed Crewe and Alsager College of Higher Education in South Cheshire. The University of Law, the largest provider of vocation legal training in Europe, has a campus in the city.

The three universities are grouped around Oxford Road on the southern side of the city centre, which forms Europe's largest urban higher-education precinct. Together they have a combined population of 76,025 students in higher education as of 2015, although almost 6,000 of them were based at Manchester Metropolitan University's campuses at Crewe and Alsager in Cheshire.

One of Manchester's notable secondary schools is Manchester Grammar School. Established in 1515, as a free grammar school next to what is now the Cathedral, it moved in 1931 to Old Hall Lane in Fallowfield, south Manchester, to accommodate the growing student body. In the post-war period, it was a direct grant grammar school (i.e. partially state funded), but it reverted to independent status in 1976 after abolition of the direct-grant system. Its previous premises are now used by Chetham's School of Music. There are three schools nearby: William Hulme's Grammar School, Withington Girls' School and Manchester High School for Girls.

In 2010, the Manchester Local Education Authority was ranked last out of Greater Manchester's ten LEAs and 147th out of 150 in the country LEAs based on the percentage of pupils attaining at least five A*–C grades at General Certificate of Secondary Education (GCSE) including maths and English (38.6 per cent compared with the national average of 50.7 per cent). The LEA also had the highest occurrence of absences: 11.11 per cent of "half-day sessions missed by pupils", well above the national average of 5.8 per cent. Of the schools in the LEA with 30 or more pupils, four had 90 per cent or more pupils achieving at least five A*–C grades at GCSE including maths and English: Manchester High School for Girls, St Bede's College, Manchester Islamic High School for Girls, and The King David High School. Three managed 25 per cent or less: Plant Hill Arts College, North Manchester High School for Boys, Brookway High School and Sports College.

Manchester is well known as a city of sport. Two decorated Premier League football clubs bear the city name – Manchester United and Manchester City. Manchester United play its home games at Old Trafford, in the Manchester suburb of Trafford, the largest club football ground in the United Kingdom. Manchester City's home ground is the City of Manchester Stadium (also known as the Etihad Stadium for sponsorship purposes); its former ground, Maine Road was demolished in 2003. The City of Manchester Stadium was initially built as the main athletics stadium for the 2002 Commonwealth Games and was then reconfigured into a football stadium before Manchester City's arrival. Manchester has hosted domestic, continental and international football competitions at Fallowfield Stadium, Maine Road, Old Trafford and the City of Manchester Stadium. Competitions hosted in city include the FIFA World Cup (1966), UEFA European Football Championship (1996), Olympic Football (2012), UEFA Champions League Final (2003), UEFA Cup Final (2008), four FA Cup Finals (1893, 1911, 1915, 1970) and three League Cup Finals (1977, 1978, 1984).

First-class sporting facilities were built for the 2002 Commonwealth Games, including the City of Manchester Stadium, the National Squash Centre and the Manchester Aquatics Centre. Manchester has competed twice to host the Olympic Games, beaten by Atlanta for 1996 and Sydney for 2000. The National Cycling Centre includes a velodrome, BMX Arena and Mountainbike trials, and is the home of British Cycling, UCI ProTeam Team Sky and Sky Track Cycling. The Manchester Velodrome was built as a part of the bid for the 2000 games and has become a catalyst for British success in cycling. The velodrome hosted the UCI Track Cycling World Championships for a record third time in 2008. The National Indoor BMX Arena (2,000 capacity) adjacent to the velodrome opened in 2011. The Manchester Arena hosted the FINA World Swimming Championships in 2008. Manchester Cricket Club evolved into Lancashire County Cricket Club and play at Old Trafford Cricket Ground, as do Manchester Originals, a new city-based cricket team founded in 2019 which will play in the new cricket competition The Hundred, representing Lancashire and Manchester. Manchester also hosted the World Squash Championships in 2008, and also hosted the 2010 World Lacrosse Championship in July 2010. Recent sporting events hosted by Manchester include the 2013 Ashes series, 2013 Rugby League World Cup and the 2015 Rugby World Cup.

The ITV franchise Granada Television is partly headquartered on the old Granada Studios site in Quay Street and partly at a new location at MediaCityUK as part of the initial phase of its migration to Salford Quays. It produces "Coronation Street," local news and programmes for North West England. Although its influence has waned, Granada had been described as "the best commercial television company in the world".

Manchester was one of the BBC's three main centres in England. Programmes including "Mastermind", and "Real Story", were made at New Broadcasting House. The "Cutting It" series set in the city's Northern Quarter and "The Street" were set in Manchester as was "Life on Mars". The first edition of "Top of the Pops" was broadcast from a studio in Rusholme on New Year's Day 1964. Manchester was the regional base for BBC One North West Region programmes before it relocated to MediaCityUK in nearby Salford Quays. The Manchester television channel, Channel M, owned by the Guardian Media Group operated from 2000, but closed in 2012. Manchester is also covered by two internet television channels: Quays News and Manchester.tv. The city had a new terrestrial channel from January 2014 when YourTV Manchester, which won the OFCOM licence bid in February 2013. It began its first broadcast, but in 2015, That's Manchester took over to air on 31 May and launched the freeview channel 8 service slot, before moving to channel 7 in April 2016.

The city has the highest number of local radio stations outside London, including BBC Radio Manchester, Hits Radio Manchester, Capital Manchester, Greatest Hits Manchester, Heart North West, Smooth North West, Gold, 96.2 The Revolution, NMFM (North Manchester FM) and XS Manchester. Student radio stations include Fuse FM at the University of Manchester and MMU Radio at the Manchester Metropolitan University. A community radio network is coordinated by Radio Regen, with stations covering Ardwick, Longsight and Levenshulme (All FM 96.9) and Wythenshawe (Wythenshawe FM 97.2). Defunct radio stations include Sunset 102, which became Kiss 102, then Galaxy Manchester), and KFM which became Signal Cheshire (now Imagine FM). These stations and pirate radio played a significant role in the city's house music culture, the Madchester scene.
"The Guardian" newspaper was founded in 1821 as "The Manchester Guardian". Its head office is still in the city, though many of its management functions were moved to London in 1964. Its sister publication, the "Manchester Evening News", has the largest circulation of a UK regional evening newspaper. The paper is free in the city centre on Thursdays and Fridays, but paid for in the suburbs. Despite its title, it is available all day. The "Metro North West" is available free at Metrolink stops, rail stations and other busy locations. The MEN group distributes several local weekly free papers. For many years most national newspapers had offices in Manchester: "The Daily Telegraph", "Daily Express", "Daily Mail", "Daily Mirror", "The Sun". At its height, 1,500 journalists were employed, though in the 1980s office closures began and today the "second Fleet Street" is no more. An attempt to launch a Northern daily newspaper, the "North West Times", employing journalists made redundant by other titles, closed in 1988. Another attempt was made with the "North West Enquirer", which hoped to provide a true "regional" newspaper for the North West, much in the same vein as the "Yorkshire Post" does for Yorkshire or "The Northern Echo" does for the North East; it folded in October 2006.

Manchester has formal twinning arrangements (or "friendship agreements") with several places. In addition, the British Council maintains a metropolitan centre in Manchester.

Manchester is home to the largest group of consuls in the UK outside London. The expansion of international trade links during the Industrial Revolution led to the introduction of the first consuls in the 1820s and since then over 800, from all parts of the world, have been based in Manchester. Manchester hosts consular services for most of the north of England.







</doc>
<doc id="20208" url="https://en.wikipedia.org/wiki?curid=20208" title="Margaret Murray">
Margaret Murray

Margaret Alice Murray (13 July 1863 – 13 November 1963) was an Anglo-Indian Egyptologist, archaeologist, anthropologist, historian, and folklorist. The first woman to be appointed as a lecturer in archaeology in the United Kingdom, she worked at University College London (UCL) from 1898 to 1935. She served as President of the Folklore Society from 1953 to 1955, and published widely over the course of her career.

Born to a wealthy middle-class English family in Calcutta, British India, Murray divided her youth between India, Britain, and Germany, training as both a nurse and a social worker. Moving to London, in 1894 she began studying Egyptology at UCL, developing a friendship with department head Flinders Petrie, who encouraged her early academic publications and appointed her Junior Professor in 1898. In 1902–03 she took part in Petrie's excavations at Abydos, Egypt, there discovering the Osireion temple and the following season investigated the Saqqara cemetery, both of which established her reputation in Egyptology. Supplementing her UCL wage by giving public classes and lectures at the British Museum and Manchester Museum, it was at the latter in 1908 that she led the unwrapping of Khnum-nakht, one of the mummies recovered from the Tomb of the Two Brothers – the first time that a woman had publicly unwrapped a mummy. Recognising that British Egyptomania reflected the existence of a widespread public interest in Ancient Egypt, Murray wrote several books on Egyptology targeted at a general audience.

Murray also became closely involved in the first-wave feminist movement, joining the Women's Social and Political Union and devoting much time to improving women's status at UCL. Unable to return to Egypt due to the First World War, she focused her research on the witch-cult hypothesis, the theory that the witch trials of Early Modern Christendom were an attempt to extinguish a surviving pre-Christian, pagan religion devoted to a Horned God. Although later academically discredited, the theory gained widespread attention and proved a significant influence on the emerging new religious movement of Wicca. From 1921 to 1931 Murray undertook excavations of prehistoric sites on Malta and Menorca and developed her interest in folkloristics. Awarded an honorary doctorate in 1927, she was appointed Assistant Professor in 1928 and retired from UCL in 1935. That year she visited Palestine to aid Petrie's excavation of Tall al-Ajjul and in 1937 she led a small excavation at Petra in Jordan. Taking on the presidency of the Folklore Society in later life, she lectured at such institutions as the University of Cambridge and City Literary Institute, and continued to publish in an independent capacity until her death.

Murray's work in Egyptology and archaeology was widely acclaimed and earned her the moniker of "The Grand Old Woman of Egyptology", although after her death many of her contributions to the field were overshadowed by those of Petrie. Conversely, Murray's work in folkloristics and the history of witchcraft has been academically discredited and her methods in these areas heavily criticised. The influence of her witch-cult theory in both religion and literature has been examined by various scholars, and she herself has been dubbed the "Grandmother of Wicca".

Margaret Murray was born on 13 July 1863 in Calcutta, Bengal Presidency, then a major military city in British India. A member of the wealthy British imperial elite, she lived in the city with her family: parents James and Margaret Murray, an older sister named Mary, and her paternal grandmother and great-grandmother. James Murray, born in India of English descent, was a businessman and manager of the Serampore paper mills who was thrice elected President of the Calcutta Chamber of Commerce. His wife, Margaret (née Carr), had moved to India from Britain in 1857 to work as a missionary, preaching Christianity and educating Indian women. She continued with this work after marrying James and giving birth to her two daughters.
Although most of their lives were spent in the European area of Calcutta, which was walled off from the indigenous sectors of the city, Murray encountered members of indigenous society through her family's employment of 10 Indian servants and through childhood holidays to Mussoorie. The historian Amara Thornton has suggested that Murray's Indian childhood continued to exert an influence over her throughout her life, expressing the view that Murray could be seen as having a hybrid transnational identity that was both British and Indian. During her childhood, Murray never received a formal education, and in later life expressed pride in the fact that she had never had to sit an exam before entering university.

In 1870, Margaret and her sister Mary were sent to Britain, there moving in with their uncle John, a vicar, and his wife Harriet at their home in Lambourn, Berkshire. Although John provided them with a strongly Christian education and a belief in the inferiority of women, both of which she would reject, he awakened Murray's interest in archaeology through taking her to see local monuments. In 1873, the girls' mother arrived in Europe and took them with her to Bonn in Germany, where they both became fluent in German. In 1875 they returned to Calcutta, staying there till 1877. They then moved with their parents back to England, where they settled in Sydenham, South London. There, they spent much time visiting The Crystal Palace, while their father worked at his firm's London office. In 1880, they returned to Calcutta, where Margaret remained for the next seven years. She became a nurse at the Calcutta General Hospital, which was run by the Sisters of the Anglican Sisterhood of Clower, and there was involved with the hospital's attempts to deal with a cholera outbreak. In 1887, she returned to England, moving to Rugby, Warwickshire, where her uncle John had moved, now widowed. Here she took up employment as a social worker dealing with local underprivileged people. When her father retired and moved to England, she moved into his house in Bushey Heath, Hertfordshire, living with him until his death in 1891. In 1893 she then travelled to Madras, Tamil Nadu, where her sister had moved to with her new husband.

Encouraged by her mother and sister, Murray decided to enroll at the newly opened department of Egyptology at University College London (UCL) in Bloomsbury, Central London. Having been founded by an endowment from Amelia Blanford Edwards, one of the co-founders of the Egypt Exploration Fund (EEF), the department was run by the pioneering early archaeologist Sir William Flinders Petrie, and based in the Edwards Library of UCL's South Cloisters. Murray began her studies at UCL at age 30 in January 1894, as part of a class composed largely of other women and older men. There, she took courses in the Ancient Egyptian and Coptic languages which were taught by Francis Llewellyn Griffith and Walter Ewing Crum respectively.

Murray soon got to know Petrie, becoming his copyist and illustrator and producing the drawings for the published report on his excavations at Qift, "Koptos". In turn, he aided and encouraged her to write her first research paper, "The Descent of Property in the Early Periods of Egyptian History", which was published in the "Proceedings of the Society for Biblical Archaeology" in 1895. Becoming Petrie's "de facto" though unofficial assistant, Murray began to give some of the linguistic lessons in Griffith's absence. In 1898 she was appointed to the position of Junior Lecturer, responsible for teaching the linguistic courses at the Egyptology department; this made her the first female lecturer in archaeology in the United Kingdom. In this capacity, she spent two days a week at UCL, devoting the other days to caring for her ailing mother. As time went on, she came to teach courses on Ancient Egyptian history, religion, and language. Among Murray's students – to whom she referred as "the Gang" – were several who went on to produce noted contributions to Egyptology, including Reginald Engelbach, Georgina Aitken, Guy Brunton, and Myrtle Broome. She supplemented her UCL salary by teaching evening classes in Egyptology at the British Museum.

At this point, Murray had no experience in field archaeology, and so during the 1902–03 field season, she travelled to Egypt to join Petrie's excavations at Abydos. Petrie and his wife, Hilda Petrie, had been excavating at the site since 1899, having taken over the archaeological investigation from French Coptic scholar Émile Amélineau. Murray at first joined as site nurse, but was subsequently taught how to excavate by Petrie and given a senior position. This led to some issues with some of the male excavators, who disliked the idea of taking orders from a woman. This experience, coupled with discussions with other female excavators (some of whom were active in the feminist movement) led Murray to adopt openly feminist viewpoints. While excavating at Abydos, Murray uncovered the Osireion, a temple devoted to the god Osiris which had been constructed by order of Pharaoh Seti I during the period of the New Kingdom. She published her site report as "The Osireion at Abydos" in 1904; in the report, she examined the inscriptions that had been discovered at the site to discern the purpose and use of the building.

During the 1903–04 field season, Murray returned to Egypt, and at Petrie's instruction began her investigations at the Saqqara cemetery near to Cairo, which dated from the period of the Old Kingdom. Murray did not have legal permission to excavate the site, and instead spent her time transcribing the inscriptions from ten of the tombs that had been excavated during the 1860s by Auguste Mariette. She published her findings in 1905 as "Saqqara Mastabas I", although would not publish translations of the inscriptions until 1937 as "Saqqara Mastabas II". Both "The Osireion at Abydos" and "Saqqara Mastabas I" proved to be very influential in the Egyptological community, with Petrie recognising Murray's contribution to his own career.

On returning to London, Murray took an active role in the feminist movement, volunteering and financially donating to the cause and taking part in feminist demonstrations, protests, and marches. Joining the Women's Social and Political Union, she was present at large marches like the Mud March of 1907 and the Women's Coronation Procession of June 1911. She concealed the militancy of her actions in order to retain the image of respectability within academia. Murray also pushed the professional boundaries for women throughout her own career, and mentored other women in archaeology and throughout academia. As women could not use the men's common room, she successfully campaigned for UCL to open a common room for women, and later ensured that a larger, better-equipped room was converted for the purpose; it was later renamed the Margaret Murray Room. At UCL, she became a friend of fellow female lecturer Winifred Smith, and together they campaigned to improve the status and recognition of women in the university, with Murray becoming particularly annoyed at female staff who were afraid of upsetting or offending the male university establishment with their demands. Feeling that students should get nutritious yet affordable lunches, for many years she sat on the UCL Refectory Committee.

Various museums around the United Kingdom invited Murray to advise them on their Egyptological collections, resulting in her cataloguing the Egyptian artefacts owned by the Dublin National Museum, the National Museum of Antiquities in Edinburgh, and the Society of Antiquaries of Scotland, being elected a Fellow of the latter in thanks.
Petrie had established connections with the Egyptological wing of Manchester Museum in Manchester, and it was there that many of his finds had been housed. Murray thus often travelled to the museum to catalogue these artefacts, and during the 1906–07 school year regularly lectured there. In 1907, Petrie excavated the Tomb of the Two Brothers, a Middle Kingdom burial of two Egyptian priests, Nakht-ankh and Khnum-nakht, and it was decided that Murray would carry out the public unwrapping of the latter's mummified body. Taking place at the museum in May 1908, it represented the first time that a woman had led a public mummy unwrapping and was attended by over 500 onlookers, attracting press attention. Murray was particularly keen to emphasise the importance that the unwrapping would have for the scholarly understanding of the Middle Kingdom and its burial practices, and lashed out against members of the public who saw it as immoral; she declared that "every vestige of ancient remains must be carefully studied and recorded without sentimentality and without fear of the outcry of the ignorant". She subsequently published a book about her analysis of the two bodies, "The Tomb of the Two Brothers", which remained a key publication on Middle Kingdom mummification practices into the 21st century.
Murray was dedicated to public education, hoping to infuse Egyptomania with solid scholarship about Ancient Egypt, and to this end authored a series of books aimed at a general audience. In 1905 she published "Elementary Egyptian Grammar" which was followed in 1911 by "Elementary Coptic (Sahidic) Grammar". In 1913, she published "Ancient Egyptian Legends" for John Murray's "The Wisdom of the East" series. She was particularly pleased with the increased public interest in Egyptology that followed Howard Carter's discovery of the tomb of Pharaoh Tutankhamun in 1922. From at least 1911 until his death in 1940, Murray was a close friend of the anthropologist Charles Gabriel Seligman of the London School of Economics, and together they co-authored a variety of papers on Egyptology that were aimed at an anthropological audience. Many of these dealt with subjects that Egyptological journals would not publish, such as the "Sa" sign for the uterus, and thus were published in "Man", the journal of the Royal Anthropological Institute. It was at Seligman's recommendation that she was invited to become a member of the Institute in 1916.

In 1914, Petrie launched the academic journal "Ancient Egypt", published through his own British School of Archaeology in Egypt (BSAE), which was based at UCL. Given that he was often away from London excavating in Egypt, Murray was left to operate as "de facto" editor much of the time. She also published many research articles in the journal and authored many of its book reviews, particularly of the German-language publications which Petrie could not read.

The outbreak of the First World War in 1914, in which the United Kingdom went to war against Germany and the Ottoman Empire, meant that Petrie and other staff members were unable to return to Egypt for excavation. Instead, Petrie and Murray spent much of the time reorganising the artefact collections that they had attained over the past decades. To aid Britain's war effort, Murray enrolled as a volunteer nurse in the Volunteer Air Detachment of the College Women's Union Society, and for several weeks was posted to Saint-Malo in France. After being taken ill herself, she was sent to recuperate in Glastonbury, Somerset, where she became interested in Glastonbury Abbey and the folklore surrounding it which connected it to the legendary figure of King Arthur and to the idea that the Holy Grail had been brought there by Joseph of Aramathea. Pursuing this interest, she published the paper "Egyptian Elements in the Grail Romance" in the journal "Ancient Egypt", although few agreed with her conclusions and it was criticised for making unsubstantiated leaps with the evidence by the likes of Jessie Weston.

Murray's interest in folklore led her to develop an interest in the witch trials of Early Modern Europe. In 1917, she published a paper in "Folklore", the journal of the Folklore Society, in which she first articulated her version of the witch-cult theory, arguing that the witches persecuted in European history were actually followers of "a definite religion with beliefs, ritual, and organization as highly developed as that of any cult in the end". She followed this up with papers on the subject in the journals "Man" and the "Scottish Historical Review". She articulated these views more fully in her 1921 book "The Witch-Cult in Western Europe", published by Oxford University Press after receiving a positive peer review by Henry Balfour, and which received both criticism and support on publication. Many reviews in academic journals were critical, with historians claiming that she had distorted and misinterpreted the contemporary records that she was using, but the book was nevertheless influential.
As a result of her work in this area, she was invited to provide the entry on "witchcraft" for the fourteenth edition of the "Encyclopædia Britannica" in 1929. She used the opportunity to propagate her own witch-cult theory, failing to mention the alternate theories proposed by other academics. Her entry would be included in the encyclopedia until 1969, becoming readily accessible to the public, and it was for this reason that her ideas on the subject had such a significant impact. It received a particularly enthusiastic reception by occultists such as Dion Fortune, Lewis Spence, Ralph Shirley, and J. W. Brodie Innes, perhaps because its claims regarding an ancient secret society chimed with similar claims common among various occult groups. Murray joined the Folklore Society in February 1927, and was elected to the society's council a month later, although she stood down in 1929. Murray reiterated her witch-cult theory in her 1933 book, "The God of the Witches", which was aimed at a wider, non-academic audience. In this book, she cut out or toned down what she saw as the more unpleasant aspects of the witch-cult, such as animal and child sacrifice, and began describing the religion in more positive terms as "the Old Religion".

From 1921 to 1927, Murray led archaeological excavations on Malta, assisted by Edith Guest and Gertrude Caton Thompson. She excavated the Bronze Age megalithic monuments of Santa Sofia, Santa Maria tal-Bakkari, Għar Dalam, and Borġ in-Nadur, all of which were threatened by the construction of a new aerodrome. In this she was funded by the Percy Sladen Memorial Fund. Her resulting three-volume excavation report came to be seen as an important publication within the field of Maltese archaeology. During the excavations, she had taken an interest in the island's folklore, resulting in the 1932 publication of her book "Maltese Folktales", much of which was a translation of earlier stories collected by Manuel Magri and her friend Liza Galea. In 1932 Murray returned to Malta to aid in the cataloguing of the Bronze Age pottery collection held in Malta Museum, resulting in another publication, "Corpus of the Bronze Age Pottery of Malta".

On the basis of her work in Malta, Louis C. G. Clarke, the curator of the Cambridge Museum of Ethnology and Anthropology, invited her to lead excavations on the island of Menorca from 1930 to 1931. With the aid of Guest, she excavated the talaiotic sites of Trepucó and Sa Torreta de Tramuntana, resulting in the publication of "Cambridge Excavations in Minorca". Murray also continued to publish works on Egyptology for a general audience, such as "Egyptian Sculpture" (1930) and "Egyptian Temples" (1931), which received largely positive reviews. In the summer of 1925 she led a team of volunteers to excavate Homestead Moat in Whomerle Wood near to Stevenage, Hertfordshire; she did not publish an excavation report and did not mention the event in her autobiography, with her motives for carrying out the excavation remaining unclear.

In 1924, UCL promoted Murray to the position of assistant professor, and in 1927 she was awarded an honorary doctorate for her career in Egyptology. That year, Murray was tasked with guiding Mary of Teck, the Queen consort, around the Egyptology department during the latter's visit to UCL. The pressures of teaching had eased by this point, allowing Murray to spend more time travelling internationally; in 1920 she returned to Egypt and in 1929 visited South Africa, where she attended the meeting of the British Association for the Advancement of Science, whose theme was the prehistory of southern Africa. In the early 1930s she travelled to the Soviet Union, where she visited museums in Leningrad, Moscow, Kharkiv, and Kiev, and then in late 1935 she undertook a lecture tour of Norway, Sweden, Finland, and Estonia.
Although having reached legal retirement age in 1927, and thus unable to be offered another five-year contract, Murray was reappointed on an annual basis each year until 1935. At this point, she retired, expressing the opinion that she was glad to leave UCL, for reasons that she did not make clear. In 1933, Petrie had retired from UCL and moved to Jerusalem in Mandatory Palestine with his wife; Murray therefore took over as editor of the "Ancient Egypt" journal, renaming it "Ancient Egypt and the East" to reflect its increasing research interest in the ancient societies that surrounded and interacted with Egypt. The journal folded in 1935, perhaps due to Murray's retirement. Murray then spent some time in Jerusalem, where she aided the Petries in their excavation at Tall al-Ajjul, a Bronze Age mound south of Gaza.

During Murray's 1935 trip to Palestine, she had taken the opportunity to visit Petra in neighbouring Jordan. Intrigued by the site, in March and April 1937 she returned in order to carry out a small excavation in several cave dwellings at the site, subsequently writing both an excavation report and a guidebook on Petra. Back in England, from 1934 to 1940, Murray aided the cataloguing of Egyptian antiquities at Girton College, Cambridge, and also gave lectures in Egyptology at the university until 1942.
During the Second World War, Murray evaded the Blitz of London by moving to Cambridge, where she volunteered for a group (probably the Army Bureau of Current Affairs or The British Way and Purpose) who educated military personnel to prepare them for post-war life. Based in the city, she embarked on research into the town's Early Modern history, examining documents stored in local parish churches, Downing College, and Ely Cathedral; she never published her findings. In 1945, she briefly became involved in the "Who put Bella in the Wych Elm?" murder case.

After the war ended she returned to London, settling into a bedsit room in Endsleigh Street, which was close to University College London (UCL) and the Institute of Archaeology (then an independent institution, now part of UCL); she continued her involvement with the former and made use of the latter's library. On most days she visited the British Museum in order to consult their library, and twice a week she taught adult education classes on Ancient Egyptian history and religion at the City Literary Institute; upon her retirement from this position she nominated her former pupil, Veronica Seton-Williams, to replace her.

Murray's interest in popularising Egyptology among the wider public continued; in 1949 she published "Ancient Egyptian Religious Poetry", her second work for John Murray's "The Wisdom of the East" series. That same year she also published "The Splendour That Was Egypt", in which she collated many of her UCL lectures. The book adopted a diffusionist perspective that argued that Egypt influenced Greco-Roman society and thus modern Western society. This was seen as a compromise between Petrie's belief that other societies influenced the emergence of Egyptian civilisation and Grafton Elliot Smith's highly unorthodox and heavily criticised hyperdiffusionist view that Egypt was the source of all global civilisation. The book received a mixed reception from the archaeological community.

In 1953, Murray was appointed to the presidency of the Folklore Society following the resignation of former president Allan Gomme. The Society had initially approached John Mavrogordato for the post, but he had declined, with Murray accepting the nomination several months later. Murray remained President for two terms, until 1955. In her 1954 presidential address, "England as a Field for Folklore Research", she lamented what she saw as the English people's disinterest in their own folklore in favour of that from other nations. For the autumn 1961 issue of "Folklore", the society published a "festschrift" to Murray to commemorate her 98th birthday. The issue contained contributions from various scholars paying tribute to her – with papers dealing with archaeology, fairies, Near Eastern religious symbols, Greek folk songs – but notably not about witchcraft, potentially because no other folklorists were willing to defend her witch-cult theory.

In May 1957, Murray had championed the archaeologist T. C. Lethbridge's controversial claims that he had discovered three pre-Christian chalk hill figures on Wandlebury Hill in the Gog Magog Hills, Cambridgeshire. Privately she expressed concern about the reality of the figures. Lethbridge subsequently authored a book championing her witch-cult theory in which he sought the cult's origins in pre-Christian culture. In 1960, she donated her collection of papers – including correspondences with a wide range of individuals across the country – to the Folklore Society Archive, where it is now known as "the Murray Collection".

Crippled with arthritis, Murray had moved into a home in North Finchley, north London, where she was cared for by a retired couple who were trained nurses; from here she occasionally took taxis into central London to visit the UCL library.
Amid failing health, in 1962 Murray moved into the Queen Victoria Memorial Hospital in Welwyn, Hertfordshire, where she could receive 24-hour care; she lived here for the final 18 months of her life. To mark her hundredth birthday, on 13 July 1963 a group of her friends, former students, and doctors gathered for a party at nearby Ayot St. Lawrence. Two days later, her doctor drove her to UCL for a second birthday party, again attended by many of her friends, colleagues, and former students; it was the last time that she visited the university. In "Man", the journal of the Royal Anthropological Institute, it was noted that Murray was "the only Fellow of the Institute to [reach their centenary] within living memory, if not in its whole history". That year she published two books; one was "The Genesis of Religion", in which she argued that humanity's first deities had been goddesses rather than male gods. The second was her autobiography, "My First Hundred Years", which received predominantly positive reviews. She died on 13 November 1963, and her body was cremated.

The later folklorists Caroline Oates and Juliette Wood have suggested that Murray was best known for her witch-cult theory, with biographer Margaret S. Drower expressing the view that it was her work on this subject which "perhaps more than any other, made her known to the general public". It has been claimed that Murray's was the "first feminist study of the witch trials", as well as being the first to have actually "empowered the witches" by giving the (largely female) accused both free will and a voice distinct from that of their interrogators. The theory was faulty, in part because all of her academic training was in Egyptology, with no background knowledge in European history, but also because she exhibited a "tendency to generalize wildly on the basis of very slender evidence". Oates and Wood, however, noted that Murray's interpretations of the evidence fitted within wider perspectives on the past that existed at the time, stating that "Murray was far from isolated in her method of reading ancient ritual origins into later myths". In particular, her approach was influenced by the work of the anthropologist James Frazer, who had argued for the existence of a pervasive dying-and-resurrecting god myth, and she was also influenced by the interpretative approaches of E. O. James, Karl Pearson, Herbert Fleure, and Harold Peake.

In "The Witch-Cult in Western Europe", Murray stated that she had restricted her research to Great Britain, although made some recourse to sources from France, Flanders, and New England. She drew a division between what she termed "Operative Witchcraft", which referred to the performance of charms and spells with any purpose, and "Ritual Witchcraft", by which she meant "the ancient religion of Western Europe", a fertility-based faith that she also termed "the Dianic cult". She claimed that the cult had "very probably" once been devoted to the worship of both a male deity and a "Mother Goddess" but that "at the time when the cult is recorded the worship of the male deity appears to have superseded that of the female". In her argument, Murray claimed that the figure referred to as the Devil in the trial accounts was the witches' god, "manifest and incarnate", to whom the witches offered their prayers. She claimed that at the witches' meetings, the god would be personified, usually by a man or at times by a woman or an animal; when a human personified this entity, Murray claimed that they were usually dressed plainly, though they appeared in full costume for the witches' Sabbaths.

Members joined the cult either as children or adults through what Murray called "admission ceremonies"; Murray asserted that applicants had to agree to join of their own free will, and agree to devote themselves to the service of their deity. She also claimed that in some cases, these individuals had to sign a covenant or were baptised into the faith. At the same time, she claimed that the religion was largely passed down hereditary lines. Murray described the religion as being divided into covens containing thirteen members, led by a coven officer who was often termed the "Devil" in the trial accounts, but who was accountable to a "Grand Master". According to Murray, the records of the coven were kept in a secret book, with the coven also disciplining its members, to the extent of executing those deemed traitors.

Describing this witch-cult as "a joyous religion", she claimed that the two primary festivals that it celebrated were on May Eve and November Eve, although that other dates of religious observation were 1 February and 1 August, the winter and summer solstices, and Easter. She asserted that the "General Meeting of all members of the religion" were known as Sabbaths, while the more private ritual meetings were known as Esbats. The Esbats, Murray claimed, were nocturnal rites that began at midnight, and were "primarily for business, whereas the Sabbath was purely religious". At the former, magical rites were performed both for malevolent and benevolent ends. She also asserted that the Sabbath ceremonies involved the witches paying homage to the deity, renewing their "vows of fidelity and obedience" to him, and providing him with accounts of all the magical actions that they had conducted since the previous Sabbath. Once this business had been concluded, admissions to the cult or marriages were conducted, ceremonies and fertility rites took place, and then the Sabbath ended with feasting and dancing.
Deeming Ritual Witchcraft to be "a fertility cult", she asserted that many of its rites were designed to ensure fertility and rain-making. She claimed that there were four types of sacrifice performed by the witches: blood-sacrifice, in which the neophyte writes their name in blood; the sacrifice of animals; the sacrifice of a non-Christian child to procure magical powers; and the sacrifice of the witches' god by fire to ensure fertility.
She interpreted accounts of witches shapeshifting into various animals as being representative of a rite in which the witches dressed as specific animals which they took to be sacred. She asserted that accounts of familiars were based on the witches' use of animals, which she divided into "divining familiars" used in divination and "domestic familiars" used in other magic rites.

Murray asserted that a pre-Christian fertility-based religion had survived the Christianization process in Britain, although that it came to be "practised only in certain places and among certain classes of the community". She believed that folkloric stories of fairies in Britain were based on a surviving race of dwarfs, who continued to live on the island up until the Early Modern period. She asserted that this race followed the same pagan religion as the witches, thus explaining the folkloric connection between the two. In the appendices to the book, she also alleged that Joan of Arc and Gilles de Rais were members of the witch-cult and were executed for it, a claim which has been refuted by historians, especially in the case of Joan of Arc.

The later historian Ronald Hutton commented that "The Witch-Cult in Western Europe" "rested upon a small amount of archival research, with extensive use of printed trial records in 19th-century editions, plus early modern pamphlets and works of demonology". He also noted that the book's tone was generally "dry and clinical, and every assertion was meticulously footnoted to a source, with lavish quotation". It was not a bestseller; in its first thirty years, only 2,020 copies were sold. However, it led many people to treat Murray as an authority on the subject; in 1929, she was invited to provide the entry on "Witchcraft" for the "Encyclopædia Britannica", and used it to present her interpretation of the subject as if it were universally accepted in scholarship. It remained in the encyclopedia until being replaced in 1969.

Murray followed "The Witch-Cult in Western Europe" with "The God of the Witches", published by the popular press Sampson Low in 1931; although similar in content, unlike her previous volume it was aimed at a mass market audience. The tone of the book also differed strongly from its predecessor, containing "emotionally inflated [language] and coloured with religious phraseology" and repeatedly referring to the witch-cult as "the Old Religion". In this book she also "cut out or toned down" many of the claims made in her previous volume which would have painted the cult in a bad light, such as those which discussed sex and the sacrifice of animals and children.

In this book she began to refer to the witches' deity as the Horned God, and asserted that it was an entity who had been worshipped in Europe since the Palaeolithic.
She further asserted that in the Bronze Age, the worship of the deity could be found throughout Europe, Asia, and parts of Africa, claiming that the depiction of various horned figures from these societies proved that. Among the evidence cited were the horned figures found at Mohenjo-Daro, which are often interpreted as depictions of Pashupati, as well as the deities Osiris and Amon in Egypt and the Minotaur of Minoan Crete. Within continental Europe, she claimed that the Horned God was represented by Pan in Greece, Cernunnos in Gaul, and in various Scandinavian rock carvings. Claiming that this divinity had been declared the Devil by the Christian authorities, she nevertheless asserted that his worship was testified in officially Christian societies right through to the Modern period, citing folkloric practices such as the Dorset Ooser and the Puck Fair as evidence of his veneration.

In 1954, she published "The Divine King in England", in which she greatly extended on the theory, taking influence from Frazer's "The Golden Bough", an anthropological book that made the claim that societies all over the world sacrificed their kings to the deities of nature. In her book, she claimed that this practice had continued into medieval England, and that, for instance, the death of William II was really a ritual sacrifice.
No academic took the book seriously, and it was ignored by many of her supporters.

Upon initial publication, Murray's thesis gained a favourable reception from many readers, including some significant scholars, albeit none who were experts in the witch trials. Historians of Early Modern Britain like George Norman Clark and Christopher Hill incorporated her theories into their work, although the latter subsequently distanced himself from the theory. For the 1961 reprint of "The Witch-Cult in Western Europe", the Medieval historian Steven Runciman provided a foreword in which he accepted that some of Murray's "minor details may be open to criticism", but in which he was otherwise supportive of her thesis. Her theories were recapitulated by Arno Runeberg in his 1947 book "Witches, Demons and Fertility Magic" as well as Pennethorne Hughes in his 1952 book "Witches". As a result, the Canadian historian Elliot Rose, writing in 1962, claimed that the Murrayite interpretations of the witch trials "seem to hold, at the time of writing, an almost undisputed sway at the higher intellectual levels", being widely accepted among "educated people".

Rose suggested that the reason that Murray's theory gained such support was partly because of her "imposing credentials" as a member of staff at UCL, a position that lent her theory greater legitimacy in the eyes of many readers. He further suggested that the Murrayite view was attractive to many as it confirmed "the general picture of pre-Christian Europe a reader of Frazer or [Robert] Graves would be familiar with". Similarly, Hutton suggested that the cause of the Murrayite theory's popularity was because it "appealed to so many of the emotional impulses of the age", including "the notion of the English countryside as a timeless place full of ancient secrets", the literary popularity of Pan, the widespread belief that the majority of British had remained pagan long after the process of Christianisation, and the idea that folk customs represented pagan survivals. At the same time, Hutton suggested, it seemed more plausible to many than the previously dominant rationalist idea that the witch trials were the result of mass delusion. Related to this, the folklorist Jacqueline Simpson suggested that part of the Murrayite theory's appeal was that it appeared to give a "sensible, demystifying, liberating approach to a longstanding but sterile argument" between the rationalists who denied that there had been any witches and those, like Montague Summers, who insisted that there had been a real Satanic conspiracy against Christendom in the Early Modern period replete with witches with supernatural powers. "How refreshing", noted the historian Hilda Ellis Davidson, "and exciting her first book was "at that period". A new approach, and such a surprising one."

Murray's theories never received support from experts in the Early Modern witch trials, and from her early publications onward many of her ideas were challenged by those who highlighted her "factual errors and methodological failings". Indeed, the majority of scholarly reviews of her work produced during the 1920s and 1930s were largely critical. George L. Burr reviewed both of her initial books on the witch-cult for the "American Historical Review". He stated that she was not acquainted with the "careful general histories by modern scholars" and criticised her for assuming that the trial accounts accurately reflected the accused witches' genuine experiences of witchcraft, regardless of whether those confessions had been obtained through torture and coercion. He also charged her with selectively using the evidence to serve her interpretation, for instance by omitting any supernatural or miraculous events that appear in the trial accounts. W. R. Halliday was highly critical in his review for "Folklore", as was E. M. Loeb in his review for "American Anthropologist".

Soon after, one of the foremost specialists of the trial records, L'Estrange Ewen, brought out a series of books which rejected Murray's interpretation.
Rose suggested that Murray's books on the witch-cult "contain an incredible number of minor errors of fact or of calculation and several inconsistencies of reasoning". He accepted that her case "could, perhaps, still be proved by somebody else, though I very much doubt it". Highlighting that there is a gap of about a thousand years between the Christianisation of Britain and the start of the witch trials there, he argues that there is no evidence for the existence of the witch-cult anywhere in the intervening period. He further criticises Murray for treating pre-Christian Britain as a socially and culturally monolithic entity, whereas in reality, it contained a diverse array of societies and religious beliefs. He also challenges Murray's claim that the majority of Britons in the Middle Ages remained pagan as "a view grounded on ignorance alone".

Murray did not respond directly to the criticisms of her work, but reacted to her critics in a hostile manner; in later life she asserted that she eventually ceased reading reviews of her work, and believed that her critics were simply acting out of their own Christian prejudices to non-Christian religion.
Simpson noted that despite these critical reviews, within the field of British folkloristics Murray's theories were permitted "to pass unapproved but unchallenged, either out of politeness or because nobody was really interested enough to research the topic". As evidence, she noted that no substantial research articles on the subject of witchcraft were published in "Folklore" between Murray's in 1917 and Rossell Hope Robbins' in 1963. She also highlighted that when regional studies of British folklore were published in this period by folklorists like Theo Brown, Ruth Tongue, or Enid Porter, none adopted the Murrayite framework for interpreting witchcraft beliefs, thus evidencing her claim that Murray's theories were widely ignored by scholars of folkloristics.

Murray's work was increasingly criticised following her death in 1963, with the definitive academic rejection of the Murrayite witch-cult theory occurring during the 1970s. During these decades, a variety of scholars across Europe and North America – such as Alan Macfarlane, Erik Midelfort, William Monter, Robert Muchembled, Gerhard Schormann, Bente Alver and Bengt Ankarloo – published in-depth studies of the archival records from the witch trials, leaving no doubt that those tried for witchcraft were not practitioners of a surviving pre-Christian religion.
In 1971, the English historian Keith Thomas stated that on the basis of this research, there was "very little evidence to suggest that the accused witches were either devil-worshippers or members of a pagan fertility cult". He stated that Murray's conclusions were "almost totally groundless" because she ignored the systematic study of the trial accounts provided by Ewen and instead used sources very selectively to argue her point.

In 1975, the historian Norman Cohn commented that Murray's "knowledge of European history, even of English history, was superficial and her grasp of historical method was non-existent", adding that her ideas were "firmly set in an exaggerated and distorted version of the Frazerian mould". That same year, the historian of religion Mircea Eliade described Murray's work as "hopelessly inadequate", containing "numberless and appalling errors". In 1996, the feminist historian Diane Purkiss stated that although Murray's thesis was "intrinsically improbable" and commanded "little or no allegiance within the modern academy", she felt that male scholars like Thomas, Cohn, and Macfarlane had unfairly adopted an androcentric approach by which they contrasted their own, male and methodologically sound interpretation against Murray's "feminised belief" about the witch-cult.

Hutton stated that Murray had treated her source material with "reckless abandon", in that she had taken "vivid details of alleged witch practices" from "sources scattered across a great extent of space and time" and then declared them to be normative of the cult as a whole. Simpson outlined how Murray had selected her use of evidence very specifically, particularly by ignoring and/or rationalising any accounts of supernatural or miraculous events in the trial records, thereby distorting the events that she was describing. Thus, Simpson pointed out, Murray rationalised claims that the cloven-hoofed Devil appeared at the witches' Sabbath by stating that he was a man with a special kind of shoe, and similarly asserted that witches' claims to have flown through the air on broomsticks were actually based on their practice of either hopping along on broomsticks or smearing hallucinogenic salves onto themselves. Concurring with this assessment, the historian Jeffrey Burton Russell, writing with the independent author Brooks Alexander, stated that "Murray's use of sources, in general, is appalling". The pair went on to claim that "today, scholars are agreed that Murray was more than just wrong – she was completely and embarrassingly wrong on nearly all of her basic premises".

The Italian historian Carlo Ginzburg has been cited as being willing to give "some slight support" to Murray's theory. Ginzburg stated that although her thesis had been "formulated in a wholly uncritical way" and contained "serious defects", it did contain "a kernel of truth". He stated his opinion that she was right in claiming that European witchcraft had "roots in an ancient fertility cult", something that he argued was vindicated by his work researching the "benandanti", an agrarian visionary tradition recorded in the Friuli district of Northeastern Italy during the 16th and 17th centuries. Several historians and folklorists have pointed out that Ginzburg's arguments are very different to Murray's: whereas Murray argued for the existence of a pre-Christian witches' cult whose members physically met during the witches' Sabbaths, Ginzburg argued that some of the European visionary traditions that were conflated with witchcraft in the Early Modern period had their origins in pre-Christian fertility religions. Moreover, other historians have expressed criticism of Ginzburg's interpretation of the "benandanti"; Cohn stated that there was "nothing whatsoever" in the source material to justify the idea that the "benandanti" were the "survival of an age-old fertility cult". Echoing these views, Hutton commented that Ginzburg's claim that the "benandanti" visionary traditions were a survival from pre-Christian practices was an idea resting on "imperfect material and conceptual foundations". He added that Ginzburg's "assumption" that "what was being dreamed about in the sixteenth century had in fact been acted out in religious ceremonies" dating to "pagan times", was entirely "an inference of his own" and not one supported by the documentary evidence.

On researching the history of UCL's Egyptology department, the historian Rosalind M. Janssen stated that Murray was "remembered with gratitude and immense affection by all her former students. A wise and witty teacher, two generations of Egyptologists have forever been in her debt." Alongside teaching them, Murray was known to socialise with her UCL students outside of class hours. The archaeologist Ralph Merrifield, who knew Murray through the Folklore Society, described her as a "diminutive and kindly scholar, who radiated intelligence and strength of character into extreme old age". Davidson, who also knew Murray through the Society, noted that at their meetings "she would sit near the front, a bent and seemingly guileless old lady dozing peacefully, and then in the middle of a discussion would suddenly intervene with a relevant and penetrating comment which showed that she had missed not one word of the argument". The later folklorist Juliette Wood noted that many members of the Folklore Society "remember her fondly", adding that Murray had been "especially keen to encourage younger researchers, even those who disagreed with her ideas".

One of Murray's friends in the Society, E. O. James, described her as a "mine of information and a perpetual inspiration ever ready to impart her vast and varied stores of specialised knowledge without reserve, or, be it said, much if any regard for the generally accepted opinions and conclusions of the experts!" Davidson described her as being "not at all assertive ... [she] never thrust her ideas on anyone. [In relation to her witch-cult theory,] she behaved in fact rather like someone who was a fully convinced member of some unusual religious sect, or perhaps, of the Freemasons, but never on any account got into arguments about it in public." The archaeologist Glyn Daniel observed that Murray remained mentally alert into her old age, commenting that "her vigour and forthrightness and ruthless energy never deserted her".

Murray never married, instead devoting her life to her work, and for this reason, Hutton drew comparisons between her and two other prominent female British scholars of the period, Jane Harrison and Jessie Weston. Murray's biographer Kathleen L. Sheppard stated that she was deeply committed to public outreach, particularly when it came to Egyptology, and that as such she "wanted to change the means by which the public obtained knowledge about Egypt's history: she wished to throw open the doors to the scientific laboratory and invite the public in". She considered travel to be one of her favourite activities, although due to restraints on her time and finances she was unable to do this regularly; her salary remained small and the revenue from her books was meagre.

Raised a devout Christian by her mother, Murray had initially become a Sunday School teacher to preach the faith, but after entering the academic profession she rejected religion, gaining a reputation among other members of the Folklore Society as a noted sceptic and a rationalist. She was openly critical of organised religion, although continued to maintain a personal belief in a God of some sort, relating in her autobiography that she believed in "an unseen over-ruling Power", "which science calls Nature and religion calls God".
She was also a believer and a practitioner of magic, performing curses against those she felt deserved it; in one case she cursed a fellow academic, Jaroslav Černý, when she felt that his promotion to the position of Professor of Egyptology over her friend Walter Bryan Emery was unworthy. Her curse entailed mixing up ingredients in a frying pan, and was undertaken in the presence of two colleagues. In another instance, she was claimed to have created a wax image of Kaiser Wilhelm II and then melted it during the First World War.

Hutton noted that Murray was one of the earliest women to "make a serious impact upon the world of professional scholarship", and the archaeologist Niall Finneran described her as "one of the greatest characters of post-war British archaeology". Upon her death, Daniel referred to her as "the Grand Old Woman of Egyptology", with Hutton noting that Egyptology represented "the core of her academic career". In 2014, Thornton referred to her as "one of Britain's most famous Egyptologists". However, according to the archaeologist Ruth Whitehouse, Murray's contributions to archaeology and Egyptology were often overlooked as her work was overshadowed by that of Petrie, to the extent that she was often thought of primarily as one of Petrie's assistants rather than as a scholar in her own right. By her retirement she had come to be highly regarded within the discipline, although, according to Whitehouse, Murray's reputation declined following her death, something that Whitehouse attributed to the rejection of her witch-cult theory and the general erasure of women archaeologists from the discipline's male-dominated history.

In his obituary for Murray in "Folklore", James noted that her death was "an event of unusual interest and importance in the annals of the Folk-Lore Society in particular as well as in the wider sphere in which her influence was felt in so many directions and
disciplines". However, later academic folklorists, such as Simpson and Wood, have cited Murray and her witch-cult theory as an embarrassment to their field, and to the Folklore Society specifically. Simpson suggested that Murray's position as President of the Society was a causal factor in the mistrustful attitude that many historians held toward folkloristics as an academic discipline, as they erroneously came to believe that all folklorists endorsed Murray's ideas. Similarly, Catherine Noble stated that "Murray caused considerable damage to the study of witchcraft".

In 1935, UCL introduced the Margaret Murray Prize, awarded to the student who is deemed to have produced the best dissertation in Egyptology; it continued to be presented annually into the 21st century. In 1969, UCL named one of their common rooms in her honour, but it was converted into an office in 1989. In June 1983, Queen Elizabeth The Queen Mother visited the room and there was gifted a copy of Murray's "My First Hundred Years". UCL also hold two busts of Murray, one kept in the Petrie Museum and the other in the library of the UCL Institute of Archaeology. This sculpture was commissioned by one of her students, Violet MacDermot, and produced by the artist Stephen Rickard. UCL also possess a watercolour painting of Murray by Winifred Brunton; formerly exhibited in the Petrie Gallery, it was later placed into the Art Collection stores.
In 2013, on the 150th anniversary of Murray's birth and the 50th of her death, the UCL Institute of Archaeology's Ruth Whitehouse described Murray as "a remarkable woman" whose life was "well worth celebrating, both in the archaeological world at large and especially in UCL".

The historian of archaeology Rosalind M. Janssen titled her study of Egyptology at UCL "The First Hundred Years" "as a tribute" to Murray. Murray's friend Margaret Stefana Drower authored a short biography of her, which was included as a chapter in the 2004 edited volume on "Breaking Ground: Pioneering Women Archaeologists". In 2013, Lexington Books published "The Life of Margaret Alice Murray: A Woman's Work in Archaeology", a biography of Murray authored by Kathleen L. Sheppard, then an assistant professor at Missouri University of Science and Technology; the book was based upon Sheppard's doctoral dissertation produced at the University of Oklahoma. Although characterising it as being "written in a clear and engaging manner", one reviewer noted that Sheppard's book focuses on Murray the "scientist" and as such neglects to discuss Murray's involvement in magical practices and her relationship with Wicca.

Murray's witch-cult theories provided the blueprint for the contemporary Pagan religion of Wicca, with Murray being referred to as the "Grandmother of Wicca". The Pagan studies scholar Ethan Doyle White stated that it was the theory which "formed the historical narrative around which Wicca built itself", for on its emergence in England during the 1940s and 1950s, Wicca claimed to be the survival of this witch-cult. Wicca's theological structure, revolving around a Horned God and Mother Goddess, was adopted from Murray's ideas about the ancient witch-cult, and Wiccan groups were named "covens" and their meetings termed "esbats", both words that Murray had popularised. As with Murray's witch-cult, Wicca's practitioners entered via an initiation ceremony; Murray's claims that witches wrote down their spells in a book may have been an influence on Wicca's Book of Shadows. Wicca's early system of seasonal festivities were also based on Murray's framework.

Noting that there is no evidence of Wicca existing before the publication of Murray's books, Merrifield commented that for those in 20th century Britain who wished to form their own witches' covens, "Murray may have seemed the ideal fairy godmother, and her theory became the pumpkin coach that could transport them into the realm of fantasy for which they longed". The historian Philip Heselton suggested that the New Forest coven – the oldest alleged Wiccan group – was founded "circa" 1935 by esotericists aware of Murray's theory and who may have believed themselves to be reincarnated witch-cult members. It was Gerald Gardner, who claimed to be an initiate of the New Forest coven, who established the tradition of Gardnerian Wicca and popularised the religion; according to Simpson, Gardner was the only member of the Folklore Society to "wholeheartedly" accept Murray's witch-cult hypothesis. The duo knew each other, with Murray writing the foreword to Gardner's 1954 book "Witchcraft Today", although in that foreword she did not explicitly specify whether she believed Gardner's claim that he had discovered a survival of her witch-cult. In 2005, Noble suggested that "Murray's name might be all but forgotten today if it were not for Gerald Gardner".

Murray's witch-cult theories were likely also a core influence on the non-Gardnerian Wiccan traditions that were established in Britain and Australia between 1930 and 1970 by the likes of Bob Clay-Egerton, Robert Cochrane, Charles Cardell, and Rosaleen Norton.
The prominent Wiccan Doreen Valiente eagerly searched for what she believed were other surviving remnants of the Murrayite witch-cult around Britain. Valiente remained committed to a belief in Murray's witch-cult after its academic rejection, and she described Murray as "a remarkable woman". In San Francisco during the late 1960s, Murray's writings were among the sources used by Aidan A. Kelly in the creation of his Wiccan tradition, the New Reformed Orthodox Order of the Golden Dawn. In Los Angeles during the early 1970s, they were used by Zsuzsanna Budapest when she was establishing her feminist-oriented tradition of Dianic Wicca. The Murrayite witch-cult theory also provided the basis for the ideas espoused in "Witchcraft and the Gay Counterculture", a 1978 book written by the American gay liberation activist Arthur Evans.

Members of the Wiccan community gradually became aware of academia's rejection of the witch-cult theory. Accordingly, belief in its literal truth declined during the 1980s and 1990s, with many Wiccans instead coming to view it as a myth that conveyed metaphorical or symbolic truths. Others insisted that the historical origins of the religion did not matter and that instead Wicca was legitimated by the spiritual experiences it gave to its participants. In response, Hutton authored "The Triumph of the Moon", a historical study exploring Wicca's early development; on publication in 1999 the book exerted a strong impact on the British Pagan community, further eroding belief in the Murrayite theory among Wiccans. Conversely, other practitioners clung on to the theory, treating it as an important article of faith and rejecting post-Murrayite scholarship on European witchcraft. Several prominent practitioners continued to insist that Wicca was a religion with origins stretching back to the Palaeolithic, but others rejected the validity of historical scholarship and emphasised intuition and emotion as the arbiter of truth. A few "counter-revisionist" Wiccans – among them Donald H. Frew, Jani Farrell-Roberts, and Ben Whitmore – published critiques in which they attacked post-Murrayite scholarship on matters of detail, but none defended Murray's original hypothesis completely.

Simpson noted that the publication of the Murray thesis in the "Encyclopædia Britannica" made it accessible to "journalists, film-makers popular novelists and thriller writers", who adopted it "enthusiastically". It influenced the work of Aldous Huxley and Robert Graves. It was also an influence on the American horror author H. P. Lovecraft, who cited "The Witch-Cult in Western Europe" in his writings about the fictional cult of Cthulhu.

The author Sylvia Townsend Warner cited Murray's work on the witch-cult as an influence on her 1926 novel "Lolly Willowes", and sent a copy of her book to Murray in appreciation, with the two meeting for lunch shortly after. There was nevertheless some difference in their depictions of the witch-cult; whereas Murray had depicted an organised pre-Christian cult, Warner depicted a vague family tradition that was explicitly Satanic.
In 1927, Warner lectured on the subject of witchcraft, exhibiting a strong influence from Murray's work. Analysing the relationship between Murray and Warner, the English literature scholar Mimi Winick characterised both as being "engaged in imagining new possibilities for women in modernity".

A bibliography of Murray's published work was published in "Folklore" by Wilfrid Bonser in 1961, and her friend Drower produced a posthumous limited bibliography in 2004, and another limited bibliography appeared in Kathleen L. Sheppard's 2013 biography of her.



</doc>
<doc id="20209" url="https://en.wikipedia.org/wiki?curid=20209" title="March 24">
March 24

March 24th is the 365th and last day of the year in many European implementations of the Julian calendar.





</doc>
<doc id="20210" url="https://en.wikipedia.org/wiki?curid=20210" title="March 23">
March 23





</doc>
<doc id="20211" url="https://en.wikipedia.org/wiki?curid=20211" title="March 22">
March 22





</doc>
<doc id="20212" url="https://en.wikipedia.org/wiki?curid=20212" title="Aoraki / Mount Cook">
Aoraki / Mount Cook

Aoraki / Mount Cook is the highest mountain in New Zealand. Its height, as of 2014, is listed as . It lies in the Southern Alps, the mountain range which runs the length of the South Island. A popular tourist destination, it is also a favourite challenge for mountain climbers. Aoraki / Mount Cook consists of three summits, from South to North the Low Peak (), Middle Peak () and High Peak. The summits lie slightly south and east of the main divide of the Southern Alps, with the Tasman Glacier to the east and the Hooker Glacier to the southwest.

The mountain is in the Aoraki/Mount Cook National Park, in the Canterbury region. The park was established in 1953 and along with Westland National Park, Mount Aspiring National Park and Fiordland National Park forms one of the UNESCO World Heritage Sites. The park contains more than 140 peaks standing over and 72 named glaciers, which cover 40 percent of its .

The peak is located at the northern end of the Kirikirikatata / Mount Cook Range, where it meets with the main spine of the Main Divide, forming a massif between the Hooker Valley to the southwest and the Tasman Valley east of the mountain. These two valleys provide the closest easily accessible view points of Aoraki / Mount Cook. A lookout point at the end of the Hooker Valley Track located only 10 km from the peak has views of the entire mountainside.

The settlement of Mount Cook Village, also referred to as "Aoraki / Mount Cook", is a tourist centre and base camp for the mountain. It is 7 km from the end of the Tasman Glacier and 15 km south of Aoraki / Mount Cook's summit.

On clear days, Aoraki / Mount Cook is visible from the West Coast as far north as Greymouth, some 150 kilometres away, and from most of State Highway 80 along Lake Pukaki and State Highway 6 south of Lake Pukaki. The near horizontal ridge connecting the mountain's three summits forms a distinctive blocky shape when viewed from an eastern or western direction.
Another popular view point is from Lake Matheson on the West Coast, described as the "view of views", where on calm days, the peaks of Aoraki / Mount Cook and Mt Tasman are reflected in Lake Matheson.

Aoraki / Mount Cook receives substantial orographic precipitation throughout the year, as breezy, moisture-laden westerly winds dominate all year-round, bringing rainclouds from the Tasman Sea with them.

Annual precipitation around the mountain ranges varies greatly as the local climate is dominated by the eastward movement of depressions and anticyclones from across the Tasman Sea. The Aoraki / Mount Cook massif is a major obstacle to the prevailing westerly winds as they push depressions and associated cold fronts of moist air from the subtropics in the northwest against the mountain range. As the air rises towards the peaks, it expands and cools, and forms clouds. Rain and snowfall are often heaviest around the level and can last for several days if the front is slow-moving.

As a result of the local weather patterns, the western slopes of Aoraki / Mount Cook can receive well over of annual precipitation, whereas the nearby Mount Cook Village, only south of the mountain receives of rain or snowfall.
While the weather on the eastern side of the mountain is generally better, rain or snow can quickly become widespread on that side as well if the wind turns to the south or southeast. This brings with it a rapid drop in temperature and poor visibility, adding to the difficult climbing conditions on Aoraki / Mount Cook.

Temperatures at the mountain's base in the Hooker Valley around range from to , and generally fall just over 1 °C for every 200 metres of altitude.

From about and higher, semi-permanent snow and ice fields exist during winter. Winter and spring are usually less settled than summer and autumn. Anticyclones often bring days of settled weather in summer, or clear cold conditions in winter with severe frost.

' is the name of a person in the traditions of the Ngāi Tahu iwi; an early name for the South Island is ' (Aoraki's Canoe). In the past many believed it meant "Cloud Piercer", a romantic rendering of the name's components: ' (world, daytime, cloud, etc.) and ' or ' (day, sky, weather, etc.). Historically, the Māori name has been spelt ', using the standard Māori form.

Aoraki / Mount Cook became known to Māori after their arrival in New Zealand some time around the 14th century CE. The first Europeans who may have seen Aoraki / Mount Cook were members of Abel Tasman's crew, who saw a "large land uplifted high" (probably some part of the Southern Alps) while off the west coast of the South Island, just north of present-day Greymouth on 13 December 1642 during Tasman's first Pacific voyage. The English name of "Mount Cook" was given to the mountain in 1851 by Captain John Lort Stokes to honour Captain James Cook who surveyed and circumnavigated the islands of New Zealand in 1770. Captain Cook did not sight the mountain during his exploration.

Following the settlement between Ngāi Tahu and the Crown in 1998, the name of the mountain was officially changed from Mount Cook to Aoraki / Mount Cook to incorporate its historic Māori name, Aoraki. As part of the settlement, a number of South Island placenames were amended to incorporate their original Māori name. Signifying the importance of Aoraki / Mount Cook, it is the only one of these names where the Māori name precedes the English. Under the settlement the Crown agreed to return title to Aoraki / Mount Cook to Ngāi Tahu, who would then formally gift it back to the nation. Neither transfer has yet occurred; Ngāi Tahu can decide when this will happen.

The Southern Alps in the South Island were formed by tectonic uplifting and pressure as the Pacific and Indo-Australian Plates collided along the island's western coast. The uplifting continues, raising Aoraki / Mount Cook an average of each year. However, erosive forces are also powerful shapers of the mountains. The severe weather is due to the mountain's jutting into powerful westerly winds of the Roaring Forties which run around approximately 45°S latitude, south of both Africa and Australia. The Southern Alps are the first obstacle the winds encounter after South America, having moved east across the Southern Ocean.

The height of Aoraki / Mount Cook was established in 1881 by G. J. Roberts (from the west side) and in 1889 by T. N. Brodrick (from the Canterbury side). Their measurements agreed closely at . The height was reduced by when approximately 12–14 million cubic metres of rock and ice fell off the northern peak on 14 December 1991. Two decades of erosion of the ice cap exposed after this collapse reduced the height by another 30 m to 3724 m, as revealed by new GPS data from a University of Otago climbing expedition in November 2013.

Aoraki / Mount Cook lies in the centre of the distinctive Alpine Fault, a 650 km long active fault in the Southern Alps. It is responsible for the uplift of Aoraki / Mt Cook and is believed to move every 100 to 300 years. It last moved in 1717.

The average annual rainfall in the surrounding lowlands, in particular to the west, is around . This very high rainfall leads to temperate rainforests in these coastal lowlands and a reliable source of snow in the mountains to keep the glaciers flowing. These include the Tasman Glacier to the east of the mountain and the smaller Hooker Glacier immediately to its south.

The vegetation in the valleys to the east, in particular the Tasman Valley, is noticeably less lush than that on the western slopes of the mountain. Forest would normally grow to about 1,300 m in this area, but a lack of soil due to scree, rock falls and the effects of glaciation prevent this in most localities around the mountain. Snow tussock and other alpine plants cling to as high as 1,900 m.
Above the snowline, only lichen can be found amongst the rock, snowfields and ice that dominate the highest parts of Aoraki / Mt Cook.

The first recorded attempt on the summit was made by the Irishman Rev. William S. Green and the Swiss hotelier Emil Boss and the Swiss mountain guide Ulrich Kaufmann on 2 March 1882 via the Tasman and Linda Glaciers. Mt Cook Guidebook author Hugh Logan believe they came within 50 metres of the summit.

The first known ascent was on 25 December 1894, when New Zealanders Tom Fyfe, John Michael (Jack) Clarke and George Graham reached the summit via the Hooker Valley and the north ridge. Despite an earlier failed attempt on 20 December, the local climbers were spurred on by their desire for the first ascent to be made by New Zealand mountaineers amid reports that the American mountaineer Edward FitzGerald had his eye on the summit.<ref name="Aoraki/Mt Cook"></ref> The party reached the summit at approximately 1:30pm after bounding up the last leg of the mountain full of excitement at reaching the top. The route they had successfully traversed was not repeated again until the 100th ascent over 60 years later in 1955. Swiss guide Matthias Zurbriggen of FitzGerald's party made the second ascent on 14 March 1895 from the Tasman Glacier side, via the ridge that now bears his name. This is credited as the first solo ascent, although Zurbriggen was accompanied part of the way up the ridge by J Adamson. After Zurbriggen's ascent it was another ten years before the mountain was climbed again. In February 1905 Jack Clarke with four others completed the third ascent following Zurbriggen's route. So Clarke therefore became the first person to do a repeat ascent.

The first woman to ascend the mountain was Freda Du Faur, an Australian, on 3 December 1910. Local guide George Bannister, a nephew of another guide, Pahikore Te Koeti Turanga of Ngāi Tahu, was the first Māori to successfully scale the peak in 1912.
A traverse of the three peaks was first accomplished in 1913 by Freda Du Faur and guides Peter and Alex Graham. This 'grand traverse' was repeated in January 1916 by Conrad Kain, guiding the 57-year-old Mrs. Jane Thomson, considered at the time "a marvellous feat unequalled for daring in the annals of the Southern Alps".

Sir Edmund Hillary made his first ascent in January 1948. In February 1948 with Ruth Adams, Harry Ayres and Mick Sullivan, Hillary made the first ascent of the South Ridge to the Low Peak In order to celebrate the life of Hillary the South Ridge was renamed as Hillary Ridge in August 2011.

Aoraki / Mount Cook is a technically challenging mountain with a high level of glaciation. Its level of difficulty is often underestimated and can change dramatically depending on weather, snow and ice conditions. The climb crosses large crevasses, and involves risks of ice and rock falls, avalanches and rapidly changing weather conditions.

Since the early 20th century, around 80 people have died attempting to climb the mountain, making it New Zealand's deadliest peak. The climbing season traditionally runs from November to February, and hardly a season goes by without at least one fatality.

According to Māori legend, Aoraki was a young boy who, along with his three brothers, were the sons of Rakinui, the Sky Father. On their voyage around the Papatūānuku, the Earth Mother, their canoe became stranded on a reef and tilted. Aoraki and his brothers climbed onto the top side of their canoe. However, the south wind froze them and turned them to stone. Their canoe became the Te Waka o Aoraki, the South Island, and their prows, the Marlborough Sounds. Aoraki, the tallest, became the highest peak, and his brothers created the Kā Tiritiri o te Moana, the Southern Alps.

Ngāi Tahu, the main iwi (tribe) of New Zealand's southern region, consider Aoraki as the most sacred of the ancestors that they had descended from. Aoraki brings the iwi with its sense of community and purpose, and remains the physical form of Aoraki and the link between the worlds of the supernatural and nature.





</doc>
<doc id="20213" url="https://en.wikipedia.org/wiki?curid=20213" title="Multiple-image Network Graphics">
Multiple-image Network Graphics

Multiple-image Network Graphics (MNG) is a graphics file format, published in 2001, for animated images. Its specification is publicly documented and there are free software reference implementations available.

MNG is closely related to the PNG image format. When PNG development started in early 1995, developers decided not to incorporate support for animation, because the majority of the PNG developers felt that overloading a single file type with both still and animation features is a bad design, both for users (who have no simple way of determining to which class a given image file belongs) and for web servers (which should use a MIME type starting with image/ for stills and video/ for animations—GIF notwithstanding). However, work soon started on MNG as an animation-supporting version of PNG. Version 1.0 of the MNG specification was released on 31 January 2001.

Gwenview has native MNG support. GIMP can export images as MNG files. Imagemagick can create a MNG file from a series of PNG files. With the MNG plugin, Irfanview can read a MNG file. If MPlayer is linked against libmng, as it usually is, MPlayer and thus all graphical front-ends like Gnome MPlayer can display MNG files.

Mozilla browsers and Netscape 6.0, 6.01 and 7.0 included native support for MNG until the code was removed in 2003 due to code size and little actual usage, causing complaints on the Mozilla development site. Mozilla later added support for APNG as a simpler alternative. Similarly, early versions of the Konqueror browser included MNG support but it was later dropped. MNG support was never included in Google Chrome, Internet Explorer, Opera, or Safari.
Web servers generally don't come pre-configured to support MNG files.

The MNG developers had hoped that MNG would replace GIF for animated images on the World Wide Web, just as PNG had done for still images. However, with the expiration of LZW patents and existence of alternative file formats such as Flash and SVG, combined with lack of MNG-supporting viewers and services, web usage was far less than expected.

The structure of MNG files is essentially the same as that of PNG files, differing only in the slightly different signature (codice_1 in hexadecimal, where codice_2 is ASCII for "MNG" – see Portable Network Graphics: File header) and the use of a much greater variety of chunks to support all the animation features that it provides. Images to be used in the animation are stored in the MNG file as encapsulated PNG or JNG images.

Two versions of MNG of reduced complexity are also defined: MNG-LC (low complexity) and MNG-VLC (very low complexity). These allow applications to include some level of MNG support without having to implement the entire MNG specification, just as the SVG standard offers the "SVG Basic" and "SVG Tiny" subsets.

MNG does not have a registered MIME media type, but codice_3 or codice_4 can be used.
MNG animations may be included in HTML pages using the codice_5 or codice_6 tag.

MNG can either be lossy or lossless, depending whether the frames are encoded in PNG (lossless) or JNG (lossy).

The most common alternatives are Animated GIF and Adobe Flash, with the relative newcomer video alternative to GIF recently gaining momentum. Animated GIF images are restricted to 256 colors and are used in simple scenarios but are supported in all major web browsers. Adobe Flash is a common alternative for creating complex and/or interactive animations and is natively supported by Internet Explorer 10 and Google Chrome, although support is deprecated as of 2016.

In web pages, it is possible to create pseudo-animations by writing JavaScript code that loads still PNG or JPEG images of each frame and displays them one by one for a specified time interval. Apart from requiring the user to have JavaScript support and choose not to disable it, this method can be CPU- and bandwidth-intensive for pages with more than one image, large images, or high framerates, and does not allow the animation to be saved in one image file or posted on image-based sites such as flickr or imageboards.

Most web browsers support APNG, a non-standard extension to PNG for simple GIF-like animations. Another alternative is SVG images with embedded PNG or JPEG graphics, using SVG animation (if supported) or JavaScript to flip between images.

Internet Explorer supports neither APNG nor SVG animation.

Another approach uses CSS 3 features, notably CSS Animation, which now has some level of support in most major web browsers. CSS Sprites (providing several images as tiles in a single large image file) can be used as animations by varying which part of the large image is visible using CSS Animation or JavaScript.




</doc>
<doc id="20215" url="https://en.wikipedia.org/wiki?curid=20215" title="Mississippi John Hurt">
Mississippi John Hurt

John Smith Hurt (March 8, 1893 – November 2, 1966), better known as Mississippi John Hurt, was an American country blues singer and guitarist.

Raised in Avalon, Mississippi, Hurt taught himself to play the guitar around the age of nine. He worked as a sharecropper and began playing at dances and parties, singing to a melodious fingerpicked accompaniment. His first recordings, made for Okeh Records in 1928, were commercial failures, and he continued to work as a farmer.

Dick Spottswood and Tom Hoskins, a blues enthusiast, located Hurt in 1963 and persuaded him to move to Washington, D.C. He was recorded by the Library of Congress in 1964. This helped further the American folk music revival, which led to the rediscovery of many other bluesmen of Hurt's era. Hurt performed on the university and coffeehouse concert circuit with other Delta blues musicians who were brought out of retirement. He also recorded several albums for Vanguard Records.

Hurt returned to Mississippi, where he died, in Grenada, a year later.

Material recorded by him has been re-released by many record labels. His songs have been recorded by Bob Dylan, Dave Van Ronk, Jerry Garcia, Beck, Doc Watson, John McCutcheon, Taj Mahal, Bruce Cockburn, David Johansen, Bill Morrissey, Gillian Welch, Josh Ritter, Chris Smither, Guthrie Thomas, Parsonsfield, and Rory Block.

Hurt was born in Teoc, Carroll County, Mississippi, and raised in Avalon, Mississippi. He taught himself to play guitar at the age of nine, stealthily playing the guitar of a friend of his mother's, who often stayed at the Hurt home while courting a woman who lived nearby. As a youth he played old-time music for friends and at dances. He worked as a farmhand and sharecropper into the 1920s.

His fast, highly syncopated style of playing was meant for dancing. On occasion, a medicine show would come through the area. Hurt recalled that one wanted to hire him: "One of them wanted me, but I said no because I just never wanted to get away from home." In 1923, he played with the fiddle player Willie Narmour as a substitute for Narmour's regular partner, Shell Smith.

When Narmour got a chance to record for Okeh Records as a prize for winning first place in a 1928 fiddle contest, he recommended Hurt to Okeh producer Tommy Rockwell. After auditioning "Monday Morning Blues" at his home, Hurt took part in two recording sessions, in Memphis and New York City (see Discography below). While in Memphis, he recalled seeing "many, many blues singers ... Lonnie Johnson, Blind Lemon Jefferson, Bessie Smith, and lots, lots more." Hurt described his first recording session as follows:

Hurt attempted further negotiations with Okeh to record again, but his records were commercial failures. Okeh went out of business during the Great Depression, and Hurt returned to Avalon and obscurity, working as a sharecropper and playing at local parties and dances.

Hurt's renditions of "Frankie" and "Spike Driver Blues" were included in "The Anthology of American Folk Music" in 1952 which generated considerable interest in locating him. When a copy of "Avalon Blues" was discovered in 1963, it led musicologist Dick Spottswood to locate Avalon in an atlas, and ask Tom Hoskins, who was traveling that way, to enquire after Hurt. When Hoskins arrived in Avalon the first person he asked directed him to Hurt's cabin.

Hoskins persuaded an apprehensive Hurt to perform several songs for him, to ensure that he was genuine. Hoskins was convinced and, seeing that Hurt's guitar playing skills were still intact, encouraged him to move to Washington, D.C., and perform for a broader audience. His performance at the 1963 Newport Folk Festival caused his star to rise in the folk revival occurring at that time. He performed extensively at colleges, concert halls, and coffeehouses and appeared on "The Tonight Show Starring Johnny Carson". He also recorded three albums for Vanguard Records. Much of his repertoire was also recorded for the Library of Congress. His fans particularly liked the ragtime songs "Salty Dog" and "Candy Man" and the blues ballads "Spike Driver Blues" (a variant of "John Henry") and "Frankie".

Hurt's influence spanned several music genres, including blues, spirituals, country, bluegrass, folk, and contemporary rock and roll. A soft-spoken man, his nature was reflected in the work, which consisted of a mellow mix of country, blues, and old-time music.

Hurt died on November 2, 1966, of a heart attack, in hospital at Grenada, Mississippi. His last recordings had been done at a hotel in New York City in February and July of that year, and were not released until 1972 on the Vanguard LP "Last Sessions".

Hurt used a fast, syncopated fingerpicking style of guitar playing that he taught himself. He was influenced by few other musicians, among whom was an elderly, unrecorded blues singer from the area where he lived, Rufus Hanks, who played twelve-string guitar and harmonica. He also recalled listening to the country singer Jimmie Rodgers. On occasion, Hurt would use an open tuning and a slide, as he did in his arrangement of "The Ballad of Casey Jones". According to the music critic Robert Christgau, "the school of John Fahey proceeded from his finger-picking, and while he's not the only quietly conversational singer in the modern folk tradition, no one else has talked the blues with such delicacy or restraint."

There is a memorial to Hurt in Avalon, Mississippi. It is parallel to RR2, the rural road on which he grew up.

The American singer-songwriter Tom Paxton, who met Hurt and played on the same bill with him at the Gaslight in Greenwich Village around 1963, wrote and recorded a song about him in 1977, "Did You Hear John Hurt?", which he still frequently plays in live performances.

The first track of John Fahey's 1968 solo acoustic guitar album "Requia" is "Requiem for John Hurt". Fahey's posthumous live album, "The Great Santa Barbara Oil Slick", also features a version of the piece, entitled "Requiem for Mississippi John Hurt".

The British folk and blues artist Wizz Jones recorded a tribute song, "Mississippi John", for his 1977 album "Magical Flight".

The Delta blues artist Rory Block recorded the album "Avalon: A Tribute to Mississippi John Hurt", released in 2013 as part of her "Mentor Series".

The New England singer-songwriter Bill Morrissey released the Grammy-nominated album "Songs of Mississippi John Hurt" in 1999.

In 2017, John Hurt's life story was told in the award-winning documentary series "American Epic". The film featured unseen film footage of Hurt performing and being interviewed, and radically improved restorations of his 1920s recordings. Director Bernard MacMahon stated that Hurt "was the inspiration for "American Epic"". Hurt's life was profiled in the accompanying book, "".

Sources for this section are as follows:.







</doc>
<doc id="20216" url="https://en.wikipedia.org/wiki?curid=20216" title="Moravia">
Moravia

Moravia ( , , ; ; ; ; ) is a historical region in the east of the Czech Republic and one of three historical Czech lands, with Bohemia and Czech Silesia.

The medieval and early modern Margraviate of Moravia was a crown land of the Lands of the Bohemian Crown from 1348 to 1918, an imperial state of the Holy Roman Empire from 1004 to 1806, a crown land of the Austrian Empire from 1804 to 1867, and a part of Austria-Hungary from 1867 to 1918. Moravia was one of the five lands of Czechoslovakia founded in 1918, in 1928 it was merged with Czech Silesia, and dissolved during the abolition of the land system in 1949 following the communist coup d'état.

Its area of 22,623.41 km is home to more than 3 million people.
The people are historically named Moravians, a subgroup of Czechs, the other group being called Bohemians. Moravia also had been home of a large German-speaking population until their expulsion in 1945. The land takes its name from the Morava river, which runs from its north to south, being its principal watercourse. Moravia's largest city and historical capital is Brno. Before being sacked by the Swedish army during the Thirty Years' War, Olomouc was another capital, and it is still the capital of the archbishopric.

The region and former margraviate of Moravia, "Morava" in Czech, is named after its principal river Morava. It is theorized that the river's name is derived from Proto-Indo-European "*mori": "waters", or indeed any word denoting "water" or a "marsh".

The German name for Moravia is "Mähren", from the river's German name "March". This could have a different etymology, as "march" is a term used in the Medieval times for an outlying territory, a border or a frontier (cf. English "march").

Moravia occupies most of the eastern part of the Czech Republic. Moravian territory is naturally strongly determined, in fact, as the Morava river basin, with strong effect of mountains in the west ("de facto" main European continental divide) and partly in the east, where all the rivers rise.

Moravia occupies an exceptional position in Central Europe. All the highlands in the west and east of this part of Europe run west-east, and therefore form a kind of filter, making north-south or south north movement more difficult. Only Moravia with the depression of the westernmost Outer Subcarpathia, wide, between the Bohemian Massif and the Outer Western Carpathians (gripping the meridian at a constant angle of 30°), provides a comfortable connection between the Danubian and Polish regions, and this area is thus of great importance in terms of the possible migration routes of large mammals – both as regards periodically recurring seasonal migrations triggered by climatic oscillations in the prehistory, when permanent settlement started.

Moravia borders Bohemia in the west, Lower Austria in the south(west), Slovakia in the southeast, Poland very shortly in the north, and Czech Silesia in the northeast. Its natural boundary is formed by the Sudetes mountains in the north, the Carpathians in the east and the Bohemian-Moravian Highlands in the west (the border runs from Králický Sněžník in the north, over Suchý vrch, across Upper Svratka Highlands and Javořice Highlands to tripoint nearby Slavonice in the south). The Thaya river meanders along the border with Austria and the tripoint of Moravia, Austria and Slovakia is at the confluence of the Thaya and Morava rivers. The northeast border with Silesia runs partly along the Moravice, Oder and Ostravice rivers. Between 1782–1850, Moravia (also thus known as "Moravia-Silesia") also included a small portion of the former province of Silesia – the Austrian Silesia (when Frederick the Great annexed most of ancient Silesia (the land of upper and middle Oder river) to Prussia, Silesia's southernmost part remained with the Habsburgs).

Today Moravia includes the South Moravian Region, the Zlín Region, vast majority of the Olomouc Region, southeastern half of the Vysočina Region and parts of the Moravian-Silesian, Pardubice and South Bohemian regions.

Geologically, Moravia covers a transitive area between the Bohemian Massif and the Carpathians (from (north)west to southeast), and between the Danube basin and the North European Plain (from south to northeast). Its core geomorphological features are three wide valleys, namely the Dyje-Svratka Valley ("Dyjsko-svratecký úval"), the Upper Morava Valley ("Hornomoravský úval") and the Lower Morava Valley ("Dolnomoravský úval"). The first two form the westernmost part of the Outer Subcarpathia, the last is the northernmost part of the Vienna Basin. The valleys surround the low range of Central Moravian Carpathians. The highest mountains of Moravia are situated on its northern border in Hrubý Jeseník, the highest peak is Praděd (1491 m). Second highest is the massive of Králický Sněžník (1424  m) the third are the Moravian-Silesian Beskids at the very east, with Smrk (1278 m), and then south from here Javorníky (1072). The White Carpathians along the southeastern border rise up to 970 m at Velká Javořina. The spacious, but moderate Bohemian-Moravian Highlands on the west reach 837 m at Javořice.

The fluvial system of Moravia is very cohesive, as the region border is similar to the watershed of the Morava river, and thus almost the entire area is drained exclusively by a single stream. Morava's far biggest tributaries are Thaya (Dyje) from the right (or west) and Bečva (east). Morava and Thaya meet at the southernmost and lowest (148 m) point of Moravia. Small peripheral parts of Moravia belong to the catchment area of Elbe, Váh and especially Oder (the northeast). The watershed line running along Moravia's border from west to north and east is part of the European Watershed. For centuries, there has been plans to build a waterway across Moravia to join the Danube and Oder river systems, using the natural route through the Moravian Gate.

Evidence of the presence of members of the human genus, "Homo", dates back more than 600,000 years in the paleontological area of Stránská Skála.

Attracted by suitable living conditions, early modern humans settled in the region by the Paleolithic period. The Předmostí archeological (Cro-magnon) site in Moravia is dated to between 24,000 and 27,000 years old. Caves in Moravský kras were used by mammoth hunters. Venus of Dolní Věstonice, the oldest ceramic figure in the world, was found in the excavation of Dolní Věstonice by Karel Absolon.

Around 60 BC, the Celtic Volcae people withdrew from the region and were succeeded by the Germanic Quadi. Some of the events of the Marcomannic Wars took place in Moravia in AD 169–180. After the war exposed the weakness of Rome's northern frontier, half of the Roman legions (16 out of 33) were stationed along the Danube. In response to increasing numbers of Germanic settlers in frontier regions like Pannonia, Dacia, Rome established two new frontier provinces on the left shore of the Danube, Marcomannia and Sarmatia, including today's Moravia and western Slovakia.

In the 2nd century AD, a Roman fortress stood on the vineyards hill known as and ("hillfort"), situated above the former village Mušov and above today's beach resort at Pasohlávky. During the reign of the Emperor Marcus Aurelius, the 10th Legion was assigned to control the Germanic tribes who had been defeated in the Marcomannic Wars. In 1927, the archeologist Gnirs, with the support of president Tomáš Garrigue Masaryk, began research on the site, located 80 km from Vindobona and 22 km to the south of Brno. The researchers found remnants of two masonry buildings, a "praetorium" and a "balneum" ("bath"), including a "hypocaustum". The discovery of bricks with the stamp of the Legio X Gemina and coins from the period of the emperors Antoninus Pius, Marcus Aurelius and Commodus facilitated dating of the locality.

A variety of Germanic and major Slavic tribes crossed through Moravia during the Migration Period before Slavs established themselves in the 6th century AD. At the end of the 8th century, the Moravian Principality came into being in present-day south-eastern Moravia, Záhorie in south-western Slovakia and parts of Lower Austria. In 833 AD, this became the state of Great Moravia with the conquest of the Principality of Nitra (present-day Slovakia). Their first king was Mojmír I (ruled 830–846). Louis the German invaded Moravia and replaced Mojmír I with his nephew Rastiz who became St. Rastislav. St. Rastislav (846–870) tried to emancipate his land from the Carolingian influence, so he sent envoys to Rome to get missionaries to come. When Rome refused he turned to Constantinople to the Byzantine emperor Michael. The result was the mission of Saints Cyril and Methodius who translated liturgical books into Slavonic, which had lately been elevated by the Pope to the same level as Latin and Greek. Methodius became the first Moravian archbishop, but after his death the German influence again prevailed and the disciples of Methodius were forced to flee. Great Moravia reached its greatest territorial extent in the 890s under Svatopluk I. At this time, the empire encompassed the territory of the present-day Czech Republic and Slovakia, the western part of present Hungary (Pannonia), as well as Lusatia in present-day Germany and Silesia and the upper Vistula basin in southern Poland. After Svatopluk's death in 895, the Bohemian princes defected to become vassals of the East Frankish ruler Arnulf of Carinthia, and the Moravian state ceased to exist after being overrun by invading Magyars in 907.

Following the defeat of the Magyars by Emperor Otto I at the Battle of Lechfeld in 955, Otto's ally Boleslaus I, the Přemyslid ruler of Bohemia, took control over Moravia. Bolesław I Chrobry of Poland annexed Moravia in 999, and ruled it until 1019, when the Přemyslid prince Bretislaus recaptured it. Upon his father's death in 1034, Bretislaus became the ruler of Bohemia. In 1055, he decreed that Bohemia and Moravia would be inherited together by primogeniture, although he also provided that his younger sons should govern parts (quarters) of Moravia as vassals to his oldest son.

Throughout the Přemyslid era, junior princes often ruled all or part of Moravia from Olomouc, Brno or Znojmo, with varying degrees of autonomy from the ruler of Bohemia. Dukes of Olomouc often acted as the "right hand" of Prague dukes and kings, while Dukes of Brno and especially those of Znojmo were much more insubordinate. Moravia reached its height of autonomy in 1182, when Emperor Frederick I elevated Conrad II Otto of Znojmo to the status of a margrave, immediately subject to the emperor, independent of Bohemia. This status was short-lived: in 1186, Conrad Otto was forced to obey the supreme rule of Bohemian duke Frederick. Three years later, Conrad Otto succeeded to Frederick as Duke of Bohemia and subsequently canceled his margrave title. Nevertheless, the margrave title was restored in 1197 when Vladislaus III of Bohemia resolved the succession dispute between him and his brother Ottokar by abdicating from the Bohemian throne and accepting Moravia as a vassal land of Bohemian (i.e., Prague) rulers. Vladislaus gradually established this land as Margraviate, slightly administratively different from Bohemia. After the Battle of Legnica, the Mongols carried their raids into Moravia.

The main line of the Přemyslid dynasty became extinct in 1306, and in 1310 John of Luxembourg became Margrave of Moravia and King of Bohemia. In 1333, he made his son Charles the next Margrave of Moravia (later in 1346, Charles also became the King of Bohemia). In 1349, Charles gave Moravia to his younger brother John Henry who ruled in the margraviate until his death in 1375, after him Moravia was ruled by his oldest son Jobst of Moravia who was in 1410 elected the Holy Roman King but died in 1411 (he is buried with his father in the Church of St. Thomas in Brno – the Moravian capital from which they both ruled). Moravia and Bohemia remained within the Luxembourg dynasty of Holy Roman kings and emperors (except during the Hussite wars), until inherited by Albert II of Habsburg in 1437.

After his death followed the interregnum until 1453; land (as the rest of lands of the Bohemian Crown) was administered by the landfriedens ("landfrýdy"). The rule of young Ladislaus the Posthumous subsisted only less than five years and subsequently (1458) the Hussite George of Poděbrady was elected as the king. He again reunited all Czech lands (then Bohemia, Moravia, Silesia, Upper & Lower Lusatia) into one-man ruled state. In 1466, Pope Paul II excommunicated George and forbade all Catholics (i.e. about 15% of population) from continuing to serve him. The Hungarian crusade followed and in 1469 Matthias Corvinus conquered Moravia and proclaimed himself (with assistance of rebelling Bohemian nobility) as the king of Bohemia.

The subsequent 21-year period of a divided kingdom was decisive for the rising awareness of a specific Moravian identity, distinct from that of Bohemia. Although Moravia was reunited with Bohemia in 1490 when Vladislaus Jagiellon, king of Bohemia, also became king of Hungary, some attachment to Moravian "freedoms" and resistance to government by Prague continued until the end of independence in 1620. In 1526, Vladislaus' son Louis died in battle and the Habsburg Ferdinand I was elected as his successor.

After the death of King Louis II of Hungary and Bohemia in 1526, Ferdinand I of Austria was elected King of Bohemia and thus ruler of the Crown of Bohemia (including Moravia). The epoch 1526–1620 was marked by increasing animosity between Catholic Habsburg kings (emperors) and the Protestant Moravian nobility (and other Crowns') estates. Moravia, like Bohemia, was a Habsburg possession until the end of World War I. In 1573 the Jesuit University of Olomouc was established; this was the first university in Moravia. The establishment of a special papal seminary, Collegium Nordicum, made the University a centre of the Catholic Reformation and effort to revive Catholicism in Central and Northern Europe. The second largest group of students were from Scandinavia.

Brno and Olomouc served as Moravia's capitals until 1641. As the only city to successfully resist the Swedish invasion, Brno become the sole capital following the capture of Olomouc. The Margraviate of Moravia had, from 1348 in Olomouc and Brno, its own Diet, or parliament, "zemský sněm" ("Landtag" in German), whose deputies from 1905 onward were elected separately from the ethnically separate German and Czech constituencies.

The oldest surviving theatre building in Central Europe, the Reduta Theatre, was established in 17th-century Moravia. Ottoman Turks and Tatars invaded the region in 1663, taking 12,000 captives. In 1740, Moravia was invaded by Prussian forces under Frederick the Great, and Olomouc was forced to surrender on 27 December 1741. A few months later the Prussians were repelled, mainly because of their unsuccessful siege of Brno in 1742. In 1758, Olomouc was besieged by Prussians again, but this time its defenders forced the Prussians to withdraw following the Battle of Domstadtl. In 1777, a new Moravian bishopric was established in Brno, and the Olomouc bishopric was elevated to an archbishopric. In 1782, the Margraviate of Moravia was merged with Austrian Silesia into "Moravia-Silesia", with Brno as its capital. This lasted until 1850. Moravia was briefly one of 17 former crown lands of the Cisleithanian part of Austria-Hungary after 1867. According to Austro-Hungarian census of 1910 the proportion of Czech in the population of Moravia at the time (2.622.000) was 71,8 %, while the proportion of Germans was 27,6 %.

Following the break-up of the Austro-Hungarian Empire in 1918, Moravia became part of Czechoslovakia. As one of the five lands of Czechoslovakia, it had restricted autonomy. In 1928 Moravia ceased to exist as a territorial unity and was merged with Czech Silesia into the Moravian-Silesian Land (yet with the natural dominance of Moravia). By the Munich Agreement (1938), the southwestern and northern peripheries of Moravia, which had a German-speaking majority, were annexed by Nazi Germany, and during the German occupation of Czechoslovakia (1939–1945), the remnant of Moravia was an administrative unit within the Protectorate of Bohemia and Moravia. During the WW II Moravia lost 46,306 Jews according to religion.

In 1945 after the end of World War II and Allied defeat of Germany, Czechoslovakia expelled the ethnic German minority of Moravia to Germany and Austria. The Moravian-Silesian Land was restored with Moravia as part of it and towns and villages that were left by the former German inhabitants, were re-settled by Czech-speakers. In 1949 the territorial division of Czechoslovakia was radically changed, as the Moravian-Silesian Land was abolished and Lands were replaced by ""kraje"" (regions), whose borders substantially differ from the historical Bohemian-Moravian border, so Moravia politically ceased to exist after more than 1100 years (833–1949) of its history. Although another administrative reform in 1960 implemented (among others) the North Moravian and the South Moravian regions ("Severomoravský" and "Jihomoravský kraj"), with capitals in Ostrava and Brno respectively, their joint area was only roughly alike the historical state and, chiefly, there was no land or federal autonomy, unlike Slovakia.

After the fall of the Soviet Union and the whole Eastern Bloc, the Czechoslovak Federal Assembly condemned the cancellation of Moravian-Silesian land and expressed "firm conviction that this injustice will be corrected" in 1990. However, after the breakup of Czechoslovakia into Czech Republic and Slovakia in 1993, Moravian area remained integral to the Czech territory, and the latest administrative division of Czech Republic (introduced in 2000) is similar to the administrative division of 1949. Nevertheless, the federalist or separatist movement in Moravia is completely marginal.

The centuries-lasting historical Bohemian-Moravian border has been preserved up to now only by the Czech Roman Catholic Administration, as the Ecclesiastical Province of Moravia corresponds with the former Moravian-Silesian Land. The popular perception of the Bohemian-Moravian border's location is distorted by the memory of the 1960 regions (whose boundaries are still partly in use).

An area in South Moravia, around Hodonín and Břeclav, is part of the Viennese Basin. Petroleum and lignite are found there in abundance. The main economic centres of Moravia are Brno, Olomouc and Zlín, plus Ostrava lying directly on the Moravian-Silesian border. As well as agriculture in general, Moravia is noted for its viticulture; it contains 94% of the Czech Republic's vineyards and is at the centre of the country's wine industry. Wallachia have at least a 400-year-old tradition of slivovitz making.

Czech automotive industry also had a large role in the industry of Moravia, plants such as in Prostějov or Tatra in Kopřivnice had produced many aerodynamic automobiles in the 20th century.

Moravia is also the centre of the Czech firearm industry, as the vast majority of Czech firearms manufacturers (e.g. CZUB, Zbrojovka Brno, Czech Small Arms, Czech Weapons, ZVI, Great Gun) are settled in Moravia. Almost all well-known Czech sporting, self-defence, military and hunting firearms come from Moravia. Also, Meopta rifle scopes are of Moravian origin. The original Bren gun was conceived here, as was the assault rifles CZ-805 BREN or Sa vz. 58, and handguns CZ 75 or ZVI Kevin (also known as the "Micro Desert Eagle").

The Zlín Region hosts several aircraft manufacturers, namely Let Kunovice (also known as Aircraft Industries, a.s.), ZLIN AIRCRAFT a.s. Otrokovice (former well-known name Moravan Otrokovice), Evektor-Aerotechnik and Czech Sport Aircraft. Sport aircraft are also manufactured in Jihlava by Jihlavan Airplanes/Skyleader.

Aircraft production in the region started in 1930s and there are signs of recovery in recent years and the production is expected to grow from 2013 onwards.

Machinery has been the most important industrial sector in the region, especially in South Moravia, for many decades. The main centres of machinery production are Brno (Zbrojovka Brno, Zetor, První brněnská strojírna, Siemens), Blansko (ČKD Blansko, Metra), Adamov (ADAST), Kuřim (TOS Kuřim), Boskovice (Minerva, Novibra) and Břeclav (Otis Elevator Company), together with a large number of other variously sized machinery or machining factories, companies or workshops spread all over Moravia.

The beginnings of the electrical industry in Moravia date back to 1918. The biggest centres of electrical production are Brno (VUES, ZPA Brno, EM Brno), Drásov, Frenštát pod Radhoštěm and Mohelnice (currently Siemens).


The Moravians are generally a Slavic ethnic group who speak various (generally more archaic) dialects of Czech. Before the expulsion of Germans from Moravia the Moravian German minority also referred to themselves as "Moravians" ("Mährer"). Those expelled and their descendants continue to identify as Moravian.

Moravia historically had a large minority of ethnic Germans, some of whom had arrived as early as the 13th century at the behest of the Přemyslid dynasty. Germans continued to come to Moravia in waves, culminating in the 18th century. They lived in the main city centres and in the countryside along the border with Austria (stretching up to Brno) and along the border with Silesia at Jeseníky, and also in two language islands, around Jihlava and around Moravská Třebová. After the Second World War, Czechoslovakia almost fully expelled them in retaliation for Nazi German efforts to create a Greater Germanic Reich in Central Europe.

Notable people from Moravia include (in order of birth):


Moravia can be divided on dialectal and lore basis into several ethnographic regions of comparable significance. In this sense, it is more heterogenous than Bohemia. Significant parts of Moravia, usually those formerly inhabited by the German speakers, are dialectally indifferent, as they have been resettled by people from various Czech (and Slovak) regions.

The principal cultural regions of Moravia are:








</doc>
<doc id="20217" url="https://en.wikipedia.org/wiki?curid=20217" title="Murray Rothbard">
Murray Rothbard

Murray Newton Rothbard (; March 2, 1926 – January 7, 1995) was an American heterodox economist of the Austrian School, historian, and a political theorist whose writings and personal influence played a seminal role in the development of modern right-libertarianism. Rothbard was the founder and leading theoretician of anarcho-capitalism, a staunch advocate of historical revisionism and a central figure in the 20th-century American libertarian movement. He wrote over twenty books on political theory, revisionist history, economics, and other subjects.

Rothbard asserted that all services provided by the "monopoly system of the corporate state" could be provided more efficiently by the private sector and wrote that the state is "the organization of robbery systematized and writ large". He called fractional-reserve banking a form of fraud and opposed central banking. He categorically opposed all military, political, and economic interventionism in the affairs of other nations. According to his protégé Hans-Hermann Hoppe, "[t]here would be no anarcho-capitalist movement to speak of without Rothbard".

Economist Jeffrey Herbener, who calls Rothbard his friend and "intellectual mentor", wrote that Rothbard received "only ostracism" from mainstream academia. Rothbard rejected mainstream economic methodologies and instead embraced the praxeology of his most important intellectual precursor, Ludwig von Mises. To promote his economic and political ideas, Rothbard joined Lew Rockwell and Burton Blumert in 1982 to establish the Mises Institute in Alabama.

Rothbard's parents were David and Rae Rothbard, Jewish immigrants to the United States from Poland and Russia, respectively. David Rothbard was a chemist. Murray attended Birch Wathen, a private school in New York City. Rothbard later stated that he much preferred Birch Wathen to the "debasing and egalitarian public school system" he had previously attended in the Bronx.

Rothbard wrote of having grown up as a "right-winger" (adherent of the "Old Right") among friends and neighbors who were "communists or fellow-travelers". He was a member of The New York Young Republican Club in his youth. Rothbard characterized his immigrant father as an individualist who embraced the American values of minimal government, free enterprise, private property and "a determination to rise by one's own merits ... "[A]ll socialism seemed to me monstrously coercive and abhorrent".
He attended Columbia University, where he received a Bachelor of Arts degree in mathematics in 1945 and eleven years later his Ph.D. in economics in 1956. The delay in receiving his Ph.D. was due in part to conflict with his advisor, Joseph Dorfman, and in part to Arthur Burns’s rejecting his doctoral dissertation. Burns was a longtime friend of the Rothbard family and their neighbor at their Manhattan apartment building. It was only after Burns went on leave from the Columbia faculty to head President Eisenhower's Council of Economic Advisors that Rothbard's thesis was accepted and he received his doctorate. Rothbard later stated that all of his fellow students there were extreme leftists and that he was one of only two Republicans on the Columbia campus at the time.

During the 1940s, Rothbard became acquainted with Frank Chodorov and read widely in libertarian-oriented works by Albert Jay Nock, Garet Garrett, Isabel Paterson, H. L. Mencken, and others as well as Austrian economist Ludwig von Mises. In the early 1950s, when Mises was teaching at the Wall Street division of New York University Business School, Rothbard attended Mises' unofficial seminar. Rothbard was greatly influenced by Mises' book, "Human Action". Rothbard attracted the attention of the William Volker Fund, a group that provided financial backing to promote various right-wing ideologies in the 1950s and early 1960s. The Volker Fund paid Rothbard to write a textbook to explain "Human Action" in a form which could be used to introduce college undergraduates to Mises' views; a sample chapter he wrote on money and credit won Mises's approval. For ten years, Rothbard was paid a retainer by the Volker Fund, which designated him a "senior analyst". As Rothbard continued his work, he enlarged the project. The result was Rothbard's book "Man, Economy, and State", published in 1962. Upon its publication, Mises praised Rothbard's work effusively.

In 1953, he married JoAnn Schumacher (1928–1999), whom he called Joey, in New York City. JoAnn was his editor and a close adviser as well as hostess of his Rothbard Salon. They enjoyed a loving marriage and Rothbard often called her "the indispensable framework" behind his life and achievements. According to Joey, patronage from the Volker Fund allowed Rothbard to work from home as a freelance theorist and pundit for the first fifteen years of their marriage. The Volker Fund collapsed in 1962, leading Rothbard to seek employment from various New York academic institutions. He was offered a part-time position teaching economics to the engineering students of Brooklyn Polytechnic Institute in 1966 at age 40. This institution had no economics department or economics majors and Rothbard derided its social science department as "Marxist". However, Justin Raimondo writes that Rothbard liked his role with Brooklyn Polytechnic because working only two days a week gave him freedom to contribute to developments in libertarian politics.

Rothbard continued in this role for twenty years until 1986. Then 60 years old, Rothbard left Brooklyn Polytechnic Institute for the Lee Business School at the University of Nevada, Las Vegas (UNLV), where he held the title of S.J. Hall Distinguished Professor of Economics, an endowed chair paid for by a libertarian businessman. According to Rothbard's friend, colleague and fellow Misesian economist Hans-Hermann Hoppe, Rothbard led a "fringe existence" in academia, but he was able to attract a large number of "students and disciples" through his writings, thereby becoming "the creator and one of the principal agents of the contemporary libertarian movement". Rothbard maintained his position at UNLV from 1986 until his death. Rothbard founded the Center for Libertarian Studies in 1976 and the "Journal of Libertarian Studies" in 1977. In 1982, he co-founded the Ludwig von Mises Institute in Auburn, Alabama and was vice president of academic affairs until 1995. The Institute's "Review of Austrian Economics", a heterodox economics journal later renamed the "Quarterly Journal of Austrian Economics", was also founded by Rothbard in 1987.
After Rothbard's death, Joey reflected on Rothbard's happiness and bright spirit, saying that "he managed to make a living for 40 years without having to get up before noon. This was important to him". She recalled how Rothbard would begin every day with a phone conversation with his colleague Lew Rockwell: "Gales of laughter would shake the house or apartment, as they checked in with each other. Murray thought it was the best possible way to start a day". Rothbard was irreligious and agnostic toward the existence of God, describing himself as a "mixture of an agnostic and a Reform Jew". Despite identifying as an agnostic and an atheist, Rothbard was critical of the "left-libertarian hostility to religion". In Rothbard's later years, many of his friends anticipated that he would convert to Catholicism, but he never did. "The New York Times" obituary called Rothbard "an economist and social philosopher who fiercely defended individual freedom against government intervention".

In 1954, Rothbard, along with several other attendees of Mises' seminar, joined the circle of novelist Ayn Rand, the founder of Objectivism. He soon parted from her, writing among other things that her ideas were not as original as she proclaimed, but similar to those of Aristotle, Thomas Aquinas and Herbert Spencer. In 1958, after the publication of Rand's novel "Atlas Shrugged", Rothbard wrote a "fan letter" to her, calling the book "an infinite treasure house" and "not merely the greatest novel ever written, [but] one of the very greatest books ever written, fiction or nonfiction". He also wrote: "[Y]ou introduced me to the whole field of natural rights and natural law philosophy", prompting him to learn "the glorious natural rights tradition". Rothbard rejoined Rand's circle for a few months, but he soon broke with Rand once more over various differences, including his defense of anarchism.

Later, Rothbard satirized Rand's acolytes in his unpublished one-act play "Mozart Was a Red" written as a farce and the essay "The Sociology of the Ayn Rand Cult". Rothbard characterized Rand's circle as a "dogmatic, personality cult". His play parodies Rand (through the character Carson Sand) and her friends and is set during a visit from Keith Hackley, a fan of Sand's novel "The Brow of Zeus" (a play on Rand's most famous novel, "Atlas Shrugged").

Rothbard died of a heart attack on January 7, 1995 at the age of 68. He was buried in Oakwood Cemetery, Unionville, Virginia.

Rothbard was an advocate and practitioner of the Austrian School tradition of his teacher Ludwig von Mises. Like Mises, Rothbard rejected the application of the scientific method to economics and dismissed econometrics, empirical and statistical analysis and other tools of mainstream social science as useless for the study of economics. He instead embraced praxeology, the strictly "a priori" methodology of Mises. Praxeology conceives of economic laws as akin to geometric or mathematical axioms: fixed, unchanging, objective and discernible through logical reasoning without the use of any evidence. On the account of Misesian economist Hans-Hermann Hoppe, eschewing the scientific method and empirical evidence distinguishes the Misesian approach "from all other current economic schools". Mark Skousen of Chapman University and the Foundation for Economic Education, a critic of mainstream economics, praises Rothbard as brilliant, his writing style persuasive, his economic arguments nuanced and logically rigorous and his Misesian methodology sound. However, citing Rothbard's absence of academic publications, Skousen concedes that Rothbard was effectively "outside the discipline" of mainstream economics and that his work "fell on deaf ears" outside his ideological circles. Paralleling Skousen's remarks, Hoppe laments the fact that all non-Misesian economists dismiss as "dogmatic and unscientific" the Misesian approach, which both he and Rothbard embraced.

Rothbard wrote extensively on Austrian business cycle theory and as part of this approach strongly opposed central banking, fiat money and fractional-reserve banking and advocated a gold standard and a 100% reserve requirement for banks.

Rothbard authored a series of scathing polemics against modern mainstream economics. He was critical of Adam Smith, calling him a "shameless plagiarist" who set economics off-track, ultimately leading to the rise of Marxism. Instead, Rothbard praised Smith's contemporaries' works, including Richard Cantillon, Anne Robert Jacques Turgot and Étienne Bonnot de Condillac for developing the subjective theory of value. In response to Rothbard's charge that Smith's "The Wealth of Nations" was largely plagiarized, David D. Friedman castigated Rothbard's scholarship and character, saying that he "was [either] deliberately dishonest or never really read the book he was criticizing". Tony Endres called Rothbard's treatment of Adam Smith a "travesty".

Rothbard was equally scathing in his criticism of John Maynard Keynes, labeling Keynes weak on economic theory and a shallow political opportunist. Rothbard also wrote more generally that Keynesian-style governmental regulation of money and credit created a "dismal monetary and banking situation". He demeaned John Stuart Mill as a "wooly man of mush" and speculated that Mill's "soft" personality led his economic thought astray.

Rothbard was critical of monetarist economist Milton Friedman. In a polemic entitled "Milton Friedman Unraveled", he maligned Friedman as a "statist", a "favorite of the establishment", a friend of and "apologist" for Richard Nixon and a "pernicious influence" on public policy. Rothbard said that libertarians should scorn rather than celebrate Friedman's academic prestige and political influence. Noting that Rothbard has "been nasty to me and my work", Friedman responded to Rothbard's criticism by calling him a "cult builder and a dogmatist".

In a memorial volume published by the Mises Institute, Rothbard's protégé and libertarian theorist Hans-Hermann Hoppe wrote that the work "Man, Economy, and State" "presented a blistering refutation of all variants of mathematical economics" and included it among Rothbard's "almost mind-boggling achievements". Hoppe lamented that like his own mentor Ludwig von Mises, Rothbard died without winning the Nobel Prize that Hoppe says Rothbard deserved "twice over". Although Hoppe acknowledged that Rothbard and his work were largely ignored by academia, he called Rothbard an "intellectual giant" comparable to Aristotle, John Locke, and Immanuel Kant.

Although he self-identified as an Austrian economist, Rothbard's methodology was at odds with that of many other Austrians. In 1956, Rothbard deprecated the views of Austrian economist Fritz Machlup, stating that Machlup was no praxeologist and calling him instead a "positivist" who failed to represent the views of Ludwig von Mises. Rothbard stated that in fact Machlup shared the opposing positivist view associated with economist Milton Friedman. Mises and Machlup had been colleagues in 1920s Vienna before each relocated to the United States and Mises later urged his American protege Israel Kirzner to pursue his PhD studies with Machlup at Johns Hopkins University.

According to libertarian economists Tyler Cowen and Richard Fink, Rothbard wrote that the term evenly rotating economy (ERE) can be used to analyze complexity in a world of change. The words ERE had been introduced by Mises as an alternative nomenclature for the mainstream economic method of static equilibrium and general equilibrium analysis. Cowen and Fink found "serious inconsistencies in both the nature of the ERE and its suggested uses". With the sole exception of Rothbard, no other economist adopted Mises' term and the concept continued to be called "equilibrium analysis".

In a 2011 article critical of Rothbard's "reflexive opposition" to inflation, "The Economist" noted that his views are increasingly gaining influence among politicians and laypeople on the right. The article contrasted Rothbard's categorical rejection of inflationary policies with the monetary views of "sophisticated Austrian-school monetary economists such as George Selgin and Larry White", [who] follow Hayek in treating stability of nominal spending as a monetary ideal—a position "not all that different from Mr [Scott] Sumner's".

According to economist Peter Boettke, Rothbard is better described as a property rights economist than as an Austrian economist. In 1988, Boettke noted that Rothbard "vehemently attacked all of the books of the younger Austrians".

Although Rothbard adopted Ludwig von Mises' deductive methodology for his social theory and economics, he parted with Mises on the question of ethics. Specifically, he rejected Mises conviction that ethical values remain subjective and opposed utilitarianism in favor of principle-based, natural law reasoning. In defense of his free market views, Mises employed utilitarian economic arguments aimed at demonstrating that interventionist policies made all of society worse off. On the other hand, Rothbard concluded that interventionist policies do in fact benefit some people, including certain government employees and beneficiaries of social programs. Therefore, unlike Mises, Rothbard attempted to assert an objective, natural law basis for the free market. He called this principle "self-ownership", loosely basing the idea on the writings of John Locke and also borrowing concepts from classical liberalism and the anti-imperialism of the Old Right.

Rothbard accepted the labor theory of property, but rejected the Lockean proviso, arguing that if an individual mixes his labor with unowned land then he becomes the proper owner eternally and that after that time it is private property which may change hands only by trade or gift.

Rothbard was a strong critic of egalitarianism. The title essay of Rothbard's 1974 book "Egalitarianism as a Revolt Against Nature and Other Essays" held: "Equality is not in the natural order of things, and the crusade to make everyone equal in every respect (except before the law) is certain to have disastrous consequences". In it, Rothbard wrote: "At the heart of the egalitarian left is the pathological belief that there is no structure of reality; that all the world is a tabula rasa that can be changed at any moment in any desired direction by the mere exercise of human will".

Noam Chomsky critiqued Rothbard's ideal society as "a world so full of hate that no human being would want to live in it... First of all, it couldn't function for a second—and if it could, all you'd want to do is get out, or commit suicide or something."

Various theorists have espoused legal philosophies similar to anarcho-capitalism. However, Rothbard was the first person to use the term as in the mid-20th century he synthesized elements from the Austrian School of economics, classical liberalism and 19th-century American individualist anarchists. According to Lew Rockwell, Rothbard was the "conscience" of all the various strains of libertarian anarchism, whose contemporary advocates are former "colleagues" of Rothbard personally inspired by his example.

During his years at graduate school in the late 1940s, Rothbard considered whether a strict "laissez-faire" policy would require that private police agencies replace government protective services. He visited Baldy Harper, a founder of the Foundation for Economic Education, who doubted the need for any government whatsoever. During this period, Rothbard was influenced by 19th-century American individualist anarchists like Lysander Spooner and Benjamin Tucker and the Belgian economist Gustave de Molinari who wrote about how such a system could work. Thus, he "combined the "laissez-faire" economics of Mises with the absolutist views of human rights and rejection of the state" from individualist anarchists. In an unpublished memo written around 1949, Rothbard concluded that in order to believe in "laissez-faire" one must also embrace anarchism.

Rothbard began to consider himself a private property anarchist in 1950 and later began to use "anarcho-capitalist" to describe his political ideology. In his anarcho-capitalist model, a system of protection agencies compete in a free market and are voluntarily supported by consumers who choose to use their protective and judicial services. Anarcho-capitalism would mean the end of the state monopoly on force.

In "Man, Economy, and State", Rothbard divides the various kinds of state intervention in three categories: "autistic intervention", which is interference with private non-economic activities; "binary intervention", which is forced exchange between individuals and the state; and "triangular intervention", which is state-mandated exchange between individuals. According to Sanford Ikeda, Rothbard's typology "eliminates the gaps and inconsistencies that appear in Mises's original formulation". Rothbard writes in "Power and Market" that the role of the economist in a free market is limited, but it is much larger in a government that solicits economic policy recommendations. Rothbard argues that self-interest therefore prejudices the views of many economists in favor of increased government intervention.

Michael O'Malley, associate professor of history at George Mason University, characterizes Rothbard's "overall tone regard[ing]" the civil rights movement and the women's suffrage movement to be "contemptuous and hostile". Rothbard vilified women's rights activists, attributing the growth of the welfare state to politically active spinsters "whose busybody inclinations were not fettered by the responsibilities of health and heart". Rothbard had pointed out in his "Origins of the Welfare State" that progressives had evolved from elitist Gilded Age pietist Protestants that wanted to bring a secularized version of millennialism under a welfare state, which was spearheaded by a coalition of Yankee Protestant and Jewish women and "lesbian spinsters".

Rothbard called for the elimination of "the entire 'civil rights' structure" stating that it "tramples on the property rights of every American". He consistently favored repeal of the 1964 Civil Rights Act, including Title VII regarding employment discrimination, and called for overturning the "Brown v. Board of Education" decision on the grounds that forced integration of schools was aggressive. In an essay called "Right-wing Populism", Rothbard proposed a set of measures to "reach out" to the "middle and working classes", which included urging the police to crack down on "street criminals", writing that "cops must be unleashed" and "allowed to administer instant punishment, subject of course to liability when they are in error". He also advocated that the police "clear the streets of bums and vagrants. Where will they go? Who cares? Hopefully, they will disappear, that is, move from the ranks of the petted and cosseted bum class to the ranks of the productive members of society."

Rothbard held strong opinions about many leaders of the civil rights movement. He considered black separatist Malcolm X to be a "great black leader" and integrationist Martin Luther King Jr. to be favored by whites because he "was the major restraining force on the developing Negro revolution". Rothbard rejected the idea of "compulsory integration" and felt that "self-help, pride, thrift, Negro businesses, etc... cannot hope to flourish within the context of the black reality in America: permanent oppression by the white 'power structure.' None of these good and libertarian things can be achieved without first and foremost, getting the white-run U. S. and local and state governments off the backs of the Negro people." In 1993 he rejected the vision of a "separate black nation", asking "does anyone really believe that ... New Africa would be content to strike out on its own, with no massive "foreign aid" from the U.S.A.?". Rothbard also suggested that opposition to King, whom he demeaned as a "coercive integrationist", should be a litmus test for members of his "paleolibertarian" political movement.

Political scientist Jean Hardisty commented on Rothbard's acceptance of the evidence presented in Richard Herrnstein and Charles Murray's book "The Bell Curve", that blacks tend to score on average lower than whites on IQ tests.

Like Randolph Bourne, Rothbard believed that "war is the health of the state". According to David Gordon, this was the reason for Rothbard's opposition to aggressive foreign policy. Rothbard believed that stopping new wars was necessary and that knowledge of how government had led citizens into earlier wars was important. Two essays expanded on these views "War, Peace, and the State" and "The Anatomy of the State". Rothbard used insights of Vilfredo Pareto, Gaetano Mosca and Robert Michels to build a model of state personnel, goals and ideology. In an obituary for his friend historical revisionist Harry Elmer Barnes, Rothbard wrote:
Rothbard's colleague Joseph Stromberg notes that Rothbard made two exceptions to his general condemnation of war: "the American Revolution and the War for Southern Independence, as viewed from the Confederate side". Rothbard condemned the "Northern war against slavery", saying it was inspired by "fanatical" religious faith and characterized by "a cheerful willingness to uproot institutions, to commit mayhem and mass murder, to plunder and loot and destroy, all in the name of high moral principle". He celebrated Jefferson Davis, Robert E. Lee and other prominent Confederates as heroes while denouncing Abraham Lincoln, Ulysses S. Grant and other Union leaders for "open[ing] the Pandora's Box of genocide and the extermination of civilians" in their war against the South.

Rothbard's "The Libertarian Forum" blamed the Middle East conflict on Israeli aggression "fueled by American arms and money". Rothbard warned that the Middle East conflict would draw the United States into a world war. He was anti-Zionist and opposed United States involvement in the Middle East. Rothbard criticized the Camp David Accords for having betrayed Palestinian aspirations and opposed Israel's 1982 invasion of Lebanon. In his essay, "War Guilt in the Middle East", Rothbard states that Israel refused "to let these refugees return and reclaim the property taken from them". He took negative views of the two state solution for the Israeli–Palestinian conflict, saying: On the one hand there are the Palestinian Arabs, who have tilled the soil or otherwise used the land of Palestine for centuries; and on the other, there are a group of external fanatics, who come from all over the world, and who claim the entire land area as "given" to them as a collective religion or tribe at some remote or legendary time in the past. There is no way the two claims can be resolved to the satisfaction of both parties. There can be no genuine settlement, no "peace" in the face of this irrepressible conflict; there can only be either a war to the death, or an uneasy practical compromise which can satisfy no one. That is the harsh reality of the Middle East.

Rothbard embraced "historical revisionism" as an antidote to what he perceived to be the dominant influence exerted by corrupt "court intellectuals" over mainstream historical narratives. Rothbard wrote that these mainstream intellectuals distorted the historical record in favor of "the state" in exchange for "wealth, power, and prestige" from the state. Rothbard characterized the revisionist task as "penetrating the fog of lies and deception of the State and its Court Intellectuals, and to present to the public the true history". He was influenced by and called a champion of the historian Harry Elmer Barnes, a Holocaust denier. Rothbard endorsed Barnes's revisionism on World War II, favorably citing his view that "the murder of Germans and Japanese was the overriding aim of World War II". In addition to broadly supporting his historical views, Rothbard promoted Barnes as an influence for future revisionists.

Rothbard's endorsing of World War II revisionism and his association with Barnes and other Holocaust deniers have drawn criticism from within the political right. Kevin D. Williamson wrote an opinion piece published by "National Review" which condemned Rothbard for "making common cause with the 'revisionist' historians of the Third Reich", a term he used to describe American Holocaust deniers associated with Rothbard, such as James J. Martin of the Institute for Historical Review. The piece also characterized "Rothbard and his faction" as being "culpably indulgent" of Holocaust denial, the view which "specifically denies that the Holocaust actually happened or holds that it was in some way exaggerated".

In an article for Rothbard's 50th birthday, Rothbard's friend and Buffalo State College historian Ralph Raico stated that Rothbard "is the main reason that revisionism has become a crucial part of the whole libertarian position".

In the "Ethics of Liberty", Rothbard explores issues regarding children's rights in terms of self-ownership and contract. These include support for a woman's right to abortion, condemnation of parents showing aggression towards children and opposition to the state forcing parents to care for children. He also holds children have the right to run away from parents and seek new guardians as soon as they are able to choose to do so. He asserted that parents have the right to put a child out for adoption or sell the rights to the child in a voluntary contract in what Rothbard suggests will be a "flourishing free market in children". He believes that selling children as consumer goods in accord with market forces—while "superficially monstrous"—will benefit "everyone" involved in the market: "the natural parents, the children, and the foster parents purchasing".

In Rothbard's view of parenthood, "the parent should not have a legal obligation to feed, clothe, or educate his children, since such obligations would entail positive acts coerced upon the parent and depriving the parent of his rights". Thus, Rothbard stated that parents should have the legal right to let any infant die by starvation and should be free to engage in other forms of child neglect. However, according to Rothbard, "the purely free society will have a flourishing free market in children". In a fully libertarian society, he wrote, "the existence of a free baby market will bring such 'neglect' down to a minimum".

Economist Gene Callahan of Cardiff University, formerly a scholar at the Rothbard-affiliated Mises Institute, observes that Rothbard allows "the logical elegance of his legal theory" to "trump any arguments based on the moral reprehensibility of a parent idly watching her six-month-old child slowly starve to death in its crib".

Rothbard consistently advocated for abolition of the subpoena power, court attendance, contempt of court powers, coerced testimony of witnesses, compulsory jury duty, and the bail system, arguing that all these functions of the judiciary were violations of natural rights and American common law. He instead advocated that until a defendant is convicted, he or she should not be held in prison or jails, writing that "except in those cases where the criminal has been caught red-handed and where a certain presumption of guilt therefore exists, it is impossible to justify any imprisonment before conviction, let alone before trial. And even when someone is caught red-handed, there is an important reform that needs to be instituted to keep the system honest: subjecting the police and the other authorities to the same law as everyone else. If everyone is supposed to be subject to the same criminal law, then exempting the authorities from that law gives them a legal license to commit continual aggression. The policeman who apprehends a criminal and arrests him, and the judicial and penal authorities who incarcerate him before trial and conviction—all should be subject to the universal law". Rothbard argued that police who make wrongful arrests or indictments should be charged with kidnapping.

In "The Ethics of Liberty", Rothbard advocates for a "frankly retributive theory of punishment" or a system of "a tooth (or two teeth) for a tooth". Rothbard emphasizes that all punishment must be proportional, stating that "the criminal, or invader, loses his rights to the extent that he deprived another man of his". Applying his retributive theory, Rothbard states that a thief "must pay double the extent of theft". Rothbard gives the example of a thief who stole $15,000 and says he not only would have to return the stolen money, but also provide the victim an additional $15,000, money to which the thief has forfeited his right. The thief would be "put in a [temporary] state of enslavement to his victim" if he is unable to pay him immediately. Rothbard also applies his theory to justify beating and torturing violent criminals, although the beatings are required to be proportional to the crimes for which they are being punished.

In chapter twelve of "Ethics", Rothbard turns his attention to suspects arrested by the police. He argues that police should be able to torture certain types of criminal suspects, including accused murderers, for information related to their alleged crime. Writes Rothbard: "Suppose ... police beat and torture a suspected murderer to find information (not to wring a confession, since obviously a coerced confession could never be considered valid). If the suspect turns out to be guilty, then the police should be exonerated, for then they have only ladled out to the murderer a parcel of what he deserves in return; his rights had already been forfeited by more than that extent. But if the suspect is not convicted, then that means that the police have beaten and tortured an innocent man, and that they in turn must be put into the dock for criminal assault". Gene Callahan examines this position and concludes that Rothbard rejects the widely held belief that torture is inherently wrong, no matter who the victim. Callahan goes on to state that Rothbard's scheme gives the police a strong motive to frame the suspect after having tortured him or her.

In an essay condemning "scientism in the study of man", Rothbard rejected the application of causal determinism to human beings, arguing that the actions of human beings—as opposed to those of everything else in nature—are not determined by prior causes, but by "free will". He argued that "determinism as applied to man, is a self-contradictory thesis, since the man who employs it relies implicitly on the existence of free will". Rothbard opposed what he considered the overspecialization of the academy and sought to fuse the disciplines of economics, history, ethics and political science to create a "science of liberty". Rothbard described the moral basis for his anarcho-capitalist position in two of his books: "For a New Liberty", published in 1973; and "The Ethics of Liberty", published in 1982. In his "Power and Market" (1970), Rothbard describes how a stateless economy might function.

As a young man, Rothbard considered himself part of the Old Right, an anti-statist and anti-interventionist branch of the Republican Party. In the 1948 presidential election, Rothbard, "as a Jewish student at Columbia, horrified his peers by organizing a Students for Strom Thurmond chapter, so staunchly did he believe in states' rights". He was a member of The New York Young Republican Club.

By the late 1960s, Rothbard's "long and winding yet somehow consistent road had taken him from anti-New Deal and anti-interventionist Robert Taft supporter into friendship with the quasi-pacifist Nebraska Republican Congressman Howard Buffett (father of Warren Buffett) then over to the League of (Adlai) Stevensonian Democrats and, by 1968, into tentative comradeship with the anarchist factions of the New Left". Rothbard advocated an alliance with the New Left anti-war movement on the grounds that the conservative movement had been completely subsumed by the statist establishment. However, Rothbard later criticized the New Left for supporting a "People's Republic" style draft. It was during this phase that he associated with Karl Hess and founded "" with Leonard Liggio and George Resch, which existed from 1965 to 1968.

From 1969 to 1984, he edited "The Libertarian Forum", also initially with Hess (although Hess's involvement ended in 1971). The "Libertarian Forum" provided a platform for Rothbard's writing. Despite its small readership, it engaged conservatives associated with the "National Review" in nationwide debate. Rothbard rejected the view that Ronald Reagan's 1980 election as President was a victory for libertarian principles and he attacked Reagan's economic program in a series of "Libertarian Forum" articles. In 1982, Rothbard called Reagan's claims of spending cuts a "fraud" and a "hoax" and accused Reaganites of doctoring the economic statistics in order to give the false impression that their policies were successfully reducing inflation and unemployment. He further criticized the "myths of Reaganomics" in 1987.

Rothbard criticized the "frenzied nihilism" of left-wing libertarians, but also criticized right-wing libertarians who were content to rely only on education to bring down the state; he believed that libertarians should adopt any moral tactic available to them in order to bring about liberty.

Imbibing Randolph Bourne's idea that "war is the health of the state", Rothbard opposed all wars in his lifetime and engaged in anti-war activism. During the 1970s and 1980s, Rothbard was active in the Libertarian Party. He was frequently involved in the party's internal politics. He was one of the founders of the Cato Institute and "came up with the idea of naming this libertarian think tank after "Cato's Letters", a powerful series of British newspaper essays by John Trenchard and Thomas Gordon which played a decisive influence upon America's Founding Fathers in fomenting the Revolution". From 1978 to 1983, he was associated with the Libertarian Party Radical Caucus, allying himself with Justin Raimondo, Eric Garris and Williamson Evers. He opposed the "low-tax liberalism" espoused by 1980 Libertarian Party presidential candidate Ed Clark and Cato Institute president Edward H Crane III. According to Charles Burris, "Rothbard and Crane became bitter rivals after disputes emerging from the 1980 LP presidential campaign of Ed Clark carried over to strategic direction and management of Cato".

Rothbard split with the Radical Caucus at the 1983 national convention over cultural issues and aligned himself with what he called the "right-wing populist" wing of the party, notably Lew Rockwell and Ron Paul, who ran for President on the Libertarian Party ticket in 1988. Rothbard "worked closely with Lew Rockwell (joined later by his long-time friend Burton Blumert) in nurturing the Ludwig von Mises Institute, and the publication, "The Rothbard-Rockwell Report"; which after Rothbard's 1995 death evolved into the website, "LewRockwell.com"".

In 1989, Rothbard left the Libertarian Party and began building bridges to the post-Cold War anti-interventionist right, calling himself a paleolibertarian, a conservative reaction against the cultural liberalism of mainstream libertarianism. Paleolibertarianism sought to appeal to disaffected working class whites through a synthesis of cultural conservatism and libertarian economics. According to "Reason", Rothbard advocated right-wing populism in part because he was frustrated that mainstream thinkers were not adopting the libertarian view and suggested that former KKK Grand Wizard David Duke and Wisconsin Senator Joseph McCarthy were models for an "Outreach to the Rednecks" effort that could be used by a broad libertarian/paleoconservative coalition. Working together, the coalition would expose the "unholy alliance of 'corporate liberal' Big Business and media elites, who, through big government, have privileged and caused to rise up a parasitic Underclass". Rothbard blamed this "Underclass" for "looting and oppressing the bulk of the middle and working classes in America". Rothbard noted that Duke's substantive political program in a Louisiana governor's race had "nothing" in it that "could not also be embraced by paleoconservatives or paleo-libertarians; lower taxes, dismantling the bureaucracy, slashing the welfare system, attacking affirmative action and racial set-asides, calling for equal rights for all Americans, including whites".

Rothbard supported the presidential campaign of Pat Buchanan in 1992 and wrote that "with Pat Buchanan as our leader, we shall break the clock of social democracy". When Buchanan dropped out of the Republican primary race, Rothbard then shifted his interest and support to Ross Perot, who Rothbard wrote had "brought an excitement, a verve, a sense of dynamics and of open possibilities to what had threatened to be a dreary race". Rothbard ultimately supported George H. W. Bush over Bill Clinton in the 1992 election.

Like Buchanan, Rothbard opposed the North American Free Trade Agreement (NAFTA). However, he had become disillusioned with Buchanan by 1995, believing that the latter's "commitment to protectionism was mutating into an all-round faith in economic planning and the nation state".

After Rothbard's death in 1995, Lew Rockwell, president of the Mises Institute, told "The New York Times" that Rothbard was "the founder of right-wing anarchism". William F. Buckley Jr. wrote a critical obituary in the "National Review", criticizing Rothbard's "defective judgment" and views on the Cold War. The Mises Institute published "Murray N. Rothbard, In Memoriam" which included memorials from 31 individuals, including libertarians and academics. Journalist Brian Doherty has summarized Buckley's obituary as follows: "[W]hen Rothbard died in 1995, his old pal William Buckley took pen in hand to piss on his grave". Hoppe, Rockwell, and Rothbard's colleagues at the Mises Institute took a different view, arguing that he was one of the most important philosophers in history.

Articles

Books

Book contributions

Monographs






</doc>
<doc id="20218" url="https://en.wikipedia.org/wiki?curid=20218" title="Mel Brooks">
Mel Brooks

Mel Brooks (born Melvyn Kaminsky; June 28, 1926) is an American director, writer, actor, comedian, producer and composer. He is known as a creator of broad film farces and comedic parodies. Brooks began his career as a comic and a writer for the early TV variety show "Your Show of Shows". Together with Carl Reiner, he created the comic character The 2000 Year Old Man. He wrote, with Buck Henry, the hit television comedy series "Get Smart", which ran from 1965 to 1970.

In middle age, Brooks became one of the most successful film directors of the 1970s, with many of his films being among the top 10 moneymakers of the year they were released. His best-known films include "The Producers" (1967), "The Twelve Chairs" (1970), "Blazing Saddles" (1974), "Young Frankenstein" (1974), "Silent Movie" (1976), "High Anxiety" (1977), "History of the World, Part I" (1981), "Spaceballs" (1987), and "" (1993). A musical adaptation of his first film, "The Producers", ran on Broadway from 2001 to 2007.

In 2001, having previously won an Emmy, a Grammy and an Oscar, he joined a small list of EGOT winners with his Tony Award for "The Producers". He received a Kennedy Center Honor in 2009, a Hollywood Walk of Fame star in 2010, the 41st AFI Life Achievement Award in June 2013, a British Film Institute Fellowship in March 2015, a National Medal of Arts in September 2016, and a BAFTA Fellowship in February 2017. Three of his films ranked in the American Film Institute's list of the top 100 comedy films of the past 100 years (1900–2000), all of which ranked in the top 15 of the list: "Blazing Saddles" at number 6, "The Producers" at number 11, and "Young Frankenstein" at number 13.

Brooks was married to the actress Anne Bancroft from 1964 until her death in 2005. Their son Max Brooks is an actor and author, known for his novel "" (2006).

Brooks was born Melvyn Kaminsky on June 28, 1926, in Brooklyn, New York, to Max (1895-1929) and Kate (née Brookman) Kaminsky (1896-1989), and grew up in Williamsburg. His father's family were German Jews from Danzig (present-day Gdańsk, Poland); his mother's family were Jews from Kiev, in the Pale of Settlement of the Russian Empire (present-day Ukraine). He had three older brothers: Irving, Lenny, and Bernie. Brooks' father died of kidney disease at 34 when Brooks was 2 years old. He has said of his father's death, "There's an outrage there. I may be angry at God, or at the world, for that. And I'm sure a lot of my comedy is based on anger and hostility. Growing up in Williamsburg, I learned to clothe it in comedy to spare myself problems—like a punch in the face."

Brooks was a small, sickly boy who often was bullied and teased by his classmates because of his size. He grew up in tenement housing. At age 9, Brooks went to a Broadway show with his uncle Joe—a taxi driver who would drive the Broadway doormen back to Brooklyn for free and was given the tickets in gratitude—and saw "Anything Goes" with William Gaxton, Ethel Merman and Victor Moore at the Alvin Theater. After the show, he told his uncle that he was not going to work in the garment district like everyone else but was absolutely going into show business.

When Brooks was 14 he gained employment as a pool tummler. Brooks kept his guests amused with his crazy antics. In a "Playboy" interview Brooks explained that one day he stood at the edge of a diving board wearing a large overcoat and 2 suitcases full of rocks who then announced: "Business is terrible! I can't go on!" before jumping, fully clothed into the pool. He was taught by Buddy Rich (who had also grown up in Williamsburg) how to play the drums and started to earn money as a musician when he was 14. During Brooks' time as a drummer he was given his first opportunity as a comedian at the age of 16 following an ill MC. During his teens, Melvyn Kaminsky officially changed his name to Mel Brooks. (from his mother's maiden name Brookman) after being confused with the trumpeter Max Kaminsky.

After attending Abraham Lincoln High School for a year, Brooks graduated from Eastern District High School, studying for a year at Brooklyn College as a psychology major in 1946.

Brooks was drafted into the United States Army in 1944. After scoring highly on the Army General Classification Test (a Stanford-Binet-type IQ test), he was sent to the elite Army Specialized Training Program (ASTP) at the Virginia Military Institute to be taught skills such as military engineering, foreign languages or medicine. Manpower shortages led the Army to disband the ASTP so Brooks returned to basic training at Fort Sill, Oklahoma in May 1944.

Brooks served as a corporal in the 1104 Engineer Combat Battalion, 78th Infantry Division, defusing land mines as the allies advanced into Nazi Germany. With the end of the war in Europe, Brooks took part in organizing shows for captured Germans and American forces.

After the war, Brooks started working in various Borscht Belt resorts and nightclubs in the Catskill Mountains as a drummer and pianist. After a regular comic at one of the nightclubs was too sick to perform one night, Brooks started working as a stand-up comic, telling jokes and doing movie-star impressions. He also began acting in summer stock in Red Bank, New Jersey, and did some radio work. He eventually worked his way up to the comically aggressive job of tummler (master entertainer) at Grossinger's, one of the Borscht Belt's most famous resorts. Brooks found more rewarding work behind the scenes, becoming a comedy writer for television. In 1949 his friend Sid Caesar hired Brooks to write jokes for the NBC series "The Admiral Broadway Revue", paying him $50 a week.

In 1950 Caesar created the revolutionary variety comedy series "Your Show of Shows" and hired Brooks as a writer along with Carl Reiner, Neil Simon, Danny Simon, and head writer Mel Tolkin. The show was an immediate hit and has been influential to all variety and sketch-comedy TV shows since. Reiner, as creator of "The Dick Van Dyke Show", based Morey Amsterdam's character Buddy Sorell on Brooks. Likewise, the film "My Favorite Year" (1982) is loosely based on Brooks' experiences as a writer on the show including an encounter with the actor Errol Flynn. Neil Simon's play "Laughter on the 23rd Floor" (1993) is also loosely based on the production of the show, and the character Ira Stone is based on Brooks."Your Show of Shows" ended in 1954 when performer Imogene Coca left to host her own show. Caesar then created "Caesar's Hour" with most of the same cast and writers (including Brooks and adding Woody Allen and Larry Gelbart). "Caesar's Hour" ran from 1954 until 1957.

Brooks and co-writer Reiner had become close friends and began to casually improvise comedy routines when they were not working. Reiner played the straight-man interviewer and would set Brooks up as anything from a Tibetan monk to an astronaut. As Reiner explained: "In the evening, we'd go to a party and I'd pick a character for him to play. I never told him what it was going to be." On one of these occasions, Reiner's suggestion concerned a 2000 year-old-man who had witnessed the crucifixion of Jesus Christ (who "came in the store but never bought anything"), had been married several hundred times, and had "over forty-two thousand children, and not one comes to visit me." At first Brooks and Reiner only performed the routine for friends but, by the late 1950s, it gained a reputation in New York City. Kenneth Tynan saw the comedy duo perform at a party in 1959 and wrote that Brooks "was the most original comic improvisor I had ever seen."

In 1960, Brooks moved from New York to Hollywood. He and Reiner began performing the "2000 Year Old Man" act on "The Steve Allen Show". Their performances led to the release of the comedy album "2000 Years with Carl Reiner and Mel Brooks" that sold over a million copies in 1961. They eventually expanded their routine with two more albums in 1961 and 1962, a revival in 1973, a 1975 animated TV special, and a reunion album in 1998. At one point, when Brooks had financial and career struggles, the record sales from the 2000 Year Old Man were his chief source of income.

Brooks adapted the 2000 Year Old Man character to create the 2500 Year Old Brewmaster for Ballantine Beer in the 1960s. Interviewed by Dick Cavett in a series of ads, the Brewmaster (in a German accent, as opposed to the 2000 Year Old Man's Yiddish accent) said he was inside the original Trojan horse and "could've used a six-pack of fresh air."

Brooks was involved in the creation of the Broadway musical "All American" which debuted on Broadway in 1962. Brooks wrote the play with lyrics by Lee Adams, and music by Charles Strouse. The show starred Ray Bolger as a southern science professor at a large university who uses the principles of engineering on the college's football team and the team begins to win games. The show was directed by Joshua Logan, whose script doctored the second act and added a gay subtext to the plot. The show ran for 80 performances and received two Tony Award nominations.

The animated short film "The Critic" (1963), a satire of arty, esoteric cinema, was conceived by Brooks and directed by Ernest Pintoff. Brooks supplied running commentary as the baffled moviegoer trying to make sense of the obscure visuals. The short film won the Academy Award for Animated Short Film.

With comedy writer Buck Henry, Brooks created a comedic TV show titled "Get Smart" about a bumbling James Bond-inspired spy. Brooks explains: "I was sick of looking at all those nice sensible situation comedies. They were such distortions of life... I wanted to do a crazy, unreal comic-strip kind of thing about something besides a family. No one had ever done a show about an idiot before. I decided to be the first." The show stars Don Adams as Maxwell Smart, Agent 86. The series ran from 1965 until 1970, although Brooks had little involvement after the first season. "Get Smart" was highly rated for most of its production and won seven Emmy Awards, including Outstanding Comedy Series in 1968 and 1969.

For several years, Brooks had been toying with a bizarre and unconventional idea about a musical comedy of Adolf Hitler. Brooks explored the idea as a novel and a play before finally writing a script. Eventually, he was able to find two producers to fund the show, Joseph E. Levine and Sidney Glazier, and made his first feature film, "The Producers" (1967).

"The Producers" was so brazen in its satire that major studios would not touch it, nor would many exhibitors. Brooks finally found an independent distributor who released it as an art film, a specialized attraction. In 1968, Brooks received an Oscar for Best Original Screenplay for the film instead of such writers as Stanley Kubrick and John Cassavetes. "The Producers" became a smash underground hit, first on the nationwide college circuit, then in revivals and on home video. Brooks later turned it into a musical, which became hugely successful on Broadway, receiving an unprecedented twelve Tony awards.

With the moderate financial success of the film "The Producers", Glazier financed Brooks' next film, "The Twelve Chairs" (1970). Loosely based on Ilf and Petrov's 1928 Russian novel of the same name about greedy materialism in post-revolutionary Russia, the film stars Ron Moody, Frank Langella, and Dom DeLuise as three men individually searching for a fortune in diamonds hidden in a set of 12 antique chairs. Brooks makes a cameo appearance as an alcoholic ex-serf who "yearns for the regular beatings of yesteryear." The film was shot in Yugoslavia with a budget of $1.5 million. The film received poor reviews and was not financially successful.

Brooks then wrote an adaptation of Oliver Goldsmith's "She Stoops to Conquer", but was unable to sell the idea to any studio and believed that his career was over. In 1972, Brooks met agent David Begelman, who helped him set up a deal with Warner Brothers to hire Brooks (as well as Richard Pryor, Andrew Bergman, Norman Steinberg, and Al Uger) as a script doctor for an unproduced script called "Tex-X". Eventually, Brooks was hired as director for what would become "Blazing Saddles" (1974), his third film.

"Blazing Saddles" starred Cleavon Little, Gene Wilder, Harvey Korman, Slim Pickens, Madeline Kahn, Alex Karras, and Brooks himself, with cameos by Dom DeLuise and Count Basie. The film had music by Brooks and John Morris, and had a modest budget of $2.6 million. This film is a satire on the Western film genre and references older films such as "Destry Rides Again" (1939), "High Noon" (1952), "Once Upon a Time in the West" (1968), and "The Treasure of the Sierra Madre" (1948), as well as a surreal scene towards the end of the film referencing the extravagant musicals of Busby Berkeley.

Upon its release, "Blazing Saddles" was the second-highest US grossing film of 1974, earning $119.5 million worldwide. Despite mixed reviews, the film was a success with younger audiences. It was nominated for three Academy Awards: Best Actress in a Supporting Role for Madeline Kahn, Best Film Editing, and Best Music, Original Song. The film won the Writers Guild of America Award for "Best Comedy Written Directly for the Screen" and in 2006 it was deemed "culturally, historically or aesthetically significant" by the Library of Congress and was selected for preservation in the National Film Registry. Brooks has said that the film "has to do with love more than anything else. I mean when that black guy rides into that Old Western town and even a little old lady says 'Up yours, nigger!', you know that his heart is broken. So it's really the story of that heart being mended."

When Gene Wilder replaced Gig Young as the Waco Kid, he did so only if Brooks agreed that his next film would be an idea that Wilder had been working on; a spoof of the Universal series of "Frankenstein" films from several decades earlier. After the filming of "Blazing Saddles" was completed, Wilder and Brooks began writing the script for "Young Frankenstein" and shot the film in the spring of 1974. It starred Wilder, Marty Feldman, Peter Boyle, Teri Garr, Madeline Kahn, Cloris Leachman and Kenneth Mars, with Gene Hackman in a cameo role. Brooks' voice can be heard three times, first as the wolf howl when the characters are on their way to the castle, second as the voice of Victor Frankenstein when the characters discover the laboratory, and third as the cat sound when Gene Wilder accidentally throws a dart out of the window in a scene with Kenneth Mars. Composer John Morris again provided the music score and Universal monsters film special effects veteran Kenneth Strickfaden worked on the film.

"Young Frankenstein" was the third-highest-grossing film domestically of 1974, just behind "Blazing Saddles". It earned $86 million worldwide and received two Academy Award nominations: Academy Award for Writing Adapted Screenplay and Academy Award for Best Sound. It received some of the best reviews of Brooks' career and even critic Pauline Kael liked the film, saying: "Brooks makes a leap up as a director because, although the comedy doesn't build, he carries the story through...Brooks even has a satisfying windup, which makes this just about the only comedy of recent years that doesn't collapse."

In 1975, at the height of his movie career, Brooks tried TV again with "When Things Were Rotten", a Robin Hood parody that lasted only 13 episodes. Nearly 20 years later, in response to the 1991 hit film "", Brooks mounted another Robin Hood parody with "" (1993). Brooks' film resurrected several pieces of dialogue from his TV series, as well as from earlier Brooks films.

Brooks followed up his two hit films with an audacious idea: the first feature-length silent comedy in four decades. "Silent Movie" (1976) was written by Brooks and Ron Clark, starring Brooks in his first leading role, Dom DeLuise, Marty Feldman, Sid Caesar, Bernadette Peters, and in cameo roles playing themselves: Paul Newman, Burt Reynolds, James Caan, Liza Minnelli, Anne Bancroft, and the non-speaking Marcel Marceau who ironically uttered the film's only word of audible dialogue: "Non!" Although not as successful as his previous two films, "Silent Movie" was a hit and grossed $36 million. Later that year, Brooks was named number 5 on a list of the Top Ten Box Office Stars.

Brooks' parody of the films of Alfred Hitchcock in "High Anxiety" (1977) was written by Brooks, Ron Clark, Rudy De Luca, and Barry Levinson. It was the first movie produced by Brooks himself. It starred Brooks, Madeline Kahn, Cloris Leachman, Harvey Korman, Ron Carey, Howard Morris, and Dick Van Patten. The film satirizes such Hitchcock films as "Vertigo", "Spellbound", "Psycho", "The Birds", "North by Northwest", "Dial M for Murder", and "Suspicion". Brooks stars as Professor Richard H. (for Harpo) Thorndyke, a Nobel Prize-winning psychologist who also happens to suffer from "high anxiety".

By 1980 "Siskel and Ebert" called Mel Brooks and Woody Allen "the two most successful comedy directors in the world today ... America's two funniest filmmakers." Released that year was the dramatic film "The Elephant Man" directed by David Lynch and produced by Brooks. Knowing that anyone seeing a poster reading "Mel Brooks presents "The Elephant Man"" would expect a comedy, he set up the company Brooksfilms. Brooksfilms has since produced a number of non-comedy films, including "Frances" (1982), "The Fly" (1986), and "84 Charing Cross Road" (1987), starring Anthony Hopkins and Anne Bancroft, along with comedies, including Richard Benjamin's "My Favorite Year" (1982), which was partially based on Mel Brooks' real life. Brooks sought to purchase the rights to "84 Charing Cross Road" for his wife, Anne Bancroft, for many years. He also produced the comedy "Fatso" (1980) that Bancroft directed.

In 1981, Brooks joked that the only genres that he had not spoofed were historical epics and Biblical spectacles. "History of the World Part I" was a tongue-in-cheek look at human culture from the Dawn of Man to the French Revolution. The film was written, produced, and directed by Brooks with narration by Orson Welles. This film was another modest financial hit, earning $31 million. It received mixed critical reviews. Critic Pauline Kael, who for years had been critical of Brooks, said: "Either you get stuck thinking about the bad taste or you let yourself laugh at the obscenity in the humor as you do Buñuel's perverse dirty jokes."

Brooks produced and starred in (but did not write or direct) a remake of Ernst Lubitsch's 1942 film "To Be or Not to Be". Brooks' 1983 version was directed by Alan Johnson and starred Brooks, Anne Bancroft, Charles Durning, Tim Matheson, Jose Ferrer, and Christopher Lloyd. The film garnered international publicity by featuring a controversial song on its soundtrack – "To Be or Not to Be (The Hitler Rap) – satirizing German society in the 1940s with Brooks playing Hitler.

The second movie Brooks directed in the 1980s came in the form of "Spaceballs" (1987), a parody of science fiction, mainly "Star Wars". The film starred Bill Pullman, John Candy, Rick Moranis, Daphne Zuniga, Dick Van Patten, Joan Rivers, Dom DeLuise, and Brooks. In 1989, Brooks (with co-executive producer Alan Spencer) made another attempt at television success with the sitcom "The Nutt House", which featured Brooks regulars Harvey Korman and Cloris Leachman and was originally broadcast on NBC, but the network only aired five of the eleven episodes produced before canceling the series. During the next decade, Brooks directed "Life Stinks" (1991), "" (1993), and "" (1995). "People" magazine suggested, "anyone in a mood for a hearty laugh couldn't do better than "Robin Hood: Men in Tights", which gave fans a parody of Robin Hood, especially ""."

Like Brooks' other films, it is filled with one-liners and even the occasional breaking of the fourth wall. "Robin Hood: Men in Tights" was Brooks' second time exploring the life of Robin Hood, the first, as mentioned above, having been with his 1975 TV show, "When Things Were Rotten". "Life Stinks" was a financial and critical failure, but is notable as being the only film that Brooks directed that is neither a parody nor a film about other films or theater. ("The Twelve Chairs" was actually a parody of the original novel.) In the 2000s, Brooks worked on an animated series sequel to "Spaceballs" called "", which premiered on September 21, 2008, on G4 TV. Brooks has also supplied vocal roles for animation. He voiced Bigweld, the master inventor, in the animated film "Robots" (2005), and in the later animated film "Mr. Peabody & Sherman" (2014) he had a cameo appearance as Albert Einstein. He returned, to voice Dracula's father, Vlad, in "Hotel Transylvania 2" (2015) and "" (2018).

The musical adaptation of his film "The Producers" to the Broadway stage broke the Tony record with 12 wins, a record that had previously been held for 37 years by "Hello, Dolly!" at 10 wins. This success led to a big-screen version of the Broadway adaptation/remake with actors Matthew Broderick, Nathan Lane, Gary Beach, and Roger Bart reprising their stage roles, in addition to new cast members Uma Thurman and Will Ferrell in 2005. In early April 2006, Brooks began composing the score to a Broadway musical adaptation of "Young Frankenstein", which he says is "perhaps the best movie [he] ever made." The world premiere was performed at Seattle's Paramount Theater, between August 7, 2007, and September 1, 2007, after which it opened on Broadway at the former Lyric Theater (then the Hilton Theatre), New York, on October 11, 2007. It earned mixed reviews from the critics.

Brooks joked about the concept of a musical adaptation of "Blazing Saddles" in the final number in "Young Frankenstein", in which the full company sings, "next year, "Blazing Saddles"!" In 2010, Mel Brooks confirmed this, saying that the musical could be finished within a year. No creative team or plan has been announced.

Brooks is one of the few people who have received an Oscar, an Emmy, a Tony, and a Grammy. He was awarded his first Grammy for Best Spoken Comedy Album in 1999 for his recording of "The 2000 Year Old Man in the Year 2000" with Carl Reiner. His two other Grammys came in 2002 for Best Musical Show Album for the cast album of "The Producers" and for Best Long Form Music Video for the DVD "Recording the Producers – A Musical Romp with Mel Brooks". He won his first of four Emmy awards in 1967 for Outstanding Writing Achievement in Variety for a Sid Caesar special and went on to win three consecutive Emmys in 1997, 1998, and 1999 for Outstanding Guest Actor in a Comedy Series for his role of Uncle Phil on "Mad About You". Brooks won his Academy Award for Original Screenplay (Oscar) in 1968 for "The Producers". He won his three Tony awards in 2001 for his work on the musical, "The Producers" for Best Musical, Best Original Musical Score, and Best Book of a Musical.

Brooks won a Hugo Award and a Nebula Award for "Young Frankenstein". In a 2005 poll by Channel 4 to find "The Comedian's Comedian", he was voted No. 50 of the top 50 comedy acts ever by fellow comedians and comedy insiders.

The American Film Institute (AFI) list three of Brooks' films on their AFI's 100 Years...100 Laughs list: "Blazing Saddles" (#6), "The Producers" (#11), and "Young Frankenstein" (#13).

On December 5, 2009, Brooks was one of five recipients of the 2009 Kennedy Center Honors at the John F. Kennedy Center for the Performing Arts in Washington, DC. He was inducted into the Hollywood Walk of Fame on April 23, 2010 with a motion pictures star located at 6712 Hollywood Boulevard. American Masters produced a biography on Brooks which premiered May 20, 2013, on PBS. The AFI presented Brooks with its highest tribute, the AFI Life Achievement Award, in June 2013. In 2014 Brooks was honored in a handprint and footprint ceremony at TCL Chinese Theatre. His concrete handprints include a six-fingered left hand as he wore a prosthetic finger when making his prints. On March 20, 2015, Brooks was awarded a British Film Institute Fellowship from the British Film Institute.

Brooks was married to Florence Baum (1926-2008) from 1953-1962, their marriage ending in divorce. They had three children: Stephanie, Nicky, and Eddie. Brooks married stage, film and television actress Anne Bancroft in 1964, and they remained together until her death in 2005. They had met at a rehearsal for the "Perry Como Variety Show" in 1961, and were married three years later on August 5, 1964, at the Manhattan Marriage Bureau. Their son, Max Brooks, was born in 1972, and their grandson, Henry Michael Brooks, was born in 2005.

In 2010, Brooks credited Bancroft with having been "the guiding force" behind his involvement in developing "The Producers" and "Young Frankenstein" for the musical theater, saying of an early meeting with her: "From that day, until her death…we were glued together."

Regarding religion, Brooks stated, "I'm rather secular. I'm basically Jewish. But I think I'm Jewish not because of the Jewish religion at all. I think it's the relationship with the people and the pride I have. The tribe surviving so many misfortunes, and being so brave and contributing so much knowledge to the world and showing courage."

Brooks' great-nephew from his brother Lenny, Todd Kaminsky, is a New York state senator for state senate district 9 on Long Island and formerly represented Long Island's state assembly district 20 in the New York State Assembly.




</doc>
<doc id="20219" url="https://en.wikipedia.org/wiki?curid=20219" title="Mycoplasma genitalium">
Mycoplasma genitalium

Mycoplasma genitalium (MG, commonly known as Mgen), is a sexually transmitted, small and pathogenic bacterium that lives on the skin cells of the urinary and genital tracts in humans. Mgen is becoming increasingly common. Resistance to multiple antibiotics is occurring, including azithromycin which until recently was the most reliable treatment. The bacteria was first isolated from urogenital tract of humans in 1981, and was eventually identified as a new species of "Mycoplasma" in 1983. It can cause negative health effects in men and women. It also increases the risk factor for HIV spread with higher occurrences in homosexual men and those previously treated with the azithromycin antibiotics.

Specifically, it causes urethritis in both men and women, and also cervicitis and pelvic inflammation in women. It presents clinically similar symptoms to that of "Chlamydia trachomatis" infection and has shown higher incidence rates, compared to both "Chlamydia trachomatis" and "Neisseria gonorrhoeae" infections in some populations. Its complete genome sequence was published in 1995 (size 0.58 Mbp, with 475 genes). It was regarded as a cellular unit with the smallest genome size (in Mbp) until 2003 when a new species of Archaea, namely "Nanoarchaeum equitans", was sequenced (0.49 Mbp, with 540 genes). However, Mgen still has the smallest genome of any known (naturally occurring) self-replicating organism and thus is often the organism of choice in minimal genome research.

The synthetic genome of Mgen named "Mycoplasma genitalium" JCVI-1.0 (after the research centre, J. Craig Venter Institute, where it was synthesised) was produced in 2008, becoming the first organism with a synthetic genome. In 2014, a protein was described called Protein M from "M. genitalium".

Infection with Mgen produces a combination of clinical symptoms, but can be asymptomatic. It causes inflammation in the urethra (urethritis) both in men and women, which is associated with mucopurulent discharge in the urinary tract, and burning while urinating. In women, it causes cervicitis and pelvic inflammatory diseases (PID), including endometritis and salpingitis. Women may also experience bleeding after sex and it is also linked with tubal factor infertility. For men, the most common signs are painful urination or a watery discharge from the penis. Polymerase chain reaction analyses indicated that it is a cause of acute non-gonococcal urethritis (NGU) and probably chronic NGU. It is strongly associated with persistent and recurring non-gonococcal urethritis (NGU) responsible for 15 percent to 20 percent of symptomatic NGU cases in men. Unlike other "Mycoplasma", the infection is not associated with bacterial vaginosis. It is highly associated with the intensity of HIV infection. Some scientists are doing research to see if Mgen could play a role in the development of prostate and ovarian cancers and lymphomas in some individuals. These studies have yet to find conclusive evidence to suggest a link.

The genome of "M. genitalium" consists of 525 genes in one circular DNA of 580,070 base pairs. Scott N. Peterson and his team at the University of North Carolina at Chapel Hill reported the first genetic map using pulsed-field gel electrophoresis in 1991. They performed an initial study of the genome using random sequencing in 1993, by which they found 100,993 nucleotides and 390 protein-coding genes. Collaborating with researchers at the Institute for Genomic Research, which included Craig Venter, they made the complete genome sequence in 1995 using shotgun sequencing. Only 470 predicted coding regions (out of 482 protein encoding genes) were identified, including genes required for DNA replication, transcription and translation, DNA repair, cellular transport, and energy metabolism. It was the second complete bacterial genome ever sequenced, after "Haemophilus influenzae". In 2006, the team at the J. Craig Venter Institute reported that only 382 genes are essential for biological functions. The small genome of "M. genitalium" made it the organism of choice in The Minimal Genome Project, a study to find the smallest set of genetic material necessary to sustain life.

There is a consistent association of "M. genitalium" infection and female reproductive tract syndromes. "M. genitalium" infection was significantly associated with increased risk of preterm birth, spontaneous abortion, cervicitis, and pelvic inflammatory disease. Infertility risk is also strongly associated with infection with "M. genitalium", although evidence suggests it is not associated with male infertility. When "M. genitalium" is a co-infectious agent risk associations are stronger and statistically significant. "M. genitalium" is strongly associated with HIV-1.

Recent research shows that prevalence of Mgen is currently higher than other commonly occurring STIs (Sexually Transmitted Infections). Mgen is a fastidious organism with prolonged growth durations. This makes detection of the pathogen in clinical specimens and subsequent isolation, extremely difficult. Lacking a cell wall, mycoplasma remains unaffected by commonly used antibiotics. The absence of specific serological assays leaves nucleic acid amplification tests (NAAT) as the only viable option for detection of Mgen DNA or RNA. However, samples with positive NAAT for the pathogen should be tested for macrolide resistance mutations, which are strongly correlated to azithromycin treatment failures, owing to rapid rates of mutation of the pathogen. Mutations in the 23S rRNA gene of Mgen have been linked with clinical treatment failure and high level in vitro macrolide resistance. Macrolide resistance mediating mutations have been observed in 20-50% of cases in the UK, Denmark, Sweden, Australia, and Japan. Resistance is also developing towards the second-line antimicrobials like fluoroquinolone.

According to the European guidelines, the indication for commencement of diagnosis for Mgen infection are:


Screening for Mgen with a combination of detection and macrolide resistance mutations will provide the adequate information required to develop personalised antimicrobial treatments, in order to optimise patient management and control the spread of antimicrobial resistance (AMR).

Owing to the widespread macrolide resistance, samples that are positive for Mgen should ideally be followed up with an assay capable of detecting mutations that mediate antimicrobial resistance. The European Guideline on Mgen infections, in 2016, recommended complementing the molecular detection of Mgen with an assay capable of detecting macrolide resistance-associated mutations.

The U.S. Centers for Disease Control and Prevention has one specific recommended regimen with azithromycin and another specific recommended regimen with doxycycline. As alternative regimens, the agency has specific regimens each with erythromycin or erythromycin ethylsuccinate or ofloxacin or levofloxacin.

Treatment of "Mycoplasma genitalium" infections is becoming increasingly difficult due to rapidly growing antimicrobial resistance. Diagnosis and treatment is further hampered by the fact that "Mycoplasma genitalium" infections are not routinely tested. Studies have demonstrated that a 5-day course of azithromycin has a superior cure rate compared to a single, larger dose. Further, a single dose of azithromycin can lead to the bacteria becoming resistant to azithromycin. Among Swedish patients, doxycycline was shown to be relatively ineffective (with a cure rate of 48% for women and 38% for men); and treatment with a single dose of azithromycin is not prescribed due to it inducing antimicrobial resistance. The five-day treatment with azithromycin showed no development of antimicrobial resistance. Based on these findings, UK doctors are moving to the 5-day azithromycin regimen. Doxycycline is also still used, and moxifloxacin is used as a second-line treatment in case doxycyline and azithromycin are not able to eradicate the infection.
In patients where doxycycline, azithromycin and moxifloxacin all failed, pristinamycin has been shown to still be able to eradicate the infection.

"Mycoplasma genitalium" was originally isolated in 1980 from urethral specimens of two male patients suffering from non-gonococcal urethritis in the genitourinary medicine (GUM) clinic at St Mary's Hospital, Paddington, London. It was reported in 1981 by a team led by Joseph G. Tully. Under electron microscopy, it appears as a flask-shaped cell with a narrow terminal portion that is crucial for its attachment to the host cell surfaces. The bacterial cell is slightly elongated somewhat like a vase, and measures 0.6-0.7 μm in length, 0.3-0.4 μm at the broadest region, and 0.06-0.08 μm at the tip. The base is broad while the tip is stretched into a narrow neck, which terminates with a cap. The terminal region has a specialised region called nap, which is absent in other "Mycoplasma". Serological tests indicated that the bacterium was not related to known species of "Mycoplasma". The comparison of genome sequences with other urinogenital bacteria, such as "M. hominis" and "Ureaplasma parvum", revealed that "M. genitalium" is significantly different, especially in the energy-generating pathways, although it shared a core genome of ~250 protein-encoding genes.

On 6 October 2007, Craig Venter announced that a team of scientists led by Nobel laureate Hamilton Smith at the J. Craig Venter Institute had successfully constructed synthetic DNA with which they planned to make the first synthetic genome. Reporting in "The Guardian", Venter said that they had stitched together a DNA strand containing 381 genes, consisting of 580,000 base pairs, based on the genome of "M. genitalium". On 24 January 2008, they announced the successful creation of a synthetic bacterium, which they named "Mycoplasma genitalium" JCVI-1.0 (the name of the strain indicating J. Craig Venter Institute with its specimen number). They synthesised and assembled the complete 582,970-base pair genome of the bacterium. The final stages of synthesis involved cloning the DNA into the bacterium "E. coli" for nucleotide production and sequencing. This produced large fragments of approximately 144,000 base pairs or 1/4th of the whole genome. Finally, the products were cloned inside the yeast "Saccharomyces cerevisiae" to synthesize the 580,000 base pairs. The molecular size of the synthetic bacterial genome is 360,110 kilodaltons (kDa). Printed in 10-point font, the letters of the genome cover 147 pages.

On 20 July 2012, Stanford University and the J. Craig Venter Institute announced successful simulation of the complete life cycle of a "Mycoplasma genitalium" cell, in the journal "Cell". The entire organism is modeled in terms of its molecular components, integrating all cellular processes into a single model. Using object oriented programming to model the interactions of 28 categories of molecules, including DNA, RNA, proteins, and metabolites, and running on a 128-core Linux cluster, the simulation takes 10 hours for a single "M. genitalium" cell to divide once — about the same time the actual cell takes — and generates half a gigabyte of data.

The discovery of Protein M, a new protein from "M. genitalium", was announced in February 2014. The protein was identified during investigations on the origin of multiple myeloma, a B-cell hematologic neoplasm. To understand the long-term "Mycoplasma" infection, it was found that antibodies from multiple myeloma patients' blood were recognised by "M. genitalium". The antibody reactivity was due to a protein never known before, and is chemically responsive to all types of human and nonhuman antibodies available. The protein is about 50 kDa in size, and composed of 556 amino acids.

Future research must focus on the development of novel antimicrobials and treatment algorithms that emphasize on dual antimicrobial therapy and AMR testing in treatment protocols. Importantly, most patients with MG are treated syndromically and this treatment is even more compromised by the emerging resistances to several antimicrobials. This also stresses the importance of evidence-based knowledge regarding the activity of novel antimicrobials against several pathogens that cause STIs. The rapid development of AMR in Mgen suggests that single-dose antimicrobial monotherapy may be inappropriate even for uncomplicated STIs. For Mgen, antimicrobial combination therapy and AMR testing, in conjunction with the development and evaluation of new classes of antimicrobials, are of utmost importance. Some of the novel antimicrobials, particularly the fluoroketolide solithromycin, might at least temporarily replace azithromycin in the treatment of Mgen. Ultimately, the only sustainable solution to control these infections might be the development of vaccines, a task that remains to be incredibly difficult with most pathogens of commonly occurring STIs, being unculturable.



</doc>
<doc id="20221" url="https://en.wikipedia.org/wiki?curid=20221" title="Mehmet Ali Ağca">
Mehmet Ali Ağca

Mehmet Ali Ağca (; born 9 January 1958) is a Turkish assassin and Grey Wolves member who murdered left-wing journalist Abdi İpekçi on 1 February 1979, and later shot and wounded Pope John Paul II on 13 May 1981, after escaping from a Turkish prison. After serving 19 years of imprisonment in Italy where he was visited by the Pope, he was deported to Turkey, where he served a ten-year sentence. In 2007, he converted to Roman Catholicism and was released from jail on 18 January 2010. Ağca has described himself as a mercenary with no political orientation, although he is known to have been a member of the Turkish ultra-nationalist Grey Wolves organization and the state-sponsored Counter-Guerrilla.

On 27 December 2014, 33 years after his crime, Mehmet Ali Ağca publicly arrived at the Vatican to lay white roses on the recently canonized John Paul II's tomb and said he wanted to meet Pope Francis – a request that was denied.

Ağca was born in the Hekimhan district, Malatya Province in Turkey. As a youth, he became a petty criminal and a member of numerous street gangs in his hometown. He became a smuggler between Turkey and Bulgaria. He claims to have received two months of training in weaponry and terrorist tactics in Syria as a member of the Marxist Popular Front for the Liberation of Palestine paid for by the Communist Bulgarian government, although the PFLP has denied this.

After training he went to work for the ultranationalist Turkish Grey Wolves.

On 1 February 1979, in Istanbul, under orders from the Grey Wolves, he murdered Abdi İpekçi, editor of the major Turkish newspaper "Milliyet". After being denounced by an informant, he was caught and sentenced to life in prison. After serving six months, he escaped with the help of Abdullah Çatlı, second-in-command of the Grey Wolves, and fled to Bulgaria, which was a base of operation for the Turkish mafia. According to investigative journalist Lucy Komisar, Mehmet Ali Ağca had worked with Abdullah Çatlı in the 1979 assassination, who "then reportedly helped organize Ağca's escape from an Istanbul military prison, and some have suggested Çatlı was even involved in the Pope's assassination attempt". According to Reuters, Ağca had "escaped with suspected help from sympathizers in the security services". Lucy Komisar added that at the scene of the Mercedes-Benz crash where Çatlı died, he was found with a passport under the name of "Mehmet Özbay" — an alias also used by Mehmet Ali Ağca.

 Beginning in August 1980, Ağca began criss-crossing the Mediterranean region.

According to Ağca's later testimony, he met with three accomplices in Rome, one a fellow Turk and the other two Bulgarians. The operation was commanded by Zilo Vassilev, the Bulgarian military attaché in Italy. He said that he was assigned this mission by Turkish mafioso Bekir Çelenk in Bulgaria. "Le Monde diplomatique", however, has alleged that the assassination attempt was organized by Abdullah Çatlı "in exchange for the sum of 3 million marks", paid by Bekir Çelenk to the Grey Wolves.

According to Ağca, the plan was for him and the back-up gunman Oral Çelik to open fire in St. Peter's Square and escape to the Bulgarian embassy under the cover of the panic generated by a small explosion. On 13 May they sat in the square, writing postcards and waiting for the Pope to arrive. When the Pope passed them, Ağca fired several shots and wounded him, but was grabbed by spectators and Vatican security chief Camillo Cibin. This prevented him from finishing the assassination or escaping. Four bullets hit John Paul II, two of them lodging in his lower intestine, the others hitting his left hand and right arm. Two bystanders were also hit. Çelik panicked and fled without setting off his bomb or opening fire. The Pope survived the assassination attempt.

Ağca was sentenced in July 1981 to life imprisonment in Italy for the assassination attempt. Following his shooting, Pope John Paul II asked people to "pray for my brother (Ağca), whom I have sincerely forgiven." In 1983, the pope and Ağca met and spoke privately at the prison where Ağca was being held. The Pope was also in touch with Ağca's family over the years, meeting his mother in 1987 and his brother a decade later. After serving almost 20 years of a life sentence in prison in Italy, at the request of Pope John Paul II, Ağca was pardoned by the then Italian president Carlo Azeglio Ciampi in June 2000 and deported to Turkey.

Following his extradition to Turkey, he was imprisoned for the 1979 murder of Abdi İpekçi and for two bank raids carried out in the 1970s. Ağca was arrested on June 25 and incarcerated in the Maltepe Military Prison. He fled to Bulgaria on 25 November and was sentenced to death "in absentia". Ağca was extradited to Turkey in 2000 by benefiting from the Conditional Amnesty Law. This consideration granted to the ex-convict elicited strong reactions. Both cases about Ağca were merged and tried before the Kadıköy 1st High Criminal Court. The single trial concerned the hijacking of Cengiz Aydos's taxi in 1979, robbing the Yıldırım jewelry store in Kızıltoprak on 22 March 1979 and stealing money from the Fruko soda storage on 4 April 1979.

On 9 June 1997, Air Malta Flight 830 was hijacked by two men. After landing in Cologne, the hijackers demanded the release of Ağca, who at the time was serving a life sentence in Italy for trying to assassinate Pope John Paul II in 1981. Ağca was not released and the hijackers surrendered.

On 18 January 2000, the judges dismissed the charges because of the statute of limitation on the case filed for the jewelry store robbery and for "breach of the Firearms Act" (law no. 6136). For embezzlement and money theft Ağca was sentenced to 36 years of imprisonment. Ağca's lawyers applied for their client's release under Law no. 4516 on Parole and Deferral of Penalties in December 2000. Their request was denied by the 1st High Criminal Court of Kartal. The lawyers filed an appeal against this decision, but the appeals court upheld the ruling. Ağca's life sentence was reduced to 10 years in prison for murder under a Turkish law that shortened prison sentences if served in a foreign prison. The money-laundering conviction and 36-year sentence were overturned because of the statute of limitations for robbery, which was 7 years under Turkish law.

In early February 2005, during the Pope's illness, Ağca sent a letter to the Pope wishing him well and also warning him that the world would end soon. When the Pope died on 2 April 2005, Ağca's brother Adnan gave an interview in which he said that Ağca and his entire family were grieving, and that the Pope had been a great friend to them.

Ağca was released on parole on 12 January 2006. Mustafa Demirbağ, his lawyer, explained his release as a combination of amnesty and penal reform: an amnesty in 2000 deducted 10 years from his time, the court then deducted his 20 years in the Italian prison based on a new article in the penal code, and so he became eligible for parole for good behavior. However, a report from the French AFP news agency stated that "The Turkish judicial authorities still haven't explained exactly which legal resources he had access to", and former minister of Justice Hikmet Sami Türk, in government at the time of Ağca's extradition, claimed that, from a legal viewpoint, his release was a "serious mistake" at best, and that he should have not been freed before 2012. However, on 20 January 2006, the Turkish Supreme Court ruled that his time served in Italy could not be deducted from his Turkish sentence and he was again imprisoned.

On 2 May 2008, Ağca asked to be awarded Polish citizenship as he wished to spend the final years of his life in Poland, Pope John Paul II's country of birth. Ağca stated that upon his release he wanted to visit Pope John Paul II's tomb and partner with Dan Brown on writing a book.

Ağca was released from jail on January 18, 2010. He was transferred to a military hospital in order to assess if, at 52, he was still fit for compulsory military service. The military found him unfit for military service for having "antisocial personality disorder". In a statement, he announced: "I will meet you in the next three days. In the name of God Almighty, I proclaim the end of the world in this century. All the world will be destroyed, every human being will die. I am not God, I am not son of God, I am Christ eternal."

The former assassin visited the tomb of John Paul II on 27 December 2014.

Ağca manifested a desire to become a Catholic priest in 2016 and go to Fatima, Portugal to celebrate the 100th anniversary of the Marian apparitions there (Our Lady of Fátima).

In November 2010, he accused Cardinal Agostino Casaroli as the mastermind behind the assassination attempt on John Paul II in 1981.

It has also been alleged that the Soviet Union's KGB ordered the assassination, because of John Paul II's support for the Solidarity labor movement in Poland. Ağca stated this during one of his interrogations before trial.

However, when Ağca published his memoirs in 2013, his story changed completely, writing that the Iranian government and Ayatollah Khomeini ordered the assassination attempt on John Paul II.

According to this new version of the events, Ağca received instructions and training in weapons and explosives in Iran, from Mohsen Rezai, under the orders of Ayatollah Jaffar Subhani and Ayatollah Khomeiny. In his book, Ağca acknowledges that he lied previously about the Bulgarian and Soviet connection. He stayed in Sofia for about a month, but he was not in contact with any Bulgarian or other intelligence officers, he was in transit from Turkey to Western Europe, and was delayed in Sofia because his fake Indian passport was of such poor quality that on several occasions he had to bribe officials who became suspicious. So, he waited to receive a much better quality Turkish passport from the Grey Wolves: a genuine passport issued by the Turkish government to another person, Faruk Faruk Özgün, only the photo of Özgün was replaced by a photo of Ağca.

When Pope John Paul II visited him in prison in Italy, on 27 December 1983 (two and a half years after the assassination attempt), Ağca recalls in his memoirs he kissed the hand of the pope, having kissed three years earlier the hand of Khomeiny in Iran, and when asked, he told John Paul II “Khomeiny and the Iranian government gave me the order to kill you”.

Ağca's shooting of the Pope and possible KGB involvement is featured in Tom Clancy's 2002 novel, "Red Rabbit", and Frederick Forsyth's novel, "The Fourth Protocol". He has also been mentioned in the book, "The Third Revelation", by Ralph McInerny,
and was portrayed by actors Sebastian Knapp in the ABC TV biopic movie "", Massimiliano Ubaldi in CBS' TV miniseries "Pope John Paul II" (both 2005) and Alkis Zanis in the 2006 Canadian TV sequel "".



 


</doc>
<doc id="20223" url="https://en.wikipedia.org/wiki?curid=20223" title="March 17">
March 17





</doc>
<doc id="20224" url="https://en.wikipedia.org/wiki?curid=20224" title="Mummy">
Mummy

A mummy is a dead human or an animal whose skin and organs have been preserved by either intentional or accidental exposure to chemicals, extreme cold, very low humidity, or lack of air, so that the recovered body does not decay further if kept in cool and dry conditions. Some authorities restrict the use of the term to bodies deliberately embalmed with chemicals, but the use of the word to cover accidentally desiccated bodies goes back to at least 1615 AD (see the section Etymology and meaning).

Mummies of humans and animals have been found on every continent, both as a result of natural preservation through unusual conditions, and as cultural artifacts. Over one million animal mummies have been found in Egypt, many of which are cats. Many of the Egyptian animal mummies are sacred ibis, and radiocarbon dating suggests the Egyptian Ibis mummies that have been analyzed were from time frame that falls between approximately 450 and 250 BC.

In addition to the mummies of ancient Egypt, deliberate mummification was a feature of several ancient cultures in areas of America and Asia with very dry climates. The Spirit Cave mummies of Fallon, Nevada in North America were accurately dated at more than 9,400 years old. Before this discovery, the oldest known deliberate mummy was a child, one of the Chinchorro mummies found in the Camarones Valley, Chile, which dates around 5050 BC. The oldest known naturally mummified human corpse is a severed head dated as 6,000 years old, found in 1936 AD at the site named Inca Cueva No. 4 in South America.

The English word "mummy" is derived from medieval Latin "mumia", a borrowing of the medieval Arabic word "mūmiya" (مومياء) and from a Persian word "mūm" (wax), which meant an embalmed corpse, and as well as the bituminous embalming substance, and also meant "bitumen". The Medieval English term "mummy" was defined as "medical preparation of the substance of mummies", rather than the entire corpse, with Richard Hakluyt in 1599 AD complaining that "these dead bodies are the Mummy which the Phisistians and Apothecaries doe against our willes make us to swallow". These substances were defined as mummia.

The OED defines a mummy as "the body of a human being or animal embalmed (according to the ancient Egyptian or some analogous method) as a preparation for burial", citing sources from 1615 AD onward. However, Chamber's "Cyclopædia" and the Victorian zoologist Francis Trevelyan Buckland define a mummy as follows: "A human or animal body desiccated by exposure to sun or air. Also applied to the frozen carcase of an animal imbedded in prehistoric snow".

Wasps of the genus "Aleiodes" are known as "mummy wasps" because they wrap their caterpillar prey as "mummies".

While interest in the study of mummies dates as far back as Ptolemaic Greece, most structured scientific study began at the beginning of the 20th century. Prior to this, many rediscovered mummies were sold as curiosities or for use in pseudoscientific novelties such as mummia. The first modern scientific examinations of mummies began in 1901, conducted by professors at the English-language Government School of Medicine in Cairo, Egypt. The first X-ray of a mummy came in 1903, when professors Grafton Elliot Smith and Howard Carter used the only X-ray machine in Cairo at the time to examine the mummified body of Thutmose IV. British chemist Alfred Lucas applied chemical analyses to Egyptian mummies during this same period, which returned many results about the types of substances used in embalming. Lucas also made significant contributions to the analysis of Tutankhamun in 1922.

Pathological study of mummies saw varying levels of popularity throughout the 20th century. In 1992, the First World Congress on Mummy Studies was held in Puerto de la Cruz on Tenerife in the Canary Islands. More than 300 scientists attended the Congress to share nearly 100 years of collected data on mummies. The information presented at the meeting triggered a new surge of interest in the subject, with one of the major results being integration of biomedical and bioarchaeological information on mummies with existing databases. This was not possible prior to the Congress due to the unique and highly specialized techniques required to gather such data.

In more recent years, CT scanning has become an invaluable tool in the study of mummification by allowing researchers to digitally "unwrap" mummies without risking damage to the body. The level of detail in such scans is so intricate that small linens used in tiny areas such as the nostrils can be digitally reconstructed in 3-D. Such modelling has been utilized to perform digital autopsies on mummies to determine cause of death and lifestyle, such as in the case of Tutankhamun.

Mummies are typically divided into one of two distinct categories: anthropogenic or spontaneous. Anthropogenic mummies were deliberately created by the living for any number of reasons, the most common being for religious purposes. Spontaneous mummies, such as Ötzi, were created unintentionally due to natural conditions such as extremely dry heat or cold, or anaerobic conditions such as those found in bogs. While most individual mummies exclusively belong to one category or the other, there are examples of both types being connected to a single culture, such as those from the ancient Egyptian culture and the Andean cultures of South America.

The earliest ancient Egyptian mummies were created naturally due to the environment in which they were buried. In the era prior to 3500 BC, Egyptians buried the dead in pit graves, without regard to social status. Pit graves were often shallow. This characteristic allowed for the hot, dry sand of the desert to dehydrate the bodies, leading to natural mummification.

The natural preservation of the dead had a profound effect on ancient Egyptian religion. Deliberate mummification became an integral part of the rituals for the dead beginning as early as the 2nd dynasty (about 2800 BC). New research of an 11-year study by University of York, Macquarie University and University of Oxford suggests mummification occurred 1,500 years earlier than first thought. Egyptians saw the preservation of the body after death as an important step to living well in the afterlife. As Egypt gained more prosperity, burial practices became a status symbol for the wealthy as well. This cultural hierarchy lead to the creation of elaborate tombs, and more sophisticated methods of embalming.
By the 4th dynasty (about 2600 BC) Egyptian embalmers began to achieve "true mummification" through a process of evisceration, followed by preserving the body in various minerals and oils. Much of this early experimentation with mummification in Egypt is unknown.

The few documents that directly describe the mummification process date to the Greco-Roman period. The majority of the papyri that have survived only describe the ceremonial rituals involved in embalming, not the actual surgical processes involved. A text known as "The Ritual of Embalming" does describe some of the practical logistics of embalming, however, there are only two known copies and each is incomplete. With regards to mummification shown in images, there are apparently also very few. The tomb of Tjay designated TT23, is one of only two known which show the wrapping of a mummy (Riggs 2014).

Another text that describes the processes being used in latter periods is Herodotus' Histories. Written in Book 2 of the "Histories" is one of the most detailed descriptions of the Egyptian mummification process, including the mention of using natron in order to dehydrate corpses for preservation. However, these descriptions are short and fairly vague, leaving scholars to infer the majority of the techniques that were used by studying mummies that have been unearthed.

By utilizing current advancements in technology, scientists have been able to uncover a plethora of new information about the techniques used in mummification. A series of CT scans performed on a 2,400-year-old mummy in 2008 revealed a tool that was left inside the cranial cavity of the skull. The tool was a rod, made of an organic material, that was used to break apart the brain to allow it to drain out of the nose. This discovery helped to dispel the claim within Herodotus' works that the rod had been a hook made of iron. Earlier experimentation in 1994 by researchers Bob Brier and Ronald Wade supported these findings. While attempting to replicate Egyptian mummification, Brier and Wade discovered that removal of the brain was much easier when the brain was liquefied and allowed to drain with the help of gravity, as opposed to trying to pull the organ out piece-by-piece with a hook.

Through various methods of study over many decades, modern Egyptologists now have an accurate understanding of how mummification was achieved in ancient Egypt. The first and most important step was to halt the process of decomposition, by removing the internal organs and washing out the body with a mix of spices and palm wine. The only organ left behind was the heart, as tradition held the heart was the seat of thought and feeling and would therefore still be needed in the afterlife. After cleansing, the body was then dried out with natron inside the empty body cavity as well as outside on the skin. The internal organs were also dried and either sealed in individual jars, or wrapped to be replaced within the body. This process typically took forty days.

After dehydration, the mummy was wrapped in many layers of linen cloth. Within the layers, Egyptian priests placed small amulets to guard the decedent from evil. Once the mummy was completely wrapped, it was coated in a resin in order to keep the threat of moist air away. Resin was also applied to the coffin in order to seal it. The mummy was then sealed within its tomb, alongside the worldly goods that were believed to help aid it in the afterlife.

Aspergillus niger has been found in the mummies of ancient Egyptian tombs and can be inhaled when they are disturbed.

Mummification is one of the defining customs in ancient Egyptian society for people today. The practice of preserving the human body is believed to be a quintessential feature of Egyptian life. Yet even mummification has a history of development and was accessible to different ranks of society in different ways during different periods. There were at least three different processes of mummification according to Herodotus. They range from "the most perfect" to the method employed by the "poorer classes".

The most expensive process was to preserve the body by dehydration and protect against pests, such as insects. Almost all of the actions Herodotus described serve one of these two functions.

First, the brain was removed from the cranium through the nose; the gray matter was discarded. Modern mummy excavations have shown that instead of an iron hook inserted through the nose as Herodotus claims, a rod was used to liquefy the brain via the cranium, which then drained out the nose by gravity. The embalmers then rinsed the skull with certain drugs that mostly cleared any residue of brain tissue and also had the effect of killing bacteria. Next, the embalmers made an incision along the flank with a sharp blade fashioned from an Ethiopian stone and removed the contents of the abdomen. Herodotus does not discuss the separate preservation of these organs and their placement either in special jars or back in the cavity, a process that was part of the most expensive embalming, according to archaeological evidence.

The abdominal cavity was then rinsed with palm wine and an infusion of crushed, fragrant herbs and spices; the cavity was then filled with spices including myrrh, cassia, and, Herodotus notes, "every other sort of spice except frankincense", also to preserve the person.

The body was further dehydrated by placing it in natron, a naturally occurring salt, for seventy days. Herodotus insists that the body did not stay in the natron longer than seventy days. Any shorter time and the body is not completely dehydrated; any longer, and the body is too stiff to move into position for wrapping. The embalmers then wash the body again and wrapped it with linen bandages. The bandages were covered with a gum that modern research has shown is both waterproofing agent and an antimicrobial agent.

At this point, the body was given back to the family. These "perfect" mummies were then placed in wooden cases that were human-shaped. Richer people placed these wooden cases in stone sarcophagi that provided further protection. The family placed the sarcophagus in the tomb upright against the wall, according to Herodotus.

The second process that Herodotus describes was used by middle-class people or people who "wish to avoid expense". In this method, an oil derived from cedar trees was injected with a syringe into the abdomen. A rectal plug prevented the oil from escaping. This oil probably had the dual purpose of liquefying the internal organs but also of disinfecting the abdominal cavity. (By liquefying the organs, the family avoided the expense of canopic jars and separate preservation.) The body was then placed in natron for seventy days. At the end of this time, the body was removed and the cedar oil, now containing the liquefied organs, was drained through the rectum. With the body dehydrated, it could be returned to the family. Herodotus does not describe the process of burial of such mummies, but they were perhaps placed in a shaft tomb. Poorer people used coffins fashioned from terracotta.

The third and least-expensive method the embalmers offered was to clear the intestines with an unnamed liquid, injected as an enema. The body was then placed in natron for seventy days and returned to the family. Herodotus gives no further details.

In Christian tradition, some bodies of saints are naturally conserved and venerated.

In addition to the mummies of Egypt, there have been instances of mummies being discovered in other areas of the African continent. The bodies show a mix of anthropogenic and spontaneous mummification, with some being thousands of years old.

The mummified remains of an infant were discovered during an expedition by archaeologist Fabrizio Mori to Libya during the winter of 1958–1959 in the natural cave structure of Uan Muhuggiag. After curious deposits and cave paintings were discovered on the surfaces of the cave, expedition leaders decided to excavate. Uncovered alongside fragmented animal bone tools was the mummified body of an infant, wrapped in animal skin and wearing a necklace made of ostrich egg shell beads. Professor Tongiorgi of the University of Pisa radiocarbon-dated the infant to between 5,000–8,000 years old. A long incision located on the right abdominal wall, and the absence of internal organs, indicated that the body had been eviscerated post-mortem, possibly in an effort to preserve the remains. A bundle of herbs found within the body cavity also supported this conclusion. Further research revealed that the child had been around 30 months old at the time of death, though gender could not be determined due to poor preservation of the sex organs.

The first mummy to be discovered in South Africa was found in the Baviaanskloof Wilderness Area by Dr. Johan Binneman in 1999. Nicknamed Moses, the mummy was estimated to be around 2,000 years old. After being linked to the indigenous Khoi culture of the region, the National Council of Khoi Chiefs of South Africa began to make legal demands that the mummy be returned shortly after the body was moved to the Albany Museum in Grahamstown.

The mummies of Asia are usually considered to be accidental. The decedents were buried in just the right place where the environment could act as an agent for preservation. This is particularly common in the desert areas of the Tarim Basin and Iran. Mummies have been discovered in more humid Asian climates, however these are subject to rapid decay after being removed from the grave.

Mummies from various dynasties throughout China's history have been discovered in several locations across the country. They are almost exclusively considered to be unintentional mummifications. Many areas in which mummies have been uncovered are difficult for preservation, due to their warm, moist climates. This makes the recovery of mummies a challenge, as exposure to the outside world can cause the bodies to decay in a matter of hours.

An example of a Chinese mummy that was preserved despite being buried in an environment not conducive to mummification is Xin Zhui. Also known as Lady Dai, she was discovered in the early 1970s at the Mawangdui archaeological site in Changsha. She was the wife of the marquis of Dai during the Han dynasty, who was also buried with her alongside another young man often considered to be a very close relative. However, Xin Zhui's body was the only one of the three to be mummified. Her corpse was so well-preserved that surgeons from the Hunan Provincial Medical Institute were able to perform an autopsy. The exact reason why her body was so completely preserved has yet to be determined.

Among the mummies discovered in China are those termed Tarim mummies because of their discovery in the Tarim Basin. The dry desert climate of the basin proved to be an excellent agent for desiccation. For this reason, over 200 Tarim mummies, which are over 4,000 years old, were excavated from a cemetery in the present-day Xinjiang region. The mummies were found buried in upside-down boats with hundreds of 13-foot-long wooden poles in the place of tombstones. DNA sequence data shows that the mummies had Haplogroup R1a (Y-DNA) characteristic of western Eurasia in the area of East-Central Europe, Central Asia and Indus Valley. This has created a stir in the Turkic-speaking Uighur population of the region, who claim the area has always belonged to their culture, while it was not until the 10th century when the Uighurs are said by scholars to have moved to the region from Central Asia. American Sinologist Victor H. Mair claims that ""the earliest mummies in the Tarim Basin were exclusively Caucasoid, or Europoid"" with "east Asian migrants arriving in the eastern portions of the Tarim Basin around 3,000 years ago", while Mair also notes that it was not until 842 that the Uighur peoples settled in the area. Other mummified remains have been recovered from around the Tarim Basin at sites including Qäwrighul, Yanghai, Shengjindian, Shanpula (Sampul), Zaghunluq, and Qizilchoqa.

As of 2012, at least eight mummified human remains have been recovered from the Douzlakh Salt Mine at Chehr Abad in northwestern Iran. Due to their salt preservation, these bodies are collectively known as Saltmen. Carbon-14 testing conducted in 2008 dated three of the bodies to around 400 BC. Later isotopic research on the other mummies returned similar dates, however, many of these individuals were found to be from a region that is not closely associated with the mine. It was during this time that researchers determined the mine suffered a major collapse, which likely caused the death of the miners. Since there is significant archaeological data that indicates the area was not actively inhabited during this time period, current consensus holds that the accident occurred during a brief period of temporary mining activity.

In 1993, a team of Russian archaeologists led by Dr. Natalia Polosmak discovered the Siberian Ice Maiden, a Scytho-Siberian woman, on the Ukok Plateau in the Altai Mountains near the Mongolian border. The mummy was naturally frozen due to the severe climatic conditions of the Siberian steppe. Also known as Princess Ukok, the mummy was dressed in finely detailed clothing and wore an elaborate headdress and jewelry. Alongside her body were buried six decorated horses and a symbolic meal for her last journey. Her left arm and hand were tattooed with animal style figures, including a highly stylized deer.

The Ice Maiden has been a source of some recent controversy. The mummy's skin has suffered some slight decay, and the tattoos have faded since the excavation. Some residents of the Altai Republic, formed after the breakup of the Soviet Union, have requested the return of the Ice Maiden, who is currently stored in Novosibirsk in Siberia.

Another Siberian mummy, a man, was discovered much earlier in 1929. His skin was also marked with tattoos of two monsters resembling griffins, which decorated his chest, and three partially obliterated images which seem to represent two deer and a mountain goat on his left arm.

Philippine mummies are called Kabayan Mummies. They are common in Igorot culture and their heritage. The mummies are found in some areas named Kabayan, Sagada and among others. The mummies are dated between the 14th and 19th centuries.

The European continent is home to a diverse spectrum of spontaneous and anthropogenic mummies. Some of the best-preserved mummies have come from bogs located across the region. The Capuchin monks that inhabited the area left behind hundreds of intentionally-preserved bodies that have provided insight into the customs and cultures of people from various eras. One of the oldest mummies (nicknamed Ötzi) was discovered on this continent. New mummies continue to be uncovered in Europe well into the 21st Century.

The United Kingdom, the Republic of Ireland, Germany, the Netherlands, Sweden, and Denmark have produced a number of bog bodies, mummies of people deposited in sphagnum bogs, apparently as a result of murder or ritual sacrifices. In such cases, the acidity of the water, low temperature and lack of oxygen combined to tan the body's skin and soft tissues. The skeleton typically disintegrates over time. Such mummies are remarkably well preserved on emerging from the bog, with skin and internal organs intact; it is even possible to determine the decedent's last meal by examining stomach contents. The Haraldskær Woman was discovered by labourers in a bog in Jutland in 1835. She was erroneously identified as an early medieval Danish queen, and for that reason was placed in a royal sarcophagus at the Saint Nicolai Church, Vejle, where she currently remains. Another bog body, also from Denmark, known as the Tollund Man was discovered in 1950. The corpse was noted for its excellent preservation of the face and feet, which appeared as if the man had recently died. Only the head of Tollund Man remains, due to the decomposition of the rest of his body, which was not preserved along with the head.

The mummies of the Canary Islands belong to the indigenous Guanche people and date to the time before 14th Century Spanish explorers settled in the area. All deceased people within the Guanche culture were mummified during this time, though the level of care taken with embalming and burial varied depending on individual social status. Embalming was carried out by specialized groups, organized according to gender, who were considered unclean by the rest of the community. The techniques for embalming were similar to those of the ancient Egyptians; involving evisceration, preservation, and stuffing of the evacuated bodily cavities, then wrapping of the body in animal skins. Despite the successful techniques utilized by the Guanche, very few mummies remain due to looting and desecration.

The majority of mummies recovered in the Czech Republic come from underground crypts. While there is some evidence of deliberate mummification, most sources state that desiccation occurred naturally due to unique conditions within the crypts.

The Capuchin Crypt in Brno contains three hundred years of mummified remains directly below the main altar. Beginning in the 18th Century when the crypt was opened, and continuing until the practice was discontinued in 1787, the Capuchin monks of the monastery would lay the deceased on a pillow of bricks on the ground. The unique air quality and topsoil within the crypt naturally preserved the bodies over time.

Approximately fifty mummies were discovered in an abandoned crypt beneath the Church of St. Procopius of Sázava in Vamberk in the mid-1980s. Workers digging a trench accidentally broke into the crypt, which began to fill with waste water. The mummies quickly began to deteriorate, though thirty-four were able to be rescued and stored temporarily at the District Museum of the Orlické Mountains until they could be returned to the monastery in 2000. The mummies range in age and social status at time of death, with at least two children and one priest. The majority of the Vamberk mummies date from the 18th century.

The Klatovy catacombs currently house an exhibition of Jesuit mummies, alongside some aristocrats, that were originally interred between 1674–1783. In the early 1930s, the mummies were accidentally damaged during repairs, resulting in the loss of 140 bodies. The newly updated airing system preserves the thirty-eight bodies that are currently on display.

Apart from several bog bodies, Denmark has also yielded several other mummies, such as the three Borum Eshøj mummies, the Skrydstrup Woman and the Egtved Girl, who were all found inside burial mounds, or tumuli.

In 1875, the Borum Eshøj grave mound was uncovered, which had been built around three coffins, which belonged to a middle aged man and woman as well as a man in his early twenties. Through examination, the woman was discovered to be around 50–60 years old. She was found with several artifacts made of bronze, consisting of buttons, a belt plate, and rings, showing she was of higher class. All of the hair had been removed from the skull later when farmers had dug through the casket. Her original hairstyle is unknown. The two men wore kilts, and the younger man wore a sheath which contained a bronze dagger. All three mummies were dated to 1351–1345 BC.

The Skrydstrup Woman was unearthed from a tumulus in Southern Jutland, in 1935. Carbon-14 dating showed that she had died around 1300 BC; examination also revealed that she was around 18–19 years old at the time of death, and that she had been buried in the summertime. Her hair had been drawn up in an elaborate hairstyle, which was then covered by a horse hair hairnet made by the sprang technique. She was wearing a blouse and a necklace as well as two golden earrings, showing she was of higher class.

The Egtved Girl, dated to 1370 BC, was also found inside a sealed coffin within a tumulus, in 1921. She was wearing a bodice and a skirt, including a belt and bronze bracelets. Found with the girl, at her feet, were the cremated remains of a child and, by her head, a box containing some bronze pins, a hairnet, and an awl.

In 1994, 265 mummified bodies were found in the crypt of a Dominican church in Vác, Hungary from the 1729–1838 period. The discovery proved to be scientifically important, and by 2006 an exhibition was established in the Museum of Natural History in Budapest. Unique to the Hungarian mummies are their elaborately decorated coffins, with no two being exactly alike.

The varied geography and climatology of Italy has led to many cases of spontaneous mummification. Italian mummies display the same diversity, with a conglomeration of natural and intentional mummification spread across many centuries and cultures.

The oldest natural mummy in Europe was discovered in 1991 in the Ötztal Alps on the Austrian-Italian border. Nicknamed Ötzi, the mummy is a 5,300-year-old male believed to be a member of the Tamins-Carasso-Isera cultural group of South Tyrol. Despite his age, a recent DNA study conducted by Walther Parson of Innsbruck Medical University revealed Ötzi has 19 living genetic relatives.

The Capuchin Catacombs of Palermo were built in the 16th century by the monks of Palermo's Capuchin monastery. Originally intended to hold the deliberately mummified remains of dead friars, interment in the catacombs became a status symbol for the local population in the following centuries. Burials continued until the 1920s, with one of the final burials being that of Rosalia Lombardo. In all, the catacombs host nearly 8000 mummies. (See: Catacombe dei Cappuccini)

The most recent discovery of mummies in Italy came in 2010, when sixty mummified human remains were found in the crypt of the Conversion of St Paul church in Roccapelago di Pievepelago, Italy. Built in the 15th century as a cannon hold and later converted in the 16th century, the crypt had been sealed once it had reached capacity, leaving the bodies to be protected and preserved. The crypt was reopened during restoration work on the church, revealing the diverse array of mummies inside. The bodies were quickly moved to a museum for further study.

The mummies of North America are often steeped in controversy, as many of these bodies have been linked to still-existing native cultures. While the mummies provide a wealth of historically-significant data, native cultures and tradition often demands the remains be returned to their original resting places. This has led to many legal actions by Native American councils, leading to most museums keeping mummified remains out of the public eye.

Kwäday Dän Ts'ìnchi ("Long ago person found" in the Southern Tutchone language of the Champagne and Aishihik First Nations), was found in August 1999 by three First Nations hunters at the edge of a glacier in Tatshenshini-Alsek Provincial Park, British Columbia, Canada. According to the Kwäday Dän Ts'ìnchi Project, the remains are the oldest well preserved mummy discovered in North America. (The Spirit Cave mummy although not well preserved, is much older.) Initial radiocarbon tests date the mummy to around 550 years-old.

In 1972, eight remarkably preserved mummies were discovered at an abandoned Inuit settlement called Qilakitsoq, in Greenland. The "Greenland Mummies" consisted of a six-month-old baby, a four-year-old boy, and six women of various ages, who died around 500 years ago. Their bodies were naturally mummified by the sub-zero temperatures and dry winds in the cave in which they were found.

Intentional mummification in pre-Columbian Mexico was practiced by the Aztec culture. These bodies are collectively known as Aztec mummies. Genuine Aztec mummies were "bundled" in a woven wrap and often had their faces covered by a ceremonial mask. Public knowledge of Aztec mummies increased due to traveling exhibits and museums in the 19th and 20th centuries, though these bodies were typically naturally desiccated remains and not actually the mummies associated with Aztec culture. (See: Aztec mummy)

Natural mummification has been known to occur in several places in Mexico; this includes the mummies of Guanajuato. A collection of these mummies, most of which date to the late 19th century, have been on display at "El Museo de las Momias" in the city of Guanajuato since 1970. The museum claims to have the smallest mummy in the world on display (a mummified fetus). It was thought that minerals in the soil had the preserving effect, however it may rather be due to the warm, arid climate. Mexican mummies are also on display in the small town of Encarnación de Díaz, Jalisco.

Spirit Cave Man was discovered in 1940 during salvage work prior to guano mining activity that was scheduled to begin in the area. The mummy is a middle-aged male, found completely dressed and lying on a blanket made of animal skin. Radiocarbon tests in the 1990s dated the mummy to being nearly 9,000 years old. The remains are currently held at the Nevada State Museum. There has been some controversy within the local Native American community, who began petitioning to have the remains returned and reburied in 1995.

Mummies from the Oceania are not limited only to Australia. Discoveries of mummified remains have also been located in New Zealand, and the Torres Strait, though these mummies have been historically harder to examine and classify. Prior to the 20th Century, most literature on mummification in the region was either silent or anecdotal. However, the boom of interest generated by the scientific study of Egyptian mummification lead to more concentrated study of mummies in other cultures, including those of Oceania.

The aboriginal mummification traditions found in Australia are thought be related to those found in the Torres Strait islands, the inhabitants of which achieved a high level of sophisticated mummification techniques (See:Torres Strait). Australian mummies lack some of the technical ability of the Torres Strait mummies, however much of the ritual aspects of the mummification process are similar. Full-body mummification was achieved by these cultures, but not the level of artistic preservation as found on smaller islands. The reason for this seems to be for easier transport of bodies by more nomadic tribes.

The mummies of the Torres Strait have a considerably higher level of preservation technique as well as creativity compared to those found on Australia. The process began with removal of viscera, after which the bodies were set in a seated position on a platform and either left to dry in the sun or smoked over a fire in order to aid in desiccation. In the case of smoking, some tribes would collect the fat that drained from the body to mix with ocher to create red paint that would then be smeared back on the skin of the mummy. The mummies remained on the platforms, decorated with the clothing and jewelry they wore in life, before being buried.

Some Māori tribes from New Zealand would keep mummified heads as trophies from tribal warfare. They are also known as Mokomokai. In the 19th Century, many of the trophies were acquired by Europeans who found the tattooed skin to be a phenomenal curiosity. Westerners began to offer valuable commodities in exchange for the uniquely tattooed mummified heads. The heads were later put on display in museums, 16 of which being housed across France alone. In 2010, the Rouen City Hall of France returned one of the heads to New Zealand, despite earlier protests by the Culture Ministry of France.

There is also evidence that some Maori tribes may have practiced full-body mummification, though the practice is not thought to have been widespread. The discussion of Maori mummification has been historically controversial, with some experts in past decades claiming that such mummies have never existed. Contemporary science does now acknowledge the existence of full-body mummification in the culture. There is still controversy, however, as to the nature of the mummification process. Some bodies appear to be spontaneously created by the natural environment, while others exhibit signs of deliberate practices. General modern consensus tends to agree that there could be a mixture of both types of mummification, similar to that of the ancient Egyptian mummies.

The South American continent contains some of the oldest mummies in the world, both deliberate and accidental. The bodies were preserved by the best agent for mummification: the environment. The Pacific coastal desert in Peru and Chile is one of the driest areas in the world and the dryness facilitated mummification. Rather than developing elaborate processes such as later-dynasty ancient Egyptians, the early South Americans often left their dead in naturally dry or frozen areas, though some did perform surgical preparation when mummification was intentional. Some of the reasons for intentional mummification in South America include memorialization, immortalization, and religious offerings. A large number of mummified bodies have been found in pre-Columbian cemeteries scattered around Peru. The bodies had often been wrapped for burial in finely-woven textiles.

The Chinchorro mummies are the oldest intentionally prepared mummified bodies ever found. Beginning in 5th millennium BC and continuing for an estimated 3,500 years, all human burials within the Chinchorro culture were prepared for mummification. The bodies were carefully prepared, beginning with removal of the internal organs and skin, before being left in the hot, dry climate of the Atacama Desert, which aided in desiccation. A large number of Chinchorro mummies were also prepared by skilled artisans to be preserved in a more artistic fashion, though the purpose of this practice is widely debated.

Several naturally-preserved, unintentional mummies dating from the Incan period (1438–1532 AD) have been found in the colder regions of Argentina, Chile, and Peru. These are collectively known as "ice mummies". The first Incan ice mummy was discovered in 1954 atop El Plomo Peak in Chile, after an eruption of the nearby volcano Sabancaya melted away ice that covered the body. The Mummy of El Plomo was a male child who was presumed to be wealthy due to his well-fed bodily characteristics. He was considered to be the most well-preserved ice mummy in the world until the discovery of Mummy Juanita in 1995.

Mummy Juanita was discovered near the summit of Ampato in the Peruvian section of the Andes mountains by archaeologist Johan Reinhard. Her body had been so thoroughly frozen that it had not been desiccated; much of her skin, muscle tissue, and internal organs retained their original structure. She is believed to be a ritual sacrifice, due to the close proximity of her body to the Incan capital of Cusco, as well as the fact she was wearing highly intricate clothing to indicate her special social status. Several Incan ceremonial artifacts and temporary shelters uncovered in the surrounding area seem to support this theory.

More evidence that the Inca left sacrificial victims to die in the elements, and later be unintentionally preserved, came in 1999 with the discovery of the Llullaillaco mummies on the border of Argentina and Chile. The three mummies are children, two girls and one boy, who are thought to be sacrifices associated with the ancient ritual of "qhapaq hucha". Recent biochemical analysis of the mummies has revealed that the victims had consumed increasing quantities of alcohol and coca, possibly in the form of chicha, in the months leading up to sacrifice. The dominant theory for the drugging reasons that, alongside ritual uses, the substances probably made the children more docile. Chewed coca leaves found inside the eldest child's mouth upon her discovery in 1999 supports this theory.

The bodies of Inca emperors and wives were mummified after death. In 1533, the Spanish conquistadors of the Inca Empire viewed the mummies in the Inca capital of Cuzco. The mummies were displayed, often in lifelike positions, in the palaces of the deceased emperors and had a retinue of servants to care for them. The Spanish were impressed with the quality of the mummification which involved removal of the organs, embalming, and freeze-drying.

The population revered the mummies of the Inca emperors. This reverence seemed idolatry to the Roman Catholic Spanish and in 1550 they confiscated the mummies. The mummies were taken to Lima where they were displayed in the San Andres Hospital. The mummies deteriorated in the humid climate of Lima and eventually they were either buried or destroyed by the Spanish.

An attempt to find the mummies of the Inca emperors beneath the San Andres hospital in 2001 was unsuccessful. The archaeologists found a crypt, but it was empty. Possibly the mummies had been removed when the building was repaired after an earthquake.

Monks whose bodies remain incorrupt without any traces of deliberate mummification are venerated by some Buddhists who believe they successfully were able to mortify their flesh to death. Self-mummification was practiced until the late 1800s in Japan and has been outlawed since the early 1900s.

Many Mahayana Buddhist monks were reported to know their time of death and left their last testaments and their students accordingly buried them sitting in lotus position, put into a vessel with drying agents (such as wood, paper, or lime) and surrounded by bricks, to be exhumed later, usually after three years. The preserved bodies would then be decorated with paint and adorned with gold.

Bodies purported to be those of self-mummified monks are exhibited in several Japanese shrines, and it has been claimed that the monks, prior to their death, stuck to a sparse diet made up of salt, nuts, seeds, roots, pine bark, and "urushi" tea.

In the 1830s, Jeremy Bentham, the founder of utilitarianism, left instructions to be followed upon his death which led to the creation of a sort of modern-day mummy. He asked that his body be displayed to illustrate how the "horror at dissection originates in ignorance"; once so displayed and lectured about, he asked that his body parts be preserved, including his skeleton (minus his skull, which despite being mis-preserved, was displayed beneath his feet until theft required it to be stored elsewhere), which were to be dressed in the clothes he usually wore and "seated in a Chair usually occupied by me when living in the attitude in which I am sitting when engaged in thought". His body, outfitted with a wax head created because of problems preparing it as Bentham requested, is on open display in the University College London.

During the early 20th century, the Russian movement of Cosmism, as represented by Nikolai Fyodorovich Fyodorov, envisioned scientific resurrection of dead people. The idea was so popular that, after Vladimir Lenin's death, Leonid Krasin and Alexander Bogdanov suggested to cryonically preserve his body and brain in order to revive him in the future. Necessary equipment was purchased abroad, but for a variety of reasons the plan was not realized. Instead his body was embalmed and placed on permanent exhibition in the Lenin Mausoleum in Moscow, where it is displayed to this day. The mausoleum itself was modeled by Alexey Shchusev on the Pyramid of Djoser and the Tomb of Cyrus.

In late 19th-century Venezuela, a German-born doctor named Gottfried Knoche conducted experiments in mummification at his laboratory in the forest near La Guaira. He developed an embalming fluid (based on an aluminum chloride compound) that mummified corpses without having to remove the internal organs. The formula for his fluid was never revealed and has not been discovered. Most of the several dozen mummies created with the fluid (including himself and his immediate family) have been lost or were severely damaged by vandals and looters.

In 1975, an esoteric organization by the name of Summum introduced "Modern Mummification", a service that utilizes modern techniques along with aspects of ancient methods of mummification. The first person to formally undergo Summum's process of modern mummification was the founder of Summum, Summum Bonum Amen Ra, who died in January 2008. Summum is currently considered to be the only "commercial mummification business" in the world.

In 2010, a team led by forensic archaeologist Stephen Buckley mummified Alan Billis using techniques based on 19 years of research of 18th-dynasty Egyptian mummification. The process was filmed for television, for the documentary "Mummifying Alan: Egypt's Last Secret". Billis made the decision to allow his body to be mummified after being diagnosed with terminal cancer in 2009. His body currently resides at London's Gordon Museum.

Plastination is a technique used in anatomy to conserve bodies or body parts. The water and fat are replaced by certain plastics, yielding specimens that can be touched, do not smell or decay, and even retain most microscopic properties of the original sample.

The technique was invented by Gunther von Hagens when working at the anatomical institute of the Heidelberg University in 1978. Von Hagens has patented the technique in several countries and is heavily involved in its promotion, especially as the creator and director of the Body Worlds traveling exhibitions, exhibiting plastinated human bodies internationally. He also founded and directs the Institute for Plastination in Heidelberg.

More than 40 institutions worldwide have facilities for plastination, mainly for medical research and study, and most affiliated to the International Society for Plastination.

In the Middle Ages, based on a mistranslation from the Arabic term for bitumen, it was thought that mummies possessed healing properties. As a result, it became common practice to grind Egyptian mummies into a powder to be sold and used as medicine. When actual mummies became unavailable, the sun-desiccated corpses of criminals, slaves and suicidal people were substituted by mendacious merchants. Mummies were said to have a lot of healing properties. Francis Bacon and Robert Boyle recommended them for healing bruises and preventing bleeding. The trade in mummies seems to have been frowned upon by Turkish authorities who ruled Egypt – several Egyptians were imprisoned for boiling mummies to make oil in 1424. However, mummies were in high demand in Europe and it was possible to buy them for the right amount of money. John Snaderson, an English tradesman who visited Egypt in the 16th century shipped six hundred pounds of mummy back to England.

The practice developed into a wide-scale business that flourished until the late 16th century. Two centuries ago, mummies were still believed to have medicinal properties to stop bleeding, and were sold as pharmaceuticals in powdered form as in mellified man. Artists also made use of Egyptian mummies; a brownish pigment known as mummy brown, based on "mummia" (sometimes called alternatively "caput mortuum", Latin for "death's head"), which was originally obtained by grinding human and animal Egyptian mummies. It was most popular in the 17th century, but was discontinued in the early 19th century when its composition became generally known to artists who replaced the said pigment by a totally different blend -but keeping the original name, mummia or mummy brown-yielding a similar tint and based on ground minerals (oxides and fired earths) and or blends of powdered gums and oleoresins (such as myrrh and frankincense) as well as ground bitumen. These blends appeared on the market as forgeries of powdered mummy pigment but were ultimately considered as acceptable replacements, once antique mummies were no longer permitted to be destroyed. Many thousands of mummified cats were also sent from Egypt to England to be processed for use in fertilizer.

During the 19th century, following the discovery of the first tombs and artifacts in Egypt, egyptology was a huge fad in Europe, especially in Victorian England. European aristocrats would occasionally entertain themselves by purchasing mummies, having them unwrapped, and holding observation sessions. The pioneer of this kind of entertainment in Britain was Thomas Pettigrew known as "Mummy" Pettigrew due to his work. Such unrolling sessions destroyed hundreds of mummies, because the exposure to the air caused them to disintegrate.

The use of mummies as fuel for locomotives was documented by Mark Twain (likely as a joke or humor), but the truth of the story remains debatable. During the American Civil War, mummy-wrapping linens were said to have been used to manufacture paper. Evidence for the reality of these claims is still equivocal. Researcher Ben Radford reports that, in her book "The Mummy Congress", Heather Pringle writes: "No mummy expert has ever been able to authenticate the story ... Twain seems to be the only published source — and a rather suspect one at that". Pringle also writes that there is no evidence for the "mummy paper" either. Radford also says that many journalists have not done a good job with their research, and while it is true that mummies were often not shown respect in the 1800s, there is no evidence for this rumor.

While mummies were used in medicine, some researchers have brought into question these other uses such as making paper and paint, fueling locomotives and fertilizing land.

Bibliography

Books

Online

Video


</doc>
<doc id="20226" url="https://en.wikipedia.org/wiki?curid=20226" title="Melilla">
Melilla

Melilla ( , , ; ) is a Spanish autonomous city located on the northwest coast of Africa, sharing a border with Morocco. It has an area of . Melilla is one of two permanently inhabited Spanish cities in mainland Africa, the other being nearby Ceuta. It was part of the Province of Málaga until 14 March 1995, when the city's Statute of Autonomy was passed.

Melilla, like Ceuta, was a free port before Spain joined the European Union in 1986. As of 2011, Melilla had a population of 78,476, made up of ethnic Iberian Catholics (primarily from Andalusia and Catalonia), ethnic Riffian Berbers, and a small number of Sephardic Jews and Sindhi Hindus. Spanish and Riffian-Berber are the two most widely spoken languages, the former being the official language.

Melilla, like Ceuta, is officially claimed by Morocco.

The current Berber name of Melilla is "Mřič" or "Mlilt", which means the "white one". Melilla was an ancient Berber village. It was a Phoenician and later Punic trade establishment under the name of Rusadir ("Rusaddir" for the Romans and "Russadeiron" () for the Greeks). Later Rome absorbed it as part of the Roman province of Mauretania Tingitana. Rusaddir is mentioned by Ptolemy (IV, 1) and Pliny (V, 18) who called it ""oppidum et portus"" (a fortified town and port). It was also cited by Mela (I, 33) as "Rusicada," and by the "Itinerarium Antonini". Rusaddir was said to have once been the seat of a bishop, but there is no record of any bishop of the purported see, which is not included in the Catholic Church's list of titular sees. As centuries passed, it was ruled by Vandal, Byzantine and Hispano-Visigothic bands. The political history is similar to that of towns in the region of the Moroccan Rif and southern Spain. Local rule passed through a succession of Amazigh, Phoenician, Punic, Roman, Umayyad, Idrisid, Almoravid, Almohad, Marinid, and then Wattasid rulers. During the Middle Ages, it was known as the Berber city of Mlila. It was part of the Kingdom of Fez when the Catholic Monarchs, Queen Isabella I of Castile and King Ferdinand II of Aragon, asked Juan Alfonso Pérez de Guzmán, 3rd Duke of Medina Sidonia, to take the city.

In the Conquest of Melilla, the duke sent Pedro Estopiñán, who conquered the city in 1497 virtually without any violence, a few years after Castile had taken control of the Nasrid Kingdom of Granada in 1492, which was the last remnant of Al-Andalus, the Muslim state in Iberia. Some Spaniards envisioned continuing the southward conquest of Muslim lands, deeper into Morocco, but such action was not seriously attempted. Spain's imperial energy was directed elsewhere, to the newly discovered continent across the Atlantic. The Muslims tried to take back control of Melilla in later centuries: they besieged it during 1694–1696 and 1774–1775. One Spanish officer reflected, "an hour in Melilla, from the point of view of merit, was worth more than thirty years of service to Spain."

The current limits of the Spanish territory around the Melilla fortress were fixed by treaties with Morocco in 1859, 1860, 1861, and 1894. In the late 19th century, as Spanish influence expanded in this area, the Crown authorized Melilla as the only centre of trade on the Rif coast between Tetuan and the Algerian frontier. The value of trade increased, with goat skins, eggs and beeswax being the principal exports, and cotton goods, tea, sugar and candles being the chief imports.
In 1893, the Rif Berbers launched the First Melillan campaign to take back this area; Spain sent 25,000 Spanish soldiers to defend against them. The conflict was also known as the "Margallo War", after Spanish General Juan García y Margallo, who was killed in the battle, and was the Governor of Melilla.
In 1908 two companies under the protection of Bou Hmara, a chieftain then ruling the Rif region, started mining lead and iron some 20 kilometers (12.4 miles) from Melilla. They started to construct a railway between the port and the mines. In October of that year the Bou Hmara's vassals revolted against him and raided the mines, which remained closed until June 1909. By July the workmen were again attacked and several were killed. Severe fighting between the Spaniards and the tribesmen followed, in the Second Melillan campaign.

In 1910, with the Rif having submitted, the Spaniards restarted the mines and undertook harbor works at Mar Chica, but hostilities broke out again in 1911. On 22 July 1921, the Berbers under the leadership of Abd el Krim inflicted a grave defeat on the Spanish (see Battle of Annual). The Berbers controlled the area until 1926, when the Spanish Protectorate finally managed to regain the area.
The city was used as one of the staging grounds for the July 1936 military coup d'état that started the Spanish Civil War. A statue of Francisco Franco, the putschist general assuming the control of the Army of Africa in 1936, is still prominently featured, the last statue of Franco in Spain.

On 6 November 2007, King Juan Carlos I and Queen Sofia visited the city, which caused a massive demonstration of support. The visit also sparked protests from the Moroccan government. It was the first time a Spanish monarch had visited Melilla in 80 years.

Melilla (and Ceuta) have declared the Muslim holiday of Eid al-Adha or Feast of the Sacrifice, as an official public holiday from 2010 onward. This is the first time a non-Christian religious festival has been officially celebrated in Spain since the Reconquista.

Melilla is located in the northwest of the African continent, next to the Alboran Sea and across the sea from the Spanish provinces of Granada and Almería. The city layout is arranged in a wide semicircle around the beach and the Port of Melilla, on the eastern side of the peninsula of Cape Tres Forcas, at the foot of Mount Gurugú and the mouth of the Río de Oro, above sea level. The urban nucleus was originally a fortress, Melilla la Vieja, built on a peninsular mound about in height.

The Moroccan settlement of Beni Ansar lies immediately south of Melilla. The nearest Moroccan city is Nador, and the ports of Melilla and Nador are both within the same bay; nearby is the Bou Areg Lagoon

Melilla has held local elections for its 25-seat legislature every four years since 1979. Since its Statute of Autonomy in 1995, the legislature has been called the Assembly and its leader the Mayor-President. In the most recent election in 2019, the People's Party (PP) won the most seats (10 seats), but could not maintain the role of Mayor-President for Juan José Imbroda, who had held office since 2000. The regionalist and leftist party Coalition for Melilla (CPM, 8 seats), the Spanish Socialist Workers' Party (PSOE, 4 seats) and Citizens–Party of the Citizenry (Cs, 1 seat) voted in favour of the candidate of Cs, Eduardo de Castro.

Melilla is subdivided into eight districts ("distritos"), which are further subdivided into neighbourhoods ("barrios"):

The government of Morocco has requested from Spain the sovereignty of the cities of Ceuta and Melilla, Perejil Island, and some other small territories. The Spanish position is that both Ceuta and Melilla are integral parts of the Spanish state (and, therefore, are not considered colonies), and have been since the 15th century; both cities also have the same semi-autonomous status as the mainland region in Spain. Melilla has been under Spanish rule for longer than cities in northern Spain such as Pamplona or Tudela, and was conquered roughly in the same period as the last Muslim cities of Southern Spain such as Granada, Málaga, Ronda or Almería: Spain claims that the enclaves were established before the creation of the Kingdom of Morocco. Morocco denies these claims and maintains that the Spanish presence on or near its coast is a remnant of the colonial past which should be ended. The United Nations list of Non-Self-Governing Territories does not include these Spanish territories and the dispute remains bilaterally debated between Spain and Morocco.

Melilla has a warm Mediterranean climate influenced by its proximity to the sea, rendering much cooler summers and more precipitation than inland areas deeper into Africa. The climate, in general, has a lot in common with the type found in southern coastal Spain on the European mainland, with relatively small temperature differences between seasons.

The principal industry is fishing. Cross-border commerce (legal or smuggled) and Spanish and European grants and wages are the other income sources.

Melilla is regularly connected to the Iberian peninsula by air and sea traffic and is also economically connected to Morocco: most of its fruit and vegetables are imported across the border. Moroccans in the city's hinterland are attracted to it: 36,000 Moroccans cross the border daily to work, shop or trade goods. The port of Melilla offers several daily connections to Almería and Málaga. Melilla Airport offers daily flights to Almería, Málaga and Madrid. Spanish operators Air Europa and Iberia operate in Melilla's airport.

Many people travelling between Europe and Morocco use the ferry links to Melilla, both for passengers and for freight. Because of this, the port and related companies form an important economic driver for the city.

Melilla's Capilla de Santiago, or James's Chapel, by the city walls, is the only authentic Gothic structure in Africa.

In the first quarter of the 20th century, Melilla became a thriving port benefitting from the recently established Protectorate of Spanish Morocco in the contiguous Rif. The new architectural style of Modernisme was expressed by a new bourgeois class. This style, frequently referred to as the Catalan version of Art Nouveau, was extremely popular in the early part of the 20th century in Spain.

The workshops inspired by the Catalan architect Enrique Nieto continued in the modernist style, even after Modernisme went out of fashion elsewhere. Accordingly, Melilla has the second most important concentration of Modernist works in Spain after Barcelona. Nieto was in charge of designing the main Synagogue, the Central Mosque and various Catholic Churches.
Melilla has been praised as an example of multiculturalism, being a small city in which one can find four major religions represented. However, the Christian majority of the past, constituting around 65% of the population at one point, has been shrinking, while the number of native Muslim inhabitants has steadily increased to its present 45% of the population. The Jewish and Hindu communities have also been shrinking due to economic emigration to mainland Spain (notably Malaga and Madrid).

Jews, who had lived in Melilla for centuries, have been leaving the city in recent years (from 20% of the population before World War II to less than 5% today). Most of the Jewish population has left for Israel and Venezuela. There is a small, autonomous, and commercially important Hindu community present in Melilla, which numbers about 100 members today.

The amateur radio call sign used for both cities is EA9.

Melilla has been a popular destination for refugees and people leaving countries with poor economies in order to enter the European Union. The border is secured by the Melilla border fence, a six-metre-tall double fence with watch towers; yet refugees frequently manage to cross it. Detection wires, radar, and day/night vision cameras are planned to increase security and prevent illegal immigration. In February 2014, over 200 migrants from sub-Saharan Africa scaled a security fence to get into the Melilla migrant reception centre. The reception centre, built for 480 migrants, was already overcrowded with 1,300 people. 

In recent years, the Spanish government has urged Moroccan security forces to stem the flow of migrants traveling towards Melilla. In 2015, Moroccan police dispersed migrant camps in the forests surrounding Melilla by torching makeshift homes and arresting migrants. Since the 2014 incident, Spain has installed additional security measures, including increased fencing, camera surveillance systems, and a more salient troop presence. Attempted border crossings by migrants has decreased at both Melilla and Ceuta since its peak in 2015-2016; arrivals are down twenty-five percent since 2018. However, attempts by migrants to swarm the security fences at Melilla have been widely broadcast by Spanish media sources, creating a sense of urgency in mainland Spain. This fear over African migrants is seen by many as the main factor leading to the rise of Vox, Spain's populist party. Vox officials have frequently pointed to the immigration situation at Melilla and Ceuta proof of a crisis at Spain's border.

Melilla Airport is serviced by Air Nostrum, flying to the Spanish cities of Málaga, Madrid, Barcelona, Las Palmas de Gran Canaria, Palma de Mallorca, Granada, Badajoz, Sevilla and Almería. In April 2013, a local enterprise set up Melilla Airlines, flying from the city to Málaga. The city is linked to Málaga, Almería and Motril by ferry.

Three roads connect Melilla and Morocco but require clearance through border checkpoints.

Melilla is a surfing destination. The city's football club, UD Melilla, plays in the third tier of Spanish football, the Segunda División B. The club was founded in 1943 and since 1945 have played at the 12,000-seater Estadio Municipal Álvarez Claro. Until the other club was dissolved in 2012, UD Melilla played the Ceuta-Melilla derby against AD Ceuta. The clubs travelled to each other via the Spanish mainland to avoid entering Morocco. The second-highest ranked club in the city are Casino del Real CF of the fourth-tier Tercera División. Football in the exclave is administered by the Melilla Football Federation.

Melilla is twinned with:






</doc>
<doc id="20229" url="https://en.wikipedia.org/wiki?curid=20229" title="Macaroni">
Macaroni

Macaroni (, Italian: Maccheroni) is dry pasta shaped like narrow tubes. Made with durum wheat, macaroni is commonly cut in short lengths; curved macaroni may be referred to as elbow macaroni. Some home machines can make macaroni shapes, but like most pasta, macaroni is usually made commercially by large-scale extrusion. The curved shape is created by different speeds of extrusion on opposite sides of the pasta tube as it comes out of the machine.

In North America, the word "macaroni" is often used synonymously with elbow-shaped macaroni, as it is the variety most often used in macaroni and cheese recipes. In Italy, the noun "maccheroni" refers to straight, tubular, square-ended "pasta corta" ("short-length pasta"). Maccheroni may also refer to long pasta dishes such as "maccheroni alla chitarra" and "frittata di maccheroni", which are prepared with long pasta like spaghetti.

The name comes from Italian "maccheroni" , plural form of "maccherone". The many variants sometimes differ from each other because of the texture of each pasta: "rigatoni" and "tortiglioni", for example, have ridges down their lengths, while "chifferi", "lumache", "lumaconi", "pipe", "pipette", etc. refer to elbow-shaped pasta similar to macaroni in North American culture.

However, the product as well as the name derive from the ancient Greek "Macaria". The academic consensus supports that the word is derived from the Greek μακαρία ("makaria"), a kind of barley broth which was served to commemorate the dead. In turn, that comes from μάκαρες ("makares") meaning "blessed dead", and ultimately from μακάριος ("makarios"), collateral of μάκαρ ("makar") which means "blessed, happy".

However, the Italian linguist G. Alessio argues that the word can have two origins. The first is the Medieval Greek μακαρώνεια ("makarōneia") "dirge" (stated in sec. XIII by James of Bulgaria), which would mean "funeral meal" and then "food to serve" during this office (see modern Eastern Thrace's μαχαρωνιά - "macharōnia" in the sense of "rice-based dish served at the funeral"), in which case, the term would be composed of the double root of μακάριος "blessed" and αἰωνίος ("aiōnios"), "eternally". The second is the Greek μακαρία "barley broth", which would have added the suffix "-one".

In his book "Delizia! The Epic History of Italians and their Food" (2007), John Dickie instead says that the word macaroni, and its earlier variants like "maccheroni", "comes from "maccare", meaning to pound or crush."

The word first appears in English as "makerouns" in the 1390 "Forme of Cury" which records the earliest recipe for macaroni cheese. The word later came to be applied to overdressed dandies and was associated with foppish Italian fashions of dress and periwigs, as in the eighteenth-century British song "Yankee Doodle".

The Russian language borrowed the word (as ) as a generic term for all varieties of pasta; this also holds for several other Slavic languages, as well as for Estonian, Turkish, Greek, and Brazilian Portuguese. In Iran, all sorts of pasta are collectively called "makaroni".

As is the case with dishes made with other types of pasta, macaroni and cheese is a popular dish in North America, and is often made with elbow macaroni. The same dish, known simply as macaroni cheese, is also popular in Great Britain, where it originated. A sweet macaroni pudding containing milk and sugar (and rather similar to a rice pudding) was also popular with the British during the Victorian era.
In areas with large Chinese populations open to Western cultural influence, such as Hong Kong, Macao, Malaysia and Singapore, the local Chinese have adopted macaroni as an ingredient for Chinese-style Western cuisine. In Hong Kong's "cha chaan teng" ("tea restaurants") and Southeast Asia's "kopi tiam" ("coffee shops"), macaroni are cooked in water and then rinsed to remove starch, and served in clear broth with ham or frankfurter sausages, peas, black mushrooms, and optionally eggs, reminiscent of noodle soup dishes. This is often a course for breakfast or light lunch fare.




</doc>
<doc id="20232" url="https://en.wikipedia.org/wiki?curid=20232" title="Messenger RNA">
Messenger RNA

Messenger RNA (mRNA) is a large family of RNA molecules that convey genetic information from DNA to the ribosome, where they specify the amino acid sequence of the protein products of gene expression. The RNA polymerase enzyme transcribes genes into primary transcript mRNA (known as pre-mRNA) leading to processed, mature mRNA. This mature mRNA is then translated into a polymer of amino acids: a protein, as summarized in the central dogma of molecular biology.

As in DNA, mRNA genetic information is in the sequence of nucleotides, which are arranged into codons consisting of three base pairs each. Each codon encodes for a specific amino acid, except the stop codons, which terminate protein synthesis. This process of translation of codons into amino acids requires two other types of RNA: Transfer RNA (tRNA), that mediates recognition of the codon and provides the corresponding amino acid, and ribosomal RNA (rRNA), that is the central component of the ribosome's protein-manufacturing machinery.

The existence of mRNA was first suggested by Jacques Monod and François Jacob, and subsequently discovered by Jacob, Sydney Brenner and Matthew Meselson at the California Institute of Technology in 1961.

It should not be confused with mitochondrial DNA.

The brief existence of an mRNA molecule begins with transcription, and ultimately ends in degradation. During its life, an mRNA molecule may also be processed, edited, and transported prior to translation. Eukaryotic mRNA molecules often require extensive processing and transport, while prokaryotic mRNA molecules do not. A molecule of eukaryotic mRNA and the proteins surrounding it are together called a messenger RNP.

Transcription is when RNA is made from DNA. During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed. This process is similar in eukaryotes and prokaryotes. One notable difference, however, is that eukaryotic RNA polymerase associates with mRNA-processing enzymes during transcription so that processing can proceed quickly after the start of transcription. The short-lived, unprocessed or partially processed product is termed "precursor mRNA", or "pre-mRNA"; once completely processed, it is termed "mature mRNA".

Processing of mRNA differs greatly among eukaryotes, bacteria, and archea. Non-eukaryotic mRNA is, in essence, mature upon transcription and requires no processing, except in rare cases . Eukaryotic pre-mRNA, however, requires several processing steps before its transport to the cytoplasm and its translation by the ribosome.

The extensive processing of eukaryotic pre-mRNA that leads to the mature mRNA is the RNA splicing, a mechanism by which introns or outrons (non-coding regions) are removed and exons (coding regions) are joined together.

A "5' cap" (also termed an RNA cap, an RNA 7-methylguanosine cap, or an RNA mG cap) is a modified guanine nucleotide that has been added to the "front" or 5' end of a eukaryotic messenger RNA shortly after the start of transcription. The 5' cap consists of a terminal 7-methylguanosine residue that is linked through a 5'-5'-triphosphate bond to the first transcribed nucleotide. Its presence is critical for recognition by the ribosome and protection from RNases.

Cap addition is coupled to transcription, and occurs co-transcriptionally, such that each influences the other. Shortly after the start of transcription, the 5' end of the mRNA being synthesized is bound by a cap-synthesizing complex associated with RNA polymerase. This enzymatic complex catalyzes the chemical reactions that are required for mRNA capping. Synthesis proceeds as a multi-step biochemical reaction.

In some instances, an mRNA will be edited, changing the nucleotide composition of that mRNA. An example in humans is the apolipoprotein B mRNA, which is edited in some tissues, but not others. The editing creates an early stop codon, which, upon translation, produces a shorter protein.

Polyadenylation is the covalent linkage of a polyadenylyl moiety to a messenger RNA molecule. In eukaryotic organisms most messenger RNA (mRNA) molecules are polyadenylated at the 3' end, but recent studies have shown that short stretches of uridine (oligouridylation) are also common. The poly(A) tail and the protein bound to it aid in protecting mRNA from degradation by exonucleases. Polyadenylation is also important for transcription termination, export of the mRNA from the nucleus, and translation. mRNA can also be polyadenylated in prokaryotic organisms, where poly(A) tails act to facilitate, rather than impede, exonucleolytic degradation.

Polyadenylation occurs during and/or immediately after transcription of DNA into RNA. After transcription has been terminated, the mRNA chain is cleaved through the action of an endonuclease complex associated with RNA polymerase. After the mRNA has been cleaved, around 250 adenosine residues are added to the free 3' end at the cleavage site. This reaction is catalyzed by polyadenylate polymerase. Just as in alternative splicing, there can be more than one polyadenylation variant of an mRNA.

Polyadenylation site mutations also occur. The primary RNA transcript of a gene is cleaved at the poly-A addition site, and 100–200 A's are added to the 3’ end of the RNA. If this site is altered, an abnormally long and unstable mRNA construct will be formed.

Another difference between eukaryotes and prokaryotes is mRNA transport. Because eukaryotic transcription and translation is compartmentally separated, eukaryotic mRNAs must be exported from the nucleus to the cytoplasm—a process that may be regulated by different signaling pathways. Mature mRNAs are recognized by their processed modifications and then exported through the nuclear pore by binding to the cap-binding proteins CBP20 and CBP80, as well as the transcription/export complex (TREX). Multiple mRNA export pathways have been identified in eukaryotes.

In spatially complex cells, some mRNAs are transported to particular subcellar destinations. In mature neurons, certain mRNA are transported from the soma to dendrites. One site of mRNA translation is at polyribosomes selectively localized beneath synapses. The mRNA for Arc/Arg3.1 is induced by synaptic activity and localizes selectively near active synapses based on signals generated by NMDA receptors. Other mRNAs also move into dendrites in response to external stimuli, such as β-actin mRNA. Upon export from the nucleus, actin mRNA associates with ZBP1 and the 40S subunit. The complex is bound by a motor protein and is transported to the target location (neurite extension) along the cytoskeleton. Eventually ZBP1 is phosphorylated by Src in order for translation to be initiated. In developing neurons, mRNAs are also transported into growing axons and especially growth cones. Many mRNAs are marked with so-called "zip codes," which target their transport to a specific location.

Because prokaryotic mRNA does not need to be processed or transported, translation by the ribosome can begin immediately after the end of transcription. Therefore, it can be said that prokaryotic translation is "coupled" to transcription and occurs "co-transcriptionally".

Eukaryotic mRNA that has been processed and transported to the cytoplasm (i.e., mature mRNA) can then be translated by the ribosome. Translation may occur at ribosomes free-floating in the cytoplasm, or directed to the endoplasmic reticulum by the signal recognition particle. Therefore, unlike in prokaryotes, eukaryotic translation "is not" directly coupled to transcription. It is even possible in some contexts that reduced mRNA levels are accompanied by increased protein levels, as has been observed for mRNA/protein levels of EEF1A1 in breast cancer.

Coding regions are composed of codons, which are decoded and translated (in eukaryotes usually into one and in prokaryotes usually into several) into proteins by the ribosome. Coding regions begin with the start codon and end with a stop codon. In general, the start codon is an AUG triplet and the stop codon is UAA, UAG, or UGA. The coding regions tend to be stabilised by internal base pairs, this impedes degradation. In addition to being protein-coding, portions of coding regions may serve as regulatory sequences in the pre-mRNA as exonic splicing enhancers or exonic splicing silencers.

Untranslated regions (UTRs) are sections of the mRNA before the start codon and after the stop codon that are not translated, termed the five prime untranslated region (5' UTR) and three prime untranslated region (3' UTR), respectively. These regions are transcribed with the coding region and thus are exonic as they are present in the mature mRNA. Several roles in gene expression have been attributed to the untranslated regions, including mRNA stability, mRNA localization, and translational efficiency. The ability of a UTR to perform these functions depends on the sequence of the UTR and can differ between mRNAs. Genetic variants in 3' UTR have also been implicated in disease susceptibility because of the change in RNA structure and protein translation.

The stability of mRNAs may be controlled by the 5' UTR and/or 3' UTR due to varying affinity for RNA degrading enzymes called ribonucleases and for ancillary proteins that can promote or inhibit RNA degradation. (See also, C-rich stability element.)

Translational efficiency, including sometimes the complete inhibition of translation, can be controlled by UTRs. Proteins that bind to either the 3' or 5' UTR may affect translation by influencing the ribosome's ability to bind to the mRNA. MicroRNAs bound to the 3' UTR also may affect translational efficiency or mRNA stability.

Cytoplasmic localization of mRNA is thought to be a function of the 3' UTR. Proteins that are needed in a particular region of the cell can also be translated there; in such a case, the 3' UTR may contain sequences that allow the transcript to be localized to this region for translation.

Some of the elements contained in untranslated regions form a characteristic secondary structure when transcribed into RNA. These structural mRNA elements are involved in regulating the mRNA. Some, such as the SECIS element, are targets for proteins to bind. One class of mRNA element, the riboswitches, directly bind small molecules, changing their fold to modify levels of transcription or translation. In these cases, the mRNA regulates itself.

The 3' poly(A) tail is a long sequence of adenine nucleotides (often several hundred) added to the 3' end of the pre-mRNA. This tail promotes export from the nucleus and translation, and protects the mRNA from degradation.

An mRNA molecule is said to be monocistronic when it contains the genetic information to translate only a single protein chain (polypeptide). This is the case for most of the eukaryotic mRNAs. On the other hand, polycistronic mRNA carries several open reading frames (ORFs), each of which is translated into a polypeptide. These polypeptides usually have a related function (they often are the subunits composing a final complex protein) and their coding sequence is grouped and regulated together in a regulatory region, containing a promoter and an operator. Most of the mRNA found in bacteria and archaea is polycistronic, as is the human mitochondrial genome. Dicistronic or bicistronic mRNA encodes only two proteins.

In eukaryotes mRNA molecules form circular structures due to an interaction between the eIF4E and poly(A)-binding protein, which both bind to eIF4G, forming an mRNA-protein-mRNA bridge. Circularization is thought to promote cycling of ribosomes on the mRNA leading to time-efficient translation, and may also function to ensure only intact mRNA are translated (partially degraded mRNA characteristically have no m7G cap, or no poly-A tail).

Other mechanisms for circularization exist, particularly in virus mRNA. Poliovirus mRNA uses a cloverleaf section towards its 5' end to bind PCBP2, which binds poly(A)-binding protein, forming the familiar mRNA-protein-mRNA circle. Barley yellow dwarf virus has binding between mRNA segments on its 5' end and 3' end (called kissing stem loops), circularizing the mRNA without any proteins involved.

RNA virus genomes (the + strands of which are translated as mRNA) are also commonly circularized. During genome replication the circularization acts to enhance genome replication speeds, cycling viral RNA-dependent RNA polymerase much the same as the ribosome is hypothesized to cycle.

Different mRNAs within the same cell have distinct lifetimes (stabilities). In bacterial cells, individual mRNAs can survive from seconds to more than an hour. However, the lifetime averages between 1 and 3 minutes, making bacterial mRNA much less stable than eukaryotic mRNA. In mammalian cells, mRNA lifetimes range from several minutes to days. The greater the stability of an mRNA the more protein may be produced from that mRNA. The limited lifetime of mRNA enables a cell to alter protein synthesis rapidly in response to its changing needs. There are many mechanisms that lead to the destruction of an mRNA, some of which are described below.

In general, in prokaryotes the lifetime of mRNA is much shorter than in eukaryotes. Prokaryotes degrade messages by using a combination of ribonucleases, including endonucleases, 3' exonucleases, and 5' exonucleases. In some instances, small RNA molecules (sRNA) tens to hundreds of nucleotides long can stimulate the degradation of specific mRNAs by base-pairing with complementary sequences and facilitating ribonuclease cleavage by RNase III. It was recently shown that bacteria also have a sort of 5' cap consisting of a triphosphate on the 5' end. Removal of two of the phosphates leaves a 5' monophosphate, causing the message to be destroyed by the exonuclease RNase J, which degrades 5' to 3'.

Inside eukaryotic cells, there is a balance between the processes of translation and mRNA decay. Messages that are being actively translated are bound by ribosomes, the eukaryotic initiation factors eIF-4E and eIF-4G, and poly(A)-binding protein. eIF-4E and eIF-4G block the decapping enzyme (DCP2), and poly(A)-binding protein blocks the exosome complex, protecting the ends of the message. The balance between translation and decay is reflected in the size and abundance of cytoplasmic structures known as P-bodies The poly(A) tail of the mRNA is shortened by specialized exonucleases that are targeted to specific messenger RNAs by a combination of cis-regulatory sequences on the RNA and trans-acting RNA-binding proteins. Poly(A) tail removal is thought to disrupt the circular structure of the message and destabilize the cap binding complex. The message is then subject to degradation by either the exosome complex or the decapping complex. In this way, translationally inactive messages can be destroyed quickly, while active messages remain intact. The mechanism by which translation stops and the message is handed-off to decay complexes is not understood in detail.

The presence of AU-rich elements in some mammalian mRNAs tends to destabilize those transcripts through the action of cellular proteins that bind these sequences and stimulate poly(A) tail removal. Loss of the poly(A) tail is thought to promote mRNA degradation by facilitating attack by both the exosome complex and the decapping complex. Rapid mRNA degradation via AU-rich elements is a critical mechanism for preventing the overproduction of potent cytokines such as tumor necrosis factor (TNF) and granulocyte-macrophage colony stimulating factor (GM-CSF). AU-rich elements also regulate the biosynthesis of proto-oncogenic transcription factors like c-Jun and c-Fos.

Eukaryotic messages are subject to surveillance by nonsense mediated decay (NMD), which checks for the presence of premature stop codons (nonsense codons) in the message. These can arise via incomplete splicing, V(D)J recombination in the adaptive immune system, mutations in DNA, transcription errors, leaky scanning by the ribosome causing a frame shift, and other causes. Detection of a premature stop codon triggers mRNA degradation by 5' decapping, 3' poly(A) tail removal, or endonucleolytic cleavage.

In metazoans, small interfering RNAs (siRNAs) processed by Dicer are incorporated into a complex known as the RNA-induced silencing complex or RISC. This complex contains an endonuclease that cleaves perfectly complementary messages to which the siRNA binds. The resulting mRNA fragments are then destroyed by exonucleases. siRNA is commonly used in laboratories to block the function of genes in cell culture. It is thought to be part of the innate immune system as a defense against double-stranded RNA viruses.

MicroRNAs (miRNAs) are small RNAs that typically are partially complementary to sequences in metazoan messenger RNAs. Binding of a miRNA to a message can repress translation of that message and accelerate poly(A) tail removal, thereby hastening mRNA degradation. The mechanism of action of miRNAs is the subject of active research.

There are other ways by which messages can be degraded, including non-stop decay and silencing by Piwi-interacting RNA (piRNA), among others.

Full length mRNA molecules have been proposed as therapeutics since the beginning of the biotech era but there was little traction until the 2010s, when Moderna Therapeutics was founded and managed to raise almost a billion dollars in venture funding in its first three years.

Theoretically, the administered mRNA sequence can cause a cell to make a protein, which in turn could directly treat a disease or could function as a vaccine; more indirectly the protein could drive an endogenous stem cell to differentiate in a desired way.

The primary challenges of RNA therapy center on delivering the RNA to directed cells, more even than determining what sequence to deliver. Naked RNA sequences will naturally degrade after preparation; they may trigger the body's immune system to attack them as an invader; and they are impermeable to the cell membrane. Once within the cell, they must then leave the cell's transport mechanism to take action within the cytoplasm, which houses the ribosomes that direct manufacture of proteins.





</doc>
<doc id="20237" url="https://en.wikipedia.org/wiki?curid=20237" title="Mount Saint Vincent University">
Mount Saint Vincent University

Mount Saint Vincent University, often referred to as The Mount, is a public primarily undergraduate university located in Halifax, Nova Scotia, Canada, and was established in 1873. Mount Saint Vincent offers undergraduate programs in Arts, Science, Education, and Professional Studies.The Mount has 13 graduate degrees in areas including Applied Human Nutrition, School Psychology, Child and Youth Study, Education, Family Studies and Gerontology, Public Relations and Women's Studies. The Mount offers a doctorate program, a Ph.D. in Educational Studies, through a joint-initiative with St. Francis Xavier University and Acadia University. The Mount offers more than 190 courses, over 10 full undergraduate degree programs and four graduate degree, programs online.

The university attracts many students in part because of its small class sizes, specialty programs, and location. The Mount has Canada Research Chairs in Gender Identity and Social Practices as well as Food Security and Policy Change. This institution is unique nationwide as it has a Chair in learning disabilities, Master of Public Relations program, Bachelor of Science in Communication Studies, and numerous other programs, faculty, and research initiatives.

Established by the Sisters of Charity of Saint Vincent de Paul as a women's college in 1873, the Mount was one of the few institutions of higher education for women in Canada at a time when women could not vote. The original purpose of the academy was to train novices and young sisters as teachers, but the Sisters also recognized a need to educate other young women. Over the ensuing years, the order developed a convent, schools, an orphanage, and health care facilities throughout the Halifax area, as well as across North America.

Architect Charles Welsford West designed the Romanesque chapel and annex (1903–05) at Mount St. Vincent Academy (now University). He served as the Architect, Nova Scotia Public Works & Mines 1932-1950.

By 1912, the Sisters of Charity of Saint Vincent de Paul recognized the need to offer greater opportunity through university education and adopted a plan to establish a college for young women. It was two years later in 1914 that the Sisters partnered with Dalhousie University, enabling Mount Saint Vincent to offer the first two years of a bachelor's degree program to be credited toward a Dalhousie degree.

In 1925, the Nova Scotia Legislature awarded the Mount the right to grant its own degrees, making it the only independent women's college in the British Commonwealth. By 1951, degrees were offered in Arts, Secretarial Science, Music, Home Economics, Library Science, Nursing and Education.

A new charter was granted in 1966 and the College became Mount Saint Vincent University, bringing forth the establishment of a Board of Governors and Senate. This was also a period of tremendous growth – with enrollment increases, new construction and new agreements. In 1967 the Mount began admitting men as students. The University continued to evolve with the expansion of programs during the 1970s and entered into several new fields, including Child Study, Public Relations, Gerontology, Tourism and Hospitality Management, Cooperative Education and Distance Education. In July 1988, the Sisters of Charity of Saint Vincent de Paul officially transferred ownership of the institution to the Board of Governors.

After a fire in 1951 burned down Mount Saint Vincent’s solitary building, the people of Halifax came together to support students by providing alternative accommodations for their classes. In recognition of the generosity of their community, the Sisters of Charity established a memorial holiday in appreciation of their gesture. Caritas Day, named after the Christian virtue of charity, takes place on the last Wednesday of January of each year. No classes are held on this day, and students are encouraged to volunteer their time instead. Caritas Day is an opportunity for students and faculty alike to connect with the Sisters of Charity and come together outside of class time in a setting that is both personally and academically beneficial.

Mount Saint Vincent University offers over 40 undergraduate degrees in the Arts, Sciences and Professional Studies. Professional Studies programs include Applied Human Nutrition, Business Administration, Child and Youth Study, Family Studies and Gerontology, Information Technology, Public Relations, Non-profit Leadership and Tourism and Hospitality Management. All undergraduate programs are work-experience eligible, meaning any Mount student can take part in a work placement (practicum, co-op, internship) as part of their program. 

The Mount also offers diplomas in Business Administration and Tourism & Hospitality Management, and certificates in Accounting, Business Administration, Marketing, Proficiency in French and Non-profit Leadership.

Following consolidation of post-secondary programs across Nova Scotia in the 1990s, the Mount became home to the only education program in the Halifax area. The faculty of Education is home to the only school psychology graduate program in Atlantic Canada. Graduates of this program are eligible to become registered psychologists in Nova Scotia and several other provinces in Canada.

The Mount houses 16 research centres and institutes.

The Department of Applied Human Nutrition has an accredited dietetic program. The University is accredited by a professional organization such as the Dietitians of Canada and the university's graduates may subsequently become registered dietitians.

Mount Saint Vincent University is the only university in Canada to offer a Master of Public Relations program (MPR). The MPR program graduated its first class in October 2009. The Canadian Public Relations Society (CPRS) recognizes MSVU's MPR program for excellence in PR education in its Pathways to the Profession guide.

Academic programs are supported by a wide variety of electronic and print research resources in the MSVU Library. Research services include drop-in reference assistance, research appointments and classroom workshops.

January 2019 marked the 40th anniversary of the Mount's co-operative education program. It is the longest-standing nationally accredited co-op program in the Maritime Provinces, offering an optional co-op program in 1979 for students in the Bachelor of Business Administration program. Four decades later, more than 8,000 Business Administration, Public Relations, and Tourism & Hospitality Management students have taken their learning from the classroom to the workplace, completing paid work terms in industries related to their field of study (today co-op is a required part of the Public Relations and Tourism & Hospitality Management degrees). Since 2014, the Mount Co-op Office has also enabled experiential opportunities for Arts and Science students through an Arts & Science Internship Program. 

Mount Saint Vincent University is home to the Centre for Women in Business, a not-for-profit university business development centre (UBDC), dedicated to assisting with entrepreneurial activities both within the university and throughout Nova Scotia. Founded in 1992 by the University's Department of Business & Tourism, this remains the only UBDC in Canada with a primary focus on women. The Centre has served more than 7500 clients over the past 18 years.

The Mount Saint Vincent University Art Gallery is located on the first floor of Seton Academic Centre. The gallery opened in 1971 as a resource to Mount Saint Vincent, communities served by the university, artists, Metro Halifax residents and art publics everywhere. Admission is always free of charge.

MSVU Art Gallery reflects the University's educational aims by devoting a significant part of its activities to the representation of women as cultural subjects and producers. Its exhibitions explore various forms of cultural production, highlighting the achievements of Nova Scotian artists and themes relevant to academic programs offered by the university.

The Mount was the first Nova Scotia university to add a wikuom to its campus facilities. First raised on June 12, 2017, the wikuom is a welcoming traditional Mi'kmaq space where both Indigenous and non-Indigenous communities can gather and learn together. 

The Mount is also home to the Aboriginal Student Centre (ASC), which is home to ASC staff who provide academic advising, counselling and other support services to students. The ASC hosts a number of events, including the Mount's Mid-Winter Feast, Blanket Exercises, Cultural Workshops, Mini-Mount Camps, and more.

Home to the Mystics, the Mount competes in the Atlantic Colleges Athletic Association (ACAA) in Women's & Men's Basketball, Women's & Men's Soccer, Cross Country and Women's Volleyball. The Mystics hold a championship titles in all sports, making them the most acclaimed team of the ACAA division. 


Notable graduates of the Mount include:


During the 1995 G7 summit, Mount Saint Vincent University awarded an honorary Doctor of Laws degree to Hillary Clinton.



</doc>
<doc id="20239" url="https://en.wikipedia.org/wiki?curid=20239" title="Minimal pair">
Minimal pair

In phonology, minimal pairs are pairs of words or phrases in a particular language, spoken or signed, that differ in only one phonological element, such as a phoneme, toneme or chroneme, and have distinct meanings. They are used to demonstrate that two phones are two separate phonemes in the language.

Many phonologists in the middle part of the 20th century had a strong interest in developing techniques for discovering the phonemes of unknown languages, and in some cases, they set up writing systems for the languages. The major work of Kenneth Pike on the subject is "Phonemics: a technique for reducing languages to writing". The minimal pair was an essential tool in the discovery process and was found by substitution or commutation tests.

As an example for English vowels, the pair "let" + "lit" can be used to demonstrate that the phones (in let) and (in lit) actually represent distinct phonemes and . An example for English consonants is the minimal pair of "pat" + "bat". The following table shows other pairs demonstrating the existence of various distinct phonemes in English. All of the possible minimal pairs for any language may be set out in the same way.

Phonemic differentiation may vary between different dialects of a language so a particular minimal pair in one accent may be a pair of homophones in another. That means not that one of the phonemes is absent in the homonym accent but only that it is not contrastive in the same range of contexts.

In addition to the minimal pairs of vowels and consonants provided above, others may be found:

Many languages show contrasts between long and short vowels and consonants. A distinctive difference in length is attributed by some phonologists to a unit called a chroneme. Thus, Italian has the following minimal pair that is based on long and short :

However, in such a case it is not easy to decide whether a long vowel or consonant should be treated as having an added chroneme or simply as a geminate sound with phonemes.

Classical Latin, German, some Italian dialects, almost all Uralic languages, Thai, and many other languages also have distinctive length in vowels. An example is the "cŭ/cū" minimal pair in the dialect that is spoken near Palmi (Calabria, Italy):

In some languages like Italian, word-initial consonants are geminated after certain vowel-final words in the same prosodic unit. Sometimes, the phenomenon can create some syntactic-gemination-minimal-pairs:
In the example, the graphical accent on "dà" is just a diacritical mark that does not change the pronunciation of the word itself. However, in some specific areas, like Tuscany, both phrases are pronounced and so can be distinguished only from the context.

Minimal pairs for tone contrasts in tone languages can be established; some writers refer to that as a contrast involving a toneme. For example, Kono distinguishes high tone and low tone on syllables:
Languages in which stress may occur in different positions within the word often have contrasts that can be shown in minimal pairs, as in Greek and Spanish:
English-speakers are able to hear the difference between, for example, "great ape" and "grey tape", but phonemically, the two phrases are identical: . The difference between the two phrases, which constitute a minimal pair, is said to be one of juncture. At the word boundary, a "plus juncture" /+/ is posited and said to be the factor conditioning allophones to allow distinctivity: the result is that "great ape" has an diphthong shortened by pre-fortis clipping and, since it is not syllable-initial, a with little aspiration (variously , , , , etc., depending on dialect); meanwhile in "grey tape", the has its full length and the is aspirated .

Only languages with allophonic differences associated with grammatical boundaries have juncture as a phonological element. It is claimed that French does not have juncture as a phonological element so, for example, "" (little holes) and "" (little wheels), phonemically both , are phonetically identical.

The principle of a simple binary opposition between the two members of a minimal pair may be extended to cover a minimal set in which a number of words differ from one another in terms of one phone in a particular position in the word. For example, the vowels , , , , of Swahili are shown to be distinct by the following set of words:
"pata" 'hinge', "peta" 'bend', "pita" 'pass', "pota" 'twist', "puta" 'thrash'. However, establishing such sets is not always straightforward and may require very complex study of multiple oppositions as expounded by, for example, Nikolai Trubetzkoy.

Minimal pairs were an important part of the theory of pronunciation teaching during its development in the period of structuralist linguistics, particularly in the 1940s and 1950s, and minimal pair drills were widely used to train students to discriminate among the phonemes of the target language. However, later writers have criticized the approach as being artificial and lacking in relevance to language learners' needs.

Some writers have claimed that learners are likely not to hear differences between phones if the difference is not a phonemic one. One of the objectives of contrastive analysis of languages' sound systems was to identify points of likely difficulty for language learners that would arise from differences in phoneme inventories between the native language and the target language. However, experimental evidence for this claim is hard to find, and the claim should be treated with caution.

In the past, signs were considered holistic forms without internal structure. However, the discovery in the mid-20th century that minimal pairs also exist in sign languages showed that sign languages have sublexical structure. Signs consist of phonemes, which are specifications for location, movement, handshape, orientation, and non-manual elements. When signs differ in only one of these specifications, they form a minimal pair. For instance, the German Sign Language signs shoes and socks are identical in form apart from their handshapes.




</doc>
<doc id="20242" url="https://en.wikipedia.org/wiki?curid=20242" title="Minestrone">
Minestrone

Minestrone (; ) is a thick soup of Italian origin made with vegetables, often with the addition of pasta or rice, sometimes both. Common ingredients include beans, onions, celery, carrots, stock, and tomatoes.

There is no set recipe for minestrone, since it can be usually made out of whatever vegetables one has. It can be vegetarian, contain meat, or contain an animal bone-based stock (such as chicken stock). Angelo Pellegrini, however, argued that the base of minestrone is bean broth, and that borlotti beans (also called Roman beans) "are the beans to use for genuine minestrone".

Some of the earliest origins of minestrone soup pre-date the expansion of the Latin tribes of Rome into what became the Roman Kingdom (later Roman Republic and Empire), when the local diet was "vegetarian by necessity" and consisted mostly of vegetables, such as onions, lentils, cabbage, garlic, broad beans, mushrooms, carrots, asparagus, and turnips.

During this time, the main dish of a meal would have been "pulte", a simple but filling porridge of spelt flour cooked in salt water, to which whatever vegetables that were available would have been added.

It was not until the 2nd century B.C., when Rome had conquered Italy and monopolized the commercial and road networks, that a huge diversity of products flooded the capital and began to change their diet, and by association, the diet of Italy most notably with the more frequent inclusion of meats, including as a stock for soups.

Spelt flour was also removed from soups, as bread had been introduced into the Roman diet by the Greeks, and "pulte" became a meal largely for the poor.

The ancient Romans recognized the health benefits of a simple or "frugal" diet (from the Latin "fruges", the common name given to cereals, vegetables and legumes) and thick vegetable soups and vegetables remained a staple.

Marcus Apicius's ancient cookbook "De Re Coquinaria" described "polus", a Roman soup dating back to 30 AD made up of farro, chickpeas, and fava beans, with onions, garlic, lard, and greens thrown in.

As eating habits and ingredients changed in Italy, so did minestrone. Apicius updates the "pultes" and "pulticulae" with fancy trimmings such as cooked brains and wine.

The introduction of tomatoes and potatoes from the Americas in the mid-16th century changed the soup by making available two ingredients which have since become staples.

The tradition of not losing rural roots continues today, and minestrone is now known in Italy as belonging to the style of cooking called "cucina povera" (literally "poor kitchen") meaning dishes that have rustic, rural roots, as opposed to "cucina nobile" or the cooking style of the aristocracy and nobles.

Like many Italian dishes, minestrone was probably originally not a dish made for its own sake. In other words, one did not gather the ingredients of minestrone with the intention of making minestrone. The ingredients were pooled from ingredients for other dishes, often side dishes or "contorni" plus whatever was left over, rather like the "pulte".

There are two schools of thought on when the recipe for minestrone became more formalized. One argues that in the 17th and 18th centuries minestrone emerged as a soup using exclusively fresh vegetables and was made for its own sake (meaning it no longer relied on left-overs), while the other school of thought argues that the dish had always been prepared exclusively with fresh vegetables for its own sake since the pre-Roman "pulte", but the name minestrone lost its meaning of being made with left-overs.

The word "minestrone", meaning a thick vegetable soup, is attested in English from 1871. It is from Italian "minestrone", the augmentative form of "minestra", "soup", or more literally, "that which is served", from "minestrare", "to serve" and cognate with "administer" as in "to administer a remedy".

Because of its unique origins and the absence of a fixed recipe, minestrone varies widely across Italy depending on traditional cooking times, ingredients, and season. Minestrone ranges from a thick and dense texture with very boiled-down vegetables, to a more brothy soup with large quantities of diced and lightly cooked vegetables; it may also include meats.

In modern Italian there are three words corresponding to the English word "soup": "zuppa", which is used in the sense of tomato soup, or fish soup; "minestra", which is used in the sense of a more substantial soup such as a vegetable soup, and also for "dry" soups, namely pasta dishes; and "minestrone", which means a very substantial or large soup or stew, though the meaning has now come to be associated with this particular dish.

"Minestrone alla Genovese" is a variant typical of Liguria, which contains greater use of herbs, including pesto.

"Minestra" is a variant from Malta, which prominently features "kunserva" (a thick tomato paste), potatoes, kohlrabi, cauliflower and sometimes spaghetti. 



</doc>
<doc id="20254" url="https://en.wikipedia.org/wiki?curid=20254" title="Miranda (moon)">
Miranda (moon)

Miranda, also designated Uranus V, is the smallest and innermost of Uranus's five round satellites. It was discovered by Gerard Kuiper on 16 February 1948 at McDonald Observatory in Texas, and named after Miranda from William Shakespeare's play "The Tempest". Like the other large moons of Uranus, Miranda orbits close to its planet's equatorial plane. Because Uranus orbits the Sun on its side, Miranda's orbit is perpendicular to the ecliptic and shares Uranus' extreme seasonal cycle.

At just 470 km in diameter, Miranda is one of the smallest closely observed objects in the Solar System that might be in hydrostatic equilibrium (spherical under its own gravity). The only close-up images of Miranda are from the "Voyager 2" probe, which made observations of Miranda during its Uranus flyby in January 1986. During the flyby, Miranda's southern hemisphere pointed towards the Sun, so only that part was studied.

Miranda probably formed from an accretion disc that surrounded the planet shortly after its formation, and, like other large moons, it is likely differentiated, with an inner core of rock surrounded by a mantle of ice. Miranda has one of the most extreme and varied topographies of any object in the Solar System, including Verona Rupes, a 20-kilometer-high scarp that is the highest cliff in the Solar System, and chevron-shaped tectonic features called "coronae". The origin and evolution of this varied geology, the most of any Uranian satellite, are still not fully understood, and multiple hypotheses exist regarding Miranda's evolution.

Miranda was discovered on 16 February 1948 by planetary astronomer Gerard Kuiper using the McDonald Observatory's Otto Struve Telescope. Its motion around Uranus was confirmed on 1 March 1948. It was the first satellite of Uranus discovered in nearly 100 years. Kuiper elected to name the object "Miranda" after the character in Shakespeare's "The Tempest", because the four previously discovered moons of Uranus, Ariel, Umbriel, Titania and Oberon, had all been named after characters of Shakespeare or Alexander Pope. However, the previous moons had been named specifically after fairies, whereas Miranda was a human. Subsequently, discovered satellites of Uranus were named after characters from Shakespeare and Pope, whether fairies or not. The moon is also designated Uranus V.

Of Uranus's five round satellites, Miranda orbits closest to it, at roughly 129,000 km from the surface; about a quarter again as far as its most distant ring. Its orbital period is 34 hours, and, like that of the Moon, is synchronous with its rotation period, which means it always shows the same face to Uranus, a condition known as tidal locking. Miranda's orbital inclination (4.34°) is unusually high for a body so close to its planet, and roughly ten times that of the other major Uranian satellites. The reason for this is still uncertain; there are no mean-motion resonances between the moons that could explain it, leading to the hypothesis that the moons occasionally pass through secondary resonances, which at some point in the past led to Miranda being locked for a time into a 3:1 resonance with Umbriel, before chaotic behaviour induced by the secondary resonances moved it out of it again. In the Uranian system, due to the planet's lesser degree of oblateness, and the larger relative size of its satellites, escape from a mean-motion resonance is much easier than for satellites of Jupiter or Saturn. Miranda's orbit is the most inclined of any of Uranus's large satellites, at 4.232°, it is 10-20 times that of Titania, Ariel and Umbriel and 73 times that of Oberon.

At 1.2 g/cm, Miranda is the least dense of Uranus's round satellites. That density suggests a composition of more than 60% water ice. Miranda's surface may be mostly water ice, though it is far rockier than its corresponding satellites in the Saturn system, indicating that heat from radioactive decay may have led to internal differentiation, allowing silicate rock and organic compounds to settle in its interior. Miranda is too small for any internal heat to have been retained over the age of the Solar System. Miranda is the least spherical of Uranus's satellites, with an equatorial diameter 3% wider than its polar diameter. Only water has been detected so far on Miranda's surface, though it has been speculated that methane, ammonia, carbon monoxide or nitrogen may also exist at 3% concentrations. These bulk properties are similar to Saturn's moon Mimas, though Mimas is smaller, less dense, and more oblate.

Precisely how a body as small as Miranda could have enough internal energy to produce the myriad geological features seen on its surface is not established with certainty, though the currently favoured hypothesis is that it was driven by tidal heating during a past time when it was in 3:1 orbital resonance with Umbriel. The resonance would have increased Miranda's orbital eccentricity to 0.1, and generated tidal friction due to the varying tidal forces from Uranus. As Miranda approached Uranus, tidal force increased; as it retreated, tidal force decreased, causing flexing that would have warmed Miranda's interior by 20 K, enough to trigger melting. The period of tidal flexing could have lasted for up to 100 million years. Also, if clathrate existed within Miranda, as has been hypothesised for the satellites of Uranus, it may have acted as an insulator, since it has a lower conductivity than water, increasing Miranda's temperature still further. Miranda may have also once been in a 5:3 orbital resonance with Ariel, which would have also contributed to its internal heating. However, the maximum heating attributable to the resonance with Umbriel was likely about three times greater.

Due to Uranus's near-sideways orientation, only Miranda's southern hemisphere was visible to "Voyager 2" when it arrived. The observed surface has patchwork regions of broken terrain, indicating intense geological activity in Miranda's past, and is criss-crossed by huge canyons, believed to be the result of extensional tectonics; as liquid water froze beneath the surface, it expanded, causing the surface ice to split, creating graben. The canyons are hundreds of kilometers long and tens of kilometers wide. Miranda also has the largest known cliff in the Solar System, Verona Rupes, which has a height of . Some of Miranda's terrain is possibly less than 100 million years old based on crater counts, while sizeable regions possess crater counts that indicate ancient terrain.

While crater counts suggest that the majority of Miranda's surface is old, with a similar geological history to the other Uranian satellites, few of those craters are particularly large, indicating that most must have formed after a major resurfacing event in its distant past. Craters on Miranda also appear to possess softened edges, which could be the result either of ejecta or of cryovolcanism. The temperature at Miranda's south pole is roughly 85 K, a temperature at which pure water ice adopts the properties of rock. Also, the cryovolcanic material responsible for the surfacing is too viscous to have been pure liquid water, but too fluid to have been solid water. Rather, it is believed to have been a viscous, lava-like mixture of water and ammonia, which freezes at 176 K (-97 °C), or perhaps ethanol.

Miranda's observed hemisphere contains three giant 'racetrack'-like grooved structures called coronae, each at least wide and up to deep, named Arden, Elsinore and Inverness after locations in Shakespeare's plays. Inverness is lower in altitude than the surrounding terrain (though domes and ridges are of comparable elevation), while Elsinore is higher, The relative sparsity of craters on their surfaces means they overlay the earlier cratered terrain. The coronae, which are unique to Miranda, initially defied easy explanation; one early hypothesis was that Miranda, at some time in its distant past, (prior to any of the current cratering) had been completely torn to pieces, perhaps by a massive impact, and then reassembled in a random jumble. The heavier core material fell through the crust, and the coronae formed as the water re-froze.

However, the current favoured hypothesis is that they formed via extensional processes at the tops of diapirs, or upwellings of warm ice from within Miranda itself. The coronae are surrounded by rings of concentric faults with a similar low-crater count, suggesting they played a role in their formation. If the coronae formed through downwelling from a catastrophic disruption, then the concentric faults would present as compressed. If they formed through upwelling, such as by diapirism, then they would be extensional tilt blocks, and present extensional features, as current evidence suggests they do. The concentric rings would have formed as ice moved away from the heat source. The diapirs may have changed the density distribution within Miranda, which could have caused Miranda to reorient itself, similar to a process believed to have occurred at Saturn's geologically active moon Enceladus. Evidence suggests the reorientation would have been as extreme as 60 degrees from the sub-Uranian point. The positions of all the coronae require a tidal heating pattern consistent with Miranda being solid, and lacking an internal liquid ocean. It is believed through computer modelling that Miranda may have an additional corona on the unimaged hemisphere.

Miranda's apparent magnitude is +16.6, making it invisible to many amateur telescopes. Virtually all known information regarding its geology and geography was obtained during the flyby of Uranus made by "Voyager 2" on Jan 25 1986, The closest approach of "Voyager 2" to Miranda was —significantly less than the distances to all other Uranian moons. Of all the Uranian satellites, Miranda had the most visible surface. The discovery team had expected Miranda to resemble Mimas, and found themselves at a loss to explain the moon's unique geography in the 24-hour window before releasing the images to the press. In 2017, as part of its Planetary Science Decadal Survey, NASA evaluated the possibility of an orbiter to return to Uranus some time in the 2020s. Uranus was the preferred destination over Neptune due to favourable planetary alignments meaning shorter flight times.



</doc>
<doc id="20257" url="https://en.wikipedia.org/wiki?curid=20257" title="Mars in fiction">
Mars in fiction

Fictional representations of Mars have been popular for over a century. Interest in Mars has been stimulated by the planet's dramatic red color, by early scientific speculations that its surface conditions might be capable of supporting life, and by the possibility that Mars could be colonized by humans in the future. Almost as popular as stories about Mars are stories about Martians engaging in activity (frequently invasions) away from their home planet.

In the 20th century, actual spaceflights to the planet Mars, including seminal events such as the first artificial object to impact the surface of Mars in 1971, and then later the first landing of "the first mechanized device to successfully operate on Mars" in 1976 (in the Viking program by the United States), inspired a great deal of interest in Mars-related fiction. Exploration of the planet has continued in the 21st century on to the present day.

The following works of fiction deal with the planet itself, with any assumed Martian civilization as part of its planetary landscape.Mars has been seen as the perfect distance away from Earth to create the idea of a different life. As this allowed for early works to fuel the minds of what Mars could hold. The ideas of Mars as science fiction, would first start with Giovanni Schiaparelli in 1877. The ideas of Mars will grow and change with new information changing the way Mars would be seen in the science fiction world.

Several early modern writers, including Athanasius Kircher (1602–1680) and Emanuel Swedenborg (1688-1772), hypothesized contact with Mars. Early science fiction about Mars often involved the first voyages to the planet, sometimes as an invasion force, more often for the purposes of exploration.






By the 1930s, stories about reaching Mars had become somewhat trite, and the focus shifted to Mars as an alien landscape. In the following stories, human contact and basic exploration had taken place sometime in the past; Mars is a setting rather than a goal.





Mariner 4 in July 1965 found that Mars—contrary to expectations—is heavily cratered, with a very thin atmosphere. No canals were found; while scientists did not believe that Mars was a moist planet, the lack of surface water surprised them. Science fiction had so influenced real explorations of the planet, however—Carl Sagan was among the many fans who became scientists—that after Mariner 9 in 1971-1972, craters were named after Wells, Burroughs, and other authors. The Mariner and Viking space probes confirmed that the Martian environment is extremely hostile to life. By the 1970s, the ideas of canals and ancient civilizations had to be abandoned.

Authors soon began writing stories based on the new Mars (frequently treating it as a desert planet). Most of these works feature humans struggling to tame the planet, and some of them refer to terraforming (using technology to transform a planet's environment to be Earthlike).

A common theme, particularly among American writers, is that of a Martian colony fighting for independence from Earth. It appeared already in Heinlein's "Red Planet" and is a major plot element in Greg Bear's "Moving Mars" and Kim Stanley Robinson's "Mars" trilogy. It is also part of the plot of the movie "Total Recall" and the television series "Babylon 5". Many video games also use this concept, such as the "Red Faction" and "Zone of the Enders" series, and "". A historical rebellion of Mars against Earth is also mentioned in the "Star Trek" series of novels, which are not considered canon.

In the decades following Mariner and Apollo, the once-popular subgenre of realistic stories about a first expedition to Mars fell out of fashion, possibly due to the failure of the Apollo Program to continue on to Mars. The early 1990s saw a revival and re-envisioning of realistic novels about Mars expeditions. Early novels in this renaissance were Jack Williamson's novel "Beachhead" and Ben Bova's novel "Mars" (both 1992), which envisioned large-scale expeditions to Mars according to the thinking of the 1990s. These were followed by Gregory Benford's "The Martian Race" (1999), Geoffrey A. Landis's "Mars Crossing" (2000), and Robert Zubrin's "First Landing" (2002), which took as their starting points the smaller and more focused expedition strategies evolved in the late 1990s, mostly building on the concepts of Mars Direct.






Several post-Mariner works are homages to the older phase of Mars fiction, circumventing the scientific picture of a dry and lifeless Mars with an unbreathable atmosphere through such science fiction generic staples as positing its future terraforming, or creating alternate history versions of Mars, where Burroughs' Barsoom, Bradbury's "Martian Chronicles" or "The War of the Worlds" are literal truth.

Nostalgia for the older Mars also frequently appears in comics and role-playing games, particularly of the steampunk genre:


In the following works of fiction, the Martian setting is of secondary importance to the work as a whole.

The "Doctor Who" television series has Mars as the uninhabitable homeworld of the Ice Warriors, a recurring adversary of the Second and Third Doctors from 1967 to 1974. In "Pyramids of Mars" (1975), the Fourth Doctor defeats Sutekh, last of the Osirians, who had been imprisoned for his crimes beneath a pyramid, with a signal to keep him paralyzed sent from a Martian pyramid. In "The Waters of Mars" (2009), an episode set on the planet itself, the Tenth Doctor implies that the Ice Warriors have become extinct. (This episode also introduces a viral, water-borne Martian named the Flood.) The episode is set in 2059, and implies that the first human colony on Mars will arrive in 2057, two years before the episode is set (as told in dialogue).






The Martian is a favorite character of classical science fiction; he was frequently found away from his home planet, often invading Earth, but sometimes simply a lonely character representing alienness from his surroundings. Martians, other than human beings transplanted to Mars, became rare in fiction after Mariner, except in exercises of deliberate nostalgia – more frequently in some genres, such as comics and animation, than in written literature.



</doc>
<doc id="20258" url="https://en.wikipedia.org/wiki?curid=20258" title="McIntosh (apple)">
McIntosh (apple)

The McIntosh ( ), McIntosh Red, or colloquially the Mac, is an apple cultivar, the national apple of Canada. The fruit has red and green skin, a tart flavour, and tender white flesh, which ripens in late September. In the 20th century it was the most popular cultivar in Eastern Canada and New England, and is considered an all-purpose apple, suitable both for cooking and eating raw. Apple Inc. employee Jef Raskin named the Macintosh line of personal computers after the fruit.

John McIntosh discovered the original McIntosh sapling on his Dundela farm in Upper Canada in 1811. He and his wife bred it, and the family started grafting the tree and selling the fruit in 1835. In 1870, it entered commercial production, and became common in northeastern North America after 1900. While still important in production, the fruit's popularity fell in the early 21st century in the face of competition from varieties such as the Gala. According to the US Apple Association website it is one of the fifteen most popular apple cultivars in the United States.

The McIntosh or McIntosh Red (nicknamed the "Mac"), is the most popular apple cultivar in eastern Canada and the northeastern United States. It also sells well in eastern Europe.

A spreading tree that is moderately vigorous, the McIntosh bears annually or in alternate years. The tree is hardy to at least USDA Hardiness zone 4a, or . 50% or more of its flowers die at or below.

The McIntosh apple is a small- to medium-sized round fruit with a short stem. It has a red and green skin that is thick, tender, and easy to peel. Its white flesh is sometime tinged with green or pink and is juicy, tender, and firm, soon becoming soft. The flesh is easily bruised.

The fruit is considered "all-purpose", suitable both for eating raw and for cooking. It is used primarily for dessert, and requires less time to cook than most cultivars. It is usually blended when used for juice.

The fruit grows best in cool areas where nights are cold and autumn days are clear; otherwise, it suffers from poor colour and soft flesh, and tends to fall from the tree before harvest. It stores for two to three months in air, but is prone to scald, flesh softening, chilling sensitivity, and coprinus rot. It can become mealy when stored at temperatures below . The fruit is optimally stored in a controlled atmosphere in which temperatures are between , and air content is 1.5–4.5% oxygen and 1–5% carbon dioxide; under such conditions, the McIntosh will keep for five to eight months.

The McIntosh is most commonly cultivated in Canada, the United States, and eastern Europe. The parentage of the McIntosh is unknown, but the Snow Apple (or Fameuse), Fall St Lawrence, and Alexander have been speculated. It is one of the top five apple cultivars used in cloning, and research indicates the McIntosh combines well for winter hardiness.

If unsprayed, the McIntosh succumbs easily to apple scab, which may lead to entire crops being unmarketable. It has generally low susceptibility to fire blight, powdery mildew, cedar-apple rust, quince rust, and hawthorn rust. It is susceptible to fungal diseases such as "Nectria" canker, brown rot, black rot, race 1 of apple rust (but resists race 2). It is moderately resistant to "Pezicula" bark rot and "Alternaria" leaf blotch, and resists brown leaf spot well.

The McIntosh is one of the most common cultivars used in cloning; a 1996 study found that the McIntosh was a parent in 101 of 439 cultivars selected, more than any other founding clone. It was used in over half of the Canadian cultivars selected, and was used extensively in the United States and Eastern Europe as well; rarely was it used elsewhere. Offspring of the McIntosh include: the Jersey Black hybrid the Macoun, the Newtown Pippin hybrid the Spartan, the Cortland; the Empire; the Jonamac, the Jersey Mac, the Lobo, the Melba, the Summered, the Tydeman's Red, and possibly the Paula Red.

Apple trees were introduced to Canada at the Habitation at Port-Royal (modern Port Royal, Annapolis County, Nova Scotia) as early as 1606 by French settlers. Following its introduction, apple cultivation spread inland.

The McIntosh's discoverer, John McIntosh (1777 – ), left his native Mohawk Valley home in New York State in 1796 to follow his love, Dolly Irwin, who had been taken to Upper Canada by her Loyalist parents. She had died by the time he found her, but he settled as a farmer in Upper Canada. He married Hannah Doran in 1801, and they farmed along the Saint Lawrence River until 1811, when McIntosh exchanged the land he had with his brother-in-law Edward Doran for a plot in Dundela.

While clearing the overgrown plot McIntosh discovered some wild apple seedlings on his farm. He transplanted the seedlings next to his house. One of the seedlings bore particularly good fruit. The McIntosh grandchildren dubbed the fruit it produced "Granny's apple", as they often saw their grandmother taking care of the tree in the orchard. McIntosh was selling seedlings from the tree by 1820, but they did not produce fruit of the quality of the original.

John McIntosh's son Allan (1815–1899) learned grafting about 1835; with this cloning the McIntoshes could maintain the distinctive properties of the fruit of the original tree. Allan and brother Sandy (1825–1906), nicknamed "Sandy the Grafter", increased production and promotion of the cultivar. Earliest sales were in 1835, and in 1836 the cultivar was renamed the "McIntosh Red"; it entered commercial production in 1870. The apple became popular after 1900, when the first sprays for apple scab were developed. A house fire damaged the original McIntosh tree in 1894; it last produced fruit in 1908, and died and fell over in 1910.

Horticulturist William Tyrrell Macoun of the Central Experimental Farm in Ottawa is credited with popularizing the McIntosh in Canada. He stated the McIntosh needed "no words of praise", that it was "one of the finest appearing and best dessert apples grown". The Macoun, a hybrid of the McIntosh and Jersey Black grown by the Agricultural Experiment Station in Geneva, NY, was named for him in 1923. In the northeastern United States the McIntosh replaced a large number of Baldwins that were killed in a severe winter in 1933–34. In the late 1940s, Canadian ambassador to the United Nations Andrew McNaughton told Soviet Minister for Foreign Affairs Andrei Gromyko that the McIntosh Red was Canada's best apple.

The McIntosh made up 40% of the Canadian apple market by the 1960s; and at least thirty varieties of McIntosh hybrid were known by 1970. Its popularity later waned in the face of competition from foreign imports; in the first decade of the 21st century, the Gala accounted for 33% of the apple market in Ontario to the McIntosh's 12%, and the Northern Spy had become the preferred apple for pies. Production remained important to Ontario, however, as of McIntoshes were produced in 2010.

The original tree discovered by John McIntosh bore fruit for more than ninety years, and died in 1910. Horticulturalists from the Upper Canada Village heritage park saved cuttings from the last known first-generation McIntosh graft before it died in 2011 for producing clones.

The McIntosh has been designated the national apple of Canada. A popular subscription funded a plaque placed from the original McIntosh tree in 1912. The Ontario Archaeological and Historic Sites Board replaced the plaque with a more descriptive one in 1962, and the Historic Sites and Monuments Board of Canada put up another in a park nearby in 2001, by a painted mural commemorating the fruit.

Apple Inc. employee Jef Raskin named the Macintosh line of personal computers after the McIntosh. He deliberately misspelled the name to avoid conflict with the hi-fi equipment manufacturer McIntosh Laboratory. Apple's attempt in 1982 to trademark the name Macintosh was nevertheless denied due to the phonetic similarity between Apple's product and the name of the hi-fi manufacturer. Apple licensed the rights to the name in 1983, and bought the trademark in 1986.

In 1995 the Royal Canadian Mint commissioned Toronto artist Roger Hill to design a commemorative silver dollar for release in 1996. Mint engraver Sheldon Beveridge engraved the image of a group of three McIntoshes and a McIntosh blossom which adorn one side with a ribbon naming the variety. An inscription on the edge reads "1796 Canada Dollar 1996". Issued sheathed in a silver cardboard sleeve in a black leatherette case, 133,779 pieces of the proof were sold, as well as 58,834 pieces of the uncirculated version in a plastic capsule and silver sleeve.




</doc>
<doc id="20261" url="https://en.wikipedia.org/wiki?curid=20261" title="Machete">
Machete

A machete (; ) is a broad blade used either as an implement like an axe, or in combat like a short sword. The blade is typically long and usually under thick. In the Spanish language, the word is a diminutive form of the word "macho", which was used to refer to sledgehammers. In the English language, an equivalent term is matchet, though it is less commonly used. In the English-speaking Caribbean, such as Jamaica, Barbados, Guyana, and Grenada and in Trinidad and Tobago, the term "cutlass" is used for these agricultural tools.

In various tropical and subtropical countries, the machete is frequently used to cut through rainforest undergrowth and for agricultural purposes (e.g. cutting sugar cane). Besides this, in Latin America a common use is for such household tasks as cutting large foodstuffs into pieces—much as a cleaver is used—or to perform crude cutting tasks, such as making simple wooden handles for other tools. It is common to see people using machetes for other jobs, such as splitting open coconuts, yard work, removing small branches and plants, chopping animals' food, and clearing bushes.

Machetes are often considered tools and used by adults. However, many hunter–gatherer societies and cultures surviving through subsistence agriculture begin teaching babies to use sharp tools, including machetes, before their first birthdays.

Because the machete is common in many tropical countries, it is often the weapon of choice for uprisings. For example, the Boricua Popular Army are unofficially called "macheteros" because of the machete-wielding laborers of sugar cane fields of past Puerto Rico.

Many of the killings in the 1994 Rwandan genocide were performed with machetes, and they were the primary weapon used by the Interahamwe militias there. Machetes were also a distinctive tool and weapon of the Haitian "Tonton Macoute".

In 1762, the Kingdom of Great Britain invaded Cuba in the Battle of Havana, and peasant guerrillas led by Pepe Antonio, a Guanabacoa councilman, used machetes in the defense of the city. The machete was also the most iconic weapon during the independence wars in that country (1868–1898), although it saw limited battlefield use. Carlos Manuel de Céspedes, owner of the sugar refinery "La Demajagua" near Manzanillo, freed his slaves on 10 October 1868. He proceeded to lead them, armed with machetes, in revolt against the Spanish government. The first cavalry charge using machetes as the primary weapon was carried out on 4 November 1868 by Máximo Gómez, a sergeant born in the Dominican Republic, who later became the general in chief of the Cuban Army.

The machete was (and still is) a common side arm and tool for many ethnic groups in West Africa. Machetes in this role are referenced in Chinua Achebe's "Things Fall Apart".

Some countries have a name for the blow of a machete; the Spanish "machetazo" is sometimes used in English. In the British Virgin Islands, Grenada, Jamaica, Saint Kitts and Nevis, Barbados, Saint Lucia, and Trinidad and Tobago, the word "planass" means to hit someone with the flat of the blade of a machete or cutlass. To strike with the sharpened edge is to "chop". Throughout the Caribbean, the term 'cutlass' refers to a laborers' cutting tool.

The Brazilian Army's Instruction Center on Jungle Warfare developed a machete with a blade in length and a very pronounced clip point. This machete is issued with a 5-inch Bowie knife and a sharpening stone in the scabbard; collectively called a "jungle kit" ("Conjunto de Selva" in Portuguese); it is manufactured by Indústria de Material Bélico do Brasil (IMBEL).

Many fictitious slashers have used it as a weapon in horror and fighting movies, the most well known and notorious being Jason Voorhees, from the "Friday the 13th" movie series and Quincy, from the Downtown Defenders (franchise).

The tsakat is used primarily in southern Armenia and Artsakh when clearing areas or hiking. It's especially well suited for clearing the plentiful blackberry plants in these regions.

The "panga" or "tapanga" is a variant used in East and Southern Africa. This name may be of Swahili etymology; not to be confused with the Panga fish. The "panga" blade broadens on the backside and has a length of . The upper inclined portion of the blade may be sharpened.

This tool has been used as a weapon: during the Mau Mau Uprising; in the Rwandan Genocide; in South Africa particularly in the 1980s and early 1990s when the former province of Natal was wracked by conflict between the African National Congress and the Zulu-nationalist Inkatha Freedom Party.

In the Philippines, the "bolo" is a very similar tool, but with the blade swelling just before the tip to make the knife even more efficient for chopping. Variations include the longer and more pointed "iták" intended for combat; this was used during the Philippine Revolution against the Spanish colonial authorities, later becoming a signature weapon of guerrillas in the Philippine–American War. Filipinos still use the "bolo" for everyday tasks, such as clearing vegetation and chopping various large foodstuffs. These are also commonly found in most Filipino kitchens, with some sets displayed on the walls and other sets for less practical use. The "bolo" is also used in training in "eskrima", the indigenous martial art of the Philippines.

Other similar tools include the "parang" and the "golok" (from Malaysia and Indonesia); however, these tend to have shorter, thicker blades with a primary grind, and are more effective on woody vegetation. The Nepalese "kukri" is a curved blade that is often used for similar tasks.

In Thailand, more variations exist, such as the "e-nep", or "nep", which translates as "leaf" (มีดเหน็บ). It may resemble some forms of Muslim blades like the "jambiya", or the Nepali "khukuri", having aspects of both with the up-swept tip and protruding belly. Another design found in Thailand is the "e-toh", which is prominent in Southern China, Laos, and other northern parts of South East Asia. Generally, "e-tohs" must have forward weighted tips, and are used around the home for splitting stove wood or chopping bone. The Chinese "dao", with its triangular tip, is found in Thailand as the "hua-tad" (หัวแตด), which translates roughly as "head chopper". The most common blade in Thailand is called the "pra", (พร้า) it can describe long straight designs, or billhook designs. The primary purpose of a "pra" is farm work and clearing vegetation.

In the various regions of Ecuador, it is still used as an everyday tool in agricultural labors, such as clearing, chopping, cutting and felling. In the Pacific coast region, the machete has a long history of use and can be seen as part of the everyday dress of the rural male inhabitants, especially in the provinces of Manabi, Los Rios and Guayas. In its day, the machete and the skills related to it were seen as a token of manliness, and it was carried, sword-like, in ornamented sheaths made out of leather or in sashes around the waist. Its use was not limited to agriculture: it also had a double role as a ready-to-hand weapon for self-defense or attack. Although modern laws in Ecuador now prohibit its use as a weapon, there are still cases of vicious fighting or intimidation related to it. Being a part of the male dress, it also has a part in the cultural expressions of the coastal rural regions of Ecuador, such as dances, horse taming contests and skill exhibitions.

In the southern Brazilian state of Rio Grande do Sul, the machete made by Spanish is largely used. It is used to clear paths through the bush, and was used to fight against the Brazilian Empire in the Ragamuffin War. There, the machete is called "facão" or "facón" (literally "big knife"). Today, this region has a dance called the "dança dos facões" (machetes' dance) in which the dancers, who also always men, knock their machetes while dancing, simulating a battle. "Maculelê", an Afro-Brazilian dance and martial art, can also be performed with "facões". This practice began in the city of Santo Amaro, Bahia, in the northeastern part of the country.

In southern Mexico and Central America it is widely used to clear bush and often hundreds of "macheteros" are contracted to assist in clearing paths for the construction of new roads or structures. Many people in the rural regions own machetes to clear the constant overgrowth of jungle bush. In the recent drug cartel wars of the region, many homicides and decapitations are suspected of being committed with machetes or similar tools.
The "taiga" is a machete of Russian origin that combines the functions of machetes, axes, knives, saws, and shovels into one tool. It is easily distinguished by the large swell at the end of the blade to facilitate chopping. The "taiga" is used by military air and special forces, including the "Spetsnaz".

The modern machete is very similar to some forms of the medieval falchion, a short sword popular from the 13th century onwards. The cutting edge of the falchion is curved, widening toward the point, and has a straight, unsharpened back edge. The machete differs from the falchion mainly in the lack of a guard and a simpler hilt, though some machetes do have a guard for greater hand protection during work.

The "kopis" is an ancient Greek weapon comparable to the machete. The "makhaira" is also similar, but was intended primarily to be a weapon rather than a tool.

The "seax" is a Germanic weapon that is also similar in function, although different in shape.

The "kukri" is a Nepalese curved blade used for many purposes similar to the machete.

The "parang" is a Malaysian knife that many machetes are based on.

The "grosse messer" is a large "medieval" knife, employed both as a tool and as a weapon.

The "dao" is a traditional Chinese weapon resembling the machete. It is also known as "The General of All Weapons". 

The fascine knife is a somewhat similar tool and weapon used by European armies throughout the late 18th to early 20th centuries. The Spanish Army called its fascine knives "machetes". Whereas infantry were usually issued short sabres as side arms, engineers and artillerymen often received fascine knives, as besides being side arms they also served as useful tools for the construction of fortifications and other utilitarian tasks. They differ from machetes in that they generally have far thicker, tapered blades optimized for chopping European vegetation (the thin, flat blade of the machete is better for soft plants found in tropical environments), sword-like hilts and guards, and sometimes a sawback-blade. Some later models could be fixed to rifles as bayonets as well.

The katana, typically acquired through trade, was used by the Ainu people in a machete-like fashion rather than a weapon as it was originally intended to be.

Both the materials used and the shape of the machete itself are important to make a good machete. In the past, the most famous manufacturer of machetes in Latin America and the Spanish-speaking Caribbean was Collins Company of Collinsville, Connecticut. The company was founded as Collins & Company in 1826 by Samuel W. Collins to make axes. Its first machetes were sold in 1845 and became so famous that all good machetes were called "un Collins". In the English-speaking Caribbean, Robert Mole & Sons of Birmingham, England, was long considered the manufacturer of agricultural cutlasses of the best quality. Some Robert Mole blades survive as souvenirs of travelers to Trinidad, Jamaica, and, less commonly, St. Lucia.

Since the 1950s, however, manufacturing shortcuts have resulted in a quality decline of machetes. Today, most modern factory-made machetes are of very simple construction, consisting of a blade and full-length tang punched from a single piece of flat steel plate of uniform thickness (and thus lack a primary grind), and a simple grip of two plates of wood or plastic bolted or riveted together around the tang. Finally, both sides are ground down to a rough edge so that the purchaser can sharpen the blade to their specific geometry using a file. These machetes are occasionally provided with a simple cord loop as a sort of lanyard, and a canvas scabbard—although in some regions where machetes are valuable, commonly used tools, the users may make decorative leather scabbards for them.

Toughness is important because of the twisting and impact forces that the relatively thin blade may encounter, while edge retention is secondary. Medium to high carbon spring steels, such as 1050 to 1095, are well suited to this application (with better machetes using the latter), and are relatively easy to sharpen. Most stainless steel machetes should be avoided, as many high-carbon stainless-steel machetes cannot stand up to repeated impacts, and will easily break if abused.

In comparison to most other knives, which are commonly heat treated to a very high degree of hardness, many machete blades are tempered to maximum toughness, often nearly spring tempered. This results in a tougher blade, more resistant to chipping and breaking, with an edge that is easier to sharpen but does not retain sharpness as well, due to its lower hardness.

A properly constructed machete will have a convex or flat primary bevel from the spine to the edge, which is formed by a secondary bevel. Better machetes will also have a slight distal taper.

Colombia is the largest exporter of machetes worldwide.

The flag of Angola features a machete, along with a cog-wheel.

The machete is also a performance weapon used in variations of the Brazilian martial dance called "maculelê", often practiced by practitioners of "capoeira". Machetes are a distinctive characteristic in the folkloric dances of the state of Nayarit. It is also seen in the state of Durango, in the folkloric dance called "Danza de los Machetes" which consists of blind-folded dancers juggling machetes and pitching them at increasing speeds between one another.

Traditional forms of fencing with machetes include Colombian grima in Colombia, "Juego del garrote" in Venezuela, and "tire machèt" in Haiti.

https://bestmachete.co/best-machete


</doc>
<doc id="20264" url="https://en.wikipedia.org/wiki?curid=20264" title="Mushroom">
Mushroom

A mushroom, or toadstool, is the fleshy, spore-bearing fruiting body of a fungus, typically produced above ground, on soil, or on its food source.

The standard for the name "mushroom" is the cultivated white button mushroom, "Agaricus bisporus"; hence the word "mushroom" is most often applied to those fungi (Basidiomycota, Agaricomycetes) that have a stem (stipe), a cap (pileus), and gills (lamellae, sing. lamella) on the underside of the cap. "Mushroom" also describes a variety of other gilled fungi, with or without stems, therefore the term is used to describe the fleshy fruiting bodies of some Ascomycota. These gills produce microscopic spores that help the fungus spread across the ground or its occupant surface.

Forms deviating from the standard morphology usually have more specific names, such as "bolete", "puffball", "stinkhorn", and "morel", and gilled mushrooms themselves are often called "agarics" in reference to their similarity to "Agaricus" or their order Agaricales. By extension, the term "mushroom" can also refer to either the entire fungus when in culture, the thallus (called a mycelium) of species forming the fruiting bodies called mushrooms, or the species itself.

Identifying mushrooms requires a basic understanding of their macroscopic structure. Most are Basidiomycetes and gilled. Their spores, called basidiospores, are produced on the gills and fall in a fine rain of powder from under the caps as a result. At the microscopic level, the basidiospores are shot off basidia and then fall between the gills in the dead air space. As a result, for most mushrooms, if the cap is cut off and placed gill-side-down overnight, a powdery impression reflecting the shape of the gills (or pores, or spines, etc.) is formed (when the fruit body is sporulating). The color of the powdery print, called a spore print, is used to help classify mushrooms and can help to identify them. Spore print colors include white (most common), brown, black, purple-brown, pink, yellow, and creamy, but almost never blue, green, or red.

While modern identification of mushrooms is quickly becoming molecular, the standard methods for identification are still used by most and have developed into a fine art harking back to medieval times and the Victorian era, combined with microscopic examination. The presence of juices upon breaking, bruising reactions, odors, tastes, shades of color, habitat, habit, and season are all considered by both amateur and professional mycologists. Tasting and smelling mushrooms carries its own hazards because of poisons and allergens. Chemical tests are also used for some genera.

In general, identification to genus can often be accomplished in the field using a local mushroom guide. Identification to species, however, requires more effort; one must remember that a mushroom develops from a button stage into a mature structure, and only the latter can provide certain characteristics needed for the identification of the species. However, over-mature specimens lose features and cease producing spores. Many novices have mistaken humid water marks on paper for white spore prints, or discolored paper from oozing liquids on lamella edges for colored spored prints.

Typical mushrooms are the fruit bodies of members of the order Agaricales, whose type genus is "Agaricus" and type species is the field mushroom, "Agaricus campestris". However, in modern molecularly defined classifications, not all members of the order Agaricales produce mushroom fruit bodies, and many other gilled fungi, collectively called mushrooms, occur in other orders of the class Agaricomycetes. For example, chanterelles are in the Cantharellales, false chanterelles such as "Gomphus" are in the Gomphales, milk-cap mushrooms ("Lactarius", "Lactifluus") and russulas ("Russula"), as well as "Lentinellus", are in the Russulales, while the tough, leathery genera "Lentinus" and "Panus" are among the Polyporales, but "Neolentinus" is in the Gloeophyllales, and the little pin-mushroom genus, "Rickenella", along with similar genera, are in the Hymenochaetales.

Within the main body of mushrooms, in the Agaricales, are common fungi like the common fairy-ring mushroom, shiitake, enoki, oyster mushrooms, fly agarics and other Amanitas, magic mushrooms like species of "Psilocybe", paddy straw mushrooms, shaggy manes, etc.

An atypical mushroom is the lobster mushroom, which is a deformed, cooked-lobster-colored parasitized fruitbody of a "Russula" or "Lactarius", colored and deformed by the mycoparasitic Ascomycete "Hypomyces lactifluorum".

Other mushrooms are not gilled, so the term "mushroom" is loosely used, and giving a full account of their classifications is difficult. Some have pores underneath (and are usually called boletes), others have spines, such as the hedgehog mushroom and other tooth fungi, and so on. "Mushroom" has been used for polypores, puffballs, jelly fungi, coral fungi, bracket fungi, stinkhorns, and cup fungi. Thus, the term is more one of common application to macroscopic fungal fruiting bodies than one having precise taxonomic meaning. Approximately 14,000 species of mushrooms are described.

The terms "mushroom" and "toadstool" go back centuries and were never precisely defined, nor was there consensus on application. Between 1400 and 1600 AD, the terms "mushrom, mushrum, muscheron, mousheroms, mussheron, or musserouns" were used.

The term "mushroom" and its variations may have been derived from the French word "mousseron" in reference to moss ("mousse"). Delineation between edible and poisonous fungi is not clear-cut, so a "mushroom" may be edible, poisonous, or unpalatable.

Cultural or social phobias of mushrooms and fungi may be related. The term "fungophobia" was coined by William Delisle Hay of England, who noted a national superstition or fear of "toadstools".

The word "toadstool" has apparent analogies in Dutch "padde(n)stoel" (toad-stool/chair, mushroom) and German "Krötenschwamm" (toad-fungus, alt. word for panther cap). In German folklore and old fairy tales, toads are often depicted sitting on toadstool mushrooms and catching, with their tongues, the flies that are said to be drawn to the "Fliegenpilz", a German name for the toadstool, meaning "flies' mushroom". This is how the mushroom got another of its names, "Krötenstuhl" (a less-used German name for the mushroom), literally translating to "toad-stool".

A mushroom develops from a nodule, or pinhead, less than two millimeters in diameter, called a primordium, which is typically found on or near the surface of the substrate. It is formed within the mycelium, the mass of threadlike hyphae that make up the fungus. The primordium enlarges into a roundish structure of interwoven hyphae roughly resembling an egg, called a "button". The button has a cottony roll of mycelium, the universal veil, that surrounds the developing fruit body. As the egg expands, the universal veil ruptures and may remain as a cup, or volva, at the base of the stalk, or as warts or volval patches on the cap. Many mushrooms lack a universal veil, therefore they do not have either a volva or volval patches. Often, a second layer of tissue, the partial veil, covers the bladelike gills that bear spores. As the cap expands, the veil breaks, and remnants of the partial veil may remain as a ring, or annulus, around the middle of the stalk or as fragments hanging from the margin of the cap. The ring may be skirt-like as in some species of "Amanita", collar-like as in many species of "Lepiota", or merely the faint remnants of a cortina (a partial veil composed of filaments resembling a spiderweb), which is typical of the genus "Cortinarius". Mushrooms lacking partial veils do not form an annulus.

The stalk (also called the stipe, or stem) may be central and support the cap in the middle, or it may be off-center and/or lateral, as in species of "Pleurotus" and "Panus". In other mushrooms, a stalk may be absent, as in the polypores that form shelf-like brackets. Puffballs lack a stalk, but may have a supporting base. Other mushrooms, such as truffles, jellies, earthstars, and bird's nests, usually do not have stalks, and a specialized mycological vocabulary exists to describe their parts.

The way the gills attach to the top of the stalk is an important feature of mushroom morphology. Mushrooms in the genera "Agaricus", "Amanita", "Lepiota" and "Pluteus", among others, have free gills that do not extend to the top of the stalk. Others have decurrent gills that extend down the stalk, as in the genera "Omphalotus" and "Pleurotus". There are a great number of variations between the extremes of free and decurrent, collectively called attached gills. Finer distinctions are often made to distinguish the types of attached gills: adnate gills, which adjoin squarely to the stalk; notched gills, which are notched where they join the top of the stalk; adnexed gills, which curve upward to meet the stalk, and so on. These distinctions between attached gills are sometimes difficult to interpret, since gill attachment may change as the mushroom matures, or with different environmental conditions.

A hymenium is a layer of microscopic spore-bearing cells that covers the surface of gills. In the nongilled mushrooms, the hymenium lines the inner surfaces of the tubes of boletes and polypores, or covers the teeth of spine fungi and the branches of corals. In the Ascomycota, spores develop within microscopic elongated, sac-like cells called asci, which typically contain eight spores in each ascus. The Discomycetes, which contain the cup, sponge, brain, and some club-like fungi, develop an exposed layer of asci, as on the inner surfaces of cup fungi or within the pits of morels. The Pyrenomycetes, tiny dark-colored fungi that live on a wide range of substrates including soil, dung, leaf litter, and decaying wood, as well as other fungi, produce minute, flask-shaped structures called perithecia, within which the asci develop.

In the Basidiomycetes, usually four spores develop on the tips of thin projections called sterigmata, which extend from club-shaped cells called a basidia. The fertile portion of the Gasteromycetes, called a gleba, may become powdery as in the puffballs or slimy as in the stinkhorns. Interspersed among the asci are threadlike sterile cells called paraphyses. Similar structures called cystidia often occur within the hymenium of the Basidiomycota. Many types of cystidia exist, and assessing their presence, shape, and size is often used to verify the identification of a mushroom.

The most important microscopic feature for identification of mushrooms is the spores. Their color, shape, size, attachment, ornamentation, and reaction to chemical tests often can be the crux of an identification. A spore often has a protrusion at one end, called an apiculus, which is the point of attachment to the basidium, termed the apical germ pore, from which the hypha emerges when the spore germinates.

Many species of mushrooms seemingly appear overnight, growing or expanding rapidly. This phenomenon is the source of several common expressions in the English language including "to mushroom" or "mushrooming" (expanding rapidly in size or scope) and "to pop up like a mushroom" (to appear unexpectedly and quickly). In reality, all species of mushrooms take several days to form primordial mushroom fruit bodies, though they do expand rapidly by the absorption of fluids.

The cultivated mushroom, as well as the common field mushroom, initially form a minute fruiting body, referred to as the pin stage because of their small size. Slightly expanded, they are called buttons, once again because of the relative size and shape. Once such stages are formed, the mushroom can rapidly pull in water from its mycelium and expand, mainly by inflating preformed cells that took several days to form in the primordia.

Similarly, there are other mushrooms, like "Parasola plicatilis" (formerly "Coprinus plicatlis"), that grow rapidly overnight and may disappear by late afternoon on a hot day after rainfall. The primordia form at ground level in lawns in humid spaces under the thatch and after heavy rainfall or in dewy conditions balloon to full size in a few hours, release spores, and then collapse. They "mushroom" to full size.

Not all mushrooms expand overnight; some grow very slowly and add tissue to their fruiting bodies by growing from the edges of the colony or by inserting hyphae. For example, "Pleurotus nebrodensis" grows slowly, and because of this combined with human collection, it is now critically endangered.

Though mushroom fruiting bodies are short-lived, the underlying mycelium can itself be long-lived and massive. A colony of "Armillaria solidipes" (formerly known as "Armillaria ostoyae") in Malheur National Forest in the United States is estimated to be 2,400 years old, possibly older, and spans an estimated . Most of the fungus is underground and in decaying wood or dying tree roots in the form of white mycelia combined with black shoelace-like rhizomorphs that bridge colonized separated woody substrates.

Raw brown mushrooms are 92% water, 4% carbohydrates, 2% protein and less than 1% fat. In a 100 gram (3.5 ounce) amount, raw mushrooms provide 22 calories and are a rich source (20% or more of the Daily Value, DV) of B vitamins, such as riboflavin, niacin and pantothenic acid, selenium (37% DV) and copper (25% DV), and a moderate source (10-19% DV) of phosphorus, zinc and potassium (table). They have minimal or no Vitamin C and sodium content.

The vitamin D content of a mushroom depends on postharvest handling, in particular the unintended exposure to sunlight. The US Department of Agriculture provided evidence that UV-exposed mushrooms contain substantial amounts of vitamin D. When exposed to ultraviolet (UV) light, even after harvesting, ergosterol in mushrooms is converted to vitamin D, a process now used intentionally to supply fresh vitamin D mushrooms for the functional food grocery market. In a comprehensive safety assessment of producing vitamin D in fresh mushrooms, researchers showed that artificial UV light technologies were equally effective for vitamin D production as in mushrooms exposed to natural sunlight, and that UV light has a long record of safe use for production of vitamin D in food.

Mushrooms are used extensively in cooking, in many cuisines (notably Chinese, Korean, European, and Japanese).

Most mushrooms sold in supermarkets have been commercially grown on mushroom farms. The most popular of these, "Agaricus bisporus", is considered safe for most people to eat because it is grown in controlled, sterilized environments. Several varieties of "A. bisporus" are grown commercially, including whites, crimini, and portobello. Other cultivated species available at many grocers include "Hericium erinaceus", shiitake, maitake (hen-of-the-woods), "Pleurotus", and enoki. In recent years, increasing affluence in developing countries has led to a considerable growth in interest in mushroom cultivation, which is now seen as a potentially important economic activity for small farmers.

China is a major edible mushroom producer. The country produces about half of all cultivated mushrooms, and around of mushrooms are consumed per person per year by over a billion people. In 2014, Poland was the world's largest mushroom exporter, reporting an estimated annually.

Separating edible from poisonous species requires meticulous attention to detail; there is no single trait by which all toxic mushrooms can be identified, nor one by which all edible mushrooms can be identified. People who collect mushrooms for consumption are known as mycophagists, and the act of collecting them for such is known as mushroom hunting, or simply "mushrooming". Even edible mushrooms may produce allergic reactions in susceptible individuals, from a mild asthmatic response to severe anaphylactic shock. Even the cultivated "A. bisporus" contains small amounts of hydrazines, the most abundant of which is agaritine (a mycotoxin and carcinogen). However, the hydrazines are destroyed by moderate heat when cooking.

A number of species of mushrooms are poisonous; although some resemble certain edible species, consuming them could be fatal. Eating mushrooms gathered in the wild is risky and should only be undertaken by individuals knowledgeable in mushroom identification. Common best practice is for wild mushroom pickers to focus on collecting a small number of visually distinctive, edible mushroom species that cannot be easily confused with poisonous varieties.

Many mushroom species produce secondary metabolites that can be toxic, mind-altering, antibiotic, antiviral, or bioluminescent. Although there are only a small number of deadly species, several others can cause particularly severe and unpleasant symptoms. Toxicity likely plays a role in protecting the function of the basidiocarp: the mycelium has expended considerable energy and protoplasmic material to develop a structure to efficiently distribute its spores. One defense against consumption and premature destruction is the evolution of chemicals that render the mushroom inedible, either causing the consumer to vomit the meal (see emetics), or to learn to avoid consumption altogether. In addition, due to the propensity of mushrooms to absorb heavy metals, including those that are radioactive, European mushrooms may, as late as 2008, include toxicity from the 1986 Chernobyl disaster and continue to be studied.

Mushrooms with psychoactive properties have long played a role in various native medicine traditions in cultures all around the world. They have been used as sacrament in rituals aimed at mental and physical healing, and to facilitate visionary states. One such ritual is the "velada" ceremony. A practitioner of traditional mushroom use is the "shaman" or "curandera" (priest-healer).

Psilocybin mushrooms possess psychedelic properties. Commonly known as "magic mushrooms" or shrooms", they are openly available in smart shops in many parts of the world, or on the black market in those countries that have outlawed their sale. Psilocybin mushrooms have been reported as facilitating profound and life-changing insights often described as mystical experiences. Recent scientific work has supported these claims, as well as the long-lasting effects of such induced spiritual experiences.
Psilocybin, a naturally occurring chemical in certain psychedelic mushrooms such as "Psilocybe cubensis", is being studied for its ability to help people suffering from psychological disorders, such as obsessive–compulsive disorder. Minute amounts have been reported to stop cluster and migraine headaches. A double-blind study, done by the Johns Hopkins Hospital, showed psychedelic mushrooms could provide people an experience with substantial personal meaning and spiritual significance. In the study, one third of the subjects reported ingestion of psychedelic mushrooms was the single most spiritually significant event of their lives. Over two-thirds reported it among their five most meaningful and spiritually significant events. On the other hand, one-third of the subjects reported extreme anxiety. However, the anxiety went away after a short period of time. Psilocybin mushrooms have also shown to be successful in treating addiction, specifically with alcohol and cigarettes.

A few species in the genus "Amanita", most recognizably "A. muscaria", but also "A. pantherina", among others, contain the psychoactive compound muscimol. The muscimol-containing chemotaxonomic group of "Amanitas" contains no amatoxins or phallotoxins, and as such are not hepatoxic, though if not properly cured will be non-lethally neurotoxic due to the presence of ibotenic acid. The "Amanita" intoxication is similar to Z-drugs in that it includes CNS depressant and sedative-hypnotic effects, but also dissociation and delirium in high doses.

Some mushrooms are used or studied as possible treatments for diseases, particularly their extracts, including polysaccharides, glycoproteins and proteoglycans. In some countries, extracts of polysaccharide-K, schizophyllan, polysaccharide peptide, or lentinan are government-registered adjuvant cancer therapies, even though clinical evidence of efficacy in humans has not been confirmed.

Historically in traditional Chinese medicine, mushrooms are believed to have medicinal value, although there is no evidence for such uses.

Mushrooms can be used for dyeing wool and other natural fibers. The chromophores of mushroom dyes are organic compounds and produce strong and vivid colors, and all colors of the spectrum can be achieved with mushroom dyes. Before the invention of synthetic dyes, mushrooms were the source of many textile dyes.

Some fungi, types of polypores loosely called mushrooms, have been used as fire starters (known as tinder fungi).

Mushrooms and other fungi play a role in the development of new biological remediation techniques (e.g., using mycorrhizae to spur plant growth) and filtration technologies (e.g. using fungi to lower bacterial levels in contaminated water).





</doc>
<doc id="20266" url="https://en.wikipedia.org/wiki?curid=20266" title="Mainframe computer">
Mainframe computer

Mainframe computers or mainframes (colloquially referred to as "big iron") are computers used primarily by large organizations for critical applications; bulk data processing, such as census, industry and consumer statistics, enterprise resource planning; and transaction processing. They are larger and have more processing power than some other classes of computers: minicomputers, servers, workstations, and personal computers.

The term originally referred to the large cabinets called "main frames" that housed the central processing unit and main memory of early computers. Later, the term was used to distinguish high-end commercial machines from less powerful units. Most large-scale computer system architectures were established in the 1960s, but continue to evolve. Mainframe computers are often used as servers.

Modern mainframe design is characterized less by raw computational speed and more by:

Their high stability and reliability enable these machines to run uninterrupted for very long periods of time, with mean time between failures (MTBF) measured in decades.

Mainframes have high availability, one of the primary reasons for their longevity, since they are typically used in applications where downtime would be costly or catastrophic. The term reliability, availability and serviceability (RAS) is a defining characteristic of mainframe computers. Proper planning and implementation is required to realize these features. In addition, mainframes are more secure than other computer types: the NIST vulnerabilities database, US-CERT, rates traditional mainframes such as IBM Z (previously called z Systems, System z and zSeries), Unisys Dorado and Unisys Libra as among the most secure with vulnerabilities in the low single digits as compared with thousands for Windows, UNIX, and Linux. Software upgrades usually require setting up the operating system or portions thereof, and are non-disruptive only when using virtualizing facilities such as IBM z/OS and Parallel Sysplex, or Unisys XPCL, which support workload sharing so that one system can take over another's application while it is being refreshed.

In the late 1950s, mainframes had only a rudimentary interactive interface (the console), and used sets of punched cards, paper tape, or magnetic tape to transfer data and programs. They operated in batch mode to support back office functions such as payroll and customer billing, most of which were based on repeated tape-based sorting and merging operations followed by line printing to preprinted continuous stationery. When interactive user terminals were introduced, they were used almost exclusively for applications (e.g. airline booking) rather than program development. Typewriter and Teletype devices were common control consoles for system operators through the early 1970s, although ultimately supplanted by keyboard/display devices.

By the early 1970s, many mainframes acquired interactive user terminals operating as timesharing computers, supporting hundreds of users simultaneously along with batch processing. Users gained access through keyboard/typewriter terminals and specialized text terminal CRT displays with integral keyboards, or later from personal computers equipped with terminal emulation software. By the 1980s, many mainframes supported graphic display terminals, and terminal emulation, but not graphical user interfaces. This form of end-user computing became obsolete in the 1990s due to the advent of personal computers provided with GUIs. After 2000, modern mainframes partially or entirely phased out classic "green screen" and color display terminal access for end-users in favour of Web-style user interfaces.

The infrastructure requirements were drastically reduced during the mid-1990s, when CMOS mainframe designs replaced the older bipolar technology. IBM claimed that its newer mainframes reduced data center energy costs for power and cooling, and reduced physical space requirements compared to server farms.

Modern mainframes can run multiple different instances of operating systems at the same time. This technique of virtual machines allows applications to run as if they were on physically distinct computers. In this role, a single mainframe can replace higher-functioning hardware services available to conventional servers. While mainframes pioneered this capability, virtualization is now available on most families of computer systems, though not always to the same degree or level of sophistication.

Mainframes can add or hot swap system capacity without disrupting system function, with specificity and granularity to a level of sophistication not usually available with most server solutions. Modern mainframes, notably the IBM zSeries, System z9 and System z10 servers, offer two levels of virtualization: logical partitions (LPARs, via the PR/SM facility) and virtual machines (via the z/VM operating system). Many mainframe customers run two machines: one in their primary data center, and one in their backup data center—fully active, partially active, or on standby—in case there is a catastrophe affecting the first building. Test, development, training, and production workload for applications and databases can run on a single machine, except for extremely large demands where the capacity of one machine might be limiting. Such a two-mainframe installation can support continuous business service, avoiding both planned and unplanned outages. In practice many customers use multiple mainframes linked either by Parallel Sysplex and shared DASD (in IBM's case), or with shared, geographically dispersed storage provided by EMC or Hitachi.

Mainframes are designed to handle very high volume input and output (I/O) and emphasize throughput computing. Since the late-1950s, mainframe designs have included subsidiary hardware (called "channels" or "peripheral processors") which manage the I/O devices, leaving the CPU free to deal only with high-speed memory. It is common in mainframe shops to deal with massive databases and files. Gigabyte to terabyte-size record files are not unusual. Compared to a typical PC, mainframes commonly have hundreds to thousands of times as much data storage online, and can access it reasonably quickly. Other server families also offload I/O processing and emphasize throughput computing.

Mainframe return on investment (ROI), like any other computing platform, is dependent on its ability to scale, support mixed workloads, reduce labor costs, deliver uninterrupted service for critical business applications, and several other risk-adjusted cost factors.

Mainframes also have execution integrity characteristics for fault tolerant computing. For example, z900, z990, System z9, and System z10 servers effectively execute result-oriented instructions twice, compare results, arbitrate between any differences (through instruction retry and failure isolation), then shift workloads "in flight" to functioning processors, including spares, without any impact to operating systems, applications, or users. This hardware-level feature, also found in HP's NonStop systems, is known as lock-stepping, because both processors take their "steps" ("i.e." instructions) together. Not all applications absolutely need the assured integrity that these systems provide, but many do, such as financial transaction processing.

IBM, with z Systems, continues to be a major manufacturer in the mainframe market. Unisys manufactures ClearPath Libra mainframes, based on earlier Burroughs MCP products and ClearPath Dorado mainframes based on Sperry Univac OS 1100 product lines. In 2000, Hitachi co-developed the zSeries z900 with IBM to share expenses, but subsequently the two companies have not collaborated on new Hitachi models. Hewlett-Packard sells its unique NonStop systems, which it acquired with Tandem Computers and which some analysts classify as mainframes. Groupe Bull's GCOS, Fujitsu (formerly Siemens) BS2000, and Fujitsu-ICL VME mainframes are still available in Europe, and Fujitsu (formerly Amdahl) GS21 mainframes globally. NEC with ACOS and Hitachi with AP10000-VOS3 still maintain mainframe hardware businesses in the Japanese market.

The amount of vendor investment in mainframe development varies with market share. Fujitsu and Hitachi both continue to use custom S/390-compatible processors, as well as other CPUs (including POWER and Xeon) for lower-end systems. Bull uses a mixture of Itanium and Xeon processors. NEC uses Xeon processors for its low-end ACOS-2 line, but develops the custom NOAH-6 processor for its high-end ACOS-4 series. IBM also develops custom processors in-house, such as the zEC12. Unisys produces code compatible mainframe systems that range from laptops to cabinet-sized mainframes that utilize homegrown CPUs as well as Xeon processors. Furthermore, there exists a market for software applications to manage the performance of mainframe implementations. In addition to IBM, significant players in this market include BMC, Compuware, and CA Technologies.

Several manufacturers and their successors produced mainframe computers from the late 1950s until the early 21st Century, with gradually decreasing numbers and a gradual transition to simulation on Intel chips rather than proprietary hardware. The US group of manufacturers was first known as "IBM and the Seven Dwarfs": usually Burroughs, UNIVAC, NCR, Control Data, Honeywell, General Electric and RCA, although some lists varied. Later, with the departure of General Electric and RCA, it was referred to as IBM and the BUNCH. IBM's dominance grew out of their 700/7000 series and, later, the development of the 360 series mainframes. The latter architecture has continued to evolve into their current zSeries mainframes which, along with the then Burroughs and Sperry (now Unisys) MCP-based and OS1100 mainframes, are among the few mainframe architectures still extant that can trace their roots to this early period. While IBM's zSeries can still run 24-bit System/360 code, the 64-bit zSeries and System z9 CMOS servers have nothing physically in common with the older systems. Notable manufacturers outside the US were Siemens and Telefunken in Germany, ICL in the United Kingdom, Olivetti in Italy, and Fujitsu, Hitachi, Oki, and NEC in Japan. The Soviet Union and Warsaw Pact countries manufactured close copies of IBM mainframes during the Cold War; the BESM series and Strela are examples of an independently designed Soviet computer.

Shrinking demand and tough competition started a shakeout in the market in the early 1970s—RCA sold out to UNIVAC and GE sold its business to Honeywell; between 1986 and 1990 Honeywell was bought out by Bull; UNIVAC became a division of Sperry, which later merged with Burroughs to form Unisys Corporation in 1986.

During the 1980s, minicomputer-based systems grew more sophisticated and were able to displace the lower-end of the mainframes. These computers, sometimes called "departmental computers" were typified by the DEC VAX.

In 1991, AT&T Corporation briefly owned NCR. During the same period, companies found that servers based on microcomputer designs could be deployed at a fraction of the acquisition price and offer local users much greater control over their own systems given the IT policies and practices at that time. Terminals used for interacting with mainframe systems were gradually replaced by personal computers. Consequently, demand plummeted and new mainframe installations were restricted mainly to financial services and government. In the early 1990s, there was a rough consensus among industry analysts that the mainframe was a dying market as mainframe platforms were increasingly replaced by personal computer networks. InfoWorld's Stewart Alsop infamously predicted that the last mainframe would be unplugged in 1996; in 1993, he cited Cheryl Currid, a computer industry analyst as saying that the last mainframe "will stop working on December 31, 1999", a reference to the anticipated Year 2000 problem (Y2K).

That trend started to turn around in the late 1990s as corporations found new uses for their existing mainframes and as the price of data networking collapsed in most parts of the world, encouraging trends toward more centralized computing. The growth of e-business also dramatically increased the number of back-end transactions processed by mainframe software as well as the size and throughput of databases. Batch processing, such as billing, became even more important (and larger) with the growth of e-business, and mainframes are particularly adept at large-scale batch computing. Another factor currently increasing mainframe use is the development of the Linux operating system, which arrived on IBM mainframe systems in 1999 and is typically run in scores or up to ~ 8,000 virtual machines on a single mainframe. Linux allows users to take advantage of open source software combined with mainframe hardware RAS. Rapid expansion and development in emerging markets, particularly People's Republic of China, is also spurring major mainframe investments to solve exceptionally difficult computing problems, e.g. providing unified, extremely high volume online transaction processing databases for 1 billion consumers across multiple industries (banking, insurance, credit reporting, government services, etc.) In late 2000, IBM introduced 64-bit z/Architecture, acquired numerous software companies such as Cognos and introduced those software products to the mainframe. IBM's quarterly and annual reports in the 2000s usually reported increasing mainframe revenues and capacity shipments. However, IBM's mainframe hardware business has not been immune to the recent overall downturn in the server hardware market or to model cycle effects. For example, in the 4th quarter of 2009, IBM's System z hardware revenues decreased by 27% year over year. But MIPS (millions of instructions per second) shipments increased 4% per year over the past two years. Alsop had himself photographed in 2000, symbolically eating his own words ("death of the mainframe").

In 2012, NASA powered down its last mainframe, an IBM System z9. However, IBM's successor to the z9, the z10, led a New York Times reporter to state four years earlier that "mainframe technology — hardware, software and services — remains a large and lucrative business for I.B.M., and mainframes are still the back-office engines behind the world’s financial markets and much of global commerce". , while mainframe technology represented less than 3% of IBM's revenues, it "continue[d] to play an outsized role in Big Blue's results".

In 2015, IBM launched the IBM z13, in June 2017 the IBM z14 and in September 2019 IBM launched the latest version of the product, the IBM z15.

A supercomputer is a computer at the leading edge of data processing capability, with respect to calculation speed. Supercomputers are used for scientific and engineering problems (high-performance computing) which crunch numbers and data, while mainframes focus on transaction processing. The differences are:

Mainframes and supercomputers cannot always be clearly distinguished; up until the early 1990s, many supercomputers were based on a mainframe architecture with supercomputing extensions. An example of such a system is the HITAC S-3800, which was instruction-set compatible with IBM System/370 mainframes, and could run the Hitachi VOS3 operating system (a fork of IBM MVS). The S-3800 therefore can be seen as being both simultaneously a supercomputer and also an IBM-compatible mainframe. 

In 2007, an amalgamation of the different technologies and architectures for supercomputers and mainframes has led to the so-called gameframe.





</doc>
<doc id="20268" url="https://en.wikipedia.org/wiki?curid=20268" title="Microsoft Excel">
Microsoft Excel

Microsoft Excel is a spreadsheet developed by Microsoft for Windows, macOS, Android and iOS. It features calculation, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications. It has been a very widely applied spreadsheet for these platforms, especially since version 5 in 1993, and it has replaced Lotus 1-2-3 as the industry standard for spreadsheets. Excel forms part of the Microsoft Office suite of software.

Microsoft Excel has the basic features of all spreadsheets, using a grid of "cells" arranged in numbered "rows" and letter-named "columns" to organize data manipulations like arithmetic operations. It has a battery of supplied functions to answer statistical, engineering and financial needs. In addition, it can display data as line graphs, histograms and charts, and with a very limited three-dimensional graphical display. It allows sectioning of data to view its dependencies on various factors for different perspectives (using "pivot tables" and the "scenario manager"). It has a programming aspect, "Visual Basic for Applications", allowing the user to employ a wide variety of numerical methods, for example, for solving differential equations of mathematical physics, and then reporting the results back to the spreadsheet. It also has a variety of interactive features allowing user interfaces that can completely hide the spreadsheet from the user, so the spreadsheet presents itself as a so-called "application", or "decision support system" (DSS), via a custom-designed user interface, for example, a stock analyzer, or in general, as a design tool that asks the user questions and provides answers and reports. In a more elaborate realization, an Excel application can automatically poll external databases and measuring instruments using an update schedule, analyze the results, make a Word report or PowerPoint slide show, and e-mail these presentations on a regular basis to a list of participants. Excel was not designed to be used as a database.

Microsoft allows for a number of optional command-line switches to control the manner in which Excel starts.

Excel 2016 has 484 functions. Of these, 360 existed prior to Excel 2010. Microsoft classifies these functions in 14 categories. Of the 484 current functions, 386 may be called from VBA as methods of the object "WorksheetFunction" and 44 have the same names as VBA functions.

The Windows version of Excel supports programming through Microsoft's Visual Basic for Applications (VBA), which is a dialect of Visual Basic. Programming with VBA allows spreadsheet manipulation that is awkward or impossible with standard spreadsheet techniques. Programmers may write code directly using the Visual Basic Editor (VBE), which includes a window for writing code, debugging code, and code module organization environment. The user can implement numerical methods as well as automating tasks such as formatting or data organization in VBA and guide the calculation using any desired intermediate results reported back to the spreadsheet.

VBA was removed from Mac Excel 2008, as the developers did not believe that a timely release would allow porting the VBA engine natively to Mac OS X. VBA was restored in the next version, Mac Excel 2011, although the build lacks support for ActiveX objects, impacting some high level developer tools.

A common and easy way to generate VBA code is by using the Macro Recorder. The Macro Recorder records actions of the user and generates VBA code in the form of a macro. These actions can then be repeated automatically by running the macro. The macros can also be linked to different trigger types like keyboard shortcuts, a command button or a graphic. The actions in the macro can be executed from these trigger types or from the generic toolbar options. The VBA code of the macro can also be edited in the VBE. Certain features such as loop functions and screen prompt by their own properties, and some graphical display items, cannot be recorded but must be entered into the VBA module directly by the programmer. Advanced users can employ user prompts to create an interactive program, or react to events such as sheets being loaded or changed.

Macro Recorded code may not be compatible with Excel versions. Some code that is used in Excel 2010 cannot be used in Excel 2003. Making a Macro that changes the cell colours and making changes to other aspects of cells may not be backward compatible.

VBA code interacts with the spreadsheet through the Excel "Object Model", a vocabulary identifying spreadsheet objects, and a set of supplied functions or "methods" that enable reading and writing to the spreadsheet and interaction with its users (for example, through custom toolbars or "command bars" and "message boxes"). User-created VBA subroutines execute these actions and operate like macros generated using the macro recorder, but are more flexible and efficient.

From its first version Excel supported end user programming of macros (automation of repetitive tasks) and user defined functions (extension of Excel's built-in function library). In early versions of Excel these programs were written in a macro language whose statements had formula syntax and resided in the cells of special purpose macro sheets (stored with file extension .XLM in Windows.) XLM was the default macro language for Excel through Excel 4.0. Beginning with version 5.0 Excel recorded macros in VBA by default but with version 5.0 XLM recording was still allowed as an option. After version 5.0 that option was discontinued. All versions of Excel, including Excel 2010 are capable of running an XLM macro, though Microsoft discourages their use.

Excel supports charts, graphs, or histograms generated from specified groups of cells. The generated graphic component can either be embedded within the current sheet, or added as a separate object.

These displays are dynamically updated if the content of cells change. For example, suppose that the important design requirements are displayed visually; then, in response to a user's change in trial values for parameters, the curves describing the design change shape, and their points of intersection shift, assisting the selection of the best design.

Additional features are available using add-ins. Several are provided with Excel, including:


Versions of Excel up to 7.0 had a limitation in the size of their data sets of 16K (2 = ) rows. Versions 8.0 through 11.0 could handle 64K (2 = ) rows and 256 columns (2 as label 'IV'). Version 12.0 onwards, including the current Version 16.x, can handle over 1M (2 = ) rows, and (2 as label 'XFD') columns.

Microsoft Excel up until 2007 version used a proprietary binary file format called Excel Binary File Format (.XLS) as its primary format. Excel 2007 uses Office Open XML as its primary file format, an XML-based format that followed after a previous XML-based format called "XML Spreadsheet" ("XMLSS"), first introduced in Excel 2002.

Although supporting and encouraging the use of new XML-based formats as replacements, Excel 2007 remained backwards-compatible with the traditional, binary formats. In addition, most versions of Microsoft Excel can read CSV, DBF, SYLK, DIF, and other legacy formats. Support for some older file formats was removed in Excel 2007. The file formats were mainly from DOS-based programs.

OpenOffice.org has created documentation of the Excel format. Since then Microsoft made the Excel binary format specification available to freely download.

The "XML Spreadsheet" format introduced in Excel 2002 is a simple, XML based format missing some more advanced features like storage of VBA macros. Though the intended file extension for this format is ".xml", the program also correctly handles XML files with ".xls" extension. This feature is widely used by third-party applications (e.g. "MySQL Query Browser") to offer "export to Excel" capabilities without implementing binary file format. The following example will be correctly opened by Excel if saved either as "Book1.xml" or "Book1.xls":

Microsoft Excel 2007, along with the other products in the Microsoft Office 2007 suite, introduced new file formats. The first of these (.xlsx) is defined in the Office Open XML (OOXML) specification.

Windows applications such as Microsoft Access and Microsoft Word, as well as Excel can communicate with each other and use each other's capabilities. The most common are Dynamic Data Exchange: although strongly deprecated by Microsoft, this is a common method to send data between applications running on Windows, with official MS publications referring to it as "the protocol from hell". As the name suggests, it allows applications to supply data to others for calculation and display. It is very common in financial markets, being used to connect to important financial data services such as Bloomberg and Reuters.

OLE Object Linking and Embedding allows a Windows application to control another to enable it to format or calculate data. This may take on the form of "embedding" where an application uses another to handle a task that it is more suited to, for example a PowerPoint presentation may be embedded in an Excel spreadsheet or vice versa.

Excel users can access external data sources via Microsoft Office features such as (for example) codice_1 connections built with the Office Data Connection file format. Excel files themselves may be updated using a Microsoft supplied ODBC driver.

Excel can accept data in real time through several programming interfaces, which allow it to communicate with many data sources such as Bloomberg and Reuters (through addins such as Power Plus Pro).

Alternatively, Microsoft Query provides ODBC-based browsing within Microsoft Excel.

Programmers have produced APIs to open Excel spreadsheets in a variety of applications and environments other than Microsoft Excel. These include opening Excel documents on the web using either ActiveX controls, or plugins like the Adobe Flash Player. The Apache POI opensource project provides Java libraries for reading and writing Excel spreadsheet files. ExcelPackage is another open-source project that provides server-side generation of Microsoft Excel 2007 spreadsheets. PHPExcel is a PHP library that converts Excel5, Excel 2003, and Excel 2007 formats into objects for reading and writing within a web application. Excel Services is a current .NET developer tool that can enhance Excel's capabilities. Excel spreadsheets can be accessed from Python with xlrd and openpyxl. js-xlsx and js-xls can open Excel spreadsheets from JavaScript.

Microsoft Excel protection offers several types of passwords:
All passwords except "password to open a document" can be removed instantly regardless of Microsoft Excel version used to create the document. These types of passwords are used primarily for shared work on a document. Such password-protected documents are not encrypted, and a data sources from a set password is saved in a document's header. "Password to protect workbook" is an exception – when it is set, a document is encrypted with the standard password “"VelvetSweatshop"”, but since it is known to public, it actually does not add any extra protection to the document. The only type of password that can prevent a trespasser from gaining access to a document is "password to open a document". The cryptographic strength of this kind of protection depends strongly on the Microsoft Excel version that was used to create the document.

In "Microsoft Excel 95" and earlier versions, password to open is converted to a 16-bit key that can be instantly cracked. In "Excel 97/2000" the password is converted to a 40-bit key, which can also be cracked very quickly using modern equipment. As regards services which use rainbow tables (e.g. Password-Find), it takes up to several seconds to remove protection. In addition, password-cracking programs can brute-force attack passwords at a rate of hundreds of thousands of passwords a second, which not only lets them decrypt a document, but also find the original password.

In "Excel 2003/XP" the encryption is slightly better – a user can choose any encryption algorithm that is available in the system (see Cryptographic Service Provider). Due to the CSP, an "Excel" file can't be decrypted, and thus the "password to open" can't be removed, though the brute-force attack speed remains quite high. Nevertheless, the older "Excel 97/2000" algorithm is set by the default. Therefore, users who do not change the default settings lack reliable protection of their documents.

The situation changed fundamentally in "Excel 2007", where the modern AES algorithm with a key of 128 bits started being used for decryption, and a 50,000-fold use of the hash function SHA1 reduced the speed of brute-force attacks down to hundreds of passwords per second. In "Excel 2010", the strength of the protection by the default was increased two times due to the use of a 100,000-fold SHA1 to convert a password to a key.

Microsoft Excel Viewer was a freeware program for viewing and printing spreadsheet documents created by Excel. The Microsoft Excel Viewer was retired in April, 2018 in lieu of Excel Online. Excel Viewer is similar to Microsoft Word Viewer in functionality. (There is not a current version for the Mac.) Excel Viewer is available for Microsoft Windows and Windows CE handheld PCs, such as the NEC MobilePro. It is also possible to open Excel files using certain online tools and services. Online excel viewers do not require users to have Microsoft Excel installed.

In addition to issues with spreadsheets in general, other problems specific to Excel include numeric precision, misleading statistics functions, mod function errors, date limitations and more.

Despite the use of 15-figure precision, Excel can display many more figures (up to thirty) upon user request. But the displayed figures are "not" those actually used in its computations, and so, for example, the difference of two numbers may differ from the difference of their displayed values. Although such departures are usually beyond the 15th decimal, exceptions do occur, especially for very large or very small numbers. Serious errors can occur if decisions are made based upon automated comparisons of numbers (for example, using the Excel "If" function), as equality of two numbers can be unpredictable.

In the figure the fraction 1/9000 is displayed in Excel. Although this number has a decimal representation that is an infinite string of ones, Excel displays only the leading 15 figures. In the second line, the number one is added to the fraction, and again Excel displays only 15 figures. In the third line, one is subtracted from the sum using Excel. Because the sum in the second line has only eleven 1's after the decimal, the difference when 1 is subtracted from this displayed value is three 0's followed by a string of eleven 1's. However, the difference reported by Excel in the third line is three 0's followed by a string of "thirteen" 1's and two extra erroneous digits. This is because Excel calculates with about half a digit more than it displays.

Excel works with a modified 1985 version of the IEEE 754 specification. Excel's implementation involves conversions between binary and decimal representations, leading to accuracy that is on average better than one would expect from simple fifteen digit precision, but that can be worse. See the main article for details.

Besides accuracy in user computations, the question of accuracy in Excel-provided functions may be raised. Particularly in the arena of statistical functions, Excel has been criticized for sacrificing accuracy for speed of calculation.

As many calculations in Excel are executed using VBA, an additional issue is the accuracy of VBA, which varies with variable type and user-requested precision.

The accuracy and convenience of statistical tools in Excel has been criticized, as mishandling missing data, as returning incorrect values due to inept handling of round-off and large numbers, as only selectively updating calculations on a spreadsheet when some cell values are changed, and as having a limited set of statistical tools. Microsoft has announced some of these issues are addressed in Excel 2010.

Excel has issues with modulo operations. In the case of excessively large results, Excel will return the error warning instead of an answer.

Excel includes February 29, 1900, incorrectly treating 1900 as a leap year, even though e.g. 2100 is correctly treated as a non-leap year. The bug originated from Lotus 1-2-3 (deliberately implemented to save computer memory), and was also purposely implemented in Excel, for the purpose of bug compatibility. This legacy has later been carried over into Office Open XML file format.

Thus a (not necessarily whole) number greater than or equal to 61 interpreted as a date and time is the (real) number of days after December 30, 1899, 0:00, a non-negative number less than 60 is the number of days after December 31, 1899, 0:00, and numbers with whole part 60 represent the fictional day.

Excel supports dates with years in the range 1900-9999, except that December 31, 1899 can be entered as 0 and is displayed as 0-jan-1900.

Converting a fraction of a day into hours, minutes and days by treating it as a moment on the day January 1, 1900, does not work for a negative fraction.

Entering text that happens to be in a form that is interpreted as a date, the text can be unintentionally changed to a standard date format. A similar problem occurs when a text happens to be in the form of a floating point notation of a number. In these cases the original exact text cannot be recovered from the result.

This issue has caused a well known problem in the analysis of DNA, for example in bioinformatics. As first reported in 2004, genetic scientists found that Excel automatically and incorrectly converts certain gene names into dates. A follow-up study in 2016 found many peer reviewed scientific journal papers had been affected and that "Of the selected journals, the proportion of published articles with Excel files containing gene lists that are affected by gene name errors is 19.6 %." Excel parses the copied and pasted data and sometimes changes them depending on what it thinks they are. For example, MARCH1 (Membrane Associated Ring-CH-type finger 1) gets converted to the date March 1 (1-Mar) and SEPT2 (Septin 2) is converted into September 2 (2-Sep) etc. While some secondary news sources reported this as a fault with Excel, the original authors of the 2016 paper placed the blame with the researchers mis-using Excel.

The following functions return incorrect results when passed a string longer than 255 characters:

Microsoft Excel will not open two documents with the same name and instead will display the following error:
The reason is for calculation ambiguity with linked cells. If there is a cell ='[Book1.xlsx]Sheet1'!$G$33, and there are two books named "Book1" open, there is no way to tell which one the user means.

Microsoft originally marketed a spreadsheet program called Multiplan in 1982. Multiplan became very popular on CP/M systems, but on MS-DOS systems it lost popularity to Lotus 1-2-3. Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and the first Windows version was 2.05 (to synchronize with the Macintosh version 2.2) in November 1987. Lotus was slow to bring 1-2-3 to Windows and by the early 1990s Excel had started to outsell 1-2-3 and helped Microsoft achieve its position as a leading PC software developer. This accomplishment solidified Microsoft as a valid competitor and showed its future of developing GUI software. Microsoft maintained its advantage with regular new releases, every two years or so.

Excel 2.0 is the first version of Excel for the Intel platform. Versions prior to 2.0 were only available on the Apple Macintosh.

The first Windows version was labeled "2" to correspond to the Mac version. This included a run-time version of Windows.

"BYTE" in 1989 listed Excel for Windows as among the "Distinction" winners of the BYTE Awards. The magazine stated that the port of the "extraordinary" Macintosh version "shines", with a user interface as good as or better than the original.

Included toolbars, drawing capabilities, outlining, add-in support, 3D charts, and many more new features.

Introduced auto-fill.

Also, an easter egg in Excel 4.0 reveals a hidden animation of a dancing set of numbers 1 through 3, representing Lotus 1-2-3, which was then crushed by an Excel logo.

With version 5.0, Excel has included Visual Basic for Applications (VBA), a programming language based on Visual Basic which adds the ability to automate tasks in Excel and to provide user-defined functions (UDF) for use in worksheets. VBA is a powerful addition to the application and includes a fully featured integrated development environment (IDE). Macro recording can produce VBA code replicating user actions, thus allowing simple automation of regular tasks. VBA allows the creation of forms and in‑worksheet controls to communicate with the user. The language supports use (but not creation) of ActiveX (COM) DLL's; later versions add support for class modules allowing the use of basic object-oriented programming techniques.

The automation functionality provided by VBA made Excel a target for macro viruses. This caused serious problems until antivirus products began to detect these viruses. Microsoft belatedly took steps to prevent the misuse by adding the ability to disable macros completely, to enable macros when opening a workbook or to trust all macros signed using a trusted certificate.

Versions 5.0 to 9.0 of Excel contain various Easter eggs, including a "Hall of Tortured Souls", although since version 10 Microsoft has taken measures to eliminate such undocumented features from their products.

5.0 was released in a 16-bit x86 version for Windows 3.1 and later in a 32-bit version for NT 3.51 (x86/Alpha/PowerPC)

Released in 1995 with Microsoft Office for Windows 95, this is the first major version after Excel 5.0, as there is no Excel 6.0 with all of the Office applications standardizing on the same major version number.

Internal rewrite to 32-bits. Almost no external changes, but faster and more stable.

Included in Office 97 (for x86 and Alpha). This was a major upgrade that introduced the paper clip office assistant and featured standard VBA used instead of internal Excel Basic. It introduced the now-removed Natural Language labels.

This version of Excel includes a flight simulator as an Easter Egg.

Included in Office 2000. This was a minor upgrade, but introduced an upgrade to the clipboard where it can hold multiple objects at once. The Office Assistant, whose frequent unsolicited appearance in Excel 97 had annoyed many users, became less intrusive.

Included in Office XP. Very minor enhancements.

Included in Office 2003. Minor enhancements, most significant being the new Tables.

Included in Office 2007. This release was a major upgrade from the previous version. Similar to other updated Office products, Excel in 2007 used the new Ribbon menu system. This was different from what users were used to, and was met with mixed reactions. One study reported fairly good acceptance by users except highly experienced users and users of word processing applications with a classical WIMP interface, but was less convinced in terms of efficiency and organisation. However, an online survey reported that a majority of respondents had a negative opinion of the change, with advanced users being "somewhat more negative" than intermediate users, and users reporting a self-estimated reduction in productivity.

Added functionality included the SmartArt set of editable business diagrams. Also added was an improved management of named variables through the "Name Manager", and much improved flexibility in formatting graphs, which allow ("x, y") coordinate labeling and lines of arbitrary weight. Several improvements to pivot tables were introduced.

Also like other office products, the Office Open XML file formats were introduced, including ".xlsm" for a workbook with macros and ".xlsx" for a workbook without macros.

Specifically, many of the size limitations of previous versions were greatly increased. To illustrate, the number of rows was now 1,048,576 (2) and columns was 16,384 (2; the far-right column is XFD). This changes what is a valid "A1" reference versus a named range. This version made more extensive use of multiple cores for the calculation of spreadsheets; however, VBA macros are not handled in parallel and XLL add‑ins were only executed in parallel if they were thread-safe and this was indicated at registration.

Included in Office 2010, this is the next major version after v12.0, as version number 13 was skipped.

Minor enhancements and 64-bit support, including the following:

Included in Office 2013, along with a lot of new tools included in this release:

Included in Office 2016, along with a lot of new tools included in this release:



Excel offers many user interface tweaks over the earliest electronic spreadsheets; however, the essence remains the same as in the original spreadsheet software, VisiCalc: the program displays cells organized in rows and columns, and each cell may contain data or a formula, with relative or absolute references to other cells.

Excel 2.0 for Windows, which was modeled after its Mac GUI-based counterpart, indirectly expanded the installed base of the then-nascent Windows environment. Excel 2.0 was released a month before Windows 2.0, and the installed base of Windows was so low at that point in 1987 that Microsoft had to bundle a runtime version of Windows 1.0 with Excel 2.0. Unlike Microsoft Word, there never was a DOS version of Excel.

Excel became the first spreadsheet to allow the user to define the appearance of spreadsheets (fonts, character attributes and cell appearance). It also introduced intelligent cell recomputation, where only cells dependent on the cell being modified are updated (previous spreadsheet programs recomputed everything all the time or waited for a specific user command). Excel introduced auto-fill, the ability to drag and expand the selection box to automatically copy cell or row contents to adjacent cells or rows, adjusting the copies intelligently by automatically incrementing cell references or contents. Excel also introduced extensive graphing capabilities.

Because Excel is widely used, it has been attacked by hackers. While Excel is not directly exposed to the Internet, if an attacker can get a victim to open a file in Excel, and there is an appropriate security bug in Excel, then the attacker can gain control of the victim's computer. UK's GCHQ has a tool named TORNADO ALLEY with this purpose.





</doc>
<doc id="20269" url="https://en.wikipedia.org/wiki?curid=20269" title="Michael Hutchence">
Michael Hutchence

Michael Kelland John Hutchence (22 January 1960 – 22 November 1997) was an Australian musician, singer-songwriter and actor who co-founded the rock band INXS, which sold over 60 million records worldwide and was inducted into the ARIA Hall of Fame in 2001. Hutchence was the lead singer and lyricist of INXS from 1977 until his death. According to rock music historian Ian McFarlane, "Hutchence was the archetypal rock showman. He exuded an overtly sexual, macho cool with his flowing locks, and lithe and exuberant stage movements." Hutchence was named 'Best International Artist' at the 1991 BRIT Awards, with INXS winning the related group award.

Hutchence was a member of the short-lived pop rock group Max Q. He also recorded some solo material and acted in feature films, including "Dogs in Space" (1986), "Frankenstein Unbound" (1990), and "Limp" (1997).

Hutchence had a string of love affairs with prominent actresses, models and singers, and his private life was often reported in the Australian and international press. In July 1996, Hutchence and English television presenter Paula Yates had a daughter, Heavenly Hiraani Tiger Lily.

On the morning of 22 November 1997, Hutchence was found dead in his hotel room in Sydney. His death was reported by the New South Wales Coroner to be the result of suicide by hanging.

Michael Kelland John Hutchence was born on 22 January 1960, to Sydney businessman Kelland ("Kell") Frank Hutchence (1924-2002) and make-up artist Patricia Glassop (née Kennedy). Kelland's parents were sea captain Frank Hutchence and Mabs from England who settled in Sydney in 1922. Michael joined elder half-sister Tina; both siblings were of Irish ancestry from their mother's side, as Patricia's father was from County Cork in Ireland. 

Following Kell's business interests, the Hutchence family moved to Brisbane (where younger brother Rhett was born) and later to Hong Kong. During the early years in Hong Kong, both boys attended Beacon Hill School in Kowloon Tong. While in Hong Kong, Michael showed promise as a swimmer before breaking his arm badly. He then began to show interest in poetry and performed his first song in a local toy store commercial. Michael attended King George V School during his early teens.

The family returned to Sydney in 1972, buying a house in Belrose near the Northern Beaches. Hutchence attended Davidson High School, where he met and befriended Andrew Farriss. Around this time, Hutchence and Farriss spent a lot of time jamming in the garage with Andrew's brothers. Farriss then convinced Hutchence to join his band, Doctor Dolphin, alongside classmates Kent Kerny and Neil Sanders. Bass guitarist Garry Beers and drummer Geoff Kennelly from nearby Forest High School filled out the line-up. Hutchence's parents separated when he was 15; for a short time in 1976, he lived with his mother and half-sister Tina in California. Hutchence later returned to Sydney with his mother.

In 1977, a new band, The Farriss Brothers, was formed with Tim Farriss on lead guitar, his younger brother Andrew as keyboardist, and youngest brother Jon on drums. Andrew brought Hutchence on board as a vocalist and Beers on bass guitar, and Tim brought in his former bandmate Kirk Pengilly to play guitar and saxophone. The band made their debut on 16 August 1977 at Whale Beach, 40 km (25 mi) north of Sydney.

Hutchence, the Farriss brothers, Kerny, Sanders, Beers and Kennelly briefly performed as The Vegetables, singing "We Are the Vegetables". Ten months later, they returned to Sydney and recorded a set of demos. The Farriss Brothers regularly supported hard rockers Midnight Oil on the pub rock circuit, and were renamed as INXS in 1979. Their first performance under the new name was on 1 September at the Oceanview Hotel in Toukley. In May 1980, the group released their first single, "Simple Simon"/"We Are the Vegetables" which was followed by the debut album "INXS" in October. Their first Top 40 Australian hit on the Kent Music Report Singles Chart, "Just Keep Walking", was released in September 1980.

Hutchence became the main spokesperson for the band. He co-wrote almost all of INXS's songs with Andrew Farriss.

According to Hutchence, most of the songs on the band's second album, "Underneath the Colours", were written within a fairly short space of time: "Most bands shudder at the prospect of having 20 years to write their first album and four days to write their second. For us, though, it was good. It left less room for us to go off on all sorts of tangents". Soon after recording sessions for "Underneath the Colours" – produced by Richard Clapton – had finished, band members started work on outside projects. Hutchence recorded "Speed Kills", written by Don Walker of hard rockers Cold Chisel, for the "Freedom" (1982) film soundtrack, directed by Scott Hicks. It was Hutchence's first solo single and was released by WEA in early 1982.

In March 1985, after Hutchence and INXS recorded their album "The Swing" (1984), WEA released the Australian version of "Dekadance", as a limited edition cassette only EP of six tracks including remixes from the album. The cassette also included a cover version of Nancy Sinatra and Lee Hazlewood's hit "Jackson", which Hutchence sang as a duet with Jenny Morris, a backing singer for "The Swing" sessions. The EP reached No 2 on the Kent Music Report Albums Chart. Hutchence provided vocals for new wave band Beargarden's 1985 single release.

On 19 May, INXS won seven awards at the 1984 "Countdown" Music and Video Awards ceremony, including 'Best Songwriter' for Hutchence and Andrew, and 'Most Popular Male' for Hutchence. They performed "Burn for You", dressed in Akubras (a brand of hats) and Drizabones (a brand of outdoor coats/oilskin jackets) followed by Hutchence and Morris singing "Jackson" to close.

In 1986, Hutchence played Sam, the lead male role, in the Australian film "Dogs in Space", directed by long-time INXS music video collaborator Richard Lowenstein. Sam's girlfriend, Anna, was portrayed by Saskia Post as a "fragile peroxide blonde in op-shop clothes". Hutchence provided four songs on the film's soundtrack. 

Late in 1986, before commencing work on a new INXS album and while supposedly taking an eight-month break, the band's management decided to stage the Australian Made tour as a series of major outdoor concerts across the country. The roster featured INXS, Jimmy Barnes (Cold Chisel), Models, Divinyls, Mental as Anything, The Triffids and I'm Talking. To promote the tour, Hutchence and Barnes shared vocals on The Easybeats cover "Good Times" and "Laying Down the Law", which Barnes cowrote with Beers, Andrew Farriss, Jon Farriss, Hutchence and Pengilly. "Good Times" was used as the theme for the concert series of 1986–1987. It peaked at No. 2 on the Australian charts, and months later was featured in the Joel Schumacher film "The Lost Boys" and its soundtrack, allowing it to peak at No. 47 in the U.S. on 1 August 1987. Divinyls' lead singer Chrissie Amphlett enjoyed the tour and reconnected with Hutchence, stating that "[he] was a sweet man, who said in one interview that he wanted me to have his baby."

In 1987, Hutchence provided vocals for Richard Clapton's album "Glory Road", which was produced by Jon Farriss.

INXS released "Kick" in October 1987, and the album provided the band with worldwide popularity. "Kick" peaked at No. 1 in Australia, No. 3 on the US "Billboard" 200, No. 9 in UK, and No. 15 in Austria. The band's most successful studio album, "Kick" has been certified six times platinum by the RIAA and spawned four US top 10 singles ("New Sensation", "Never Tear Us Apart", "Devil Inside" and "Need You Tonight", the last of which reached the top of the US "Billboard" singles charts). According to "1001 Songs: The Great Songs of All Time and the Artists, Stories and Secrets Behind Them", the single "Need You Tonight" is not lyrically complex; it is Hutchence's performance where "he sings in kittenish whisper, gently drawing back with the incredible lust of a tiger hunting in the night" that makes the song "as sexy and funky as any white rock group has ever been". In September 1988, the band swept the MTV Video Music Awards with the video for "Need You Tonight/Mediate" winning in five categories.

In 1989, Hutchence collaborated further with Olsen for the Max Q project, and was joined by members of Olsen's previous groups including Whirlywirld, No and Orchestra of Skin and Bone. They released a self-titled album and three singles, "Way of the World", "Sometimes" and "Monday Night by Satellite". Max Q disbanded in 1990. "Max Q" showed Hutchence exploring the darker side of his music and, with Olsen, he created "one of the most innovative dance music albums of the decade". Hutchence wrote most of the music and provided "an extraordinary performance ... it was one of the most significant statements Hutchence was to make". In 1990, Hutchence portrayed nineteenth-century Romantic poet Percy Shelley in Roger Corman's film version of "Frankenstein Unbound", which was based on a science fiction time travel story of the same name written by Brian Aldiss.

In 1990, INXS released "X", which spawned more international hits such as "Suicide Blonde" and "Disappear" (both Top 10 in the US). "Suicide Blonde" peaked at No. 2 in Australia and No. 11 in the UK. Hutchence, with Andrew Farriss, wrote the song after Hutchence's then-girlfriend, Kylie Minogue, used the phrase "suicide blonde" to describe her look during her 1989 film, "The Delinquents"; the film depicted Minogue in a platinum blonde wig. Hutchence won the 'Best International Artist' at the 1991 BRIT Awards with INXS winning the related group award. Hutchence provided vocals for pub rockers Noiseworks' album, "Love Versus Money" (1991).
"Welcome to Wherever You Are" was released by INXS in August 1992. It received good critical reviews and went to No. 1 in the UK.

Hutchence and INXS faced reduced commercial success with "Full Moon, Dirty Hearts", especially in the U.S. The band took time off to rest and be with their families, while Hutchence remained in the public eye through his romances. He commenced work on a self-titled solo album in the mid-1990s.

After a period of inactivity and releases that received lukewarm reviews, INXS recorded the band's 10th official album, "Elegantly Wasted", in 1996.

Hutchence was a baritone. In 2013, News.com.au ranked Hutchence fourth in a list of the 15 greatest Australian singers of all time. Billboard described Hutchence as "charismatic," with a "seductive purr and [a] lithe, magnetic stage presence." Paul Donoughue of ABC.net.au wrote that Hutchence had "a phenomenal voice — moody, sexual, and dynamic, able to shift effortlessly from fragile to cocksure." Reviewing an INXS concert, Dave Simpson of "The Guardian" wrote, "Watching Hutchence, hair flailing, crotch thrusting, a mischievous smile forever creeping across his leathery face, I realised that here was a man born to be onstage, living and loving every minute, an explosion of sexual energy". Hutchence biographer Toby Creswell asserted that "Hutchence was, without question, one of the truly great frontmen — he expressed the music in a dynamic way that few others could."

According to "People", Hutchence's "public brawls and onetime open drug use led London tabloids to dub him the 'wild man of rock.'" Hutchence was romantically linked to Kylie Minogue, Belinda Carlisle, Helena Christensen, and Kym Wilson.

In August 1992, Helena Christensen and Hutchence were walking late at night on a street in Copenhagen after drinking heavily when he refused to move for a taxi. The taxi driver then assaulted him, causing him to fall backwards and hit his head on the roadway. Hutchence suffered a fractured skull in the altercation. Hutchence did not immediately seek medical assistance for the injury, instead waiting several days before seeing a doctor. As a result, Hutchence's fractured skull left him with an almost complete loss of the sense of smell and significant loss of taste. This injury led to periods of depression and increased levels of aggression; he had not fully recovered after two weeks in a Copenhagen hospital. According to INXS bandmate Beers, Hutchence pulled a knife and threatened to kill him during the 1993 recording of "Full Moon, Dirty Hearts" on the isle of Capri. Beers said: "Over those six weeks, Michael threatened or physically confronted nearly every member of the band."

In the mid-1990s, Hutchence became romantically involved with Paula Yates. He had met Yates in 1985, during an interview for her program, "The Tube". Yates interviewed Hutchence again in 1994 for her "Big Breakfast" show, and their affair was soon uncovered by the British press. At the time, Yates was married to The Boomtown Rats' lead singer and Live Aid organiser Bob Geldof. Media scrutiny was intense, and Hutchence assaulted a photographer who had followed the couple. Yates' separation from Geldof in February 1995 sparked a public and at times bitter custody battle over their daughters. Yates and Geldof divorced in May 1996. On 22 July 1996, Yates gave birth to Hutchence's daughter, Heavenly Hiraani Tiger Lily Hutchence.

In September 1996, Yates and Hutchence made headlines when they were arrested for suspicion of drug possession after the family nanny reportedly found a small amount of opium in a shoebox underneath their bed. The case was later dropped due to lack of evidence.

Hutchence and INXS went on a world tour to support the April 1997 release of "Elegantly Wasted". The final 20th anniversary tour was to occur in Australia in November and December. During the tour, Yates planned to visit Hutchence with their daughter and Yates's three children, but Geldof had taken legal action to prevent the visit.

On the morning of 22 November 1997, Hutchence, aged 37, was found dead in Room 524 at the Ritz-Carlton hotel in Double Bay, Sydney.
Actress Kym Wilson was the last person to see Hutchence alive, after partying with him in his hotel room prior to his death.
Geldof and Yates each gave police statements on the phone calls they exchanged with Hutchence on the morning of his death; however, they did not volunteer their phone records. Yates's statement on 26 November indicated that she had informed Hutchence of the Geldof girls' custody hearing being adjourned until 17 December, which meant that Yates would not be able to bring Tiger and the Geldof girls to Australia for a visit as previously intended. According to Yates, Hutchence "was frightened and couldn't stand a minute more without his baby... [he] was terribly upset and he said, 'I don't know how I'll live without seeing Tiger'". Yates indicated that Hutchence said he was going to phone Geldof "to let the girls come to Australia". 

Geldof's police statements and evidence to the coroner indicated that Geldof did receive a call from Hutchence, who was "hectoring and abusive and threatening" during their phone conversation. The occupant in the room next to Hutchence's heard a loud male voice and swearing at about 5 am; the coroner was satisfied that this was Hutchence arguing with Geldof.

At 9:54 am on 22 November, Hutchence spoke with a former girlfriend, Michèle Bennett; according to Bennett, Hutchence was crying, sounded upset, and told her he needed to see her. Bennett arrived at his hotel room door at about 10:40 am, but there was no response. Hutchence's body was discovered by a hotel maid at 11:50 am. Police reported that Hutchence was found "in a kneeling position facing the door. He had used his snakeskin belt to tie a knot on the automatic door closure at the top of the door, and had strained his head forward into the loop so hard that the buckle had broken."

On 6 February 1998, after an autopsy and coronial inquest, New South Wales State Coroner, Derrick Hand, presented his report. The report ruled that Hutchence's death was suicide while depressed and under the influence of alcohol and other drugs. "An analysis report of Hutchence's blood [indicated] the presence of alcohol, cocaine, Prozac and prescription drugs." In producing his coroner's report, Hand had specifically considered the suggestions of accidental death (coupled with the fact that Hutchence left no suicide note), but had discounted them based on substantial evidence presented to the contrary. In a 1999 interview on "60 Minutes" (and in a documentary film on Channel 4), Yates claimed that Hutchence's death may have resulted from autoerotic asphyxiation; this claim contradicted her previous statements to police investigators and the coroner.

On 27 November 1997, Hutchence's funeral was held at St Andrew's Cathedral, Sydney. His casket was carried out of the cathedral by members of INXS and by his younger brother, Rhett; "Never Tear Us Apart" was played in the background. Nick Cave, a friend of Hutchence, performed his 1997 song "Into My Arms" during the funeral and requested that television cameras be switched off. Rhett claimed in his 2004 book, "Total XS", that on the previous day at the funeral parlour, Yates had put a gram of heroin into Hutchence's pocket.

Following Hutchence's death, INXS continued recording and performing until 2012. According to the Recording Industry Association of America (RIAA), INXS has sold 30 million units in the United States alone, making them the second-highest selling Australian music act in the United States behind AC/DC. INXS has sold over 60 million records worldwide. INXS was inducted into the ARIA Hall of Fame in 2001.

Hutchence's solo album, "Michael Hutchence", was released in October 1999. He had started on the album in 1995, recording songs in between INXS sessions; he had last worked on it three days prior to his death. The last song he recorded was "Possibilities". The album includes "Slide Away", a duet with U2's Bono; Bono's vocals were recorded after Hutchence's death.

The 1999 movie "Limp" includes a cameo by Hutchence.

On 18 June 2000, Patricia Glassop and Tina Schorr released their book, "Just a Man: The Real Michael Hutchence", which has been described as "an odd biography ... [that] combines the basic facts of Hutchence's early life ... with an almost too-intimate view of the authors' feelings".

Paula Yates died on 17 September 2000 of an accidental heroin overdose; she was discovered in the presence of Hutchence's then-four-year-old daughter, Tiger. Soon after Yates's death, Geldof assumed foster custody of Tiger so that she could be brought up with her three older half-sisters, Fifi, Peaches and Pixie. In 2007, Tiger was adopted by Bob Geldof, the father of her half-sisters. As of 2019, Tiger's legal name is Heavenly Hiraani Tiger Lily Hutchence Geldof. 

On 12 December 2002, Hutchence's father, Kelland, died of cancer in Sydney. Kelland had helped create and maintain a memorial website for his son.

On 20 August 2005, Melbourne's "The Age" reported on the disposition of Hutchence's estate and assets, estimated at between $10 to $20 million but containing virtually nothing. The remainder of his estate had reportedly been sold off or swallowed in legal fees.

In July 2009, Hutchence's mother, Patricia Glassop, protested that Geldof had prevented access to her granddaughter for three years. Glassop died on 21 September 2010.

A documentary about Hutchence entitled "Michael Hutchence: The Last Rockstar" aired in 2017. In 2019, "Mystify: Michael Hutchence"—another documentary about Hutchence's life—was released.





General

Specific



</doc>
<doc id="20270" url="https://en.wikipedia.org/wiki?curid=20270" title="Motorola 68000">
Motorola 68000

The Motorola 68000 ("'sixty-eight-thousand'"; also called the m68k or Motorola 68k, ""sixty-eight-kay"") is a 16/32-bit CISC microprocessor, introduced in 1979 by Motorola Semiconductor Products Sector.

The design implements a 32-bit instruction set, with 32-bit registers and a 32-bit internal data bus. The address bus is 24-bits and does not use memory segmentation, which made it popular with programmers. Internally, it uses a 16-bit data ALU and two additional 16-bit ALUs used mostly for addresses, and has a 16-bit external data bus. For this reason, Motorola referred to it as a 16/32-bit processor.

As one of the first widely available processors with a 32-bit instruction set, and running at relatively high speeds for the era, the 68k was a popular design through the 1980s. It was widely used in a new generation of personal computers with graphical user interfaces, including the Apple Macintosh, Commodore Amiga, Atari ST and many others. It competed primarily against the Intel 8088, found in the IBM PC, which it easily outperformed. The 68k and 8088 pushed other designs, like the Zilog Z8000 and , into niche markets, and made Motorola a major player in the CPU space.

The 68k was soon expanded with additional family members, implementing full 32-bit ALUs as part of the growing Motorola 68000 series. The original 68k is generally software forward-compatible with the rest of the line despite being limited to a 16-bit wide external bus. After 40 years in production, the 68000 architecture is still in use.

Motorola's first widely-produced CPU was the Motorola 6800. Although a capable design, it was eclipsed by more powerful designs, such as the Zilog Z80, and less powerful but faster designs, such as the MOS 6502. As the sales prospects of the 6800 dimmed, Motorola began a totally new design to replace it. This became the Motorola Advanced Computer System on Silicon project, or MACSS, begun in 1976.

The MACSS aimed to develop an entirely new architecture without backward compatibility with the 6800. It ultimately did retain a bus protocol compatibility mode for existing 6800 peripheral devices, and a version with an 8-bit data bus was produced. However, the designers mainly focused on the future, or forward compatibility, which gave the 68000 design a head start against later 32-bit instruction set architectures. For instance, the CPU registers are 32 bits wide, though few self-contained structures in the processor itself operate on 32 bits at a time. The MACSS team drew heavily on the influence of minicomputer processor design, such as the PDP-11 and VAX systems, which were similarly microcode-based.

In the mid 1970s, the 8-bit microprocessor manufacturers raced to introduce the 16-bit generation. National Semiconductor had been first with its IMP-16 and PACE processors in 1973–1975, but these had issues with speed. Intel had worked on their advanced 16/32-bit Intel iAPX 432 (alias 8800) since 1975 and their Intel 8086 since 1976 (it was introduced in 1978 but became really widespread in the form of the almost identical 8088 in the IBM PC a few years later). Arriving late to the 16-bit arena afforded the new processor more transistors (roughly 40,000 active versus 20,000 active in the 8086), 32-bit macroinstructions, and acclaimed general ease of use.

The original MC68000 was fabricated using an HMOS process with a 3.5 µm feature size. Formally introduced in September 1979, initial samples were released in February 1980, with production chips available over the counter in November. Initial speed grades were 4, 6, and 8 MHz. 10 MHz chips became available during 1981, and 12.5 MHz chips by June 1982. The 16.67 MHz "12F" version of the MC68000, the fastest version of the original HMOS chip, was not produced until the late 1980s. 

The 68k instruction set was particularly well suited to implement Unix, and the 68000 and its successors became the dominant CPUs for Unix-based workstations including Sun workstations and Apollo/Domain workstations. The 68000 also was used for mass-market computers such as the Apple Lisa, Macintosh, Amiga, and Atari ST. The 68000 was used in Microsoft Xenix systems, as well as an early NetWare Unix-based Server. The 68000 was used in the first generation of desktop laser printers, including the original Apple Inc. LaserWriter and the HP LaserJet.

In 1982, the 68000 received a minor update to its ISA to support virtual memory and to conform to the Popek and Goldberg virtualization requirements. The updated chip was called the 68010. It also added a new "loop mode" which sped up small loops, and increased overall performance by about 10% at the same clock speeds. A further extended version, which exposed 31 bits of the address bus, was also produced in small quantities as the 68012.

To support lower-cost systems and control applications with smaller memory sizes, Motorola introduced the 8-bit compatible MC68008, also in 1982. This was a 68000 with an 8-bit data bus and a smaller (20-bit) address bus. After 1982, Motorola devoted more attention to the 68020 and 88000 projects.

Several other companies were second-source manufacturers of the HMOS 68000. These included Hitachi (HD68000), who shrank the feature size to 2.7 µm for their 12.5 MHz version, Mostek (MK68000), Rockwell (R68000), Signetics (SCN68000), Thomson/SGS-Thomson (originally EF68000 and later TS68000), and Toshiba (TMP68000). Toshiba was also a second-source maker of the CMOS 68HC000 (TMP68HC000).

Encrypted variants of the 68000, being the Hitachi FD1089 and FD1094, store decryption keys for opcodes and opcode data in battery-backed memory and were used in certain Sega arcade systems including System 16 to prevent piracy and illegal bootleg games.

The 68HC000, the first CMOS version of the 68000, was designed by Hitachi and jointly introduced in 1985. Motorola's version was called the MC68HC000, while Hitachi's was the HD68HC000. The 68HC000 was eventually offered at speeds of 8–20 MHz. Except for using CMOS circuitry, it behaved identically to the HMOS MC68000, but the change to CMOS greatly reduced its power consumption. The original HMOS MC68000 consumed around 1.35 watts at an ambient temperature of 25 °C, regardless of clock speed, while the MC68HC000 consumed only 0.13 watts at 8 MHz and 0.38 watts at 20 MHz. (Unlike CMOS circuits, HMOS still draws power when idle, so power consumption varies little with clock rate.) Apple selected the 68HC000 for use in the Macintosh Portable.

Motorola replaced the MC68008 with the MC68HC001 in 1990. This chip resembled the 68HC000 in most respects, but its data bus could operate in either 16-bit or 8-bit mode, depending on the value of an input pin at reset. Thus, like the 68008, it could be used in systems with cheaper 8-bit memories.

The later evolution of the 68000 focused on more modern embedded control applications and on-chip peripherals. The 68EC000 chip and SCM68000 core removed the M6800 peripheral bus, and excluded the MOVE from SR instruction from user mode programs, making the 68EC000 and 68SEC000 the only 68000 CPUs not 100% object code compatible with previous 68000 CPUs when run in User Mode. When run in Supervisor Mode, there was no difference. In 1996, Motorola updated the standalone core with fully static circuitry, drawing only 2 µW in low-power mode, calling it the MC68SEC000.

Motorola ceased production of the HMOS MC68000 and MC68008 in 1996, but its spin-off company Freescale Semiconductor was still producing the MC68HC000, MC68HC001, MC68EC000, and MC68SEC000, as well as the MC68302 and MC68306 microcontrollers and later versions of the DragonBall family. The 68000's architectural descendants, the 680x0, CPU32, and Coldfire families, were also still in production. More recently, with the Sendai fab closure, all 68HC000, 68020, 68030, and 68882 parts have been discontinued, leaving only the 68SEC000 in production.

After being succeeded by "true" 32-bit microprocessors, the 68000 was used as the core of many microcontrollers. In 1989, Motorola introduced the MC68302 communications processor.

At its introduction, the 68000 was first used in high-priced systems, including multiuser microcomputers like the WICAT 150, early Alpha Microsystems computers, Sage II / IV, Tandy TRS-80 Model 16, and ; single-user workstations such as Hewlett-Packard's HP 9000 Series 200 systems, the first Apollo/Domain systems, Sun Microsystems' Sun-1, and the Corvus Concept; and graphics terminals like Digital Equipment Corporation's VAXstation 100 and Silicon Graphics' IRIS 1000 and 1200. Unix systems rapidly moved to the more capable later generations of the 68k line, which remained popular in that market throughout the 1980s.

By the mid-1980s, falling production cost made the 68000 viable for use in personal and home computers, starting with the Apple Lisa and Macintosh, and followed by the Commodore Amiga, Atari ST, and Sharp X68000. On the other hand, the Sinclair QL microcomputer was the most commercially important utilisation of the 68008, along with its derivatives, such as the ICL One Per Desk business terminal. Helix Systems (in Missouri, United States) designed an extension to the SWTPC SS-50 bus, the SS-64, and produced systems built around the 68008 processor.

While the adoption of RISC and x86 displaced the 68000 series as desktop/workstation CPU, the processor found substantial use in embedded applications. By the early 1980s, quantities of 68000 CPUs could be purchased for less than 30 USD per part.

Video game manufacturers used the 68000 as the backbone of many arcade games and home game consoles: Atari's "Food Fight", from 1982, was one of the first 68000-based arcade games. Others included Sega's System 16, Capcom's CP System and CPS-2, and SNK's Neo Geo. By the late 1980s, the 68000 was inexpensive enough to power home game consoles, such as Sega's Mega Drive/Genesis console and also the Sega CD attachment for it (A Sega CD system has three CPUs, two of them 68000s). The 1993 multi-processor Atari Jaguar console used a 68000 as a support chip, although some developers used it as the primary processor due to familiarity. The 1994 multi-processor Sega Saturn console used the 68000 as a sound co-processor (much as the Mega Drive/Genesis uses the Z80 as a co-processor for sound and/or other purposes).

Certain arcade games (such as "Steel Gunner" and others based on Namco System 2) use a dual 68000 CPU configuration, and systems with a triple 68000 CPU configuration also exist (such as "Galaxy Force" and others based on the Sega Y Board), along with a quad 68000 CPU configuration, which has been used by Jaleco (one 68000 for sound has a lower clock rate compared to the other 68000 CPUs) for games such as "Big Run" and "Cisco Heat"; a fifth 68000 (at a different clock rate compared to the other 68000 CPUs) was additionally used in the Jaleco arcade game "Wild Pilot" for I/O processing.

The 68000 also saw great success as an embedded controller. As early as 1981, laser printers such as the Imagen Imprint-10 were controlled by external boards equipped with the 68000. The first HP LaserJet—introduced in 1984—came with a built-in 8 MHz 68000. Other printer manufacturers adopted the 68000, including Apple with its introduction of the LaserWriter in 1985, the first PostScript laser printer. The 68000 continued to be widely used in printers throughout the rest of the 1980s, persisting well into the 1990s in low-end printers.

The 68000 also saw success in the field of industrial control systems. Among the systems benefited from having a 68000 or derivative as their microprocessor were families of programmable logic controllers (PLCs) manufactured by Allen-Bradley, Texas Instruments and subsequently, following the acquisition of that division of TI, by Siemens. Users of such systems do not accept product obsolescence at the same rate as domestic users, and it is entirely likely that despite having been installed over 20 years ago, many 68000-based controllers will continue in reliable service well into the 21st century.

In a number of digital oscilloscopes from the 80s, the 68000 has been used as a waveform display processor; some models including the LeCroy 9400/9400A also use the 68000 as a waveform math processor (including addition, subtraction, multiplication, and division of two waveforms/references/waveform memories), and some digital oscilloscopes using the 68000 (including the 9400/9400A) can also perform fast Fourier transform functions on a waveform.

The 683XX microcontrollers, based on the 68000 architecture, are used in networking and telecom equipment, television set-top boxes, laboratory and medical instruments, and even handheld calculators. The MC68302 and its derivatives have been used in many telecom products from Cisco, 3com, Ascend, Marconi, Cyclades and others. Past models of the Palm PDAs and the Handspring Visor used the DragonBall, a derivative of the 68000. AlphaSmart uses the DragonBall family in later versions of its portable word processors. Texas Instruments uses the 68000 in its high-end graphing calculators, the TI-89 and TI-92 series and Voyage 200. Early versions of these used a specialized microcontroller with a static 68EC000 core; later versions use a standard MC68SEC000 processor.

A modified version of the 68000 formed the basis of the IBM XT/370 hardware emulator of the System 370 processor.

The 68000 has a 24-bit external address bus and two byte-select signals "replaced" A0. These 24 lines can therefore address 16 MB of physical memory with byte resolution. Address storage and computation uses 32 bits internally; however, the 8 high-order address bits are ignored due to the physical lack of device pins. This allows it to run software written for a logically flat 32-bit address space, while accessing only a 24-bit physical address space. Motorola's intent with the internal 32-bit address space was forward compatibility, making it feasible to write 68000 software that would take full advantage of later 32-bit implementations of the 68000 instruction set.

However, this did not prevent programmers from writing forward incompatible software. "24-bit" software that discarded the upper address byte, or used it for purposes other than addressing, could fail on 32-bit 68000 implementations. For example, early (pre-7.0) versions of Apple's Mac OS used the high byte of memory-block master pointers to hold flags such as "locked" and "purgeable". Later versions of the OS moved the flags to a nearby location, and Apple began shipping computers which had "32-bit clean" ROMs beginning with the release of the 1989 Mac IIci.

The 68000 family stores multi-byte integers in memory in big-endian order.

The CPU has eight 32-bit general-purpose data registers (D0-D7), and eight address registers (A0-A7). The last address register is the stack pointer, and assemblers accept the label SP as equivalent to A7. This was a good number of registers at the time in many ways. It was small enough to allow the 68000 to respond quickly to interrupts (even in the worst case where all 8 data registers D0–D7 and 7 address registers A0–A6 needed to be saved, 15 registers in total), and yet large enough to make most calculations fast, because they could be done entirely within the processor without keeping any partial results in memory. (Note that an exception routine in supervisor mode can also save the user stack pointer A7, which would total 8 address registers. However, the dual stack pointer (A7 and supervisor-mode A7') design of the 68000 makes this normally unnecessary, except when a task switch is performed in a multitasking system.)

Having two types of registers was mildly annoying at times, but not hard to use in practice. Reportedly, it allowed the CPU designers to achieve a higher degree of parallelism, by using an auxiliary execution unit for the address registers.

The 68000 comparison, arithmetic, and logic operations set bit flags in a status register to record their results for use by later conditional jumps. The bit flags are "zero" (Z), "carry" (C), "overflow" (V), "extend" (X), and "negative" (N). The "extend" (X) flag deserves special mention, because it is separate from the carry flag. This permits the extra bit from arithmetic, logic, and shift operations to be separated from the carry for flow-of-control and linkage.

The designers attempted to make the assembly language orthogonal. That is, instructions are divided into operations and address modes, and almost all address modes are available for almost all instructions. There are 56 instructions and a minimum instruction size of 16 bits. Many instructions and addressing modes are longer to include additional address or mode bits.

The CPU, and later the whole family, implements two levels of privilege. User mode gives access to everything except privileged instructions such as interrupt level controls. Supervisor privilege gives access to everything. An interrupt always becomes supervisory. The supervisor bit is stored in the status register, and is visible to user programs.

An advantage of this system is that the supervisor level has a separate stack pointer. This permits a multitasking system to use very small stacks for tasks, because the designers do not have to allocate the memory required to hold the stack frames of a maximum stack-up of interrupts.

The CPU recognizes seven interrupt levels. Levels 1 through 5 are strictly prioritized. That is, a higher-numbered interrupt can always interrupt a lower-numbered interrupt. In the status register, a privileged instruction allows one to set the current minimum interrupt level, blocking lower or equal priority interrupts. For example, if the interrupt level in the status register is set to 3, higher levels from 4 to 7 can cause an exception. Level 7 is a level triggered non-maskable interrupt (NMI). Level 1 can be interrupted by any higher level. Level 0 means no interrupt. The level is stored in the status register, and is visible to user-level programs.

Hardware interrupts are signalled to the CPU using three inputs that encode the highest pending interrupt priority. A separate Encoder is usually required to encode the interrupts, though for systems that do not require more than three hardware interrupts it is possible to connect the interrupt signals directly to the encoded inputs at the cost of additional software complexity. The interrupt controller can be as simple as a 74LS148 priority encoder, or may be part of a VLSI peripheral chip such as the MC68901 Multi-Function Peripheral (used in the Atari ST range of computers and Sharp X68000), which also provided a UART, timer, and parallel I/O.

The "exception table" (interrupt vector table interrupt vector addresses) is fixed at addresses 0 through 1023, permitting 256 32-bit vectors. The first vector (RESET) consists of two vectors, namely the starting stack address, and the starting code address. Vectors 3 through 15 are used to report various errors: bus error, address error, illegal instruction, zero division, CHK and CHK2 vector, privilege violation (to block privilege escalation), and some reserved vectors that became line 1010 emulator, line 1111 emulator, and hardware breakpoint. Vector 24 starts the real interrupts: spurious interrupt (no hardware acknowledgement), and level 1 through level 7 autovectors, then the 16 TRAP vectors, then some more reserved vectors, then the user defined vectors.

Since at a minimum the starting code address vector must always be valid on reset, systems commonly included some nonvolatile memory (e.g. ROM) starting at address zero to contain the vectors and bootstrap code. However, for a general purpose system it is desirable for the operating system to be able to change the vectors at runtime. This was often accomplished by either pointing the vectors in ROM to a jump table in RAM, or through use of bank switching to allow the ROM to be replaced by RAM at runtime.

The 68000 does not meet the Popek and Goldberg virtualization requirements for full processor virtualization because it has a single unprivileged instruction "MOVE from SR", which allows user-mode software read-only access to a small amount of privileged state. The 68EC000 and 68SEC000, which are later derivatives of the 68000 are, however, as the Move from SR-instruction is privileged. The same change was introduced on the 68010 and later CPUs.

The 68000 is also unable to easily support virtual memory, which requires the ability to trap and recover from a failed memory access. The 68000 does provide a bus error exception which can be used to trap, but it does not save enough processor state to resume the faulted instruction once the operating system has handled the exception. Several companies did succeed in making 68000-based Unix workstations with virtual memory that worked by using two 68000 chips running in parallel on different phased clocks. When the "leading" 68000 encountered a bad memory access, extra hardware would interrupt the "main" 68000 to prevent it from also encountering the bad memory access. This interrupt routine would handle the virtual memory functions and restart the "leading" 68000 in the correct state to continue properly synchronized operation when the "main" 68000 returned from the interrupt.

These problems were fixed in the next major revision of the 68k architecture, with the release of the MC68010. The Bus Error and Address Error exceptions push a large amount of internal state onto the supervisor stack in order to facilitate recovery, and the MOVE from SR instruction was made privileged. A new unprivileged "MOVE from CCR" instruction is provided for use in its place by user mode software; an operating system can trap and emulate user-mode MOVE from SR instructions if desired.

The standard addressing modes are:


Plus: access to the status register, and, in later models, other special registers.

Most instructions have dot-letter suffixes, permitting operations to occur on 8-bit bytes (".b"), 16-bit words (".w"), and 32-bit longs (".l").

Like many CPUs of its era the cycle timing of some instructions varied depending on the source operand(s). For example, the unsigned multiply instruction takes (38+2n) clock cycles to complete where 'n' is equal to the number of bits set in the operand. To create a function that took a fixed cycle count required the addition of extra code after the multiply instruction. This would typically consume extra cycles for each bit that wasn't set in the original multiplication operand.

Most instructions are dyadic, that is, the operation has a source, and a destination, and the destination is changed. Notable instructions were:


The 68EC000 is a low-cost version of the 68000 with a slightly different pinout, designed for embedded controller applications. The 68EC000 can have either a 8-bit or 16-bit data bus, switchable at reset.

The processors are available in a variety of speeds including 8 and 16 MHz configurations, producing 2,100 and 4,376 Dhrystones each. These processors have no floating-point unit, and it is difficult to implement an FPU coprocessor (MC68881/2) with one because the EC series lacks necessary coprocessor instructions.

The 68EC000 was used as a controller in many audio applications, including Ensoniq musical instruments and sound cards, where it was part of the MIDI synthesizer. On Ensoniq sound boards, the controller provided several advantages compared to competitors without a CPU on board. The processor allowed the board to be configured to perform various audio tasks, such as MPU-401 MIDI synthesis or MT-32 emulation, without the use of a TSR program. This improved software compatibility, lowered CPU usage, and eliminated host system memory usage.

The Motorola 68EC000 core was later used in the m68k-based DragonBall processors from Motorola/Freescale.

It also was used as a sound controller in the Sega Saturn game console and as a controller for the HP JetDirect Ethernet controller boards for the mid-1990s LaserJet printers.

The 68000 assembly code below is for a subroutine named , which copies a null-terminated string of 8-bit characters to a destination string, converting all alphabetic characters to lower case.

The subroutine establishes a call frame using register A6 as the frame pointer. This kind of calling convention supports reentrant and recursive code and is typically used by languages like C and C++. The subroutine then retrieves the parameters passed to it ( and ) from the stack. It then loops, reading an ASCII character (a single byte) from the string, checking whether it is a capital alphabetic character, and if so, converting it into a lower-case character, otherwise leaving it as it is, then writing the character into the string. Finally, it checks whether the character was a null character; if not, it repeats the loop, otherwise it restores the previous stack frame (and A6 register) and returns. Note that the string pointers (registers A0 and A1) are auto-incremented in each iteration of the loop.
In contrast, the code below is for a stand-alone function, even on the most restrictive version of AMS for the TI-89 series of calculators, being kernel-independent, with no values looked up in tables, files or libraries when executing, no system calls, no exception processing, minimal registers to be used, nor the need to save any. It is valid for historical Julian dates from 1 March 1 AD, or for Gregorian ones. In less than two dozen operations it calculates a day number compatible with ISO 8601 when called with three inputs stored at their corresponding LOCATIONS:





</doc>
<doc id="20272" url="https://en.wikipedia.org/wiki?curid=20272" title="Minicomputer">
Minicomputer

A minicomputer, or colloquially mini, is a class of smaller computers that was developed in the mid-1960's and sold for much less than mainframe and mid-size computers from IBM and its direct competitors. In a 1970 survey, "The New York Times" suggested a consensus definition of a minicomputer as a machine costing less than (), with an input-output device such as a teleprinter and at least four thousand words of memory, that is capable of running programs in a higher level language, such as Fortran or BASIC. The class formed a distinct group with its own software architectures and operating systems. Minis were designed for control, instrumentation, human interaction, and communication switching as distinct from calculation and record keeping. Many were sold indirectly to original equipment manufacturers (OEMs) for final end use application. During the two decade lifetime of the minicomputer class (1965–1985), almost 100 companies formed and only a half dozen remained.

When single-chip CPU microprocessors appeared, beginning with the Intel 4004 in 1971, the term "minicomputer" came to mean a machine that lies in the middle range of the computing spectrum, in between the smallest mainframe computers and the microcomputers. The term "minicomputer" is little used today; the contemporary term for this class of system is "midrange computer", such as the higher-end SPARC, Power ISA and Itanium-based systems from Oracle, IBM and Hewlett-Packard.

The term "minicomputer" developed in the 1960s to describe the smaller computers that became possible with the use of transistors and core memory technologies, minimal instructions sets and less expensive peripherals such as the ubiquitous Teletype Model 33 ASR. They usually took up one or a few 19-inch rack cabinets, compared with the large mainframes that could fill a room.

The definition of minicomputer is vague with the consequence that there are a number of candidates for the "first" minicomputer, ranging from the CDC 160 circa 1960 to the DEC PDP-8 circa 1965. An early and highly successful minicomputer was Digital Equipment Corporation's (DEC) 12-bit PDP-8, which was built using discrete transistors and cost from upwards when launched in 1964. Later versions of the PDP-8 took advantage of small-scale integrated circuits. The important precursors of the PDP-8 include the PDP-5, LINC, the TX-0, the TX-2, and the PDP-1. DEC gave rise to a number of minicomputer companies along Massachusetts Route 128, including Data General, Wang Laboratories, Apollo Computer, and Prime Computer.

Minicomputers were also known as midrange computers. They grew to have relatively high processing power and capacity. They were used in manufacturing process control, telephone switching and to control laboratory equipment. In the 1970s, they were the hardware that was used to launch the computer-aided design (CAD) industry and other similar industries where a smaller dedicated system was needed.

The 7400 series of TTL integrated circuits started appearing in minicomputers in the late 1960s. The 74181 arithmetic logic unit (ALU) was commonly used in the CPU data paths. Each 74181 had a bus width of four bits, hence the popularity of "bit-slice" architecture. Some scientific computers, such as the Nicolet 1080, would use the 7400 series in groups of five ICs (parallel) for their uncommon twenty bits architecture. The 7400 series offered data-selectors, multiplexers, three-state buffers, memories, etc. in dual in-line packages with one-tenth inch spacing, making major system components and architecture evident to the naked eye. Starting in the 1980s, many minicomputers used VLSI circuits.

At the launch of the MITS Altair 8800 in 1975, "Radio Electronics" magazine referred to the system as a "minicomputer", although the term microcomputer soon became usual for personal computers based on single-chip microprocessors. At the time, microcomputers were 8-bit single-user, relatively simple machines running simple program-launcher operating systems like CP/M or MS-DOS, while minis were much more powerful systems that ran full multi-user, multitasking operating systems, such as VMS and Unix, and although the classical mini was a 16-bit computer, the emerging higher performance superminis were 32-bit.

The decline of the minis happened due to the lower cost of microprocessor-based hardware, the emergence of inexpensive and easily deployable local area network systems, the emergence of the 68020, 80286 and the 80386 microprocessors, and the desire of end-users to be less reliant on inflexible minicomputer manufacturers and IT departments or "data centers". The result was that minicomputers and computer terminals were replaced by networked workstations, file servers and PCs in some installations, beginning in the latter half of the 1980s.

During the 1990s, the change from minicomputers to inexpensive PC networks was cemented by the development of several versions of Unix and Unix-like systems that ran on the Intel x86 microprocessor architecture, including Solaris, Linux, FreeBSD, NetBSD and OpenBSD. Also, the Microsoft Windows series of operating systems, beginning with Windows NT, now included server versions that supported preemptive multitasking and other features required for servers.

As microprocessors have become more powerful, the CPUs built up from multiple components – once the distinguishing feature differentiating mainframes and midrange systems from microcomputers – have become increasingly obsolete, even in the largest mainframe computers.

Digital Equipment Corporation (DEC) was once the leading minicomputer manufacturer, at one time the second-largest computer company after IBM. But as the minicomputer declined in the face of generic Unix servers and Intel-based PCs, not only DEC, but almost every other minicomputer company including Data General, Prime, Computervision, Honeywell and Wang Laboratories, many based in New England (hence the end of the Massachusetts Miracle), also collapsed or merged. DEC was sold to Compaq in 1998, while Data General was acquired by EMC Corporation.

Today only a few proprietary minicomputer architectures survive. The IBM System/38 operating system, which introduced many advanced concepts, lives on with IBM's AS/400. Realising the importance of the myriad lines of 'legacy code' (programs) written, 'AS' stands for 'Application System'. Great efforts were made by IBM to enable programs originally written for the System/34 and System/36 to be moved to the AS/400. The AS/400 was replaced by the iSeries, which was subsequently replaced by the System i. In 2008, the System i was replaced by the IBM Power Systems. By contrast, competing proprietary computing architectures from the early 1980s, such as DEC's VAX, Wang VS and Hewlett Packard's HP 3000 have long been discontinued without a compatible upgrade path. OpenVMS runs HP Alpha and Intel IA-64 (Itanium) CPU architectures.

Tandem Computers, which specialized in reliable large-scale computing, was acquired by Compaq, and a few years afterward the combined entity merged with Hewlett Packard. The NSK-based NonStop product line was re-ported from MIPS processors to Itanium-based processors branded as 'HP Integrity NonStop Servers'. As in the earlier migration from stack machines to MIPS microprocessors, all customer software was carried forward without source changes. Integrity NonStop continues to be HP's answer for the extreme scaling needs of its very largest customers. The NSK operating system, now termed NonStop OS, continues as the base software environment for the NonStop Servers, and has been extended to include support for Java and integration with popular development tools like Visual Studio and Eclipse.

A variety of companies emerged that built turnkey systems around minicomputers with specialized software and, in many cases, custom peripherals that addressed specialized problems such as computer-aided design, computer-aided manufacturing, process control, manufacturing resource planning, and so on. Many if not most minicomputers were sold through these original equipment manufacturers and value-added resellers.

Several pioneering computer companies first built minicomputers, such as DEC, Data General, and Hewlett-Packard (HP) (who now refers to its HP3000 minicomputers as "servers" rather than "minicomputers"). And although today's PCs and servers are clearly microcomputers physically, architecturally their CPUs and operating systems have developed largely by integrating features from minicomputers.

In the software context, the relatively simple OSs for early microcomputers were usually inspired by minicomputer OSs (such as CP/M's similarity to Digital's single user OS/8 and RT-11 and multi-user RSTS time-sharing system). Also, the multiuser OSs of today are often either inspired by, or directly descended from, minicomputer OSs. UNIX was originally a minicomputer OS, while Windows NT kernel—the foundation for all current versions of Microsoft Windows-borrowed design ideas liberally from VMS. Many of the first generation of PC programmers were educated on minicomputer systems.





</doc>
<doc id="20273" url="https://en.wikipedia.org/wiki?curid=20273" title="March 18">
March 18





</doc>
<doc id="20282" url="https://en.wikipedia.org/wiki?curid=20282" title="Mechanized infantry">
Mechanized infantry

Mechanized infantry (or mechanised infantry) are infantry units equipped with armored personnel carriers (APCs) or infantry fighting vehicles (IFVs) for transport and combat (see also mechanized force).

Mechanized infantry is distinguished from motorized infantry in that its vehicles provide a degree of protection from hostile fire, as opposed to "soft-skinned" wheeled vehicles (trucks or jeeps) for motorized infantry. Most APCs and IFVs are fully tracked or are all-wheel drive vehicles (6×6 or 8×8), for mobility across rough ground. Some nations distinguish between mechanized and armored (or armoured) infantry, designating troops carried by APCs as mechanized and those in IFVs as armored.

The support weapons for mechanized infantry are also provided with motorized transport, or they are built directly into combat vehicles to keep pace with the mechanized infantry in combat. For units equipped with most types of APC or any type of IFV, fire support weapons, such as machine guns, autocannons, small-bore direct-fire howitzers, and anti-tank guided missiles are often mounted directly on the infantry's own transport vehicles.

Compared with "light" truck-mobile infantry, mechanized infantry can maintain rapid tactical movement and, if mounted in IFVs, have more integral firepower. It requires more combat supplies (ammunition and especially fuel) and ordnance supplies (spare vehicle components), and a comparatively larger proportion of manpower is required to crew and maintain the vehicles. For example, most APCs mount a section of seven or eight infantrymen but have a crew of two. Most IFVs carry only six or seven infantry but require a crew of three. To be effective in the field, mechanized units also require many mechanics, with specialized maintenance and recovery vehicles and equipment.

Some of the first mechanized infantry were German assault teams mounted on A7V tanks during World War I. The vehicles were extra-large to let them carry sizeable assault teams and would regularly carry infantry on board in addition to their already large crews that were trained as storm troopers. All machine-gun-armed A7V tanks carried two small flame throwers for their dismounts to use. A7V tank would often carry a second officer to lead the assault team.

During the Battle of St. Quentin, A7Vs were accompanied by 20 storm troopers from Rohr Assault Battalion, but it is unspecified if they were acting as dismounts or were accompanying the tanks on foot. During the battle, tank crews were reported to have dismounted and attacked enemy positions with grenades and flamethrowers on numerous occasions.

Another example of the use of such a method of fighting is the capture of Villers-Bretonneux, in which A7Vs would suppress the defenders with machine gun fire and assault teams would dismount and attack them with grenades.

Towards the end of World War I, all the armies involved were faced with the problem of maintaining the momentum of an attack. Tanks, artillery, or infiltration tactics could all be used to break through an enemy defense, but almost all offensives launched in 1918 ground to a halt after a few days. The following infantry quickly became exhausted, and artillery, supplies and fresh formations could not be brought forward over the battlefields quickly enough to maintain the pressure on the regrouping enemy

It was widely acknowledged that cavalry was too vulnerable to be used on most European battlefields, but many armies continued to deploy them. Motorized infantry could maintain rapid movement, but their trucks required either a good road network or firm open terrain, such as desert. They were unable to traverse a battlefield obstructed by craters, barbed wire, and trenches. Tracked or all-wheel drive vehicles were to be the solution.

Following the war, development of mechanized forces was largely theoretical for some time, but many nations began rearming in the 1930s. The British Army had established an Experimental Mechanized Force in 1927, but it failed to pursue that line because of budget constraints and the prior need to garrison the frontiers of the British Empire.

Although some proponents of mobile warfare, such as J. F. C. Fuller, advocated building "tank fleets", other, such as Heinz Guderian in Germany, Adna R. Chaffee Jr. in the United States, and Mikhail Tukhachevsky in the Soviet Union, recognized that tank units required close support from infantry and other arms and that such supporting arms needed to maintain the same pace as the tanks.

As the Germans rearmed in the 1930s, they equipped some infantry units in their new "Panzer" divisions with the half-track Sd.Kfz. 251, which could keep up with tanks on most terrain. The French Army also created "light mechanized" ("légère mécanisée") divisions in which some of the infantry units possessed small tracked carriers. Together with the motorization of the other infantry and support units, this gave both armies highly mobile combined-arms formations. The German doctrine was to use them to exploit breakthroughs in "Blitzkrieg" offensives, whereas the French envisaged them being used to shift reserves rapidly in a defensive battle.

As World War II progressed, most major armies integrated tanks or assault guns with mechanized infantry, as well as other supporting arms, such as artillery and engineers, as combined arms units.

Allied armored formations included a mechanized infantry element for combined arms teamwork. For example, US armored divisions had a balance of three battalions each of tanks, armored infantry, and self-propelled artillery. The US armored infantry was fully equipped with M2 and M3 halftracks. In the British and Commonwealth armies, "Type A armoured brigades," intended for independent operations or to form part of armored divisions, had a "motor infantry" battalion mounted in Bren Carriers or later in lend-lease halftracks. "Type B" brigades lacked a motor infantry component and were subordinated to infantry formations.

The Canadian Army and, subsequently the British Army, used expedients such as the Kangaroo APC, usually for specific operations rather than to create permanent mechanized infantry formations. The first such operation was Operation Totalize in the Battle of Normandy, which failed to achieve its ultimate objectives but showed that mechanized infantry could incur far fewer casualties than dismounted troops in set-piece operations.

The German Army, having introduced mechanized infantry in its "Panzer" divisions, later named them "Panzergrenadier" units. In the middle of the war, it created entire mechanized infantry divisions and named Panzergrenadier divisions.

Because the German economy could not produce adequate numbers of its half-track APC, barely a quarter or a third of the infantry in Panzer or Panzergrenadier divisions were mechanized, except in a few favored formations. The rest were moved by truck. However, most German reconnaissance units in such formations were also primarily mechanized infantry and could undertake infantry missions when it was needed. The Allies generally used jeeps, armored cars, or light tanks for reconnaissance.

The Red Army began the war while still in the process of reorganizing its armored and mechanized formations, most of which were destroyed during the first months of the German Invasion of the Soviet Union. About a year later, the Soviets recreated division-sized mechanized infantry units, termed mechanized corps, usually with one tank brigade and three mechanized infantry brigades, with motorized supporting arms. They were generally used in the exploitation phase of offensives, as part of the prewar Soviet concept of deep operations.

The Soviet Army also created several cavalry mechanized groups in which tanks, mechanized infantry and horsed cavalry were mixed. They were also used in the exploitation and pursuit phases of offensives. Red Army mechanized infantry were generally carried on tanks or trucks, with only a few dedicated lend-lease half-track APCs.

The New Zealand Army ultimately fielded a division of a roughly similar composition to a Soviet mechanized corps, which fought in the Italian Campaign, but it had little scope for mobile operations until near the end of the war.

The Romanian Army fielded a mixed assortment of vehicles. These amounted to 126 French-designed Renault UE Chenillettes which were licence-built locally, 34 captured and refurbished Soviet armored tractors, 27 German-made armored half-tracks of the Sd.Kfz. 250 and Sd.Kfz. 251 types, over 200 Czechoslovak Tatra, Praga and Skoda trucks (the Tatra trucks were a model which was specifically built for the Romanian Army) as well as 300 German Horch 901 4x4 field cars. Sd.Kfz. 8 and Sd.Kfz. 9 half-tracks were also acquired, as well as nine vehicles of the Sd.Kfz. 10 type and 100 RSO/01 fully tracked tractors. The Romanians also produced five prototypes of an indigenous artillery tractor.

In the postwar era, the early years of the Cold War, the Soviet Army and NATO further developed the equipment and doctrine for mechanized infantry. With the exception of airborne formations, the Red Army mechanized all its infantry formations. Initially, wheeled APCs, like the BTR-152, were used, some of which lacked overhead protection and were therefore vulnerable to artillery fire. It still gave the Soviet Army greater strategic flexibility because of the large land area and the long borders of the Soviet Union and its allies in the Warsaw Pact.

The US Army established the basic configuration of the tracked APC with the M75 and M59 before it adopted the lighter M113, which could be carried by Lockheed C-130 Hercules and other transport aircraft. The vehicle gave infantry the same mobility as tanks but with much less effective armor protection (it still had nuclear, biological, and chemical protection).

In the Vietnam War, the M113 was often fitted with extra armament and used as an "ad hoc" infantry fighting vehicle. Early operations by the Army of the Republic of Vietnam using the vehicle showed that troops were far more effective while they were mounted in the vehicles than when they dismounted. American doctrine subsequently emphasized mounted tactics. The Americans ultimately deployed a mechanized brigade and ten mechanized battalions to Vietnam.

Even more important for future developments was the Soviet BMP-1, which was the first true IFV. Its introduction prompted the development of similar vehicles in Western armies, such as the West German Marder and American M2 Bradley. Unlike the APC, which was intended merely to transport the infantry from place to place under armor, the IFV possessed heavy firepower that could support the infantry in attack or defense. Many IFVs were also equipped with firing ports from which their infantry could fire their weapons from inside, but they were generally not successful and have been dropped from modern IFVs.

Soviet organization led to different tactics between the "light" and the "heavy" varieties of mechanized infantry. In the Soviet Army, a first-line "motor rifle" division from the 1970s onward usually had two regiments equipped with wheeled BTR-60 APCs and one with the tracked BMP-1 IFV. The "light" regiments were intended to make dismounted attacks on the division's flanks, and the BMP-equipped "heavy" regiment remained mounted and supported the division's tank regiment on the main axis of advance. Both types of infantry regiment still were officially titled "motor rifle" units.

A line of development in the Soviet Armed Forces from the 1980s was the provision of specialized IFVs for use by the Russian Airborne Troops. The first of them was the BMD-1, which had the same firepower as the BMP-1 but be carried in or even parachuted from the standard Soviet transport aircraft. That made airborne formations into mechanized infantry at the cost of reducing "bayonet" strength, as the BMD could carry only three or at most four paratroopers in addition to its three-man crew. They were used in that role in the Soviet invasion of Afghanistan in 1979.

At present, almost all infantry units from industrialized nations are provided with some type of motor transport. Infantry units equipped with IFVs rather than lighter vehicles are commonly designated as "heavy", indicating more combat power but also more costly long-range transportation requirements. In Operation Desert Shield, during the buildup phase of the First Gulf War, the U.S. Army was concerned about the lack of mobility, protection and firepower offered by existing rapid deployment (i.e., airborne) formations; and also about the slowness of deploying regular armored units. The experience led the U.S. Army to form combat brigades based on the Stryker wheeled IFV.

In the British Army, "heavy" units equipped with the Warrior IFV are described as "armoured infantry", and units with the Bulldog APC as "mechanised infantry". This convention is becoming widespread; for example the French Army has ""motorisées"" units equipped with the wheeled VAB and ""mécanisées"" (armoured) units with the tracked AMX-10P.

The transport and other logistic requirements have led many armies to adopt wheeled APCs when their existing stocks of tracked APCs require replacement. An example is the Canadian Army, which has used the LAV III wheeled IFV in fighting in Afghanistan. The Italian, Spanish and Swedish armies are adopting (and exporting) new indigenous-produced tracked IFVs. The Swedish CV90 IFV in particular has been adopted by several armies.
A recent trend seen in the Israel Defense Forces and the Armed Forces of the Russian Federation is the development and introduction of exceptionally well-armored APCs (HAPC), such as the IDF Achzarit, that are converted from obsolete main battle tanks (such as the Soviet T-55). Such vehicles are usually expedients, and lack of space prevents the armament of an IFV being carried in addition to an infantry section or squad. In the Russian Army, such vehicles were introduced for fighting in urban areas, where the risk from short range infantry anti-tank weapons, such as the RPG-7, is highest, after Russian tank and motor infantry units suffered heavy losses fighting Chechen troops in Grozny during the First Chechen War in 1995.

Many APCs and IFVs currently under development are intended for rapid deployment by aircraft. New technologies that promise reduction in weight, such as electric drive, may be incorporated. However, facing a similar threat in post-invasion Iraq to that which prompted the Russians to convert tanks to APCs, the occupying armies have found it necessary to apply extra armor to existing APCs and IFVs, which adds to the overall size and weight. Some of the latest designs (such as the German Puma) are intended to allow a light, basic model vehicle, which is air-transportable, to be fitted in the field with additional protection, thereby ensuring both strategic flexibility and survivability.

It is generally accepted that single weapons system types are much less effective without the support of the full combined arms team; the pre-World War II notion of "tank fleets" has proven to be as unsound as the World War I idea of unsupported infantry attacks. Though many nations' armored formations included an organic mechanized infantry component at the start of World War II, the proportion of mechanized infantry in such combined arms formations was increased by most armies as the war progressed.

The lesson was re-learned, first by the Pakistani Army in the 1965 War with India, where the nation fielded two different types of armored divisions: one which was almost exclusively armor (the 1st), while another was more balanced (the 6th). The latter division showed itself to be far more combat capable than the former.

Having achieved spectacular successes in the offensive with tank-heavy formations during the Six-Day War, the Israel Defense Forces found in the Yom Kippur War of 1973 that a doctrine that relied primarily on tanks and aircraft had proven inadequate. As a makeshift remedy, paratroopers were provided with motorized transport and used as mechanized infantry in coordination with the armor.




</doc>
<doc id="20284" url="https://en.wikipedia.org/wiki?curid=20284" title="Micah">
Micah

Micah (; ) is a given name.

Micah is the name of several people in the Hebrew Bible (Old Testament), and means "Who is like God?" The name is sometimes found with theophoric extensions. Suffix theophory in "Yah" and in "Yahweh" results in Michaiah or Michaihu (), meaning "who is like Yahweh?" Suffix theophory in "El" results in "Michael" (), meaning "who is like god".

In German and Dutch, Micah is spelled and the "ch" in the name is pronounced either or ; the first is more common in female names, the latter in male names. The name is not as common as Michael or Michiel.





</doc>
<doc id="20285" url="https://en.wikipedia.org/wiki?curid=20285" title="Malachi">
Malachi

Malachi, Malachias, Malache or Mal'achi (; ) was the traditional writer of the Book of Malachi, the last book of the Neviim (prophets) section in the Hebrew Bible. No allusion is made to him by Ezra, however, and he does not directly mention the restoration of the temple. The editors of the 1906 Jewish Encyclopedia implied that he prophesied after Haggai and Zechariah (; , ) and speculated that he delivered his prophecies about 420 BCE, after the second return of Nehemiah from Persia (Book of Nehemiah ), or possibly before his return, comparing with ( with ).

In the Septuagint, or Greek Old Testament, the Prophetic Books are placed last, making the Book of Malachi the last protocanonical book before the Deuterocanonical books or The New Testament. According to the 1897 Easton's Bible Dictionary, it is possible that Malachi is not a proper name, but simply means "messenger of YHWH". The Greek Old Testament superscription is ἐν χειρὶ ἀγγέλου αὐτοῦ, (by the hand of his messenger).

Because Malachi's name does not occur elsewhere in the Bible, some scholars doubt whether "Malachi" is intended to be the personal name of the prophet. None of the other prophetic books of the Hebrew Bible or the Greek Old Testament are anonymous. The form "mal'akhi", signifies "my messenger"; it occurs in Malachi 3:1 (compare to Malachi 2:7). But this form of itself would hardly be appropriate as a proper name without some additional syllable such as Yah, whence "mal'akhiah", i.e. "messenger of Elohim." Haggai, in fact, is expressly designated "messenger of Elohim" (Haggai 1:13). Besides, the superscriptions prefixed to the book, in both the Septuagint and the Vulgate, warrant the supposition that Malachi's full name ended with the syllable -yah. At the same time the Greek Old Testament translates the last clause of Malachi 1:1, "by the hand of his messenger," and the Targum reads, "by the hand of my angel, whose name is called Ezra the scribe." 

The Jews of his day ascribed the Book of Malachi, the last book of prophecy, to Ezra, but if Ezra's name was originally associated with the book, it would hardly have been dropped by the collectors of the prophetic canon who lived only a century or two after Ezra's time. Certain traditions ascribe the book to Zerubbabel and Nehemiah; others, still, to Malachi, whom they designate as a Levite and a member of the "Great Synagogue." Certain modern scholars, however, on the basis of the similarity of the title (compare Malachi 1:1 to Zechariah 9:1 and Zechariah 12:1), declare it to be anonymous. Professor G.G. Cameron, suggests that the termination of the word "Malachi" is adjectival, and equivalent to the Latin angelicus, signifying "one charged with a message or mission" (a missionary). The term would thus be an official title, and the thought would not be unsuitable to one whose message closed the prophetical canon of the Old Testament.

Opinions vary as to the prophet's exact date, but nearly all scholars agree that Malachi prophesied during the Persian period, and after the reconstruction and dedication of the second temple in 516 BCE (compare Malachi 1:10 ; Malachi 3:1, Malachi 3:10). The prophet speaks of the "people's governor" (Hebrew "pechah", Malachi 1:8), as do Haggai and Nehemiah (Haggai 1:1 ; Nehemiah 5:14 ; Nehemiah 12:26). The social conditions portrayed appear to be those of the period of the Restoration. More specifically, Malachi probably lived and labored during the times of Ezra and Nehemiah. The abuses which Malachi mentions in his writings correspond so exactly with those which Nehemiah found on his second visit to Jerusalem in 432 BCE (Nehemiah 13:7) that it seems reasonably certain that he prophesied concurrently with Nehemiah or shortly after.

According to Rabbi W. Gunther Plaut, "Malachi describes a priesthood that is forgetful of its duties, a Temple that is underfunded because the people have lost interest in it, and a society in which Jewish men divorce their Jewish wives to marry out of the faith." 





</doc>
<doc id="20286" url="https://en.wikipedia.org/wiki?curid=20286" title="Martin Fowler (software engineer)">
Martin Fowler (software engineer)

Martin Fowler (born 1963) is a British software developer, author and international public speaker on software development, specialising in object-oriented analysis and design, UML, patterns, and agile software development methodologies, including extreme programming.

His 1999 book "Refactoring" popularised the practice of code refactoring. In 2004 he introduced Presentation Model (PM), an architectural pattern.

Fowler was born and grew up in Walsall, England, where he went to Queen Mary's Grammar School for his secondary education. He graduated at University College London in 1986. In 1994 he moved to the United States, where he lives near Boston, Massachusetts in the suburb of Melrose.

Fowler started working with software in the early 1980s. Out of university in 1986 he started working in software development for Coopers & Lybrand until 1991. In 2000 he joined ThoughtWorks, a systems integration and consulting company, where he serves as Chief Scientist.

Fowler has written nine books on the topic of software development (see "Publications"). He is a member of the "Agile Alliance" and helped create the Manifesto for Agile Software Development in 2001, along with 16 fellow signatories. He maintains a "bliki", a mix of blog and wiki. He popularised the term Dependency Injection as a form of Inversion of Control.




</doc>
<doc id="20287" url="https://en.wikipedia.org/wiki?curid=20287" title="Microsoft Word">
Microsoft Word

Microsoft Word (or simply Word) is a word processor developed by Microsoft. It was first released on October 25, 1983 under the name "Multi-Tool Word" for Xenix systems. Subsequent versions were later written for several other platforms including IBM PCs running DOS (1983), Apple Macintosh running the Classic Mac OS (1985), AT&T Unix PC (1985), Atari ST (1988), OS/2 (1989), Microsoft Windows (1989), SCO Unix (1994), and macOS (formerly OS X; 2001).

Commercial versions of Word are licensed as a standalone product or as a component of Microsoft Office, Windows RT or the discontinued Microsoft Works suite.

In 1981, Microsoft hired Charles Simonyi, the primary developer of Bravo, the first GUI word processor, which was developed at Xerox PARC. Simonyi started work on a word processor called "Multi-Tool Word" and soon hired Richard Brodie, a former Xerox intern, who became the primary software engineer.

Microsoft announced Multi-Tool Word for Xenix and MS-DOS in 1983. Its name was soon simplified to "Microsoft Word". Free demonstration copies of the application were bundled with the November 1983 issue of "PC World", making it the first to be distributed on-disk with a magazine. That year Microsoft demonstrated Word running on Windows.

Unlike most MS-DOS programs at the time, Microsoft Word was designed to be used with a mouse. Advertisements depicted the Microsoft Mouse, and described Word as a WYSIWYG, windowed word processor with the ability to undo and display bold, italic, and underlined text, although it could not render fonts. It was not initially popular, since its user interface was different from the leading word processor at the time, WordStar. However, Microsoft steadily improved the product, releasing versions 2.0 through 5.0 over the next six years. In 1985, Microsoft ported Word to the classic Mac OS (known as Macintosh System Software at the time). This was made easier by Word for DOS having been designed for use with high-resolution displays and laser printers, even though none were yet available to the general public. Following the precedents of LisaWrite and MacWrite, Word for Mac OS added true WYSIWYG features. It fulfilled a need for a word processor that was more capable than MacWrite. After its release, Word for Mac OS's sales were higher than its MS-DOS counterpart for at least four years.

The second release of Word for Mac OS, shipped in 1987, was named Word 3.0 to synchronize its version number with Word for DOS; this was Microsoft's first attempt to synchronize version numbers across platforms. Word 3.0 included numerous internal enhancements and new features, including the first implementation of the Rich Text Format (RTF) specification, but was plagued with bugs. Within a few months, Word 3.0 was superseded by a more stable Word 3.01, which was mailed free to all registered users of 3.0. After MacWrite Pro was discontinued in the mid-1990s, Word for Mac OS never had any serious rivals. Word 5.1 for Mac OS, released in 1992, was a very popular word processor owing to its elegance, relative ease of use and feature set. Many users say it is the best version of Word for Mac OS ever created.

In 1986, an agreement between Atari and Microsoft brought Word to the Atari ST under the name "Microsoft Write". The Atari ST version was a port of Word 1.05 for the Mac OS and was never updated.

The first version of Word for Windows was released in 1989. With the release of Windows 3.0 the following year, sales began to pick up and Microsoft soon became the market leader for word processors for IBM PC-compatible computers. In 1991, Microsoft capitalized on Word for Windows' increasing popularity by releasing a version of Word for DOS, version 5.5, that replaced its unique user interface with an interface similar to a Windows application. When Microsoft became aware of the Year 2000 problem, it made Microsoft Word 5.5 for DOS available for download free. , it is still available for download from Microsoft's web site.
In 1991, Microsoft embarked on a project code-named Pyramid to completely rewrite Microsoft Word from the ground up. Both the Windows and Mac OS versions would start from the same code base. It was abandoned when it was determined that it would take the development team too long to rewrite and then catch up with all the new capabilities that could have been added in the same time without a rewrite. Instead, the next versions of Word for Windows and Mac OS, dubbed version 6.0, both started from the code base of Word for Windows 2.0.

With the release of Word 6.0 in 1993, Microsoft again attempted to synchronize the version numbers and coordinate product naming across platforms, this time across DOS, Mac OS, and Windows (this was the last version of Word for DOS). It introduced AutoCorrect, which automatically fixed certain typing errors, and AutoFormat, which could reformat many parts of a document at once. While the Windows version received favorable reviews (e.g., from "InfoWorld"), the Mac OS version was widely derided. Many accused it of being slow, clumsy and memory intensive, and its user interface differed significantly from Word 5.1. In response to user requests, Microsoft offered Word 5 again, after it had been discontinued. Subsequent versions of Word for macOS are no longer direct ports of Word for Windows, instead featuring a mixture of ported code and native code.

Word for Windows is available stand-alone or as part of the Microsoft Office suite. Word contains rudimentary desktop publishing capabilities and is the most widely used word processing program on the market. Word files are commonly used as the format for sending text documents via e-mail because almost every user with a computer can read a Word document by using the Word application, a Word viewer or a word processor that imports the Word format (see Microsoft Word Viewer).

Word 6 for Windows NT was the first 32-bit version of the product, released with Microsoft Office for Windows NT around the same time as Windows 95. It was a straightforward port of Word 6.0. Starting with Word 95, releases of Word were named after the year of its release, instead of its version number.

Word 2010 allows more customization of the Ribbon, adds a Backstage view for file management, has improved document navigation, allows creation and embedding of screenshots, and integrates with Word Web App.

The Mac was introduced January 24, 1984 and Microsoft introduced Word 1.0 for Mac a year later, January 18, 1985. The DOS, Mac, and Windows versions are quite different from each other. Only the Mac version was WYSIWYG and used a Graphical User Interface, far ahead of the other platforms. Each platform restarted their version numbering at "1.0" (https://winworldpc.com/product/microsoft-word/1x-mac). There was no version 2 on the Mac, but version 3 came out January 31, 1987 as described above. Word 4.0 came out November 6, 1990, and added automatic linking with Excel, the ability to flow text around graphics and a WYSIWYG page view editing mode.

Word 5.1 for Mac, released in 1992 ran on the original 68000 CPU, and was the last to be specifically designed as a Macintosh application. The later Word 6 was a Windows port and poorly received. Word 5.1 continued to run well till the very last Classic MacOS. Many people continue to run Word 5.1 to this day under an emulated Mac classic system for some of its excellent features like document generation and renumbering or to access their old files.
In 1997, Microsoft formed the Macintosh Business Unit as an independent group within Microsoft focused on writing software for Mac OS. Its first version of Word, Word 98, was released with Office 98 Macintosh Edition. Document compatibility reached parity with Word 97, and it included features from Word 97 for Windows, including spell and grammar checking with squiggles. Users could choose the menus and keyboard shortcuts to be similar to either Word 97 for Windows or Word 5 for Mac OS.

Word 2001, released in 2000, added a few new features, including the Office Clipboard, which allowed users to copy and paste multiple items. It was the last version to run on classic Mac OS and, on Mac OS X, it could only run within the Classic Environment. Word X, released in 2001, was the first version to run natively on, and required, Mac OS X, and introduced non-contiguous text selection.

Word 2004 was released in May 2004. It included a new Notebook Layout view for taking notes either by typing or by voice. Other features, such as tracking changes, were made more similar with Office for Windows.

Word 2008, released on January 15, 2008, included a Ribbon-like feature, called the Elements Gallery, that can be used to select page layouts and insert custom diagrams and images. It also included a new view focused on publishing layout, integrated bibliography management, and native support for the new Office Open XML format. It was the first version to run natively on Intel-based Macs.

Word 2011, released in October 2010, replaced the Elements Gallery in favor of a Ribbon user interface that is much more similar to Office for Windows, and includes a full-screen mode that allows users to focus on reading and writing documents, and support for Office Web Apps.

Microsoft Word's native file formats are denoted either by a codice_1 or codice_2 filename extension.

Although the codice_1 extension has been used in many different versions of Word, it actually encompasses four distinct file formats:

The newer codice_2 extension signifies the Office Open XML international standard for Office documents and is used by Word 2007 and later for Windows, Word 2008 and later for macOS, as well as by a growing number of applications from other vendors, including OpenOffice.org Writer, an open source word processing program.

During the late 1990s and early 2000s, the default Word document format (.DOC) became a "de facto" standard of document file formats for Microsoft Office users. There are different versions of "Word Document Format" used by default in Word 97–2007. Each binary word file is a Compound File, a hierarchical file system within a file. According to Joel Spolsky, Word Binary File Format is extremely complex mainly because its developers had to accommodate an overwhelming number of features and prioritize performance over anything else.

As with all OLE Compound Files, Word Binary Format consists of "storages", which are analogous to computer folders, and "streams", which are similar to computer files. Each storage may contain streams or other storages. Each Word Binary File must contain a stream called "WordDocument" stream and this stream must start with a File Information Block (FIB). FIB serves as the first point of reference for locating everything else, such as where the text in a Word document starts, ends, what version of Word created the document and other attributes.

Word 2007 and later continue to support the DOC file format, although it is no longer the default.

The .docx "XML " format introduced in Word 2003 was a simple, XML-based format called WordprocessingML.

Opening a Word Document file in a version of Word other than the one with which it was created can cause incorrect display of the document. The document formats of the various versions change in subtle and not so subtle ways (such as changing the font, or the handling of more complex tasks like footnotes). Formatting created in newer versions does not always survive when viewed in older versions of the program, nearly always because that capability does not exist in the previous version. Rich Text Format (RTF), an early effort to create a format for interchanging formatted text between applications, is an optional format for Word that retains most formatting and all content of the original document.

Plugins permitting the Windows versions of Word to read and write formats it does not natively support, such as international standard OpenDocument format (ODF) (ISO/IEC 26300:2006), are available. Up until the release of Service Pack 2 (SP2) for Office 2007, Word did not natively support reading or writing ODF documents without a plugin, namely the SUN ODF Plugin or the OpenXML/ODF Translator. With SP2 installed, ODF format 1.1 documents can be read and saved like any other supported format in addition to those already available in Word 2007. The implementation faces substantial criticism, and the ODF Alliance and others have claimed that the third-party plugins provide better support. Microsoft later declared that the ODF support has some limitations.

In October 2005, one year before the Microsoft Office 2007 suite was released, Microsoft declared that there was insufficient demand from Microsoft customers for the international standard OpenDocument format support, and that therefore it would not be included in Microsoft Office 2007. This statement was repeated in the following months. As an answer, on October 20, 2005 an online petition was created to demand ODF support from Microsoft.

In May 2006, the ODF plugin for Microsoft Office was released by the OpenDocument Foundation. Microsoft declared that it had no relationship with the developers of the plugin.

In July 2006, Microsoft announced the creation of the Open XML Translator project – tools to build a technical bridge between the Microsoft Office Open XML Formats and the OpenDocument Format (ODF). This work was started in response to government requests for interoperability with ODF. The goal of project was not to add ODF support to Microsoft Office, but only to create a plugin and an external toolset. In February 2007, this project released a first version of the ODF plugin for Microsoft Word.

In February 2007, Sun released an initial version of its ODF plugin for Microsoft Office. Version 1.0 was released in July 2007.

Microsoft Word 2007 (Service Pack 1) supports (for output only) PDF and XPS formats, but only after manual installation of the Microsoft 'Save as PDF or XPS' add-on. On later releases, this was offered by default.

Among its features, Word includes a built-in spell checker, a thesaurus, a dictionary, and utilities for manipulating and editing text. The following are some aspects of its feature set.

Several later versions of Word include the ability for users to create their own formatting templates, allowing them to define a file in which the title, heading, paragraph, and other element designs differ from the standard Word templates. Users can find how to do this under the Help section located near the top right corner (Word 2013 on Windows 8).

For example, Normal.dot is the master template from which all Word documents are created. It determines the margin defaults as well as the layout of the text and font defaults. Although normal.dot is already set with certain defaults, the user can change normal.dot to new defaults. This will change other documents which were created using the template, usually in unexpected ways.

Word can import and display images in common bitmap formats such as JPG and GIF. It can also be used to create and display simple line-art. Microsoft Word added support for the common SVG vector image format in 2017 for Office 365 ProPlus subscribers and this functionality was also included in the Office 2019 release.

WordArt enables drawing text in a Microsoft Word document such as a title, watermark, or other text, with graphical effects such as skewing, shadowing, rotating, stretching in a variety of shapes and colors and even including three-dimensional effects. Users can apply formatting effects such as shadow, bevel, glow, and reflection to their document text as easily as applying bold or underline. Users can also spell-check text that uses visual effects, and add text effects to paragraph styles.

A Macro is a rule of pattern that specifies how a certain input sequence (often a sequence of characters) should be mapped to an output sequence according to defined process. Frequently used or repetitive sequences of keystrokes and mouse movements can be automated.
Like other Microsoft Office documents, Word files can include advanced macros and even embedded programs. The language was originally WordBasic, but changed to Visual Basic for Applications as of Word 97.

This extensive functionality can also be used to run and propagate viruses in documents. The tendency for people to exchange Word documents via email, USB flash drives, and floppy disks made this an especially attractive vector in 1999. A prominent example was the Melissa virus, but countless others have existed.

These macro viruses were the only known cross-platform threats between Windows and Macintosh computers and they were the only infection vectors to affect any macOS system up until the advent of video codec trojans in 2007. Microsoft released patches for Word X and Word 2004 that effectively eliminated the macro problem on the Mac by 2006.

Word's macro security setting, which regulates when macros may execute, can be adjusted by the user, but in the most recent versions of Word, is set to HIGH by default, generally reducing the risk from macro-based viruses, which have become uncommon.

Before Word 2010 (Word 14) for Windows, the program was unable to correctly handle ligatures defined in OpenType fonts. Those ligature glyphs with Unicode codepoints may be inserted manually, but are not recognized by Word for what they are, breaking spell checking, while custom ligatures present in the font are not accessible at all. Since Word 2010, the program now has advanced typesetting features which can be enabled: OpenType ligatures, kerning, and hyphenation. Other layout deficiencies of Word include the inability to set crop marks or thin spaces. Various third-party workaround utilities have been developed.

In Word 2004 for Mac OS X, support of complex scripts was inferior even to Word 97, and Word 2004 did not support Apple Advanced Typography features like ligatures or glyph variants.

Microsoft Word supports bullet lists and numbered lists. It also features a numbering system that helps add correct numbers to pages, chapters, headers, footnotes, and entries of tables of content; these numbers automatically change to correct ones as new items are added or existing items are deleted. Bullets and numbering can be applied directly to paragraphs and convert them to lists. Word 97 through 2003, however, had problems adding correct numbers to numbered lists. In particular, a second irrelevant numbered list might have not started with number one, but instead resumed numbering after the last numbered list. Although Word 97 supported a hidden marker that said the list numbering must restart afterwards, the command to insert this marker (Restart Numbering command) was only added in Word 2003. However, if one were to cut the first item of the listed and paste it as another item (e.g. fifth), then the restart marker would have moved with it and the list would have restarted in the middle instead of at the top.

Users can also create tables in "Word". Depending on the version, "Word" can perform simple calculations — along with support for formulas and equations as well.

Available in certain versions of Word (e.g., Word 2007), AutoSummarize highlights passages or phrases that it considers valuable and can be a quick way of generating a crude abstract or an executive summary. The amount of text to be retained can be specified by the user as a percentage of the current amount of text.

According to Ron Fein of the Word 97 team, AutoSummarize cuts wordy copy to the bone by counting words and ranking sentences. First, AutoSummarize identifies the most common words in the document (barring "a" and "the" and the like) and assigns a "score" to each word – the more frequently a word is used, the higher the score. Then, it "averages" each sentence by adding the scores of its words and dividing the sum by the number of words in the sentence – the higher the average, the higher the rank of the sentence. "It's like the ratio of wheat to chaff," explains Fein.

AutoSummarize was removed from Microsoft Word for Mac OS X 2011, although it was present in Word for Mac 2008. AutoSummarize was removed from the Office 2010 release version (14) as well.

There are three password types that can be set in Microsoft Word:


The second and the third type of passwords were developed by Microsoft for convenient shared use of documents rather than for their protection. There is no encryption of documents that are protected by such passwords, and Microsoft Office protection system saves a hash sum of a password in a document's header where it can be easily accessed and removed by the specialized software.
"Password to open a document" offers much tougher protection that had been steadily enhanced in the subsequent editions of Microsoft Office.

"Word 95" and all the preceding editions had the weakest protection that utilized a conversion of a password to a 16-bit key.

Key length in "Word 97" and "2000" was strengthened up to 40 bit. However, modern cracking software allows removing such a password very quickly – a persistent cracking process takes one week at most. Use of rainbow tables reduces password removal time to several seconds. Some password recovery software can not only remove a password, but also find an actual password that was used by a user to encrypt the document using brute-force attack approach. Statistically, the possibility of recovering the password depends on the password strength.

Word's 2003/XP version default protection remained the same but an option that allowed advanced users choosing a Cryptographic Service Provider was added. If a strong CSP is chosen, guaranteed document decryption becomes unavailable, and therefore a password can't be removed from the document. Nonetheless, a password can be fairly quickly picked with brute-force attack, because its speed is still high regardless of the CSP selected. Moreover, since the CSPs are not active by the default, their use is limited to advanced users only.

Word 2007 offers a significantly more secure document protection which utilizes the modern Advanced Encryption Standard (AES) that converts a password to a 128-bit key using a SHA-1 hash function 50000 times. It makes password removal impossible (as of today, no computer that can pick the key in reasonable amount of time exists), and drastically slows the brute-force attack speed down to several hundreds of passwords per second.

Word's 2010 protection algorithm was not changed apart from increasing number of SHA-1 conversions up to 100000 times, and consequently, the brute-force attack speed decreased two times more.

"BYTE" in 1984 criticized the documentation for Word 1.1 and 2.0 for DOS, calling it "a complete farce". It called the software "clever, put together well, and performs some extraordinary feats", but concluded that "especially when operated with the mouse, has many more limitations than benefits ... extremely frustrating to learn and operate efficiently". "PC Magazine" review was very mixed, stating "I've run into weird word processors before, but this is the first time one's nearly knocked me down for the count" but acknowledging that Word's innovations were the first that caused the reviewer to consider abandoning WordStar. While the review cited an excellent WYSIWYG display, sophisticated print formatting, windows, and footnoting as merits, it criticized many small flaws, very slow performance, and "documentation apparently produced by Madame Sadie's Pain Palace". It concluded that Word was "two releases away from potential greatness".

"Compute!'s Apple Applications" in 1987 stated that "despite a certain awkwardness", Word 3.01 "will likely become the major Macintosh word processor" with "far too many features to list here". While criticizing the lack of true WYSIWYG, the magazine concluded that ""Word" is marvelous. It's like a Mozart or Edison, whose occasional gaucherie we excuse because of his great gifts".

"Compute!" in 1989 stated that Word 5.0's integration of text and graphics made it "a solid engine for basic desktop publishing". The magazine approved of improvements to text mode, described the $75 price for upgrading from an earlier version as "the deal of the decade", and concluded that "as a high-octane word processor, "Word" is definitely worth a look".

During the first quarter of 1996, "Microsoft Word" accounted for 80% of the worldwide word processing market.

Despite its commercial success, it has also been argued in the scientific community that "Word" might not be well-suited for large-scale projects with high typographical demands, due to issues such as file compatibility, poor typography, low image quality and limited feature scalability.




</doc>
<doc id="20288" url="https://en.wikipedia.org/wiki?curid=20288" title="Microsoft Office">
Microsoft Office

Microsoft Office, or simply Office, is a family of client software, server software, and services developed by Microsoft. It was first announced by Bill Gates on August 1, 1988, at COMDEX in Las Vegas. Initially a marketing term for an office suite (bundled set of productivity applications), the first version of Office contained Microsoft Word, Microsoft Excel, and Microsoft PowerPoint. Over the years, Office applications have grown substantially closer with shared features such as a common spell checker, OLE data integration and Visual Basic for Applications scripting language. Microsoft also positions Office as a development platform for line-of-business software under the Office Business Applications brand. On July 10, 2012, Softpedia reported that Office was being used by over a billion people worldwide.

Office is produced in several versions targeted towards different end-users and computing environments. The original, and most widely used version, is the desktop version, available for PCs running the Windows and macOS operating systems. Office in a browser, previously known as Office Online, is a version of the software that runs within a web browser, while Microsoft also maintains Office apps for Android and iOS.

Since Office 2013, Microsoft has promoted Office 365 as the primary means of obtaining Microsoft Office: it allows use of the software and other services on a subscription business model, and users receive free feature updates to the software for the lifetime of the subscription, including new features and cloud computing integration that are not necessarily included in the "on-premises" releases of Office sold under conventional license terms. In 2017, revenue from Office 365 overtook conventional license sales.

The current on-premises, desktop version of Office is Office 2019, released on September 24, 2018.

Unless stated otherwise, desktop applications are available for Windows and macOS.




Office Mobile includes the scaled-down and touch-optimised versions of Word, Excel and PowerPoint. Other Office applications such as OneNote, Lync and Outlook are available as standalone apps. It is supported on Android, iOS, Windows 10 and Windows 10 Mobile.

Office Mobile enables users to save and access documents on OneDrive, OneDrive for Business, and SharePoint. Additionally, the Windows Phone version also allows users to save files locally on the device. According to Microsoft, Office Mobile for iPhone and Android are "very similar" to each other, whereas the Windows Phone version provides a "richer, more integrated experience".

Office Mobile for iPhone was released on June 14, 2013 in the United States. Support for 135 markets and 27 languages was rolled out over a few days. It requires iOS 8 or later. Although the app also works on iPad devices, excluding the first generation, it is designed for a small screen. Office Mobile was released for Android phones on July 31, 2013 in the United States. Support for 117 markets and 33 languages was added gradually over several weeks. It is supported on Android 4.0 and later. Office Mobile for both iPhone and Android, available for free from the App Store and Google Play Store respectively, initially required a qualifying Office 365 subscription to activate, but in March 2014, with the release of Office for iPad, the apps were updated making them fully free for home use, though a subscription is still required for business use.

On March 27, 2014, Microsoft released Word, Excel and PowerPoint for iPad. On November 6, 2014, Microsoft released updated versions of Word, Excel and PowerPoint for iPhone.

On January 29, 2015, Microsoft released Word, Excel and PowerPoint for Android tablets. On June 24, 2015, Microsoft released updated versions of Word, Excel and Powerpoint for Android phones. The Android version is also supported on certain Chrome OS machines.

In January 2015, Microsoft unveiled updated universal app versions of the Office applications for Windows 10 devices—including PCs, tablets and smartphones—that are based upon the previously released Android and iOS apps.

Office Mobile is or was also available, though no longer supported, on Windows Mobile, Windows Phone and Symbian. There is also Office RT, a touch-optimized version of the standard desktop Office suite, pre-installed on Windows RT.

Most versions of Microsoft Office (including Office 97 and later) use their own widget set and do not exactly match the native operating system. This is most apparent in Microsoft Office XP and 2003, where the standard menus were replaced with a colored, flat-looking, shadowed menu style. The user interface of a particular version of Microsoft Office often heavily influences a subsequent version of Microsoft Windows. For example, the toolbar, colored buttons and the gray-colored 3D look of Office 4.3 were added to Windows 95, and the ribbon, introduced in Office 2007, has been incorporated into several programs bundled with Windows 7 and later. In 2012, Office 2013 replicated the flat, box-like design of Windows 8.

Users of Microsoft Office may access external data via connection-specifications saved in Office Data Connection (.odc) files.

Both Windows and Office use service packs to update software. Office had non-cumulative service releases, which were discontinued after Office 2000 Service Release 1.

Past versions of Office often contained Easter eggs. For example, Excel 97 contained a reasonably functional flight-simulator.

Microsoft Office prior to Office 2007 used proprietary file formats based on the OLE Compound File Binary Format. This forced users who share data to adopt the same software platform. In 2008, Microsoft made the entire documentation for the binary Office formats freely available for download and granted any possible patents rights for use or implementations of those binary format for free under the Open Specification Promise. Previously, Microsoft had supplied such documentation freely but only on request.

Starting with Office 2007, the default file format has been a version of Office Open XML, though different than the one standardized and published by Ecma International and by ISO/IEC. Microsoft has granted patent rights to the formats technology under the Open Specification Promise and has made available free downloadable converters for previous versions of Microsoft Office including Office 2003, Office XP, Office 2000 and Office 2004 for Mac OS X. Third-party implementations of Office Open XML exist on the Windows platform (LibreOffice, all platforms), macOS platform (iWork '08, NeoOffice, LibreOffice) and Linux (LibreOffice and OpenOffice.org 3.0). In addition, Office 2010, Service Pack 2 for Office 2007, and Office 2016 for Mac supports the OpenDocument Format (ODF) for opening and saving documents - only the old ODF 1.0 (2006 ISO/IEC standard) is supported, not the 1.2 version (2015 ISO/IEC standard).

Microsoft provides the ability to remove metadata from Office documents. This was in response to highly publicized incidents where sensitive data about a document was leaked via its metadata. Metadata removal was first available in 2004, when Microsoft released a tool called "Remove Hidden Data Add-in for Office 2003/XP" for this purpose. It was directly integrated into Office 2007 in a feature called the "Document Inspector".

A major feature of the Office suite is the ability for users and third party companies to write add-ins (plug-ins) that extend the capabilities of an application by adding custom commands and specialized features. One of the new features is the Office Store. Plugins and other tools can be downloaded by users. Developers can make money by selling their applications in the Office Store. The revenue is divided between the developer and Microsoft where the developer gets 80% of the money. Developers are able to share applications with all Office users.

The app travels with the document, and it is for the developer to decide what the recipient will see when they open it. The recipient will either have the option to download the app from the Office Store for free, start a free trial or be directed to payment. With Office's cloud abilities, IT department can create a set of apps for their business employees in order to increase their productivity. When employees go to the Office Store, they'll see their company's apps under "My Organization". The apps that employees have personally downloaded will appear under "My Apps". Developers can use web technologies like HTML5, XML, CSS3, JavaScript, and APIs for building the apps. An application for Office is a webpage that is hosted inside an Office client application. User can use apps to amplify the functionality of a document, email message, meeting request, or appointment. Apps can run in multiple environments and by multiple clients, including rich Office desktop clients, Office Web Apps, mobile browsers, and also on-premises and in the cloud. The type of add-ins supported differ by Office versions:


Microsoft Office has a security feature that allows users to encrypt Office (Word, Excel, PowerPoint, Access, Skype Business) documents with a user-provided password. The password can contain up to 255 characters and uses AES 128-bit advanced encryption by default. Passwords can also be used to restrict modification of the entire document, worksheet or presentation. Due to lack of document encryption, though, these passwords can be removed using a third-party cracking software.

All versions of Microsoft Office products before Microsoft Office 2019 are eligible for ten years of support following their release, during which Microsoft releases security updates for the product version and provides paid technical support. The ten-year period is divided into two five-year phases: The mainstream phase and the extended phase. During the mainstream phase, Microsoft may provide limited complimentary technical support and release non-security updates or change the design of the product. During the extended phase, said services stop. Office 2019 only receives 5 years of mainstream and 2 years of extended support.

Microsoft supports Office for the Windows and macOS platforms, as well as mobile versions for Windows Phone, Android and iOS platforms. Beginning with Mac Office 4.2, the macOS and Windows versions of Office share the same file format, and are interoperable. Visual Basic for Applications support was dropped in Microsoft Office 2008 for Mac, then reintroduced in Office for Mac 2011.

Microsoft tried in the mid-1990s to port Office to RISC processors such as NEC/MIPS and IBM/PowerPC, but they met problems such as memory access being hampered by data structure alignment requirements. Microsoft Word 97 and Excel 97 however did ship for the DEC Alpha platform. Difficulties in porting Office may have been a factor in discontinuing Windows NT on non-Intel platforms.

The Microsoft Office applications and suites are sold via retail channels, and volume licensing for larger organizations (also including the "Home Use Program". allowing users at participating organizations to buy low-cost licenses for use on their personal devices as part of their employer's volume license agreement).

In 2010, Microsoft introduced a software as a service platform known as Office 365, to provide cloud-hosted versions of Office's server software, including Exchange e-mail and SharePoint, on a subscription basis (competing in particular with Google Apps). Following the release of Office 2013, Microsoft began to offer Office 365 plans for the consumer market, with access to Microsoft Office software on multiple devices with free feature updates over the life of the subscription, as well as other services such as OneDrive storage.

Microsoft has since promoted Office 365 as the primary means of purchasing Microsoft Office. Although there are still "on-premises" releases roughly every three years, Microsoft marketing emphasizes that they do not receive new features or access to new cloud-based services as they are released unlike Office 365, as well as other benefits for consumer and business markets. Office 365 revenue overtook traditional license sales for Office in 2017.

Microsoft Office is available in several editions, which regroup a given number of applications for a specific price. Current retail editions are grouped by category:

Post-secondary students may obtain the University edition of Microsoft Office 365 subscription. It is limited to one user and two devices, plus the subscription price is valid for four years instead of just one. Apart from this, the University edition is identical in features to the Home Premium version. This marks the first time Microsoft does not offer physical or permanent software at academic pricing, in contrast to the University versions of Office 2010 and Office 2011. In addition, students eligible for DreamSpark program may receive select standalone Microsoft Office apps free of charge.




Microsoft Office has been criticized in the past for using proprietary file formats rather than open standards, which forces users who share data into adopting the same software platform. However, on February 15, 2008, Microsoft made the entire documentation for the binary Office formats freely available under the Open Specification Promise. Also, Office Open XML, the document format for the latest versions of Office for Windows and Mac, has been standardized under both Ecma International and ISO. Ecma International has published the Office Open XML specification free of copyrights and Microsoft has granted patent rights to the formats technology under the Open Specification Promise and has made available free downloadable converters for previous versions of Microsoft Office including Office 2003, Office XP, Office 2000 and Office 2004 for the Mac. Third-party implementations of Office Open XML exist on the Mac platform (iWork 08) and Linux (OpenOffice.org 2.3 – Novell Edition only).

Another point of criticism Microsoft Office has faced was the lack of support in its Mac versions for Unicode and Bi-directional text languages, notably Arabic and Hebrew. This issue, which had existed since the first release in 1989, was addressed in the 2016 version.

On 13 November 2018, a report initiated by the Government of the Netherlands concluded that Microsoft Office 2016 and Office 365 do not comply with GDPR, the European law which regulates data protection and privacy for all citizens in and outside the EU and EFTA region. The investigation was initiated by the observation that Microsoft does not reveal or share publicly any data collected about users of its software. In addition, the company does not provide users of its (Office) software an option to turn off diagnostic and telemetry data sent back to the company. Researchers found that most of the data that the Microsoft software collects and "sends home" is diagnostics. Researchers also observed that Microsoft "seemingly tried to make the system GDPR compliant by storing Office documents on servers based in the EU". However, they discovered the software packages collected additional data that contained private user information, some of which was stored on servers located in the US. The Netherlands Ministry of Justice hired Privacy Company to probe and evaluate the use of Microsoft Office products in the public sector. "Microsoft systematically collects data on a large scale about the individual use of Word, Excel, PowerPoint and Outlook. Covertly, without informing people," researchers of the Privacy Company stated in their blogpost. "Microsoft does not offer any choice with regard to the amount of data, or possibility to switch off the collection, or ability to see what data are collected, because the data stream is encoded."

The researchers commented that there is no need for Microsoft to store information such as IPs and email addresses, which are collected automatically by the software. "Microsoft should not store these transient, functional data, unless the retention is strictly necessary, for example, for security purposes," the researchers conclude in the final report by the Netherlands Ministry of Justice.

As a result of this in depth study and its conclusions, the Netherlands regulatory body concluded that Microsoft has violated GDPR "on many counts" including "lack of transparency and purpose limitation, and the lack of a legal ground for the processing." Microsoft has provided the Dutch authorities with an "improvement plan" that should satisfy Dutch regulators that it "would end all violations." The Dutch regulatory body is monitoring the situation and states that "If progress is deemed insufficient or if the improvements offered are unsatisfactory, SLM Microsoft Rijk will reconsider its position and may ask the Data Protection Authority to carry out a prior consultation and to impose enforcement measures." When asked for a response by an IT professional publication, a Microsoft spokesperson stated: "We are committed to our customers’ privacy, putting them in control of their data and ensuring that Office ProPlus and other Microsoft products and services comply with GDPR and other applicable laws. We appreciate the opportunity to discuss our diagnostic data handling practices in Office ProPlus with the Dutch Ministry of Justice and look forward to a successful resolution of any concerns."
The user privacy data issue affects ProPlus subscriptions of Microsoft Office 2016 and Microsoft Office 365, including the online version of Microsoft Office 365.

Microsoft Office for Windows started in October 1990 as a bundle of three applications designed for Microsoft Windows 3.0: Microsoft Word for Windows 1.1, Microsoft Excel for Windows 2.0, and Microsoft PowerPoint for Windows 2.0.

Microsoft Office for Windows 1.5 updated the suite with Microsoft Excel 3.0.

Version 1.6 added Microsoft Mail for PC Networks 2.1 to the bundle.

Microsoft Office 3.0, also called Microsoft Office 92, was released on August 30, 1992 and contained Word 2.0, Excel 4.0, PowerPoint 3.0 and Mail 3.0. It was the first version of Office also released on CD-ROM. In 1993, The Microsoft Office Professional was released, which added Microsoft Access 1.1.

Microsoft Office 4.0 was released containing Word 6.0, Excel 4.0a, PowerPoint 3.0 and Mail in 1993. Word's version number jumped from 2.0 to 6.0 so that it would have the same version number as the MS-DOS and Macintosh versions (Excel and PowerPoint were already numbered the same as the Macintosh versions).

Microsoft Office 4.2 for Windows NT was released in 1994 for i386, Alpha, MIPS and PowerPC architectures, containing Word 6.0 and Excel 5.0 (both 32-bit, PowerPoint 4.0 (16-bit), and Microsoft Office Manager 4.2 (the precursor to the Office Shortcut Bar)).

Microsoft Office 95 was released on August 24, 1995. Software version numbers were altered again to create parity across the suite—every program was called version 7.0 meaning all but Word missed out versions. It was designed as a fully 32-bit version to match Windows 95. Office 95 was available in two versions, Office 95 Standard and Office 95 Professional. The standard version consisted of Word 7.0, Excel 7.0, PowerPoint 7.0, and Schedule+ 7.0. The professional edition contained all of the items in the standard version plus Microsoft Access 7.0. If the professional version was purchased in CD-ROM form, it also included Bookshelf.

The logo used in Office 95 returns in Office 97, 2000 and XP. Microsoft Office 98 Macintosh Edition also uses a similar logo.

Microsoft Office 97 (Office 8.0) included hundreds of new features and improvements, such as introducing command bars, a paradigm in which menus and toolbars were made more similar in capability and visual design. Office 97 also featured Natural Language Systems and grammar checking. Office 97 was the first version of Office to include the Office Assistant. In Brazil, it was also the first version to introduce the Registration Wizard, a precursor to Microsoft Product Activation.

Microsoft Office 2000 (Office 9.0) introduced adaptive menus, where little-used options were hidden from the user. It also introduced a new security feature, built around digital signatures, to diminish the threat of macro viruses. Office 2000 automatically trusts macros (written in VBA 6) that were digitally signed from authors who have been previously designated as trusted. The Registration Wizard, a precursor to Microsoft Product Activation, remained in Brazil and was also extended to Australia and New Zealand, though not for volume-licensed editions. Academic software in the United States and Canada also featured the Registration Wizard.

Microsoft Office XP (Office 10.0 or Office 2002) was released in conjunction with Windows XP, and was a major upgrade with numerous enhancements and changes over Office 2000. Office XP introduced the Safe Mode feature, which allows applications such as Outlook to boot when it might otherwise fail by bypassing a corrupted registry or a faulty add-in. Smart tag is a technology introduced with Office XP in Word and Excel and discontinued in Office 2010. Office XP includes integrated voice command and text dictation capabilities, as well as handwriting recognition. It was the first version to require Microsoft Product Activation worldwide and in all editions as an anti-piracy measure, which attracted widespread controversy. Product Activation remained absent from Office for Mac releases until it was introduced in Office 2011 for Mac.

Microsoft Office 2003 (Office 11.0) was released in 2003. It featured a new logo. Two new applications made their debut in Office 2003: Microsoft InfoPath and OneNote. It is the first version to use new, more colorful icons. Outlook 2003 provides improved functionality in many areas, including Kerberos authentication, RPC over HTTP, Cached Exchange Mode, and an improved junk mail filter.

Microsoft Office 2007 (Office 12.0) was released in 2007. Office 2007's new features include a new graphical user interface called the Fluent User Interface, replacing the menus and toolbars that have been the cornerstone of Office since its inception with a tabbed toolbar, known as the Ribbon; new XML-based file formats called Office Open XML; and the inclusion of Groove, a collaborative software application.

Microsoft Office 2010 (Office 14.0, because Microsoft skipped 13.0) was finalized on April 15, 2010 and made available to consumers on June 15, 2010. The main features of Office 2010 include the backstage file menu, new collaboration tools, a customizable ribbon, protected view and a navigation panel. This is the first version to ship in 32-bit and 64-bit variants. Microsoft Office 2010 featured a new logo, which resembled the 2007 logo, except in gold, and with a modification in shape.

Microsoft released Service Pack 1 for Office 2010 on June 28, 2011 and Service Pack 2 on July 16, 2013.

A technical preview of Microsoft Office 2013 (Build 15.0.3612.1010) was released on January 30, 2012, and a Customer Preview version was made available to consumers on July 16, 2012. It sports a revamped application interface; the interface is based on Metro, the interface of Windows Phone and Windows 8. Microsoft Outlook has received the most pronounced changes so far; for example, the Metro interface provides a new visualization for scheduled tasks. PowerPoint includes more templates and transition effects, and OneNote includes a new splash screen. On May 16, 2011, new images of Office 15 were revealed, showing Excel with a tool for filtering data in a timeline, the ability to convert Roman numerals to Arabic numerals, and the integration of advanced trigonometric functions. In Word, the capability of inserting video and audio online as well as the broadcasting of documents on the Web were implemented. Microsoft has promised support for Office Open XML Strict starting with version 15, a format Microsoft has submitted to the ISO for interoperability with other office suites, and to aid adoption in the public sector. This version can read and write ODF 1.2 (Windows only).

On October 24, 2012, Office 2013 Professional Plus was released to manufacturing and was made available to TechNet and MSDN subscribers for download. On November 15, 2012, the 60-day trial version was released for public download. Office 2013 was released to general availability on January 29, 2013.

Service Pack 1 for Office 2013 was released on February 25, 2014.

On January 22, 2015, the Microsoft Office blog announced that the next version of the suite for Windows desktop, Office 2016, was in development. On May 4, 2015, a public preview of Microsoft Office 2016 was released. Office 2016 was released for Mac OS X on July 9, 2015 and for Windows on September 22, 2015.

On September 26, 2017, Microsoft announced that the next version of the suite for Windows desktop, Office 2019, was in development. On April 27, 2018, Microsoft released Office 2019 Commercial Preview for Windows 10. It was released to general availability for Windows 10 and for macOS on September 24, 2018.

Prior to packaging its various office-type Mac OS software applications into Office, Microsoft released Mac versions of Word 1.0 in 1984, the first year of the Macintosh computer; Excel 1.0 in 1985; and PowerPoint 1.0 in 1987. Microsoft does not include its Access database application in Office for Mac.

Microsoft has noted that some features are added to Office for Mac before they appear in Windows versions, such as Office for Mac 2001's Office Project Gallery and PowerPoint Movie feature, which allows users to save presentations as QuickTime movies. However, Microsoft Office for Mac has been long criticized for its lack of support of Unicode and for its lack of support for right-to-left languages, notably Arabic, Hebrew and Persian.

Microsoft Office for Mac was introduced for Mac OS in 1989, before Office was released for Windows. It included Word 4.0, Excel 2.2, PowerPoint 2.01, and Mail 1.37. It was originally a limited-time promotion but later became a regular product. With the release of Office on CD-ROM later that year, Microsoft became the first major Mac publisher to put its applications on CD-ROM.

Microsoft Office 1.5 for Mac was released in 1991 and included the updated Excel 3.0, the first application to support Apple's System 7 operating system.

Microsoft Office 3.0 for Mac was released in 1992 and included Word 5.0, Excel 4.0, PowerPoint 3.0 and Mail Client. Excel 4.0 was the first application to support new AppleScript.

Microsoft Office 4.2 for Mac was released in 1994. (Version 4.0 was skipped to synchronize version numbers with Office for Windows) Version 4.2 included Word 6.0, Excel 5.0, PowerPoint 4.0 and Mail 3.2. It was the first Office suite for Power Macintosh. Its user interface was identical to Office 4.2 for Windows leading many customers to comment that it wasn't Mac-like enough. The final release for Mac 68K was Office 4.2.1, which updated Word to version 6.0.1, somewhat improving performance.

Microsoft Office 98 Macintosh Edition was unveiled at MacWorld Expo/San Francisco in 1998. It introduced the Internet Explorer 4.0 web browser and Outlook Express, an Internet e-mail client and usenet newsgroup reader. Office 98 was re-engineered by Microsoft's Macintosh Business Unit to satisfy customers' desire for software they felt was more Mac-like. It included drag–and-drop installation, self-repairing applications and Quick Thesaurus, before such features were available in Office for Windows. It also was the first version to support QuickTime movies.

Microsoft Office 2001 was launched in 2000 as the last Office suite for the classic Mac OS. It required a PowerPC processor. This version introduced Entourage, an e-mail client that included information management tools such as a calendar, an address book, task lists and notes.
Microsoft Office v. X was released in 2001 and was the first version of Microsoft Office for Mac OS X. Support for Office v. X ended on January 9, 2007 after the release of the final update, 10.1.9 Office v.X includes Word X, Excel X, PowerPoint X, Entourage X, MSN Messenger for Mac and Windows Media Player 9 for Mac; it was the last version of Office for Mac to include Internet Explorer for Mac.

Microsoft Office 2004 for Mac was released on May 11, 2004. It includes Microsoft Word, Excel, PowerPoint, Entourage and Virtual PC. It is the final version of Office to be built exclusively for PowerPC and to officially support G3 processors, as its sequel lists a G4, G5 or Intel processor as a requirement. It was notable for supporting Visual Basic for Applications (VBA), which is unavailable in Office 2008. This led Microsoft to extend support for Office 2004 from October 13, 2009 to January 10, 2012. VBA functionality was reintroduced in Office 2011, which is only compatible with Intel processors.

Microsoft Office 2008 for Mac was released on January 15, 2008. It was the only Office for Mac suite to be compiled as a universal binary, being the first to feature native Intel support and the last to feature PowerPC support for G4 and G5 processors, although the suite is unofficially compatible with G3 processors. New features include native Office Open XML file format support, which debuted in Office 2007 for Windows, and stronger Microsoft Office password protection employing AES-128 and SHA-1. Benchmarks suggested that compared to its predecessor, Office 2008 ran at similar speeds on Intel machines and slower speeds on PowerPC machines. Office 2008 also lacked Visual Basic for Applications (VBA) support, leaving it with only 15 months of additional mainstream support compared to its predecessor. Nevertheless, five months after it was released, Microsoft said that Office 2008 was "selling faster than any previous version of Office for Mac in the past 19 years" and affirmed "its commitment to future products for the Mac."

Microsoft Office for Mac 2011 was released on October 26, 2010. It is the first version of Office for Mac to be compiled exclusively for Intel processors, dropping support for the PowerPC architecture. It features an OS X version of Outlook to replace the Entourage email client. This version of Outlook is intended to make the OS X version of Office work better with Microsoft's Exchange server and with those using Office for Windows. Office 2011 includes a Mac-based Ribbon similar to Office for Windows.

Microsoft OneNote for Mac was released on March 17, 2014. It marks the company's first release of the note-taking software on the Mac. It is available as a free download to all users of the Mac App Store in OS X Mavericks.

Microsoft Outlook 2016 for Mac debuted on October 31, 2014. It requires a paid Office 365 subscription, meaning that traditional Office 2011 retail or volume licenses cannot activate this version of Outlook. On that day, Microsoft confirmed that it would release the next version of Office for Mac in late 2015.

Despite dropping support for older versions of OS X and only keeping support for 64-bit-only versions of OS X, these versions of OneNote and Outlook are 32-bit applications like their predecessors.

The first Preview version of Microsoft Office 2016 for Mac was released on March 5, 2015. On July 9, 2015, Microsoft released the final version of Microsoft Office 2016 for Mac which includes Word, Excel, PowerPoint, Outlook and OneNote. It was immediately made available for Office 365 subscribers with either a Home, Personal, Business, Business Premium, E3 or ProPlus subscription. A non–Office 365 edition of Office 2016 was made available as a one-time purchase option on September 22, 2015.



</doc>
<doc id="20289" url="https://en.wikipedia.org/wiki?curid=20289" title="MultiMate">
MultiMate

MultiMate was a word processor developed by Multimate International for IBM PC MS-DOS computers in the early 1980s.

With 1,000 computers, Connecticut Mutual Life Insurance was one of the first large-volume customers for the IBM PC. It hired W. H. Jones & Associates to write word-processing software for the computer that would not require retraining its employees, already familiar with Wang Laboratories word processing systems. W. H. Jones' head Will Jones and five other developers created the software. W. H. Jones retained the right to sell the program elsewhere, and WordMate appeared in December 1982. The company renamed itself to SoftWord Systems, then Multimate International, while renaming WordMate to MultiMate. Advertisements stated that MultiMate "mimic[ked] the features and functions of a dedicated system", and that it was "modeled after the Wang word processor". Like Connecticut Mutual, many customers purchased it because of the similarity with the Wang.

MultiMate was not marketed heavily to end-users, but was quickly popular with insurance companies, law firms, other business computer users and US government agencies and the military. While the Wang WP keyboard was different from the original PC keyboard, MultiMate compensated by providing a large plastic template that clipped on the PC keyboard, and stick-on labels for the fronts of the PC keys. The template and labels color-coded the combination keystrokes using the SHIFT, ALT and CTRL keys with all 10 of the PC's function keys and many of the character keys. Like Wang systems, MultiMate controlled most editing operations with function keys, assigning four functions to each of the 10 function keys, which IBM initially located at the left side of the keyboard in two vertical rows. It also included a "document summary" screen for each document, another Wang feature, which allowed more sophisticated document-management than the brief file names allowed by MS-DOS and PC DOS. As Drop-down lists were popularized by other programs, they became a later addition to MultiMate.

MultiMate's popularity rapidly grew. In January 1983 some employees were paid late because of slow sales, but two months later revenue grew 25-fold after good reviews appeared in magazines. The company's fiscal 1984 sales were $15 million or more, and by early 1985 MultiMate's installed base in companies was as large as former market leader WordStar's. Jones sold the company to Ashton-Tate in December 1985 for about $20 million. an Ashton-Tate press release called the acquisition "the largest ever in the microcomputer software industry".

Other MultiMate products included foreign language versions of the software (i.e., "MultiTexto" in Spanish), a hardware interface card for file-transfer with Wang systems and versions of MultiMate for different PC clone MS-DOS computers, and for use on Novell, 3COM and IBM's PC Token Ring networks. Early attempts to create a MultiMate Data Manager and List Manager in-house never reached the market.

Multimate International developed the core word processing software and utilities (file conversion, printer drivers), but purchased and adapted sub-programs for spelling and grammar checking, list management, outlining and print-time incorporation of graphics in word processing documents (MultiMate GraphLink). In addition to rebranding such externally developed programs, Multimate rewrote the documentation for each program and adapted the program interfaces to more closely resemble the word processor. The last version of MultiMate was packaged with many of these add-on programs under the product name "MultiMate Advantage" to compete with other word processor software of the day, especially IBM DisplayWrite for DOS, which Multimate International developers saw as their main competition in the business market, and to a lesser extent WordPerfect, the DOS incarnation of Microsoft Word and the Samna word processor, which had its roots in another office word processing computer.

One of the first "clone" versions of MultiMate was bundled with an early portable PC made by Corona. Other versions were written to match PCs by Radio Shack, Texas Instruments, Toshiba, the early Grid laptop and the IBM PC Junior.

The detailed MultiMate word processor documentation, which quickly grew to three volumes, gave the product a solid "office production" feel, using high-quality paper with its main reference section presented in a padded binder with fold-out easel. (A company legend was that the MultiMate user manual was written first, by an experienced Wang WP manager, then the programmers were told to write software to match it, which is how the Wang WP was created.)

Early versions of the program came with both color-coded key stickers and a plastic full-keyboard template to make Wang operators more comfortable with the smaller IBM PC keyboard. MultiMate eventually sold a hardware keyboard with dedicated function keys and issued versions of its software for networked PCs. It adapted list-management, graphics and outlining software from other vendors to the look-and-feel of MultiMate, shipping the expanded version as MultiMate Advantage, with additional volumes of MultiMate-style documentation for the add-on programs.

Early releases of MultiMate also gave users unlimited access to a toll-free support number and a promise of low-cost upgrades, which contributed to its dedicated user population. Support policies later were brought in line with Ashton-Tate's standard practices.

MultiMate was especially good at supporting a variety of PC clones and hundreds of computer printers, each of which required its own printer driver. Such printer support was very strong with daisy-wheel and dot-matrix printers, but did not take much advantage of the development of PostScript fonts and laser printers.

Ashton-Tate never released a Windows version. It discontinued MultiMate's development efforts on VMS and Unix platforms and closed a development group in Dublin, Ireland. The product was dropped after the company was purchased by Borland.

"PC Magazine" in February 1983 stated that MultiMate "virtually remakes your computer into a Wang-like dedicated word processor", and that it was "very fast, easy to learn, and capable" with many features. The review noted the application's inability to use more than 128K of RAM, but praised the documentation and built-in help, and stated that many commands required half the keystrokes of the WordStar equivalent. The review concluded "MultiMate stands head and shoulders above many if not most [IBM PC word processors] ... an impressive entrant".

"BYTE" in 1984 was less positive. It described version 3.20 as being "very safe" because of many backups and safeguards and praised the formatting features, customization ability, and quality of the (very busy) toll-free help line. The review, however, called MultiMate "the klunkiest package" of five tested word processors because of the overemphasis on safety, criticized the built-in help and slow performance, and reported being unable to use the spell checker because of its poor quality.



</doc>
<doc id="20290" url="https://en.wikipedia.org/wiki?curid=20290" title="Mohammad Najibullah">
Mohammad Najibullah

Najibullah Ahmadzai (Pashto/; 6 August 1947 – 27 September 1996), commonly known as Najibullah or Dr. Najib, was the President of Afghanistan from 1987 until 1992, when the mujahideen took over Kabul. He had previously held different careers under the People's Democratic Party of Afghanistan (PDPA) and was a graduate of Kabul University. Following the Saur Revolution and the establishment of the Democratic Republic of Afghanistan, Najibullah was a low profile bureaucrat: he was sent into exile as Ambassador to Iran during Hafizullah Amin's rise to power. He returned to Afghanistan following the Soviet intervention which toppled Amin's rule and placed Babrak Karmal as head of state, party and government. During Karmal's rule, Najibullah became head of the KHAD, the Afghan equivalent of the Soviet KGB. He was a member of the Parcham faction led by Karmal.

During Najibullah's tenure as KHAD head, it became one of the most brutally efficient governmental organs. Because of this, he gained the attention of several leading Soviet officials, such as Yuri Andropov, Dmitriy Ustinov and Boris Ponomarev. In 1981, Najibullah was appointed to the PDPA Politburo. In 1985 Najibullah stepped down as state security minister to focus on PDPA politics; he had been appointed to the PDPA Secretariat. Mikhail Gorbachev, the last Soviet leader, was able to get Karmal to step down as PDPA General Secretary in 1986, and replace him with Najibullah. For a number of months Najibullah was locked in a power struggle against Karmal, who still retained his post of Chairman of the Revolutionary Council. Najibullah accused Karmal of trying to wreck his policy of National Reconciliation, which were a series of efforts by Najibullah to end the conflict.

During his tenure as leader of Afghanistan, the Soviets began their withdrawal, and from 1989 until 1992, his government tried to solve the ongoing civil war without Soviet troops on the ground. While direct Soviet assistance ended with the withdrawal, the Soviet Union still supported Najibullah with economic and military aid, while Pakistan and the United States continued its support for the mujahideen. Throughout his tenure, he tried to build support for his government via the National Reconciliation reforms by distancing from socialism in favor of Afghan nationalism, abolishing the one-party state and letting non-communists join the government. He remained open to dialogue with the mujahideen and other groups, made Islam an official religion, and invited exiled businessmen back to re-take their properties. In the 1990 constitution all references to communism were removed and Islam became the state religion. These changes, coupled with others, did not win Najibullah any significant support due to his role at KHAD. With the dissolution of the Soviet Union in December 1991, Najibullah was left without foreign aid. This, coupled with the internal collapse of his government, led to his resignation in April 1992.

After a failed attempt to flee to India, Najibullah remained in Kabul living in the United Nations headquarters until 1996, when the Taliban movement took Kabul. The Taliban abducted Najibullah and his brother from UN custody in the early morning hours of 27 September, tortured both of them and hanged their bodies from a traffic post the next day. 

By the 21st century however, public opinion turned positive and he is now seen to have been a strong and patriotic leader with a "normal" regime compared to his PDPA predecessors and the mayhem that happened after his ousting. In 2017 a pro-Najibist "Watan Party" was created as a continuation of Najibullah's party.

Najibullah was born in 6 august 1947 in the city of Kabul, in the Kingdom of Afghanistan. His ancestral village is located between the towns of Said Karam and Gardēz in Paktia Province, this place is known as Mehlan. He was educated at Habibia High School in Kabul, St. Joseph's School in Baramulla, India and Kabul University, where he graduated with a doctor degree in medicine in 1975. He belongs to the Ahmadzai sub-tribe of the Ghilzai Pashtun tribe in Gardiz.

In 1965 Najibullah joined the Parcham faction of the Communist People's Democratic Party of Afghanistan (PDPA). He served as Babrak Karmal's close associate and bodyguard during the latter's tenure in the lower house of parliament (1965–1973), Najibullah earned the nickname Najib-e-Gaw (Najib the Bull) due in equal parts to his imposing heft and temperament. In 1977 he was elected to the Central Committee.

In April 1978 the PDPA took power in Afghanistan, with Najibullah a member of the ruling Revolutionary Council. However, the Khalq faction of the PDPA gained supremacy over his own Parcham faction, and after a brief stint as Ambassador to Iran, he was dismissed from government and went into exile in Europe.

He returned to Kabul after the Soviet intervention in 1979. In 1980, he was appointed the head of KHAD, the Afghan equivalent to the Soviet KGB, and was promoted to the rank of Major General. He was appointed following lobbying made by the Soviets, most notable among them was Yuri Andropov, the KGB Chairman. During his six years as head of KHAD he had two to four deputies under his command, who in turn were responsible for an estimated 12 departments. According to evidence, Najibullah was dependent on his family and his professional network, and appointed more often than not people he knew to top positions within the KHAD. In June 1981, Najibullah, along with Mohammad Aslam Watanjar, a former tank commander and the then Minister of Communications and Major General Mohammad Rafi, the Minister of Defence were appointed to the PDPA Politburo. Under Najibullah, KHAD's personnel increased from 120 to 25,000 to 30,000. KHAD employees were amongst the best-paid government bureaucrats in communist Afghanistan, and because of it, the political indoctrination of KHAD officials was a top priority. During a PDPA conference Najibullah, talking about the indoctrination programme of KHAD officials, said "a weapon in one hand, a book in the other." Terrorist activities launched by KHAD reached its peak under Najibullah. He reported directly to the Soviet KGB, and a big part of KHAD's budget came from the Soviet Union itself.

As time would show, Najibullah was very efficient, and during his tenure as leader of KHAD, thousands were arrested, tortured, and executed. There are first-hand accounts of survivors who stated that Najibullah would personally participate in the torture of high-profile anti-communist citizens. KHAD targeted anti-communist citizens, political opponents, and educated members of society. It was this efficiency which made him interesting to the Soviets. Because of this, KHAD became known for its ruthlessness. During his ascension to power, several Afghan politicians did not want Najibullah to succeed Babrak Karmal because of the fact that Najibullah was known for exploiting his powers for his own benefit. It didn't help either that during his period as KHAD chief that the Pul-i Charki had become the home of several Khalqist politicians. Another problem was that Najibullah allowed graft, theft, bribery and corruption on a scale not seen previously. As would later be proven by the power struggle he had with Karmal after becoming PDPA General Secretary, despite Najibullah heading the KHAD for five years, Karmal still had sizable support in the organisation.

He was appointed to the PDPA Secretariat in November 1985. Najibullah's ascent to power was proven by turning KHAD from a government organ to a ministry in January 1986. With the situation in Afghanistan deteriorating, and the Soviet leadership looking for ways to withdraw, Mikhail Gorbachev wanted Karmal to resign as PDPA General Secretary. The question of who was to succeed Karmal was hotly debated, but Gorbachev supported Najibullah. Yuri Andropov, Boris Ponomarev and Dmitriy Ustinov all thought highly of Najibullah, and negotiations of who would succeed Karmal might have begun as early as 1983. Despite this, Najibullah was not the only choice the Soviets had. A GRU report argued that he was a Pashtun nationalist, a stance which could decrease the regime's popularity even more. The GRU preferred Assadullah Sarwari, earlier head of ASGA, the pre-KHAD secret police, who they believed would be better able to balance between the Pashtuns, Tajiks and Uzbeks. Another viable candidate was Abdul Qadir, who had been a participant in the Saur Revolution. Najibullah succeeded Karmal as PDPA General Secretary on 4 May 1986 at the 18th PDPA meeting, but Karmal still retained his post as Chairman of the Presidium of the Revolutionary Council.

On 15 May Najibullah announced that a collective leadership had been established, which was led by himself consisted of himself as head of party, Karmal as head of state and Sultan Ali Keshtmand as Chairman of the Council of Ministers. When Najibullah took the office of PDPA General Secretary, Karmal still had enough support in the party to disgrace Najibullah. Karmal went as far as to spread rumours that Najibullah's rule was little more than an interregnum, and that he would soon be reappointed to the general secretaryship. As it turned out, Karmal's power base during this period was KHAD. The Soviet leadership wanted to ease Karmal out of politics, but when Najibullah began to complain that he was hampering his plans of National Reconciliation, the Soviet Politburo decided to remove Karmal; this motion was supported by Andrei Gromyko, Yuli Vorontsov, Eduard Shevardnadze, Anatoly Dobrynin and Viktor Chebrikov. A meeting in the PDPA in November relieved Karmal of his Revolutionary Council chairmanship, and he was exiled to Moscow where he was given a state-owned apartment and a dacha. In his position as Revolutionary Council chairman Karmal was succeeded by Haji Mohammad Chamkani, who was not a member of the PDPA.

In 1986 there were more than 100,000 political prisoners and there had been more than 16,500 extrajudicial executions. Its main objectives were the opponents of communism and the most educated classes in society.

In September 1986 the National Compromise Commission (NCC) was established on the orders of Najibullah. The NCC's goal was to contact counter-revolutionaries "in order to complete the Saur Revolution in its new phase." Allegedly, an estimated 40,000 rebels were contacted by the government. At the end of 1986, Najibullah called for a six-months ceasefire and talks between the various opposition forces, this was part of his policy of National Reconciliation. The discussions, if fruitful, would lead to the establishment of a coalition government and be the end of the PDPA's monopoly of power. The programme failed, but the government was able to recruit disillusioned mujahideen fighters as government militias. In many ways, the National Reconciliation led to an increasing number of urban dwellers to support his rule, and the stabilisation of the Afghan defence forces.

In September 1986 a new constitution was written, which was adopted on 29 November 1987. The constitution weakened the powers of the head of state by canceling his absolute veto. The reason for this move, according to Najibullah, was the need for real-power sharing. On 13 July 1987 the official name of Afghanistan was changed from the Democratic Republic of Afghanistan to Republic of Afghanistan, and in June 1988 the Revolutionary Council, whose members were elected by the party leadership, was replaced by a National Assembly, an organ in which members were to be elected by the people. The PDPA's socialist stance was denied even more than previously, in 1989 the Minister of Higher Education began to work on the "de-Sovietisation" of universities, and in 1990 it was even announced by a party member that all PDPA members were Muslims and that the party had abandoned Marxism. Many parts of the Afghan government's economic monopoly was also broken, this had more to do with the tight situation than any ideological conviction. Abdul Hakim Misaq, the Mayor of Kabul, even stated that traffickers of stolen goods would not be prosecuted by law as long as their goods were given to the market. Yuli Vorontsov, on Gorbachev's orders, was able to get an agreement with the PDPA leadership to offer the posts of Gossoviet chairman (the state planning organ), the Council of Ministers chairmanship (head of government), ministries of defence, state security, communications, finance, presidencies of banks and the Supreme Court. It should be noted, the PDPA still demanded it held on to all deputy ministers, retained its majority in the state bureaucracy and that it retained all its provincial governors. The government was not willing to concede all of these positions, and when the offer was broadcast, the ministries of defence and state security.

Local elections were held in 1987. It began when the government introduced a law permitting the formation of other political parties, announced that it would be prepared to share power with representatives of opposition groups in the event of a coalition government, and issued a new constitution providing for a new bicameral National Assembly (Meli Shura), consisting of a Senate (Sena) and a House of Representatives (Wolesi Jirga), and a president to be indirectly elected to a 7-year term. The new political parties had to oppose colonialism, imperialism, neo-colonialism, Zionism, racial discrimination, apartheid and fascism. Najibullah stated that only the extremist part of the opposition could not join the planned coalition government. No parties had to share the PDPA's policy or ideology, but they could not oppose the bond between Afghanistan and the Soviet Union. A parliamentary election was held in 1988. The PDPA won 46 seats in the House of Representatives and controlled the government with support from the National Front, which won 45 seats, and from various newly recognized left-wing parties, which had won a total of 24 seats. Although the election was boycotted by the Mujahideen, the government left 50 of the 234 seats in the House of Representatives, as well as a small number of seats in the Senate, vacant in the hope that the guerrillas would end their armed struggle and participate in the government. The only armed opposition party to make peace with the government was Hizbollah, a small Shi'a party not to be confused with the bigger party in Iran or the Lebanese organization.

Several figures of the intelligentsia took Najibullah's offer seriously, even if they sympathised or were against the regime. Their hopes were dampened when the Najibullah government introduced the state of emergency on 18 February 1989, four days after the Soviet withdrawal. 1,700 intellectuals were arrested in February alone, and until November 1991 the government still supervised and restricted freedom of speech. Another problem was that party members took his policy seriously too, Najibullah recanted that most party members felt "panic and pessimism." At the Second Conference of the party, the majority of members, maybe up to 60 percent, were radical socialists. According to Soviet advisors (in 1987), a bitter debate within the party had broken out between those who advocated the islamisation of the party and those who wanted to defend the gains of the Saur Revolution. Opposition to his policy of National Reconciliation was met party-wide, but especially from Karmalists. Many people did not support the handing out of the already small state resources the Afghan state had at its disposal. On the other side, several members were proclaiming anti-Soviet slogans as they accused the National Reconciliation programme to be supported and developed by the Soviet Union. Najibullah reassured the inter-party opposition that he would not give up the gains of the Saur Revolution, but to the contrary, preserve them, not give up the PDPA's monopoly on power, or to collaborate with reactionary Mullahs.

During Babrak Karmal's later years, and during Najibullah's tenure, the PDPA tried to improve their standing with Muslims by moving, or appearing to move, to the political centre. They wanted to create a new image for the party and state. In 1987 Najibullah re-added Ullah to his name to appease the Muslim community. Communist symbols were either replaced or removed. These measures did not contribute to any notable increase in support for the government, because the mujahideen had a stronger legitimacy to protect Islam than the government; they had rebelled against what they saw as an anti-Islamic government, that government was the PDPA. Islamic principles were embedded in the 1987 constitution, for instance, Article 2 of the constitution stated that Islam was the state religion, and Article 73 stated that the head of state had to be born into a Muslim Afghan family. The 1990 constitution stated that Afghanistan was an Islamic state, and the last references to communism were removed. Article 1 of the 1990 Constitution said that Afghanistan was an "independent, unitary and Islamic state."

Najibullah continued Karmal's economic policies. The augmenting of links with the Eastern Bloc and the Soviet Union continued, and so did bilateral trade. He encouraged the development of the private sector in industry. The Five-Year Economic and Social Development Plan which was introduced in January 1986 continued until March 1992, one month before the government's fall. According to the plan, the economy, which had grown less than 2 percent annually until 1985, would grow 25 percent in the plan. Industry would grow 28 percent, agriculture 14–16 percent, domestic trade by 150 percent and foreign trade with 15 percent. As expected, none of these targets were met, and 2 percent growth annually which had been the norm before the plan continued under Najibullah. The 1990 constitution gave due attention to the private sector. Article 20 was about the establishment of private firms, and Article 25 encouraged foreign investments in the private sector.

While he may have been the "de jure" leader of Afghanistan, Soviet advisers still did the majority of work when Najibullah took power. As Gorbachev remarked "We're still doing everything ourselves [...]. That's all our people know how to do. They've tied Najibullah hand and foot." Fikryat Tabeev, the Soviet ambassador to Afghanistan, was accused of acting like a governor general by Gorbachev. Tabeev was recalled from Afghanistan in July 1986, but while Gorbachev called for the end of Soviet management of Afghanistan, he could not help but to do some managing himself. At a Soviet Politburo meeting, Gorbachev said "It's difficult to build a new building out of old material [...] I hope to God that we haven't made a mistake with Najibullah." As time would prove, the problem was that Najibullah's aims were the opposite of the Soviet Union's; Najibullah was opposed to a Soviet withdrawal, the Soviet Union wanted a Soviet withdrawal. This was logical, considering the fact that the Afghan military was on the brink of dissolution. The only means of survival seemed to Najibullah was to retain the Soviet presence. In July 1986 six regiments, which consisted up to 15,000 troops, were withdrawn from Afghanistan. The aim of this early withdrawal was, according to Gorbachev, to show the world that the Soviet leadership was serious about leaving Afghanistan. The Soviets told the United States Government that they were planning to withdraw, but the United States Government didn't believe it. When Gorbachev met with Ronald Reagan during his visit the United States, Reagan called for the dissolution of the Afghan army.

On 14 April 1988 the Afghan and Pakistani governments signed the Geneva Accords, and the Soviet Union and the United States signed as guarantors; the treaty specifically stated that the Soviet military had to withdraw from Afghanistan by 15 February 1989. Gorbachev later confided to Anatoly Chernyaev, a personal advisor to Gorbachev, that the Soviet withdrawal would be criticised for creating a bloodbath which could have been averted if the Soviets stayed. During a Politburo meeting Eduard Shevardnadze said "We will leave the country in a deplorable situation", and further talked about the economic collapse, and the need to keep at least 10 to 15,000 troops in Afghanistan. In this Vladimir Kryuchkov, the KGB Chairman, supported him. This stance, if implemented, would be a betrayal of the Geneva Accords just signed. During the second phase of the Soviet withdrawal, in 1989, Najibullah told Valentin Varennikov openly that he would do everything to slow down the Soviet departure. Varennikov in turn replied that such a move would not help, and would only lead to an international outcry against the war. Najibullah would repeat his position later that year, to a group of senior Soviet representatives in Kabul. This time Najibullah stated that Ahmad Shah Massoud was the main problem, and that he needed to be killed. In this, the Soviets agreed, but repeated that such a move would be a breach of the Geneva Accords; to hunt for Massoud so early on would disrupt the withdrawal, and would mean that the Soviet Union would fail to meet its deadline for withdrawal.

During his January 1989 visit to Shevardnadze Najibullah wanted to retain a small presence of Soviet troops in Afghanistan, and called for moving Soviet bombers to military bases close to the Afghan–Soviet border and place them on permanent alert. Najibullah also repeated his claims that his government could not survive if Massoud remained alive. Shevardnadze again repeated that troops could not stay, since it would lead to international outcry, but said he would look into the matter. Shevardnadze demanded that the Soviet embassy created a plan in which at least 12,000 Soviet troops would remain in Afghanistan either under direct control of the United Nations or remain as "volunteers". The Soviet military leadership, when hearing of Shevardnadze's plan, became furious. But they followed orders, and named the operation "Typhoon", maybe ironic considering that Operation Typhoon was the German military operation against the city of Moscow during World War II. Shevardnadze contacted the Soviet leadership about moving a unit to break the siege of Kandahar, and to protect convoys from and to the city. The Soviet leadership were against Shevardnadze's plan, and Chernyaev even believed it was part of Najibullah's plan to keep Soviet troops in the country. To which Shevardnadze replied angrily "You've not been there, [...] You've no idea all the things we have done there in the past ten years." At a Politburo meeting on 24 January, Shevardnadze argued that the Soviet leadership could not be indifferent to Najibullah and his government; again, Shevardnadze received support from Kryuchkov. In the end Shevardnadze lost the debate, and the Politburo reaffirmed their commitment to withdraw from Afghanistan. There was still a small presence of Soviet troops after the Soviet withdrawal; for instance, parachutists who protected the Soviet embassy staff, military advisors and special forces and reconnaissance troops still operated in the "outlying provinces", especially along the Afghan–Soviet border.

Soviet military aid continued after their withdrawal, and massive quantities of food, fuel, ammunition and military equipment was given to the government. Varennikov visited Afghanistan in May 1989 to discuss ways and means to deliver the aid to the government. In 1990 Soviet aid amounted to an estimated 3 billion United States dollars. As it turned out, the Afghan military was entirely dependent on Soviet aid to function. When the Soviet Union was dissolved on 26 December 1991, Najibullah turned to former Soviet Central Asia for aid. These newly independent states had no wish to see Afghanistan being taken over by religious fundamentalists, and supplied Afghanistan with 6 million barrels of oil and 500,000 tons of wheat to survive the winter.

With the Soviets' withdrawal in 1989, the Afghan army was left on its own to battle the insurgents. The most effective, and largest, assaults on the mujahideen were undertaken during the 1985–86 period. These offensives had forced the mujahideen on the defensive near Herat and Kandahar. The Soviets ensued a bomb and negotiate during 1986, and a major offensive that year included 10,000 Soviet troops and 8,000 Afghan troops.

Pakistani people and establishment continued to support the Afghan mujahideen even if it was in contravention of the Geneva Accords. At the beginning most observers expected the Najibullah government to collapse immediately, and to be replaced with an Islamic fundamentalist government. The Central Intelligence Agency stated in a report that the new government would be ambivalent, or even worse, hostile towards the United States. Almost immediately after the Soviet withdrawal, the Battle of Jalalabad broke out between Afghan government forces and the mujahideen. The offensive against the city began when the mujahideen bribed several government military officers, from there, they tried to take the airport, but were repulsed with heavy casualties. The willingness of the common Afghan government soldier to fight increased when the mujahideen began to execute people during the battle. During the battle Najibullah called for Soviet assistance. Gorbachev called an emergency session of the Politburo to discuss his proposal, but Najibullah's request was rejected. Other attacks against the city failed, and by April the government forces were on the offensive. During the battle over four hundred Scud missiles were shot, which were fired by a Soviet crew which had stayed behind. When the battle ended in July, the mujahideen had lost an estimated 3,000 troops. One mujahideen commander lamented "the battle of Jalalabad lost us credit won in ten years of fighting."

Hardline Khalqist Shahnawaz Tanai attempted to overthrow Najibullah in a failed coup attempt in March 1990.

From 1989 to 1990, the Najibullah government was partially successful in building up the Afghan defence forces. The Ministry of State Security had established a local militia force which stood at an estimated 100,000 men. The 17th Division in Herat, which had begun the 1979 Herat uprising against PDPA-rule, stood at 3,400 regular troops and 14,000 tribal men. In 1988, the total number of security forces available to the government stood at 300,000. This trend did not continue, and by the summer of 1990, the Afghan government forces were on the defensive again. By the beginning of 1991, the government controlled only 10 percent of Afghanistan, the eleven-year Siege of Khost had ended in a mujahideen victory and the morale of the Afghan military finally collapsed. In the Soviet Union, Kryuchkov and Shevardnadze had both supported continuing aid to the Najibullah government, but Kryuchkov had been arrested following the failed 1991 Soviet coup d'état attempt and Shevardnadze had resigned from his posts in the Soviet government in December 1990 – there were no longer any pro-Najibullah people in the Soviet leadership and the Soviet Union was in the middle of an economic and political crisis, which would lead directly to the dissolution of the Soviet Union on 26 December 1991. At the same time Boris Yeltsin became Russia's new hope, and he had no wish to continue to aid Najibullah's government, which he considered a relic of the past. In the autumn of 1991, Najibullah wrote to Shevardnadze "I didn't want to be president, you talked me into it, insisted on it, and promised support. Now you are throwing me and the Republic of Afghanistan to its fate."

In January 1992, the Russian government ended its aid to the Najibullah government. The effects were felt immediately: the Afghan Air Force, the most effective part of the Afghan military, was grounded due to lack of fuel. The Afghan mujahideen continued to be supported by Pakistan and establishment. Major cities were lost to the rebels, and terrorist attacks became common in Kabul. On the fifth anniversary of his policy of National Reconciliation, Najibullah blamed the Soviet Union for the disaster that had stricken Afghanistan. The day the Soviet Union withdrew was hailed by Najibullah as the Day of National Salvation. But it was too late, and his government's collapse was imminent.

On March 18, 1992, Najibullah offered his government's immediate resignation, and followed the United Nations (UN) plan to be replaced by an interim government with all parties involved in the struggle. In mid-April Najibullah accepted a UN plan to hand power to a seven-man council, and several days later on 14 April, Najibullah was forced to resign on the orders of the Watan Party because of the loss of Bagram Airbase and the town of Charikar. Abdul Rahim Hatef became acting head of state following Najibullah's resignation. The mujahideen forces took Kabul shortly thereafter and most of them signed the Peshawar Accord, creating the new Islamic State of Afghanistan.

Not long before Kabul's fall, Najibullah appealed to the UN for amnesty, which he was granted. But his attempt to flee to the airport was thwarted by troops of Abdul Rashid Dostum - once loyal to him, but now allied with Ahmad Shah Massoud - who controlled the airport. At the UN compound in Kabul, while waiting for the UN to negotiate his safe passage to India, he engaged himself in translating Peter Hopkirk's book "The Great Game" into his mother tongue Pashto. India was at a difficult position in deciding to allow Najibullah political asylum and safely escort him out of the country. Supporters claimed he had always been close to India and should not be denied asylum, but others said doing so would risk antagonizing India's relationship with the new mujahideen government formed under the Peshawar Accord. India also refused to let him take refuge at the Indian embassy as it risked creating "subcontinental rivalries" and reprisals against Kabul's Indian community, arguing that Najibullah would be far safer at the UN compound. All attempts failed and he eventually sought haven in the local UN headquarters, where he would stay until 1996.

In 1994, India sent senior diplomat M. K. Bhadrakumar to Kabul to hold talks with Ahmad Shah Massoud, the defence minister, to consolidate relations with the Afghan authorities, reopen the embassy, and allow Najibullah to fly to India, but Massoud refused. Bhadrakumar wrote in 2016 that he believed Massoud did not want Najibullah to leave as Massoud could strategically make use of him, and that Massoud "probably harboured hopes of a co-habitation with Najib somewhere in the womb of time because that extraordinary Afghan politician was a strategic asset to have by his side". At the time, Massoud was commanding the government's forces fighting the militias of Dostum and Gulbuddin Hekmatyar during the Battle of Kabul.

A few months before his death, he quoted, "Afghans keep making the same mistake," reflecting upon his translation to a visitor.

In September 1996 when the Pakistan-backed Taliban were about to enter Kabul, Massoud offered Najibullah an opportunity to flee the capital. Najibullah refused. The reasons as to why he refused remain unclear. Massoud himself has claimed that Najibullah feared that "if he fled with the Tajiks, he would be for ever damned in the eyes of his fellow Pashtuns." Others, like general Tokhi, who was with Dr. Najibullah until the day before his torture and execution, have stated that Najibullah mistrusted Massoud after his militia had repeatedly put the UN compound under rocket fire and had effectively barred Najibullah from leaving Kabul. "If they wanted Najibullah to flee Kabul in safety," Tokhi said, "they could have provided him the opportunity as they did with other high ranking officials from the communist party from 1992 to 1996." Thus when Massoud's militia came to both Dr. Najibullah and General Tokhi and asked them to come with them to flee Kabul, they rejected the offer. Najibullah was at the UN compound when the Taliban soldiers came for him on the evening of 26 September 1996. The Taliban kidnapped and then dragged his dead, and castrated body behind a truck through the streets. His brother Ahmadzai was given the same treatment. Najibullah and Ahmadzai's bodies were hanged on public display in order to show the public that a new era had begun. At first Najibullah and Ahmadzai were denied an Islamic funeral because of their "crimes", but the bodies were later handed over to the International Committee of the Red Cross who in turn sent their bodies to Paktia Province where both of them were given a proper funeral by their fellow Ahmadzai tribesmen.

There was widespread international condemnation,<ref name="un-51/108">"Situation of human rights in Afghanistan" United Nations Resolution 51/108, Article 10. 12 December 1996. Retrieved 15 June 2015 ""Endorses the Special Rapporteur's condemnation of the abduction from United Nations premises of the former President of Afghanistan, Mr. Najibullah, and of his brother, and of their subsequent summary execution;""</ref> particularly from the Muslim world. The United Nations issued a statement which condemned the execution of Najibullah, and claimed that it would further destabilise Afghanistan. The Taliban responded by issuing death sentences on Dostum, Massoud and Burhanuddin Rabbani. India, which had been supporting Najibullah, strongly condemned his public execution and began to support Massoud's United Front/Northern Alliance in an attempt to contain the rise of the Taliban. On the 20th anniversary of his death, in 2016, Afghanistan's Research Center blamed the Pakistani Inter-Services Intelligence (ISI) for his death, saying that the plan to kill Najibullah was implemented by the ISI through the Taliban they backed.

After Najibullah's death, the brutal civil war between mujahideen factions, the hardline Taliban regime, continued fighting, and enduring problems with corruption and poverty, his image among the Afghan people dramatically improved, and Najibullah came to be seen as a strong and patriotic leader. Since the 2010s, posters and pictures of him have become a common sight in many Afghan cities.

On July 28, 2017, thousands attended an event at a Kabul hotel for the fourth "consultative gathering for a legal relaunch of Hezb-e Watan [Homeland Party]".



</doc>
<doc id="20292" url="https://en.wikipedia.org/wiki?curid=20292" title="Multiplan">
Multiplan

Multiplan was an early spreadsheet program developed by Microsoft. Known initially by the code name "EP" (for "Electronic Paper"), it was introduced in 1982 as a competitor for VisiCalc.

Multiplan was released first for computers running CP/M; it was developed using a Microsoft proprietary p-code C compiler as part of a portability strategy that facilitated ports to systems such as MS-DOS, Xenix, Commodore 64 and 128, Texas Instruments TI-99/4A (on four 6K GROMs and a single 8K ROM), Radio Shack TRS-80 Model II, TRS-80 Model 4, TRS-80 Model 100 (on ROM), Apple II, and Burroughs B-20 series. The CP/M version was also runnable on the TRS-80 Model II and 4, Commodore 128, and Apple II with a CP/M card. In France, Multiplan was also released for the Thomson computers in 1986.

Despite the release of Microsoft Chart, a graphics companion program, Multiplan continued to be outsold by Lotus 1-2-3. It was replaced by Microsoft Excel, which followed some years later on both the Apple Macintosh (1985) and Microsoft Windows (1987).

Although over a million copies were sold, Multiplan was not able to mount an effective challenge to Lotus 1-2-3. According to Bill Gates, this was due to the excessive number of ports (there were approximately 100 different versions of Multiplan). He also believed that it was a mistake to release 8-bit versions instead of focusing on the newer 16-bit machines and as a result, "We decided to let [Lotus] have the character-based DOS market while we would instead focus on the next generation--graphical software on the Macintosh and Windows."

Around 1983, during the development of the first release of Windows, Microsoft had plans to make a Windows version. However the plans changed a year later.

A version was available for the Apple Lisa 2 running Microsoft/SCO Xenix 3. It fit on one 400K microfloppy diskette.

A fundamental difference between Multiplan and its competitors was Microsoft's decision to use R1C1 addressing instead of the A1 addressing introduced by VisiCalc. Although R1C1-style formulae are more straightforward than A1-style formulae for instance, "RC[-1]" (meaning "current row, previous column") is expressed as "A1" in cell B1, then "A2" in cell B2, etc. most spreadsheet users prefer the A1 addressing style introduced by VisiCalc.

Microsoft carried Multiplan's R1C1 legacy forward into Microsoft Excel, which offers both addressing modes, although A1 is MS Excel's default addressing mode.

"Ahoy!" called the Commodore 64 version of Multiplan, distributed by Human Engineered Software, a "professional quality spreadsheet ... There is not enough room in this article to mention all the mathematical operations performed ... Documentation is lengthy but well written". A second review in the magazine noted the limitation of the computer's 40-column screen, but praised the ability to stop any ongoing action. It also praised the documentation, and concluded that "its ease of use and foolproof design make "Multiplan" an outstanding value". "BYTE" said that "Multiplan for the Macintosh is a winner", stating that combining other versions' power and features with the Macintosh's graphics and user interface "rivals, and in many ways exceeds, anything else available in the spreadsheet genre".



</doc>
<doc id="20297" url="https://en.wikipedia.org/wiki?curid=20297" title="MOS Technology 6502">
MOS Technology 6502

The MOS Technology 6502 (typically "sixty-five-oh-two" or "six-five-oh-two") is an 8-bit microprocessor that was designed by a small team led by Chuck Peddle for MOS Technology.

When it was introduced in 1975, the 6502 was, by a considerable margin, the least expensive microprocessor on the market. It initially sold for less than one-sixth the cost of competing designs from larger companies, such as Motorola and Intel. Its introduction caused rapid decreases in pricing across the entire processor market. Along with the Zilog Z80, it sparked a series of projects that resulted in the home computer revolution of the early 1980s.

Popular home video game consoles and computers, such as the Atari 2600, Atari 8-bit family, Apple II, Nintendo Entertainment System, Commodore 64, Atari Lynx, BBC Micro and others, used the 6502 or variations of the basic design. Soon after the 6502's introduction, MOS Technology was purchased outright by Commodore International, who continued to sell the microprocessor and licenses to other manufacturers. In the early days of the 6502, it was second-sourced by Rockwell and Synertek, and later licensed to other companies.

In its CMOS form, which was developed by the Western Design Center (WDC), the 6502 family continues to be widely used in embedded systems, with estimated production volumes in the hundreds of millions.

The 6502 was designed by many of the same engineers that had designed the Motorola 6800 microprocessor family. Motorola started the 6800 microprocessor project in 1971 with Tom Bennett as the main architect. The chip layout began in late 1972, the first 6800 chips were fabricated in February 1974 and the full family was officially released in November 1974. John Buchanan was the designer of the 6800 chip and Rod Orgill, who later did the 6501, assisted Buchanan with circuit analyses and chip layout. Bill Mensch joined Motorola in June 1971 after graduating from the University of Arizona (at age 26). His first assignment was helping define the peripheral ICs for the 6800 family and later he was the principal designer of the 6820 Peripheral Interface Adapter (PIA). Motorola's engineers could run analog and digital simulations on an IBM 370-165 mainframe computer. Bennett hired Chuck Peddle in 1973 to do architectural support work on the 6800 family products already in progress. He contributed in many areas, including the design of the 6850 ACIA (serial interface).

Motorola's target customers were established electronics companies such as Hewlett-Packard, Tektronix, TRW, and Chrysler. In May 1972, Motorola's engineers began visiting select customers and sharing the details of their proposed 8-bit microprocessor system with ROM, RAM, parallel and serial interfaces. In early 1974, they provided engineering samples of the chips so that customers could prototype their designs. Motorola's "total product family" strategy did not focus on the price of the microprocessor, but on reducing the customer's total design cost. They offered development software on a timeshare computer, the "EXORciser" debugging system, onsite training and field application engineer support. Both Intel and Motorola had initially announced a $360 price for a single microprocessor. The actual price for production quantities was much less. Motorola offered a design kit containing the 6800 with six support chips for $300.

Peddle, who would accompany the salespeople on customer visits, found that customers were put off by the high cost of the microprocessor chips. To lower the price, the IC chip size would have to shrink so that more chips could be produced on each silicon wafer. At the same time, these visits invariably resulted in the engineers he presented to producing lists of required instructions that were much smaller than "all these fancy instructions" that had been included in the 6800. This presented the opportunity to lower the cost of the system by removing the unneeded functionality while moving to a newer fabrication technology, "depletion-mode" MOS transistors. Peddle and other team members started outlining the design of an improved feature, reduced size microprocessor. At that time, Motorola's new semiconductor fabrication facility in Austin, Texas, was having difficulty producing MOS chips, and mid-1974 was the beginning of a year-long recession in the semiconductor industry. Also, many of the Mesa, Arizona, employees were displeased with the upcoming relocation to Austin.

Motorola's Semiconductor Products Division management was overwhelmed with problems and showed no interest in Peddle's low-cost microprocessor proposal. Eventually Peddle was given an official letter telling him to stop working on the system. Peddle responded to the order by informing Motorola that the letter represented an official declaration of "project abandonment", and as such, the intellectual property he had developed to that point was now his. In a November 1975 interview, Motorola's Chairman, Robert Galvin, ultimately agreed that Peddle's concept was a good one and that the division missed an opportunity, "We did not choose the right leaders in the Semiconductor Products division." The division was reorganized and the management replaced. The new group vice-president John Welty said, "The semiconductor sales organization lost its sensitivity to customer needs and couldn't make speedy decisions."

Peddle began looking outside Motorola for a source of funding for this new project. He initially approached MOSTEK CEO L.J. Sevin, but Sevin declined and later admitted this was because he was afraid Motorola would sue them.

While Peddle was visiting Ford Motor Company on one of his sales trips, Bob Johnson, later head of Ford's engine automation division, mentioned that their former colleague John Paivinen had moved to General Instrument and taught himself semiconductor design, and was now doing some very interesting work on calculator chipsets at a new company in Pennsylvania. After the MOSTEC efforts fell through, Peddle approached Paivinen, who "immediately got it."

MOS Technology was formed in 1969 by three executives from General Instrument, Mort Jaffe, Don McLaughlin, and John Paivinen, to produce metal-oxide-semiconductor (MOS) integrated circuits. Allen-Bradley, a supplier of electronic components and industrial controls, acquired a majority interest in 1970. The company designed and fabricated custom ICs for customers and had developed a line of calculator chips.

On 19 August 1974, Chuck Peddle, Bill Mensch, Rod Orgill, Harry Bawcom, Ray Hirt, Terry Holdt, and Wil Mathys left Motorola to join MOS Technology at Valley Forge, Pennsylvania. (Mike Janes joined later.) Of the seventeen chip designers and layout people on the 6800 team, seven left. There were 30 to 40 other marketers, application engineers and system engineers on the 6800 team. That December, Gary Daniels transferred into the 6800 microprocessor group. Tom Bennett did not want to leave the Phoenix area so Daniels took over the microprocessor development in Austin. His first project was a "depletion-mode" version of the 6800; this cut the chip area nearly in half and doubled the speed. The faster parts were available in July 1976. This was followed by the 6802 which added 128 bytes of RAM and an on-chip clock oscillator circuit. 

The goal of the team was to design and produce a low-cost microprocessor for embedded applications and to target as wide as possible a customer base. This would be possible only if the microprocessor was low cost, and the price goal was set at $5 in volume.

A smaller chip area means more chips per silicon wafer, which in turn results in greater yield as defects are generally randomly but uniformly scattered across the wafer area. So the more chips per wafer, the smaller the ratio of defective chips to total wafer chips. The original 6800 chips were intended to be 180 x 180 mils but layout was completed at 212 x 212 mils (5.4 x 5.4 mm) or an area of 29.0 mm. For the new design, the cost goal demanded a size goal of 153 x 168 mils (3.9 x 4.3 mm) or an area of 16.6 mm. At that time the technical literature would state the length and width of each chip in "mils" (0.001 inch).

The size goal required n-channel "depletion-load" MOS transistors, a more advanced process than MOS Technology's calculator chips used. John Paivinen was able to have the fabrication process ready by June 1975.

Chuck Peddle, Rod Orgill, and Wil Mathys designed the initial architecture of the new processors. A September 1975 article in EDN magazine gives this summary of the design:

The MOS Technology 650X family represents a conscious attempt of eight former Motorola employees who worked on the development of the 6800 system to put out a part that would replace and outperform the 6800, yet undersell it. With the benefit of hindsight gained on the 6800 project, the MOS Technology team headed by Chuck Peddle, made the following architectural changes in the Motorola CPU…
Given the size limits, the entire chip design had to be constantly considered. Mench and Paivinen worked on the instruction decoder while Mench, Peddle and Orgill worked on the ALU and registers. The size limits were particularly hard on the register layout and required the removal of the second "B" accumulator found in the 6800. A key advance, developed at a party, was a way to share some of the internal wiring to allow the ALU to be reduced in size.

The 16-bit 6800 index register with an 8-bit offset in the instruction was replaced with two 8-bit index registers with an 8-bit or 16-bit offset. Three-state control was eliminated from the address bus outputs. A clock generator was included on the chip. The address bus was always active so the VMA (valid-memory address) output was eliminated. An "8080-type" RDY signal for single-cycle stepping was added.

There would be two microprocessors: the 6501 would plug into the same socket as the Motorola 6800, while the 6502 would work with 6800 family peripherals and have an on-chip clock oscillator. These processors would not run 6800 software because they had a different instruction set, different registers, and mostly different addressing modes.

The chip high-level design had to be turned into drawings of transistors and interconnects. At MOS Technology, the "layout" was a very manual process done with color pencils and vellum paper. The layout consisted of thousands of polygon shapes on six different drawings; one for each layer of the semiconductor fabrication process. Rod Orgill was responsible for the 6501 design; he had assisted John Buchanan at Motorola on the 6800. Bill Mensch did the 6502; he was the designer of the 6820 Peripheral Interface Adapter (PIA) at Motorola. Harry Bawcom, Mike Janes and Sydney-Anne Holt helped with the layout.

In spite of their best efforts, the final design ended up being 5 mils too wide. The first 6502 chips were 168 x 183 mils (4.3 x 4.7 mm) or an area of 19.8 mm. The Rotate Right instruction (ROR) did not work in the first silicon, so the instruction was temporarily omitted from the published documents, but the next iteration of the design shrank the chip and fixed the Rotate Right instruction, which was then included in revised documentation.

MOS Technology's microprocessor introduction was quite different from the traditional months-long product launch. The first run of a new integrated circuit is normally used for internal testing and shared with select customers as "engineering samples". These chips often have a minor design defect or two that will be corrected before production begins. Chuck Peddle's goal was to sell the first run 6501 and 6502 chips to the attendees at the Wescon trade show in San Francisco beginning on September 16, 1975. Peddle was a very effective spokesman and the MOS Technology microprocessors were extensively covered in the trade press. One of the earliest was a full-page story on the MCS6501 and MCS6502 microprocessors in the July 24, 1975 issue of "Electronics" magazine. Stories also ran in "EE Times" (August 24, 1975), "EDN" (September 20, 1975), "Electronic News" (November 3, 1975), "Byte" (November 1975) and "Microcomputer Digest" (November 1975). Advertisements for the 6501 appeared in several publications the first week of August 1975. The 6501 would be for sale at Wescon for $20 each. In September 1975, the advertisements included both the 6501 and the 6502 microprocessors. The 6502 would cost only $25.

When MOS Technology arrived at Wescon, they found that exhibitors were not permitted to sell anything on the show floor. They rented the MacArthur Suite at the St. Francis Hotel and directed customers there to purchase the processors. At the suite, the processors were stored in large jars to imply that the chips were in production and readily available. The customers did not know the bottom half of each jar contained non-functional chips. The chips were $20 and $25 while the documentation package was an additional $10. Users were encouraged to make photocopies of the documents, an inexpensive way for MOS Technology to distribute product information. The processors were supposed to have 56 instructions, but the Rotate Right (ROR) instruction did not work correctly on these chips, so the preliminary data sheets listed just 55 instructions. The reviews in "Byte" and "EDN" noted the lack of the ROR instruction. The next revision of the layout fixed this problem and the May 1976 datasheet listed 56 instructions. Peddle wanted every interested engineer and hobbyist to have access to the chips and documentation; other semiconductor companies only wanted to deal with "serious" customers. For example, Signetics was introducing the 2650 microprocessor and its advertisements asked readers to write for information on their company letterhead.

The 6501/6502 introduction in print and at Wescon was an enormous success. The downside was that the extensive press coverage got Motorola's attention. In October 1975, Motorola reduced the price of a single 6800 microprocessor from $175 to $69. The $300 system design kit was reduced to $150 and it now came with a printed circuit board. On November 3, 1975, Motorola sought an injunction in Federal Court to stop MOS Technology from making and selling microprocessor products. They also filed a lawsuit claiming patent infringement and misappropriation of trade secrets. Motorola claimed that seven former employees joined MOS Technology to create that company's microprocessor products.

Motorola was a billion-dollar company with a plausible case and lawyers. On October 30, 1974, Motorola had filed numerous patent applications on the microprocessor family and was granted twenty-five patents. The first was in June 1976 and the second was to Bill Mensch on July 6, 1976, for the 6820 PIA chip layout. These patents covered the 6800 bus and how the peripheral chips interfaced with the microprocessor. Motorola began making transistors in 1950 and had a portfolio of semiconductor patents. Allen-Bradley decided not to fight this case and sold their interest in MOS Technology back to the founders. Four of the former Motorola engineers were named in the suit: Chuck Peddle, Will Mathys, Bill Mensch and Rod Orgill. All were named inventors in the 6800 patent applications. During the discovery process, Motorola found that one engineer, Mike Janes, had ignored Peddle's instructions and brought his 6800 design documents to MOS Technology. In March 1976, the now independent MOS Technology was running out of money and had to settle the case. They agreed to drop the 6501 processor, pay Motorola $200,000 and return the documents that Motorola contended were confidential. Both companies agreed to cross-license microprocessor patents. That May, Motorola dropped the price of a single 6800 microprocessor to $35. By November, Commodore had acquired MOS Technology.

With legal troubles behind them, MOS was still left with the problem of getting developers to try their processor, prompting Chuck Peddle to design the MDT-650 ("microcomputer development terminal") single-board computer. Another group inside the company designed the KIM-1, which was sold semi-complete and could be turned into a usable system with the addition of a 3rd party computer terminal and compact cassette drive. Much to their amazement, the KIM-1 sold well to hobbyists and tinkerers, as well as to the engineers to which it had been targeted. The related Rockwell AIM 65 control/training/development system also did well. The software in the AIM 65 was based on that in the MDT. Another roughly similar product was the Synertek SYM-1.

One of the first "public" uses for the design was the Apple I microcomputer, introduced in 1976. The 6502 was next used in the Commodore PET and the Apple II, both released in 1977. It was later used in the Atari 8-bit family and Acorn Atom home computers, the BBC Micro, Commodore VIC-20 and other designs both for home computers and business, such as Ohio Scientific and Oric. The 6510, a direct successor of the 6502 with a digital I/O port and a tri-state address bus, was the CPU utilized in the best-selling Commodore 64 home computer. Commodore's floppy disk drive, the 1541, had a processor of its own—it too was a 6502.

Another important use of the 6500 family was in video games. The first to make use of the processor design was the Atari VCS, later renamed the Atari 2600. The VCS used an offshoot of the 6502 called the 6507, which had fewer pins and, as a result, could address only 8 KB of memory. Millions of the Atari consoles would be sold, each with a MOS processor. Another significant use was by the Nintendo Entertainment System and Famicom. The 6502 used in the NES was a second source version by Ricoh, a partial system-on-a-chip, that lacked the binary-coded decimal mode but added 22 memory-mapped registers and on-die hardware for sound generation, joypad reading, and sprite list DMA. Called 2A03 in NTSC consoles and 2A07 in PAL consoles (the difference being the memory divider ratio and a lookup table for audio sample rates), this processor was produced exclusively for Nintendo. The Atari Lynx used a 4 MHz version of the chip, the 65SC02.

In the 1980s, a popular electronics magazine Elektor/Elektuur used the processor in its microprocessor development board Junior Computer.

The 6502 is a little-endian 8-bit processor with a 16-bit address bus. The original versions were fabricated using an process technology chip with an advertised die size of (), or an area of 16.6 mm.

The internal logic runs at the same speed as the external clock rate, but despite the low clock speeds (typically in the neighborhood of 1 to 2 MHz), the 6502's performance was competitive with other contemporary CPUs using significantly faster clocks. This is partly due to a simple state machine implemented by combinational (clockless) logic to a greater extent than in many other designs; the two-phase clock (supplying two synchronizations per cycle) can thereby control the whole "machine"-cycle directly. Typical instructions might take half as many cycles to complete on the 6502 than contemporary designs. Like most simple CPUs of the era, the dynamic NMOS 6502 chip is not sequenced by a microcode ROM but uses a PLA (which occupied about 15% of the chip area) for instruction decoding and sequencing. As in most 8-bit microprocessors, the chip does some limited overlapping of fetching and execution.

The low clock frequency moderated the speed requirement of memory and peripherals attached to the CPU, as only about 50% of the clock cycle was available for memory access (due to the asynchronous design, this fraction varied strongly among chip versions). This was critical at a time when affordable memory had access times in the range . The original NMOS 6502 was minimalistically engineered and efficiently manufactured, and therefore cheap—an important factor in getting design wins in the very price-sensitive game-console and home-computer markets.

Like its precursor, the 6800, the 6502 has very few registers. The 6502's registers include one 8-bit accumulator register (A), two 8-bit index registers (X and Y), 7 processor status flag bits (P), an 8-bit stack pointer (S), and a 16-bit program counter (PC). This compares to a typical design of the same era, the Z80, which has eight general-purpose 8-bit registers, which can be combined into four 16-bit ones. The Z80 also had a complete set of alternate registers which made a total of sixteen general-purpose registers.
In order to make up somewhat for the lack of registers, the 6502 included a "zero-page" addressing mode that uses one address byte in the instruction instead of the two needed to address the full 64 KB of memory. This provides fast access to the first 256 bytes of RAM by using shorter instructions. Chuck Peddle has said in interviews that the specific intention was to allow these first 256 bytes of RAM to be used like registers.

The stack address space is to memory page $01, i.e. the address range $0100–$01FF (256–511). Software access to the stack is done via four implied addressing mode instructions, whose functions are to push or pop (pull) the accumulator or the processor status register. The same stack is also used for subroutine calls via the JSR (jump to subroutine) and RTS (return from subroutine) instructions and for interrupt handling.

The chip uses the index and stack registers effectively with several addressing modes, including a fast "direct page" or "zero page" mode, similar to that found on the PDP-8, that accesses memory locations from addresses 0 to 255 with a single 8-bit address (saving the cycle normally required to fetch the high-order byte of the address)—code for the 6502 uses the zero page much as code for other processors would use registers. On some 6502-based microcomputers with an operating system, the operating system uses most of zero page, leaving only a handful of locations for the user.

Addressing modes also include "implied" (1-byte instructions); "absolute" (3 bytes); "indexed absolute" (3 bytes); "indexed zero-page" (2 bytes); "relative" (2 bytes); "accumulator" (1); "indirect,x" and "indirect,y" (2); and "immediate" (2). Absolute mode is a general-purpose mode. Branch instructions use a signed 8-bit offset relative to the instruction after the branch; the numerical range −128..127 therefore translates to 128 bytes backward and 127 bytes forward from the instruction following the branch (which is 126 bytes backward and 129 bytes forward from the start of the branch instruction). Accumulator mode uses the accumulator as an effective address and does not need any operand data. Immediate mode uses an 8-bit literal operand.

The indirect modes are useful for array processing and other looping. With the 5/6 cycle "(indirect),y" mode, the 8-bit Y register is added to a 16-bit base address read from zero page, which is located by a single byte following the opcode. The Y register is therefore an "index" register in the sense that it is used to hold an actual "index" (as opposed to the X register in the 6800, where a base address was directly stored and to which an immediate offset could be added). Incrementing the index register to walk the array byte-wise takes only two additional cycles. With the less frequently used "(indirect,x)" mode the effective address for the operation is found at the zero page address formed by adding the second byte of the instruction to the contents of the X register. Using the indexed modes, the zero page effectively acts as a set of up to 128 additional (though very slow) address registers.

The 6502 is capable of performing addition and subtraction in binary or binary-coded decimal. Placing the CPU into BCD mode with the codice_1 (set D flag) instruction results in decimal arithmetic, in which codice_2 would result in $00 and the carry (C) flag being set. In binary mode (codice_3, clear D flag), the same operation would result in $9A and the carry flag being cleared. Other than Atari BASIC, BCD mode was seldom used in home-computer applications.

See the article for a simple but characteristic example of 6502 assembly language.

6502 instruction operation codes ("opcodes") are eight-bits long and have the general form aaabbbcc, where aaa and cc define the opcode, and bbb defines the addressing mode.

For instance, consider the codice_4 instruction, which performs a bitwise OR on the bits in the accumulator with another value. The instruction opcode is of the form 000bbb01, where bbb may be 010 for an immediate mode value (constant), 001 for zero page fixed address, 011 for an absolute address, and so on.

This pattern is not absolute and there are a number of exceptions. However, where it does apply it allows one to easily deconstruct opcode values back to assembler mnemonics for the majority of instructions, handling the edge cases with special-purpose code.

Of the 256 possible opcodes available using an 8-bit pattern, the original 6502 uses 151 of them, organized into 56 instructions with (possibly) multiple addressing modes. Depending on the instruction and addressing mode, the opcode may require zero, one or two additional bytes for operands. Hence 6502 machine instructions vary in length from one to three bytes. The operand is stored in the 6502's customary little-endian format.

The 65C816, the 16-bit CMOS descendant of the 6502, also supports 24-bit addressing, which results in instructions being assembled with three-byte operands, also arranged in little-endian format.

The remaining 105 opcodes are undefined. In the original design, instructions where the low-order 4-bits ("nibble") were 3, 7, B or F were not used, providing room for future expansion. Likewise, the $2xxx column had only a single entry, codice_5. The remaining 25 empty slots were distributed. Some of the empty slots were used in the 65C02 to provide both new instructions as well as variations on existing ones with new addressing modes. The $Fxxx instructions were initially left free to allow 3rd party vendors to add their own instructions, but later versions of the 65C02 standardized a set of bit fiddling instructions developed by Rockwell Semiconductor.

A 6502 assembly language statement consists of a three-character instruction mnemonic, followed by any operands. Instructions that do not take a separate operand but target a single register based on the addressing mode combine the target register in the instruction mnemonic, so the assembler uses codice_6 as opposed to codice_7 to increment the X register.

The processor's non-maskable interrupt (NMI) input is edge sensitive, which means that the interrupt is triggered by the falling edge of the signal rather than its level. The implication of this feature is that a wired-OR interrupt circuit is not readily supported. However, this also prevents nested NMI interrupts from occurring until the hardware makes the NMI input inactive again, often under control of the NMI interrupt handler.

The simultaneous assertion of the NMI and IRQ (maskable) hardware interrupt lines causes IRQ to be ignored. However, if the IRQ line remains asserted after the servicing of the NMI, the processor will immediately respond to IRQ, as IRQ is level sensitive. Thus a sort of built-in interrupt priority was established in the 6502 design.

The "Break" flag of the processor is very different from the other flag bits. It has no flag setting, resetting, or testing instructions of its own, and is not affected by the PHP and PLP instructions. It exists only on the stack, where BRK and PHP always write a 1, while IRQ and NMI always write a 0.

The "SO" input pin, when asserted, will set the processor's overflow status bit (deasserting it does not clear the overflow bit, however). This can be used by a high-speed polling device driver, which can poll the hardware once in only three cycles by using a Branch-on-oVerflow-Clear (codice_8) instruction that branches to itself. For example, the Commodore 1541 and other Commodore floppy disk drives use this technique to detect without delay whether the serializer is ready to accept or provide another byte of disk data. Obviously great care must be used in the device driver and the associated system design, as spurious assertion of the overflow bit could ruin arithmetic processing.

There were several variants of the NMOS 6502:


The MOS Technology 6512, 6513, 6514, and 6515 each rely on an external clock, instead of using an internal clock generator like the 650x (e.g. 6502). This was used to advantage in some designs where the clocks could be run asymmetrically, increasing overall CPU performance.

The 6512 was used in the BBC Micro B+64.

The Western Design Center designed and currently produces the W65C816S processor, a 16-bit, static-core successor to the 65C02, with greatly enhanced features. The W65C816S is a newer variant of the 65C816, which was the core of the Apple II computer and was the basis of the Ricoh 5A22 processor that powered the popular Super Nintendo Entertainment System. The W65C816S incorporates minor improvements over the 65C816 that make the newer chip not an exact hardware-compatible replacement for the earlier one. Currently available through electronics distributors as of September 2019, the W65C816S is rated for 14-megahertz operation.

The Western Design Center also designed and produced the 65C802, which was a 65C816 core with a 64-kilobyte address space in a 65(C)02 pin-compatible package. The 65C802 could be retrofitted to a 6502 board and would function as a 65C02 on power-up, operating in "emulation mode." As with the 65C816, a two-instruction sequence would switch the 65C802 to "native mode" operation, exposing its 16 bit accumulator and index registers, as well as other 65C816 enhanced features. The 65C802 was not widely used: new designs almost always were built around the 65C816, resulting in 65C802 production being discontinued.

The following 6502 assembly language source code is for a subroutine named codice_10, which copies a null-terminated character string from one location to another, converting upper-case letter characters to lower-case letters. The string being copied is the "source", and the string into which the converted source is stored is the "destination".

The 6502 had several bugs and quirks, which had to be accounted for when programming it.






</doc>
<doc id="20298" url="https://en.wikipedia.org/wiki?curid=20298" title="MOS Technology 65xx">
MOS Technology 65xx

The MOS Technology 65xx series is a family of 8-bit microprocessors from MOS Technology, based on the Motorola 6800 (introduced ca. 1975). The 65xx family included the 6502, used in home computers such as the Commodore PET and VIC-20, the Apple II, the Atari 800, and the British BBC Micro.

The 6501 and 6502 have 40-pin DIP packages; the 6503, 6504, 6505, and 6507 are 28-pin DIP versions, for reduced chip and circuit board cost. In all of the 28-pin versions, the pin count is reduced by leaving off some of the high-order address pins and various combinations of function pins, making those functions unavailable.

Typically, the 12 pins omitted are the three not connected (NC) pins, one of the two Vss pins, one of the clock pins, the SYNC pin, the set overflow (SO) pin, either the maskable interrupt or the non-maskable interrupt (NMI), and the four most-significant address lines (A12–A15) are the 12 pins omitted to reduce the pin count from 40 to 28. The omission of four address pins reduces the external addressability to 4 KB (from the 64 KB of the 6502), though the internal PC register and all effective address calculations remain 16-bit.

The 6507 omits both interrupt pins in order to include address line A12, providing 8 KB of external addressability but no interrupt capability. The 6507 was used in the popular Atari 2600 video game console, the design of which divides the 8 KB memory space in half, allocating the lower half to the console's internal RAM and peripherals, and the upper half to the Game Cartridge™, so Atari 2600 cartridges have a 4 KB address limit (and the same capacity limit unless the cartridge contains bank switching circuitry).

One popular 6502 based computer, the Commodore 64, used a modified 6502 CPU, the 6510. Unlike the 6503–6505 and 6507, the 6510 is a 40-pin chip that adds internal hardware: an 8-bit parallel I/O port mapped to addresses 0000 and 0001. The 6508 is another chip that, like the 6510, adds internal hardware: 256 bytes of SRAM and the same 8-bit I/O port featured by the 6510. Though these chips do not have reduced pin counts compared to the 6502, they need 8 new pins for the added parallel I/O port. In this case, no address lines are among the 8 removed pins.



</doc>
<doc id="20299" url="https://en.wikipedia.org/wiki?curid=20299" title="MOS Technology 6510">
MOS Technology 6510

The MOS Technology 6510 is an 8-bit microprocessor designed by MOS Technology. It is a modified form of the very successful 6502. The 6510 was only widely used in the Commodore 64 home computer and its variants.

The primary change from the 6502 was the addition of an 8-bit general purpose I/O port, although only six I/O pins were available in the most common version of the 6510. In addition, the address bus could be made tristate.

In the C64, the extra I/O pins of the processor were used to control the computer's memory map by bank switching, and for controlling three of the four signal lines of the Datasette tape recorder (the electric motor control, key-press sensing and write data lines; the read data line went to another I/O chip). It was possible, by writing the correct bit pattern to the processor at address $01, to completely expose almost the full 64 KB of RAM in the C64, leaving no ROM or I/O hardware exposed except for the processor I/O port itself and its data directional register.

In 1985, MOS produced the 8500, an HMOS version of the 6510. Other than the process modification, it is virtually identical to the NMOS version of the 6510. The 8500 was originally designed for use in the modernised C64, the C64C. However, in 1985, limited quantities of 8500s were found on older NMOS-based C64s. It finally made its official debut in 1987, appearing in a motherboard using the new 85xx HMOS chipset.

The 7501/8501 variant of the 6510 was introduced in 1984. Compared to the 6510, this variant extends the number of I/O port pins from 6 to 8, but omits the pins for non-maskable interrupt and clock output.. It was used in Commodore's C16, C116 and Plus/4 home computers, where its I/O port controlled not only the Datasette but also the CBM Bus interface. The main difference between 7501 and 8501 CPUs is that they were manufactured with slightly different processes: 7501 was manufactured with HMOS-1 and 8501 with HMOS-2.

The 2 MHz-capable 8502 variant was used in the Commodore 128. All these CPUs are opcode compatible (including undocumented opcodes).

The Commodore 1551 disk drive used the 6510T, a version of the 6510 with eight I/O lines. The NMI and RDY signals are not available.




</doc>
