<doc id="19692" url="https://en.wikipedia.org/wiki?curid=19692" title="Monopoly (game)">
Monopoly (game)

Monopoly is a board game currently published by Hasbro. In the game, players roll two six-sided dice to move around the game board, buying and trading properties, and developing them with houses and hotels. Players collect rent from their opponents, with the goal being to drive them into bankruptcy. Money can also be gained or lost through Chance and Community Chest cards, and tax squares; players can end up in jail, which they cannot move from until they have met one of several conditions. The game has numerous house rules, and hundreds of different editions exist, as well as many spin-offs and related media. "Monopoly" has become a part of international popular culture, having been licensed locally in more than 103 countries and printed in more than 37 languages.

"Monopoly" is derived from "The Landlord's Game" created by Lizzie Magie in the United States in 1903 as a way to demonstrate that an economy which rewards wealth creation is better than one where monopolists work under few constraints, and to promote the economic theories of Henry George—in particular his ideas about taxation. It was first published by Parker Brothers in 1935 until that firm was eventually absorbed into Hasbro in 1991. The game is named after the economic concept of monopoly—the domination of a market by a single entity.

The history of "Monopoly" can be traced back to 1903, when American anti-monopolist Lizzie Magie created a game which she hoped would explain the single tax theory of Henry George. It was intended as an educational tool to illustrate the negative aspects of concentrating land in private monopolies. She took out a patent in 1904. Her game, "The Landlord's Game", was self-published, beginning in 1906.

Magie created two sets of rules: an anti-monopolist set in which all were rewarded when wealth was created, and a monopolist set in which the goal was to create monopolies and crush opponents.

Several variant board games, based on her concept, were developed from 1906 through the 1930s; they involved both the process of buying land for its development and the sale of any undeveloped property. Cardboard houses were added and rents increased as they were added to a property. Magie patented the game again in 1923.

According to an advertisement placed in "The Christian Science Monitor", Charles Todd of Philadelphia recalled the day in 1932 when his childhood friend, Esther Jones, and her husband Charles Darrow came to their house for dinner. After the meal, the Todds introduced Darrow to "The Landlord's Game", which they then played several times. The game was entirely new to Darrow, and he asked the Todds for a written set of the rules. After that night, Darrow went on to utilize this and distribute the game himself as "Monopoly". Because of this act the Todds refused to speak to Darrow ever again.

Parker Brothers bought the game's copyrights from Darrow. When the company learned Darrow was not the sole inventor of the game, it bought the rights to Magie's patent for just $500.

Parker Brothers began selling the game on February 6, 1935. Cartoonist F. O. Alexander contributed the design. U. S. patent number US 2026082 A was issued to Charles Darrow on December 31, 1935, for the game board design and was assigned to Parker Brothers Inc. The original version of the game in this format was based on the streets of Atlantic City, New Jersey.

In 1936, Parker Brothers began licensing the game for sale outside the United States. In 1941, the British Secret Intelligence Service had John Waddington Ltd., the licensed manufacturer of the game in the United Kingdom, create a special edition for World War II prisoners of war held by the Nazis. Hidden inside these games were maps, compasses, real money, and other objects useful for escaping. They were distributed to prisoners by fake charity organizations created by the British Secret Service.

In the Nazi-occupied Netherlands, the German government and its collaborators were displeased with Dutch people using Monopoly Game sets with American or British locales, and developed a version with Dutch locations. Since that version had in itself no specific pro-Nazi elements, it continued in use after the war, and formed the base for Monopoly games used in the Netherlands up to the present.

Economics professor Ralph Anspach published a game "Anti-Monopoly" in 1973, and was sued for trademark infringement by Parker Brothers in 1974. The case went to trial in 1976. Anspach won on appeals in 1979, as the 9th Circuit Court determined that the trademark "Monopoly" was generic and therefore unenforceable. The United States Supreme Court declined to hear the case, allowing the appellate court ruling to stand. This decision was overturned by the passage of Public Law 98-620 in 1984. With that law in place, Parker Brothers and its parent company, Hasbro, continue to hold valid trademarks for the game "Monopoly". However, "Anti-Monopoly" was exempted from the law and Anspach later reached a settlement with Hasbro and markets his game under license from them.

The research that Anspach conducted during the course of the litigation was what helped bring the game's history before Charles Darrow into the spotlight.

In 1991, Hasbro acquired Parker Bros. and thus "Monopoly". Before the Hasbro acquisition, Parker Bros. acted as a publisher only issuing two versions at a time, a regular and deluxe. Hasbro moved to create and license other versions and involve the public in varying the game. A new wave of licensed products began in 1994, when Hasbro granted a license to USAopoly to begin publishing a San Diego Edition of "Monopoly", which has since been followed by over a hundred more licensees including Winning Moves Games (since 1995) and Winning Solutions, Inc. (since 2000) in the United States.

In 2003, the company held a national tournament on a chartered train going from Chicago to Atlantic City (see ). Also in 2003, Hasbro sued the maker of Ghettopoly and won. In February 2005, the company sued RADGames over their Super Add-On accessory board game that fit in the center of the board. The judge initially issued an injunction on February 25, 2005, to halt production and sales before ruling in RADGames' favor in April 2005.

In 2008, the Speed Die was added to all regular Monopoly set. After polling their Facebook followers, Hasbro Gaming took the top house rules and added them to a House Rule Edition released in the Fall of 2014 and added them as optional rules in 2015. In January 2017, Hasbro invited Internet users to vote on a new set of game pieces, with this new regular edition to be issued in March 2017.

On May 1, 2018, the Monopoly Mansion hotel agreement was announced by Hasbro's managing director for South-East Asia, Hong Kong and Taiwan, Jenny Chew Yean Nee with M101 Holdings Sdn Bhd. M101 has the five-star, 225-room hotel, then under construction, located at the M101 Bukit Bintang in Kuala Lumpur and would have a 1920s Gatsby feel. M101's Sirocco Group would manage the hotel when it opens in 2019.

The "Monopoly" game-board consists of forty spaces containing twenty-eight properties—twenty-two streets (grouped into eight color groups), four railroads, and two utilities—three Chance spaces, three Community Chest spaces, a Luxury Tax space, an Income Tax space, and the four corner squares: GO, (In) Jail/Just Visiting, Free Parking, and Go to Jail.

There have since been some changes to the board. Not all of the Chance and Community Chest cards as shown in the 1935 patent were used in editions from 1936/1937 onwards. Graphics with the Mr. Monopoly character (then known as "Rich Uncle Pennybags") were added in that same time-frame. A graphic of a chest containing coins was added to the Community Chest spaces, as were the flat purchase prices of the properties. Traditionally, the Community Chest cards were yellow (although they were sometimes printed on blue stock) with no decoration or text on the back; the Chance cards were orange with no text or decoration on the back.

Hasbro commissioned a major graphic redesign to the U.S. Standard Edition of the game in 2008 along with some minor revisions. Among the changes: the colors of Mediterranean and Baltic Avenues changed from purple to brown, and the colors of the GO square changed from red to black. A flat $200 Income Tax was imposed (formerly the player's choice of $200 or 10% of their total holdings, which they could not calculate until after making their final decision). Originally the amount was $300 but was changed a year after the game's debut, and the Luxury Tax amount increased to $100 from $75. There were also changes to the Chance and Community Chest cards; for example, the "poor tax" and "grand opera opening" cards became "speeding fine" and "it is your birthday", respectively; though their effects remained the same; the player must pay only $50 instead of $150 for the school tax. In addition, a player now gets $50 instead of $45 for sale of stock, and the Advance to Illinois Avenue card now has the added text indicating a player collects $200 if they pass Go on the way there.
All the Chance and Community Chest cards received a graphic upgrade in 2008 as part of the graphic refresh of the game. Mr. Monopoly's classic line illustration was also now usually replaced by renderings of a 3D Mr. Monopoly model. The backs of the cards have their respective symbols, with Community Chest cards in blue, and Chance cards in orange.

Additionally, recent versions of "Monopoly" replace the dollar sign ($) with an M with two horizontal strokes through it.

In the U.S. versions shown below, the properties are named after locations in (or near) Atlantic City, New Jersey.
Atlantic City's Illinois Avenue was renamed Martin Luther King Jr. Blvd. in the 1980s. St. Charles Place no longer exists, as the Showboat Atlantic City was developed where it once ran.

Different versions have been created based on various current consumer interests such as: "Dog-opoly", "Cato-poly", "Bug-opoly", and TV/movie games among others.

It was not until 1995 that Parker Brothers acknowledged the misspelling of "Marvin Gardens", formally apologizing to the residents of Marven Gardens.

Short Line refers to the Shore Fast Line, a streetcar line that served Atlantic City. The B&O Railroad did not serve Atlantic City. A booklet included with the reprinted 1935 edition states that the four railroads that served Atlantic City in the mid-1930s were the Jersey Central, the Seashore Lines, the Reading Railroad, and the Pennsylvania Railroad.

The Baltimore & Ohio (now part of CSX) was the parent of the Reading. There is a tunnel in Philadelphia where track to the south was B. & O. and track to the north is Reading. The Central of N.J. did not have a track to Atlantic City but was the daughter of the Reading (and granddaughter of the B. & O.) Their track ran from the New York City area to Delaware Bay and some trains ran on the Reading-controlled track to Atlantic City.

The actual "Electric Company" and "Water Works" serving the city are respectively Atlantic City Electric Company (a subsidiary of Exelon) and the Atlantic City Municipal Utilities Authority.

In the 1930s, John Waddington Ltd. (Waddingtons) was a printing company in Leeds that had begun to branch out into packaging and the production of playing cards. Waddingtons had sent the card game "Lexicon" to Parker Brothers hoping to interest them in publishing the game in the United States. In a similar fashion, Parker Brothers sent over a copy of "Monopoly" to Waddingtons early in 1935 before the game had been put into production in the United States.

Victor Watson, the managing director of Waddingtons, gave the game to his son Norman, head of the card games division, to test over the weekend. Norman was impressed by the game and persuaded his father to call Parker Brothers on Monday morning – transatlantic calls then being almost unheard of. This call resulted in Waddingtons obtaining a license to produce and market the game outside the United States. 

Watson felt that for the game to be a success in the United Kingdom, the American locations would have to be replaced, so Victor and his secretary, Marjory Phillips, went to London to scout out locations. The Angel, Islington is not a street in London but a building (and the name of the road intersection where it is located). It had been a coaching inn that stood on the Great North Road. By the 1930s, the inn had become a J. Lyons and Co. tea room (today The Co-operative Bank). Some accounts say that Marjory and Victor met at the Angel to discuss the selection and celebrated the fact by including it on the "Monopoly" board. In 2003, a plaque commemorating the naming was unveiled at the site by Victor Watson's grandson, who is also named Victor.

During World War II, the British Secret Service contacted Waddington (who could also print on silk) to make "Monopoly" sets that included escape maps, money, a compass and file, all hidden in copies of the game sent by fake POW relief charities to prisoners of war.

The standard British board, produced by Waddingtons, was for many years the version most familiar to people in countries in the Commonwealth (except Canada, where the U.S. edition with Atlantic City-area names was reprinted), although local variants of the board are now also found in several of these countries.

In 1998, Winning Moves procured the "Monopoly" license from Hasbro and created new UK city and regional editions with sponsored squares. Initially, in December 1998, the game was sold in just a few W H Smith stores, but demand was high, with almost fifty thousand games shipped in the four weeks leading to Christmas. Winning Moves still produces new annually.

The original income tax choice from the 1930s U.S. board is replaced by a flat rate on the UK board, and the $75 Luxury Tax space is replaced with the £100 Super Tax space, the same as the current German board. In 2008, the U.S. Edition was changed to match the UK and various European editions, including a flat $200 Income Tax value and an increased $100 Luxury Tax amount.

In cases where a national company produced the game, the $ (dollar) sign replaced the £ (pound), but the place names were unchanged.

Beginning in the U.K. in 2005, a revised version of the game, titled "Monopoly Here and Now", was produced, replacing game scenarios, properties, and tokens with newer equivalents. Similar boards were produced for Germany and France. Variants of these first editions appeared with Visa-branded debit cards taking the place of cash – the later U.S. "Electronic Banking" edition has unbranded debit cards.

The success of the first "Here and Now" editions prompted Hasbro U.S. to allow online voting for twenty-six landmark properties across the United States to take their places along the game-board. The popularity of this voting, in turn, led to the creation of similar websites, and secondary game-boards per popular vote to be created in the U.K., Canada, France, Germany, Australia, New Zealand, Ireland, and other nations.

In 2006, Winning Moves Games released the "", with a 30% larger game-board and revised game play. Other streets from Atlantic City (eight, one per color group) were included, along with a third "utility", the Gas Company. In addition, $1,000 denomination notes (first seen in Winning Moves' "Monopoly: The Card Game") are included. Game play is further changed with bus tickets (allowing non-dice-roll movement along one side of the board), a speed die (itself adopted into variants of the "Atlantic City standard edition"; see below), skyscrapers (after houses and hotels), and train depots that can be placed on the Railroad spaces.

This edition was adapted for the U.K. market in 2007, and is sold by Winning Moves U.K. After the initial U.S. release, critiques of some of the rules caused the company to issue revisions and clarifications on their website.

In September 2006, the U.S. edition of "Monopoly Here and Now" was released. This edition features top landmarks across the U.S. The properties were decided by votes over the Internet in the spring of 2006.

Monetary values are multiplied by 10,000 (e.g., one collects $2,000,000 instead of $200 for passing GO and pays that much for Income Tax (or 10% of their total, as this edition was launched prior to 2008), each player starts with $15,000,000 instead of $1,500, etc.). Also, the Chance and Community Chest cards are updated, the Railroads are replaced by Airports (Chicago O'Hare, Los Angeles International, New York City's JFK, and Atlanta's Hartsfield-Jackson), and the Utilities (Electric Company and Water Works) are replaced by Service Providers (Internet Service Provider and Cell Phone Service Provider). The houses and hotels are blue and silver, not green and red as in most editions of "Monopoly". The board uses the traditional U.S. layout; the cheapest properties are purple, not brown, and "Interest on Credit Card Debt" replaces "Luxury Tax". 

Despite the updated Luxury Tax space, and the Income Tax space no longer using the 10% option, this edition uses paper "Monopoly" money, and not an electronic banking unit like the "Here and Now World Edition". However, a similar edition of "Monopoly", the "Electronic Banking" edition, does feature an electronic banking unit and bank cards, as well as a different set of tokens. Both "Here and Now" and "Electronic Banking" feature an updated set of tokens from the Atlantic City edition.

It is also notable that three states (California, Florida, and Texas) are represented by two cities each (Los Angeles and San Francisco, Miami and Orlando, and Dallas and Houston). No other state is represented by more than one city (not including the airports). One landmark, Texas Stadium, has been demolished and no longer exists. Another landmark, Jacobs Field, still exists, but was renamed Progressive Field in 2008.

In 2015, in honor of the game's 80th birthday, Hasbro held an online vote to determine which cities would make it into an updated version of the Here and Now edition of the game. This second edition is more a spin-off as the winning condition has changed to completing your passport instead of bankrupting your opponents. Community Chest is replaced with Here and Now cards while the Here and Now space replaced the railroads. Houses and hotels have been removed.

Hasbro released a World edition with the top voted cities from all around the world, as well as at least a Here & Now edition with the voted-on U.S. cities.

"Monopoly Empire" has uniquely branded tokens and places based on popular brands. Instead of buying properties, players buy popular brands one by one and slide their billboards onto their Empire towers. Instead of building houses and hotels, players collect rent from their rivals based on their tower height. How a player wins is by being the first player to fill his or her tower with billboards. Every space on the board is a brand name, including Xbox, Coca-Cola, McDonald's and Samsung.

Monopoly Token Madness

This version of Monopoly contains an extra eight "golden" tokens. That includes a penguin, a television, a race car, a Mr. Monopoly emoji, a rubber duck, a watch, a wheel and a bunny slipper.

Monopoly Jackpot

During the game, players travel around the gameboard buying properties and collecting rent. If they land on a Chance space, or roll the Chance icon on a die, they can spin the Chance spinner to try to make more money. Players may hit the "Jackpot", go bankrupt, or be sent to Jail. The player who has the most cash when the bank crashes wins.

Monopoly: Ultimate Banking Edition

In this version, there is no cash. The Monopoly Ultimate Banking game features an electronic ultimate banking piece with touch technology. Players can buy properties instantly and set rents by tapping. Each player has a bankcard and their cash is tracked by the Ultimate Banking unit. It can scan the game's property cards and boost or crash the market. Event cards and Location spaces replace Chance and Community Chest cards. On an Event Space, rents may be raised or lowered, a player may earn or lose money, or someone could be sent to Jail. Location Spaces allow players to pay and move to any property space on the gameboard.

Monopoly Voice Banking

In this version, there's no cash or cards. The Voice Banking game allows the player to respond with your voice with the Top Hat. The hat responds by purchasing properties, paying rent, and making buildings.

"Ms. Monopoly" is a version of the game released in 2019, in which female players earn more than male players.

All property deeds, houses, and hotels are held by the bank until bought by the players. A standard set of "Monopoly" pieces includes:

A deck of thirty-two Chance and Community Chest cards (sixteen each) which players draw when they land on the corresponding squares of the track, and follow the instructions printed on them.

A title deed for each property is given to a player to signify ownership, and specifies purchase price, mortgage value, the cost of building houses and hotels on that property, and the various rents depending on how developed the property is. Properties include:

The purchase price for properties varies from $60 to $400 on a U.S. Standard Edition set.

A pair of six-sided dice is included, with a "Speed Die" added for variation in 2007. The 1999 Millennium Edition featured two jewel-like dice which were the subject of a lawsuit from Michael Bowling, owner of dice maker Crystal Caste. Hasbro lost the suit in 2008 and had to pay $446,182 in royalties. Subsequent printings of the game reverted to normal six-sided dice.

32 houses and 12 hotels made of wood or plastic (the original and current "Deluxe Edition" have wooden houses and hotels; the current "base set" uses plastic buildings). Unlike money, houses and hotels have a finite supply. If no more are available, no substitute is allowed. In most editions, houses are green and hotels red.

Older U.S. standard editions of the game included a total of $15,140 in the following denominations:

Newer (September 2008 and later) U.S. editions provide a total of $20,580–30 of each denomination instead. The colors of some of the bills are also changed: $10s are now blue instead of yellow, $20s are a brighter green than before, and $50s are now purple instead of blue.

Each player begins the game with his or her token on the Go square, and $1,500 (or 1,500 of a localized currency) in play money ($2,500 with the Speed Die). Before September 2008, the money was divided with greater numbers of 20 and 10-dollar bills. Since then, the U.S. version has taken on the British version's initial cash distributions.

Although the U.S. version is indicated as allowing eight players, the cash distribution shown above is not possible with all eight players since it requires 32 $100 bills and 40 $1 bills. However, the amount of cash contained in the game is enough for eight players with a slight alteration of bill distribution.

Pre-Euro German editions of the game started with 30,000 "Spielmark" in eight denominations (abbreviated as "M."), and later used seven denominations of the "Deutsche Mark" ("DM."). In the classic Italian game, each player received L. 350,000 ($3500) in a two-player game, but L. 50,000 ($500) less for each player more than two. Only in a six-player game does a player receive the equivalent of $1,500. The classic Italian games were played with only four denominations of currency. Both Spanish editions (the Barcelona and Madrid editions) started the game with 150,000 in play money, with a breakdown identical to that of the American version.

According to the Parker Brothers rules, Monopoly money is theoretically unlimited; if the bank runs out of money it may issue as much as needed "by merely writing on any ordinary paper".
However, Hasbro's published Monopoly rules make no mention of this. Additional paper money can be bought at certain locations, notably game and hobby stores, or downloaded from various websites and printed and cut by hand. One such site has created a $1,000 bill; while a $1,000 bill can be found in "" and "Monopoly: The Card Game", both published by Winning Moves Games, this note is not a standard denomination for "classic" versions of Monopoly.

In several countries there is also a version of the game that features electronic banking. Instead of receiving paper money, each player receives a plastic bank card that is inserted into a calculator-like electronic device that keeps track of the player's balance.

Each player is represented by a small metal or plastic token that is moved around the edge of the board according to the roll of two six-sided dice. The number of tokens (and the tokens themselves) have changed over the history of the game with many appearing in special editions only, and some available with non-game purchases. After prints with wood tokens in 1937, a set of eight tokens was introduced. Two more were added in late 1937, and tokens changed again in 1942. During World War II, the game tokens were switched back to wood. Early localized editions of the standard edition (including some Canadian editions, which used the U.S. board layout) did not include pewter tokens but instead had generic wooden pawns identical to those that "Sorry!" had. 

Many of the early tokens were created by companies such as Dowst Miniature Toy Company, which made metal charms and tokens designed to be used on charm bracelets. The battleship and cannon were also used briefly in the Parker Brothers war game "Conflict" (released in 1940), but after the game failed on the market, the premade pieces were recycled for "Monopoly" usage. By 1943, there were ten tokens which included the Battleship, Boot, Cannon, Horse and rider, Iron, Racecar, Scottie Dog, Thimble, Top hat, and Wheelbarrow. These tokens remained the same until the late 1990s, when Parker Brothers was sold to Hasbro.

In 1998, a Hasbro advertising campaign asked the public to vote on a new playing piece to be added to the set. The candidates were a "bag of money", a bi-plane, and a piggy bank. The bag ended up winning 51 percent of the vote compared to the other two which failed to go above 30%. This new token was added to the set in 1999 bringing the number of tokens to eleven. Another 1998 campaign poll asked people which monopoly token was their favorite. The most popular was the Race Car at 18% followed by the Dog (16%), Cannon (14%) and Top Hat (10%). The least favorite in the poll was the Wheelbarrow at 3% followed by Thimble (7%) and the Iron (7%). The "Cannon", and "Horse and rider" were both retired in 2000 with no new tokens taking their place. Another retirement came in 2007 with the sack of money that brought down the total token count to eight again.

In 2013, a similar promotional campaign was launched encouraging the public to vote on one of several possible new tokens to replace an existing one. The choices were a guitar, a diamond ring, a helicopter, a robot, and a cat. This new campaign was different than the one in 1998 as one piece was retired and replaced with a new one. Both were chosen by a vote that ran on Facebook from January 8 to February 5, 2013. The cat took the top spot with 31% of the vote over the iron which was replaced. In January 2017, Hasbro placed the line of tokens in the regular edition with another vote which included a total of 64 options. The eight playable tokens at the time included the Battleship, Boot, Cat, Racecar, Scottie Dog, Thimble, Top hat, and Wheelbarrow. By March 17, 2017, Hasbro retired three tokens which included the thimble, wheelbarrow, and boot, these were replaced by a penguin, a Tyrannosaurus and a rubber duck.

Over the years Hasbro has released tokens for special or collector's editions of the game. One of the first tokens to come out included a Steam Locomotive which was only released in Deluxe Editions. A Director's Chair token was released in 2011 in limited edition copies of "". Shortly after the 2013 Facebook voting campaign, a limited-edition Golden Token set was released exclusively at various national retailers, such as Target in the U.S., and Tesco in the U.K. 

The set contained the Battleship, Boot, Iron, Racecar, Scottie Dog, Thimble, Top hat and Wheelbarrow as well as the iron's potential replacements. These replacement tokens included the cat, the guitar, the diamond ring, the helicopter, and the robot. Hasbro released a 64-token limited edition set in 2017 called Monopoly Signature Token Collection to include all of the candidates that were not chosen in the vote held that year.

Players take turns in order with the initial player determined by chance before the game. A typical turn begins with the rolling of the dice and advancing a piece clockwise around the board the corresponding number of squares. If a player rolls doubles, they roll again after completing that portion of their turn. In some versions of the game a player who rolls three consecutive sets of doubles on one turn has been "caught speeding" and is immediately sent to jail instead of moving the amount shown on the dice for the third roll.

A player who lands on or passes the Go space collects $200 from the bank. Players who land on either Income Tax or Luxury Tax pay the indicated amount to the bank. In older editions of the game, two options were given for Income Tax: either pay a flat fee of $200 or 10% of total net worth (including the current values of all the properties and buildings owned). No calculation could be made before the choice, and no latitude was given for reversing an unwise calculation. In 2008, the calculation option was removed from the official rules, and simultaneously the Luxury Tax was increased to $100 from its original $75. No reward or penalty is given for landing on Free Parking.

Properties can only be developed once a player owns all the properties in that color group. They then must be developed equally. A house must be built on each property of that color before a second can be built. Each property within a group must be within one house level of all the others within that group.

If a player lands on a Chance or Community Chest space, they draw the top card from the respective deck and follow its instructions. This may include collecting or paying money to the bank or another player or moving to a different space on the board. Two types of cards that involve jail, "Go to Jail" and "Get Out of Jail Free", are explained below.

A player is sent to jail for doing any of the following:

When a player is sent to jail, they move directly to the Jail space and their turn ends ("Do not pass Go. Do not collect $200."). If an ordinary dice roll (not one of the above events) ends with the player's token on the Jail corner, they are "Just Visiting" and can move ahead on their next turn without incurring any penalty.

If a player is in jail, they do not take a normal turn and must either pay a fine of $50 to be released, use a Chance or Community Chest Get Out of Jail Free card, or attempt to roll doubles on the dice. If a player fails to roll doubles, they lose their turn. Failing to roll doubles for three consecutive turns requires the player to either pay the $50 fine or use a Get Out of Jail Free card, after which they move ahead according to the total rolled. Players in jail may not buy properties directly from the bank since they are unable to move. They can engage all other transactions, such as mortgaging properties, selling/trading properties to other players, buying/selling houses and hotels, collecting rent, and bidding on property auctions. A player who rolls doubles to leave jail does not roll again; however, if the player pays the fine or uses a card to get out and then rolls doubles, they do take another turn.

If the player lands on an unowned property, whether street, railroad, or utility, they can buy the property for its listed purchase price. If they decline this purchase, the property is auctioned off by the bank to the highest bidder, including the player who declined to buy. If the property landed on is already owned and unmortgaged, they must pay the owner a given rent; the amount depends on whether the property is part of a set or its level of development.

When a player owns all the properties in a color group and none of them are mortgaged, they may develop them during their turn or in between other player's turns. Development involves buying miniature houses or hotels from the bank and placing them on the property spaces; this must be done uniformly across the group. That is, a second house cannot be built on any property within a group until all of them have one house. Once the player owns an entire group, they can collect double rent for any undeveloped properties within it. Although houses and hotels cannot be built on railroads or utilities, the given rent increases if a player owns more than one of either type. If there is a housing shortage (more demand for houses to be built than what remains in the bank), then a housing auction is conducted to determine who will get to purchase each house.

Properties can also be mortgaged, although all developments on a monopoly must be sold before any property of that color can be mortgaged or traded. The player receives half the purchase price from the bank for each mortgaged property. This must be repaid with 10% interest to clear the mortgage. Houses and hotels can be sold back to the bank for half their purchase price. Players cannot collect rent on mortgaged properties and may not give improved property away to others; however, trading mortgaged properties is allowed. The player receiving the mortgaged property must immediately pay the bank the mortgage price plus 10% or pay just the 10% amount and keep the property mortgaged; if the player chooses the latter, they must pay the 10% again when they pay off the mortgage.

A player who cannot pay what they owe is bankrupt and eliminated from the game. If the bankrupt player owes the bank, they must turn all their assets over to the bank, who then auctions off their properties (if they have any), except buildings. If the debt is owed to another player instead, all assets are given to that opponent, except buildings which must be returned to the bank. The new owner must either pay off any mortgages held by the bank on such properties received or pay a fee of 10% of the mortgaged value to the bank if they choose to leave the properties mortgaged. The winner is the remaining player left after all of the others have gone bankrupt.

If a player runs out of money but still has assets that can be converted to cash, they can do so by selling buildings, mortgaging properties, or trading with other players. To avoid bankruptcy the player must be able to raise enough cash to pay the full amount owed.

A player cannot choose to go bankrupt; if there is any way to pay what they owe, even by returning all their buildings at a loss, mortgaging all their real estate and giving up all their cash, even knowing they are likely going bankrupt the next time, they must do so.

From 1936, the rules booklet included with each Monopoly set contained a short section at the end providing rules for making the game shorter, including dealing out two Title Deed cards to each player before starting the game, by setting a time limit or by ending the game after the second player goes bankrupt. A later version of the rules included this variant, along with the time limit game, in the main rules booklet, omitting the last, the second bankruptcy method, as a third short game.

Many house rules have emerged for the game since its creation. Well-known is the "Free Parking jackpot rule", where all the money collected from Income Tax, Luxury Tax, Chance and Community Chest goes to the center of the board instead of the bank. Many people add $500 to start each pile of Free Parking money, guaranteeing a minimum payout. When a player lands on Free Parking, they may take the money. 

Another rule is that if a player lands directly on Go, they collect double the amount, or $400, instead of $200. House rules that slow or prevent money being returned to the bank in this way may have a side effect of increasing the time it takes for players to become bankrupt, lengthening the game considerably, as well as decreasing the effects of strategy and prudent investment.

Video game and computer game versions of "Monopoly" have options where popular house rules can be used. In 2014, Hasbro determined five popular house rules by public Facebook vote, and released a "House Rules Edition" of the board game. Rules selected include a "Free Parking" house rule without additional money and forcing players to traverse the board once before buying properties.

According to Jim Slater in "The Mayfair Set", the Orange property group is the best to own because players land on them more often, as a result of the Chance cards "Go to Jail", "Advance to St. Charles Place (Pall Mall)", "Advance to Reading Railroad (Kings Cross Station)" and "Go Back Three Spaces".

In all, during game play, Illinois Avenue (Trafalgar Square) (Red), New York Avenue (Vine Street) (Orange), B&O Railroad (Fenchurch Street Station), and Reading Railroad (Kings Cross Station) are the most frequently landed-upon properties. Mediterranean Avenue (Old Kent Road) (brown), Baltic Avenue (Whitechapel Road) (brown), Park Place (Park Lane) (blue), and Oriental Avenue (The Angel Islington) (light blue) are the least-landed-upon properties. Among the property groups, the Railroads are most frequently landed upon, as no other group has four properties; Orange has the next highest frequency, followed by Red.

One common criticism of "Monopoly" is that although it has carefully defined termination conditions, it may take an unlimited amount of time to reach them. Edward P. Parker, a former president of Parker Brothers, is quoted as saying, "We always felt that forty-five minutes was about the right length for a game, but "Monopoly" could go on for hours. Also, a game was supposed to have a definite end somewhere. In "Monopoly" you kept going around and around."

Hasbro states that the longest game of "Monopoly" ever played lasted 1,680 hours (70 days or 10 weeks or 2.3 months).

Numerous add-ons have been produced for "Monopoly", sold independently from the game both before its commercialization and after, with three official ones discussed below:

The original "Stock Exchange" add-on was published by Capitol Novelty Co. of Rensselaer, New York in early 1936. It was marketed as an add-on for "Monopoly", "Finance", or "Easy Money" games. Shortly after Capitol Novelty introduced "Stock Exchange", Parker Brothers bought it from them then marketed their own, slightly redesigned, version as an add-on specifically for their "new" "Monopoly" game; the Parker Brothers version was available in June 1936. The Free Parking square is covered over by a new Stock Exchange space and the add-on included three Chance and three Community Chest cards directing the player to "Advance to Stock Exchange".

The "Stock Exchange" add-on was later redesigned and re-released in 1992 under license by Chessex, this time including a larger number of new Chance and Community Chest cards. This version included ten new Chance cards (five "Advance to Stock Exchange" and five other related cards) and eleven new Community Chest cards (five "Advance to Stock Exchange" and six other related cards; the regular Community Chest card "From sale of stock you get $45" is removed from play when using these cards). Many of the original rules applied to this new version (in fact, one optional play choice allows for playing in the original form by only adding the "Advance to Stock Exchange" cards to each deck).

A "Monopoly Stock Exchange Edition" was released in 2001 (although not in the U.S.), this time adding an electronic calculator-like device to keep track of the complex stock figures. This was a full edition, not just an add-on, that came with its own board, money and playing pieces. Properties on the board were replaced by companies on which shares could be floated, and offices and home offices (instead of houses and hotels) could be built.

Playmaster, another official add-on, released in 1982, is an electronic device that keeps track of all player movement and dice rolls as well as what properties are still available. It then uses this information to call random auctions and mortgages making it easier to free up cards of a color group. It also plays eight short tunes when key game functions occur; for example when a player lands on a railroad it plays "I've Been Working on the Railroad", and a police car's siren sounds when a player goes to Jail.

In 2009, Hasbro released two minigames that can be played as stand-alone games or combined with the "Monopoly" game. In "Get Out of Jail", the goal is to manipulate a spade under a jail cell to flick out various colored prisoners. The game can be used as an alternative to rolling doubles to get out of jail. In "Free Parking", players attempt to balance taxis on a wobbly board. The "Free Parking" add-on can also be used with the Monopoly game. When a player lands on the Free Parking, the player can take the Taxi Challenge, and if successful, can move to any space on the board.

First included in Winning Moves' "Monopoly: The Mega Edition" variant, this third, six-sided die is rolled with the other two, and accelerates game-play when in use. In 2007, Parker Brothers began releasing its standard version (also called the Speed Die Edition) of "Monopoly" with the same die (originally in blue, later in red). Its faces are: 1, 2, 3, two "Mr. Monopoly" sides, and a bus. The numbers behave as normal, adding to the other two dice, unless a "triple" is rolled, in which case the player can move to any space on the board. If "Mr. Monopoly" is rolled while there are unowned properties, the player advances forward to the nearest one. Otherwise, the player advances to the nearest property on which rent is owed. In the "Monopoly: Mega Edition", rolling the bus allows the player to take the regular dice move, then either take a bus ticket or move to the nearest draw card space. 

Mega rules specifies that triples do not count as doubles for going to jail as the player does not roll again. Used in a regular edition, the bus (properly "get off the bus") allows the player to use only one of the two numbered dice or the sum of both, thus a roll of 1, 5, and bus would let the player choose between moving 1, 5, or 6 spaces. The Speed Die is used throughout the game in the "Mega Edition", while in the "Regular Edition" it is used by any player who has passed GO at least once. In these editions it remains optional, although use of the Speed Die was made mandatory for use in the 2009 U.S. and World Monopoly Championship, as well as the 2015 World Championship.

Parker Brothers and its licensees have also sold several spin-offs of "Monopoly". These are not add-ons, as they do not function as an addition to the "Monopoly" game, but are simply additional games with the flavor of "Monopoly":

Besides the many variants of the actual game (and the "Monopoly Junior" spin-off) released in either video game or computer game formats (e.g., Commodore 64, Macintosh, Windows-based PC, Game Boy, Game Boy Advance, Nintendo Entertainment System, iPad, Genesis, Super NES, etc.), two spin-off computer games have been created. An electronic hand-held version was marketed from 1997 to 2001.

"Monopoly"-themed slot machines and lotteries have been produced by WMS Gaming in conjunction with International Game Technology for land-based casinos.WagerWorks, who have the online rights to "Monopoly", have created online "Monopoly" themed games.

London's Gamesys Group have also developed "Monopoly"-themed gambling games. The British quiz machine brand itbox also supports a "Monopoly" trivia and chance game.

There was also a live, online version of "Monopoly". Six painted taxis drive around London picking up passengers. When the taxis reach their final destination, the region of London that they are in is displayed on the online board. This version takes far longer to play than board-game "Monopoly", with one game lasting 24 hours. Results and position are sent to players via e-mail at the conclusion of the game.

The "McDonald's Monopoly" game is a sweepstakes advertising promotion of McDonald's and Hasbro that has been offered in Argentina, Australia, Austria, Brazil, Canada, France, Germany, Hong Kong, Ireland, the Netherlands, New Zealand, Poland, Portugal, Romania, Russia, Singapore, South Africa, Spain, Switzerland, Taiwan, United Kingdom and United States.

A short-lived "Monopoly" game show aired on Saturday evenings from June 16 to September 1, 1990, on ABC. The show was produced by Merv Griffin and hosted by Mike Reilly. The show was paired with a summer-long "Super Jeopardy!" tournament, which also aired during this period on ABC.

From 2010 to 2014, The Hub aired the game show "Family Game Night" with Todd Newton. For the first two seasons, teams earned cash in the form of "Monopoly Crazy Cash Cards" from the "Monopoly Crazy Cash Corner", which was then inserted to the "Monopoly Crazy Cash Machine" at the end of the show. In addition, beginning with Season 2, teams won "Monopoly Party Packages" for winning the individual games. For Season 3, there was a Community Chest. Each card on Mr. Monopoly had a combination of three colors. Teams used the combination card to unlock the chest. If it was the right combination, they advanced to the Crazy Cash Machine for a brand-new car. For the show's fourth season, a new game was added called Monopoly Remix, featuring Park Place and Boardwalk, as well as Income Tax and Luxury Tax.

To honor the game's 80th anniversary, a game show in syndication on March 28, 2015, called "Monopoly Millionaires' Club" was launched. It was connected with a multi-state lottery game of the same name and hosted by comedian Billy Gardell from "Mike & Molly". The game show was filmed at the Rio All Suite Hotel and Casino and at Bally's Las Vegas in Las Vegas, with players having a chance to win up to $1,000,000. However, the lottery game connected with the game show (which provided the contestants) went through multiple complications and variations, and the game show last aired at the end of April 2016.

In November 2008, Ridley Scott was announced to direct Universal Pictures' film version of the game, based on a script written by Pamela Pettler. The film was co-produced by Hasbro's Brian Goldner, as part of a deal with Hasbro to develop movies based on the company's line of toys and games. The story was being developed by author Frank Beddor. However, Universal eventually halted development in February 2012 then opted out of the agreement and rights reverted to Hasbro.

In October 2012, Hasbro announced a new partnership with production company Emmett/Furla Films, and said they would develop a live-action version of "Monopoly", along with Action Man and Hungry Hungry Hippos. Emmett/Furla/Oasis dropped out of the production of this satire version that was to be directed by Ridley Scott.

In July 2015, Hasbro announced that Lionsgate will distribute a "Monopoly" film with Andrew Niccol writing the film as a family-friendly action adventure film co-financed and produced by Lionsgate and Hasbro's Allspark Pictures.

In January 2019, it was announced that Allspark Pictures would now be producing an untitled "Monopoly" film in conjunction with Kevin Hart's company HartBeat Productions and The Story Company. Hart is attached to star in the film and Tim Story is attached to direct, and no logline or writer for this iteration of the long-gestating project has been announced. 

The documentary "", covering the history and players of the game, won an Audience Award for Best Documentary at the 2010 Anaheim International Film Festival. The film played theatrically in the U.S. beginning in March 2011 and was released on Amazon and iTunes on February 14, 2012. The television version of the film won four regional Emmy Awards from the Pacific Southwest Chapter of NATAS. The film is directed by Kevin Tostado and narrated by Zachary Levi.

Until 1999, U.S. entrants had to win a state/district/territory competition to represent that state/district/territory at the once every four year national championship. The 1999 U.S. National Tournament had 50 contestants - 49 State Champions (Oklahoma was not represented) and the reigning national champion.

Qualifying for the National Championship has been online since 2003. For the 2003 Championship, qualification was limited to the first fifty people who correctly completed an online quiz. Out of concerns that such methods of qualifying might not always ensure a competition of the best players, the 2009 Championship qualifying was expanded to include an online multiple-choice quiz (a score of 80% or better was required to advance); followed by an online five-question essay test; followed by a two-game online tournament at Pogo.com. The process was to have produced a field of 23 plus one: Matt McNally, the 2003 national champion, who received a bye and was not required to qualify. However, at the end of the online tournament, there was an eleven-way tie for the last six spots. The decision was made to invite all of those who had tied for said spots. In fact, two of those who had tied and would have otherwise been eliminated, Dale Crabtree of Indianapolis, Indiana, and Brandon Baker, of Tuscaloosa, Alabama, played in the final game and finished third and fourth respectively.

The 2009 "Monopoly" U.S. National Championship was held on April 14–15 in Washington, D.C. In his first tournament ever, Richard Marinaccio, an attorney from Sloan, New York (a suburb of Buffalo), prevailed over a field that included two previous champions to be crowned the 2009 U.S. National Champion. In addition to the title, Marinaccio took home $20,580—the amount of money in the bank of the board game—and competed in the 2009 World Championship in Las Vegas, Nevada, on October 21–22, where he finished in third place.

In 2015, Hasbro used a competition that was held solely online to determine who would be the U.S. representative to compete at the 2015 "Monopoly" World Championship. Interested players took a twenty-question quiz on "Monopoly" strategy and rules and submitted a hundred-word essay on how to win a "Monopoly" tournament. Hasbro then selected Brian Valentine of Washington, D.C., to be the U.S. representative.

Hasbro conducts a worldwide "Monopoly" tournament. The first "Monopoly" World Championships took place in Grossinger's Resort in New York, in November 1973, but they did not include competitors from outside the United States until 1975. It has been aired in the United States by ESPN. In 2009, forty-one players competed for the title of "Monopoly" World Champion and a cash prize of $20,580 (USD)—the total amount of Monopoly money in the current Monopoly set used in the tournament. The most recent World Championship took place September 2015 in Macau. Italian Nicolò Falcone defeated the defending world champion and players from twenty-six other countries.

Because "Monopoly" evolved in the public domain before its commercialization, "Monopoly" has seen many variant games. The game is licensed in 103 countries and printed in thirty-seven languages. Most of the variants are exact copies of the "Monopoly" games with the street names replaced with locales from a particular town, university, or fictional place. National boards have been released as well. Over the years, many specialty "Monopoly" editions, licensed by Parker Brothers/Hasbro, and produced by them, or their licensees (including USAopoly and Winning Moves Games) have been sold to local and national markets worldwide. Two well known "families" of -opoly like games, without licenses from Parker Brothers/Hasbro, have also been produced.

Several published games like "Monopoly" include:

Other unlicensed editions include: "BibleOpoly", "HomoNoPolis" and Petropolis, among others.

There have been a large number of localized editions, broken down here by region:

This list is of unauthorized, unlicensed games based on "Monopoly":
Ghettopoly
Middopoly
Memeopolis (Android app)

In 2008, Hasbro released "Monopoly Here and Now: The World Edition". This world edition features top locations of the world. The locations were decided by votes over the Internet. The result of the voting was announced on August 20, 2008.

Out of these, Gdynia is especially notable, as it is by far the smallest city of those featured and won the vote thanks to a spontaneous, large-scale mobilization of support started by its citizens. The new game uses its own currency unit, the Monopolonian (a game-based take on the Euro; designated by M). The game uses said unit in millions and thousands. As seen below, there is no dark purple color-group, as that is replaced by brown, as in the European version of the game.

It is also notable that three cities (Montreal, Toronto, and Vancouver) are from Canada and three other cities (Beijing, Hong Kong, and Shanghai) are from the People's Republic of China. No other countries are represented by more than one city.

Of the 68 cities listed on Hasbro Inc.'s website for the vote, Jerusalem was chosen as one of the 20 cities to be featured in the newest "Monopoly" World Edition. Before the vote took place, a Hasbro employee in the London office eliminated the country signifier "Israel" after the city, in response to pressure from pro-Palestinian advocacy groups. After the Israeli government protested, Hasbro Inc. issued a statement that read: "It was a bad decision, one that we rectified relatively quickly. This is a game. We never wanted to enter into any political debate. We apologize to our "Monopoly" fans."

A similar online vote was held in early 2015 for an updated version of the game. The resulting board should be released worldwide in late 2015. Lima, Peru won the vote and will hold the Boardwalk space.

Hasbro sells a "Deluxe Edition", which is mostly identical to the classic edition but has wooden houses and hotels and gold-toned tokens, including one token in addition to the standard eleven, a railroad locomotive. Other additions to the "Deluxe Edition" include a card carousel, which holds the title deed cards, and money printed with two colors of ink.

In 1978, retailer Neiman Marcus manufactured and sold an all-chocolate edition of "Monopoly" through its "Christmas Wish Book" for that year. The entire set was edible, including the money, dice, hotels, properties, tokens and playing board. The set retailed for $600.

In 2000, the FAO Schwarz store in New York City sold a custom version called "One-Of-A-Kind Monopoly" for $100,000. This special edition comes in a locking attaché case made with Napolino leather and lined in suede, and features include:

The "Guinness Book of World Records" states that a set worth $2,000,000 and made of 23-carat gold, with rubies and sapphires atop the chimneys of the houses and hotels, is the most expensive "Monopoly" set ever produced. This set was designed by artist Sidney Mobell to honor the game's 50th anniversary in 1985, and is now in the Smithsonian Institution.

"Wired" magazine believes "Monopoly" is a poorly designed game. Former Wall Streeter Derk Solko explains, "Monopoly has you grinding your opponents into dust. It's a very negative experience. It's all about cackling when your opponent lands on your space and you get to take all their money."

Most of the three to four-hour average playing time is spent waiting for other players to play their turn. "Board game enthusiasts disparagingly call this a 'roll your dice, move your mice' format."

The hobby-gaming community BoardGameGeek is especially critical. User reviews of Monopoly rank the game among the 20 worst games out of nearly 10,000 ranked in the database with an average rating of 4.422 out of 10.


Notes
Bibliography 



</doc>
<doc id="19693" url="https://en.wikipedia.org/wiki?curid=19693" title="Max Steiner">
Max Steiner

Maximilian Raoul Steiner (May 10, 1888 – December 28, 1971) was an Austrian-born American music composer for theatre and films, as well as a conductor. He was a child prodigy who conducted his first operetta when he was twelve and became a full-time professional, either composing, arranging, or conducting, when he was fifteen.

Steiner worked in England, then Broadway, and in 1929 he moved to Hollywood, where he became one of the first composers to write music scores for films. He is referred to as "the father of film music", as Steiner played a major part in creating the tradition of writing music for films, along with composers Dimitri Tiomkin, Franz Waxman, Erich Wolfgang Korngold, Alfred Newman, Bernard Herrmann, and Miklós Rózsa.

Steiner composed over 300 film scores with RKO Pictures and Warner Bros., and was nominated for 24 Academy Awards, winning three: "The Informer" (1935); "Now, Voyager" (1942); and "Since You Went Away" (1944). Besides his Oscar-winning scores, some of Steiner's popular works include "King Kong" (1933), "Little Women" (1933), "Jezebel" (1938), and "Casablanca" (1942), though he did not score its love theme, "As Time Goes By". In addition, Steiner scored "The Searchers" (1956), "A Summer Place" (1959), and "Gone with the Wind" (1939), which ranked second on AFI's list of best American film scores, and the film score for which he is best known.

He was also the first recipient of the Golden Globe Award for Best Original Score, which he won for his score for "Life with Father". Steiner was a frequent collaborator with some of the best known film directors in history, including Michael Curtiz, John Ford, and William Wyler, and scored many of the films with Errol Flynn, Bette Davis, Humphrey Bogart, and Fred Astaire. Many of his film scores are available as separate soundtrack recordings.

Max Steiner was born on May 10, 1888, in Austria-Hungary, as the only child in a wealthy business and theatrical family of Jewish heritage. He was named after his paternal grandfather, Maximilian Steiner (1839–1880), who was credited with first persuading Johann Strauss II to write for the theater, and was the influential manager of Vienna's historic Theater an der Wien. His parents were Marie Josefine/Mirjam (Hasiba) and Hungarian Jewish Gábor Steiner (1858–1944, born in Temesvár, Kingdom of Hungary, Austrian Empire), a Viennese impresario, carnival exposition manager, and inventor, responsible for building the Wiener Riesenrad. His father encouraged Steiner's musical talent, and allowed him to conduct an American operetta at the age of twelve, "The Belle of New York" which allowed Steiner to gain early recognition by the operetta's author, Gustave Kerker. Steiner's mother Marie was a dancer in stage productions put on by his grandfather when she was young, but later became involved in the restaurant business. His godfather was the composer Richard Strauss who strongly influenced Steiner's future work. Steiner often credited his family for inspiring his early musical abilities. As early as six years old, Steiner was taking three or four piano lessons a week, yet often became bored of the lessons. Because of this, he would practice improvising on his own, his father encouraging him to write his music down. Steiner cited his early improvisation as an influence of his taste in music, particularly his interest in the music of Claude Debussy which was "avant garde" for the time. In his youth, he began his composing career through his work on marches for regimental bands and hit songs for a show put on by his father.

His parents sent Steiner to the Vienna University of Technology, but he expressed little interest in scholastic subjects. He enrolled in the Imperial Academy of Music in 1904, where, due to his precocious musical talents and private tutoring by Robert Fuchs, and Gustav Mahler, he completed a four-year course in only one year, winning himself a gold medal from the academy at the age of fifteen. He studied various instruments including piano, organ, violin, double bass, and trumpet. His preferred and best instrument was the piano, but he acknowledged the importance of being familiar with what the other instruments could do. He also had courses in harmony, counterpoint, and composition. Along with Mahler and Fuchs, he cited his teachers as Felix Weingartner and Edmund Eysler.

The music of Edmund Eysler was an early influence in the pieces of Max Steiner; however, one of his first introductions to operettas was by Franz Lehár who worked for a time as a military bandmaster for Steiner's father's theatre. Steiner paid tribute to Lehár through an operetta modeled after Lehár's "Die lustige Witwe" which Steiner staged in 1907 in Vienna. Eysler was well-known for his operettas though as critiqued by Richard Traubner, the libretti were poor, with a fairly simple style, the music often relying too heavily on the Viennese waltz style. As a result, when Steiner started writing pieces for the theater, he was interested in writing libretto as his teacher had, but had minimal success. However, many of his future film scores such as "Dark Victory" (1939), "In This Our Life" (1941) and "Now Voyager" (1942) had frequent waltz melodies as influenced by Eysler. According to author of "Max Steiner's "Now Voyager"" Kate Daubney, Steiner may also have been influenced by Felix Weingartner who conducted the Vienna Opera from 1908 to 1911. Although he took composition classes from Weingartner, as a young boy, Steiner always wanted to be a great conductor.

Between 1907 and 1914, Steiner traveled between Britain and Europe to work on theatrical productions. Steiner first entered the world of professional music when he was fifteen. He wrote and conducted the operetta, "The Beautiful Greek Girl", but his father refused to stage it saying it was not good enough. Steiner took the composition to competing impresario Carl Tuschl who offered to produce it. Much to Steiner's pleasure, it ran in the Orpheum Theatre for a year. This led to opportunities to conduct other shows in various cities around the world, including Moscow and Hamburg. Upon returning to Vienna, Steiner found his father in bankruptcy. Having difficulties finding work, he moved to London (in part to follow an English showgirl he had met in Vienna). In London, he was invited to conduct Lehar's "The Merry Widow." He stayed in London for eight years conducting musicals at Daly's Theatre, the Adelphi, the Hippodrome, the London Pavilion and the Blackpool Winter Gardens. Steiner married Beatrice Stilt on September 12, 1912. The exact date of their divorce is unknown.

In England, Steiner wrote and conducted theater productions and symphonies. But the beginning of World War I in 1914 led him to be interned as an enemy alien. Fortunately, he was befriended by the Duke of Westminster, who was a fan of his work, and was given exit papers to go to America, although his money was impounded. He arrived in New York City in December 1914, with only $32. Unable to quickly find work, he resorted to menial jobs such as a copyist for Harms Music Publishing which quickly led him to jobs orchestrating stage musicals.

In New York, Max Steiner quickly acquired employment and worked for fifteen years as a musical director, arranger, orchestrator, and conductor of Broadway productions. These productions include operettas and musicals written by Victor Herbert, Jerome Kern, Vincent Youmans, and George Gershwin, among others. Steiner's credits include: "George White's Scandals" (1922) (director), "Peaches" (1923) (composer), and "Lady, Be Good" (1924) (conductor and orchestrator). During his time working on Broadway, he married Audree van Lieu on April 27, 1927. They divorced on December 14, 1933. In 1927, Steiner orchestrated and conducted Harry Tierney's "Rio Rita". Tierney himself later requested RKO Pictures in Hollywood hire Steiner to work in their music production departments. William LeBaron, RKO's head of production, traveled to New York to watch Steiner conduct and was impressed by Steiner and his musicians, who each played several instruments. Eventually, Steiner became a Hollywood asset. Steiner's final production on Broadway was "Sons O' Guns" in 1929.

By request of Harry Tierney, RKO hired Max Steiner as an orchestrator and his first film job consisted of composing music for the main and end titles and occasional "on screen" music. According to Steiner, the general opinion of filmmakers during the time was that film music was a "necessary evil," and would often slow down production and release of the film after it was filmed. Steiner's first job was for the film "Dixiana"; however, after a while RKO decided to let him go, feeling they were not using him. His agent found him a job as a musical director on an operetta in Atlantic City. Before he left RKO, they offered him a month to month contract as the head of the music department with promise of more work in the future and he agreed. Because the few composers in Hollywood were unavailable, Steiner composed his first film score for "Cimarron". The score was well received and was partially credited for the success of the film. He turned down several offers to teach film scoring technique in Moscow and Peking in order to stay in Hollywood. In 1932, Steiner was asked to add music to "Symphony of Six Million" (1932), by David O. Selznick, the new producer at RKO. Steiner composed a short segment; Selznick liked so much that he asked him to compose the theme and underscoring for the entire picture. Selznick was proud of the film, feeling it gave a realistic view of Jewish family life and tradition. "Music until then had not been used very much for underscoring." Steiner "pioneered the use of original composition as background scoring for films." The successful scoring in "Symphony of Six Million" was a turning point for Steiner's career and for the film industry. After the underscoring of "Symphony of Six Million," a third to half of the success of most films was "attributed to the extensive use of music."

The score for "King Kong" (1933) became Steiner's breakthrough and represented a paradigm shift in the scoring of fantasy and adventure films. The score was an integral part of the film, because it added realism to an unrealistic film plot. The studio's bosses were initially skeptical about the need for an original score; however, since they disliked the film's contrived special effects, they let Steiner try to improve the film with music. The studio suggested using old tracks in order to save on the cost of the film; however, "King Kong" producer Merian C. Cooper asked Steiner to score the film and said he would pay for the orchestra. Steiner took advantage of this offer and used an eighty-piece orchestra, explaining the film "was made for music." According to Steiner, "it was the kind of film that allowed you to do anything and everything, from weird chords and dissonances to pretty melodies." Steiner additionally scored the wild tribal music which accompanied the ceremony to sacrifice Ann to Kong. He wrote the score in two weeks and the music recording cost around $50,000. The film became a "landmark of film scoring," as it showed the power music has to manipulate audience emotions. Steiner constructed the score on Wagnerian leitmotif principle, which calls for special themes for leading characters and concepts. The theme of the monster is recognizable as a descending three-note chromatic motif. After the death of King Kong, the Kong theme and the Fay Wray theme converge, underlining the "Beauty and the Beast" type relationship between the characters. The music in the film's finale helped express the tender feelings Kong had for the woman without the film having to explicitly state it. The majority of the music is heavy and loud, but some of the music is a bit lighter. For example, when the ship sails into Skull Island, Steiner keeps the music calm and quiet with a small amount of texture in the harps to help characterize the ship as it cautiously moves through the misty waters. Steiner received a bonus from his work, as Cooper credited 25 percent of the film's success to the film score. Before he died, Steiner admitted "King Kong" was one of his favorite scores.

"King Kong" quickly made Steiner one of the most respected names in Hollywood. He continued as RKO's music director for two more years, until 1936. Max married Louise Klos, a harpist, in 1936. They had a son, Ron, together and they divorced in 1946. Steiner composed, arranged and conducted another 55 films, including most of Fred Astaire and Ginger Rogers's dance musicals. Additionally, Steiner wrote a sonata used in Katharine Hepburn's first film, "Bill of Divorcement" (1932). RKO producers, including Selznick, often came to him when they had problems with films, treating him as if he were a music "doctor." Steiner was asked to compose a score for "Of Human Bondage" (1934), which originally lacked music. He added musical touches to significant scenes. Director John Ford called on Steiner to score his film, "The Lost Patrol" (1934), which lacked tension without music.

John Ford hired Steiner again to compose for his next film, "The Informer" (1935), before Ford began production of the film. Ford even asked his screenwriter to meet with Steiner during the writing phase to collaborate. This was unusual for Steiner who typically refused to compose a score from anything earlier than a rough cut of the film. Because Steiner scored the music before and during film production, Ford would sometimes shoot scenes in synchronization with the music Steiner composed rather than the usual practice of film composers synchronizing music to the film's scenes. Consequently, Steiner directly influenced the development of the protagonist, Gypo. Victor McLaglen, who played Gypo, rehearsed his walking in order to match the fumbling leitmotif Steiner had created for Gypo. This unique film production practice was successful; the film was nominated for six Academy Awards and won four, including Steiner's first Academy Award for Best Score. This score helped to exemplify Steiner's ability to encompass the essence of a film in a single theme. The main title of the film's soundtrack has three specific aspects. First, the heavy-march like theme helps to describe the oppressive military and main character Gypo's inevitable downfall. Second, the character's theme is stern and sober and puts the audience into the correct mood for the film. Finally, the theme of the music contains some Irish folk song influences which serves to better characterize the Irish historical setting and influence of the film. The theme is not heard consistently throughout the film and serves rather as a framework for the other melodic motifs heard throughout different parts of the film.

The score for this film is made up of many different themes which characterize the different personages and situations in the film. Steiner helps portray the genuine love Katie has for the main character Gypo. In one scene, Katie calls after Gypo as a solo violin echos the falling cadence of her voice. In another scene, Gypo sees an advertisement for a steamship to America and instead of the advertisement, sees himself holding Katie's hand on the ship. Wedding bells are heard along with organ music and he sees Katie wearing a veil and holding a bouquet. In a later scene, the Katie theme plays as a drunk Gypo sees a beautiful woman at the bar, insinuating he had mistaken her for Katie. Other musical themes included in the film score are an Irish folk song on French horns for Frankie McPhilip, a warm string theme for Dan and Gallagher and Mary McPhillip, and a sad theme on English horn with harp for the blind man.The most important motif in the film is the theme of betrayal relating to how Gypo betrays his friend Frankie: the "blood-money" motif. The theme is heard as the Captain throws the money on the table after Frankie is killed. The theme is a four note descending tune on harp; the first interval is the tritone. As the men are deciding who will be the executioner, the motif is repeated quietly and perpetually to establish Gypo's guilt and the musical motif is synchronized with the dripping of water in the prison. As it appears in the end of the film, the theme is played at a fortissimo volume as Gypo staggers into the church, ending the climax with the clap of the cymbals, indicating Gypo's penitence, no longer needing to establish his guilt.

Silent film mannerisms are still seen in Steiner's composition such as when actions or consequences are accompanied by a "sforzato" chord immediately before it, followed by silence. An example of this is remarked in the part of the film when Frankie confronts Gypo looking at his reward for arrest poster. Steiner uses minor "Mickey Mousing" techniques in the film. Through this score, Steiner showed the potential of film music, as he attempted the show the internal struggles inside of Gypo's mind through the mixing of different themes such as the Irish "Circassian Circle," the "blood-money" motif, and Frankie's theme. According to composer and film music writer Christopher Palmer, Steiner's use of Franz Schubert's "Ave Maria" at the end of the film was the score's only flaw. Specifically, the theme as Gypo dies in the church was too void of spiritual ecstasy and similarly ruined the ending of Disney's "Fantasia".

In 1937, Steiner was hired by Frank Capra to conduct Dimitri Tiomkin's score for Lost Horizon (1937) as a safeguard in case Steiner needed to rewrite the score by an inexperienced Tiomkin; however, according to Hugo Friedholfer, Tiomkin specifically asked for Steiner, preferring him over the film studio's then music director. Producer David O. Selznick set up his own production company in 1936 and recruited Steiner to write the scores for his next three films.

In April 1937, Steiner left RKO and signed a long-term contract with Warner Bros.; he would, however, continue to work for Selznick. The first film he scored for Warner Bros. was "The Charge of the Light Brigade" (1936). Steiner became a mainstay at Warner Bros., scoring 140 of their films over the next 30 years alongside Hollywood stars such as Bette Davis, Errol Flynn, Humphrey Bogart, and James Cagney. Steiner frequently worked with composer Hugo Friedhofer who was hired as an orchestrator for Warner Bros; Friedholfer would orchestrate more than 50 of Steiner's pieces during his career. In 1938, Steiner wrote and arranged the first "composed for film" piece, "Symphony Moderne" which a character plays on the piano and later plays as a theme in "Four Daughters" (1938) and is performed by a full orchestra in "Four Wives" (1939).
In 1939, Steiner was borrowed from Warner Bros. by Selznick to compose the score for "Gone with the Wind" (1939), which became one of Steiner's most notable successes. Steiner was the only composer Selznick considered for scoring the film. Steiner was given only three months to complete the score, despite composing twelve more film scores in 1939, more than he would in any other year of his career. Because Selznick was concerned Steiner wouldn't have enough time to finish the score, he had Franz Waxman write an additional score in the case the Steiner didn't finish. To meet the deadline, Steiner sometimes worked for 20-hours straight, assisted by doctor-administered Benzedrine to stay awake. When the film was released, it was the longest film score ever composed, nearly three hours. The composition consisted of 16 main themes and nearly 300 musical segments. Due to the score's length, Steiner had help from four orchestrators and arrangers, including Heinz Roemheld, to work on the score. Selznick had asked Steiner to use only pre-existing classical music to help cut down on cost and time, but Steiner tried to convince him that filling the picture with swatches of classic concert music or popular works would not be as effective as an original score, which could be used to heighten the emotional content of scenes. Steiner ignored Selznick's wishes and composed an entirely new score. Selznick's opinion about using original scoring may have changed due to the overwhelming reaction to the film, nearly all of which contained Steiner's music. A year later, he even wrote a letter emphasizing the value of original film scores. The most well known of Steiner's themes for the score is the "Tara" theme for the O'Hara family plantation. Steiner explains Scarlett's deep-founded love for her home is why "the 'Tara' theme begins and ends with the picture and permeates the entire score". The film went on to win ten Academy Awards, although not for the best original score, which instead went to Herbert Stothart for the musical "The Wizard of Oz". The score of "Gone With The Wind" is ranked #2 by AFI as the second greatest American film score of all time.

"Now, Voyager" would be the film score for which Steiner would win his second Academy Award. Kate Daubney attributes the success of this score to Steiner's ability to "[balance] the scheme of thematic meaning with the sound of the music." Steiner used motifs and thematic elements in the music to emphasize the emotional development of the narrative. After finishing "Now, Voyager" (1942), Steiner was hired to score the music for "Casablanca" (1942). Steiner would typically wait until the film was edited before scoring it, and after watching "Casablanca", he decided the song "As Time Goes By" by Herman Hupfeld wasn't an appropriate addition to the movie and he wanted to replace it with a song of his own composition; however, Ingrid Bergman had just cut her hair short in preparation for filming "For Whom the Bell Tolls" (1943), so she couldn't re-film the section with Steiner's song. Stuck with "As Time Goes By," Steiner embraced the song and made it the center theme of his score. Steiner's score for "Casablanca" was nominated for the Academy Award for best score, losing to "The Song of Bernadette" (1943). Steiner received his third and final Oscar in 1944 for "Since You Went Away" (1944). Steiner actually first composed the theme from "Since You Went Away" while helping counterbalance Franz Waxman's moody score for "Rebecca". Producer David O. Selznick liked the theme so much, he asked Steiner to include it in "Since You Went Away". In 1947, Max married Leonette Blair.

With two exceptions, Steiner was less successful with the film noir genre due to the "modernistic" music those films often require. "The Big Sleep" and "The Letter" were his best film noir scores. "The Letter" is set in Singapore, the tale of murder begins with the loud main musical theme during the credits, which sets the tense and violent mood of the film. The main theme characterizes Leslie, the main character, by her tragic passion. The main theme is heard in the confrontation between Leslie and the murdered man's wife in the Chinese shop. Steiner portrays this scene through the jangling of wind chimes which crescendos as the wife emerges through opium smoke. The jangling continues until the wife asks Leslie to take off her shawl, after which the theme blasts indicating the breaking point of emotions of these women. Steiner's score for "The Letter" was nominated for the 1941 Academy award for best original score, losing to Pinocchio. In the score for "The Big Sleep", Steiner uses musical thematic characterization for the characters in the film. The theme for Philip Marlowe is beguiling and ironic, with a playful grace note at the end of the motif, portrayed mixed between major and minor. At the end of the film, his theme is played fully in major chords and finishes by abruptly ending the chord as the film terminates (this was an unusual film music practice in Hollywood at the time). According to Christopher Palmer, the love theme between Humphrey Bogart and Lauren Bacall is one of Steiner strongest themes. Steiner uses the contrast of high strings and low strings and brass to emphasize Bogart's feelings for Bacall opposed with the brutality of the criminal world.

Steiner had more success with the western genre of film, writing the scores for over twenty large-scale Westerns, most with epic-inspiring scores "about empire building and progress," like "Dodge City" (1939), "The Oklahoma Kid" (1939), and "The Adventures of Mark Twain" (1944). "Dodge City", starring Errol Flynn and Olivia de Havilland, is a good example of Steiner's handling of typical scenes of the Western genre. Steiner used a "lifting, loping melody" which reflected the movement and sounds of wagons, horses and cattle. Steiner showed a love for combining Westerns and romance, as he did in "They Died with Their Boots On" (1941), also starring Flynn and de Havilland. Considered his greatest Western is "The Searchers" (1956).

Although his contract ended in 1953, Steiner returned to Warner Bros. in 1958 and scored several films such as "Band of Angels", "Marjorie Morningstar", and "John Paul Jones", and later ventured into television. Steiner still preferred large orchestras and leitmotif techniques during this part of his career. Steiner's pace slowed significantly in the mid-1950s, and he began freelancing. In 1954, RCA Victor asked Steiner to prepare and conduct an orchestral suite of music from "Gone with the Wind" for a special LP, which was later issued on CD. There are also acetates of Steiner conducting the Warner Brothers studio orchestra in music from some of his film scores. Composer Victor Young and Steiner were good friends, and Steiner completed the film score for China Gate, because Young had died before he could finish it. The credit frame reads: "Music by Victor Young, extended by his old friend, Max Steiner." There are numerous soundtrack recordings of Steiner's music as soundtracks, collections, and recordings by others. Steiner wrote into his seventies, ailing and near blind, but his compositions "revealed a freshness and fertility of invention." A theme for "A Summer Place" in 1959, written when Steiner was 71, became one of Warner Brothers' biggest hit-tunes for years and a re-recorded pop standard. This memorable instrumental theme spent nine weeks at #1 on the "Billboard" Hot 100 singles chart in 1960 (in an instrumental cover version by Percy Faith). Steiner continued to score films produced by Warner until the mid sixties.

In 1963, Steiner began writing his autobiography. Although it was completed, it was never published, and is the only source available on Steiner's childhood. A copy of the manuscript resides with the rest of the Max Steiner Collection at Brigham Young University in Provo, Utah. Steiner scored his last piece in 1965; however, he claimed he would have scored more films had he been offered the opportunity. His lack of work in the last years of his life were due to Hollywood's decreased interest in his scores caused by new film producers and new taste in film music. Another contribution to his declining career was his failing eyesight and deteriorating health, which caused him to reluctantly retire. Tony Thomas cited Steiner's last few scores as, "a weak coda to a mighty career."

Steiner died of congestive heart failure in Hollywood, aged 83. He is entombed in the Great Mausoleum at Forest Lawn Memorial Park Cemetery in Glendale, California.

In the early days of sound, producers avoided underscoring music behind dialogue, feeling the audience would wonder where the music was coming from. As a result, Steiner noted, "they began to add a little music here and there to support love scenes or silent sequences." But in scenes where music might be expected, such as a nightclub, ballroom or theater, the orchestra fit in more naturally and was used often. In order to justify the addition of music in scenes where it wasn't expected, music was integrated into the scene through characters or added more conspicuously. For example, a shepherd boy might play a flute along with the orchestra heard in the background, or a random, wandering violinist might follow around a couple during a love scene; however, because half of the music was recorded on the set, Steiner says it led to a great deal of inconvenience and cost when scenes were later edited, because the score would often be ruined. As recording technology improved during this period, he was able to record the music synced to the film and could change the score after the film was edited. Steiner explains his own typical method of scoring:

Steiner often followed his instincts and his own reasoning in creating film scores. For example, when he chose to go against Selznick's instruction to use classical music for "Gone With the Wind." Steiner stated:

Scores from the classics were sometimes harmful to a picture, especially when they drew unwanted attention to themselves by virtue of their familiarity. For example, films like "2001 – A Space Odyssey", "The Sting" and "Manhattan", had scores with recognizable tunes instead of having a preferred "subliminal" effect. Steiner, was among the first to acknowledge the need for original scores for each film.

Steiner felt knowing when to start and stop was the hardest part of proper scoring, since incorrect placement of music can speed up a scene meant to be slow and vice versa: "Knowing the difference is what makes a film composer." He also notes that many composers, contrary to his own technique, would fail to subordinate the music to the film:

Although some scholars cite Steiner as the inventor of the click track technique, he, along with Roy Webb were only the first to use the technique in film scoring. Carl W. Stalling and Scott Bradley used the technique first, in cartoon music. The click-track allows the composer to sync music and film together more precisely. The technique involves punching holes into the soundtrack film based on the mathematics of metronome speed. As the holes pass through a projector, the orchestra and conductor can hear the clicking sound through headphones, allowing them to record the music along the exact timing of the film. This technique allowed conductors and orchestras to match the music with perfection to the timing of the film, eliminating the previous necessity to cut off or stop music in the middle of recording as had been done previously. Popularized by Steiner in film music, this technique allowed Steiner to "catch the action," creating sounds for small details on screen. In fact, Steiner reportedly spent more of his time matching the action to the music than composing the melodies and motifs, as creating and composing came easy to him.

With Steiner's background in his European musical training largely consisting of operas and operettas and his experience with stage music, he brought with him a slew of old-fashioned techniques he contributed to the development of the Hollywood film score. Although Steiner has been called, "the man who invented modern film music," he himself claimed that, "the idea originated with Richard Wagner ... If Wagner had lived in this century, he would have been the No. 1 film composer." Wagner was the inventor of the leitmotif, and this influenced Steiner's composition. In his music, Steiner relied heavily on leitmotifs. He would also quote pre-existing, recognizable melodies in his scores, such as national anthems. Steiner was known and often criticized for his use of Mickey Mousing or "catching the action." This technique is characterized by the precise matching of music with the actions or gestures on screen. Steiner was criticized for using this technique too frequently. For example, in Of Human Bondage, Steiner created a limping effect with his music whenever the clubfooted character walked.

One of the important principles that guided Steiner whenever possible was his rule: "Every character should have a theme." "Steiner creates a musical picture that tells us all we need to know about the character." To accomplish this, Steiner synchronized the music, the narrative action and the leitmotif as a structural framework for his compositions.

A good example of how the characters and the music worked together is best exemplified by his score for "The Glass Menagerie" (1950):

Another film which exemplifies the synchronizing of character and music is "The Fountainhead" (1949):
The character of Roark, an idealist architect (played by Gary Cooper):

In the same way that Steiner created a theme for each character in a film, Steiner's music developed themes to express emotional aspects of general scenes which originally lacked emotional content. For example:


When adding a music score to a picture, Steiner used a "spotting process" in which he and the director of the film would watch the film in its entirety and discuss where underscoring of diegetic music would begin and end. Another technique Steiner used was the mixing of realistic and background music. For example, a character humming to himself is realistic music, and the orchestra might play his tune, creating a background music effect that ties into the film. Steiner was criticized for this technique as the awareness of the film music can ruin the narrative illusion of the film; however, Steiner understood the importance of letting the film take the spotlight, making the music, "subordinate..to the picture," stating that, "if it gets too decorative, it loses its emotional appeal." Before 1932, producers of sound films tried to avoid the use of background music, because viewers would wonder where the music was coming from. Steiner was known for writing using atmospheric music without melodic content for certain neutral scenes in music. Steiner designed a melodic motion to create normal-sounding music without taking too much attention away from the film. In contrast, Steiner sometimes used diegetic, or narrative based music, in order to emphasize certain emotions or contradict them. According to Steiner, there is, "no greater counterpoint ... than gay music underlying a tragic scene or vice versa."

Three of Max Steiner's scores won the Academy Award for Best Original Score: "The Informer" (1935), "Now, Voyager" (1942), and "Since You Went Away" (1944). Steiner received a certificate for "The Informer". He originally received plaques for "Now, Voyager" and "Since You Went Away", but those plaques were replaced with Academy Award statuettes in 1946. As an individual, Steiner was nominated for a total of 20 Academy Awards, and won two. Prior to 1939, the Academy recognized a studio's music department, rather than the individual composer, with a nomination in the scoring category. During this time, five of Steiner's scores including "The Lost Patrol" and "The Charge of the Light Brigade" were nominated, but the Academy does not consider these nominations to belong to Max Steiner himself. Consequently, even though Steiner's score for "The Informer" won the Academy Award in 1936, the Academy does not officially consider Steiner as the individual winner of the award, as Steiner accepted the award on behalf of RKO's music department of which he was the department head. Steiner's 20 nominations make him the third most nominated individual in the history of the scoring categories, behind John Williams and Alfred Newman.

The United States Postal Service issued its "American Music Series" stamps on September 16, 1999, to pay tribute to renowned Hollywood composers, including Steiner. After Steiner's death, Charles Gerhardt conducted the National Philharmonic Orchestra in an RCA Victor album of highlights from Steiner's career, titled "Now Voyager". He also won a Golden Globe for Best Original Score for "Life with Father" (1947). Additional selections of Steiner scores were included on other RCA classic film albums during the early 1970s. The quadraphonic recordings were later digitally remastered for Dolby surround sound and released on CD. In 1975, Steiner was honored with a star located at 1551 Vine Street on the Hollywood Walk of Fame for his contribution to motion pictures. In 1995, Steiner was inducted posthumously into the Songwriters Hall of Fame. In commemoration of Steiner's 100th birthday, a memorial plaque was unveiled by Helmut Zilk, then Mayor of Vienna, in 1988 at Steiner's birthplace, the "Hotel Nordbahn" (now "Austria Classic Hotel Wien") on Praterstraße 72.

In Kurt London's "Film Music", London expressed the opinion that American film music was inferior to European film music because it lacked originality of composition; he cited the music of Steiner as an exception to the rule. Steiner, along with contemporaries Erich Wolfgang Korngold and Alfred Newman set the style and forms of film music of the time period and for film scores to come. Known for their similar music styles, Roy Webb was also Steiner's contemporary and they were friends until Steiner's death. Webb's score for Mighty Joe Young was reminiscent of Steiner. James Bond composer John Barry cited Steiner as an influence of his work. James Newton Howard, who composed the score for the 2005 remake of King Kong, stated that he was influenced by Steiner's score; his descending theme when Kong first appears is reminiscent of Steiner's score. In fact, during the tribal sacrifice scene of the 2005 version, the music playing is from Steiner's score of the same scene in the 1933 version. Composer of the Star Wars film score, John Williams cited Steiner as well as other European emigrant composers in the 1930s and 1940s "Golden Age" of film music as influences of his work. In fact, George Lucas wanted Williams to use the scores of Steiner and Korngold as influences for the music for Star Wars, despite the rarity of grandiose film music and the lack of use of leitmotifs and full orchestrations during the 1970s.

Often compared to his contemporary Erich Wolfgang Korngold, his rival and friend at Warner Bros., the music of Steiner was often seen by critics as inferior to Korngold. Composer David Raksin stated that the music of Korngold was, "...of a higher order with a much wider sweep;" however, according to William Darby and Jack Du Bois's "American Film Music", even though other film score composers may have produced greater individual scores than Steiner, no composer ever created as many "very good" ones as Steiner. Despite the inferiority of Steiner's individual scores, his influence was largely historical. Steiner was the one of the first composers to reintroduce music into films after the invention of talking films. Steiner's score for "King Kong" modeled the method of adding background music into a movie. Some of his contemporaries did not like his music. Miklós Rózsa criticized Steiner for his use of Mickey Mousing and did not like his music, but Rózsa conceded that Steiner had a successful career and had a good "melodic sense."

Now referred to as the "father of film music" or the "dean of film music," Steiner had written or arranged music for over three hundred films by the end of his career. George Korngold, son of Erich Korngold, produced the Classic Film Score Series albums which included the music of Steiner. Albert K. Bender established the Max Steiner Music Society with international membership, publishing journals and newsletters and a library of audio recordings. When the Steiner collection went to Brigham Young University in 1981, the organization disbanded. The Max Steiner Memorial Society was formed in the United Kingdom continue the work of the Max Steiner Music Society.

The American Film Institute respectively ranked Steiner's scores for "Gone with the Wind" (1939) and "King Kong" (1933) #2 and #13 on their list of the 25 greatest film scores. His scores for the following films were also nominated for the list:



</doc>
<doc id="19694" url="https://en.wikipedia.org/wiki?curid=19694" title="Mercury (planet)">
Mercury (planet)

Mercury is the smallest and innermost planet in the Solar System. Its orbit around the Sun takes 87.97 days, the shortest of all the planets in the Solar System. It is named after the Roman deity Mercury, the messenger of the gods.

Like Venus, Mercury orbits the Sun within Earth's orbit as an inferior planet, and its apparent distance from the Sun as viewed from Earth never exceeds 28°. This proximity to the Sun means the planet can only be seen near the western horizon after sunset or eastern horizon before sunrise, usually in twilight. At this time, it may appear as a bright star-like object, but is often far more difficult to observe than Venus. The planet telescopically displays the complete range of phases, similar to Venus and the Moon, as it moves in its inner orbit relative to Earth, which recurs over its synodic period of approximately 116 days.

Mercury rotates in a way that is unique in the Solar System. It is tidally locked with the Sun in a 3:2 spin-orbit resonance, meaning that relative to the fixed stars, it rotates on its axis exactly three times for every two revolutions it makes around the Sun. As seen from the Sun, in a frame of reference that rotates with the orbital motion, it appears to rotate only once every two Mercurian years. An observer on Mercury would therefore see only one day every two Mercurian years.

Mercury's axis has the smallest tilt of any of the Solar System's planets (about degree). Its orbital eccentricity is the largest of all known planets in the Solar System; at perihelion, Mercury's distance from the Sun is only about two-thirds (or 66%) of its distance at aphelion. Mercury's surface appears heavily cratered and is similar in appearance to the Moon's, indicating that it has been geologically inactive for billions of years. Having almost no atmosphere to retain heat, it has surface temperatures that vary diurnally more than on any other planet in the Solar System, ranging from at night to during the day across the equatorial regions. The polar regions are constantly below . The planet has no known natural satellites.

Two spacecraft have visited Mercury: "" flew by in 1974 and 1975; and "MESSENGER", launched in 2004, orbited Mercury over 4,000 times in four years before exhausting its fuel and crashing into the planet's surface on April 30, 2015. The "BepiColombo" spacecraft is planned to arrive at Mercury in 2025.

Mercury appears to have a solid silicate crust and mantle overlying a solid, iron sulfide outer core layer, a deeper liquid core layer, and a solid inner core.

Mercury is one of four terrestrial planets in the Solar System, and is a rocky body like Earth. It is the smallest planet in the Solar System, with an equatorial radius of . Mercury is also smaller—albeit more massive—than the largest natural satellites in the Solar System, Ganymede and Titan. Mercury consists of approximately 70% metallic and 30% silicate material. Mercury's density is the second highest in the Solar System at 5.427 g/cm, only slightly less than Earth's density of 5.515 g/cm. If the effect of gravitational compression were to be factored out from both planets, the materials of which Mercury is made would be denser than those of Earth, with an uncompressed density of 5.3 g/cm versus Earth's 4.4 g/cm.

Mercury's density can be used to infer details of its inner structure. Although Earth's high density results appreciably from gravitational compression, particularly at the core, Mercury is much smaller and its inner regions are not as compressed. Therefore, for it to have such a high density, its core must be large and rich in iron.

Geologists estimate that Mercury's core occupies about 55% of its volume; for Earth this proportion is 17%. Research published in 2007 suggests that Mercury has a molten core. Surrounding the core is a mantle consisting of silicates. Based on data from the mission and Earth-based observation, Mercury's crust is estimated to be thick. One distinctive feature of Mercury's surface is the presence of numerous narrow ridges, extending up to several hundred kilometers in length. It is thought that these were formed as Mercury's core and mantle cooled and contracted at a time when the crust had already solidified.

Mercury's core has a higher iron content than that of any other major planet in the Solar System, and several theories have been proposed to explain this. The most widely accepted theory is that Mercury originally had a metal–silicate ratio similar to common chondrite meteorites, thought to be typical of the Solar System's rocky matter, and a mass approximately 2.25 times its current mass. Early in the Solar System's history, Mercury may have been struck by a planetesimal of approximately 1/6 that mass and several thousand kilometers across. The impact would have stripped away much of the original crust and mantle, leaving the core behind as a relatively major component. A similar process, known as the giant impact hypothesis, has been proposed to explain the formation of the Moon.

Alternatively, Mercury may have formed from the solar nebula before the Sun's energy output had stabilized. It would initially have had twice its present mass, but as the protosun contracted, temperatures near Mercury could have been between 2,500 and 3,500 K and possibly even as high as 10,000 K. Much of Mercury's surface rock could have been vaporized at such temperatures, forming an atmosphere of "rock vapor" that could have been carried away by the solar wind.

A third hypothesis proposes that the solar nebula caused drag on the particles from which Mercury was accreting, which meant that lighter particles were lost from the accreting material and not gathered by Mercury. Each hypothesis predicts a different surface composition, and there are two space missions set to make observations. "MESSENGER", which ended in 2015, found higher-than-expected potassium and sulfur levels on the surface, suggesting that the giant impact hypothesis and vaporization of the crust and mantle did not occur because potassium and sulfur would have been driven off by the extreme heat of these events. "BepiColombo", which will arrive at Mercury in 2025, will make observations to test these hypotheses. The findings so far would seem to favor the third hypothesis; however, further analysis of the data is needed.

Mercury's surface is similar in appearance to that of the Moon, showing extensive mare-like plains and heavy cratering, indicating that it has been geologically inactive for billions of years. Because knowledge of Mercury's geology had been based only on the 1975 flyby and terrestrial observations, it is the least understood of the terrestrial planets. As data from "MESSENGER" orbiter are processed, this knowledge will increase. For example, an unusual crater with radiating troughs has been discovered that scientists called "the spider". It was later named Apollodorus.

Albedo features are areas of markedly different reflectivity, as seen by telescopic observation. Mercury has dorsa (also called "wrinkle-ridges"), Moon-like highlands, montes (mountains), planitiae (plains), rupes (escarpments), and valles (valleys).

Names for features on Mercury come from a variety of sources. Names coming from people are limited to the deceased. Craters are named for artists, musicians, painters, and authors who have made outstanding or fundamental contributions to their field. Ridges, or dorsa, are named for scientists who have contributed to the study of Mercury. Depressions or fossae are named for works of architecture. Montes are named for the word "hot" in a variety of languages. Plains or planitiae are named for Mercury in various languages. Escarpments or rupēs are named for ships of scientific expeditions. Valleys or valles are named for abandoned cities, towns, or settlements of antiquity.

Mercury was heavily bombarded by comets and asteroids during and shortly following its formation 4.6 billion years ago, as well as during a possibly separate subsequent episode called the Late Heavy Bombardment that ended 3.8 billion years ago. During this period of intense crater formation, Mercury received impacts over its entire surface, facilitated by the lack of any atmosphere to slow impactors down. During this time Mercury was volcanically active; basins such as the Caloris Basin were filled by magma, producing smooth plains similar to the maria found on the Moon.

Data from the October 2008 flyby of "MESSENGER" gave researchers a greater appreciation for the jumbled nature of Mercury's surface. Mercury's surface is more heterogeneous than either Mars's or the Moon's, both of which contain significant stretches of similar geology, such as maria and plateaus.

Craters on Mercury range in diameter from small bowl-shaped cavities to multi-ringed impact basins hundreds of kilometers across. They appear in all states of degradation, from relatively fresh rayed craters to highly degraded crater remnants. Mercurian craters differ subtly from lunar craters in that the area blanketed by their ejecta is much smaller, a consequence of Mercury's stronger surface gravity. According to IAU rules, each new crater must be named after an artist that was famous for more than fifty years, and dead for more than three years, before the date the crater is named.

The largest known crater is , with a diameter of 1,550 km. The impact that created the Caloris Basin was so powerful that it caused lava eruptions and left a concentric ring over 2 km tall surrounding the impact crater. At the antipode of the Caloris Basin is a large region of unusual, hilly terrain known as the "Weird Terrain". One hypothesis for its origin is that shock waves generated during the Caloris impact traveled around Mercury, converging at the basin's antipode (180 degrees away). The resulting high stresses fractured the surface. Alternatively, it has been suggested that this terrain formed as a result of the convergence of ejecta at this basin's antipode.

Overall, about 15 impact basins have been identified on the imaged part of Mercury. A notable basin is the 400 km wide, multi-ring Tolstoj Basin that has an ejecta blanket extending up to 500 km from its rim and a floor that has been filled by smooth plains materials. Beethoven Basin has a similar-sized ejecta blanket and a 625 km diameter rim. Like the Moon, the surface of Mercury has likely incurred the effects of space weathering processes, including Solar wind and micrometeorite impacts.

There are two geologically distinct plains regions on Mercury. Gently rolling, hilly plains in the regions between craters are Mercury's oldest visible surfaces, predating the heavily cratered terrain. These inter-crater plains appear to have obliterated many earlier craters, and show a general paucity of smaller craters below about 30 km in diameter.

Smooth plains are widespread flat areas that fill depressions of various sizes and bear a strong resemblance to the lunar maria. Notably, they fill a wide ring surrounding the Caloris Basin. Unlike lunar maria, the smooth plains of Mercury have the same albedo as the older inter-crater plains. Despite a lack of unequivocally volcanic characteristics, the localisation and rounded, lobate shape of these plains strongly support volcanic origins. All the smooth plains of Mercury formed significantly later than the Caloris basin, as evidenced by appreciably smaller crater densities than on the Caloris ejecta blanket. The floor of the Caloris Basin is filled by a geologically distinct flat plain, broken up by ridges and fractures in a roughly polygonal pattern. It is not clear whether they are volcanic lavas induced by the impact, or a large sheet of impact melt.

One unusual feature of Mercury's surface is the numerous compression folds, or rupes, that crisscross the plains. As Mercury's interior cooled, it contracted and its surface began to deform, creating wrinkle ridges and lobate scarps associated with thrust faults. The scarps can reach lengths of 1000 km and heights of 3 km. These compressional features can be seen on top of other features, such as craters and smooth plains, indicating they are more recent. Mapping of the features has suggested a total shrinkage of Mercury's radius in the range of ~1 to 7 km. Small-scale thrust fault scarps have been found, tens of meters in height and with lengths in the range of a few km, that appear to be less than 50 million years old, indicating that compression of the interior and consequent surface geological activity continue to the present.

The Lunar Reconnaissance Orbiter discovered that similar small thrust faults exist on the Moon.

Images obtained by "MESSENGER" have revealed evidence for pyroclastic flows on Mercury from low-profile shield volcanoes. "MESSENGER" data has helped identify 51 pyroclastic deposits on the surface, where 90% of them are found within impact craters. A study of the degradation state of the impact craters that host pyroclastic deposits suggests that pyroclastic activity occurred on Mercury over a prolonged interval.

A "rimless depression" inside the southwest rim of the Caloris Basin consists of at least nine overlapping volcanic vents, each individually up to 8 km in diameter. It is thus a "compound volcano". The vent floors are at a least 1 km below their brinks and they bear a closer resemblance to volcanic craters sculpted by explosive eruptions or modified by collapse into void spaces created by magma withdrawal back down into a conduit. Scientists could not quantify the age of the volcanic complex system, but reported that it could be of the order of a billion years.

The surface temperature of Mercury ranges from at the most extreme places: 0°N, 0°W, or 180°W. It never rises above 180 K at the poles,
due to the absence of an atmosphere and a steep temperature gradient between the equator and the poles. The subsolar point reaches about 700 K during perihelion (0°W or 180°W), but only 550 K at aphelion (90° or 270°W).
On the dark side of the planet, temperatures average 110 K.
The intensity of sunlight on Mercury's surface ranges between 4.59 and 10.61 times the solar constant (1,370 W·m).

Although the daylight temperature at the surface of Mercury is generally extremely high, observations strongly suggest that ice (frozen water) exists on Mercury. The floors of deep craters at the poles are never exposed to direct sunlight, and temperatures there remain below 102 K; far lower than the global average. Water ice strongly reflects radar, and observations by the 70-meter Goldstone Solar System Radar and the VLA in the early 1990s revealed that there are patches of high radar reflection near the poles. Although ice was not the only possible cause of these reflective regions, astronomers think it was the most likely.

The icy regions are estimated to contain about 10–10 kg of ice, and may be covered by a layer of regolith that inhibits sublimation. By comparison, the Antarctic ice sheet on Earth has a mass of about 4 kg, and Mars's south polar cap contains about 10 kg of water. The origin of the ice on Mercury is not yet known, but the two most likely sources are from outgassing of water from the planet's interior or deposition by impacts of comets.

Mercury is too small and hot for its gravity to retain any significant atmosphere over long periods of time; it does have a tenuous surface-bounded exosphere containing hydrogen, helium, oxygen, sodium, calcium, potassium and others at a surface pressure of less than approximately 0.5 nPa (0.005 picobars). This exosphere is not stable—atoms are continuously lost and replenished from a variety of sources. Hydrogen atoms and helium atoms probably come from the solar wind, diffusing into Mercury's magnetosphere before later escaping back into space. Radioactive decay of elements within Mercury's crust is another source of helium, as well as sodium and potassium. "MESSENGER" found high proportions of calcium, helium, hydroxide, magnesium, oxygen, potassium, silicon and sodium. Water vapor is present, released by a combination of processes such as: comets striking its surface, sputtering creating water out of hydrogen from the solar wind and oxygen from rock, and sublimation from reservoirs of water ice in the permanently shadowed polar craters. The detection of high amounts of water-related ions like O, OH, and HO was a surprise. Because of the quantities of these ions that were detected in Mercury's space environment, scientists surmise that these molecules were blasted from the surface or exosphere by the solar wind.

Sodium, potassium and calcium were discovered in the atmosphere during the 1980–1990s, and are thought to result primarily from the vaporization of surface rock struck by micrometeorite impacts including presently from Comet Encke. In 2008, magnesium was discovered by "MESSENGER". Studies indicate that, at times, sodium emissions are localized at points that correspond to the planet's magnetic poles. This would indicate an interaction between the magnetosphere and the planet's surface.

On November 29, 2012, NASA confirmed that images from "MESSENGER" had detected that craters at the north pole contained water ice. "MESSENGER" principal investigator Sean Solomon is quoted in "The New York Times" estimating the volume of the ice to be large enough to "encase Washington, D.C., in a frozen block two and a half miles deep".

Despite its small size and slow 59-day-long rotation, Mercury has a significant, and apparently global, magnetic field. According to measurements taken by , it is about 1.1% the strength of Earth's. The magnetic-field strength at Mercury's equator is about . Like that of Earth, Mercury's magnetic field is dipolar. Unlike Earth's, Mercury's poles are nearly aligned with the planet's spin axis. Measurements from both the and "MESSENGER" space probes have indicated that the strength and shape of the magnetic field are stable.

It is likely that this magnetic field is generated by a dynamo effect, in a manner similar to the magnetic field of Earth. This dynamo effect would result from the circulation of the planet's iron-rich liquid core. Particularly strong tidal effects caused by the planet's high orbital eccentricity would serve to keep the core in the liquid state necessary for this dynamo effect.

Mercury's magnetic field is strong enough to deflect the solar wind around the planet, creating a magnetosphere. The planet's magnetosphere, though small enough to fit within Earth, is strong enough to trap solar wind plasma. This contributes to the space weathering of the planet's surface. Observations taken by the spacecraft detected this low energy plasma in the magnetosphere of the planet's nightside. Bursts of energetic particles in the planet's magnetotail indicate a dynamic quality to the planet's magnetosphere.

During its second flyby of the planet on October 6, 2008, "MESSENGER" discovered that Mercury's magnetic field can be extremely "leaky". The spacecraft encountered magnetic "tornadoes" – twisted bundles of magnetic fields connecting the planetary magnetic field to interplanetary space – that were up to wide or a third of the radius of the planet. These twisted magnetic flux tubes, technically known as flux transfer events, form open windows in the planet's magnetic shield through which the solar wind may enter and directly impact Mercury's surface via magnetic reconnection This also occurs in Earth's magnetic field. The "MESSENGER" observations showed the reconnection rate is ten times higher at Mercury, but its proximity to the Sun only accounts for about a third of the reconnection rate observed by "MESSENGER".

Mercury has the most eccentric orbit of all the planets; its eccentricity is 0.21 with its distance from the Sun ranging from . It takes 87.969 Earth days to complete an orbit. The diagram illustrates the effects of the eccentricity, showing Mercury's orbit overlaid with a circular orbit having the same semi-major axis. Mercury's higher velocity when it is near perihelion is clear from the greater distance it covers in each 5-day interval. In the diagram the varying distance of Mercury to the Sun is represented by the size of the planet, which is inversely proportional to Mercury's distance from the Sun. This varying distance to the Sun leads to Mercury's surface being flexed by tidal bulges raised by the Sun that are about 17 times stronger than the Moon's on Earth. Combined with a 3:2 spin–orbit resonance of the planet's rotation around its axis, it also results in complex variations of the surface temperature.
The resonance makes a single solar day on Mercury last exactly two Mercury years, or about 176 Earth days.

Mercury's orbit is inclined by 7 degrees to the plane of Earth's orbit (the ecliptic), as shown in the diagram on the right. As a result, transits of Mercury across the face of the Sun can only occur when the planet is crossing the plane of the ecliptic at the time it lies between Earth and the Sun, which is in May or November. This occurs about every seven years on average.

Mercury's axial tilt is almost zero, with the best measured value as low as 0.027 degrees. This is significantly smaller than that of Jupiter, which has the second smallest axial tilt of all planets at 3.1 degrees. This means that to an observer at Mercury's poles, the center of the Sun never rises more than 2.1 arcminutes above the horizon.

At certain points on Mercury's surface, an observer would be able to see the Sun peek up about halfway over the horizon, then reverse and set before rising again, all within the same Mercurian day. This is because approximately four Earth days before perihelion, Mercury's angular orbital velocity equals its angular rotational velocity so that the Sun's apparent motion ceases; closer to perihelion, Mercury's angular orbital velocity then exceeds the angular rotational velocity. Thus, to a hypothetical observer on Mercury, the Sun appears to move in a retrograde direction. Four Earth days after perihelion, the Sun's normal apparent motion resumes. A similar effect would have occurred if Mercury had been in synchronous rotation: the alternating gain and loss of rotation over revolution would have caused a libration of 23.65° in longitude.

For the same reason, there are two points on Mercury's equator, 180 degrees apart in longitude, at either of which, around perihelion in alternate Mercurian years (once a Mercurian day), the Sun passes overhead, then reverses its apparent motion and passes overhead again, then reverses a second time and passes overhead a third time, taking a total of about 16 Earth-days for this entire process. In the other alternate Mercurian years, the same thing happens at the other of these two points. The amplitude of the retrograde motion is small, so the overall effect is that, for two or three weeks, the Sun is almost stationary overhead, and is at its most brilliant because Mercury is at perihelion, its closest to the Sun. This prolonged exposure to the Sun at its brightest makes these two points the hottest places on Mercury. Maximum temperature occurs when the Sun is at an angle of about 25 degrees past noon due to diurnal temperature lag, at 0.4 Mercury days and 0.8 Mercury years past sunrise. Conversely, there are two other points on the equator, 90 degrees of longitude apart from the first ones, where the Sun passes overhead only when the planet is at aphelion in alternate years, when the apparent motion of the Sun in Mercury's sky is relatively rapid. These points, which are the ones on the equator where the apparent retrograde motion of the Sun happens when it is crossing the horizon as described in the preceding paragraph, receive much less solar heat than the first ones described above.

Mercury attains inferior conjunction (nearest approach to Earth) every 116 Earth days on average, but this interval can range from 105 days to 129 days due to the planet's eccentric orbit. Mercury can come as near as to Earth, and that is slowly declining: The next approach to within is in 2679, and to within in 4487, but it will not be closer to Earth than until 28,622. Its period of retrograde motion as seen from Earth can vary from 8 to 15 days on either side of inferior conjunction. This large range arises from the planet's high orbital eccentricity. On average, Mercury is the closest planet to the Earth, and it is the closest planet to each of the other planets in the Solar System.

The longitude convention for Mercury puts the zero of longitude at one of the two hottest points on the surface, as described above. However, when this area was first visited, by , this zero meridian was in darkness, so it was impossible to select a feature on the surface to define the exact position of the meridian. Therefore, a small crater further west was chosen, called Hun Kal, which provides the exact reference point for measuring longitude. The center of Hun Kal defines the 20° West meridian. A 1970 International Astronomical Union resolution suggests that longitudes be measured positively in the westerly direction on Mercury. The two hottest places on the equator are therefore at longitudes 0°W and 180°W, and the coolest points on the equator are at longitudes 90°W and 270°W. However, the "MESSENGER" project uses an east-positive convention.

For many years it was thought that Mercury was synchronously tidally locked with the Sun, rotating once for each orbit and always keeping the same face directed towards the Sun, in the same way that the same side of the Moon always faces Earth. Radar observations in 1965 proved that the planet has a 3:2 spin–orbit resonance, rotating three times for every two revolutions around the Sun. The eccentricity of Mercury's orbit makes this resonance stable—at perihelion, when the solar tide is strongest, the Sun is nearly still in Mercury's sky.

The rare 3:2 resonant tidal locking is stabilized by the variance of the tidal force along Mercury's eccentric orbit, acting on a permanent dipole component of Mercury's mass distribution. In a circular orbit there is no such variance, so the only resonance stabilized in such an orbit is at 1:1 (e.g., Earth–Moon), when the tidal force, stretching a body along the "center-body" line, exerts a torque that aligns the body's axis of least inertia (the "longest" axis, and the axis of the aforementioned dipole) to point always at the center. However, with noticeable eccentricity, like that of Mercury's orbit, the tidal force has a maximum at perihelion and therefore stabilizes resonances, like 3:2, enforcing that the planet points its axis of least inertia roughly at the Sun when passing through perihelion.

The original reason astronomers thought it was synchronously locked was that, whenever Mercury was best placed for observation, it was always nearly at the same point in its 3:2 resonance, hence showing the same face. This is because, coincidentally, Mercury's rotation period is almost exactly half of its synodic period with respect to Earth. Due to Mercury's 3:2 spin–orbit resonance, a solar day (the length between two meridian transits of the Sun) lasts about 176 Earth days. A sidereal day (the period of rotation) lasts about 58.7 Earth days.

Simulations indicate that the orbital eccentricity of Mercury varies chaotically from nearly zero (circular) to more than 0.45 over millions of years due to perturbations from the other planets. 
This was thought to explain Mercury's 3:2 spin–orbit resonance (rather than the more usual 1:1), because this state is more likely to arise during a period of high eccentricity. 
However, accurate modeling based on a realistic model of tidal response has demonstrated that Mercury was captured into the 3:2 spin–orbit state at a very early stage of its history, within 20 (more likely, 10) million years after its formation.

Numerical simulations show that a future secular orbital resonant perihelion interaction with Jupiter may cause the eccentricity of Mercury's orbit to increase to the point where there is a 1% chance that the planet may collide with Venus within the next five billion years.

In 1859, the French mathematician and astronomer Urbain Le Verrier reported that the slow precession of Mercury's orbit around the Sun could not be completely explained by Newtonian mechanics and perturbations by the known planets. He suggested, among possible explanations, that another planet (or perhaps instead a series of smaller 'corpuscules') might exist in an orbit even closer to the Sun than that of Mercury, to account for this perturbation. (Other explanations considered included a slight oblateness of the Sun.) The success of the search for Neptune based on its perturbations of the orbit of Uranus led astronomers to place faith in this possible explanation, and the hypothetical planet was named Vulcan, but no such planet was ever found.

The perihelion precession of Mercury is 5,600 arcseconds (1.5556°) per century relative to Earth, or 574.10±0.65 arcseconds per century relative to the inertial ICRF. Newtonian mechanics, taking into account all the effects from the other planets, predicts a precession of 5,557 arcseconds (1.5436°) per century. In the early 20th century, Albert Einstein's general theory of relativity provided the explanation for the observed precession, by formalizing gravitation as being mediated by the curvature of spacetime. The effect is small: just 42.98 arcseconds per century for Mercury; it therefore requires a little over twelve million orbits for a full excess turn. Similar, but much smaller, effects exist for other Solar System bodies: 8.62 arcseconds per century for Venus, 3.84 for Earth, 1.35 for Mars, and 10.05 for 1566 Icarus.

Einstein's formula for the perihelion shift per revolution is formula_1, where formula_2 is the orbital eccentricity, formula_3 the semi-major axis, and formula_4 the orbital period. Filling in the values gives a result of 0.1035 arcseconds per revolution or 0.4297 arcseconds per Earth year, i.e., 42.97 arcseconds per century. This is in close agreement with the accepted value of Mercury's perihelion advance of 42.98 arcseconds per century.

Mercury's apparent magnitude is calculated to vary between −2.48 (brighter than Sirius) around superior conjunction and +7.25 (below the limit of naked-eye visibility) around inferior conjunction. The mean apparent magnitude is 0.23 while the standard deviation of 1.78 is the largest of any planet. The mean apparent magnitude at superior conjunction is −1.89 while that at inferior conjunction is +5.93. Observation of Mercury is complicated by its proximity to the Sun, as it is lost in the Sun's glare for much of the time. Mercury can be observed for only a brief period during either morning or evening twilight.

Mercury can, like several other planets and the brightest stars, be seen during a total solar eclipse.

Like the Moon and Venus, Mercury exhibits phases as seen from Earth. It is "new" at inferior conjunction and "full" at superior conjunction. The planet is rendered invisible from Earth on both of these occasions because of its being obscured by the Sun, except its new phase during a transit.

Mercury is technically brightest as seen from Earth when it is at a full phase. Although Mercury is farthest from Earth when it is full, the greater illuminated area that is visible and the opposition brightness surge more than compensates for the distance. The opposite is true for Venus, which appears brightest when it is a crescent, because it is much closer to Earth than when gibbous.
Nonetheless, the brightest (full phase) appearance of Mercury is an essentially impossible time for practical observation, because of the extreme proximity of the Sun. Mercury is best observed at the first and last quarter, although they are phases of lesser brightness. The first and last quarter phases occur at greatest elongation east and west of the Sun, respectively. At both of these times Mercury's separation from the Sun ranges anywhere from 17.9° at perihelion to 27.8° at aphelion. At greatest "western" elongation, Mercury rises at its earliest before sunrise, and at greatest "eastern" elongation, it sets at its latest after sunset.

Mercury can be easily seen from the tropics and subtropics more than from higher latitudes. Viewed from low latitudes and at the right times of year, the ecliptic intersects the horizon at a steep angle. Mercury is 10° above the horizon when the planet appears directly above the Sun (i.e. its orbit appears vertical) and is at maximum elongation from the Sun (28°) and also when the Sun is 18° below the horizon, so the sky is just completely dark. This angle is the maximum altitude at which Mercury is visible in a completely dark sky.

At middle latitudes, Mercury is more often and easily visible from the Southern Hemisphere than from the Northern. This is because Mercury's maximum western elongation occurs only during early autumn in the Southern Hemisphere, whereas its greatest eastern elongation happens only during late winter in the Southern Hemisphere. In both of these cases, the angle at which the planet's orbit intersects the horizon is maximized, allowing it to rise several hours before sunrise in the former instance and not set until several hours after sundown in the latter from southern mid-latitudes, such as Argentina and South Africa.

An alternate method for viewing Mercury involves observing the planet during daylight hours when conditions are clear, ideally when it is at its greatest elongation. This allows the planet to be found easily, even when using telescopes with apertures. Care must be taken to ensure the instrument isn't pointed directly towards the Sun because of the risk for eye damage. This method bypasses the limitation of twilight observing when the ecliptic is located at a low elevation (e.g. on autumn evenings).

Ground-based telescope observations of Mercury reveal only an illuminated partial disk with limited detail. The first of two spacecraft to visit the planet was , which mapped about 45% of its surface from 1974 to 1975. The second is the "MESSENGER" spacecraft, which after three Mercury flybys between 2008 and 2009, attained orbit around Mercury on March 17, 2011, to study and map the rest of the planet.

The Hubble Space Telescope cannot observe Mercury at all, due to safety procedures that prevent its pointing too close to the Sun.

Because the shift of 0.15 revolutions in a year makes up a seven-year cycle (0.15 × 7 ≈ 1.0), in the seventh year Mercury follows almost exactly (earlier by 7 days) the sequence of phenomena it showed seven years before.

The earliest known recorded observations of Mercury are from the Mul.Apin tablets. These observations were most likely made by an Assyrian astronomer around the 14th century BC. The cuneiform name used to designate Mercury on the Mul.Apin tablets is transcribed as Udu.Idim.Gu\u.Ud ("the jumping planet"). Babylonian records of Mercury date back to the 1st millennium BC. The Babylonians called the planet Nabu after the messenger to the gods in their mythology.

The ancient Greeks knew the planet as Στίλβων ("Stilbon"), meaning "the gleaming", Ἑρμάων ("Hermaon") and Ἑρμής ("Hermes"), a planetary name that is retained in modern Greek (Ερμής: "Ermis"). The Romans named the planet after the swift-footed Roman messenger god, Mercury (Latin "Mercurius"), which they equated with the Greek Hermes, because it moves across the sky faster than any other planet. The astronomical symbol for Mercury is a stylized version of Hermes' caduceus.

The Greco-Egyptian astronomer Ptolemy wrote about the possibility of planetary transits across the face of the Sun in his work "Planetary Hypotheses". He suggested that no transits had been observed either because planets such as Mercury were too small to see, or because the transits were too infrequent.

In ancient China, Mercury was known as "the Hour Star" ("Chen-xing" ). It was associated with the direction north and the phase of water in the Five Phases system of metaphysics. Modern Chinese, Korean, Japanese and Vietnamese cultures refer to the planet literally as the "water star" (), based on the Five elements. Hindu mythology used the name Budha for Mercury, and this god was thought to preside over Wednesday. The god Odin (or Woden) of Germanic paganism was associated with the planet Mercury and Wednesday. The Maya may have represented Mercury as an owl (or possibly four owls; two for the morning aspect and two for the evening) that served as a messenger to the underworld.

In medieval Islamic astronomy, the Andalusian astronomer Abū Ishāq Ibrāhīm al-Zarqālī in the 11th century described the deferent of Mercury's geocentric orbit as being oval, like an egg or a pignon, although this insight did not influence his astronomical theory or his astronomical calculations. In the 12th century, Ibn Bajjah observed "two planets as black spots on the face of the Sun", which was later suggested as the transit of Mercury and/or Venus by the Maragha astronomer Qotb al-Din Shirazi in the 13th century. (Note that most such medieval reports of transits were later taken as observations of sunspots.)

In India, the Kerala school astronomer Nilakantha Somayaji in the 15th century developed a partially heliocentric planetary model in which Mercury orbits the Sun, which in turn orbits Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century.

The first telescopic observations of Mercury were made by Galileo in the early 17th century. Although he observed phases when he looked at Venus, his telescope was not powerful enough to see the phases of Mercury. In 1631, Pierre Gassendi made the first telescopic observations of the transit of a planet across the Sun when he saw a transit of Mercury predicted by Johannes Kepler. In 1639, Giovanni Zupi used a telescope to discover that the planet had orbital phases similar to Venus and the Moon. The observation demonstrated conclusively that Mercury orbited around the Sun.

A rare event in astronomy is the passage of one planet in front of another (occultation), as seen from Earth. Mercury and Venus occult each other every few centuries, and the event of May 28, 1737 is the only one historically observed, having been seen by John Bevis at the Royal Greenwich Observatory. The next occultation of Mercury by Venus will be on December 3, 2133.

The difficulties inherent in observing Mercury mean that it has been far less studied than the other planets. In 1800, Johann Schröter made observations of surface features, claiming to have observed mountains. Friedrich Bessel used Schröter's drawings to erroneously estimate the rotation period as 24 hours and an axial tilt of 70°. In the 1880s, Giovanni Schiaparelli mapped the planet more accurately, and suggested that Mercury's rotational period was 88 days, the same as its orbital period due to tidal locking. This phenomenon is known as synchronous rotation. The effort to map the surface of Mercury was continued by Eugenios Antoniadi, who published a book in 1934 that included both maps and his own observations. Many of the planet's surface features, particularly the albedo features, take their names from Antoniadi's map.

In June 1962, Soviet scientists at the Institute of Radio-engineering and Electronics of the USSR Academy of Sciences, led by Vladimir Kotelnikov, became the first to bounce a radar signal off Mercury and receive it, starting radar observations of the planet. Three years later, radar observations by Americans Gordon Pettengill and R. Dyce, using the 300-meter Arecibo Observatory radio telescope in Puerto Rico, showed conclusively that the planet's rotational period was about 59 days. The theory that Mercury's rotation was synchronous had become widely held, and it was a surprise to astronomers when these radio observations were announced. If Mercury were tidally locked, its dark face would be extremely cold, but measurements of radio emission revealed that it was much hotter than expected. Astronomers were reluctant to drop the synchronous rotation theory and proposed alternative mechanisms such as powerful heat-distributing winds to explain the observations.

Italian astronomer Giuseppe Colombo noted that the rotation value was about two-thirds of Mercury's orbital period, and proposed that the planet's orbital and rotational periods were locked into a 3:2 rather than a 1:1 resonance. Data from subsequently confirmed this view. This means that Schiaparelli's and Antoniadi's maps were not "wrong". Instead, the astronomers saw the same features during every "second" orbit and recorded them, but disregarded those seen in the meantime, when Mercury's other face was toward the Sun, because the orbital geometry meant that these observations were made under poor viewing conditions.

Ground-based optical observations did not shed much further light on Mercury, but radio astronomers using interferometry at microwave wavelengths, a technique that enables removal of the solar radiation, were able to discern physical and chemical characteristics of the subsurface layers to a depth of several meters. Not until the first space probe flew past Mercury did many of its most fundamental morphological properties become known. Moreover, recent technological advances have led to improved ground-based observations. In 2000, high-resolution lucky imaging observations were conducted by the Mount Wilson Observatory 1.5 meter Hale telescope. They provided the first views that resolved surface features on the parts of Mercury that were not imaged in the mission. Most of the planet has been mapped by the Arecibo radar telescope, with resolution, including polar deposits in shadowed craters of what may be water ice.

Reaching Mercury from Earth poses significant technical challenges, because it orbits so much closer to the Sun than Earth. A Mercury-bound spacecraft launched from Earth must travel over into the Sun's gravitational potential well. Mercury has an orbital speed of , whereas Earth's orbital speed is . Therefore, the spacecraft must make a large change in velocity (delta-v) to enter a Hohmann transfer orbit that passes near Mercury, as compared to the delta-v required for other planetary missions.

The potential energy liberated by moving down the Sun's potential well becomes kinetic energy; requiring another large delta-v change to do anything other than rapidly pass by Mercury. To land safely or enter a stable orbit the spacecraft would rely entirely on rocket motors. Aerobraking is ruled out because Mercury has a negligible atmosphere. A trip to Mercury requires more rocket fuel than that required to escape the Solar System completely. As a result, only two space probes have visited it so far. A proposed alternative approach would use a solar sail to attain a Mercury-synchronous orbit around the Sun.

The first spacecraft to visit Mercury was NASA's (1974–1975). The spacecraft used the gravity of Venus to adjust its orbital velocity so that it could approach Mercury, making it both the first spacecraft to use this gravitational "slingshot" effect and the first NASA mission to visit multiple planets. provided the first close-up images of Mercury's surface, which immediately showed its heavily cratered nature, and revealed many other types of geological features, such as the giant scarps that were later ascribed to the effect of the planet shrinking slightly as its iron core cools. Unfortunately, the same face of the planet was lit at each of close approaches. This made close observation of both sides of the planet impossible, and resulted in the mapping of less than 45% of the planet's surface.

The spacecraft made three close approaches to Mercury, the closest of which took it to within of the surface. At the first close approach, instruments detected a magnetic field, to the great surprise of planetary geologists—Mercury's rotation was expected to be much too slow to generate a significant dynamo effect. The second close approach was primarily used for imaging, but at the third approach, extensive magnetic data were obtained. The data revealed that the planet's magnetic field is much like Earth's, which deflects the solar wind around the planet. For many years after the encounters, the origin of Mercury's magnetic field remained the subject of several competing theories.

On March 24, 1975, just eight days after its final close approach, ran out of fuel. Because its orbit could no longer be accurately controlled, mission controllers instructed the probe to shut down. is thought to be still orbiting the Sun, passing close to Mercury every few months.

A second NASA mission to Mercury, named "MESSENGER" (MErcury Surface, Space ENvironment, GEochemistry, and Ranging), was launched on August 3, 2004. It made a fly-by of Earth in August 2005, and of Venus in October 2006 and June 2007 to place it onto the correct trajectory to reach an orbit around Mercury. A first fly-by of Mercury occurred on January 14, 2008, a second on October 6, 2008, and a third on September 29, 2009. Most of the hemisphere not imaged by was mapped during these fly-bys. The probe successfully entered an elliptical orbit around the planet on March 18, 2011. The first orbital image of Mercury was obtained on March 29, 2011. The probe finished a one-year mapping mission, and then entered a one-year extended mission into 2013. In addition to continued observations and mapping of Mercury, "MESSENGER" observed the 2012 solar maximum.

The mission was designed to clear up six key issues: Mercury's high density, its geological history, the nature of its magnetic field, the structure of its core, whether it has ice at its poles, and where its tenuous atmosphere comes from. To this end, the probe carried imaging devices that gathered much-higher-resolution images of much more of Mercury than , assorted spectrometers to determine abundances of elements in the crust, and magnetometers and devices to measure velocities of charged particles. Measurements of changes in the probe's orbital velocity were expected to be used to infer details of the planet's interior structure. "MESSENGER" final maneuver was on April 24, 2015, and it crashed into Mercury's surface on April 30, 2015. The spacecraft's impact with Mercury occurred near 3:26 PM EDT on April 30, 2015, leaving a crater estimated to be in diameter.

The European Space Agency and the Japanese Space Agency developed and launched a joint mission called "BepiColombo", which will orbit Mercury with two probes: one to map the planet and the other to study its magnetosphere. Launched on October 20, 2018, "BepiColombo" is expected to reach Mercury in 2025. It will release a magnetometer probe into an elliptical orbit, then chemical rockets will fire to deposit the mapper probe into a circular orbit. Both probes will operate for one terrestrial year. The mapper probe carries an array of spectrometers similar to those on "MESSENGER", and will study the planet at many different wavelengths including infrared, ultraviolet, X-ray and gamma ray.




</doc>
<doc id="19701" url="https://en.wikipedia.org/wiki?curid=19701" title="Monty Python and the Holy Grail">
Monty Python and the Holy Grail

Monty Python and the Holy Grail is a 1975 British comedy film concerning the Arthurian legend, written and performed by the Monty Python comedy group of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones and Michael Palin, and directed by Gilliam and Jones. It was conceived during the hiatus between the third and fourth series of their BBC television series "Monty Python's Flying Circus".

While the group's first film, "And Now for Something Completely Different", was a compilation of sketches from the first two television series, "Holy Grail" is a new story that parodies the legend of King Arthur's quest for the Holy Grail. Thirty years later, Idle used the film as the basis for the musical "Spamalot".

"Monty Python and the Holy Grail" grossed more than any other British film exhibited in the US in 1975. In the US, it was selected as the second-best comedy of all time in the ABC special "". In the UK, readers of "Total Film" magazine in 2000 ranked it the fifth-greatest comedy film of all time; a similar poll of Channel 4 viewers in 2006 placed it sixth.

In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours. Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is "a silly place". As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.

Searching the land for clues to the Grail's location, Arthur and his men come to a castle occupied by rude French soldiers who claim to have the Grail and insult the Englishmen. After being driven back by a shower of catapulted barnyard animals, Bedevere comes up with a plan to sneak in using a Trojan Rabbit, but they mishandle its execution when they forget to hide inside it, and are forced to flee when the Rabbit is flung back at them. Arthur decides that the knights should go their separate ways to search for clues to the Grail's whereabouts. A modern-day historian filming a documentary describing the Arthurian legends is abruptly killed by an unknown knight on horseback, triggering a police investigation.

On the knights' travels, Arthur and Bedevere are given directions by an old man in Scene 24 and attempt to satisfy the strange requests of the dreaded Knights Who Say "Ni!". Sir Robin avoids a fight with a Three-Headed Knight by running away while the heads are arguing. Sir Galahad is led by a grail-shaped beacon to Castle Anthrax, populated by 160 nubile young women, but to his chagrin is "rescued" by Lancelot. Lancelot, after receiving an arrow-shot note from Swamp Castle believed to be from a lady being forced to marry against her will, rushes to the castle and slaughters nearly the entire wedding party, only to discover that the note was sent by an effeminate prince whose father had arranged the marriage.

Arthur and his knights regroup, and are joined by three new knights, as well as Brother Maynard and his monk brethren. They meet Tim the Enchanter, who directs them to a cave where the location of the Grail is said to be written. The entrance to the cave is guarded by the Rabbit of Caerbannog. Fatally underestimating its lethal prowess, the knights attack the Rabbit, which easily kills Sirs Bors, Gawain and Ector. Arthur uses the "Holy Hand Grenade of Antioch", provided by Brother Maynard, to destroy the creature. Inside the cave, they find an inscription from Joseph of Arimathea, directing them to the Castle of Aarrgh (the name presumably the result of Joseph dying whilst carving the inscription). 

After barely escaping a giant animated monster that devours Brother Maynard (the animator has a heart attack, which ends the beast's existence), they arrive at the Gorge of Eternal Peril. As they approach the Bridge of Death, the man from Scene 24 challenges them to correctly answer three questions in order to pass - or be cast into the Gorge. Lancelot goes first, easily answers three simple questions and crosses over. Robin is surprised by an unexpectedly difficult question, and Galahad misses the answer to an easy one, and both are flung off the bridge. During Arthur's turn, he responds to the bridge-keeper's last question with a question of his own which stumps the bridge-keeper. As a result, the man from Scene 24 is thrown into the gorge, and Arthur and Bedevere cross.

When Arthur and Bedevere reach the bridge's end, they cannot find Lancelot, unaware he has been arrested by the police investigating the historian's death. Arthur and Bedevere eventually reach the Castle of Aarrgh, only to find it occupied by the same French soldiers who taunted them before. After being driven to retreat by showers of manure, they amass a large army of knights and prepare to assault the castle. Just as they are charging in to attack, a large force of police shows up, arrests Arthur and Bedevere for the historian's death and shuts down the film's production. The movie ends with one of the officers breaking the camera.

Fifteen months before the BBC visited the set in May 1974, the Monty Python troupe assembled the first version of the screenplay. When half of the resulting material was set in the Middle Ages, and half was set in the present day, the group opted to focus on the Middle Ages, revolving on the legend of the Holy Grail. By the fourth or fifth version of their screenplay, the story was complete, and the cast joked the fact that the Grail was never retrieved would be "a big let-down ... a great anti-climax". Graham Chapman said a challenge was incorporating scenes that did not fit the Holy Grail motif.

Neither Terry Gilliam nor Terry Jones had directed a film before, and described it as a learning experience in which they would learn to make a film by making an entire full-length film. The cast humorously described the novice directing style as employing the level of mutual disrespect always found in Monty Python's work.

The film's initial budget of approximately £200,000 was raised by convincing 10 separate investors to contribute £20,000 apiece. Three of those investors were the rock bands Pink Floyd, Led Zeppelin and Genesis, who were persuaded to help fund the film by Tony Stratton-Smith, head of Charisma Records (the record label that released Python's early comedy albums). According to Terry Gilliam, the Pythons turned to rock stars like Pink Floyd, Led Zeppelin and Elton John for finance as the studios refused to fund the film and rock stars saw it as "a good tax write-off" due to UK income tax being "as high as 90%" at the time.

"Monty Python and the Holy Grail" was mostly shot on location in Scotland, particularly around Doune Castle, Glen Coe, and the privately owned Castle Stalker. The many castles seen throughout the film were mainly either Doune Castle shot from different angles or hanging miniatures. There are several exceptions to this: the very first exterior shot of a castle at the beginning of the film is Kidwelly Castle in South Wales, and the single exterior shot of the Swamp Castle during "Tale of Sir Lancelot" is Bodiam Castle in East Sussex; all subsequent shots of the exterior and interior of those scenes were filmed at Doune Castle. Production designer Julian Doyle recounted that his crew constructed walls in the forest near Doune. Terry Jones later recalled the crew had selected more castles around Scotland for locations, but during the two weeks prior to principal photography, the Scottish Department of the Environment declined permission for use of the castles in its jurisdiction, for fear of damage.

At the start of "The Tale of Sir Robin", there is a slow camera zoom in on rocky scenery (that in the voice-over is described as "the dark forest of Ewing"). This is actually a still photograph of the gorge at Mount Buffalo National Park in Victoria, Australia. Doyle stated in 2000 during an interview with "Hotdog" magazine that it was a still image filmed with candles underneath the frame (to give a heat haze). This was a low-cost method of achieving a convincing location effect.
On the DVD audio commentary, Cleese described challenges shooting and editing Castle Anthrax in "The Tale of Sir Galahad", with what he felt the most comedic take being unused because an anachronistic coat was visible in it. Castle Anthrax was also shot in one part of Doune, where costume designer Hazel Pethig advised against nudity, dressing the girls in shifts.

In the scene where the knights were combatting the Rabbit of Caerbannog, a real white rabbit was used, switched with puppets for its killings. It was covered with red liquid to simulate blood, though the rabbit's owner did not want the animal dirty and was kept unaware. The liquid was difficult to remove from the fur. He also stated that he thought that, had they been more experienced in filmmaking, the crew would have just purchased a rabbit instead. Otherwise, the rabbit himself was unharmed. Also, the rabbit-bite effects were done by special puppetry by both Gilliam and SFX technician John Horton.

As chronicled in "The Life of Python", "The First 20 Years of Monty Python", and "The Pythons' Autobiography", it was revealed that Chapman was suffering from acrophobia, trembling, and bouts of forgetfulness during filming. These were the results of Chapman's long-standing alcohol addiction, and he decided from that moment on to remain "on an even keel" while the production continued. Nearly three years after "Holy Grail", Chapman vowed to quit drinking altogether (which he successfully achieved in December 1977).

Originally the knight characters were going to ride real horses, but after it became clear that the film's small budget precluded real horses (except for a lone horse appearing in a couple of scenes), the Pythons decided that their characters would mime horse-riding while their porters trotted behind them banging coconut shells together. The joke was derived from the old-fashioned sound effect used by radio shows to convey the sound of hooves clattering. This was later referred to in the German release of the film, which translated the title as "Die Ritter der Kokosnuß" ("The Knights of the Coconut").

The opening credits of the film feature pseudo-Swedish subtitles, which soon turn into an appeal to visit Sweden and see the country's moose. The subtitles are soon stopped, but moose references continue throughout the actual credits until the credits are stopped again and restarted in a different visual style and with references to llamas, animals often mentioned in "Flying Circus". The subtitles were written by Michael Palin as a way to "entertain the 'captive' audience" at the beginning of the film.

In addition to several songs written by Python regular Neil Innes, several pieces of music were licensed from De Wolfe Music Library. These include:

"Monty Python and the Holy Grail" had its theatrical debut in the United Kingdom in 3 April 1975, followed by a United States release on 27 April 1975. It was re-released on 14 October 2015 in the United Kingdom.

The film had its television premiere 25 February 1977 on the "CBS Late Movie". Reportedly, the Pythons were displeased to discover a number of edits were done by the network to reduce use of profanity and the showing of blood. The troupe pulled back the rights and thereafter had it broadcast in the United States only on PBS and later other channels such as Comedy Central and IFC, where it runs uncut.

In Region 1, The Criterion Collection released a LaserDisc version of the film featuring audio commentary from directors Jones and Gilliam. In 2001, Columbia Tristar published a two-disc, special-edition DVD. Disc one included two commentary tracks featuring Idle, Palin and Cleese in the first, and Jones and Gilliam in the second, and "Subtitles for People Who Don't Like the Film", consisting of lines taken from William Shakespeare's "Henry IV, Part 2". Disc two includes "Monty Python and the Holy Grail in Lego" (also known as "Lego Knights" or "It's Only a Model"), a "brickfilm" version of the "Camelot Song" as sung by Lego minifigures. It was created by Spite Your Face Productions on commission from the Lego Group and Python Pictures. The project was conceived by the original film's respective producer and co-director, John Goldstone and Terry Gilliam. Disc two also includes two scenes from the film's Japanese dub, literally translated back into English through subtitles. "The Quest for the Holy Grail Locations", hosted by Palin and Jones, shows places in Scotland used for the setting titled as "England 932 A.D." (as well as the two Pythons purchasing a copy of their own script as a guide). Also included is a who's who page, advertising galleries and sing-alongs.

A limited-edition DVD release additionally included a copy of the screenplay, a limited-edition film cell/senitype, and limited-edition art cards; however, a few of the bonus features from the 'Extraordinarily Deluxe Edition' were omitted. A 35th-anniversary edition on Blu-ray was released in the US on 6 March 2012. Special features include "The Holy Book of Days," a second-screen experience that can be downloaded as an app on an iOS device and played with the Blu-ray to enhance its viewing, lost animation sequences with a new intro from animator Terry Gilliam, outtakes and extended scenes with Python member and the movie's co-director Terry Jones.

Contemporary reviews were mixed. Vincent Canby of "The New York Times" wrote in a favourable review that the film had "some low spots," but had gags which were "nonstop, occasionally inspired and should not be divulged, though it's not giving away too much to say that I particularly liked a sequence in which the knights, to gain access to an enemy castle, come up with the idea of building a Trojan rabbit." Charles Champlin of the "Los Angeles Times" was also positive, writing that the film, "like "Mad" comics, is not certain to please every taste. But its youthful exuberance and its rousing zaniness are hard not to like. As a matter of fact, the sense of fun is dangerously contagious." Penelope Gilliatt of "The New Yorker" called the film "often recklessly funny and sometimes a matter of comic genius."

Other reviews were less enthusiastic. "Variety" wrote that the storyline was "basically an excuse for set pieces, some amusing, others overdone." Gene Siskel of the "Chicago Tribune" gave the film two-and-a-half stars, writing that he felt "it contained about 10 very funny moments and 70 minutes of silence. Too many of the jokes took too long to set up, a trait shared by both "Blazing Saddles" and "Young Frankenstein". I guess I prefer Monty Python in chunks, in its original, television revue format." Gary Arnold of "The Washington Post" called the film "a fitfully amusing spoof of the Arthurian legends" but "rather poky" in tempo, citing the running gag of Swedish subtitles in the opening credits as an example of how the Pythons "don't know when to let go of any "shtik"". Geoff Brown of "The Monthly Film Bulletin" wrote in a mixed review that "the team's visual buffooneries and verbal rigamaroles (some good, some bad, but mostly indifferent) are piled on top of each other with no attention to judicious timing or structure, and a form which began as a jaunty assault on the well-made revue sketch and an ingenious misuse of television's fragmented style of presentation, threatens to become as unyielding and unfruitful as the conventions it originally attacked."

The film's reputation grew over time. In 2000, readers of "Total Film" magazine voted "Holy Grail" the fifth-greatest comedy film of all time. The next Python film, "Life of Brian", was ranked first. A 2006 poll of Channel 4 viewers on the 50 Greatest Comedy Films saw "Holy Grail" placed in sixth place (with "Life of Brian" again topping the list). In 2011, an ABC prime-time special, "", counted down the best films chosen by fans based on results of a poll conducted by ABC and "People". "Holy Grail" was selected as the second best comedy after "Airplane!". In 2016, "Empire" magazine ranked "Holy Grail" 18th in their list of the 100 best British films ("Life of Brian" was ranked 2nd), with their entry stating, "Elvis ordered a print of this comedy classic and watched it five times. If it's good enough for the King, it's good enough for you."

In a 2017 interview at Indiana University in Bloomington, John Cleese expressed disappointment with the film's conclusion. "'The ending annoys me the most'", he said after a screening of the film on the Indiana campus, adding that "'It ends the way it does because we couldn't think of any other way'". However, scripts for the film and notebooks that are among Michael Palin's private archive, which he donated to the British Library in 2017, do document at least one alternate ending that the troupe considered: "a battle between the knights of Camelot, the French, and the Killer Rabbit of Caerbannog". Due to the film's small production budget, that idea or a "much pricier option" was discarded by the Pythons in favour of the ending with "King Arthur getting arrested", which Palin deemed "'cheaper'" and "'funnier'".

In 2005, the film was adapted as a Tony Award-winning Broadway musical, "Spamalot". Written primarily by Idle, the show has more of an overarching plot and leaves out certain portions of the movie due to difficulties in rendering certain effects on stage. Nonetheless, many of the jokes from the film are present in the show.

In 2013, the Pythons lost a legal case to Mark Forstater, the film's producer, over royalties for the derivative work, "Spamalot". They owed a combined £800,000 in legal fees and back royalties to Forstater. To help cover the cost of these royalties and fees, the group arranged and performed in a stage show, "Monty Python Live (Mostly)", held at the O Arena in London in July 2014.





</doc>
<doc id="19702" url="https://en.wikipedia.org/wiki?curid=19702" title="Mutation">
Mutation

In biology, a mutation is the alteration of the nucleotide sequence of the genome of an organism, virus, or extrachromosomal DNA.

Mutations result from errors during DNA replication, mitosis, and meiosis, or other types of damage to DNA (such as pyrimidine dimers that may be caused by exposure to radiation or carcinogens), which then may undergo error-prone repair (especially microhomology-mediated end joining), or cause an error during other forms of repair, or else may cause an error during replication (translesion synthesis). Mutations may also result from insertion or deletion of segments of DNA due to mobile genetic elements. Mutations may or may not produce discernible changes in the observable characteristics (phenotype) of an organism. Mutations play a part in both normal and abnormal biological processes including: evolution, cancer, and the development of the immune system, including junctional diversity.

The genomes of RNA viruses are based on RNA rather than DNA. The RNA viral genome can be double-stranded (as in DNA) or single-stranded. In some of these viruses (such as the single-stranded human immunodeficiency virus) replication occurs quickly and there are no mechanisms to check the genome for accuracy. This error-prone process often results in mutations.

Mutation can result in many different types of change in sequences. Mutations in genes can either have no effect, alter the product of a gene, or prevent the gene from functioning properly or completely. Mutations can also occur in nongenic regions. One study on genetic variations between different species of "Drosophila" suggests that, if a mutation changes a protein produced by a gene, the result is likely to be harmful, with an estimated 70 percent of amino acid polymorphisms that have damaging effects, and the remainder being either neutral or marginally beneficial. Due to the damaging effects that mutations can have on genes, organisms have mechanisms such as DNA repair to prevent or correct mutations by reverting the mutated sequence back to its original state.

Mutations can involve the duplication of large sections of DNA, usually through genetic recombination. These duplications are a major source of raw material for evolving new genes, with tens to hundreds of genes duplicated in animal genomes every million years. Most genes belong to larger gene families of shared ancestry, detectable by their sequence homology. Novel genes are produced by several methods, commonly through the duplication and mutation of an ancestral gene, or by recombining parts of different genes to form new combinations with new functions.

Here, protein domains act as modules, each with a particular and independent function, that can be mixed together to produce genes encoding new proteins with novel properties. For example, the human eye uses four genes to make structures that sense light: three for cone cell or color vision and one for rod cell or night vision; all four arose from a single ancestral gene. Another advantage of duplicating a gene (or even an entire genome) is that this increases engineering redundancy; this allows one gene in the pair to acquire a new function while the other copy performs the original function. Other types of mutation occasionally create new genes from previously noncoding DNA.

Changes in chromosome number may involve even larger mutations, where segments of the DNA within chromosomes break and then rearrange. For example, in the Homininae, two chromosomes fused to produce human chromosome 2; this fusion did not occur in the lineage of the other apes, and they retain these separate chromosomes. In evolution, the most important role of such chromosomal rearrangements may be to accelerate the divergence of a population into new species by making populations less likely to interbreed, thereby preserving genetic differences between these populations.

Sequences of DNA that can move about the genome, such as transposons, make up a major fraction of the genetic material of plants and animals, and may have been important in the evolution of genomes. For example, more than a million copies of the Alu sequence are present in the human genome, and these sequences have now been recruited to perform functions such as regulating gene expression. Another effect of these mobile DNA sequences is that when they move within a genome, they can mutate or delete existing genes and thereby produce genetic diversity.

Nonlethal mutations accumulate within the gene pool and increase the amount of genetic variation. The abundance of some genetic changes within the gene pool can be reduced by natural selection, while other "more favorable" mutations may accumulate and result in adaptive changes.
For example, a butterfly may produce offspring with new mutations. The majority of these mutations will have no effect; but one might change the color of one of the butterfly's offspring, making it harder (or easier) for predators to see. If this color change is advantageous, the chances of this butterfly's surviving and producing its own offspring are a little better, and over time the number of butterflies with this mutation may form a larger percentage of the population.

Neutral mutations are defined as mutations whose effects do not influence the fitness of an individual. These can increase in frequency over time due to genetic drift. It is believed that the overwhelming majority of mutations have no significant effect on an organism's fitness. Also, DNA repair mechanisms are able to mend most changes before they become permanent mutations, and many organisms have mechanisms for eliminating otherwise-permanently mutated somatic cells.

Beneficial mutations can improve reproductive success.

Mutationism is one of several alternatives to evolution by natural selection that have existed both before and after the publication of Charles Darwin's 1859 book, "On the Origin of Species". In the theory, mutation was the source of novelty, creating new forms and new species, potentially instantaneously, in a sudden jump. This was envisaged as driving evolution, which was limited by the supply of mutations.

Before Darwin, biologists commonly believed in saltationism, the possibility of large evolutionary jumps, including immediate speciation. For example, in 1822 Étienne Geoffroy Saint-Hilaire argued that species could be formed by sudden transformations, or what would later be called macromutation. Darwin opposed saltation, insisting on gradualism in evolution as in geology. In 1864, Albert von Kölliker revived Geoffroy's theory. In 1901 the geneticist Hugo de Vries gave the name "mutation" to seemingly new forms that suddenly arose in his experiments on the evening primrose "Oenothera lamarckiana", and in the first decade of the 20th century, mutationism, or as de Vries named it "mutationstheorie", became a rival to Darwinism supported for a while by geneticists including William Bateson, Thomas Hunt Morgan, and Reginald Punnett.

Understanding of mutationism is clouded by the mid-20th century portrayal of the early mutationists by supporters of the modern synthesis as opponents of Darwinian evolution and rivals of the biometrics school who argued that selection operated on continuous variation. In this portrayal, mutationism was defeated by a synthesis of genetics and natural selection that supposedly started later, around 1918, with work by the mathematician Ronald Fisher. However, the alignment of Mendelian genetics and natural selection began as early as 1902 with a paper by Udny Yule, and built up with theoretical and experimental work in Europe and America. Despite the controversy, the early mutationists had by 1918 already accepted natural selection and explained continuous variation as the result of multiple genes acting on the same characteristic, such as height.

Mutationism, along with other alternatives to Darwinism like Lamarckism and orthogenesis, was discarded by most biologists as they came to see that Mendelian genetics and natural selection could readily work together; mutation took its place as a source of the genetic variation essential for natural selection to work on. However, mutationism did not entirely vanish. In 1940, Richard Goldschmidt again argued for single-step speciation by macromutation, describing the organisms thus produced as "hopeful monsters", earning widespread ridicule. In 1987, Masatoshi Nei argued controversially that evolution was often mutation-limited. Modern biologists such as Douglas J. Futuyma conclude that essentially all claims of evolution driven by large mutations can be explained by Darwinian evolution.

Four classes of mutations are (1) spontaneous mutations (molecular decay), (2) mutations due to error-prone replication bypass of naturally occurring DNA damage (also called error-prone translesion synthesis), (3) errors introduced during DNA repair, and (4) induced mutations caused by mutagens. Scientists may also deliberately introduce mutant sequences through DNA manipulation for the sake of scientific experimentation.

One 2017 study claimed that 66% of cancer-causing mutations are random, 29% are due to the environment (the studied population spanned 69 countries), and 5% are inherited.

Humans on average pass 60 new mutations to their children but fathers pass more mutations depending on their age with every year adding two new mutations to a child.

"Spontaneous mutations" occur with non-zero probability even given a healthy, uncontaminated cell. They can be characterized by the specific change:


There is increasing evidence that the majority of spontaneously arising mutations are due to error-prone replication (translesion synthesis) past DNA damage in the template strand. Naturally occurring oxidative DNA damages arise at least 10,000 times per cell per day in humans and 50,000 times or more per cell per day in rats. In mice, the majority of mutations are caused by translesion synthesis. Likewise, in yeast, Kunz et al. found that more than 60% of the spontaneous single base pair substitutions and deletions were caused by translesion synthesis.

Although naturally occurring double-strand breaks occur at a relatively low frequency in DNA, their repair often causes mutation. Non-homologous end joining (NHEJ) is a major pathway for repairing double-strand breaks. NHEJ involves removal of a few nucleotides to allow somewhat inaccurate alignment of the two ends for rejoining followed by addition of nucleotides to fill in gaps. As a consequence, NHEJ often introduces mutations.

Induced mutations are alterations in the gene after it has come in contact with mutagens and environmental causes.

"Induced mutations" on the molecular level can be caused by:

Whereas in former times mutations were assumed to occur by chance, or induced by mutagens, molecular mechanisms of mutation have been discovered in bacteria and across the tree of life. As S. Rosenberg states, "These mechanisms reveal a picture of highly regulated mutagenesis, up-regulated temporally by stress responses and activated when cells/organisms are maladapted to their environments—when stressed—potentially accelerating adaptation." Since they are self-induced mutagenic mechanisms that increase the adaptation rate of organisms, they have some times been named as adaptive mutagenesis mechanisms, and include the SOS response in bacteria, ectopic intrachromosomal recombination and other chromosomal events such as duplications.

The sequence of a gene can be altered in a number of ways. Gene mutations have varying effects on health depending on where they occur and whether they alter the function of essential proteins.
Mutations in the structure of genes can be classified into several types.

Small-scale mutations affect a gene in one or a few nucleotides. (If only a single nucleotide is affected, they are called point mutations.) Small-scale mutations include:


Large-scale mutations in chromosomal structure include:


In applied genetics, it is usual to speak of mutations as either harmful or beneficial.

Attempts have been made to infer the distribution of fitness effects (DFE) using mutagenesis experiments and theoretical models applied to molecular sequence data. DFE, as used to determine the relative abundance of different types of mutations (i.e., strongly deleterious, nearly neutral or advantageous), is relevant to many evolutionary questions, such as the maintenance of genetic variation, the rate of genomic decay, the maintenance of outcrossing sexual reproduction as opposed to inbreeding and the evolution of sex and genetic recombination. DFE can also be tracked by tracking the skewness of the distribution of mutations with putatively severe effects as compared to the distribution of mutations with putatively mild or absent effect. In summary, the DFE plays an important role in predicting evolutionary dynamics. A variety of approaches have been used to study the DFE, including theoretical, experimental and analytical methods.



One of the earliest theoretical studies of the distribution of fitness effects was done by Motoo Kimura, an influential theoretical population geneticist. His neutral theory of molecular evolution proposes that most novel mutations will be highly deleterious, with a small fraction being neutral. Hiroshi Akashi more recently proposed a bimodal model for the DFE, with modes centered around highly deleterious and neutral mutations. Both theories agree that the vast majority of novel mutations are neutral or deleterious and that advantageous mutations are rare, which has been supported by experimental results. One example is a study done on the DFE of random mutations in vesicular stomatitis virus. Out of all mutations, 39.6% were lethal, 31.2% were non-lethal deleterious, and 27.1% were neutral. Another example comes from a high throughput mutagenesis experiment with yeast. In this experiment it was shown that the overall DFE is bimodal, with a cluster of neutral mutations, and a broad distribution of deleterious mutations.

Though relatively few mutations are advantageous, those that are play an important role in evolutionary changes. Like neutral mutations, weakly selected advantageous mutations can be lost due to random genetic drift, but strongly selected advantageous mutations are more likely to be fixed. Knowing the DFE of advantageous mutations may lead to increased ability to predict the evolutionary dynamics. Theoretical work on the DFE for advantageous mutations has been done by John H. Gillespie and H. Allen Orr. They proposed that the distribution for advantageous mutations should be exponential under a wide range of conditions, which, in general, has been supported by experimental studies, at least for strongly selected advantageous mutations.

In general, it is accepted that the majority of mutations are neutral or deleterious, with advantageous mutations being rare; however, the proportion of types of mutations varies between species. This indicates two important points: first, the proportion of effectively neutral mutations is likely to vary between species, resulting from dependence on effective population size; second, the average effect of deleterious mutations varies dramatically between species. In addition, the DFE also differs between coding regions and noncoding regions, with the DFE of noncoding DNA containing more weakly selected mutations.


In multicellular organisms with dedicated reproductive cells, mutations can be subdivided into germline mutations, which can be passed on to descendants through their reproductive cells, and somatic mutations (also called acquired mutations), which involve cells outside the dedicated reproductive group and which are not usually transmitted to descendants.

A germline mutation gives rise to a "constitutional mutation" in the offspring, that is, a mutation that is present in every cell. A constitutional mutation can also occur very soon after fertilisation, or continue from a previous constitutional mutation in a parent.

The distinction between germline and somatic mutations is important in animals that have a dedicated germline to produce reproductive cells. However, it is of little value in understanding the effects of mutations in plants, which lack a dedicated germline. The distinction is also blurred in those animals that reproduce asexually through mechanisms such as budding, because the cells that give rise to the daughter organisms also give rise to that organism's germline.

A new germline mutation not inherited from either parent is called a "de novo" mutation.

Diploid organisms (e.g., humans) contain two copies of each gene—a paternal and a maternal allele. Based on the occurrence of mutation on each chromosome, we may classify mutations into three types.
A wild type or homozygous non-mutated organism is one in which neither allele is mutated.


In order to categorize a mutation as such, the "normal" sequence must be obtained from the DNA of a "normal" or "healthy" organism (as opposed to a "mutant" or "sick" one), it should be identified and reported; ideally, it should be made publicly available for a straightforward nucleotide-by-nucleotide comparison, and agreed upon by the scientific community or by a group of expert geneticists and biologists, who have the responsibility of establishing the "standard" or so-called "consensus" sequence. This step requires a tremendous scientific effort. Once the consensus sequence is known, the mutations in a genome can be pinpointed, described, and classified. The committee of the Human Genome Variation Society (HGVS) has developed the standard human sequence variant nomenclature, which should be used by researchers and DNA diagnostic centers to generate unambiguous mutation descriptions. In principle, this nomenclature can also be used to describe mutations in other organisms. The nomenclature specifies the type of mutation and base or amino acid changes.


Mutation rates vary substantially across species, and the evolutionary forces that generally determine mutation are the subject of ongoing investigation.

Changes in DNA caused by mutation in a coding region of DNA can cause errors in protein sequence that may result in partially or completely non-functional proteins. Each cell, in order to function correctly, depends on thousands of proteins to function in the right places at the right times. When a mutation alters a protein that plays a critical role in the body, a medical condition can result. Some mutations alter a gene's DNA base sequence but do not change the function of the protein made by the gene. One study on the comparison of genes between different species of "Drosophila" suggests that if a mutation does change a protein, this will probably be harmful, with an estimated 70 percent of amino acid polymorphisms having damaging effects, and the remainder being either neutral or weakly beneficial. However, studies have shown that only 7% of point mutations in noncoding DNA of yeast are deleterious and 12% in coding DNA are deleterious. The rest of the mutations are either neutral or slightly beneficial.

If a mutation is present in a germ cell, it can give rise to offspring that carries the mutation in all of its cells. This is the case in hereditary diseases. In particular, if there is a mutation in a DNA repair gene within a germ cell, humans carrying such germline mutations may have an increased risk of cancer. A list of 34 such germline mutations is given in the article DNA repair-deficiency disorder. An example of one is albinism, a mutation that occurs in the OCA1 or OCA2 gene. Individuals with this disorder are more prone to many types of cancers, other disorders and have impaired vision. On the other hand, a mutation may occur in a somatic cell of an organism. Such mutations will be present in all descendants of this cell within the same organism, and certain mutations can cause the cell to become malignant, and, thus, cause cancer.

A DNA damage can cause an error when the DNA is replicated, and this error of replication can cause a gene mutation that, in turn, could cause a genetic disorder. DNA damages are repaired by the DNA repair system of the cell. Each cell has a number of pathways through which enzymes recognize and repair damages in DNA. Because DNA can be damaged in many ways, the process of DNA repair is an important way in which the body protects itself from disease. Once DNA damage has given rise to a mutation, the mutation cannot be repaired.

Although mutations that cause changes in protein sequences can be harmful to an organism, on occasions the effect may be positive in a given environment. In this case, the mutation may enable the mutant organism to withstand particular environmental stresses better than wild-type organisms, or reproduce more quickly. In these cases a mutation will tend to become more common in a population through natural selection. Examples include the following:

HIV resistance: a specific 32 base pair deletion in human CCR5 (CCR5-Δ32) confers HIV resistance to homozygotes and delays AIDS onset in heterozygotes. One possible explanation of the etiology of the relatively high frequency of CCR5-Δ32 in the European population is that it conferred resistance to the bubonic plague in mid-14th century Europe. People with this mutation were more likely to survive infection; thus its frequency in the population increased. This theory could explain why this mutation is not found in Southern Africa, which remained untouched by bubonic plague. A newer theory suggests that the selective pressure on the CCR5 Delta 32 mutation was caused by smallpox instead of the bubonic plague.

Malaria resistance: An example of a harmful mutation is sickle-cell disease, a blood disorder in which the body produces an abnormal type of the oxygen-carrying substance hemoglobin in the red blood cells. One-third of all indigenous inhabitants of Sub-Saharan Africa carry the allele, because, in areas where malaria is common, there is a survival value in carrying only a single sickle-cell allele (sickle cell trait). Those with only one of the two alleles of the sickle-cell disease are more resistant to malaria, since the infestation of the malaria "Plasmodium" is halted by the sickling of the cells that it infests.

Antibiotic resistance: Practically all bacteria develop antibiotic resistance when exposed to antibiotics. In fact, bacterial populations already have such mutations that get selected under antibiotic selection. Obviously, such mutations are only beneficial for the bacteria but not for those infected.

Lactase persistence. A mutation allowed humans to express the enzyme lactase after they are naturally weaned from breast milk, allowing adults to digest lactose, which is probably one of the most beneficial mutations in recent human evolution.

Prions are proteins and do not contain genetic material. However, prion replication has been shown to be subject to mutation and natural selection just like other forms of replication. The human gene PRNP codes for the major prion protein, PrP, and is subject to mutations that can give rise to disease-causing prions.

A change in the genetic structure that is not inherited from a parent, and also not passed to offspring, is called a somatic mutation"." Somatic mutations are not inherited because they do not affect the germline. These types of mutations are usually prompted by environmental causes, such as ultraviolet radiation or any exposure to certain harmful chemicals, and can cause diseases including cancer.""

With plants, some somatic mutations can be propagated without the need for seed production, for example, by grafting and stem cuttings. These type of mutation have led to new types of fruits, such as the "Delicious" apple and the "Washington" navel orange.

Human and mouse somatic cells have a mutation rate more than ten times higher than the germline mutation rate for both species; mice have a higher rate of both somatic and germline mutations per cell division than humans. The disparity in mutation rate between the germline and somatic tissues likely reflects the greater importance of genome maintenance in the germline than in the soma.

An amorph, a term utilized by Muller in 1932, is a mutated allele, which has lost the ability of the parent (whether wild type or any other type) allele to encode any functional protein. An amorphic mutation may be caused by the replacement of an amino acid that deactivates an enzyme or by the deletion of part of a gene that produces the enzyme.

Cells with heterozygous mutations (one good copy of gene and one mutated copy) may function normally with the unmutated copy until the good copy has been spontaneously somatically mutated. This kind of mutation happens all the time in living organisms, but it is difficult to measure the rate. Measuring this rate is important in predicting the rate at which people may develop cancer.

Point mutations may arise from spontaneous mutations that occur during DNA replication. The rate of mutation may be increased by mutagens. Mutagens can be physical, such as radiation from UV rays, X-rays or extreme heat, or chemical (molecules that misplace base pairs or disrupt the helical shape of DNA). Mutagens associated with cancers are often studied to learn about cancer and its prevention.

A hypomorphic mutation is a mutation which results in lowered gene expression. Usually, hypomorphic mutations are recessive, but haploinsufficiency causes some alleles to be dominant.

A hypermorphic mutation results in increased gene expression.



</doc>
<doc id="19705" url="https://en.wikipedia.org/wiki?curid=19705" title="Microgyrus">
Microgyrus

A microgyrus is an area of the cerebral cortex that includes only four cortical layers instead of six.

Microgyria are believed by some to be part of the genetic lack of prenatal development which is a cause of, or one of the causes of, dyslexia.

Albert Galaburda of Harvard Medical School noticed that language centers in dyslexic brains showed microscopic flaws known as ectopias and microgyria (Galaburda "et al.", 2006, "Nature Neuroscience" 9(10): 1213-1217). Both affect the normal six-layer structure of the cortex. These flaws affect connectivity and functionality of the cortex in critical areas related to sound and visual processing. These and similar structural abnormalities may be the basis of the inevitable and hard to overcome difficulty in reading.



</doc>
<doc id="19708" url="https://en.wikipedia.org/wiki?curid=19708" title="Mercantilism">
Mercantilism

Mercantilism is a national economic policy that is designed to maximize the exports, and minimize the imports, of a nation. These policies aim to reduce a possible current account deficit or reach a current account surplus. Mercantilism includes a national economic policy aimed at accumulating monetary reserves through a positive balance of trade, especially of finished goods. Historically, such policies frequently led to war and also motivated colonial expansion. Mercantilist theory varies in sophistication from one writer to another and has evolved over time.

Mercantilism was dominant in modernized parts of Europe from the 16th to the 18th centuries, a period of proto-industrialization, before falling into decline, although some commentators argue that it is still practiced in the economies of industrializing countries, in the form of economic interventionism. It promotes government regulation of a nation's economy for the purpose of augmenting state power at the expense of rival national powers. High tariffs, especially on manufactured goods, were an almost universal feature of mercantilist policy.

With the efforts of supranational organizations such as the World Trade Organization to reduce tariffs globally, non-tariff barriers to trade have assumed a greater importance in neomercantilism.

Mercantilism became the dominant school of economic thought in Europe throughout the late Renaissance and the early-modern period (from the 15th to the 18th centuries). Evidence of mercantilistic practices appeared in early-modern Venice, Genoa, and Pisa regarding control of the Mediterranean trade in bullion. However, the empiricism of the Renaissance, which first began to quantify large-scale trade accurately, marked mercantilism's birth as a codified school of economic theories. The Italian economist and mercantilist Antonio Serra is considered to have written one of the first treatises on political economy with his 1613 work, "A Short Treatise on the Wealth and Poverty of Nations".

Mercantilism in its simplest form is bullionism, yet mercantilist writers emphasize the circulation of money and reject hoarding. Their emphasis on monetary metals accords with current ideas regarding the money supply, such as the stimulative effect of a growing money-supply. Fiat money and floating exchange rates have since rendered specie concerns irrelevant. In time, industrial policy supplanted the heavy emphasis on money, accompanied by a shift in focus from the capacity to carry on wars to promoting general prosperity. Mature neomercantilist theory recommends selective high tariffs for "infant" industries or the promotion of the mutual growth of countries through national industrial specialization.

England began the first large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). An early statement on national balance of trade appeared in "Discourse of the Common Weal of this Realm of England", 1549: "We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them." The period featured various but often disjointed efforts by the court of Queen Elizabeth (reigned 1558-1603) to develop a naval and merchant fleet capable of challenging the Spanish stranglehold on trade and of expanding the growth of bullion at home. Queen Elizabeth promoted the Trade and Navigation Acts in Parliament and issued orders to her navy for the protection and promotion of English shipping. A systematic and coherent explanation of balance of trade emerged in Thomas Mun's argument "England's Treasure by Forraign Trade or the Balance of our Forraign Trade is The Rule of Our Treasure" - written in the 1620s and published in 1664.

Elizabeth's efforts organized national resources sufficiently in the defense of England against the far larger and more powerful Spanish Empire, and in turn, paved the foundation for establishing a global empire in the 19th century. Authors noted most for establishing the English mercantilist system include Gerard de Malynes ( 1585–1641) and Thomas Mun (1571-1641), who first articulated the Elizabethan system ("England's Treasure by Forraign Trade or the Balance of Forraign Trade is the Rule of Our Treasure"), which Josiah Child ( 1630/31 – 1699) then developed further. Numerous French authors helped cement French policy around mercantilism in the 17th century. Jean-Baptiste Colbert (Intendant général, 1661-1665; Contrôleur général des finances, 1661–1683) best articulated this French mercantilism. French economic policy liberalized greatly under Napoleon (in power from 1799 to 1814/1815)

Many nations applied the theory, notably France, which was the most important state economically in Europe at the time. King Louis XIV (reigned 1643-1715) followed the guidance of Jean Baptiste Colbert, his Controller-General of Finances from 1665 to 1683. It was determined that the state should rule in the economic realm as it did in the diplomatic, and that the interests of the state as identified by the king were superior to those of merchants and of everyone else. Mercantilist economic policies aimed to build up the state, especially in an age of incessant warfare, and theorists charged the state with looking for ways to strengthen the economy and to weaken foreign adversaries.

In Europe, academic belief in mercantilism began to fade in the late-18th century after the British seized control of the Mughal Bengal, a major trading nation, and the establishment of the British India through the activities of the East India Company, in light of the arguments of Adam Smith (1723-1790) and of the classical economists.
The British Parliament's repeal of the Corn Laws under Robert Peel in 1846 symbolized the emergence of free trade as an alternative system.

Most of the European economists who wrote between 1500 and 1750 are today generally considered mercantilists; this term was initially used solely by critics, such as Mirabeau and Smith, but was quickly adopted by historians. Originally the standard English term was "mercantile system". The word "mercantilism" was introduced into English from German in the early 19th century.

The bulk of what is commonly called "mercantilist literature" appeared in the 1620s in Great Britain. Smith saw the English merchant Thomas Mun (1571–1641) as a major creator of the mercantile system, especially in his posthumously published "Treasure by Foreign Trade" (1664), which Smith considered the archetype or manifesto of the movement. Perhaps the last major mercantilist work was James Steuart's "Principles of Political Economy", published in 1767.

Mercantilist literature also extended beyond England. Italy and France produced noted writers of mercantilist themes, including Italy's Giovanni Botero (1544–1617) and Antonio Serra (1580–?) and, in France, Jean Bodin and Colbert. Themes also existed in writers from the German historical school from List, as well as followers of the American system and British free-trade imperialism, thus stretching the system into the 19th century. However, many British writers, including Mun and Misselden, were merchants, while many of the writers from other countries were public officials. Beyond mercantilism as a way of understanding the wealth and power of nations, Mun and Misselden are noted for their viewpoints on a wide range of economic matters.

The Austrian lawyer and scholar Philipp Wilhelm von Hornick, one of the pioneers of Cameralism, detailed a nine-point program of what he deemed effective national economy in his "Austria Over All, If She Only Will" of 1684, which comprehensively sums up the tenets of mercantilism:

Other than Von Hornick, there were no mercantilist writers presenting an overarching scheme for the ideal economy, as Adam Smith would later do for classical economics. Rather, each mercantilist writer tended to focus on a single area of the economy. Only later did non-mercantilist scholars integrate these "diverse" ideas into what they called mercantilism. Some scholars thus reject the idea of mercantilism completely, arguing that it gives "a false unity to disparate events". Smith saw the mercantile system as an enormous conspiracy by manufacturers and merchants against consumers, a view that has led some authors, especially Robert E. Ekelund and Robert D. Tollison, to call mercantilism "a rent-seeking society". To a certain extent, mercantilist doctrine itself made a general theory of economics impossible. Mercantilists viewed the economic system as a zero-sum game, in which any gain by one party required a loss by another. Thus, any system of policies that benefited one group would by definition harm the other, and there was no possibility of economics being used to maximize the commonwealth, or common good. Mercantilists' writings were also generally created to rationalize particular practices rather than as investigations into the best policies.

Mercantilist domestic policy was more fragmented than its trade policy. While Adam Smith portrayed mercantilism as supportive of strict controls over the economy, many mercantilists disagreed. The early modern era was one of letters patent and government-imposed monopolies; some mercantilists supported these, but others acknowledged the corruption and inefficiency of such systems. Many mercantilists also realized that the inevitable results of quotas and price ceilings were black markets. One notion that mercantilists widely agreed upon was the need for economic oppression of the working population; laborers and farmers were to live at the "margins of subsistence". The goal was to maximize production, with no concern for consumption. Extra money, free time, and education for the lower classes were seen to inevitably lead to vice and laziness, and would result in harm to the economy.

The mercantilists saw a large population as a form of wealth that made possible the development of bigger markets and armies. Opposite to mercantilism was the doctrine of physiocracy, which predicted that mankind would outgrow its resources. The idea of mercantilism was to protect the markets as well as maintain agriculture and those who were dependent upon it.

Mercantilist ideas were the dominant economic ideology of all of Europe in the early modern period, and most states embraced it to a certain degree. Mercantilism was centred on England and France, and it was in these states that mercantilist policies were most often enacted.

The policies have included:

Mercantilism arose in France in the early 16th century soon after the monarchy had become the dominant force in French politics. In 1539, an important decree banned the import of woolen goods from Spain and some parts of Flanders. The next year, a number of restrictions were imposed on the export of bullion.

Over the rest of the 16th century, further protectionist measures were introduced. The height of French mercantilism is closely associated with Jean-Baptiste Colbert, finance minister for 22 years in the 17th century, to the extent that French mercantilism is sometimes called Colbertism. Under Colbert, the French government became deeply involved in the economy in order to increase exports. Protectionist policies were enacted that limited imports and favored exports. Industries were organized into guilds and monopolies, and production was regulated by the state through a series of more than one thousand directives outlining how different products should be produced.

To encourage industry, foreign artisans and craftsmen were imported. Colbert also worked to decrease internal barriers to trade, reducing internal tariffs and building an extensive network of roads and canals. Colbert's policies were quite successful, and France's industrial output and the economy grew considerably during this period, as France became the dominant European power. He was less successful in turning France into a major trading power, and Britain and the Netherlands remained supreme in this field.

France imposed its mercantilist philosophy on its colonies in North America, especially New France. It sought to derive the maximum material benefit from the colony, for the homeland, with a minimum of imperial investment in the colony itself. The ideology was embodied in New France through the establishment under Royal Charter of a number of corporate trading monopolies including La Compagnie des Marchands, which operated from 1613 to 1621, and the Compagnie de Montmorency, from that date until 1627. It was in turn replaced by La Compagnie des Cent-Associés, created in 1627 by King Louis XIII, and the Communauté des habitants in 1643. These were the first corporations to operate in what is now Canada.

In England, mercantilism reached its peak during the Long Parliament government (1640–60). Mercantilist policies were also embraced throughout much of the Tudor and Stuart periods, with Robert Walpole being another major proponent. In Britain, government control over the domestic economy was far less extensive than on the Continent, limited by common law and the steadily increasing power of Parliament. Government-controlled monopolies were common, especially before the English Civil War, but were often controversial.

With respect to its colonies, British mercantilism meant that the government and the merchants became partners with the goal of increasing political power and private wealth, to the exclusion of other empires. The government protected its merchants—and kept others out—through trade barriers, regulations, and subsidies to domestic industries in order to maximize exports from and minimize imports to the realm. The government had to fight smuggling, which became a favorite American technique in the 18th century to circumvent the restrictions on trading with the French, Spanish, or Dutch. The goal of mercantilism was to run trade surpluses so that gold and silver would pour into London. The government took its share through duties and taxes, with the remainder going to merchants in Britain. The government spent much of its revenue on a superb Royal Navy, which not only protected the British colonies but threatened the colonies of the other empires, and sometimes seized them. Thus the British Navy captured New Amsterdam (New York) in 1664. The colonies were captive markets for British industry, and the goal was to enrich the mother country.

British mercantilist writers were themselves divided on whether domestic controls were necessary. British mercantilism thus mainly took the form of efforts to control trade. A wide array of regulations were put in place to encourage exports and discourage imports. Tariffs were placed on imports and bounties given for exports, and the export of some raw materials was banned completely. The Navigation Acts expelled foreign merchants from England's domestic trade. The nation aggressively sought colonies and once under British control, regulations were imposed that allowed the colony to only produce raw materials and to only trade with Britain. This led to friction with the inhabitants of these colonies, and mercantilist policies (such as forbidding trade with other empires and controls over smuggling) were a major irritant leading to the American Revolution.

Mercantilism taught that trade was a zero-sum game, with one country's gain equivalent to a loss sustained by the trading partner. Overall, however, mercantilist policies had a positive impact on Britain helping turn it into the world's dominant trader and the global hegemon. One domestic policy that had a lasting impact was the conversion of "wastelands" to agricultural use. Mercantilists believed that to maximize a nation's power, all land and resources had to be used to their highest and best use, and this era thus saw projects like the draining of The Fens.

The other nations of Europe also embraced mercantilism to varying degrees. The Netherlands, which had become the financial centre of Europe by being its most efficient trader, had little interest in seeing trade restricted and adopted few mercantilist policies. Mercantilism became prominent in Central Europe and Scandinavia after the Thirty Years' War (1618–48), with Christina of Sweden, Jacob Kettler of Courland, and Christian IV of Denmark being notable proponents.

The Habsburg Holy Roman Emperors had long been interested in mercantilist policies, but the vast and decentralized nature of their empire made implementing such notions difficult. Some constituent states of the empire did embrace Mercantilism, most notably Prussia, which under Frederick the Great had perhaps the most rigidly controlled economy in Europe.

Spain benefited from mercantilism early on as it brought a large amount of precious metals such as gold and silver into their treasury by way of the new world. In the long run, Spain’s economy collapsed as it was unable to adjust to the inflation that came with the large influx of bullion. Heavy intervention from the crown put crippling laws for the protection of Spanish goods and services. Mercantilist protectionist policy in Spain caused the long-run failure of the Castilian textile industry as the efficiency severely dropped off with each passing year due to the production being held at a specific level. Spain’s heavily protected industries led to famines as much of its agricultural land was required to be used for sheep instead of grain. Much of their grain was imported from the Baltic region of Europe which caused a shortage of food in the inner regions of Spain. Spain limiting the trade of their colonies is one of the causes that lead to the separation of the Dutch from the Spanish Empire. The culmination of all of these policies lead to Spain defaulting in 1557, 1575, and 1596.

During the economic collapse of the 17th century, Spain had little coherent economic policy, but French mercantilist policies were imported by Philip V with some success. Russia under Peter I (Peter the Great) attempted to pursue mercantilism, but had little success because of Russia's lack of a large merchant class or an industrial base.

Mercantilism was the economic version of warfare using economics as a tool for warfare by other means backed up by the state apparatus and was well suited to an era of military warfare. Since the level of world trade was viewed as fixed, it followed that the only way to increase a nation's trade was to take it from another. A number of wars, most notably the Anglo-Dutch Wars and the Franco-Dutch Wars, can be linked directly to mercantilist theories. Most wars had other causes but they reinforced mercantilism by clearly defining the enemy, and justified damage to the enemy's economy.

Mercantilism fueled the imperialism of this era, as many nations expended significant effort to conquer new colonies that would be sources of gold (as in Mexico) or sugar (as in the West Indies), as well as becoming exclusive markets. European power spread around the globe, often under the aegis of companies with government-guaranteed monopolies in certain defined geographical regions, such as the Dutch East India Company or the British Hudson's Bay Company (operating in present-day Canada).

With the establishment of overseas colonies by European powers early in the 17th century, mercantile theory gained a new and wider significance, in which its aim and ideal became both national and imperialistic.

Mercantilism as a weapon has continued to be used by nations through the 21st century by way of modern tariffs as it puts smaller economies in a position to conform to the larger economies goals or risk economic ruin due to an imbalance in trade. Trade wars are often dependent on such tariffs and restrictions hurting the opposing economy.

The term "mercantile system" was used by its foremost critic, Adam Smith, but Mirabeau (1715–1789) had used "mercantilism" earlier.

Mercantilism functioned as the economic counterpart of the older version of political power: divine right of kings and absolute monarchy.

Scholars debate over why mercantilism dominated economic ideology for 250 years. One group, represented by Jacob Viner, sees mercantilism as simply a straightforward, common-sense system whose logical fallacies remained opaque to people at the time, as they simply lacked the required analytical tools.

The second school, supported by scholars such as Robert B. Ekelund, portrays mercantilism not as a mistake, but rather as the best possible system for those who developed it. This school argues that rent-seeking merchants and governments developed and enforced mercantilist policies. Merchants benefited greatly from the enforced monopolies, bans on foreign competition, and poverty of the workers. Governments benefited from the high tariffs and payments from the merchants. Whereas later economic ideas were often developed by academics and philosophers, almost all mercantilist writers were merchants or government officials.

Monetarism offers a third explanation for mercantilism. European trade exported bullion to pay for goods from Asia, thus reducing the money supply and putting downward pressure on prices and economic activity. The evidence for this hypothesis is the lack of inflation in the British economy until the Revolutionary and Napoleonic Wars, when paper money came into vogue.

A fourth explanation lies in the increasing professionalisation and technification of the wars of the era, which turned the maintenance of adequate reserve funds (in the prospect of war) into a more and more expensive and eventually competitive business.

Mercantilism developed at a time of transition for the European economy. Isolated feudal estates were being replaced by centralized nation-states as the focus of power. Technological changes in shipping and the growth of urban centers led to a rapid increase in international trade. Mercantilism focused on how this trade could best aid the states. Another important change was the introduction of double-entry bookkeeping and modern accounting. This accounting made extremely clear the inflow and outflow of trade, contributing to the close scrutiny given to the balance of trade. Of course, the impact of the discovery of America cannot be ignored. New markets and new mines propelled foreign trade to previously inconceivable volumes, resulting in "the great upward movement in prices" and an increase in "the volume of merchant activity itself".

Prior to mercantilism, the most important economic work done in Europe was by the medieval scholastic theorists. The goal of these thinkers was to find an economic system compatible with Christian doctrines of piety and justice. They focused mainly on microeconomics and on local exchanges between individuals. Mercantilism was closely aligned with the other theories and ideas that began to replace the medieval worldview. This period saw the adoption of the very Machiavellian realpolitik and the primacy of the "raison d'état" in international relations. The mercantilist idea of all trade as a zero-sum game, in which each side was trying to best the other in a ruthless competition, was integrated into the works of Thomas Hobbes. This dark view of human nature also fit well with the Puritan view of the world, and some of the most stridently mercantilist legislation, such as the Navigation Ordinance of 1651, was enacted by the government of Oliver Cromwell.

Jean-Baptiste Colbert's work in 17th-century France came to exemplify classical mercantilism. In the English-speaking world, its ideas were criticized by Adam Smith with the publication of "The Wealth of Nations" in 1776 and later by David Ricardo with his explanation of comparative advantage. Mercantilism was rejected by Britain and France by the mid-19th century. The British Empire embraced free trade and used its power as the financial center of the world to promote the same. The Guyanese historian Walter Rodney describes mercantilism as the period of the worldwide development of European commerce, which began in the 15th century with the voyages of Portuguese and Spanish explorers to Africa, Asia, and the New World.

Adam Smith and David Hume were the founding fathers of anti-mercantilist thought. A number of scholars found important flaws with mercantilism long before Smith developed an ideology that could fully replace it. Critics like Hume, Dudley North and John Locke undermined much of mercantilism and it steadily lost favor during the 18th century.

In 1690, Locke argued that prices vary in proportion to the quantity of money. Locke's "Second Treatise" also points towards the heart of the anti-mercantilist critique: that the wealth of the world is not fixed, but is created by human labor (represented embryonically by Locke's labor theory of value). Mercantilists failed to understand the notions of absolute advantage and comparative advantage (although this idea was only fully fleshed out in 1817 by David Ricardo) and the benefits of trade.

For instance, imagine that Portugal was a more efficient producer of wine than England, yet in England, cloth could be produced more efficiently than it could in Portugal. Thus if Portugal specialized in wine and England in cloth, "both" states would end up "better off" if they traded. This is an example of the reciprocal benefits of trade (whether due to comparative or absolute advantage). In modern economic theory, trade is "not" a zero-sum game of cutthroat competition, because both sides can benefit from it.

Hume famously noted the impossibility of the mercantilists' goal of a constant positive balance of trade. As bullion flowed into one country, the supply would increase, and the value of bullion in that state would steadily decline relative to other goods. Conversely, in the state exporting bullion, its value would slowly rise. Eventually, it would no longer be cost-effective to export goods from the high-price country to the low-price country, and the balance of trade would reverse. Mercantilists fundamentally misunderstood this, long arguing that an increase in the money supply simply meant that everyone gets richer.

The importance placed on bullion was also a central target, even if many mercantilists had themselves begun to de-emphasize the importance of gold and silver. Adam Smith noted that at the core of the mercantile system was the "popular folly of confusing wealth with money", that bullion was just the same as any other commodity, and that there was no reason to give it special treatment. More recently, scholars have discounted the accuracy of this critique. They believe Mun and Misselden were not making this mistake in the 1620s, and point to their followers Josiah Child and Charles Davenant, who in 1699 wrote, "Gold and Silver are indeed the Measures of Trade, but that the Spring and Original of it, in all nations is the Natural or Artificial Product of the Country; that is to say, what this Land or what this Labour and Industry Produces." The critique that mercantilism was a form of rent seeking has also seen criticism, as scholars such Jacob Viner in the 1930s pointed out that merchant mercantilists such as Mun understood that they would not gain by higher prices for English wares abroad.

The first school to completely reject mercantilism was the physiocrats, who developed their theories in France. Their theories also had several important problems, and the replacement of mercantilism did not come until Adam Smith published "The Wealth of Nations" in 1776. This book outlines the basics of what is today known as classical economics. Smith spent a considerable portion of the book rebutting the arguments of the mercantilists, though often these are simplified or exaggerated versions of mercantilist thought.

Scholars are also divided over the cause of mercantilism's end. Those who believe the theory was simply an error hold that its replacement was inevitable as soon as Smith's more accurate ideas were unveiled. Those who feel that mercantilism amounted to rent-seeking hold that it ended only when major power shifts occurred. In Britain, mercantilism faded as the Parliament gained the monarch's power to grant monopolies. While the wealthy capitalists who controlled the House of Commons benefited from these monopolies, Parliament found it difficult to implement them because of the high cost of group decision making.

Mercantilist regulations were steadily removed over the course of the 18th century in Britain, and during the 19th century, the British government fully embraced free trade and Smith's laissez-faire economics. On the continent, the process was somewhat different. In France, economic control remained in the hands of the royal family, and mercantilism continued until the French Revolution. In Germany, mercantilism remained an important ideology in the 19th and early 20th centuries, when the historical school of economics was paramount.

Adam Smith rejected the mercantilist focus on production, arguing that consumption was paramount to production. He added that mercantilism was popular among merchants because it was what is now called rent seeking. John Maynard Keynes argued that encouraging production was just as important as encouraging consumption, and he favored the "new mercantilism". Keynes also noted that in the early modern period the focus on the bullion supplies was reasonable. In an era before paper money, an increase in bullion was one of the few ways to increase the money supply. Keynes said mercantilist policies generally improved both domestic and foreign investment—domestic because the policies lowered the domestic rate of interest, and investment by foreigners by tending to create a favorable balance of trade. Keynes and other economists of the 20th century also realized that the balance of payments is an important concern. Keynes also supported government intervention in the economy as necessity, as did mercantilism.

, the word "mercantilism" remains a pejorative term, often used to attack various forms of protectionism. The similarities between Keynesianism (and its successor ideas) and mercantilism have sometimes led critics to call them neo-mercantilism.

Paul Samuelson, writing within a Keynesian framework, wrote of mercantilism, "With employment less than full and Net National Product suboptimal, all the debunked mercantilist arguments turn out to be valid."

Some other systems that copy several mercantilist policies, such as Japan's economic system, are also sometimes called neo-mercantilist. In an essay appearing in the 14 May 2007 issue of "Newsweek", business columnist Robert J. Samuelson wrote that China was pursuing an essentially neo-mercantilist trade policy that threatened to undermine the post–World War II international economic structure.

Murray Rothbard, representing the Austrian School of economics, describes it this way:

In specific instances, protectionist mercantilist policies also had an important and positive impact on the state that enacted them. Adam Smith, for instance, praised the Navigation Acts, as they greatly expanded the British merchant fleet and played a central role in turning Britain into the world's naval and economic superpower from the 18th century onward. Some economists thus feel that protecting infant industries, while causing short-term harm, can be beneficial in the long term.





</doc>
<doc id="19709" url="https://en.wikipedia.org/wiki?curid=19709" title="Meat Puppets">
Meat Puppets

Meat Puppets are an American rock band formed in January 1980 in Phoenix, Arizona. The group's original lineup was Curt Kirkwood (guitar/vocals), his brother Cris Kirkwood (bass guitar/vocals), and Derrick Bostrom (drums). The Kirkwood brothers met Bostrom while attending Brophy Prep High School in Phoenix. The three then moved to Tempe, Arizona (a Phoenix suburb and home to Arizona State University), where the Kirkwood brothers purchased two adjacent homes, one of which had a shed in the back where they regularly practiced.

Meat Puppets started as a punk rock band, but like most of their labelmates on SST Records, they established their own unique style, blending punk with country and psychedelic rock, and featuring Curt's warbling vocals. Meat Puppets later gained significant exposure when the Kirkwood brothers served as guest musicians on Nirvana's MTV Unplugged performance in 1993. The band's 1994 album "Too High to Die" subsequently became their most successful release. The band broke up twice, in 1996 and 2002, but reunited again in 2006.

Meat Puppets have influenced a number of rock bands, including Nirvana, Soundgarden, Dinosaur Jr, Sebadoh, Pavement, Jawbreaker, Sublime, and Red Hot Chili Peppers.

In the late 1970s, drummer Derrick Bostrom played with guitarist Jack Knetzger in a band called Atomic Bomb Club, which began as a duo, but would come to include bassist Cris Kirkwood. The band played a few local shows and recorded some demos, but began to dissolve quickly thereafter. Derrick and Cris began rehearsing together with Cris' brother Curt Kirkwood by learning songs from Bostrom's collection of punk rock 45s. After briefly toying with the name "The Bastions of Immaturity", they settled on the name Meat Puppets in June, 1980 after a song by Curt of the same name which appears on their first album. Their earliest EP "In A Car" was made entirely of short hardcore punk with goofy lyrics, and attracted the attention of Joe Carducci as he was starting to work with legendary punk label SST Records. Carducci suggested they sign with the label, and Meat Puppets released their first album "Meat Puppets" in 1982, which among several new originals and a pair of heavily skewed Doc Watson and Bob Nolan covers, featured the songs "The Gold Mine" and "Melons Rising", two tunes Derrick and Cris originally had written and performed as Atomic Bomb Club previously. Years later, when the Meat Puppets reissued all of their albums in 1999, the five songs on In A Car would be combined with their debut album.
By the release of 1984's "Meat Puppets II", the bandmembers "were so sick of the hardcore thing," according to Bostrom. "We were really into pissing off the crowd." Here, the band experimented with acid rock and country and western sounds, while still retaining some punk influence on the tracks "Split Myself in Two" and "New Gods." This album contains some of the band's best known songs, such as "Lake of Fire" and "Plateau." While the album had been recorded in early 1983, the album's release was delayed for a year by SST. "Meat Puppets II" turned the band into one of the leading bands on SST Records, and along with the Violent Femmes, the Gun Club and others, helped establish the genre called "cow punk".

Meat Puppets II was followed by 1985's "Up on the Sun". The album's psychedelic sound resembled the folk-rock of The Byrds, while the songs still retained hardcore influences in the lengths of the songs and the tempos. Examples of this new style are the self titled track, "Enchanted Porkfist" and "Swimming Ground." Up On The Sun also sees the Kirkwood brothers harmonizing their vocals for the first time. These 2 albums were mainstays of college and independent radio at that time.

During the rest of the 1980s, Meat Puppets remained on SST and released a series of albums while touring relentlessly. Between tours they would regularly play small shows in bars around the Phoenix area such as The Mason Jar (now The Rebel Lounge) and The Sun Club in Tempe. After the release of the hard-rock styled "Out My Way" EP in 1986, however, the band was briefly sidelined by an accident when Curt's finger was broken after being slammed in their touring van's door. The accident delayed the band's next album, the even more psychedelic "Mirage", until the next year. The final result included synthesizers and electronic drums, and as such was considered their most polished sounding album to date. The tour for Mirage lasted less than 6 months, as the band found it difficult to recreate many of this album's songs in a concert atmosphere.

Their next album, the ZZ-Top inspired "Huevos", came out less than six months afterward, in late summer of 1987. In stark contrast to its predecessor, "Huevos" was recorded in a swift, fiery fashion, with many first takes, and minimal second guessing. These recordings were completed in only a matter of days, and along with a few drawings and one of Curt's paintings taken from the wall to serve as cover art (a dish of three boiled eggs, a green pepper, and a bottle of Tabasco sauce), were all sent to SST shortly before the band returned to the road en route to their next gig. Curt revealed in an interview that one of the reasons for the album being called Huevos (meaning 'eggs' in Spanish) was because of the multitude of first-takers on the record, as similarly eggs can only be used once.

"Monsters" was released in 1989, featuring new elements to their sound with extended jams (such as "Touchdown King" and "Flight of the Fire Weasel") and heavy metal ("Attacked by Monsters"). This album was mostly motivated by the Meat Puppets' desire to attract the attention of a major label, as they were becoming frustrated with SST Records by this time.

As numerous bands from the seminal SST label and other kindred punk-oriented indies had before them, Meat Puppets grappled with the decision to switch to a major label. Two years after their final studio recording for SST, 1989's "Monsters", the trio released its major-label debut, "Forbidden Places", on the indie-friendly London Records. The band chose London Records because it was the first label that ZZ Top, one of their favorite bands, was signed to.

"Forbidden Places" combined many elements of the band's sounds over the years (cowpunk, psychedelia, riffy heavier rock) while some songs had a more laid back early alternative sound. Songs include "Sam" and "Whirlpool," and the title track. Despite being a fan favorite, Forbidden Places is now out of print, and as such it remains a highly sought-after collectible online.

In 1992 following his departure from the Red Hot Chili Peppers, guitarist John Frusciante auditioned for the band. Cris Kirkwood stated "He showed up with his guitar out of its case and barefoot. We were on a major label then, we just got signed, and those guys had blown up to where they were at and John needed to get out. John gets to our pad and we started getting ready to play and I said, 'You want to use my tuner?' He said, 'No, I'll bend it in.' It was so far out. Then we jammed but it didn't come to anything. Maybe he wasn't in the right place and we were a tight little unit. It just didn't quite happen but it could have worked."
In late 1993, Meat Puppets achieved mainstream popularity when Nirvana's Kurt Cobain, who became a fan after seeing them open for Black Flag in the ‘80s, invited Cris and Curt to join him on MTV Unplugged for acoustic performances of "Plateau", "Oh Me" and "Lake of Fire" (all originally from "Meat Puppets II"). The resulting album, "MTV Unplugged in New York," served as a swan song for Nirvana, as Cobain died less than 5 months after the concert. "Lake of Fire" became a cult favorite for its particularly wrenching vocal performance from Cobain. Subsequently, the Nirvana exposure and the strength of the single "Backwater" (their highest charting single) helped lift Meat Puppets to new commercial heights. The band's studio return was 1994's "Too High To Die", produced by Butthole Surfers guitarist Paul Leary. The album featured "Backwater", which reached #47 on the Billboard Hot 100, and a hidden-track update of "Lake of Fire." This album features a more straightforward alternative rock style, with occasional moments of pop, country and neo-psychedelic moments. "Too High To Die" earned the band a gold record (500,000 sold), outselling their previous records combined.

1995's "No Joke!" was the final album recorded by the original Meat Puppets lineup. Stylistically it is very similar to Too High to Die, although much heavier and with darker lyrics. Examples of this are the single "Scum" and "Eyeball," however the band's usual laid-back style is still heard on tracks like "Chemical Garden." Though the band's drug use included cocaine, heroin, LSD and many others, Cris' use of heroin and crack cocaine became so bad he rarely left his house except to obtain more drugs. At least two people (including his wife and one of his best friends) died of overdoses at his house in Tempe, AZ during this time. The Kirkwood brothers had always had a legendary appetite for illegal substances and during the tour to support "Too High To Die" with Stone Temple Pilots, the easy availability of drugs was too much for Cris. When it was over, he was severely addicted to cocaine and heroin. When their record label discovered Cris' addictions, support for No Joke! was subsequently dropped and it was met with poor sales figures.

Derrick recorded a solo EP under the moniker "Today's Sounds" in 1996, and later on in 1999 took charge of re-issuing the Puppets' original seven records on Rykodisc as well as putting out their first live album, "Live in Montana." Curt formed a new band in Austin, TX called the Royal Neanderthal Orchestra, but they changed their name to Meat Puppets for legal reasons and released a promotional EP entitled "You Love Me" in 1999, "Golden Lies" in 2000 and "Live" in 2002. The line-up was Curt (voc/git), Kyle Ellison (voc/git), Andrew Duplantis (voc/bass) and Shandon Sahm (drums). Sahm's father was the legendary fiddler-singer-songwriter Doug Sahm of The Sir Douglas Quintet and Texas Tornados. The concluding track to "Classic Puppets" entitled "New Leaf" also dates from this incarnation of the band.

Around 2002, Meat Puppets dissolved after Duplantis left the band. Curt went on to release albums with the groups Eyes Adrift and Volcano. In 2005, he released his first solo album entitled "Snow".

Bassist Cris was arrested in December 2003 for attacking a security guard at the main post office in downtown Phoenix, AZ with the guard's baton. The guard shot Kirkwood in the stomach at least twice during the melee, causing serious gunshot injuries requiring major surgery. Kirkwood was subsequently denied bail, the judge citing Kirkwood's previous drug arrests and probation violations. He eventually went to prison at the Arizona state prison in Florence, Arizona for felony assault. He was released in July 2005.

Derrick Bostrom began a web site for the band about six months before the original trio stopped working together. The site went through many different permutations before it was essentially mothballed in 2003. In late 2005, Bostrom revamped it, this time as a "blog" for his recollections and as a place to share pieces of Meat Puppets history.

On March 24, 2006, Curt Kirkwood polled fans at his MySpace page with a bulletin that asked: "Question for all ! Would the original line up of Meat Puppets interest anyone ? Feedback is good – do you want a reunion!?" The response from fans was overwhelmingly positive within a couple of hours, leading to speculation of a full-blown Meat Puppets reunion in the near future. However, a post made by Derrick Bostrom on the official Meat Puppets site dismissed the notion.

In April 2006 "Billboard" reported that the Kirkwood brothers would reunite as Meat Puppets without original drummer Derrick Bostrom. Although Primus drummer Tim Alexander was announced as Bostrom's replacement, the position was later filled by Ted Marcus. The new lineup recorded a new full-length album, "Rise to Your Knees", in mid-to-late 2006. The album was released by Anodyne Records on July 17, 2007.

On January 20, 2007, Meat Puppets brothers performed two songs during an Army of Anyone concert, at La Zona Rosa in Austin, Texas. The first song was played with Curt Kirkwood and Cris Kirkwood along with Army of Anyone's Ray Luzier and Dean DeLeo. Then the second song was played with original members Curt and Cris Kirkwood and new Meat Puppets drummer Ted Marcus. This was in the middle of Army of Anyone's set, which they listed as "Meat Puppet Theatre" on the evening's set list. The band performed several new songs in March at the South by Southwest festival. On March 28, 2007, the band announced a West Coast tour through their MySpace page. This is the first tour with original bassist Cris in eleven years. The tour continued into the east coast and midwest later in 2007.

In 2008 they performed their classic second album live in its entirety at the ATP New York festival.

The band parted ways with Anodyne, signed to Megaforce and began recording new material in the winter of 2008. The resulting album, entitled "Sewn Together", was released on May 12, 2009.
In the summer of 2009 the band continued to tour across America. They appeared in Rochester, Minnesota outside in front of over 5,000 fans, after playing Summerfest in Milwaukee, Wisconsin the night prior. Meat Puppets performed at the 2009 Voodoo Music Experience in New Orleans over the Halloween weekend.
As of November 2009, Shandon Sahm was back as the drummer in Meat Puppets, replacing Ted Marcus. The band was chosen by Animal Collective to perform the album 'Up on the Sun' live in its entirety at the All Tomorrow's Parties festival that they curated in May 2011.

The band's thirteenth studio album, entitled "Lollipop", was released on April 12, 2011. The Dandies supported Meat Puppets on all European dates in 2011.

Meat Puppets have played several gigs in their hometown since 2009, such as the Marquee show in June 2011 with Dead Confederate.

As of early 2011 Elmo Kirkwood, son of Curt Kirkwood and nephew of Cris Kirkwood, was touring regularly with the band playing rhythm guitar.

Meat Puppets also contributed to Spin Magazine's exclusive album "", playing Nirvana's "Smells Like Teen Spirit".

In June 2012, a book titled "Too High to Die: Meet the Meat Puppets" by author Greg Prato was released, which featured all-new interviews with band members past and present and friends of the band (including Peter Buck, Kim Thayil, Scott Asheton, Mike Watt, and Henry Rollins, among others), and covered the band's entire career.

In October 2012, it was announced that the group had just completed recording new songs. "Rat Farm", the band's 14th album, was released in April 2013.

In March 2013, Meat Puppets opened for Dave Grohl's Sound City Players at the SXSW Festival in Austin, Texas.

In April 2014, Meat Puppets completed a tour with The Moistboyz, and in the summer of 2015, they toured with Soul Asylum. 

The Meat Puppets were picked to open for an 11 show tour as support of The Dean Ween Group in October 2016 after Curt Kirkwood and drummer Chuck Treece contribute to "The Deaner Album". Also the same year, Cris either produced and/or played with the following artists for Slope Records - The Exterminators, the Linecutters, and Sad Kid.

On August 17, 2017, original drummer Derrick Bostrom posted an update on his website derrickbostrom.net. He performed with Cris, Curt and Elmo Kirkwood at a concert honoring the Meat Puppets. It appears that, while Derrick enjoyed himself, that this was a one-off performance. 

On July 8, 2018, it was confirmed that Bostrom had replaced Sahm as the drummer for the band, and that keyboardist Ron Stabinsky had joined, as well.

The band released their 15th studio album, "Dusty Notes", on March 8, 2019.

On April 21, 2018 a fan-sponsored petition on MoveOn.org was initiated to induct the Meat Puppets into the Rock and Roll Hall of Fame.





</doc>
<doc id="19710" url="https://en.wikipedia.org/wiki?curid=19710" title="List of mathematics competitions">
List of mathematics competitions

Mathematics competitions or mathematical olympiads are competitive events where participants sit a mathematics test. These tests may require multiple choice or numeric answers, or a detailed written solution or proof.



















Generally, registering for these contests is based on the grade level of math at which the student works rather than the age or the enrolled grade of the student. Also normally only competitions where the participants write a full proof are called Mathematical Olympiads.






</doc>
<doc id="19711" url="https://en.wikipedia.org/wiki?curid=19711" title="Michael Polanyi">
Michael Polanyi

Michael Polanyi (; ; 11 March 1891 – 22 February 1976) was a Hungarian-British polymath, who made important theoretical contributions to physical chemistry, economics, and philosophy. He argued that positivism supplies a false account of knowing, which if taken seriously undermines humanity's highest achievements.

His wide-ranging research in physical science included chemical kinetics, x-ray diffraction, and adsorption of gases. He pioneered the theory of fibre diffraction analysis in 1921, and the dislocation theory of plastic deformation of ductile metals and other materials in 1934. He emigrated to Germany, in 1926 becoming a chemistry professor at the Kaiser Wilhelm Institute in Berlin, and then in 1933 to England, becoming first a chemistry professor, and then a social sciences professor at the University of Manchester. Two of his pupils, and his son John Charles Polanyi won Nobel Prizes in Chemistry. In 1944 Polanyi was elected to the Royal Society.

The contributions which Polanyi made to the social sciences include an understanding of tacit knowledge, and the concept of a polycentric spontaneous order to intellectual inquiry were developed in the context of his opposition to central planning.

Polanyi, born Pollacsek Mihály in Budapest, was the fifth child of Mihály and Cecília Pollacsek (born as Cecília Wohl), secular Jews from Ungvár (then in Hungary but now in Ukraine) and Wilno, then Russian Empire, respectively. His father's family were entrepreneurs, while his mother's father – Osher Leyzerovich Vol (1833 – after 1906) – was the senior teacher of Jewish history at the Vilna rabbinic seminary, from which he had graduated as a rabbi. The family moved to Budapest and Magyarized their surname to Polányi. His father built much of the Hungarian railway system, but lost most of his fortune in 1899 when bad weather caused a railway building project to go over budget. He died in 1905. Cecília Polányi established a salon that was well known among Budapest's intellectuals, and which continued until her death in 1939. His older brother was Karl Polanyi, the political economist and anthropologist, and his niece was Eva Zeisel, a world-renowned ceramist.

In 1909, after leaving his teacher-training secondary school (Mintagymnasium), Polanyi studied to be a physician, obtaining his medical diploma in 1914. He was an active member of the Galilei Society. With the support of Ignác Pfeifer, professor of chemistry at the József Technical University of Budapest, he obtained a scholarship to study chemistry at the Technische Hochschule in Karlsruhe, Germany. In the First World War, he served in the Austro-Hungarian army as a medical officer, and was sent to the Serbian front. While on sick-leave in 1916, he wrote a PhD thesis on adsorption. His research, which was encouraged by Albert Einstein, was supervised by Gusztáv Buchböck, and in 1919 the University of Budapest awarded him a doctorate.

In October 1918, Mihály Károlyi established the Hungarian Democratic Republic, and Polanyi became Secretary to the Minister of Health. When the Communists seized power in March 1919, he returned to medicine. When the Hungarian Soviet Republic was overthrown, Polanyi emigrated to Karlsruhe in Germany, and was invited by Fritz Haber to join the Kaiser Wilhelm Institut für Faserstoffchemie (fiber chemistry) in Berlin. In 1923 he converted to Christianity, and in a Roman Catholic ceremony married Magda Elizabeth Kemeny. In 1926 he became the professorial head of department of the Institut für Physikalische Chemie und Elektrochemie (now the Fritz Haber Institute). In 1929, Magda gave birth to their son John, who was awarded a Nobel Prize in chemistry in 1986. Their other son, George Polanyi, who predeceased him, became a well-known economist.

His experience of runaway inflation and high unemployment in Weimar Germany led Polanyi to become interested in economics. With the coming to power in 1933 of the Nazi party, he accepted a chair in physical chemistry at the University of Manchester. Two of his pupils, Eugene Wigner and Melvin Calvin went on to win a Nobel Prize. Because of his increasing interest in the social sciences, Manchester University created a new chair in Social Science (1948–58) for him.

In 1944 Polanyi was elected a member of the Royal Society, and on his retirement from the University of Manchester in 1958 he was elected a senior research fellow at Merton College, Oxford. In 1962 he was elected a foreign honorary member of the American Academy of Arts and Sciences.

Polanyi's scientific interests were extremely diverse, including work in chemical kinetics, x-ray diffraction, and the adsorption of gases at solid surfaces. He is also well known for his potential adsorption theory, which was disputed for quite some time. In 1921, he laid the mathematical foundation of fibre diffraction analysis. In 1934, Polanyi, at about the same time as G. I. Taylor and Egon Orowan, realised that the plastic deformation of ductile materials could be explained in terms of the theory of dislocations developed by Vito Volterra in 1905. The insight was critical in developing the field of solid mechanics.

In 1936, as a consequence of an invitation to give lectures for the Ministry of Heavy Industry in the USSR, Polanyi met Bukharin, who told him that in socialist societies all scientific research is directed to accord with the needs of the latest Five Year Plan. Polanyi noted what had happened to the study of genetics in the Soviet Union once the doctrines of Trofim Lysenko had gained the backing of the State. Demands in Britain, for example by the Marxist John Desmond Bernal, for centrally planned scientific research led Polanyi to defend the claim that science requires free debate. Together with John Baker, he founded the influential Society for Freedom in Science.

In a series of articles, re-published in "The Contempt of Freedom" (1940) and "The Logic of Liberty" (1951), Polanyi claimed that co-operation amongst scientists is analogous to the way agents co-ordinate themselves within a free market. Just as consumers in a free market determine the value of products, science is a spontaneous order that arises as a consequence of open debate amongst specialists. Science (contrary to the claims of Bukharin) flourishes when scientists have the liberty to pursue truth as an end in itself:

[S]cientists, freely making their own choice of problems and pursuing them in the light of their own personal judgment, are in fact co-operating as members of a closely knit organization.
Such self-co-ordination of independent initiatives leads to a joint result which is unpremeditated by any of those who bring it about.
Any attempt to organize the group ... under a single authority would eliminate their independent initiatives, and thus reduce their joint effectiveness to that of the single person directing them from the centre. It would, in effect, paralyse their co-operation.
He derived the phrase spontaneous order from Gestalt psychology, and it was adopted by the classical liberal economist Friederich Hayek, although the concept can be traced back to at least Adam Smith. Polanyi (unlike Hayek) argued that there are higher and lower forms of spontaneous order, and he asserted that defending scientific inquiry on utilitarian or sceptical grounds undermined the practice of science. He extends this into a general claim about free societies. Polanyi defends a free society not on the negative grounds that we ought to respect "private liberties", but on the positive grounds that "public liberties" facilitate our pursuit of objective ideals.

According to Polanyi, a free society that strives to be value-neutral undermines its own justification. But it is not enough for the members of a free society to believe that ideals such as truth, justice, and beauty, are objective, they also have to accept that they transcend our ability to wholly capture them. The objectivity of values must be combined with acceptance that all knowing is fallible.

In "Full Employment and Free Trade" (1948) Polanyi analyses the way money circulates around an economy, and in a monetarist analysis that, according to Paul Craig Roberts, was thirty years ahead of its time, he argues that a free market economy should not be left to be wholly self-adjusting. A central bank should attempt to moderate economic booms/busts via a strict/loose monetary policy.

In his book "Science, Faith and Society" (1946), Polanyi set out his opposition to a positivist account of science, noting that it ignores the role personal commitments play in the practice of science. Polanyi was invited to give the prestigious Gifford Lectures in 1951–52 at Aberdeen. A revised version of his lectures were later published as "Personal Knowledge" (1958). In this book Polanyi claims that all knowledge claims (including those that derive from rules) rely on personal judgements. He denies that a scientific method can yield truth mechanically. All knowing, no matter how formalised, relies upon commitments. Polanyi argued that the assumptions that underlie critical philosophy are not only false, they undermine the commitments that motivate our highest achievements. He advocates a fiduciary post-critical approach, in which we recognise that we believe more than we can prove, and know more than we can say. The literary critic Rita Felski has named Polanyi as an important precursor to the project of postcritique within literary studies.

A knower does not stand apart from the universe, but participates personally within it. Our intellectual skills are driven by passionate commitments that motivate discovery and validation. According to Polanyi, a great scientist not only identifies patterns, but also chooses significant questions likely to lead to a successful resolution. Innovators risk their reputation by committing to a hypothesis. Polanyi cites the example of Copernicus, who declared that the Earth revolves around the Sun. He claims that Copernicus arrived at the Earth's true relation to the Sun not as a consequence of following a method, but via "the greater intellectual satisfaction he derived from the celestial panorama as seen from the Sun instead of the Earth." His writings on the practice of science influenced Thomas Kuhn and Paul Feyerabend.

Polanyi rejected the claim by British Empiricists that experience can be reduced into sense data, but he also rejects the notion that "indwelling" within (sometimes incompatible) interpretative frameworks traps us within them. Our tacit awareness connects us, albeit fallibly, with reality. It supplies us with the context within which our articulations have meaning. Contrary to the views of his colleague and friend Alan Turing, whose work at the Victoria University of Manchester prepared the way for the first modern computer, he denied that minds are reducible to collections of rules. His work influenced the critique by Hubert Dreyfus of "First Generation" artificial intelligence.

It was while writing "Personal Knowledge" that he identified the "structure of tacit knowing". He viewed it as his most important discovery. He claimed that we experience the world by integrating our subsidiary awareness into a focal awareness. In his later work, for example his Terry Lectures, later published as "The Tacit Dimension" (1966), he distinguishes between the phenomenological, instrumental, semantic, and ontological aspects of tacit knowing, as discussed (but not necessarily identified as such) in his previous writing.

In "Life's irreducible structure" (1968), Polanyi argues that the information contained in the DNA molecule is not reducible to the laws of physics and chemistry. Although a DNA molecule cannot exist without physical properties, these properties are constrained by higher-level ordering principles. In "Transcendence and Self-transcendence" (1970), Polanyi criticises the mechanistic world view that modern science inherited from Galileo.

Polanyi advocates emergence i.e. the claim that there are several levels of reality and of causality. He relies on the assumption that boundary conditions supply degrees of freedom that, instead of being random, are determined by higher-level realities, whose properties are dependent on but distinct from the lower level from which they emerge. An example of a higher-level reality functioning as a downward causal force is consciousness – intentionality – generating meanings – intensionality.

Mind is a higher-level expression of the capacity of living organisms for discrimination. Our pursuit of self-set ideals such as truth and justice transforms our understanding of the world. The reductionistic attempt to reduce higher-level realities into lower-level realities generates what Polanyi calls a moral inversion, in which the higher is rejected with moral passion. Polanyi identifies it as a pathology of the modern mind and traces its origins to a false conception of knowledge; although it is relatively harmless in the formal sciences, that pathology generates nihilism in the humanities. Polanyi considered Marxism an example of moral inversion. The State, on the grounds of an appeal to the logic of history, uses its coercive powers in ways that disregard any appeals to morality.

Tacit knowledge, as distinct from explicit knowledge, is an influential term developed by Polanyi in "The Tacit Dimension" to describe the idea of know how, or the ability to do something, without necessarily being able to articulate it or even be aware of all the dimensions, for example being able to ride a bicycle or play a musical instrument. 





</doc>
<doc id="19712" url="https://en.wikipedia.org/wiki?curid=19712" title="Methanol">
Methanol

Methanol, also known as methyl alcohol amongst other names, is a chemical with the formula CHOH (a methyl group linked to a hydroxyl group, often abbreviated MeOH). Its formula can also be written as CH₄O. Methanol acquired the name wood alcohol because it was once produced chiefly by the destructive distillation of wood. Today, methanol is mainly produced industrially by hydrogenation of carbon monoxide.

Methanol is the simplest alcohol, consisting of a methyl group linked to a hydroxyl group. It is a light, volatile, colorless, flammable liquid with a distinctive odor similar to that of ethanol (drinking alcohol).
Methanol is however far more toxic than ethanol. At room temperature, it is a polar liquid. With more than 20 million tons produced annually, it is used as a precursor to other commodity chemicals, including formaldehyde, acetic acid, methyl tert-butyl ether, as well as a host of more specialized chemicals.
Small amounts of methanol are present in normal, healthy human individuals. One study found a mean of 4.5 ppm in the exhaled breath of test subjects. The mean endogenous methanol in humans of 0.45 g/d may be metabolized from pectin found in fruit; one kilogram of apple produces up to 1.4 g methanol.

Methanol is produced naturally in the anaerobic metabolism of many varieties of bacteria and is commonly present in small amounts in the environment. As a result, the atmosphere contains a small amount of methanol vapor. Atmospheric methanol is oxidized by air in sunlight to carbon dioxide and water over the course of days.

Researchers at the Woods Hole Oceanographic Institution have found oceanic phytoplankton to be a major source of methanol (), producing it in quantities that could rival or exceed that produced on land.

Methanol is also found in abundant quantities in star-forming regions of space and is used in astronomy as a marker for such regions. It is detected through its spectral emission lines.

In 2006, astronomers using the MERLIN array of radio telescopes at Jodrell Bank Observatory discovered a large cloud of methanol in space, 288 billion miles (463 billion km) across. In 2016, astronomers detected methanol in a planet-forming disc around the young star TW Hydrae using ALMA radio telescope.

Methanol has low acute toxicity in humans, but is dangerous because it is occasionally ingested in large volumes, often together with ethanol. As little as of pure methanol can cause permanent blindness by destruction of the optic nerve. is potentially fatal. The median lethal dose is , "i.e.", 1–2 mL/kg body weight of pure methanol. The reference dose for methanol is 0.5 mg/kg in a day. Toxic effects begin hours after ingestion, and antidotes can often prevent permanent damage. Because of its similarities in both appearance and odor to ethanol (the alcohol in beverages), it is difficult to differentiate between the two; such is also the case with denatured alcohol, adulterated liquors or very low quality alcoholic beverages. However, cases exist of methanol resistance, such as that of Mike Malloy who was the victim of a failed murder attempt by methanol in the early 1930s.

Methanol is toxic by two mechanisms. First, methanol can be fatal due to effects on the central nervous system, acting as a central nervous system depressant in the same manner as ethanol poisoning. Second, in a process of toxication, it is metabolized to formic acid (which is present as the formate ion) via formaldehyde in a process initiated by the enzyme alcohol dehydrogenase in the liver. Methanol is converted to formaldehyde via alcohol dehydrogenase (ADH) and formaldehyde is converted to formic acid (formate) via aldehyde dehydrogenase (ALDH). The conversion to formate via ALDH proceeds completely, with no detectable formaldehyde remaining. Formate is toxic because it inhibits mitochondrial cytochrome c oxidase, causing hypoxia at the cellular level, and metabolic acidosis, among a variety of other metabolic disturbances.
Outbreaks of methanol poisoning have occurred primarily due to contamination of drinking alcohol. This is more common in the developing world. In 2013 more than 1700 cases nonetheless occurred in the United States. Those affected are often adult men. Outcomes may be good with early treatment. Toxicity to methanol was described as early as 1856.

Because of its toxic properties, methanol is frequently used as a denaturant additive for ethanol manufactured for industrial uses. This addition of methanol exempts industrial ethanol (commonly known as "denatured alcohol" or "methylated spirit") from liquor excise taxation in the US and some other countries.

Methanol is primarily converted to formaldehyde, which is widely used in many areas, especially polymers. The conversion entails oxidation:
Acetic acid can be produced from methanol.
Methanol and isobutene are combined to give methyl "tert"-butyl ether (MTBE). MTBE is a major octane booster in gasoline.

Condensation of methanol to produce hydrocarbons and even aromatic systems is the basis of several technologies related to gas to liquids. These include methanol-to-hydrocarbons (MTH), methanol to gasoline (MTG), and methanol to olefins (MTO), and methanol to propylene (MTP). These conversions are catalyzed by zeolites as heterogeneous catalysts. The MTG process was once commercialized at Motunui in New Zealand.

The European Fuel Quality Directive allows fuel producers to blend up to 3% methanol, with an equal amount of cosolvent, with gasoline sold in Europe. China uses more than 4.5 billion Liters of methanol per year as a transportation fuel in low level blends for conventional vehicles, and high level blends in vehicles designed for methanol fuels.

Methanol is the precursor to most simple methylamines, methyl halides, and methyl ethers. Methyl esters are produced from methanol, including the transesterification of fats and production of biodiesel via transesterification.

Methanol is a promising energy carrier because, as a liquid, it is easier to store than hydrogen and natural gas. Its energy density is however low reflecting the fact that it represents partially combusted methane. Its energy density is 15.6 MJ/L, whereas ethanol's is 24 and gasoline's is 33 MJ/L.

Further advantages for methanol is its ready biodegradability and low toxicity. It does not persist in either aerobic (oxygen-present) or anaerobic (oxygen-absent) environments. The half-life for methanol in groundwater is just one to seven days, while many common gasoline components have half-lives in the hundreds of days (such as benzene at 10–730 days). Since methanol is miscible with water and biodegradable, it is unlikely to accumulate in groundwater, surface water, air or soil.

Methanol is occasionally used to fuel internal combustion engines. It burns forming carbon dioxide and water:
One problem with high concentrations of methanol in fuel is that alcohols corrode some metals, particularly aluminium. Methanol fuel has been proposed for ground transportation. The chief advantage of a methanol economy is that it could be adapted to gasoline internal combustion engines with minimum modification to the engines and to the infrastructure that delivers and stores liquid fuel. Its energy density is however only half that of gasoline, meaning that twice the volume of methanol would be required.

Methanol is a traditional denaturant for ethanol, the product being known as "denatured alcohol" or "methylated spirit". This was commonly used during the Prohibition to discourage consumption of bootlegged liquor, and ended up causing several deaths.

Methanol is used as a solvent and as an antifreeze in pipelines and windshield washer fluid. Methanol was used as an automobile coolant antifreeze in the early 1900s. As of May 2018, methanol was banned in the EU for use in windscreen washing or defrosting due to its risk of human consumption.

In some wastewater treatment plants, a small amount of methanol is added to wastewater to provide a carbon food source for the denitrifying bacteria, which convert nitrates to nitrogen gas and reduce the nitrification of sensitive aquifers.

Methanol is used as a destaining agent in polyacrylamide gel electrophoresis.

Direct-methanol fuel cells are unique in their low temperature, atmospheric pressure operation, which lets them be greatly miniaturized. This, combined with the relatively easy and safe storage and handling of methanol, may open the possibility of fuel cell-powered consumer electronics, such as laptop computers and mobile phones.

Methanol is also a widely used fuel in camping and boating stoves. Methanol burns well in an unpressurized burner, so alcohol stoves are often very simple, sometimes little more than a cup to hold fuel. This lack of complexity makes them a favorite of hikers who spend extended time in the wilderness. Similarly, the alcohol can be gelled to reduce risk of leaking or spilling, as with the brand "Sterno".

Methanol is mixed with water and injected into high performance diesel and gasoline engines for an increase of power and a decrease in intake air temperature in a process known as water methanol injection.

Carbon monoxide and hydrogen react over a catalyst to produce methanol. Today, the most widely used catalyst is a mixture of copper and zinc oxides, supported on alumina, as first used by ICI in 1966. At 5–10 MPa (50–100 atm) and , the reaction is characterized by high selectivity (>99.8%):

The production of synthesis gas from methane produces three moles of hydrogen for every mole of carbon monoxide, whereas the synthesis consumes only two moles of hydrogen gas per mole of carbon monoxide. One way of dealing with the excess hydrogen is to inject carbon dioxide into the methanol synthesis reactor, where it, too, reacts to form methanol according to the equation:

In terms of mechanism, the process occurs via initial conversion of CO into CO, which is then hydrogenated:
where the HO byproduct is recycled via the water-gas shift reaction
This gives an overall reaction, which is the same as listed above.

The catalytic conversion of methane to methanol is effected by enzymes including methane monooxygenases. These enzymes are mixed-function oxygenases, i.e. oxygenation is coupled with production of water and NAD.

CH + O + NADPH + H → CHOH + HO + NAD

Both Fe- and Cu-dependent enzymes have been characterized. Intense but largely fruitless efforts have been undertaken to emulate this reactivity. Methanol is more easily oxidized than is the feedstock methane, so the reactions tend not to be selective. Some strategies exist to circumvent this problem. Examples include Shilov systems and Fe- and Cu containing zeolites. These systems do not necessarily mimick the mechanisms employed by metalloenzymes, but draw some inspiration from them. Active sites can vary substantially from those known in the enzymes. For example, a dinuclear active site is proposed in the sMMO enzyme, whereas a mononuclear iron (alpha-Oxygen) is proposed in the Fe-zeolite.

Methanol is available commercially in various purity grades. Commercial methanol is generally classified according to ASTM purity grades A and AA. Methanol for chemical use normally corresponds to Grade AA. In addition to water, typical impurities include acetone and ethanol (which are very difficult to separate by distillation). UV-vis spectroscopy is a convenient method for detecting aromatic impurities. Water content can be determined by the Karl-Fischer titration.

In their embalming process, the ancient Egyptians used a mixture of substances, including methanol, which they obtained from the pyrolysis of wood. Pure methanol, however, was first isolated in 1661 by Robert Boyle, when he produced it via the distillation of buxus (boxwood). It later became known as "pyroxylic spirit". In 1834, the French chemists Jean-Baptiste Dumas and Eugene Peligot determined its elemental composition.

They also introduced the word "methylène" to organic chemistry, forming it from Greek "methy" = "alcoholic liquid" + "hȳlē" = "forest, wood, timber, material". "Methylène" designated a "radical" that was about 14% hydrogen by weight and contained one carbon atom. This would be CH, but at the time carbon was thought to have an atomic weight only six times that of hydrogen, so they gave the formula as CH. They then called wood alcohol (l'esprit de bois) "bihydrate de méthylène" (bihydrate because they thought the formula was CHO = (CH)(HO)). The term "methyl" was derived in about 1840 by back-formation from "methylene", and was then applied to describe "methyl alcohol". This was shortened to "methanol" in 1892 by the International Conference on Chemical Nomenclature. The suffix -yl, which, in organic chemistry, forms names of carbon groups, is from the word "methyl".

In 1923, the German chemists Alwin Mittasch and Mathias Pier, working for Badische-Anilin & Soda-Fabrik (BASF), developed a means to convert synthesis gas (a mixture of carbon monoxide, carbon dioxide, and hydrogen) into methanol. US patent 1,569,775 () was applied for on 4 Sep 1924 and issued on 12 January 1926; the process used a chromium and manganese oxide catalyst with extremely vigorous conditions: pressures ranging from 50 to 220 atm, and temperatures up to 450 °C. Modern methanol production has been made more efficient through use of catalysts (commonly copper) capable of operating at lower pressures. The modern low pressure methanol (LPM) process was developed by ICI in the late 1960s with the technology patent since long expired.

During World War II, methanol was used as a fuel in several German military rocket designs, under the name M-Stoff, and in a roughly 50/50 mixture with hydrazine, known as C-Stoff.

The use of methanol as a motor fuel received attention during the oil crises of the 1970s. By the mid-1990s, over 20,000 methanol "flexible fuel vehicles" capable of operating on methanol or gasoline were introduced in the U.S. In addition, low levels of methanol were blended in gasoline fuels sold in Europe during much of the 1980s and early-1990s. Automakers stopped building methanol FFVs by the late-1990s, switching their attention to ethanol-fueled vehicles. While the methanol FFV program was a technical success, rising methanol pricing in the mid- to late-1990s during a period of slumping gasoline pump prices diminished interest in methanol fuels.

In the early 1970s, a process was developed by Mobil for producing gasoline fuel from methanol.

Between the 1960s and 1980s methanol emerged as a precursor to the feedstock chemicals acetic acid and acetic anhydride. These processes include the Monsanto acetic acid synthesis, Cativa process, and Tennessee Eastman acetic anhydride process.





</doc>
<doc id="19714" url="https://en.wikipedia.org/wiki?curid=19714" title="Milk">
Milk

Milk is a nutrient-rich, white liquid food produced by the mammary glands of mammals. It is the primary source of nutrition for infant mammals (including humans who are breastfed) before they are able to digest other types of food. Early-lactation milk contains colostrum, which carries the mother's antibodies to its young and can reduce the risk of many diseases. It contains many other nutrients including protein and lactose. Interspecies consumption of milk is not uncommon, particularly among humans, many of whom consume the milk of other mammals.

As an agricultural product, milk, also called "dairy milk", is extracted from farm animals during or soon after pregnancy. Dairy farms produced about 730 million tonnes of milk in 2011, from 260 million dairy cows. India is the world's largest producer of milk, and is the leading exporter of skimmed milk powder, yet it exports few other milk products. The ever-increasing rise in domestic demand for dairy products and a large demand-supply gap could lead to India being a net importer of dairy products in the future. New Zealand, Germany and the Netherlands are the largest exporters of milk products. China and Russia were the world's largest importers of milk and milk products until 2016 when both countries became self-sufficient, contributing to a worldwide glut of milk.

Throughout the world, more than six billion people consume milk and milk products. Over 750 million people live in dairy farming households.

The term "milk" comes from "Old English "meoluc" (West Saxon), "milc" (Anglian), from Proto-Germanic *"meluks" "milk" (source also of Old Norse "mjolk", Old Frisian "melok", Old Saxon "miluk", Dutch "melk", Old High German "miluh", German "Milch", Gothic "miluks")".

In food use, from 1961, the term "milk" has been defined under Codex Alimentarius standards as: "the normal mammary secretion of milking animals obtained from one or more milkings without either addition to it or extraction from it, intended for consumption as liquid milk or for further processing." The term "dairy" relates to animal milk and animal milk production.

A substance secreted by pigeons to feed their young is called "crop milk" and bears some resemblance to mammalian milk, although it is not consumed as a milk substitute.

The definition above precludes non-animal products which resemble dairy milk in color and texture, such as almond milk, coconut milk, rice milk, and soy milk. In English, the word "milk" has been used to refer to "milk-like plant juices" since 1200 AD. Traditionally a variety of non-dairy products have been described with the word "milk", including the traditional digestive remedies milk of magnesia and milk of bismuth. Latex, the complex inedible emulsion that exudes from the stems of certain plants, is generally described as "milky" and is often sold as ""rubber milk"" because of its white appearance. The word "latex" itself is deducted from the Spanish word for milk.

A 2018 survey by the International Food Information Council Foundation suggests consumers in the United States do not typically confuse plant-based milk analogues with animal milk and dairy products. In the US, (mostly plant-based) milk alternatives now command 13% of the "milk" market, leading the US dairy industry to attempt, multiple times, to sue producers of dairy milk alternatives, to have the name "milk" limited to animal milk, so far without success. The Food and Drug Administration generally supports restricting the term "milk", while the US Department of Agriculture supports the continued use of terms such as "soymilk". In the European Union, words such as milk, butter, cheese, cream and yogurt are legally restricted to animal products, with exceptions such as "coconut milk", "almond milk", "peanut butter", and "ice cream".

Production of milk substitutes from vats of brewer's yeast is under development by organizations including Impossible Foods, Muufri, and the biohacker group "Real Vegan Cheese". Some components would be chemically identical to those in animal-derived milk; others, such as lactose, to which many people are allergic, may be substituted.

Milk consumption occurs in two distinct overall types: a natural source of nutrition for all infant mammals and a food product obtained from other mammals for consumption by humans of all ages.

In almost all mammals, milk is fed to infants through breastfeeding, either directly or by expressing the milk to be stored and consumed later. The early milk from mammals is called colostrum. Colostrum contains antibodies that provide protection to the newborn baby as well as nutrients and growth factors. The makeup of the colostrum and the period of secretion varies from species to species.

For humans, the World Health Organization recommends exclusive breastfeeding for six months and breastfeeding in addition to other food for up to two years of age or more. In some cultures it is common to breastfeed children for three to five years, and the period may be longer.

Fresh goats' milk is sometimes substituted for breast milk, which introduces the risk of the child developing electrolyte imbalances, metabolic acidosis, megaloblastic anemia, and a host of allergic reactions.

In many cultures, especially in the West, humans continue to consume milk beyond infancy, using the milk of other mammals (especially cattle, goats and sheep) as a food product. Initially, the ability to digest milk was limited to children as adults did not produce lactase, an enzyme necessary for digesting the lactose in milk. People therefore converted milk to curd, cheese and other products to reduce the levels of lactose. Thousands of years ago, a chance mutation spread in human populations in Europe that enabled the production of lactase in adulthood. This mutation allowed milk to be used as a new source of nutrition which could sustain populations when other food sources failed. Milk is processed into a variety of products such as cream, butter, yogurt, kefir, ice cream, and cheese. Modern industrial processes use milk to produce casein, whey protein, lactose, condensed milk, powdered milk, and many other food-additives and industrial products.

Whole milk, butter and cream have high levels of saturated fat. The sugar lactose is found only in milk, forsythia flowers, and a few tropical shrubs. The enzyme needed to digest lactose, lactase, reaches its highest levels in the human small intestine after birth and then begins a slow decline unless milk is consumed regularly. Those groups who do continue to tolerate milk, however, often have exercised great creativity in using the milk of domesticated ungulates, not only of cattle, but also sheep, goats, yaks, water buffalo, horses, reindeer and camels. India is the largest producer and consumer of cattle and buffalo milk in the world.

Humans first learned to consume the milk of other mammals regularly following the domestication of animals during the Neolithic Revolution or the development of agriculture. This development occurred independently in several global locations from as early as 9000–7000 BC in Mesopotamia to 3500–3000 BC in the Americas. People first domesticated the most important dairy animals – cattle, sheep and goats – in Southwest Asia, although domestic cattle had been independently derived from wild aurochs populations several times since. Initially animals were kept for meat, and archaeologist Andrew Sherratt has suggested that dairying, along with the exploitation of domestic animals for hair and labor, began much later in a separate secondary products revolution in the fourth millennium BC. Sherratt's model is not supported by recent findings, based on the analysis of lipid residue in prehistoric pottery, that shows that dairying was practiced in the early phases of agriculture in Southwest Asia, by at least the seventh millennium BC.

From Southwest Asia domestic dairy animals spread to Europe (beginning around 7000 BC but did not reach Britain and Scandinavia until after 4000 BC), and South Asia (7000–5500 BC). The first farmers in central Europe and Britain milked their animals. Pastoral and pastoral nomadic economies, which rely predominantly or exclusively on domestic animals and their products rather than crop farming, were developed as European farmers moved into the Pontic-Caspian steppe in the fourth millennium BC, and subsequently spread across much of the Eurasian steppe. Sheep and goats were introduced to Africa from Southwest Asia, but African cattle may have been independently domesticated around 7000–6000 BC. Camels, domesticated in central Arabia in the fourth millennium BC, have also been used as dairy animals in North Africa and the Arabian Peninsula. The earliest Egyptian records of burn treatments describe burn dressings using milk from mothers of male babies. In the rest of the world (i.e., East and Southeast Asia, the Americas and Australia) milk and dairy products were historically not a large part of the diet, either because they remained populated by hunter-gatherers who did not keep animals or the local agricultural economies did not include domesticated dairy species. Milk consumption became common in these regions comparatively recently, as a consequence of European colonialism and political domination over much of the world in the last 500 years.

In the Middle Ages, milk was called the "virtuous white liquor" because alcoholic beverages were safer to consume than water.

The growth in urban population, coupled with the expansion of the railway network in the mid-19th century, brought about a revolution in milk production and supply. Individual railway firms began transporting milk from rural areas to London from the 1840s and 1850s. Possibly the first such instance was in 1846, when St Thomas's Hospital in Southwark contracted with milk suppliers outside London to ship milk by rail. The Great Western Railway was an early and enthusiastic adopter, and began to transport milk into London from Maidenhead in 1860, despite much criticism. By 1900, the company was transporting over 25 million gallons annually. The milk trade grew slowly through the 1860s, but went through a period of extensive, structural change in the 1870s and 1880s.
Urban demand began to grow, as consumer purchasing power increased and milk became regarded as a required daily commodity. Over the last three decades of the 19th century, demand for milk in most parts of the country doubled, or in some cases, tripled. Legislation in 1875 made the adulteration of milk illegal – this combined with a marketing campaign to change the image of milk. The proportion of rural imports by rail as a percentage of total milk consumption in London grew from under 5% in the 1860s to over 96% by the early 20th century. By that point, the supply system for milk was the most highly organized and integrated of any food product.
The first glass bottle packaging for milk was used in the 1870s. The first company to do so may have been the New York Dairy Company in 1877. The Express Dairy Company in England began glass bottle production in 1880. In 1884, Hervey Thatcher, an American inventor from New York, invented a glass milk bottle, called "Thatcher's Common Sense Milk Jar," which was sealed with a waxed paper disk. Later, in 1932, plastic-coated paper milk cartons were introduced commercially.

In 1863, French chemist and biologist Louis Pasteur invented pasteurization, a method of killing harmful bacteria in beverages and food products. He developed this method while on summer vacation in Arbois, to remedy the frequent acidity of the local wines. He found out experimentally that it is sufficient to heat a young wine to only about for a brief time to kill the microbes, and that the wine could be nevertheless properly aged without sacrificing the final quality. In honor of Pasteur, the process became known as "pasteurization". Pasteurization was originally used as a way of preventing wine and beer from souring. Commercial pasteurizing equipment was produced in Germany in the 1880s, and producers adopted the process in Copenhagen and Stockholm by 1885.

Continued improvements in the efficiency of milk production led to a worldwide glut of milk by 2016. Russia and China became self-sufficient and stopped importing milk. Canada has tried to restrict milk production by forcing new farmers/increased capacity to "buy in" at C$24,000 per cow. Importing milk is prohibited. The European Union theoretically stopped subsidizing dairy farming in 2015. Direct subsidies were replaced by "environmental incentives" which results in the government buying milk when the price falls to €200 per . The United States has a voluntary insurance program that pays farmers depending upon the price of milk and the cost of feed.

The females of all mammal species can by definition produce milk, but cow's milk dominates commercial production. In 2011, FAO estimates 85% of all milk worldwide was produced from cows. Human milk is not produced or distributed industrially or commercially; however, human milk banks collect donated human breastmilk and redistribute it to infants who may benefit from human milk for various reasons (premature neonates, babies with allergies, metabolic diseases, etc.) but who cannot breastfeed.

In the Western world, cow's milk is produced on an industrial scale and is by far the most commonly consumed form of milk. Commercial dairy farming using automated milking equipment produces the vast majority of milk in developed countries. Dairy cattle such as the Holstein have been bred selectively for increased milk production. About 90% of the dairy cows in the United States and 85% in Great Britain are Holsteins. Other dairy cows in the United States include Ayrshire, Brown Swiss, Guernsey, Jersey and Milking Shorthorn (Dairy Shorthorn).

Aside from cattle, many kinds of livestock provide milk used by humans for dairy products. These animals include water buffalo, goat, sheep, camel, donkey, horse, reindeer and yak. The first four respectively produced about 11%, 2%, 1.4% and 0.2% of all milk worldwide in 2011.

In Russia and Sweden, small moose dairies also exist.

According to the U.S. National Bison Association, American bison (also called American buffalo) are not milked commercially; however, various sources report cows resulting from cross-breeding bison and domestic cattle are good milk producers, and have been used both during the European settlement of North America and during the development of commercial Beefalo in the 1970s and 1980s.

Swine are almost never milked, even though their milk is similar to cow's milk and perfectly suitable for human consumption. The main reasons for this are that milking a sow's numerous small teats is very cumbersome, and that sows can not store their milk as cows can. A few pig farms do sell pig cheese as a novelty item; these cheeses are exceedingly expensive.

In 2012, the largest producer of milk and milk products was India followed by the United States of America, China, Pakistan and Brazil. All 28 European Union members together produced 153.8 million tonnes of milk in 2013, the largest by any politico-economic union.

Increasing affluence in developing countries, as well as increased promotion of milk and milk products, has led to a rise in milk consumption in developing countries in recent years. In turn, the opportunities presented by these growing markets have attracted investments by multinational dairy firms. Nevertheless, in many countries production remains on a small scale and presents significant opportunities for diversification of income sources by small farms. Local milk collection centers, where milk is collected and chilled prior to being transferred to urban dairies, are a good example of where farmers have been able to work on a cooperative basis, particularly in countries such as India.

FAO reports Israel dairy farms are the most productive in the world, with a yield of milk per cow per year. This survey over 2001 and 2007 was conducted by ICAR (International Committee for Animal Recording) across 17 developed countries. The survey found that the average herd size in these developed countries increased from 74 to 99 cows per herd between 2001 and 2007. A dairy farm had an average of 19 cows per herd in Norway, and 337 in New Zealand. Annual milk production in the same period increased from per cow in these developed countries. The lowest average production was in New Zealand at per cow. The milk yield per cow depended on production systems, nutrition of the cows, and only to a minor extent different genetic potential of the animals. What the cow ate made the most impact on the production obtained. New Zealand cows with the lowest yield per year grazed all year, in contrast to Israel with the highest yield where the cows ate in barns with an energy-rich mixed diet.

The milk yield per cow in the United States, the world's largest cow milk producer, was per year in 2010. In contrast, the milk yields per cow in India and China – the second and third largest producers – were respectively and per year.

It was reported in 2007 that with increased worldwide prosperity and the competition of bio-fuel production for feed stocks, both the demand for and the price of milk had substantially increased worldwide. Particularly notable was the rapid increase of consumption of milk in China and the rise of the price of milk in the United States above the government subsidized price. In 2010 the Department of Agriculture predicted farmers would receive an average of $1.35 per U.S. gallon of cow's milk (35 cents per liter), which is down 30 cents per gallon from 2007 and below the break-even point for many cattle farmers.

The consumption of cow's milk poses numerous threats to the natural environment. Compared to plant milks, cow's milk requires the most land and water, and its production results in the greatest amount of greenhouse gas (GHG) emissions, air pollution, and water pollution. A 2010 UN report, "Assessing the Environmental Impacts of Consumption and Production", argued that animal products, including dairy, "in general require more resources and cause higher emissions than plant-based alternatives". It proposed a move away from animal products to reduce environmental damage.

The global water footprint of animal agriculture is 2,422 billion cubic meters of water (one-fourth of the total global water footprint), 19 percent of which is related to dairy cattle. A 2012 study found that 98 percent of milk’s footprint can be traced back to the cows food.

A 2010 Food and Agriculture Organization report found that the global dairy sector contributes to four percent of the total global anthropogenic GHG emissions. This figure includes emissions allotted to milk production, processing and transportation, and the emissions from fattening and slaughtering dairy cows. The same report found that 52 percent of the GHGs produced by dairy cattle is methane, and nitrous oxide makes up for another 27 percent of dairy cattle’s GHG emission. It is estimated that cows produce between 250 and 500 liters of methane a day. Methane has a heat-trapping potential nearly 100 times larger than carbon dioxide, and nitrous oxide has a global warming potential almost 300 times greater than carbon dioxide.

Milk is an emulsion or colloid of butterfat globules within a water-based fluid that contains dissolved carbohydrates and protein aggregates with minerals. Because it is produced as a food source for the young, all of its contents provide benefits for growth. The principal requirements are energy (lipids, lactose, and protein), biosynthesis of non-essential amino acids supplied by proteins (essential amino acids and amino groups), essential fatty acids, vitamins and inorganic elements, and water.

The pH of milk ranges from 6.4 to 6.8 and it changes over time. Milk from other bovines and non-bovine mammals varies in composition, but has a similar pH.

Initially milk fat is secreted in the form of a fat globule surrounded by a membrane. Each fat globule is composed almost entirely of triacylglycerols and is surrounded by a membrane consisting of complex lipids such as phospholipids, along with proteins. These act as emulsifiers which keep the individual globules from coalescing and protect the contents of these globules from various enzymes in the fluid portion of the milk. Although 97–98% of lipids are triacylglycerols, small amounts of di- and monoacylglycerols, free cholesterol and cholesterol esters, free fatty acids, and phospholipids are also present. Unlike protein and carbohydrates, fat composition in milk varies widely in the composition due to genetic, lactational, and nutritional factor difference between different species.

Like composition, fat globules vary in size from less than 0.2 to about 15 micrometers in diameter between different species. Diameter may also vary between animals within a species and at different times within a milking of a single animal. In unhomogenized cow's milk, the fat globules have an average diameter of two to four micrometers and with homogenization, average around 0.4 micrometers. The fat-soluble vitamins A, D, E, and K along with essential fatty acids such as linoleic and linolenic acid are found within the milk fat portion of the milk.

Normal bovine milk contains 30–35 grams of protein per liter of which about 80% is arranged in casein micelles. Total proteins in milk represent 3.2% of its composition (nutrition table).

The largest structures in the fluid portion of the milk are "casein micelles": aggregates of several thousand protein molecules with superficial resemblance to a surfactant micelle, bonded with the help of nanometer-scale particles of calcium phosphate. Each casein micelle is roughly spherical and about a tenth of a micrometer across. There are four different types of casein proteins: αs1-, αs2-, β-, and κ-caseins. Most of the casein proteins are bound into the micelles. There are several competing theories regarding the precise structure of the micelles, but they share one important feature: the outermost layer consists of strands of one type of protein, k-casein, reaching out from the body of the micelle into the surrounding fluid. These kappa-casein molecules all have a negative electrical charge and therefore repel each other, keeping the micelles separated under normal conditions and in a stable colloidal suspension in the water-based surrounding fluid.

Milk contains dozens of other types of proteins beside caseins and including enzymes. These other proteins are more water-soluble than caseins and do not form larger structures. Because the proteins remain suspended in whey remaining when caseins coagulate into curds, they are collectively known as "whey proteins". Lactoglobulin is the most common whey protein by a large margin. The ratio of caseins to whey proteins varies greatly between species; for example, it is 82:18 in cows and around 32:68 in humans.

Minerals or milk salts, are traditional names for a variety of cations and anions within bovine milk. Calcium, phosphate, magnesium, sodium, potassium, citrate, and chloride are all included as minerals and they typically occur at concentration of 5–40 mM. The milk salts strongly interact with casein, most notably calcium phosphate. It is present in excess and often, much greater excess of solubility of solid calcium phosphate. In addition to calcium, milk is a good source of many other vitamins. Vitamins A, B6, B12, C, D, K, E, thiamine, niacin, biotin, riboflavin, folates, and pantothenic acid are all present in milk.

For many years the most accepted theory of the structure of a micelle was that it was composed of spherical casein aggregates, called submicelles, that were held together by calcium phosphate linkages. However, there are two recent models of the casein micelle that refute the distinct micellular structures within the micelle.

The first theory attributed to de Kruif and Holt, proposes that nanoclusters of calcium phosphate and the phosphopeptide fraction of beta-casein are the centerpiece to micellular structure. Specifically in this view, unstructured proteins organize around the calcium phosphate giving rise to their structure and thus no specific structure is formed.

The second theory proposed by Horne, the growth of calcium phosphate nanoclusters begins the process of micelle formation but is limited by binding phosphopeptide loop regions of the caseins. Once bound, protein-protein interactions are formed and polymerization occurs, in which K-casein is used as an end cap, to form micelles with trapped calcium phosphate nanoclusters.

Some sources indicate that the trapped calcium phosphate is in the form of Ca9(PO4)6;
whereas, others say it is similar to the structure of the mineral brushite CaHPO4 -2H2O.

Milk contains several different carbohydrate including lactose, glucose, galactose, and other oligosaccharides. The lactose gives milk its sweet taste and contributes approximately 40% of whole cow's milk's calories. Lactose is a disaccharide composite of two simple sugars, glucose and galactose. Bovine milk averages 4.8% anhydrous lactose, which amounts to about 50% of the total solids of skimmed milk. Levels of lactose are dependent upon the type of milk as other carbohydrates can be present at higher concentrations than lactose in milks.

Other components found in raw cow's milk are living white blood cells, mammary gland cells, various bacteria, and a large number of active enzymes.

Both the fat globules and the smaller casein micelles, which are just large enough to deflect light, contribute to the opaque white color of milk. The fat globules contain some yellow-orange carotene, enough in some breeds (such as Guernsey and Jersey cattle) to impart a golden or "creamy" hue to a glass of milk. The riboflavin in the whey portion of milk has a greenish color, which sometimes can be discerned in skimmed milk or whey products. Fat-free skimmed milk has only the casein micelles to scatter light, and they tend to scatter shorter-wavelength blue light more than they do red, giving skimmed milk a bluish tint.

In most Western countries, centralized dairy facilities process milk and products obtained from milk, such as cream, butter, and cheese. In the U.S., these dairies usually are local companies, while in the Southern Hemisphere facilities may be run by large multi-national corporations such as Fonterra.

Pasteurization is used to kill harmful pathogenic bacteria by heating the milk for a short time and then immediately cooling it. Types of pasteurized milk include full cream, reduced fat, skim milk, calcium enriched, flavored, and UHT. The standard high temperature short time (HTST) process of 72 °C for 15 seconds completely kills pathogenic bacteria in milk, rendering it safe to drink for up to three weeks if continually refrigerated. Dairies print best before dates on each container, after which stores remove any unsold milk from their shelves.

A side effect of the heating of pasteurization is that some vitamin and mineral content is lost. Soluble calcium and phosphorus decrease by 5%, thiamin and vitamin B12 by 10%, and vitamin C by 20%. Because losses are small in comparison to the large amount of the two B-vitamins present, milk continues to provide significant amounts of thiamin and vitamin B12. The loss of vitamin C is not nutritionally significant, as milk is not an important dietary source of vitamin C.

Microfiltration is a process that partially replaces pasteurization and produces milk with fewer microorganisms and longer shelf life without a change in the taste of the milk. In this process, cream is separated from the skimmed milk and is pasteurized in the usual way, but the skimmed milk is forced through ceramic microfilters that trap 99.9% of microorganisms in the milk (as compared to 99.999% killing of microorganisms in standard HTST pasteurization). The skimmed milk then is recombined with the pasteurized cream to reconstitute the original milk composition.

Ultrafiltration uses finer filters than microfiltration, which allow lactose and water to pass through while retaining fats, calcium and protein. As with microfiltration, the fat may be removed before filtration and added back in afterwards. Ultrafiltered milk is used is cheesemaking, since it has reduced volume for a given protein content, and is sold directly to consumers as a higher protein, lower sugar content, and creamier alternative to regular milk.

Upon standing for 12 to 24 hours, fresh milk has a tendency to separate into a high-fat cream layer on top of a larger, low-fat milk layer. The cream often is sold as a separate product with its own uses. Today the separation of the cream from the milk usually is accomplished rapidly in centrifugal cream separators. The fat globules rise to the top of a container of milk because fat is less dense than water.

The smaller the globules, the more other molecular-level forces prevent this from happening. The cream rises in cow's milk much more quickly than a simple model would predict: rather than isolated globules, the fat in the milk tends to form into clusters containing about a million globules, held together by a number of minor whey proteins. These clusters rise faster than individual globules can. The fat globules in milk from goats, sheep, and water buffalo do not form clusters as readily and are smaller to begin with, resulting in a slower separation of cream from these milks.

Milk often is homogenized, a treatment that prevents a cream layer from separating out of the milk. The milk is pumped at high pressures through very narrow tubes, breaking up the fat globules through turbulence and cavitation. A greater number of smaller particles possess more total surface area than a smaller number of larger ones, and the original fat globule membranes cannot completely cover them. Casein micelles are attracted to the newly exposed fat surfaces. 

Nearly one-third of the micelles in the milk end up participating in this new membrane structure. The casein weighs down the globules and interferes with the clustering that accelerated separation. The exposed fat globules are vulnerable to certain enzymes present in milk, which could break down the fats and produce rancid flavors. To prevent this, the enzymes are inactivated by pasteurizing the milk immediately before or during homogenization.

Homogenized milk tastes blander but feels creamier in the mouth than unhomogenized. It is whiter and more resistant to developing off flavors. Creamline (or cream-top) milk is unhomogenized. It may or may not have been pasteurized. Milk that has undergone high-pressure homogenization, sometimes labeled as "ultra-homogenized", has a longer shelf life than milk that has undergone ordinary homogenization at lower pressures.

Ultra Heat Treatment (UHT), is a type of milk processing where all bacteria are destroyed with high heat to extend its shelf life for up to 6 months, as long as the package is not opened. Milk is firstly homogenized and then is heated to 138 degrees Celsius for 1–3 seconds. The milk is immediately cooled down and packed into a sterile container. As a result of this treatment, all the pathogenic bacteria within the milk are destroyed, unlike when the milk is just pasteurised. The milk will now keep for up for 6 months if unopened. UHT milk does not need to be refrigerated until the package is opened, which makes it easier to ship and store. But in this process there is a loss of vitamin B1 and vitamin C and there is also a slight change in the taste of the milk.

The composition of milk differs widely among species. Factors such as the type of protein; the proportion of protein, fat, and sugar; the levels of various vitamins and minerals; and the size of the butterfat globules, and the strength of the curd are among those that may vary. For example:

Donkey and horse milk have the lowest fat content, while the milk of seals and whales may contain more than 50% fat.

These compositions vary by breed, animal, and point in the lactation period.

The protein range for these four breeds is 3.3% to 3.9%, while the lactose range is 4.7% to 4.9%.

Milk fat percentages may be manipulated by dairy farmers' stock diet formulation strategies. Mastitis infection can cause fat levels to decline.

Processed cow's milk was formulated to contain differing amounts of fat during the 1950s. One cup (250 mL) of 2%-fat cow's milk contains 285 mg of calcium, which represents 22% to 29% of the daily recommended intake (DRI) of calcium for an adult. Depending on its age, milk contains 8 grams of protein, and a number of other nutrients (either naturally or through fortification) including:

The U.S. federal government document "Dietary Guidelines for Americans, 2010" recommends consumption of three glasses of fat-free or low-fat milk for adults and children 9 and older (less for younger children) per day. This recommendation is disputed by some health researchers who call for more study of the issue, given that there are other sources for calcium and vitamin D. The researchers also claim that the recommendations have been unduly influenced by the American dairy industry, and that whole milk may be better for health due to its increased ability to satiate hunger.

A 2008 review found evidence suggesting that consumption of milk is effective at promoting muscle growth. Some studies have suggested that conjugated linoleic acid, which can be found in dairy products, is an effective supplement for reducing body fat.

The amount of calcium from milk that is absorbed by the human body is disputed. Calcium from dairy products has a greater bioavailability than calcium from certain vegetables, such as spinach, that contain high levels of calcium-chelating agents, but a similar or lesser bioavailability than calcium from low-oxalate vegetables such as kale, broccoli, or other vegetables in the genus "Brassica".

Milk as a calcium source has been questioned in media, but scientific research is lacking to support the hypothesis of acidosis induced by milk. The hypothesis in question being that acidosis would lead to leaching of calcium storages in bones to neutralize pH levels (also known as acid-ash hypothesis).
Research has found no link between metabolic acidosis and consumption of milk.

A 2011 meta-analysis examining whether milk consumption might protect against hip fracture in middle-aged and older adults found no association between drinking milk and lower rates of fractures.

A 2009 meta-analysis concluded that milk consumption increases the risk of acquiring acne.

Lactose, the disaccharide sugar component of all milk, must be cleaved in the small intestine by the enzyme lactase, in order for its constituents, galactose and glucose, to be absorbed. Lactose intolerance is a condition in which people have symptoms due to not enough of the enzyme lactase in the small intestines. Those affected vary in the amount of lactose they can tolerate before symptoms develop. These may include abdominal pain, bloating, diarrhea, gas, and nausea. Severity depends on the amount a person eats or drinks. Those affected are usually able to drink at least one cup of milk without developing significant symptoms, with greater amounts tolerated if drunk with a meal or throughout the day.

Lactose intolerance does not cause damage to the gastrointestinal tract. There are four types: primary, secondary, developmental, and congenital. Primary lactose intolerance is when the amount of lactase decline as people age. Secondary lactose intolerance is due to injury to the small intestine such as from infection, celiac disease, inflammatory bowel disease, or other diseases. Developmental lactose intolerance may occur in premature babies and usually improves over a short period of time. Congenital lactose intolerance is an extremely rare genetic disorder in which little or no lactase is made from birth. When lactose intolerance is due to secondary lactase deficiency, treatment of the underlying disease allows lactase activity to return to normal levels. Lactose intolerance is different from a milk allergy.

The number of people with lactose intolerance is unknown. The number of adults who cannot produce enough lactase in their small intestine varies markedly in different populations. Since lactase's only function is the digestion of lactose in milk, in most mammal species the activity of the enzyme is dramatically reduced after weaning. Within most human populations, however, some individuals have developed, by natural evolution, the ability to maintain throughout their life high levels of lactose in their small intestine, as an adaptation to the consumption of nonhuman milk and dairy products beyond infancy. This ability, which allows them to digest lactose into adulthood, is called lactase persistence. The distribution of people with lactase persistence is not homogeneous in the world. For instance, those people with lactase persistence are more than 90% of the population in North Europe, and as low as 5% in parts of Asia and Africa.

Milk and dairy products have the potential for causing serious infection in newborn infants. Unpasteurized milk and cheeses can promote the growth of "Listeria" bacteria. "Listeria monocytogenes" can also cause serious infection in an infant and pregnant woman and can be transmitted to her infant in utero or after birth. The infection has the potential of seriously harming or even causing the death of a preterm infant, an infant of low or very low birth weight, or an infant with a congenital defect of the immune system. The presence of this pathogen can sometimes be determined by the symptoms that appear as a gastrointestinal illness in the mother. The mother can also acquire infection from ingesting food that contains other animal products such as hot dogs, delicatessen meats, and cheese.

Cow's milk allergy (CMA) is an immunologically mediated adverse reaction, rarely fatal, to one or more cow's milk proteins. 2.2–3.5% of the global infant population are allergic to cow's milk.

Milk must be offered at every meal if a United States school district wishes to get reimbursement from the federal government. A quarter of the largest school districts in the U.S. offer rice or soy milk and almost 17% of all U.S. school districts offer lactose-free milk. Of the milk served in U.S. school cafeterias, 71% is flavored, causing some school districts to propose a ban because flavored milk has added sugars. (Though some flavored milk products use artificial sweeteners instead.) The Boulder, Colorado, school district banned flavored milk in 2009 and instead installed a dispenser that keeps the milk colder.

The mammary gland is thought to have derived from apocrine skin glands. It has been suggested that the original function of lactation (milk production) was keeping eggs moist. Much of the argument is based on monotremes (egg-laying mammals). The original adaptive significance of milk secretions may have been nutrition or immunological protection. This secretion gradually became more copious and accrued nutritional complexity over evolutionary time.

Tritylodontid cynodonts seem to have displayed lactation, based on their dental replacement patterns.

Since November 1993, recombinant bovine somatotropin (rbST), also called rBGH, has been sold to dairy farmers with FDA approval. Cows produce bovine growth hormone naturally, but some producers administer an additional recombinant version of BGH which is produced through genetically engineered E. coli to increase milk production. Bovine growth hormone also stimulates liver production of insulin-like growth factor 1 (IGF1). The U.S. Food and Drug Administration, the National Institutes of Health and the World Health Organization have reported that both of these compounds are safe for human consumption at the amounts present.

Milk from cows given rBST may be sold in the United States, and the FDA stated that no significant difference has been shown between milk derived from rBST-treated and that from non-rBST-treated cows. Milk that advertises that it comes from cows not treated with rBST, is required to state this finding on its label.

Cows receiving rBGH supplements may more frequently contract an udder infection known as mastitis. Problems with mastitis have led to Canada, Australia, New Zealand, and Japan banning milk from rBST treated cows. Mastitis, among other diseases, may be responsible for the fact that levels of white blood cells in milk vary naturally.

rBGH is also banned in the European Union, for reasons of animal welfare.

Vegans and some other vegetarians do not consume milk for reasons mostly related to animal rights and environmental concerns. They may object to features of dairy farming including the necessity of keeping dairy cows pregnant, the killing of almost all the male offspring of dairy cows (either by disposal soon after birth, for veal production, or for beef), the routine separation of mother and calf soon after birth, other perceived inhumane treatment of dairy cattle, and culling of cows after their productive lives.

Other people who do not drink milk are convinced that milk is not necessary for good health or that it may cause adverse health effects. Several studies have indicated that milk consumption does not result in stronger bones. Other studies have found that milk intake increases the risk of acquiring acne.

It is often argued that it is unnatural for humans to drink milk from cows (or other animals) because mammals normally do not drink milk beyond the weaning period, nor do they drink milk from another species.

Some have criticized the American government's promotion of milk consumption. Their main concern is the financial interest that the American government has taken in the dairy industry, promoting milk as the best source of calcium. All United States schools that are a part of the federally funded National School Lunch Act are required by the federal government to provide milk for all students. The Office of Dietary Supplements recommends that healthy adults between ages 19 and 50 get about 1,000 mg of calcium per day.

Milk production is also resource intensive. On a global weighted average, for the production of a given volume of milk, a thousand times as much water has to be used.

Milk products are sold in a number of varieties based on types/degrees of:

Milk preserved by the UHT process does not need to be refrigerated before opening and has a much longer shelf life (six months) than milk in ordinary packaging. It is typically sold unrefrigerated in the UK, U.S., Europe, Latin America, and Australia.

Lactose-free milk can be produced by passing milk over lactase enzyme bound to an inert carrier. Once the molecule is cleaved, there are no lactose ill effects. Forms are available with reduced amounts of lactose (typically 30% of normal), and alternatively with nearly 0%. The only noticeable difference from regular milk is a slightly sweeter taste due to the generation of glucose by lactose cleavage. It does not, however, contain more glucose, and is nutritionally identical to regular milk.

Finland, where approximately 17% of the Finnish-speaking population has hypolactasia, has had "HYLA" (acronym for "hydrolysed lactose") products available for many years. Lactose of low-lactose level cow's milk products, ranging from ice cream to cheese, is enzymatically hydrolysed into glucose and galactose. The ultra-pasteurization process, combined with aseptic packaging, ensures a long shelf life. In 2001, Valio launched a lactose-free milk drink that is not sweet like HYLA milk but has the fresh taste of ordinary milk. Valio patented the chromatographic separation method to remove lactose. Valio also markets these products in Sweden, Estonia, Belgium, and the United States, where the company says ultrafiltration is used.

In the UK, where an estimated 4.7% of the population are affected by lactose intolerance, Lactofree produces milk, cheese, and yogurt products that contain only 0.03% lactose.

To aid digestion in those with lactose intolerance, milk with added bacterial cultures such as "Lactobacillus acidophilus" ("acidophilus milk") and bifidobacteria ("a/B milk") is available in some areas. Another milk with "Lactococcus lactis" bacteria cultures ("cultured buttermilk") often is used in cooking to replace the traditional use of naturally soured milk, which has become rare due to the ubiquity of pasteurization, which also kills the naturally occurring Lactococcus bacteria.

Lactose-free and lactose-reduced milk can also be produced via ultra filtration, which removes smaller molecules such as lactose and water while leaving calcium and proteins behind. Milk produced via these methods has a lower sugar content than regular milk.

In areas where the cattle (and often the people) live indoors, commercially sold milk commonly has vitamin D added to it to make up for lack of exposure to UVB radiation.

Reduced-fat milks often have added vitamin A palmitate to compensate for the loss of the vitamin during fat removal; in the United States this results in reduced fat milks having a higher vitamin A content than whole milk.

Milk often has flavoring added to it for better taste or as a means of improving sales. Chocolate milk has been sold for many years and has been followed more recently by strawberry milk and others. Some nutritionists have criticized flavored milk for adding sugar, usually in the form of high-fructose corn syrup, to the diets of children who are already commonly obese in the U.S.

Due to the short shelf life of normal milk, it used to be delivered to households daily in many countries; however, improved refrigeration at home, changing food shopping patterns because of supermarkets, and the higher cost of home delivery mean that daily deliveries by a milkman are no longer available in most countries.

In Australia and New Zealand, prior to metrication, milk was generally distributed in 1 pint (568mL) glass bottles. In Australia and Ireland there was a government funded "free milk for school children" program, and milk was distributed at morning recess in 1/3 pint bottles. With the conversion to metric measures, the milk industry were concerned that the replacement of the pint bottles with 500mL bottles would result in a 13.6% drop in milk consumption; hence, all pint bottles were recalled and replaced by 600mL bottles. With time, due to the steadily increasing cost of collecting, transporting, storing and cleaning glass bottles, they were replaced by cardboard cartons. A number of designs were used, including a tetrahedron which could be close-packed without waste space, and could not be knocked over accidentally. (slogan: No more crying over spilt milk.) However, the industry eventually settled on a design similar to that used in the United States.

Milk is now available in a variety of sizes in paperboard milk cartons (250 mL, 375 mL, 600 mL, 1 liter and 1.5 liters) and plastic bottles (1, 2 and 3 liters). A significant addition to the marketplace has been "long-life" milk (UHT), generally available in 1 and 2 liter rectangular cardboard cartons. In urban and suburban areas where there is sufficient demand, home delivery is still available, though in suburban areas this is often 3 times per week rather than daily. Another significant and popular addition to the marketplace has been flavored milks – for example, as mentioned above, Farmers Union Iced Coffee outsells Coca-Cola in South Australia. 

In rural India, milk is home delivered, daily, by local milkmen carrying bulk quantities in a metal container, usually on a bicycle. In other parts of metropolitan India, milk is usually bought or delivered in plastic bags or cartons via shops or supermarkets.

The current milk chain flow in India is from milk producer to milk collection agent. Then it is transported to a milk chilling center and bulk transported to the processing plant, then to the sales agent and finally to the consumer.

A 2011 survey by the Food Safety and Standards Authority of India found that nearly 70% of samples had not conformed to the standards set for milk. The study found that due to lack of hygiene and sanitation in milk handling and packaging, detergents (used during cleaning operations) were not washed properly and found their way into the milk. About 8% of samples in the survey were found to have detergents, which are hazardous to health.

In Pakistan, milk is supplied in jugs. Milk has been a staple food, especially among the pastoral tribes in this country.

Since the late 1990s, milk-buying patterns have changed drastically in the UK. The classic milkman, who travels his local milk round (route) using a milk float (often battery powered) during the early hours and delivers milk in 1 pint glass bottles with aluminium foil tops directly to households, has almost disappeared. Two of the main reasons for the decline of UK home deliveries by milkmen are household refrigerators (which lessen the need for daily milk deliveries) and private car usage (which has increased supermarket shopping). Another factor is that it is cheaper to purchase milk from a supermarket than from home delivery. In 1996, more than 2.5 billion liters of milk were still being delivered by milkmen, but by 2006 only 637 million liters (13% of milk consumed) was delivered by some 9,500 milkmen. By 2010, the estimated number of milkmen had dropped to 6,000. Assuming that delivery per milkman is the same as it was in 2006, this means milkmen deliveries now only account for 6–7% of all milk consumed by UK households (6.7 billion liters in 2008/2009).

Almost 95% of all milk in the UK is thus sold in shops today, most of it in plastic bottles of various sizes, but some also in milk cartons. Milk is hardly ever sold in glass bottles in UK shops.

In the United States, glass milk bottles have been replaced mostly with milk cartons and plastic jugs. Gallons of milk are almost always sold in jugs, while half gallons and quarts may be found in both paper cartons and plastic jugs, and smaller sizes are almost always in cartons.

The "half pint" () milk carton is the traditional unit as a component of school lunches, though some companies have replaced that unit size with a plastic bottle, which is also available at retail in 6- and 12-pack size.

Glass milk bottles are now rare. Most people purchase milk in bags, plastic bottles, or plastic-coated paper cartons. Ultraviolet (UV) light from fluorescent lighting can alter the flavor of milk, so many companies that once distributed milk in transparent or highly translucent containers are now using thicker materials that block the UV light.
Milk comes in a variety of containers with local variants:



Practically everywhere, condensed milk and evaporated milk are distributed in metal cans, 250 and 125 mL paper containers and 100 and 200 mL squeeze tubes, and powdered milk (skim and whole) is distributed in boxes or bags.

When raw milk is left standing for a while, it turns "sour". This is the result of fermentation, where lactic acid bacteria ferment the lactose in the milk into lactic acid. Prolonged fermentation may render the milk unpleasant to consume. This fermentation process is exploited by the introduction of bacterial cultures (e.g. "Lactobacilli sp., Streptococcus sp., Leuconostoc sp.", etc.) to produce a variety of fermented milk products. The reduced pH from lactic acid accumulation denatures proteins and causes the milk to undergo a variety of different transformations in appearance and texture, ranging from an aggregate to smooth consistency. Some of these products include sour cream, yogurt, cheese, buttermilk, viili, kefir, and kumis. "See Dairy product" for more information.

Pasteurization of cow's milk initially destroys any potential pathogens and increases the shelf life, but eventually results in spoilage that makes it unsuitable for consumption. This causes it to assume an unpleasant odor, and the milk is deemed non-consumable due to unpleasant taste and an increased risk of food poisoning. In raw milk, the presence of lactic acid-producing bacteria, under suitable conditions, ferments the lactose present to lactic acid. The increasing acidity in turn prevents the growth of other organisms, or slows their growth significantly. During pasteurization, however, these lactic acid bacteria are mostly destroyed.

In order to prevent spoilage, milk can be kept refrigerated and stored between in bulk tanks. Most milk is pasteurized by heating briefly and then refrigerated to allow transport from factory farms to local markets. The spoilage of milk can be forestalled by using ultra-high temperature (UHT) treatment. Milk so treated can be stored unrefrigerated for several months until opened but has a characteristic "cooked" taste. Condensed milk, made by removing most of the water, can be stored in cans for many years, unrefrigerated, as can evaporated milk. The most durable form of milk is powdered milk, which is produced from milk by removing almost all water. The moisture content is usually less than 5% in both drum- and spray-dried powdered milk.

Freezing of milk can cause fat globule aggregation upon thawing, resulting in milky layers and butterfat lumps. These can be dispersed again by warming and stirring the milk. It can change the taste by destruction of milk-fat globule membranes, releasing oxidized flavors.

Milk is used to make yogurt, cheese, ice milk, pudding, hot chocolate and french toast, among many other products. Milk is often added to dry breakfast cereal, porridge and granola. Milk is often served in coffee and tea. Steamed milk is used to prepare espresso-based drinks such as cafe latte.

The importance of milk in human culture is attested to by the numerous expressions embedded in our languages, for example, "the milk of human kindness", the expression "there's no use crying over spilt milk" (which means don't "be unhappy about what cannot be undone"), "don't milk the ram" (this means "to do or attempt something futile") and "Why buy a cow when you can get milk for free?" (which means "why pay for something that you can get for free otherwise").

In Greek mythology, the Milky Way was formed after the trickster god Hermes suckled the infant Heracles at the breast of Hera, the queen of the gods, while she was asleep. When Hera awoke, she tore Heracles away from her breast and splattered her breast milk across the heavens. In another version of the story, Athena, the patron goddess of heroes, tricked Hera into suckling Heracles voluntarily, but he bit her nipple so hard that she flung him away, spraying milk everywhere.

In many African and Asian countries, butter is traditionally made from fermented milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.

Holy books have also mentioned milk. The Bible contains references to the "Land of Milk and Honey." In the Qur'an, there is a request to wonder on milk as follows: "And surely in the livestock there is a lesson for you, We give you to drink of that which is in their bellies from the midst of digested food and blood, pure milk palatable for the drinkers" (16-The Honeybee, 66). The Ramadan fast is traditionally broken with a glass of milk and dates.

Abhisheka is conducted by Hindu and Jain priests, by pouring libations on the idol of a deity being worshipped, amidst the chanting of mantras. Usually offerings such as milk, yogurt, ghee, honey may be poured among other offerings depending on the type of abhishekam being performed.

A milksop is an "effeminate spiritless man," an expression which is attested to in the late 14th century. Milk toast is a dish consisting of milk and toast. Its soft blandness served as inspiration for the name of the timid and ineffectual comic strip character Caspar Milquetoast, drawn by H. T. Webster from 1924 to 1952. Thus, the term "milquetoast" entered the language as the label for a timid, shrinking, apologetic person. Milk toast also appeared in Disney's "Follow Me Boys" as an undesirable breakfast for the aging main character Lem Siddons.

To "milk" someone, in the vernacular of many English-speaking countries, is to take advantage of the person, by analogy to the way a farmer "milks" a cow and takes its milk. The word "milk" has had many slang meanings over time. In the 19th century, milk was used to describe a cheap and very poisonous alcoholic drink made from methylated spirits (methanol) mixed with water. The word was also used to mean defraud, to be idle, to intercept telegrams addressed to someone else, and a weakling or "milksop." In the mid-1930s, the word was used in Australia to refer to siphoning gas from a car.

Besides serving as a beverage or source of food, milk has been described as used by farmers and gardeners as an organic fungicide and fertilizer, however, its effectiveness is debated. Diluted milk solutions have been demonstrated to provide an effective method of preventing powdery mildew on grape vines, while showing it is unlikely to harm the plant.




</doc>
<doc id="19715" url="https://en.wikipedia.org/wiki?curid=19715" title="Miss Congeniality (film)">
Miss Congeniality (film)

Miss Congeniality is a 2000 American comedy film directed by Donald Petrie, written by Marc Lawrence, Katie Ford, and Caryn Lucas, and starring Sandra Bullock, Michael Caine, Benjamin Bratt, William Shatner, Ernie Hudson, and Candice Bergen. 

"Miss Congeniality" was released by Warner Bros. Pictures on December 22, 2000 and was a box office hit grossing $212 million worldwide. Bullock also garnered a Golden Globe Award for Best Actress – Motion Picture Comedy or Musical nomination. A sequel, "", was released on March 24, 2005. The movie was based on a true story of a female FBI agent who posed as a beauty pageant contestant in the Miss San Antonio Beauty Pageant of 1995. The film has also gained a cult following since its release.

In 1982, a very young Grace Hart steps into a playground fight to beat up a bully who is threatening a boy she likes. However, the boy feels humiliated at being rescued "by a girl", and rejects her rudely, whereupon she punches him in the nose and leaves to sulk alone. Years later, Gracie is now a tough Special Agent for the FBI. During a sting operation against Russian mobsters, she disobeys her superior's orders in order to save a mob boss who appears to be choking, which causes one of the other agents to be shot. She is demoted to a desk job as punishment.

Soon after, the agency is alerted, via a letter from the notorious domestic terrorist known only as "The Citizen", to a bomb threat at the upcoming 75th annual Miss United States beauty pageant in San Antonio, Texas. Gracie's partner Eric Matthews is put in charge, and he relies on Gracie's suggestions, but he takes credit for them himself. One of Gracie's ideas is to plant an agent undercover at the event. Eric then suggests that Gracie take on that role, replacing Miss New Jersey, who was just disqualified. Beauty pageant coach Victor Melling teaches Gracie how to dress, walk, and behave like a contestant. Though initially appalled, she comes to appreciate Victor's thoroughness. Gracie enters the pageant as "Gracie Lou Freebush", representing New Jersey, and becomes friends with Cheryl Frasier, who is Miss Rhode Island. As the competition begins, Gracie impresses the judges during the talent competition with her glass harp skills and self-defense techniques. 

Several suspects are identified, including the current competition director and former pageant winner Kathy Morningside, her assistant Frank Tobin, the veteran MC Stan Fields, and Cheryl, who has a history of being a radical animal rights activist. Gracie accompanies Cheryl and other contestants as they spend a night partying, where Gracie tries to dig into Cheryl's past, but inadvertently learns from the others that Kathy's past as a pageant contestant is suspect, including the fact that she won after the leading contestant suddenly came down with food poisoning. Gracie comes to believe Kathy is a "Citizen" copycat. When Gracie reports this to Eric and the team, she learns that "The Citizen" has been arrested on an unrelated charge, and because there is no further threat, their supervisor has pulled the mission. Gracie insists that she suspects something is wrong, and Eric returns to Texas to help her continue the investigation against orders.

In the final round, Gracie is stunned when she is named first runner up. Cheryl is named Miss United States, but as she goes to accept the tiara, Gracie realizes that Frank, who is actually Kathy's son, impersonated "The Citizen" to make the pageant bomb threat. She throws the tiara up at the stage scenery, where it explodes. As Kathy and Frank are arrested, Gracie determines that the two wanted to kill the pageant winner on stage as revenge for Kathy's termination from the Miss United States organization. As the event closes down and Gracie and Eric prepare to return to headquarters with a newfound interest in each other, the other contestants name Gracie as "Miss Congeniality".

Ellen DeGeneres claims that the writer was inspired when watching her training to walk in high heels and a dress in preparation for the Oscars.

The story is set in New York City and San Antonio. Scenes showing the exterior of the St. Regis Hotel, as well as a few street scenes, were shot on location in New York, and the Alamo and River Walk scenes were shot on location in San Antonio. The majority of the film was shot in Austin, Texas: scenes depicting the interior of the St. Regis were shot in Austin's Driskill Hotel; the pageant scenes were shot at the Bass Concert Hall at the University of Texas at Austin; and scenes depicting the pageant contestants in their hotel rooms were shot in the Omni Austin at South Park.

The film was the fifth highest-grossing film in North America on its opening weekend, making $13.9 million USD. It had a 5% increase in earnings the following week—enough to make the film reach #3. Overall it was a box office hit, grossing more than $106 million in the United States, and more than $212 million worldwide.

On Rotten Tomatoes the film has an approval rating of 42% based on review from 115 critics. The critical consensus reads: "Though critics say Bullock is funny and charming, she can't overcome a bad script that makes the movie feel too much like a fluffy, unoriginal sitcom." On Metacritic the film has a score of 43 out of 100, based on reviews from 20 critics, indicating "mixed or average reviews". Audiences surveyed by CinemaScore gave the film a grade A-.

A. O. Scott of "The New York Times" described it as "a standard-issue fish-out-of-water comedy" which "seems happily, deliberately second-rate, as if its ideal audience consisted of weary airline passengers". Roger Ebert for the "Chicago Sun-Times" wrote: "It isn't bad so much as it lacks any ambition to be more than it so obviously is" although he had some praise for Sandra Bullock's performance.
It was nominated for several awards, including two Golden Globes: Sandra Bullock earned a nod for Best Performance by an Actress in a Motion Picture - Comedy/Musical, and Bosson's "One in a Million" was nominated for Best Original Song in a Motion Picture.

The film's first DVD edition, released in 2001, included two audio commentaries, some deleted scenes, the theatrical trailer, and two documentaries about the making of the film. A deluxe-edition DVD, released in 2005, featured different cover art and contained the same features as the other DVD version plus a quiz hosted by William Shatner and a sneak peek at the upcoming sequel. In 2009, a double feature edition was released that included the .

A sequel, "", was released on March 24, 2005. The film starred Sandra Bullock, Regina King, Enrique Murciano, William Shatner, Ernie Hudson, Heather Burns, Diedrich Bader, and Treat Williams. The sequel was less successful both critically and commercially, earning only $101.3 million.




</doc>
<doc id="19716" url="https://en.wikipedia.org/wiki?curid=19716" title="Magnetism">
Magnetism

Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field, which acts on other currents and magnetic moments. Magnetism is one aspect of the combined phenomenon of electromagnetism. The most familiar effects occur in ferromagnetic materials, which are strongly attracted by magnetic fields and can be magnetized to become permanent magnets, producing magnetic fields themselves. Only a few substances are ferromagnetic; the most common ones are iron, cobalt and nickel and their alloys. The prefix "" refers to iron, because permanent magnetism was first observed in lodestone, a form of natural iron ore called magnetite, FeO.

Although ferromagnetism is responsible for most of the effects of magnetism encountered in everyday life, all other materials are influenced to some extent by a magnetic field, by several other types of magnetism. Paramagnetic substances such as aluminum and oxygen are weakly attracted to an applied magnetic field; diamagnetic substances such as copper and carbon are weakly repelled; while antiferromagnetic materials such as chromium and spin glasses have a more complex relationship with a magnetic field. The force of a magnet on paramagnetic, diamagnetic, and antiferromagnetic materials is usually too weak to be felt and can be detected only by laboratory instruments, so in everyday life, these substances are often described as non-magnetic.

The magnetic state (or magnetic phase) of a material depends on temperature and other variables such as pressure and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change. As with magnetizing a magnet, demagnetizing a magnet is also possible.

The strength of a magnetic fields decreases following an inverse-square law as distance from a (ideal point) magnetic pole increases, and decreases with the inverse of distance from an ideal, infinitely long wire carrying an electric current. Many magnetic objects have complex shapes and make complicated magnetic fields; only magnetic dipoles have been observed, but magnetic monopoles have been predicted by some theories.

Magnetism was first discovered in the ancient world, when people noticed that lodestones, naturally magnetized pieces of the mineral magnetite, could attract iron. The word "magnet" comes from the Greek term μαγνῆτις λίθος "magnētis lithos", "the Magnesian stone, lodestone." In ancient Greece, Aristotle attributed the first of what could be called a scientific discussion of magnetism to the philosopher Thales of Miletus, who lived from about 625 BC to about 545 BC. The ancient Indian medical text "Sushruta Samhita" describes using magnetite to remove arrows embedded in a person's body.
In ancient China, the earliest literary reference to magnetism lies in a 4th-century BC book named after its author, "The Sage of Ghost Valley".
The 2nd-century BC annals, "Lüshi Chunqiu", also notes:
"The lodestone makes iron approach, or it attracts it." 
The earliest mention of the attraction of a needle is in a 1st-century work "Lunheng" ("Balanced Inquiries"): "A lodestone attracts a needle." 
The 11th-century Chinese scientist Shen Kuo was the first person to write—in the "Dream Pool Essays"—of the magnetic needle compass and that it improved the accuracy of navigation by employing the astronomical concept of true north.
By the 12th century, the Chinese were known to use the lodestone compass for navigation. They sculpted a directional spoon from lodestone in such a way that the handle of the spoon always pointed south.

Alexander Neckam, by 1187, was the first in Europe to describe the compass and its use for navigation. In 1269, Peter Peregrinus de Maricourt wrote the "Epistola de magnete", the first extant treatise describing the properties of magnets. In 1282, the properties of magnets and the dry compasses were discussed by Al-Ashraf, a Yemeni physicist, astronomer, and geographer.

In 1600, William Gilbert published his "De Magnete, Magneticisque Corporibus, et de Magno Magnete Tellure" ("On the Magnet and Magnetic Bodies, and on the Great Magnet the Earth"). In this work he describes many of his experiments with his model earth called the terrella. From his experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses pointed north (previously, some believed that it was the pole star (Polaris) or a large magnetic island on the north pole that attracted the compass).

An understanding of the relationship between electricity and magnetism began in 1819 with work by Hans Christian Ørsted, a professor at the University of Copenhagen, who discovered by the accidental twitching of a compass needle near a wire that an electric current could create a magnetic field. This landmark experiment is known as Ørsted's Experiment. Several other experiments followed, with André-Marie Ampère, who in 1820 discovered that the magnetic field circulating in a closed-path was related to the current flowing through a surface enclosed by the path; Carl Friedrich Gauss; Jean-Baptiste Biot and Félix Savart, both of whom in 1820 came up with the Biot–Savart law giving an equation for the magnetic field from a current-carrying wire; Michael Faraday, who in 1831 found that a time-varying magnetic flux through a loop of wire induced a voltage, and others finding further links between magnetism and electricity. James Clerk Maxwell synthesized and expanded these insights into Maxwell's equations, unifying electricity, magnetism, and optics into the field of electromagnetism. In 1905, Einstein used these laws in motivating his theory of special relativity, requiring that the laws held true in all inertial reference frames.

Electromagnetism has continued to develop into the 21st century, being incorporated into the more fundamental theories of gauge theory, quantum electrodynamics, electroweak theory, and finally the standard model.

Magnetism, at its root, arises from two sources:
The magnetic properties of materials are mainly due to the magnetic moments of their atoms' orbiting electrons. The magnetic moments of the nuclei of atoms are typically thousands of times smaller than the electrons' magnetic moments, so they are negligible in the context of the magnetization of materials. Nuclear magnetic moments are nevertheless very important in other contexts, particularly in nuclear magnetic resonance (NMR) and magnetic resonance imaging (MRI).

Ordinarily, the enormous number of electrons in a material are arranged such that their magnetic moments (both orbital and intrinsic) cancel out. This is due, to some extent, to electrons combining into pairs with opposite intrinsic magnetic moments as a result of the Pauli exclusion principle (see "electron configuration"), and combining into filled subshells with zero net orbital motion. In both cases, the electrons preferentially adopt arrangements in which the magnetic moment of each electron is canceled by the opposite moment of another electron. Moreover, even when the electron configuration "is" such that there are unpaired electrons and/or non-filled subshells, it is often the case that the various electrons in the solid will contribute magnetic moments that point in different, random directions to do so that the material will not be magnetic.

Sometimes, either spontaneously, or owing to an applied external magnetic field—each of the electron magnetic moments will be, on average, lined up. A suitable material can then produce a strong net magnetic field.

The magnetic behavior of a material depends on its structure, particularly its electron configuration, for the reasons mentioned above, and also on the temperature. At high temperatures, random thermal motion makes it more difficult for the electrons to maintain alignment.

Diamagnetism appears in all materials and is the tendency of a material to oppose an applied magnetic field, and therefore, to be repelled by a magnetic field. However, in a material with paramagnetic properties (that is, with a tendency to enhance an external magnetic field), the paramagnetic behavior dominates. Thus, despite its universal occurrence, diamagnetic behavior is observed only in a purely diamagnetic material. In a diamagnetic material, there are no unpaired electrons, so the intrinsic electron magnetic moments cannot produce any bulk effect. In these cases, the magnetization arises from the electrons' orbital motions, which can be understood classically as follows:

This description is meant only as a heuristic; the Bohr-van Leeuwen theorem shows that diamagnetism is impossible according to classical physics, and that a proper understanding requires a quantum-mechanical description.

All materials undergo this orbital response. However, in paramagnetic and ferromagnetic substances, the diamagnetic effect is overwhelmed by the much stronger effects caused by the unpaired electrons.

In a paramagnetic material there are "unpaired electrons"; i.e., atomic or molecular orbitals with exactly one electron in them. While paired electrons are required by the Pauli exclusion principle to have their intrinsic ('spin') magnetic moments pointing in opposite directions, causing their magnetic fields to cancel out, an unpaired electron is free to align its magnetic moment in any direction. When an external magnetic field is applied, these magnetic moments will tend to align themselves in the same direction as the applied field, thus reinforcing it.

A ferromagnet, like a paramagnetic substance, has unpaired electrons. However, in addition to the electrons' intrinsic magnetic moment's tendency to be parallel to an applied field, there is also in these materials a tendency for these magnetic moments to orient parallel to each other to maintain a lowered-energy state. Thus, even in the absence of an applied field, the magnetic moments of the electrons in the material spontaneously line up parallel to one another.

Every ferromagnetic substance has its own individual temperature, called the Curie temperature, or Curie point, above which it loses its ferromagnetic properties. This is because the thermal tendency to disorder overwhelms the energy-lowering due to ferromagnetic order.

Ferromagnetism only occurs in a few substances; common ones are iron, nickel, cobalt, their alloys, and some alloys of rare-earth metals.

The magnetic moments of atoms in a ferromagnetic material cause them to behave something like tiny permanent magnets. They stick together and align themselves into small regions of more or less uniform alignment called magnetic domains or Weiss domains. Magnetic domains can be observed with a magnetic force microscope to reveal magnetic domain boundaries that resemble white lines in the sketch. There are many scientific experiments that can physically show magnetic fields.

When a domain contains too many molecules, it becomes unstable and divides into two domains aligned in opposite directions, so that they stick together more stably, as shown at the right.

When exposed to a magnetic field, the domain boundaries move, so that the domains aligned with the magnetic field grow and dominate the structure (dotted yellow area), as shown at the left. When the magnetizing field is removed, the domains may not return to an unmagnetized state. This results in the ferromagnetic material's being magnetized, forming a permanent magnet.

When magnetized strongly enough that the prevailing domain overruns all others to result in only one single domain, the material is magnetically saturated. When a magnetized ferromagnetic material is heated to the Curie point temperature, the molecules are agitated to the point that the magnetic domains lose the organization, and the magnetic properties they cause cease. When the material is cooled, this domain alignment structure spontaneously returns, in a manner roughly analogous to how a liquid can freeze into a crystalline solid.

In an antiferromagnet, unlike a ferromagnet, there is a tendency for the intrinsic magnetic moments of neighboring valence electrons to point in "opposite" directions. When all atoms are arranged in a substance so that each neighbor is anti-parallel, the substance is antiferromagnetic. Antiferromagnets have a zero net magnetic moment, meaning that no field is produced by them. Antiferromagnets are less common compared to the other types of behaviors and are mostly observed at low temperatures. In varying temperatures, antiferromagnets can be seen to exhibit diamagnetic and ferromagnetic properties.

In some materials, neighboring electrons prefer to point in opposite directions, but there is no geometrical arrangement in which "each" pair of neighbors is anti-aligned. This is called a spin glass and is an example of geometrical frustration.

Like ferromagnetism, ferrimagnets retain their magnetization in the absence of a field. However, like antiferromagnets, neighboring pairs of electron spins tend to point in opposite directions. These two properties are not contradictory, because in the optimal geometrical arrangement, there is more magnetic moment from the sublattice of electrons that point in one direction, than from the sublattice that points in the opposite direction.

Most ferrites are ferrimagnetic. The first discovered magnetic substance, magnetite, is a ferrite and was originally believed to be a ferromagnet; Louis Néel disproved this, however, after discovering ferrimagnetism.

When a ferromagnet or ferrimagnet is sufficiently small, it acts like a single magnetic spin that is subject to Brownian motion. Its response to a magnetic field is qualitatively similar to the response of a paramagnet, but much larger.


An electromagnet is a type of magnet in which the magnetic field is produced by an electric current. The magnetic field disappears when the current is turned off. Electromagnets usually consist of a large number of closely spaced turns of wire that create the magnetic field. The wire turns are often wound around a magnetic core made from a ferromagnetic or ferrimagnetic material such as iron; the magnetic core concentrates the magnetic flux and makes a more powerful magnet.

The main advantage of an electromagnet over a permanent magnet is that the magnetic field can be quickly changed by controlling the amount of electric current in the winding. However, unlike a permanent magnet that needs no power, an electromagnet requires a continuous supply of current to maintain the magnetic field.

Electromagnets are widely used as components of other electrical devices, such as motors, generators, relays, solenoids, loudspeakers, hard disks, MRI machines, scientific instruments, and magnetic separation equipment. Electromagnets are also employed in industry for picking up and moving heavy iron objects such as scrap iron and steel. Electromagnetism was discovered in 1820.

As a consequence of Einstein's theory of special relativity, electricity and magnetism are fundamentally interlinked. Both magnetism lacking electricity, and electricity without magnetism, are inconsistent with special relativity, due to such effects as length contraction, time dilation, and the fact that the magnetic force is velocity-dependent. However, when both electricity and magnetism are taken into account, the resulting theory (electromagnetism) is fully consistent with special relativity. In particular, a phenomenon that appears purely electric or purely magnetic to one observer may be a mix of both to another, or more generally the relative contributions of electricity and magnetism are dependent on the frame of reference. Thus, special relativity "mixes" electricity and magnetism into a single, inseparable phenomenon called electromagnetism, analogous to how relativity "mixes" space and time into spacetime.

All observations on electromagnetism apply to what might be considered to be primarily magnetism, e.g. perturbations in the magnetic field are necessarily accompanied by a nonzero electric field, and propagate at the speed of light.

In a vacuum,
where is the vacuum permeability.

In a material,
The quantity is called "magnetic polarization".

If the field is small, the response of the magnetization in a diamagnet or paramagnet is approximately linear:
the constant of proportionality being called the magnetic susceptibility. If so,

In a hard magnet such as a ferromagnet, is not proportional to the field and is generally nonzero even when is zero (see Remanence).

The phenomenon of magnetism is "mediated" by the magnetic field. An electric current or magnetic dipole creates a magnetic field, and that field, in turn, imparts magnetic forces on other particles that are in the fields.

Maxwell's equations, which simplify to the Biot–Savart law in the case of steady currents, describe the origin and behavior of the fields that govern these forces. Therefore, magnetism is seen whenever electrically charged particles are in motion—for example, from movement of electrons in an electric current, or in certain cases from the orbital motion of electrons around an atom's nucleus. They also arise from "intrinsic" magnetic dipoles arising from quantum-mechanical spin.

The same situations that create magnetic fields—charge moving in a current or in an atom, and intrinsic magnetic dipoles—are also the situations in which a magnetic field has an effect, creating a force. Following is the formula for moving charge; for the forces on an intrinsic dipole, see magnetic dipole.

When a charged particle moves through a magnetic field B, it feels a Lorentz force F given by the cross product:

where

Because this is a cross product, the force is perpendicular to both the motion of the particle and the magnetic field. It follows that the magnetic force does no work on the particle; it may change the direction of the particle's movement, but it cannot cause it to speed up or slow down. The magnitude of the force is
where formula_8 is the angle between v and B.

One tool for determining the direction of the velocity vector of a moving charge, the magnetic field, and the force exerted is labeling the index finger "V", the middle finger "B", and the thumb "F" with your right hand. When making a gun-like configuration, with the middle finger crossing under the index finger, the fingers represent the velocity vector, magnetic field vector, and force vector, respectively. See also right-hand rule.

A very common source of magnetic field found in nature is a dipole, with a "South pole" and a "North pole", terms dating back to the use of magnets as compasses, interacting with the Earth's magnetic field to indicate North and South on the globe. Since opposite ends of magnets are attracted, the north pole of a magnet is attracted to the south pole of another magnet. The Earth's North Magnetic Pole (currently in the Arctic Ocean, north of Canada) is physically a south pole, as it attracts the north pole of a compass.
A magnetic field contains energy, and physical systems move toward configurations with lower energy. When diamagnetic material is placed in a magnetic field, a "magnetic dipole" tends to align itself in opposed polarity to that field, thereby lowering the net field strength. When ferromagnetic material is placed within a magnetic field, the magnetic dipoles align to the applied field, thus expanding the domain walls of the magnetic domains.

Since a bar magnet gets its ferromagnetism from electrons distributed evenly throughout the bar, when a bar magnet is cut in half, each of the resulting pieces is a smaller bar magnet. Even though a magnet is said to have a north pole and a south pole, these two poles cannot be separated from each other. A monopole—if such a thing exists—would be a new and fundamentally different kind of magnetic object. It would act as an isolated north pole, not attached to a south pole, or vice versa. Monopoles would carry "magnetic charge" analogous to electric charge. Despite systematic searches since 1931, , they have never been observed, and could very well not exist.

Nevertheless, some theoretical physics models predict the existence of these magnetic monopoles. Paul Dirac observed in 1931 that, because electricity and magnetism show a certain symmetry, just as quantum theory predicts that individual positive or negative electric charges can be observed without the opposing charge, isolated South or North magnetic poles should be observable. Using quantum theory Dirac showed that if magnetic monopoles exist, then one could explain the quantization of electric charge—that is, why the observed elementary particles carry charges that are multiples of the charge of the electron.

Certain grand unified theories predict the existence of monopoles which, unlike elementary particles, are solitons (localized energy packets). The initial results of using these models to estimate the number of monopoles created in the Big Bang contradicted cosmological observations—the monopoles would have been so plentiful and massive that they would have long since halted the expansion of the universe. However, the idea of inflation (for which this problem served as a partial motivation) was successful in solving this problem, creating models in which monopoles existed but were rare enough to be consistent with current observations.


Some organisms can detect magnetic fields, a phenomenon known as magnetoception. Some materials in living things are ferromagnetic, though it is unclear if the magnetic properties serve a special function or are merely a byproduct of containing iron. For instance, chitons, a type of marine mollusk, produce magnetite to harden their teeth, and even humans produce magnetite in bodily tissue. Magnetobiology studies the effects of magnetic fields on living organisms; fields naturally produced by an organism are known as biomagnetism. Many biological organisms are mostly made of water, and because water is diamagnetic, extremely strong magnetic fields can repel these living things.

While heuristic explanations based on classical physics can be formulated, diamagnetism, paramagnetism and ferromagnetism can only be fully explained using quantum theory.
A successful model was developed already in 1927, by Walter Heitler and Fritz London, who derived, quantum-mechanically, how hydrogen molecules are formed from hydrogen atoms, i.e. from the atomic hydrogen orbitals formula_9 and formula_10 centered at the nuclei "A" and "B", see below. That this leads to magnetism is not at all obvious, but will be explained in the following.

According to the Heitler–London theory, so-called two-body molecular formula_11-orbitals are formed, namely the resulting orbital is:

Here the last product means that a first electron, r, is in an atomic hydrogen-orbital centered at the second nucleus, whereas the second electron runs around the first nucleus. This "exchange" phenomenon is an expression for the quantum-mechanical property that particles with identical properties cannot be distinguished. It is specific not only for the formation of chemical bonds, but also for magnetism. That is, in this connection the term exchange interaction arises, a term which is essential for the origin of magnetism, and which is stronger, roughly by factors 100 and even by 1000, than the energies arising from the electrodynamic dipole-dipole interaction.

As for the "spin function" formula_13, which is responsible for the magnetism, we have the already mentioned Pauli's principle, namely that a symmetric orbital (i.e. with the + sign as above) must be multiplied with an antisymmetric spin function (i.e. with a − sign), and "vice versa". Thus:

I.e., not only formula_15 and formula_10 must be substituted by "α" and "β", respectively (the first entity means "spin up", the second one "spin down"), but also the sign + by the − sign, and finally r by the discrete values "s" (= ±½); thereby we have formula_17 and formula_18. The "singlet state", i.e. the − sign, means: the spins are "antiparallel", i.e. for the solid we have antiferromagnetism, and for two-atomic molecules one has diamagnetism. The tendency to form a (homoeopolar) chemical bond (this means: the formation of a "symmetric" molecular orbital, i.e. with the + sign) results through the Pauli principle automatically in an "antisymmetric" spin state (i.e. with the − sign). In contrast, the Coulomb repulsion of the electrons, i.e. the tendency that they try to avoid each other by this repulsion, would lead to an "antisymmetric" orbital function (i.e. with the − sign) of these two particles, and complementary to a "symmetric" spin function (i.e. with the + sign, one of the so-called "triplet functions"). Thus, now the spins would be "parallel" (ferromagnetism in a solid, paramagnetism in two-atomic gases).

The last-mentioned tendency dominates in the metals iron, cobalt and nickel, and in some rare earths, which are "ferromagnetic". Most of the other metals, where the first-mentioned tendency dominates, are "nonmagnetic" (e.g. sodium, aluminium, and magnesium) or "antiferromagnetic" (e.g. manganese). Diatomic gases are also almost exclusively diamagnetic, and not paramagnetic. However, the oxygen molecule, because of the involvement of π-orbitals, is an exception important for the life-sciences.

The Heitler-London considerations can be generalized to the Heisenberg model of magnetism (Heisenberg 1928).

The explanation of the phenomena is thus essentially based on all subtleties of quantum mechanics, whereas the electrodynamics covers mainly the phenomenology.



</doc>
<doc id="19719" url="https://en.wikipedia.org/wiki?curid=19719" title="Filter (mathematics)">
Filter (mathematics)

In mathematics, a filter is a special subset of a partially ordered set. Filters appear in order and lattice theory, but can also be found in topology, from where they originate. The dual notion of a filter is an order ideal.

Filters were introduced by Henri Cartan in 1937 and subsequently used by Bourbaki in their book "Topologie Générale" as an alternative to the similar notion of a net developed in 1922 by E. H. Moore and H. L. Smith.

Intuitively, a filter in a partially ordered set ("poset"), "P", is a subset of "P" that includes as members those elements that are large enough to satisfy some given criterion. For example, if "x" is an element of the poset, then the set of elements that are above "x" is a filter, called the principal filter at "x". (If "x" and "y" are incomparable elements of the poset, then neither of the principal filters at "x" and "y" is contained in the other one, and conversely.)

Similarly, a filter on a set contains those subsets that are sufficiently large to contain some given "thing". For example, if the set is the real line and "x" is one of its points, then the family of sets that include "x" in their interior is a filter, called the filter of neighbourhoods of "x". The "thing" in this case is slightly larger than "x", but it still doesn't contain any other specific point of the line.

The above interpretations explain conditions 1 and 3 in the section General definition: Clearly the empty set is not "large enough", and clearly the collection of "large enough" things should be "upward-closed". However, they do not really, without elaboration, explain condition 2 of the general definition. For, why should two "large enough" things contain a "common" "large enough" thing?

Alternatively, a filter can be viewed as a "locating scheme": When trying to locate something (a point or a subset) in the space "X", call a filter the collection of subsets of "X" that might contain "what is looked for". Then this "filter" should possess the following natural structure:

An ultrafilter can be viewed as a "perfect locating scheme" where "each" subset "E" of the space "X" can be used in deciding whether "what is looked for" might lie in "E".

From this interpretation, compactness (see the mathematical characterization below) can be viewed as the property that "no location scheme can end up with nothing", or, to put it another way, "always something will be found".

The mathematical notion of filter provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, general topology and logic.

A subset "F" of a partially ordered set ("P", ≤) is a filter if the following conditions hold:


A filter is proper if it is not equal to the whole set "P". This condition is sometimes added to the definition of a filter.

While the above definition is the most general way to define a filter for arbitrary posets, it was originally defined for lattices only. In this case, the above definition can be characterized by the following equivalent statement:
A subset "F" of a lattice ("P", ≤) is a filter, if and only if it is a non-empty upper set that is closed under finite infima (or meets), i.e., for all "x", "y" in "F", it is also the case that "x" ∧ "y" is in "F". A subset "S" of "F" is a filter basis if the upper set generated by "S" is all of "F". 

The smallest filter that contains a given element "p" is a principal filter and "p" is a principal element in this situation. The principal filter for "p" is just given by the set formula_1 and is denoted by prefixing "p" with an upward arrow: 

The dual notion of a filter, i.e. the concept obtained by reversing all ≤ and exchanging ∧ with ∨, is ideal. Because of this duality, the discussion of filters usually boils down to the discussion of ideals. Hence, most additional information on this topic (including the definition of maximal filters and prime filters) is to be found in the article on ideals. There is a separate article on ultrafilters.

A special case of a filter is a filter defined on a set. Given a set "S", a partial ordering ⊆ can be defined on the powerset P("S") by subset inclusion, turning (P("S"), ⊆) into a lattice. Define a filter "F" on "S" as a non-empty subset of P("S") with the following properties:


With this definition, a filter on a set is indeed a filter.

The second property entails that "S" is in "F" (since "F" is a non-empty subset of P("S")).

If the empty set is not in "F", we say "F" is a proper filter. Property 1 implies that a proper filter on a set has the finite intersection property. The only nonproper filter on "S" is P("S").

A filter base (or filter basis) is a subset "B" of P("S") with the properties that B is non-empty and the intersection of any two members of B includes (as a subset) a member of B ("B is downward directed"). If the empty set is not a member of B, we say B is a proper filter base.

Given a filter base "B", the filter generated or spanned by "B" is defined as the minimum filter containing "B". It is the family of all the subsets of "S" that include a member of "B". Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.

If "B" and "C" are two filter bases on "S", one says "C" is finer than "B" (or that "C" is a refinement of "B") if for each "B" ∈ "B", there is a "C" ∈ "C" such that "C" ⊆ "B". If also "B" is finer than "C", one says that they are equivalent filter bases.

For every subset "T" of P("S") there is a smallest (possibly nonproper) filter "F" containing "T", called the filter generated or spanned by "T". It is constructed by taking all finite intersections of "T", which then form a filter base for "F". This filter is proper if and only if every finite intersection of elements of "T" is non-empty, and in that case we say that "T" is a filter subbase.


For every filter "F" on a set "S", the set function defined by
is finitely additive—a "measure" if that term is construed rather loosely. Therefore, the statement

can be considered somewhat analogous to the statement that φ holds "almost everywhere". That interpretation of membership in a filter is used (for motivation, although it is not needed for actual "proofs") in the theory of ultraproducts in model theory, a branch of mathematical logic.

In topology and analysis, filters are used to define convergence in a manner similar to the role of sequences in a metric space.

In topology and related areas of mathematics, a filter is a generalization of a net. Both nets and filters provide very general contexts to unify the various notions of limit to arbitrary topological spaces.

A sequence is usually indexed by the natural numbers, which are a totally ordered set. Thus, limits in first-countable spaces can be described by sequences. However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a directed set. Filters can be thought of as sets built from multiple nets. Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.

Let "X" be a topological space and "x" a point of "X".


Let "X" be a topological space and "x" a point of "X".


Indeed:

(i) implies (ii): if "F" is a filter base satisfying the properties of (i), then the filter associated to "F" satisfies the properties of (ii).

(ii) implies (iii): if "U" is any open neighborhood of "x" then by the definition of convergence "U" contains an element of "F"; since also "Y" is an element of "F", 
"U" and "Y" have non-empty intersection.

(iii) implies (i): Define formula_9. Then "F" is a filter base satisfying the properties of (i).

Let "X" be a topological space and "x" a point of "X".


Let "X" be a topological space.


Let formula_11, formula_12 be topological spaces. Let formula_13 be a filter base on formula_11 and formula_15 be a function. The image of formula_13 under formula_17 is defined as the set formula_18. The image is denoted formula_19 and forms a filter base on formula_12. 

Let formula_25 be a metric space.


</doc>
<doc id="19722" url="https://en.wikipedia.org/wiki?curid=19722" title="Metallurgy">
Metallurgy

Metallurgy is a domain of materials science and engineering that studies the physical and chemical behavior of metallic elements, their inter-metallic compounds, and their mixtures, which are called alloys.
Metallurgy encompasses both the science and the technology of metals. That is, the way in which science is applied to the production of metals, and the engineering of metal components used in products for both consumers and manufacturers. Metallurgy is distinct from the craft of metalworking. Metalworking relies on metallurgy in a similar manner to how medicine relies on medical science for technical advancement. A specialist practitioner of metallurgy is known as a . 

The science of metallurgy is subdivided into two broad categories: chemical metallurgy and physical metallurgy. Chemical metallurgy is chiefly concerned with the reduction and oxidation of metals, and the chemical performance of metals. Subjects of study in chemical metallurgy include mineral processing, the extraction of metals, thermodynamics, electrochemistry, and chemical degradation (corrosion). In contrast, physical metallurgy focuses on the mechanical properties of metals, the physical properties of metals, and the physical performance of metals. Topics studied in physical metallurgy include crystallography, material characterization, mechanical metallurgy, phase transformations, and failure mechanisms. 

Historically, metallurgy has predominately focused on the production of metals. Metal production begins with the processing of ores to extract the metal, and includes the mixture of metals to make alloys. Metal alloys are often a blend of at least two different metallic elements. However, non-metallic elements are often added to alloys in order to achieve properties suitable for an application. The study of metal production is subdivided into ferrous metallurgy (also known as "black metallurgy") and non-ferrous metallurgy (also known as "colored metallurgy").
Ferrous metallurgy involves processes and alloys based on iron while non-ferrous metallurgy involves processes and alloys based on other metals. The production of ferrous metals accounts for 95 percent of world metal production.

Modern metallurgists work in both emerging and traditional areas as part of an interdisciplinary team alongside material scientists, and other engineers. Some traditional areas include mineral processing, metal production, heat treatment, failure analysis, and the joining of metals (including welding, brazing, and soldering). Emerging areas for metallurgists include nanotechnology, superconductors, composites, biomedical materials, electronic materials (semiconductors), and surface engineering.

"Metallurgy" derives from the Ancient Greek , , "worker in metal", from , , "mine, metal" + , , "work".

The word was originally an alchemist's term for the extraction of metals from minerals, the ending "-urgy" signifying a process, especially manufacturing: it was discussed in this sense in the 1797 "Encyclopædia Britannica". In the late 19th century it was extended to the more general scientific study of metals, alloys, and related processes.

In English, the pronunciation is the more common one in the UK and Commonwealth. The pronunciation is the more common one in the US, and is the first-listed variant in various American dictionaries (e.g., "Merriam-Webster Collegiate", "American Heritage").

The earliest recorded metal employed by humans appears to be gold, which can be found free or "native". Small amounts of natural gold have been found in Spanish caves dating to the late Paleolithic period, c. 40,000 BC.
Silver, copper, tin and meteoric iron can also be found in native form, allowing a limited amount of metalworking in early cultures. Egyptian weapons made from meteoric iron in about 3000 BC were highly prized as "daggers from heaven".

Certain metals, notably tin, lead, and at a higher temperature, copper, can be recovered from their ores by simply heating the rocks in a fire or blast furnace, a process known as smelting. The first evidence of this extractive metallurgy, dating from the 5th and 6th millennia BC, has been found at archaeological sites in Majdanpek, Jarmovac near Priboj and Pločnik, in present-day Serbia. To date, the earliest evidence of copper smelting is found at the Belovode site near Plocnik. This site produced a copper axe from 5500 BC, belonging to the Vinča culture.

The earliest use of lead is documented from the late neolithic settlement of Yarim Tepe in Iraq,
"The earliest lead (Pb) finds in the ancient Near East are a 6th millennium BC bangle from Yarim Tepe in northern Iraq and a slightly later conical lead piece from Halaf period Arpachiyah, near Mosul. As native lead is extremely rare, such artifacts raise the possibility that lead smelting may have begun even before copper smelting."
Copper smelting is also documented at this site at about the same time period (soon after 6000 BC), although the use of lead seems to precede copper smelting. Early metallurgy is also documented at the nearby site of Tell Maghzaliyah, which seems to be dated even earlier, and completely lacks that pottery.

The Balkans were the site of major Neolithic cultures, including Butmir, Vinča, Varna, Karanovo, and Hamangia.

The Varna Necropolis, Bulgaria, is a burial site in the western industrial zone of Varna (approximately 4 km from the city centre), internationally considered one of the key archaeological sites in world prehistory. The oldest gold treasure in the world, dating from 4,600 BC to 4,200 BC, was discovered at the site. The gold piece dating from 4,500 BC, recently founded in Durankulak, near Varna is another important example.

Other signs of early metals are found from the third millennium BC in places like Palmela (Portugal), Los Millares (Spain), and Stonehenge (United Kingdom). However, the ultimate beginnings cannot be clearly ascertained and new discoveries are both continuous and ongoing.
In the Near East, about 3500 BC, it was discovered that by combining copper and tin, a superior metal could be made, an alloy called bronze. This represented a major technological shift known as the Bronze Age.

The extraction of iron from its ore into a workable metal is much more difficult than for copper or tin. The process appears to have been invented by the Hittites in about 1200 BC, beginning the Iron Age. The secret of extracting and working iron was a key factor in the success of the Philistines.

Historical developments in ferrous metallurgy can be found in a wide variety of past cultures and civilizations. This includes the ancient and medieval kingdoms and empires of the Middle East and Near East, ancient Iran, ancient Egypt, ancient Nubia, and Anatolia (Turkey), Ancient Nok, Carthage, the Greeks and Romans of ancient Europe, medieval Europe, ancient and medieval China, ancient and medieval India, ancient and medieval Japan, amongst others. Many applications, practices, and devices associated or involved in metallurgy were established in ancient China, such as the innovation of the blast furnace, cast iron, hydraulic-powered trip hammers, and double acting piston bellows.

A 16th century book by Georg Agricola called "De re metallica" describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. Agricola has been described as the "father of metallurgy".

Extractive metallurgy is the practice of removing valuable metals from an ore and refining the extracted raw metals into a purer form. In order to convert a metal oxide or sulphide to a purer metal, the ore must be reduced physically, chemically, or electrolytically.

Extractive metallurgists are interested in three primary streams: feed, concentrate (valuable metal oxide/sulphide) and tailings(waste). After mining, large pieces of the ore feed are broken through crushing or grinding in order to obtain particles small enough where each particle is either mostly valuable or mostly waste. Concentrating the particles of value in a form supporting separation enables the desired metal to be removed from waste products.

Mining may not be necessary, if the ore body and physical environment are conducive to leaching. Leaching dissolves minerals in an ore body and results in an enriched solution. The solution is collected and processed to extract valuable metals.

Ore bodies often contain more than one valuable metal. Tailings of a previous process may be used as a feed in another process to extract a secondary product from the original ore. Additionally, a concentrate may contain more than one valuable metal. That concentrate would then be processed to separate the valuable metals into individual constituents.

Common engineering metals include aluminium, chromium, copper, iron, magnesium, nickel, titanium, zinc, and silicon. These metals are most often used as alloys with the noted exception of silicon. Much effort has been placed on understanding the iron-carbon alloy system, which includes steels and cast irons. Plain carbon steels (those that contain essentially only carbon as an alloying element) are used in low-cost, high-strength applications where neither weight nor corrosion are a major concern. Cast irons, including ductile iron, are also part of the iron-carbon system.

Stainless steel, particularly Austenitic stainless steels, galvanized steel, , titanium alloys, or occasionally copper alloys are used where resistance to corrosion is important. Aluminium alloys and magnesium alloys are commonly used when a lightweight strong part is required such as in automotive and aerospace applications. 

Copper-nickel alloys (such as Monel) are used in highly corrosive environments and for non-magnetic applications. Iron-Manganese-Chromium alloys (Hadfield-type steels) are also used in non-magnetic applications such as directional drilling. Nickel-based superalloys like Inconel are used in high-temperature applications such as gas turbines, turbochargers, pressure vessels, and heat exchangers. For extremely high temperatures, single crystal alloys are used to minimize creep. In modern electronics, high purity single crystal silicon is essential for metal-oxide-silicon transistors (MOS) and integrated circuits. 

In production engineering, metallurgy is concerned with the production of metallic components for use in consumer or engineering products. This involves the production of alloys, the shaping, the heat treatment and the surface treatment of the product. Determining the hardness of the metal using the Rockwell, Vickers, and Brinell hardness scales is a commonly used practice that helps better understand the metal's elasticity and plasticity for different applications and production processes. The task of the metallurgist is to achieve balance between material properties such as cost, weight, strength, toughness, hardness, corrosion, fatigue resistance, and performance in temperature extremes. To achieve this goal, the operating environment must be carefully considered. In a saltwater environment, most ferrous metals and some non-ferrous alloys corrode quickly. Metals exposed to cold or cryogenic conditions may undergo a ductile to brittle transition and lose their toughness, becoming more brittle and prone to cracking. Metals under continual cyclic loading can suffer from metal fatigue. Metals under constant stress at elevated temperatures can creep.

Metals are shaped by processes such as:


Cold-working processes, in which the product's shape is altered by rolling, fabrication or other processes while the product is cold, can increase the strength of the product by a process called work hardening. Work hardening creates microscopic defects in the metal, which resist further changes of shape.

Various forms of casting exist in industry and academia. These include sand casting, investment casting (also called the lost wax process), die casting, and continuous castings. Each of these forms has advantages for certain metals and applications considering factors like magnetism and corrosion.

Metals can be heat-treated to alter the properties of strength, ductility, toughness, hardness and resistance to corrosion. Common heat treatment processes include annealing, precipitation strengthening, quenching, and tempering. The annealing process softens the metal by heating it and then allowing it to cool very slowly, which gets rid of stresses in the metal and makes the grain structure large and soft-edged so that when the metal is hit or stressed it dents or perhaps bends, rather than breaking; it is also easier to sand, grind, or cut annealed metal. Quenching is the process of cooling a high-carbon steel very quickly after heating, thus "freezing" the steel's molecules in the very hard martensite form, which makes the metal harder. There is a balance between hardness and toughness in any steel; the harder the steel, the less tough or impact-resistant it is, and the more impact-resistant it is, the less hard it is. Tempering relieves stresses in the metal that were caused by the hardening process; tempering makes the metal less hard while making it better able to sustain impacts without breaking.

Often, mechanical and thermal treatments are combined in what are known as thermo-mechanical treatments for better properties and more efficient processing of materials. These processes are common to high-alloy special steels, superalloys and titanium alloys.

Electroplating is a chemical surface-treatment technique. It involves bonding a thin layer of another metal such as gold, silver, chromium or zinc to the surface of the product. This is done by selecting the coating material electrolyte solution which is the material that is going to coat the workpiece (gold, silver, zinc). There needs to be two electrodes of different materials: one the same material as the coating material and one that is receiving the coating material. Two electrodes are electrically charged and the coating material is stuck to the work piece. It is used to reduce corrosion as well as to improve the product's aesthetic appearance. It is also used to make inexpensive metals look like the more expensive ones (gold, silver).

Shot peening is a cold working process used to finish metal parts. In the process of shot peening, small round shot is blasted against the surface of the part to be finished. This process is used to prolong the product life of the part, prevent stress corrosion failures, and also prevent fatigue. The shot leaves small dimples on the surface like a peen hammer does, which cause compression stress under the dimple. As the shot media strikes the material over and over, it forms many overlapping dimples throughout the piece being treated. The compression stress in the surface of the material strengthens the part and makes it more resistant to fatigue failure, stress failures, corrosion failure, and cracking.

Thermal spraying techniques are another popular finishing option, and often have better high temperature properties than electroplated coatings.Thermal spraying, also known as a spray welding process, is an industrial coating process that consists of a heat source (flame or other) and a coating material that can be in a powder or wire form which is melted then sprayed on the surface of the material being treated at a high velocity. The spray treating process is known by many different names such as HVOF (High Velocity Oxygen Fuel), plasma spray, flame spray, arc spray, and metalizing.

Metallurgists study the microscopic and macroscopic structure of metals using metallography, a technique invented by Henry Clifton Sorby. In metallography, an alloy of interest is ground flat and polished to a mirror finish. The sample can then be etched to reveal the microstructure and macrostructure of the metal. The sample is then examined in an optical or electron microscope, and the image contrast provides details on the composition, mechanical properties, and processing history.

Crystallography, often using diffraction of x-rays or electrons, is another valuable tool available to the modern metallurgist. Crystallography allows identification of unknown materials and reveals the crystal structure of the sample. Quantitative crystallography can be used to calculate the amount of phases present as well as the degree of strain to which a sample has been subjected.



</doc>
<doc id="19723" url="https://en.wikipedia.org/wiki?curid=19723" title="MUMPS">
MUMPS

MUMPS ("Massachusetts General Hospital Utility Multi-Programming System"), or M, is a general-purpose computer programming language originally designed in 1966 for the healthcare industry. Its differentiating feature is its "built-in" database, enabling high-level access to disk storage using simple symbolic program variables and subscripted arrays; similar to the variables used by most languages to access main memory.

It continues to be used today by many large hospitals and banks to provide high-throughput transaction data processing.

MUMPS was developed by Neil Pappalardo, Robert Greenes, and Curt Marble in Dr. Octo Barnett's animal lab at the Massachusetts General Hospital (MGH) in Boston during 1966 and 1967. It was later rewritten by technical leaders Dennis "Dan" Brevik and Paul Stylos of DEC in 1970 and 1971. Evelyn Dow was the original Marketing representative. 

The original MUMPS system was, like Unix a few years later, built on a spare DEC PDP-7. Octo Barnett and Neil Pappalardo were also involved with MGH's planning for a Hospital Information System, obtained a backward compatible PDP-9, and began using MUMPS in the admissions cycle and laboratory test reporting. MUMPS was then an interpreted language, yet even then, incorporated a hierarchical database file system to standardize interaction with the data.

Some aspects of MUMPS can be traced from Rand Corporation's JOSS through BBN's TELCOMP and STRINGCOMP. The MUMPS team deliberately chose to include portability between machines as a design goal. Another feature, not widely supported for machines of the era, in operating systems or in computer hardware, was multitasking, which was also built into the language itself.

Dan Brevik's DEC MUMPS-15 system was adapted to a DEC PDP-15, where it lived for some time. It was first installed at Health Data Management Systems of Denver in May of 1971. The portability proved to be useful and MUMPS was awarded a government research grant, and so MUMPS was released to the public domain (no longer a requirement for grants). MUMPS was soon ported to a number of other systems including the popular DEC PDP-8, the Data General Nova and on DEC PDP-11 and the Artronix PC12 minicomputer. Word about MUMPS spread mostly through the medical community, and was in widespread use, often being locally modified for their own needs. 

By the early 1970's, there were many and varied implementations of MUMPS on a range of hardware platforms. Another noteworthy platform was Paul Stylos' DEC MUMPS-11 on the PDP-11, and MEDITECH's MIIS. In the Fall of 1972, many MUMPS users attended a conference in Boston which standardized the then-fractured language, and created the MUMPS Users Group and MUMPS Development Committee (MDC) to do so. These efforts proved successful; a standard was complete by 1974, and was approved, on September 15, 1977, as ANSI standard, X11.1-1977. At about the same time DEC launched DSM-11 (Digital Standard MUMPS) for the PDP-11. This quickly dominated the market, and became the reference implementation of the time. Also, InterSystems sold ISM-11 for the PDP-11 (which was identical to DSM-11).

During the early 1980s several vendors brought MUMPS-based platforms that met the ANSI standard to market. The most significant were:

Other companies developed important MUMPS implementations:

This period also saw considerable MDC activity. The second revision of the ANSI standard for MUMPS (X11.1-1984) was approved on November 15, 1984.



The US Department of Veterans Affairs (formerly the Veterans Administration) was one of the earliest major adopters of the MUMPS language. Their development work (and subsequent contributions to the free MUMPS application codebase) was an influence on many medical users worldwide. In 1995, the Veterans Affairs' patient Admission/Tracking/Discharge system, Decentralized Hospital Computer Program (DHCP) was the recipient of the Computerworld Smithsonian Award for best use of Information Technology in Medicine. In July 2006, the Department of Veterans Affairs (VA) / Veterans Health Administration (VHA) was the recipient of the Innovations in American Government Award presented by the Ash Institute of the John F. Kennedy School of Government at Harvard University for its extension of DHCP into the Veterans Health Information Systems and Technology Architecture (VistA). Nearly the entire VA hospital system in the United States, the Indian Health Service, and major parts of the Department of Defense CHCS hospital system use MUMPS databases for clinical data tracking. In 2015 the Department of Defense awarded a 10 year contract to Leidos, Cerner, and Accenture to replace CHCS. In 2017 the Veterans Health Administration (VHA) announced that it would replace VistA with Cerner by 2024 or 2025. In 2019 the VHA announced that it would also replace its Epic Systems online appointment scheduling system with one from Cerner by 2024. 

Other healthcare IT companies using MUMPS include Epic, MEDITECH, GE Healthcare (formerly IDX Systems and Centricity), AmeriPath (part of Quest Diagnostics), Care Centric, Allscripts, Coventry Healthcare, EMIS, and Sunquest Information Systems (formerly Misys Healthcare). Many reference laboratories, such as DASA, Quest Diagnostics, and Dynacare, use MUMPS software written by or based on Antrim Corporation code. Antrim was purchased by Misys Healthcare (now Sunquest Information Systems) in 2001.

MUMPS is also widely used in financial applications. MUMPS gained an early following in the financial sector and is in use at many banks and credit unions. It is used by Ameritrade, the largest online trading service in the US, with over 12 billion transactions per day, as well as by the Bank of England and Barclays Bank, among others.

Since 2005, the use of MUMPS has been either in the form of GT.M or InterSystems Caché. The latter is being aggressively marketed by InterSystems and has had success in penetrating new markets, such as telecommunications, in addition to existing markets. The European Space Agency announced on May 13, 2010 that it will use the InterSystems Caché database to support the Gaia mission. This mission aims to map the Milky Way with unprecedented precision.

MUMPS is a language intended for and designed to build database applications. Secondary language features were included to help programmers make applications using minimal computing resources. The original implementations were interpreted, though modern implementations may be fully or partially compiled. Individual "programs" run in memory "partitions". Early MUMPS memory partitions were limited to 2048 bytes so aggressive abbreviation greatly aided multi-programming on severely resource limited hardware, because more than one MUMPS job could fit into the very small memories extant in hardware at the time. The ability to provide multi-user systems was another language design feature. The word "Multi-Programming" in the acronym points to this. Even the earliest machines running MUMPS supported multiple jobs running at the same time. With the change from mini-computers to micro-computers a
few years later, even a "single user PC" with a single 8-bit CPU and 16K or 64K of memory could support multiple users, who could connect to it from (non-graphical) video display terminals.

Since memory was tight originally, the language design for MUMPS valued very terse code. Thus, every MUMPS command or function name could be abbreviated from one to three letters in length, e.g. Quit (exit program) as Q, $P = $Piece function, R = Read command, $TR = $Translate function. Spaces and end-of-line markers are significant in MUMPS because line scope promoted the same terse language design. Thus, a single line of program code could express, with few characters, an idea for which other programming languages could require 5 to 10 times as many characters. Abbreviation was a common feature of languages designed in this period (e.g., FOCAL-69, early BASICs such as Tiny BASIC, etc.). An unfortunate side effect of this, coupled with the early need to write minimalist code, was that MUMPS programmers routinely did not comment code and used extensive abbreviations. This meant that even an expert MUMPS programmer could not just skim through a page of code to see its function but would have to
analyze it line by line.

Database interaction is transparently built into the language. The MUMPS language provides a hierarchical database made up of persistent sparse arrays, which is implicitly "opened" for every MUMPS application. All variable names prefixed with the caret character ("^") use permanent (instead of RAM) storage, will maintain their values after the application exits, and will be visible to (and modifiable by) other running applications. Variables using this shared and permanent storage are called "Globals" in MUMPS, because the scoping of these variables is "globally available" to all jobs on the system. The more recent and more common use of the name "global variables" in other languages is a more limited scoping of names, coming from the fact that unscoped variables are "globally" available to any programs running in the same process, but not shared among multiple processes. The MUMPS Storage mode (i.e. Globals
stored as persistent sparse arrays), gives the MUMPS database the characteristics of a document-oriented database.

All variable names which are not prefixed with caret character ("^") are temporary and private. Like global variables, they also have a hierarchical storage model, but are only "locally available" to a single job, thus they are called "locals". Both "globals" and "locals" can have child nodes (called "subscripts" in MUMPS terminology). Subscripts are not limited to numerals—any ASCII character or group of characters can be a subscript identifier. While this is not uncommon for modern languages such as Perl or JavaScript, it was a highly unusual feature in the late 1970s. This capability was not universally implemented in MUMPS systems before the 1984 ANSI standard, as only canonically numeric subscripts were required by the standard to be allowed. Thus, the variable named 'Car' can have
subscripts "Door", "Steering Wheel" and "Engine", each of which can contain a value and have subscripts of their own. The variable ^Car("Door") could have a nested variable subscript of "Color" for example. Thus, you could say

to modify a nested child node of ^Car. In MUMPS terms, "Color" is the 2nd subscript of the variable ^Car (both the names of the child-nodes and the child-nodes themselves are likewise called subscripts). Hierarchical variables are similar to objects with properties in many object oriented languages. Additionally, the MUMPS language design requires that all subscripts of variables are automatically kept in sorted order. Numeric subscripts (including floating-point numbers) are stored from lowest to highest. All non-numeric subscripts are stored in alphabetical order following the numbers. In MUMPS terminology, this is "canonical order". By using only non-negative integer subscripts, the MUMPS programmer can emulate the arrays data type from other languages. Although MUMPS does not natively offer a full set of DBMS features such as mandatory schemas, several DBMS systems have been built on top of it that provide
application developers with flat-file, relational and network database features.

Additionally, there are built-in operators which treat a delimited string (e.g., comma-separated values) as an array. Early MUMPS programmers would often store a structure of related information as a delimited string, parsing it after it was read in; this saved disk access time and offered considerable speed advantages on some hardware.

MUMPS has no data types. Numbers can be treated as strings of digits, or strings can be treated as numbers by numeric operators ("coerced", in MUMPS terminology). Coercion can have some odd side effects, however. For example, when a string is coerced, the parser turns as much of the string (starting from the left) into a number as it can, then discards the rest. Thus the statement codice_1 is evaluated as codice_2 in MUMPS.

Other features of the language are intended to help MUMPS applications interact with each other in a multi-user environment. Database locks, process identifiers, and atomicity of database update transactions are all required of standard MUMPS implementations.

In contrast to languages in the C or Wirth traditions, some space characters between MUMPS statements are significant. A single space separates a command from its argument, and a space, or newline, separates each argument from the next MUMPS token. Commands which take no arguments (e.g., codice_3) require two following spaces. The concept is that one space separates the command from the (nonexistent) argument, the next separates the "argument" from the next command. Newlines are also significant; an codice_4, codice_3 or codice_6 command processes (or skips) everything else till the end-of-line. To make those statements control multiple lines, you must use the codice_7 command to create a code block.

A simple Hello world program in MUMPS might be:

and would be run from the MUMPS command line with the command codice_8. Since MUMPS allows commands to be strung together on the same line, and since commands can be abbreviated to a single letter, this routine could be made more compact:

The 'codice_9' after the text generates a newline.

ANSI X11.1-1995 gives a complete, formal description of the language; an annotated version of this standard is available online.

Data types: There is one universal datatype, which is implicitly coerced to string, integer, or floating-point datatypes as context requires.

Booleans (called "truthvalues" in MUMPS): In IF commands and other syntax that has expressions evaluated as conditions, any string value is evaluated as a numeric value, and if that is a nonzero value, then it is interpreted as True. codice_10 yields 1 if a is less than b, 0 otherwise.

Declarations: None. All variables are dynamically created at the first time a value is assigned.

Lines: are important syntactic entities, unlike their status in languages patterned on C or Pascal. Multiple statements per line are allowed and are common. The scope of any IF, ELSE, and FOR command is "the remainder of current line."

Case sensitivity: Commands and intrinsic functions are case-insensitive. In contrast, variable names and labels are case-sensitive. There is no special meaning for upper vs. lower-case and few widely followed conventions. The percent sign (%) is legal as first character of variables and labels.

Postconditionals: execution of almost any command can be controlled by following it with a colon and a truthvalue expression. codice_11 sets A to "FOO" if N is less than 10; codice_12 performs PRINTERR if N is greater than 100. This construct provides a conditional whose scope is less than a full line.

Abbreviation: You can abbreviate nearly all commands and native functions to one, two, or three characters.

Reserved words: None. Since MUMPS interprets source code by context, there is no need for reserved words. You may use the names of language commands as variables, so the following is perfectly legal MUMPS code:

MUMPS can be made more obfuscated by using the contracted operator syntax, as shown in this terse example derived from the example above:

Arrays: are created dynamically, stored as B-trees, are sparse (i.e. use almost no space for missing nodes), can use any number of subscripts, and subscripts can be strings or numeric (including floating point). Arrays are always automatically stored in sorted order, so there is never any occasion to sort, pack, reorder, or otherwise reorganize the database. Built in functions such as $DATA, $ORDER, $NEXT(deprecated) and $QUERY functions provide efficient examination and traversal of the fundamental array structure, on disk or in memory.

Local arrays: variable names not beginning with caret (i.e. "^") are stored in memory by process, are private to the creating process, and expire when the creating process terminates. The available storage depends on implementation. For those implementations using partitions, it is limited to the partition size, (A small partition might be 32K). For other implementations, it may be several megabytes.

Global arrays: codice_13. These are stored on disk, are available to all processes, and are persistent when the creating process terminates. Very large globals (for example, hundreds of gigabytes) are practical and efficient in most implementations. This is MUMPS' main "database" mechanism. It is used instead of calling on the operating system to create, write, and read files.

Indirection: in many contexts, codice_14 can be used, and effectively substitutes the contents of VBL into another MUMPS statement. codice_15 sets the variable ABC to 123. codice_16 performs the subroutine named REPORT. This substitution allows for lazy evaluation and late binding as well as effectively the operational equivalent of "pointers" in other languages.

Piece function: This breaks variables into segmented pieces guided by a user specified separator string (sometimes called a "delimiter"). Those who know awk will find this familiar. codice_17 means the "third caret-separated piece of STRINGVAR." The piece function can also appear as an assignment (SET command) target.

codice_18 yields "std".

After
codice_19 causes X to become "office@world.std.com" (note that $P is equivalent to $PIECE and could be written as such).

Order function: This function treats its input as a structure, and finds the next index that exists which has the same structure except for the last subscript. It returns the sorted value that is ordered after the one given as input. (This treats the array reference as a content-addressable data rather than an address of a value)

codice_20 yields 6, codice_21 yields 10, codice_22 yields 10, codice_23 yields 15, codice_24 yields "".

Here, the argument-less "For" repeats until stopped by a terminating "Quit". This line prints a table of i and stuff(i) where i is successively 6, 10, and 15.

For iterating the database, the Order function returns the next key to use.

Multi-User/Multi-Tasking/Multi-Processor: MUMPS supports multiple simultaneous users and processes even when the underlying operating system does not (e.g., MS-DOS). Additionally, there is the ability to specify an environment for a variable, such as by specifying a machine name in a variable (as in codice_25), which can allow you to access data on remote machines.

Some aspects of MUMPS syntax differ strongly from that of more modern languages, which can cause confusion. Whitespace is not allowed within expressions, as it ends a statement: codice_26 is an error, and must be written codice_27. All operators have the same precedence and are left-associative (codice_28 evaluates to 50). The operators for "less than or equal to" and "greater than or equal to" are codice_29 and codice_30 (that is, the boolean negation operator codice_31 plus a strict comparison operator). Periods (codice_32) are used to indent the lines in a DO block, not whitespace. The ELSE command does not need a corresponding IF, as it operates by inspecting the value in the builtin system variable codice_33.

MUMPS scoping rules are more permissive than other modern languages. Declared local variables are scoped using the stack. A routine can normally see all declared locals of the routines below it on the call stack, and routines cannot prevent routines they call from modifying their declared locals. By contrast, undeclared variables (variables created by using them, rather than declaration) are in scope for all routines running in the same process, and remain in scope until the program exits.

Because MUMPS database references differ from to internal variable references only in the caret prefix, it is dangerously easy to unintentionally edit the database, or even to delete a database "table".

All of the following positions can be, and have been, supported by knowledgeable people at various times:

Some of the contention arose in response to strong M advocacy on the part of one commercial interest, InterSystems, whose chief executive disliked the name MUMPS and felt that it represented a serious marketing obstacle. Thus, favoring M to some extent became identified as alignment with InterSystems. The dispute also reflected rivalry between organizations (the M Technology Association, the MUMPS Development Committee, the ANSI and ISO Standards Committees) as to who determines the "official" name of the language. Some writers have attempted to defuse the issue by referring to the language as "M[UMPS]", square brackets being the customary notation for optional syntax elements. A leading authority, and the author of an open source MUMPS implementation, Professor Kevin O'Kane, uses only 'MUMPS'.

The most recent standard (ISO/IEC 11756:1999, re-affirmed on 25 June 2010), still mentions both M and MUMPS as officially accepted names.

Massachusetts General Hospital registered "MUMPS" as a trademark with the USPTO on November 28, 1971, and renewed it on November 16, 1992 - but let it expire on August 30, 2003.

MUMPS invites comparison with the Pick operating system. Similarities include:pick






</doc>
<doc id="19726" url="https://en.wikipedia.org/wiki?curid=19726" title="Mercury (programming language)">
Mercury (programming language)

Mercury is a functional logic programming language made for real-world uses. The first version was developed at the University of Melbourne, Computer Science department, by Fergus Henderson, Thomas Conway, and Zoltan Somogyi, under Somogyi's supervision, and released on April 8, 1995.

Mercury is a purely declarative logic programming language. It is related to both Prolog and Haskell. It features a strong, static, polymorphic type system, and a strong mode and determinism system.

The official implementation, the Melbourne Mercury Compiler, is available for most Unix and Unix-like platforms, including Linux, macOS, and for Windows (32bits only).

Mercury is based on the logic programming language Prolog. It has the same syntax and the same basic concepts such as the selective linear definite clause resolution (SLD) algorithm. It can be viewed as a pure subset of Prolog with strong types and modes. As such, it is often compared to its predecessor in features and run-time efficiency.

The language is designed using software engineering principles. Unlike the original implementations of Prolog, it has a separate compilation phase, rather than being directly interpreted. This allows a much wider range of errors to be detected before running a program. It features a strict static type and mode system and a module system.

By using information obtained at compile time (such as type and mode), programs written in Mercury typically perform significantly faster than equivalent programs written in Prolog. Its authors claim that Mercury is the fastest logic language in the world, by a wide margin.

Mercury is a purely declarative language, unlike Prolog, since it lacks "extra-logical" Prolog statements such as codice_1 (cut) and imperative input/output (I/O). This enables advanced static program analysis and program optimization, including compile-time garbage collection, but it can make certain programming constructs (such as a switch over a number of options, with a default) harder to express. (While Mercury does allow impure functionality, this serves mainly as a way to call foreign language code. All impure code must be marked explicitly.) Operations which would typically be impure (such as input/output) are expressed using pure constructs in Mercury using linear types, by threading a dummy "world" value through all relevant code.

Notable programs written in Mercury include the Mercury compiler and the Prince XML formatter. Software company Mission Critical IT has also been using Mercury since 2000 to develop enterprise applications and its Ontology-Driven software development platform, ODASE.

Mercury has several back-ends, which enable compiling Mercury code into several languages, including:



Mercury also features a foreign language interface, allowing code in other languages (depending on the chosen back-end) to be linked with Mercury code. The following foreign languages are possible:
Other languages can then be interfaced to by calling them from these languages. However, this means that foreign language code may need to be written several times for the different backends, otherwise portability between backends will be lost.

The most commonly used back-end is the original low-level C back-end.

Hello World:
Calculating the 10th Fibonacci number (in the most obvious way):

codice_2 is a "state variable", which is syntactic sugar for a pair of variables which are assigned concrete names at compilation; for example, the above is desugared to something like:
Releases are named according to the year and month of release. The current stable release is 14.01.1 (September 2014). Prior releases were numbered 0.12, 0.13, etc., and the time between stable releases can be as long as 3 years.

There is often also a snapshot "release of the day" (ROTD) consisting of the latest features and bug fixes added to the last stable release.




</doc>
<doc id="19727" url="https://en.wikipedia.org/wiki?curid=19727" title="Michael Faraday">
Michael Faraday

Michael Faraday FRS (; 22 September 1791 – 25 August 1867) was an English scientist who contributed to the study of electromagnetism and electrochemistry. His main discoveries include the principles underlying electromagnetic induction, diamagnetism and electrolysis.

Although Faraday received little formal education, he was one of the most influential scientists in history. It was by his research on the magnetic field around a conductor carrying a direct current that Faraday established the basis for the concept of the electromagnetic field in physics. Faraday also established that magnetism could affect rays of light and that there was an underlying relationship between the two phenomena. He similarly discovered the principles of electromagnetic induction and diamagnetism, and the laws of electrolysis. His inventions of electromagnetic rotary devices formed the foundation of electric motor technology, and it was largely due to his efforts that electricity became practical for use in technology.

As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented an early form of the Bunsen burner and the system of oxidation numbers, and popularised terminology such as "anode", "cathode", "electrode" and "ion". Faraday ultimately became the first and foremost Fullerian Professor of Chemistry at the Royal Institution, a lifetime position.

Faraday was an excellent experimentalist who conveyed his ideas in clear and simple language; his mathematical abilities, however, did not extend as far as trigonometry and were limited to the simplest algebra. James Clerk Maxwell took the work of Faraday and others and summarized it in a set of equations which is accepted as the basis of all modern theories of electromagnetic phenomena. On Faraday's uses of lines of force, Maxwell wrote that they show Faraday "to have been in reality a mathematician of a very high order – one from whom the mathematicians of the future may derive valuable and fertile methods." The SI unit of capacitance is named in his honour: the farad.

Albert Einstein kept a picture of Faraday on his study wall, alongside pictures of Isaac Newton and James Clerk Maxwell. Physicist Ernest Rutherford stated, "When we consider the magnitude and extent of his discoveries and their influence on the progress of science and of industry, there is no honour too great to pay to the memory of Faraday, one of the greatest scientific discoverers of all time."

Michael Faraday was born on 22 September 1791 in Newington Butts, which is now part of the London Borough of Southwark but was then a suburban part of Surrey. His family was not well off. His father, James, was a member of the Glassite sect of Christianity. James Faraday moved his wife and two children to London during the winter of 1790 from Outhgill in Westmorland, where he had been an apprentice to the village blacksmith. Michael was born in the autumn of that year. The young Michael Faraday, who was the third of four children, having only the most basic school education, had to educate himself.

At the age of 14 he became an apprentice to George Riebau, a local bookbinder and bookseller in Blandford Street. During his seven-year apprenticeship Faraday read many books, including Isaac Watts's "The Improvement of the Mind", and he enthusiastically implemented the principles and suggestions contained therein. He also developed an interest in science, especially in electricity. Faraday was particularly inspired by the book "Conversations on Chemistry" by Jane Marcet.

In 1812, at the age of 20 and at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist Humphry Davy of the Royal Institution and the Royal Society, and John Tatum, founder of the City Philosophical Society. Many of the tickets for these lectures were given to Faraday by William Dance, who was one of the founders of the Royal Philharmonic Society. Faraday subsequently sent Davy a 300-page book based on notes that he had taken during these lectures. Davy's reply was immediate, kind, and favourable. In 1813, when Davy damaged his eyesight in an accident with nitrogen trichloride, he decided to employ Faraday as an assistant. Coincidentally one of the Royal Institution's assistants, John Payne, was sacked and Sir Humphry Davy had been asked to find a replacement; thus he appointed Faraday as Chemical Assistant at the Royal Institution on 1 March 1813. Very soon Davy entrusted Faraday with the preparation of nitrogen trichloride samples, and they both were injured in an explosion of this very sensitive substance.
In the class-based English society of the time, Faraday was not considered a gentleman. When Davy set out on a long tour of the continent in 1813–15, his valet did not wish to go, so instead, Faraday went as Davy's scientific assistant and was asked to act as Davy's valet until a replacement could be found in Paris. Faraday was forced to fill the role of valet as well as assistant throughout the trip. Davy's wife, Jane Apreece, refused to treat Faraday as an equal (making him travel outside the coach, eat with the servants, etc.), and made Faraday so miserable that he contemplated returning to England alone and giving up science altogether. The trip did, however, give him access to the scientific elite of Europe and exposed him to a host of stimulating ideas.

Faraday married Sarah Barnard (1800–1879) on 12 June 1821. They met through their families at the Sandemanian church, and he confessed his faith to the Sandemanian congregation the month after they were married. They had no children.

Faraday was a devout Christian; his Sandemanian denomination was an offshoot of the Church of Scotland. Well after his marriage, he served as deacon and for two terms as an elder in the meeting house of his youth. His church was located at Paul's Alley in the Barbican. This meeting house relocated in 1862 to Barnsbury Grove, Islington; this North London location was where Faraday served the final two years of his second term as elder prior to his resignation from that post. Biographers have noted that "a strong sense of the unity of God and nature pervaded Faraday's life and work."

In June 1832, the University of Oxford granted Faraday an honorary Doctor of Civil Law degree. During his lifetime, he was offered a knighthood in recognition for his services to science, which he turned down on religious grounds, believing that it was against the word of the Bible to accumulate riches and pursue worldly reward, and stating that he preferred to remain "plain Mr Faraday to the end". Elected a member of the Royal Society in 1824, he twice refused to become President. He became the first Fullerian Professor of Chemistry at the Royal Institution in 1833.

In 1832, Faraday was elected a Foreign Honorary Member of the American Academy of Arts and Sciences. He was elected a foreign member of the Royal Swedish Academy of Sciences in 1838, and was one of eight foreign members elected to the French Academy of Sciences in 1844. In 1849 he was elected as associated member to the Royal Institute of the Netherlands, which two years later became the Royal Netherlands Academy of Arts and Sciences and he was subsequently made foreign member.

Faraday suffered a nervous breakdown in 1839 but eventually returned to his investigations into electromagnetism. In 1848, as a result of representations by the Prince Consort, Faraday was awarded a grace and favour house in Hampton Court in Middlesex, free of all expenses and upkeep. This was the Master Mason's House, later called Faraday House, and now No. 37 Hampton Court Road. In 1858 Faraday retired to live there.

Having provided a number of various service projects for the British government, when asked by the government to advise on the production of chemical weapons for use in the Crimean War (1853–1856), Faraday refused to participate citing ethical reasons.

Faraday died at his house at Hampton Court on 25 August 1867, aged 75. He had some years before turned down an offer of burial in Westminster Abbey upon his death, but he has a memorial plaque there, near Isaac Newton's tomb. Faraday was interred in the dissenters' (non-Anglican) section of Highgate Cemetery.

Faraday's earliest chemical work was as an assistant to Humphry Davy. Faraday was specifically involved in the study of chlorine; he discovered two new compounds of chlorine and carbon. He also conducted the first rough experiments on the diffusion of gases, a phenomenon that was first pointed out by John Dalton. The physical importance of this phenomenon was more fully revealed by Thomas Graham and Joseph Loschmidt. Faraday succeeded in liquefying several gases, investigated the alloys of steel, and produced several new kinds of glass intended for optical purposes. A specimen of one of these heavy glasses subsequently became historically important; when the glass was placed in a magnetic field Faraday determined the rotation of the plane of polarisation of light. This specimen was also the first substance found to be repelled by the poles of a magnet.

Faraday invented an early form of what was to become the Bunsen burner, which is in practical use in science laboratories around the world as a convenient source of heat.
Faraday worked extensively in the field of chemistry, discovering chemical substances such as benzene (which he called bicarburet of hydrogen) and liquefying gases such as chlorine. The liquefying of gases helped to establish that gases are the vapours of liquids possessing a very low boiling point and gave a more solid basis to the concept of molecular aggregation. In 1820 Faraday reported the first synthesis of compounds made from carbon and chlorine, CCl and CCl, and published his results the following year. Faraday also determined the composition of the chlorine clathrate hydrate, which had been discovered by Humphry Davy in 1810. Faraday is also responsible for discovering the laws of electrolysis, and for popularizing terminology such as anode, cathode, electrode, and ion, terms proposed in large part by William Whewell.

Faraday was the first to report what later came to be called metallic nanoparticles. In 1847 he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal. This was probably the first reported observation of the effects of quantum size, and might be considered to be the birth of nanoscience.

Faraday is best known for his work regarding electricity and magnetism. His first recorded experiment was the construction of a voltaic pile with seven ha'penny coins, stacked together with seven disks of sheet zinc, and six pieces of paper moistened with salt water. With this pile he decomposed sulfate of magnesia (first letter to Abbott, 12 July 1812).

In 1821, soon after the Danish physicist and chemist Hans Christian Ørsted discovered the phenomenon of electromagnetism, Davy and British scientist William Hyde Wollaston tried, but failed, to design an electric motor. Faraday, having discussed the problem with the two men, went on to build two devices to produce what he called "electromagnetic rotation". One of these, now known as the homopolar motor, caused a continuous circular motion that was engendered by the circular magnetic force around a wire that extended into a pool of mercury wherein was placed a magnet; the wire would then rotate around the magnet if supplied with current from a chemical battery. These experiments and inventions formed the foundation of modern electromagnetic technology. In his excitement, Faraday published results without acknowledging his work with either Wollaston or Davy. The resulting controversy within the Royal Society strained his mentor relationship with Davy and may well have contributed to Faraday's assignment to other activities, which consequently prevented his involvement in electromagnetic research for several years.

From his initial discovery in 1821, Faraday continued his laboratory work, exploring electromagnetic properties of materials and developing requisite experience. In 1824, Faraday briefly set up a circuit to study whether a magnetic field could regulate the flow of a current in an adjacent wire, but he found no such relationship. This experiment followed similar work conducted with light and magnets three years earlier that yielded identical results. During the next seven years, Faraday spent much of his time perfecting his recipe for optical quality (heavy) glass, borosilicate of lead, which he used in his future studies connecting light with magnetism. In his spare time, Faraday continued publishing his experimental work on optics and electromagnetism; he conducted correspondence with scientists whom he had met on his journeys across Europe with Davy, and who were also working on electromagnetism. Two years after the death of Davy, in 1831, he began his great series of experiments in which he discovered electromagnetic induction, recording in his laboratory diary on 28 October 1831 he was; "making many experiments with the great magnet of the Royal Society".

Faraday's breakthrough came when he wrapped two insulated coils of wire around an iron ring, and found that upon passing a current through one coil, a momentary current was induced in the other coil. This phenomenon is now known as mutual induction. The iron ring-coil apparatus is still on display at the Royal Institution. In subsequent experiments, he found that if he moved a magnet through a loop of wire an electric current flowed in that wire. The current also flowed if the loop was moved over a stationary magnet. His demonstrations established that a changing magnetic field produces an electric field; this relation was modelled mathematically by James Clerk Maxwell as Faraday's law, which subsequently became one of the four Maxwell equations, and which have in turn evolved into the generalization known today as field theory. Faraday would later use the principles he had discovered to construct the electric dynamo, the ancestor of modern power generators and the electric motor.

In 1832, he completed a series of experiments aimed at investigating the fundamental nature of electricity; Faraday used "static", batteries, and "animal electricity" to produce the phenomena of electrostatic attraction, electrolysis, magnetism, etc. He concluded that, contrary to the scientific opinion of the time, the divisions between the various "kinds" of electricity were illusory. Faraday instead proposed that only a single "electricity" exists, and the changing values of quantity and intensity (current and voltage) would produce different groups of phenomena.

Near the end of his career, Faraday proposed that electromagnetic forces extended into the empty space around the conductor. This idea was rejected by his fellow scientists, and Faraday did not live to see the eventual acceptance of his proposition by the scientific community. Faraday's concept of lines of flux emanating from charged bodies and magnets provided a way to visualize electric and magnetic fields; that conceptual model was crucial for the successful development of the electromechanical devices that dominated engineering and industry for the remainder of the 19th century.

In 1845, Faraday discovered that many materials exhibit a weak repulsion from a magnetic field: a phenomenon he termed diamagnetism.

Faraday also discovered that the plane of polarization of linearly polarized light can be rotated by the application of an external magnetic field aligned with the direction in which the light is moving. This is now termed the Faraday effect. In Sept 1845 he wrote in his notebook, "I have at last succeeded in "illuminating a magnetic curve" or "line of force" and in "magnetising a ray of light"".

Later on in his life, in 1862, Faraday used a spectroscope to search for a different alteration of light, the change of spectral lines by an applied magnetic field. The equipment available to him was, however, insufficient for a definite determination of spectral change. Pieter Zeeman later used an improved apparatus to study the same phenomenon, publishing his results in 1897 and receiving the 1902 Nobel Prize in Physics for his success. In both his 1897 paper and his Nobel acceptance speech, Zeeman made reference to Faraday's work.

In his work on static electricity, Faraday's ice pail experiment demonstrated that the charge resided only on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields emanating from them cancel one another. This shielding effect is used in what is now known as a Faraday cage.

Faraday had a long association with the Royal Institution of Great Britain. He was appointed Assistant Superintendent of the House of the Royal Institution in 1821. He was elected a member of the Royal Society in 1824. In 1825, he became Director of the Laboratory of the Royal Institution. Six years later, in 1833, Faraday became the first Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a position to which he was appointed for life without the obligation to deliver lectures. His sponsor and mentor was John 'Mad Jack' Fuller, who created the position at the Royal Institution for Faraday.

Beyond his scientific research into areas such as chemistry, electricity, and magnetism at the Royal Institution, Faraday undertook numerous, and often time-consuming, service projects for private enterprise and the British government. This work included investigations of explosions in coal mines, being an expert witness in court, and along with two engineers from Chance Brothers c.1853, the preparation of high-quality optical glass, which was required by Chance for its lighthouses. In 1846, together with Charles Lyell, he produced a lengthy and detailed report on a serious explosion in the colliery at Haswell, County Durham, which killed 95 miners. Their report was a meticulous forensic investigation and indicated that coal dust contributed to the severity of the explosion. The report should have warned coal owners of the hazard of coal dust explosions, but the risk was ignored for over 60 years until the Senghenydd Colliery Disaster of 1913.

As a respected scientist in a nation with strong maritime interests, Faraday spent extensive amounts of time on projects such as the construction and operation of lighthouses and protecting the bottoms of ships from corrosion. His workshop still stands at Trinity Buoy Wharf above the Chain and Buoy Store, next to London's only lighthouse where he carried out the first experiments in electric lighting for lighthouses.

Faraday was also active in what would now be called environmental science, or engineering. He investigated industrial pollution at Swansea and was consulted on air pollution at the Royal Mint. In July 1855, Faraday wrote a letter to "The Times" on the subject of the foul condition of the River Thames, which resulted in an often-reprinted cartoon in "Punch". (See also The Great Stink).

Faraday assisted with the planning and judging of exhibits for the Great Exhibition of 1851 in London. He also advised the National Gallery on the cleaning and protection of its art collection, and served on the National Gallery Site Commission in 1857. Education was another of Faraday's areas of service; he lectured on the topic in 1854 at the Royal Institution, and in 1862 he appeared before a Public Schools Commission to give his views on education in Great Britain. Faraday also weighed in negatively on the public's fascination with table-turning, mesmerism, and seances, and in so doing chastised both the public and the nation's educational system.
Before his famous Christmas lectures, Faraday delivered chemistry lectures for the City Philosophical Society from 1816 to 1818 in order to refine the quality of his lectures. Between 1827 and 1860 at the Royal Institution in London, Faraday gave a series of nineteen Christmas lectures for young people, a series which continues today. The objective of Faraday's Christmas lectures was to present science to the general public in the hopes of inspiring them and generating revenue for the Royal Institution. They were notable events on the social calendar among London's gentry. Over the course of several letters to his close friend Benjamin Abbott, Faraday outlined his recommendations on the art of lecturing: Faraday wrote "a flame should be lighted at the commencement and kept alive with unremitting splendour to the end". His lectures were joyful and juvenile, he delighted in filling soap bubbles with various gasses (in order to determine whether or not they are magnetic) in front of his audiences and marveled at the rich colors of polarized lights, but the lectures were also deeply philosophical. In his lectures he urged his audiences to consider the mechanics of his experiments: "you know very well that ice floats upon water ... Why does the ice float? Think of that, and philosophise". His subjects consisted of Chemistry and Electricity, and included: 1841 The Rudiments of Chemistry, 1843 First Principles of Electricity, 1848 The Chemical History of a Candle, 1851 Attractive Forces, 1853 Voltaic Electricity, 1854 The Chemistry of Combustion, 1855 The Distinctive Properties of the Common Metals, 1857 Static Electricity, 1858 The Metallic Properties, 1859 The Various Forces of Matter and their Relations to Each Other.

A statue of Faraday stands in Savoy Place, London, outside the Institution of Engineering and Technology. The Michael Faraday Memorial, designed by brutalist architect Rodney Gordon and completed in 1961, is at the Elephant & Castle gyratory system, near Faraday's birthplace at Newington Butts, London. Faraday School is located on Trinity Buoy Wharf where his workshop still stands above the Chain and Buoy Store, next to London's only lighthouse. Faraday Gardens is a small park in Walworth, London, not far from his birthplace at Newington Butts. It lies within the local council ward of Faraday in the London Borough of Southwark. Michael Faraday Primary school is situated on the Aylesbury Estate in Walworth.

A building at London South Bank University, which houses the institute's electrical engineering departments is named the Faraday Wing, due to its proximity to Faraday's birthplace in Newington Butts. A hall at Loughborough University was named after Faraday in 1960. Near the entrance to its dining hall is a bronze casting, which depicts the symbol of an electrical transformer, and inside there hangs a portrait, both in Faraday's honour. An eight-story building at the University of Edinburgh's science & engineering campus is named for Faraday, as is a recently built hall of accommodation at Brunel University, the main engineering building at Swansea University, and the instructional and experimental physics building at Northern Illinois University. The former UK Faraday Station in Antarctica was named after him.
Streets named for Faraday can be found in many British cities (e.g., London, Fife, Swindon, Basingstoke, Nottingham, Whitby, Kirkby, Crawley, Newbury, Swansea, Aylesbury and Stevenage) as well as in France (Paris), Germany (Berlin-Dahlem, Hermsdorf), Canada (Quebec City, Quebec; Deep River, Ontario; Ottawa, Ontario), and the United States (Reston, Virginia).
A Royal Society of Arts blue plaque, unveiled in 1876, commemorates Faraday at 48 Blandford Street in London's Marylebone district. From 1991 until 2001, Faraday's picture featured on the reverse of Series E £20 banknotes issued by the Bank of England. He was portrayed conducting a lecture at the Royal Institution with the magneto-electric spark apparatus. In 2002, Faraday was ranked number 22 in the BBC's list of the 100 Greatest Britons following a UK-wide vote.

The Faraday Institute for Science and Religion derives its name from the scientist, who saw his faith as integral to his scientific research. The logo of the Institute is also based on Faraday's discoveries. It was created in 2006 by a $2,000,000 grant from the John Templeton Foundation to carry out academic research, to foster understanding of the interaction between science and religion, and to engage public understanding in both these subject areas.

Faraday's life and contributions to electromagnetics was the principal topic of the tenth episode, titled "The Electric Boy", of the 2014 American science documentary series, "", which was broadcast on Fox and the National Geographic Channel.

Aldous Huxley, the literary giant who was also the grandson of T. H. Huxley, the grandnephew of Matthew Arnold, the brother of Julian Huxley, and the half-brother of Andrew Huxley, was well-versed in science. He wrote about Faraday in an essay entitled, "A Night in Pietramala": “He is always the natural philosopher. To discover truth is his sole aim and interest…even if I could be Shakespeare, I think I should still choose to be Faraday.”

In honor and remembrance of his great scientific contributions, several institutions have created prizes and awards in his name. This include:

Faraday's books, with the exception of "Chemical Manipulation", were collections of scientific papers or transcriptions of lectures. Since his death, Faraday's diary has been published, as have several large volumes of his letters and Faraday's journal from his travels with Davy in 1813–1815.




</doc>
<doc id="19728" url="https://en.wikipedia.org/wiki?curid=19728" title="Marriage">
Marriage

Marriage, also called matrimony or wedlock, is a culturally recognised union between people, called spouses, that establishes rights and obligations between them, as well as between them and their children, and between them and their in-laws. The definition of marriage varies around the world, not only between cultures and between religions, but also throughout the history of any given culture and religion. Over time, it has expanded and also constricted in terms of who and what is encompassed. Typically, it is an institution in which interpersonal relationships, usually sexual, are acknowledged or sanctioned. In some cultures, marriage is recommended or considered to be compulsory before pursuing any sexual activity. When defined broadly, marriage is considered a cultural universal. A marriage ceremony is called a wedding.

Individuals may marry for several reasons, including legal, social, libidinal, emotional, financial, spiritual, and religious purposes. Whom they marry may be influenced by gender, socially determined rules of incest, prescriptive marriage rules, parental choice and individual desire. In some areas of the world, arranged marriage, child marriage, polygamy, and sometimes forced marriage, may be practiced as a cultural tradition. Conversely, such practices may be outlawed and penalized in parts of the world out of concerns regarding the infringement of women's rights, or the infringement of children's rights (both female and male), and because of international law. Around the world, primarily in developed democracies, there has been a general trend towards ensuring equal rights for women within marriage and legally recognizing the marriages of interfaith, interracial, and same-sex couples. These trends coincide with the broader human rights movement.

Marriage can be recognized by a state, an organization, a religious authority, a tribal group, a local community, or peers. It is often viewed as a contract. When a marriage is performed and carried out by a government institution in accordance with the marriage laws of the jurisdiction, without religious content, it is a civil marriage. Civil marriage recognizes and creates the rights and obligations intrinsic to matrimony in the eyes of the state. When a marriage is performed with religious content under the auspices of a religious institution, it is a religious marriage. Religious marriage recognizes and creates the rights and obligations intrinsic to matrimony in the eyes of that religion. Religious marriage is known variously as sacramental marriage in Catholicism, nikah in Islam, nissuin in Judaism, and various other names in other faith traditions, each with their own constraints as to what constitutes, and who can enter into, a valid religious marriage.

Some countries do not recognize locally performed religious marriage on its own, and require a separate civil marriage for official purposes. Conversely, civil marriage does not exist in some countries governed by a religious legal system, such as Saudi Arabia, where marriages contracted abroad might not be recognized if they were contracted contrary to Saudi interpretations of Islamic religious law. In countries governed by a mixed secular-religious legal system, such as Lebanon and Israel, locally performed civil marriage does not exist within the country, which prevents interfaith and various other marriages that contradict religious laws from being entered into in the country; however, civil marriages performed abroad may be recognized by the state even if they conflict with religious laws. For example, in the case of recognition of marriage in Israel, this includes recognition of not only interfaith civil marriages performed abroad, but also overseas same-sex civil marriages.

The act of marriage usually creates normative or legal obligations between the individuals involved, and any offspring they may produce or adopt. In terms of legal recognition, most sovereign states and other jurisdictions limit marriage to opposite-sex couples and a diminishing number of these permit polygyny, child marriages, and forced marriages. In modern times, a growing number of countries, primarily developed democracies, have lifted bans on, and have established legal recognition for, the marriages of interfaith, interracial, and same-sex couples. In some areas, child marriages and polygamy may occur in spite of national laws against the practice.

Since the late twentieth century, major social changes in Western countries have led to changes in the demographics of marriage, with the age of first marriage increasing, fewer people marrying, and more couples choosing to cohabit rather than marry. For example, the number of marriages in Europe decreased by 30% from 1975 to 2005.

Historically, in most cultures, married women had very few rights of their own, being considered, along with the family's children, the property of the husband; as such, they could not own or inherit property, or represent themselves legally (see, for example, coverture). In Europe, the United States, and other places in the developed world, beginning in the late 19th century, marriage has undergone gradual legal changes, aimed at improving the rights of the wife. These changes included giving wives legal identities of their own, abolishing the right of husbands to physically discipline their wives, giving wives property rights, liberalizing divorce laws, providing wives with reproductive rights of their own, and requiring a wife's consent when sexual relations occur. These changes have occurred primarily in Western countries. In the 21st century, there continue to be controversies regarding the legal status of married women, legal acceptance of or leniency towards violence within marriage (especially sexual violence), traditional marriage customs such as dowry and bride price, forced marriage, marriageable age, and criminalization of consensual behaviors such as premarital and extramarital sex.

The word "marriage" derives from Middle English "mariage", which first appears in 1250–1300 CE. This in turn is derived from Old French, "marier" (to marry), and ultimately Latin, "marītāre", meaning to provide with a husband or wife and "marītāri" meaning to get married. The adjective "marīt-us -a, -um" meaning matrimonial or nuptial could also be used in the masculine form as a noun for "husband" and in the feminine form for "wife". The related word "matrimony" derives from the Old French word "matremoine", which appears around 1300 CE and ultimately derives from Latin "mātrimōnium", which combines the two concepts: "mater" meaning "mother" and the suffix -"monium" signifying "action, state, or condition".

Anthropologists have proposed several competing definitions of marriage in an attempt to encompass the wide variety of marital practices observed across cultures. Even within Western culture, "definitions of marriage have careened from one extreme to another and everywhere in between" (as Evan Gerstmann has put it).

In "The History of Human Marriage" (1891), Edvard Westermarck defined marriage as "a more or less durable connection between male and female lasting beyond the mere act of propagation till after the birth of the offspring." In "The Future of Marriage in Western Civilization" (1936), he rejected his earlier definition, instead provisionally defining marriage as "a relation of one or more men to one or more women that is recognized by custom or law".

The anthropological handbook "Notes and Queries" (1951) defined marriage as "a union between a man and a woman such that children born to the woman are the recognized legitimate offspring of both partners." In recognition of a practice by the Nuer people of Sudan allowing women to act as a husband in certain circumstances (the ghost marriage), Kathleen Gough suggested modifying this to "a woman and one or more other persons."

In an analysis of marriage among the Nayar, a polyandrous society in India, Gough found that the group lacked a husband role in the conventional sense; that unitary role in the west was divided between a non-resident "social father" of the woman's children, and her lovers who were the actual procreators. None of these men had legal rights to the woman's child. This forced Gough to disregard sexual access as a key element of marriage and to define it in terms of legitimacy of offspring alone: marriage is "a relationship established between a woman and one or more other persons, which provides a child born to the woman under circumstances not prohibited by the rules of relationship, is accorded full birth-status rights common to normal members of his society or social stratum."

Economic anthropologist Duran Bell has criticized the legitimacy-based definition on the basis that some societies do not require marriage for legitimacy. He argued that a legitimacy-based definition of marriage is circular in societies where illegitimacy has no other legal or social implications for a child other than the mother being unmarried.

Edmund Leach criticized Gough's definition for being too restrictive in terms of recognized legitimate offspring and suggested that marriage be viewed in terms of the different types of rights it serves to establish. In a 1955 article in "Man", Leach argued that no one definition of marriage applied to all cultures. He offered a list of ten rights associated with marriage, including sexual monopoly and rights with respect to children, with specific rights differing across cultures. Those rights, according to Leach, included:

In a 1997 article in "Current Anthropology", Duran Bell describes marriage as "a relationship between one or more men (male or female) in severalty to one or more women that provides those men with a demand-right of sexual access within a domestic group and identifies women who bear the obligation of yielding to the demands of those specific men." In referring to "men in severalty", Bell is referring to corporate kin groups such as lineages which, in having paid brideprice, retain a right in a woman's offspring even if her husband (a lineage member) deceases (Levirate marriage). In referring to "men (male or female)", Bell is referring to women within the lineage who may stand in as the "social fathers" of the wife's children born of other lovers. (See Nuer "ghost marriage".)

Monogamy is a form of marriage in which an individual has only one spouse during their lifetime or at any one time (serial monogamy).

Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found a strong correlation between intensive plough agriculture, dowry and monogamy. This pattern was found in a broad swath of Eurasian societies from Japan to Ireland. The majority of Sub-Saharan African societies that practice extensive hoe agriculture, in contrast, show a correlation between "bride price" and polygamy. A further study drawing on the Ethnographic Atlas showed a statistical correlation between increasing size of the society, the belief in "high gods" to support human morality, and monogamy.

In the countries which do not permit polygamy, a person who marries in one of those countries a person while still being lawfully married to another commits the crime of bigamy. In all cases, the second marriage is considered legally null and void. Besides the second and subsequent marriages being void, the bigamist is also liable to other penalties, which also vary between jurisdictions.

Governments that support monogamy may allow easy divorce. In a number of Western countries divorce rates approach 50%. Those who remarry do so on average three times. Divorce and remarriage can thus result in "serial monogamy", i.e. having multiple marriages but only one legal spouse at a time. This can be interpreted as a form of plural mating, as are those societies dominated by female-headed families in the Caribbean, Mauritius and Brazil where there is frequent rotation of unmarried partners. In all, these account for 16 to 24% of the "monogamous" category.

Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, remains an active part of her "ex-husband's" or "ex-wife's" life, as they may be tied together by transfers of resources (alimony, child support), or shared child custody. Bob Simpson notes that in the British case, serial monogamy creates an "extended family" – a number of households tied together in this way, including mobile children (possible exes may include an ex-wife, an ex-brother-in-law, etc., but not an "ex-child"). These "unclear families" do not fit the mould of the monogamous nuclear family. As a series of connected households, they come to resemble the polygynous model of separate households maintained by mothers with children, tied by a male to whom they are married or divorced.

Polygamy is a marriage which includes more than two partners. When a man is married to more than one wife at a time, the relationship is called polygyny, and there is no marriage bond between the wives; and when a woman is married to more than one husband at a time, it is called polyandry, and there is no marriage bond between the husbands. If a marriage includes multiple husbands or wives, it can be called group marriage.

A molecular genetic study of global human genetic diversity argued that sexual polygyny was typical of human reproductive patterns until the shift to sedentary farming communities approximately 10,000 to 5,000 years ago in Europe and Asia, and more recently in Africa and the Americas. As noted above, Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found that the majority of Sub-Saharan African societies that practice extensive hoe agriculture show a correlation between "Bride price" and polygamy. A survey of other cross-cultural samples has confirmed that the absence of the plough was the only predictor of polygamy, although other factors such as high male mortality in warfare (in non-state societies) and pathogen stress (in state societies) had some impact.

Marriages are classified according to the number of legal spouses an individual has. The suffix "-gamy" refers specifically to the number of spouses, as in bi-gamy (two spouses, generally illegal in most nations), and poly-gamy (more than one spouse).

Societies show variable acceptance of polygamy as a cultural ideal and practice. According to the Ethnographic Atlas, of 1,231 societies noted, 186 were monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. However, as Miriam Zeitzen writes, social tolerance for polygamy is different from the practice of polygamy, since it requires wealth to establish multiple households for multiple wives. The actual practice of polygamy in a tolerant society may actually be low, with the majority of aspirant polygamists practicing monogamous marriage. Tracking the occurrence of polygamy is further complicated in jurisdictions where it has been banned, but continues to be practiced ("de facto polygamy").

Zeitzen also notes that Western perceptions of African society and marriage patterns are biased by "contradictory concerns of nostalgia for traditional African culture versus critique of polygamy as oppressive to women or detrimental to development." Polygamy has been condemned as being a form of human rights abuse, with concerns arising over domestic abuse, forced marriage, and neglect. The vast majority of the world's countries, including virtually all of the world's developed nations, do not permit polygamy. There have been calls for the abolition of polygamy in developing countries.

Polygyny usually grants wives equal status, although the husband may have personal preferences. One type of de facto polygyny is concubinage, where only one woman gets a wife's rights and status, while other women remain legal house mistresses.

Although a society may be classified as polygynous, not all marriages in it necessarily are; monogamous marriages may in fact predominate. It is to this flexibility that Anthropologist Robin Fox attributes its success as a social support system: "This has often meant – given the imbalance in the sex ratios, the higher male infant mortality, the shorter life span of males, the loss of males in wartime, etc. – that often women were left without financial support from husbands. To correct this condition, females had to be killed at birth, remain single, become prostitutes, or be siphoned off into celibate religious orders. Polygynous systems have the advantage that they can promise, as did the Mormons, a home and family for every woman."

Nonetheless, polygyny is a gender issue which offers men asymmetrical benefits. In some cases, there is a large age discrepancy (as much as a generation) between a man and his youngest wife, compounding the power differential between the two. Tensions not only exist "between" genders, but also "within" genders; senior and junior men compete for wives, and senior and junior wives in the same household may experience radically different life conditions, and internal hierarchy. Several studies have suggested that the wive's relationship with other women, including co-wives and husband's female kin, are more critical relationships than that with her husband for her productive, reproductive and personal achievement. In some societies, the co-wives are relatives, usually sisters, a practice called "sororal polygyny"; the pre-existing relationship between the co-wives is thought to decrease potential tensions within the marriage.

Fox argues that "the major difference between polygyny and monogamy could be stated thus: while plural mating occurs in both systems, under polygyny several unions may be recognized as being legal marriages while under monogamy only one of the unions is so recognized. Often, however, it is difficult to draw a hard and fast line between the two."

As polygamy in Africa is increasingly subject to legal limitations, a variant form of "de facto" (as opposed to legal or "de jure") polygyny is being practised in urban centres. Although it does not involve multiple (now illegal) formal marriages, the domestic and personal arrangements follow old polygynous patterns. The de facto form of polygyny is found in other parts of the world as well (including some Mormon sects and Muslim families in the United States).
In some societies such as the Lovedu of South Africa, or the Nuer of the Sudan, aristocratic women may become female 'husbands.' In the Lovedu case, this female husband may take a number of polygamous wives. This is not a lesbian relationship, but a means of legitimately expanding a royal lineage by attaching these wives' children to it. The relationships are considered polygynous, not polyandrous, because the female husband is in fact assuming masculine gendered political roles.

Religious groups have differing views on the legitimacy of polygyny. It is allowed in Islam and Confucianism. Judaism and Christianity have mentioned practices involving polygyny in the past, however, outright religious acceptance of such practices was not addressed until its rejection in later passages. They do explicitly prohibit polygyny today.

Polyandry is notably more rare than polygyny, though less rare than the figure commonly cited in the "Ethnographic Atlas" (1980) which listed only those polyandrous societies found in the Himalayan Mountains. More recent studies have found 53 societies outside the 28 found in the Himalayans which practice polyandry. It is most common in egalitarian societies marked by high male mortality or male absenteeism. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

The explanation for polyandry in the Himalayan Mountains is related to the scarcity of land; the marriage of all brothers in a family to the same wife ("fraternal polyandry") allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this was prevented through the social practice of impartible inheritance (the dis-inheriting of most siblings, some of whom went on to become celibate monks and priests).

Group marriage (also known as "multi-lateral marriage") is a form of polyamory in which more than two persons form a family unit, with all the members of the group marriage being considered to be married to all the other members of the group marriage, and all members of the marriage share parental responsibility for any children arising from the marriage. No country legally condones group marriages, neither under the law nor as a common law marriage, but historically it has been practiced by some cultures of Polynesia, Asia, Papua New Guinea and the Americas – as well as in some intentional communities and alternative subcultures such as the Oneida Perfectionists in up-state New York. Of the 250 societies reported by the American anthropologist George Murdock in 1949, only the Kaingang of Brazil had any group marriages at all.

A child marriage is a marriage where one or both spouses are under the age of 18. It is related to child betrothal and teenage pregnancy.

Child marriage was common throughout history, even up until the 1900s in the United States, where in 1880 CE, in the state of Delaware, the age of consent for marriage was 7 years old. Still, in 2017, over half of the 50 United States have no explicit minimum age to marry and several states set the age as low as 14. Today it is condemned by international human rights organizations. Child marriages are often arranged between the families of the future bride and groom, sometimes as soon as the girl is born. However, in the late 1800s in England and the United States, feminist activists began calling for raised age of consent laws, which was eventually handled in the 1920s, having been raised to 16-18.

Child marriages can also occur in the context of bride kidnapping.

In the year 1552 CE, John Somerford and Jane Somerford Brereton were both married at the ages of 3 and 2, respectively. Twelve years later, in 1564, John filed for divorce.

While child marriage is observed for both boys and girls, the overwhelming majority of child spouses are girls. In many cases, only one marriage-partner is a child, usually the female, due to the importance placed upon female virginity. Causes of child marriage include poverty, bride price, dowry, laws that allow child marriages, religious and social pressures, regional customs, fear of remaining unmarried, and perceived inability of women to work for money.

Today, child marriages are widespread in parts of the world; being most common in South Asia and sub-Saharan Africa, with more than half of the girls in some countries in those regions being married before 18. The incidence of child marriage has been falling in most parts of the world. In developed countries child marriage is outlawed or restricted.

Girls who marry before 18 are at greater risk of becoming victims of domestic violence, than those who marry later, especially when they are married to a much older man.

Several kinds of same-sex marriages have been documented in Indigenous and lineage-based cultures. In the Americas, We'wha (Zuni), was a "lhamana" (male individuals who, at least some of the time, dress and live in the roles usually filled by women in that culture); a respected artist, We'wha served as an emissary of the Zuni to Washington, where he met President Grover Cleveland. We'wha had at least one husband who was generally recognized as such.

While it is a relatively new practice to grant same-sex couples the same form of legal marital recognition as commonly granted to mixed-sex couples, there is some history of recorded same-sex unions around the world. Ancient Greek same-sex relationships were like modern companionate marriages, unlike their different-sex marriages in which the spouses had few emotional ties, and the husband had freedom to engage in outside sexual liaisons. The Codex Theodosianus ("C. Th." 9.7.3) issued in 438 CE imposed severe penalties or death on same-sex relationships, but the exact intent of the law and its relation to social practice is unclear, as only a few examples of same-sex relationships in that culture exist. Same-sex unions were celebrated in some regions of China, such as Fujian. Possibly the earliest documented same-sex wedding in Latin Christendom occurred in Rome, Italy, at the San Giovanni a Porta Latina basilica in 1581.

Several cultures have practiced temporary and conditional marriages. Examples include the Celtic practice of handfasting and fixed-term marriages in the Muslim community. Pre-Islamic Arabs practiced a form of temporary marriage that carries on today in the practice of Nikah mut‘ah, a fixed-term marriage contract. The Islamic prophet Muhammad sanctioned a temporary marriage – sigheh in Iran and muta'a in Iraq – which can provide a legitimizing cover for sex workers. The same forms of temporary marriage have been used in Egypt, Lebanon and Iran to make the donation of a human ova legal for in vitro fertilisation; a woman cannot, however, use this kind of marriage to obtain a sperm donation. Muslim controversies related to Nikah Mut'ah have resulted in the practice being confined mostly to Shi'ite communities. The matrilineal Mosuo of China practice what they call "walking marriage".

In some jurisdictions cohabitation, in certain circumstances, may constitute a common-law marriage, an unregistered partnership, or otherwise provide the unmarried partners with various rights and responsibilities; and in some countries the laws recognize cohabitation in lieu of institutional marriage for taxation and social security benefits. This is the case, for example, in Australia. Cohabitation may be an option pursued as a form of resistance to traditional institutionalized marriage. However, in this context, some nations reserve the right to define the relationship as marital, or otherwise to regulate the relation, even if the relation has not been registered with the state or a religious institution.

Conversely, institutionalized marriages may not involve cohabitation. In some cases couples living together do not wish to be recognized as married. This may occur because pension or alimony rights are adversely affected; because of taxation considerations; because of immigration issues, or for other reasons. Such marriages have also been increasingly common in Beijing. Guo Jianmei, director of the center for women's studies at Beijing University, told a Newsday correspondent, "Walking marriages reflect sweeping changes in Chinese society." A "walking marriage" refers to a type of temporary marriage formed by the Mosuo of China, in which male partners live elsewhere and make nightly visits. A similar arrangement in Saudi Arabia, called misyar marriage, also involves the husband and wife living separately but meeting regularly.

There is wide cross-cultural variation in the social rules governing the selection of a partner for marriage. There is variation in the degree to which partner selection is an individual decision by the partners or a collective decision by the partners' kin groups, and there is variation in the rules regulating which partners are valid choices.

The United Nations World Fertility Report of 2003 reports that 89% of all people get married before age forty-nine. The percent of women and men who marry before age forty-nine drops to nearly 50% in some nations and reaches near 100% in other nations.

In other cultures with less strict rules governing the groups from which a partner can be chosen the selection of a marriage partner may involve either the couple going through a selection process of courtship or the marriage may be arranged by the couple's parents or an outside party, a matchmaker.

Some people want to marry a person with higher or lower status than them. Others want to marry people who have similar status. In many societies women marry men who are of higher social status. There are marriages where each party has sought a partner of similar status. There are other marriages in which the man is older than the woman.

Societies have often placed restrictions on marriage to relatives, though the degree of prohibited relationship varies widely. Marriages between parents and children, or between full siblings, with few exceptions, have been considered incest and forbidden. However, marriages between more distant relatives have been much more common, with one estimate being that 80% of all marriages in history have been between second cousins or closer. This proportion has fallen dramatically, but still more than 10% of all marriages are believed to be between people who are second cousins or more closely related. In the United States, such marriages are now highly stigmatized, and laws ban most or all first-cousin marriage in 30 states. Specifics vary: in South Korea, historically it was illegal to marry someone with the same last name and same ancestral line.

An Avunculate marriage is a marriage that occurs between an uncle and his niece or between an aunt and her nephew. Such marriages are illegal in most countries due to incest restrictions. However, a small number of countries have legalized it, including Argentina, Australia, Austria, Malaysia, and Russia.
In various societies the choice of partner is often limited to suitable persons from specific social groups. In some societies the rule is that a partner is selected from an individual's own social group – endogamy, this is often the case in class- and caste-based societies. But in other societies a partner must be chosen from a different group than one's own – exogamy, this may be the case in societies practicing totemic religion where society is divided into several exogamous totemic clans, such as most Aboriginal Australian societies. In other societies a person is expected to marry their cross-cousin, a woman must marry her father's sister's son and a man must marry his mother's brother's daughter – this is often the case if either a society has a rule of tracing kinship exclusively through patrilineal or matrilineal descent groups as among the Akan people of West Africa. Another kind of marriage selection is the levirate marriage in which widows are obligated to marry their husband's brother, mostly found in societies where kinship is based on endogamous clan groups.

Religion has commonly weighed in on the matter of which relatives, if any, are allowed to marry. Relations may be by consanguinity or affinity, meaning by blood or by marriage. On the marriage of cousins, Catholic policy has evolved from initial acceptance, through a long period of general prohibition, to the contemporary requirement for a dispensation. Islam has always allowed it, while Hindu texts vary widely.

In a wide array of lineage-based societies with a classificatory kinship system, potential spouses are sought from a specific class of relative as determined by a prescriptive marriage rule. This rule may be expressed by anthropologists using a "descriptive" kinship term, such as a "man's mother's brother's daughter" (also known as a "cross-cousin"). Such descriptive rules mask the participant's perspective: a man should marry a woman from his mother's lineage. Within the society's kinship terminology, such relatives are usually indicated by a specific term which sets them apart as potentially marriageable. Pierre Bourdieu notes, however, that very few marriages ever follow the rule, and that when they do so, it is for "practical kinship" reasons such as the preservation of family property, rather than the "official kinship" ideology.

Insofar as regular marriages following prescriptive rules occur, lineages are linked together in fixed relationships; these ties between lineages may form political alliances in kinship dominated societies. French structural anthropologist Claude Lévi-Strauss developed alliance theory to account for the "elementary" kinship structures created by the limited number of prescriptive marriage rules possible.

A pragmatic (or 'arranged') marriage is made easier by formal procedures of family or group politics. A responsible authority sets up or encourages the marriage; they may, indeed, engage a professional matchmaker to find a suitable spouse for an unmarried person. The authority figure could be parents, family, a religious official, or a group consensus. In some cases, the authority figure may choose a match for purposes other than marital harmony.

A forced marriage is a marriage in which one or both of the parties is married against their will. Forced marriages continue to be practiced in parts of the world, especially in South Asia and Africa. The line between forced marriage and consensual marriage may become blurred, because the social norms of these cultures dictate that one should never oppose the desire of one's parents/relatives in regard to the choice of a spouse; in such cultures it is not necessary for violence, threats, intimidation etc. to occur, the person simply "consents" to the marriage even if they don't want it, out of the implied social pressure and duty. The customs of bride price and dowry, that exist in parts of the world, can lead to buying and selling people into marriage.

In some societies, ranging from Central Asia to the Caucasus to Africa, the custom of bride kidnapping still exists, in which a woman is captured by a man and his friends. Sometimes this covers an elopement, but sometimes it depends on sexual violence. In previous times, "raptio" was a larger-scale version of this, with groups of women captured by groups of men, sometimes in war; the most famous example is The Rape of the Sabine Women, which provided the first citizens of Rome with their wives.

Other marriage partners are more or less imposed on an individual. For example, widow inheritance provides a widow with another man from her late husband's brothers.

In rural areas of India, child marriage is practiced, with parents often arranging the wedding, sometimes even before the child is born. This practice was made illegal under the Child Marriage Restraint Act of 1929.

The financial aspects of marriage vary between cultures and have changed over time.

In some cultures, dowries and bridewealth continue to be required today. In both cases, the financial arrangements are usually made between the groom (or his family) and the bride's family; with the bride often not being involved in the negotiations, and often not having a choice in whether to participate in the marriage.

In Early modern Britain, the social status of the couple was supposed to be equal. After the marriage, all the property (called "fortune") and expected inheritances of the wife belonged to the husband.

A dowry is "a process whereby parental property is distributed to a daughter at her marriage (i.e. "inter vivos") rather than at the holder's death ("mortis causa")… A dowry establishes some variety of conjugal fund, the nature of which may vary widely. This fund ensures her support (or endowment) in widowhood and eventually goes to provide for her sons and daughters."

In some cultures, especially in countries such as Turkey, India, Bangladesh, Pakistan, Sri Lanka, Morocco, Nepal, dowries continue to be expected. In India, thousands of dowry-related deaths have taken place on yearly basis, to counter this problem, several jurisdictions have enacted laws restricting or banning dowry (see Dowry law in India). In Nepal, dowry was made illegal in 2009. Some authors believe that the giving and receiving of dowry reflects the status and even the effort to climb high in social hierarchy.

Direct Dowry contrasts with bridewealth, which is paid by the groom or his family to the bride's parents, and with indirect dowry (or dower), which is property given to the bride herself by the groom at the time of marriage and which remains under her ownership and control.

In the Jewish tradition, the rabbis in ancient times insisted on the marriage couple entering into a prenuptial agreement, called a "ketubah". Besides other things, the "ketubah" provided for an amount to be paid by the husband in the event of a divorce or his estate in the event of his death. This amount was a replacement of the biblical dower or bride price, which was payable at the time of the marriage by the groom to the father of the bride. This innovation was put in place because the biblical bride price created a major social problem: many young prospective husbands could not raise the bride price at the time when they would normally be expected to marry. So, to enable these young men to marry, the rabbis, in effect, delayed the time that the amount would be payable, when they would be more likely to have the sum. It may also be noted that both the dower and the "ketubah" amounts served the same purpose: the protection for the wife should her support cease, either by death or divorce. The only difference between the two systems was the timing of the payment. It is the predecessor to the wife's present-day entitlement to maintenance in the event of the breakup of marriage, and family maintenance in the event of the husband not providing adequately for the wife in his will. Another function performed by the "ketubah" amount was to provide a disincentive for the husband contemplating divorcing his wife: he would need to have the amount to be able to pay to the wife.

Morning gifts, which might also be arranged by the bride's father rather than the bride, are given to the bride herself; the name derives from the Germanic tribal custom of giving them the morning after the wedding night. She might have control of this morning gift during the lifetime of her husband, but is entitled to it when widowed. If the amount of her inheritance is settled by law rather than agreement, it may be called dower. Depending on legal systems and the exact arrangement, she may not be entitled to dispose of it after her death, and may lose the property if she remarries. Morning gifts were preserved for centuries in morganatic marriage, a union where the wife's inferior social status was held to prohibit her children from inheriting a noble's titles or estates. In this case, the morning gift would support the wife and children. Another legal provision for widowhood was jointure, in which property, often land, would be held in joint tenancy, so that it would automatically go to the widow on her husband's death.

Islamic tradition has similar practices. A 'mahr', either immediate or deferred, is the woman's portion of the groom's wealth (divorce) or estate (death). These amounts are usually set on the basis of the groom's own and family wealth and incomes, but in some parts these are set very high so as to provide a disincentive for the groom exercising the divorce, or the husband's family 'inheriting' a large portion of the estate, especially if there are no male offspring from the marriage. In some countries, including Iran, the mahr or alimony can amount to more than a man can ever hope to earn, sometimes up to US$1,000,000 (4000 official Iranian gold coins). If the husband cannot pay the mahr, either in case of a divorce or on demand, according to the current laws in Iran, he will have to pay it by installments. Failure to pay the mahr might even lead to imprisonment.

Bridewealth is a common practice in parts of Southeast Asia (Thailand, Cambodia), parts of Central Asia, and in much of sub-Saharan Africa. It is also known as brideprice although this has fallen in disfavor as it implies the purchase of the bride. Bridewealth is the amount of money or property or wealth paid by the groom or his family to the parents of a woman upon the marriage of their daughter to the groom. In anthropological literature, bride price has often been explained as payment made to compensate the bride's family for the loss of her labor and fertility. In some cases, bridewealth is a means by which the groom's family's ties to the children of the union are recognized.

In some countries a married person or couple benefits from various taxation advantages not available to a single person. For example, spouses may be allowed to average their combined incomes. This is advantageous to a married couple with disparate incomes. To compensate for this, countries may provide a higher tax bracket for the averaged income of a married couple. While income averaging might still benefit a married couple with a stay-at-home spouse, such averaging would cause a married couple with roughly equal personal incomes to pay more total tax than they would as two single persons. In the United States, this is called the marriage penalty.

When the rates applied by the tax code are not based income averaging, but rather on the "sum" of individuals' incomes, higher rates will usually apply to each individual in a two-earner households in a progressive tax systems. This is most often the case with high-income taxpayers and is another situation called a marriage penalty.

Conversely, when progressive tax is levied on the individual with no consideration for the partnership, dual-income couples fare much better than single-income couples with similar household incomes. The effect can be increased when the welfare system treats the same income as a shared income thereby denying welfare access to the non-earning spouse. Such systems apply in Australia and Canada, for example.

In many Western cultures, marriage usually leads to the formation of a new household comprising the married couple, with the married couple living together in the same home, often sharing the same bed, but in some other cultures this is not the tradition. Among the Minangkabau of West Sumatra, residency after marriage is matrilocal, with the husband moving into the household of his wife's mother. Residency after marriage can also be patrilocal or avunculocal. In these cases, married couples may not form an independent household, but remain part of an extended family household.

Early theories explaining the determinants of postmarital residence connected it with the sexual division of labor. However, to date, cross-cultural tests of this hypothesis using worldwide samples have failed to find any significant relationship between these two variables. However, Korotayev's tests show that the female contribution to subsistence does correlate significantly with matrilocal residence in general. However, this correlation is masked by a general polygyny factor.

Although, in different-sex marriages, an increase in the female contribution to subsistence tends to lead to matrilocal residence, it also tends simultaneously to lead to general non-sororal polygyny which effectively destroys matrilocality. If this polygyny factor is controlled (e.g., through a multiple regression model), division of labor turns out to be a significant predictor of postmarital residence. Thus, Murdock's hypotheses regarding the relationships between the sexual division of labor and postmarital residence were basically correct, though the actual relationships between those two groups of variables are more complicated than he expected.

There has been a trend toward the neolocal residence in western societies.

Marriage laws refer to the legal requirements which determine the validity of a marriage, which vary considerably between countries.

Article 16 of the Universal Declaration of Human Rights declares that "Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. Marriage shall be entered into only with the free and full consent of the intending spouses."

A marriage bestows rights and obligations on the married parties, and sometimes on relatives as well, being the sole mechanism for the creation of affinal ties (in-laws). These may include, depending on jurisdiction:

These rights and obligations vary considerably between societies, and between groups within society. These might include arranged marriages, family obligations, the legal establishment of a nuclear family unit, the legal protection of children and public declaration of commitment.

In many countries today, each marriage partner has the choice of keeping his or her property separate or combining properties. In the latter case, called community property, when the marriage ends by divorce each owns half. In lieu of a will or trust, property owned by the deceased generally is inherited by the surviving spouse.

In some legal systems, the partners in a marriage are "jointly liable" for the debts of the marriage. This has a basis in a traditional legal notion called the "Doctrine of Necessities" whereby, in a heterosexual marriage, a husband was responsible to provide necessary things for his wife. Where this is the case, one partner may be sued to collect a debt for which they did not expressly contract. Critics of this practice note that debt collection agencies can abuse this by claiming an unreasonably wide range of debts to be expenses of the marriage. The cost of defense and the burden of proof is then placed on the non-contracting party to prove that the expense is not a debt of the family. The respective maintenance obligations, both during and eventually after a marriage, are regulated in most jurisdictions; alimony is one such method.

Marriage is an institution that is historically filled with restrictions. From age, to race, to social status, to consanguinity, to gender, restrictions are placed on marriage by society for reasons of benefiting the children, passing on healthy genes, maintaining cultural values, or because of prejudice and fear. Almost all cultures that recognize marriage also recognize adultery as a violation of the terms of marriage.

Most jurisdictions set a minimum age for marriage, that is, a person must attain a certain age to be legally allowed to marry. This age may depend on circumstances, for instance exceptions from the general rule may be permitted if the parents of a young person express their consent and/or if a court decides that said marriage is in the best interest of the young person (often this applies in cases where a girl is pregnant). Although most age restrictions are in place in order to prevent children from being forced into marriages, especially to much older partners – marriages which can have negative education and health related consequences, and lead to child sexual abuse and other forms of violence – such child marriages remain common in parts of the world. According to the UN, child marriages are most common in rural sub-Saharan Africa and South Asia. The ten countries with the highest rates of child marriage are: Niger (75%), Chad, Central African Republic, Bangladesh, Guinea, Mozambique, Mali, Burkina Faso, South Sudan, and Malawi.

To prohibit incest and eugenic reasons, marriage laws have set restrictions for relatives to marry. Direct blood relatives are usually prohibited to marry, while for branch line relatives, laws are wary.

Laws banning "race-mixing" were enforced in certain North American jurisdictions from 1691 until 1967, in Nazi Germany (The Nuremberg Laws) from 1935 until 1945, and in South Africa during most part of the Apartheid era (1949–1985). All these laws primarily banned marriage between persons of different racially or ethnically defined groups, which was termed "amalgamation" or "miscegenation" in the U.S. The laws in Nazi Germany and many of the U.S. states, as well as South Africa, also banned sexual relations between such individuals.

In the United States, laws in some but not all of the states prohibited the marriage of whites and blacks, and in many states also the intermarriage of whites with Native Americans or Asians. In the U.S., such laws were known as anti-miscegenation laws. From 1913 until 1948, 30 out of the then 48 states enforced such laws. Although an "Anti-Miscegenation Amendment" to the United States Constitution was proposed in 1871, in 1912–1913, and in 1928, no nationwide law against racially mixed marriages was ever enacted. In 1967, the Supreme Court of the United States unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws are unconstitutional. With this ruling, these laws were no longer in effect in the remaining 16 states that still had them.

The Nazi ban on interracial marriage and interracial sex was enacted in September 1935 as part of the Nuremberg Laws, the "Gesetz zum Schutze des deutschen Blutes und der deutschen Ehre" (The Law for the Protection of German Blood and German Honour). The Nuremberg Laws classified Jews as a race and forbade marriage and extramarital sexual relations at first with people of Jewish descent, but was later ended to the "Gypsies, Negroes or their bastard offspring" and people of "German or related blood". Such relations were marked as "Rassenschande" (lit. "race-disgrace") and could be punished by imprisonment (usually followed by deportation to a concentration camp) and even by death.

South Africa under apartheid also banned interracial marriage. The Prohibition of Mixed Marriages Act, 1949 prohibited marriage between persons of different races, and the Immorality Act of 1950 made sexual relations with a person of a different race a crime.

As of 2019, same-sex marriage is performed and recognized by law (nationwide or in some parts) in the following countries: Argentina, Australia, Austria, Belgium, Brazil, Canada, Colombia, Denmark, Finland, France, Germany, Iceland, Ireland, Luxembourg, Malta, Mexico, the Netherlands, New Zealand, Norway, Portugal, South Africa, Spain, Sweden, Taiwan, the United Kingdom, the United States, and Uruguay. Additionally, Armenia, Estonia and Israel recognize the marriages of same-sex couples validly entered into in other countries. Same-sex marriage is also due to soon become performed and recognized by law in Costa Rica. Furthermore, the Inter-American Court of Human Rights has issued a ruling which is expected to facilitate recognition in several countries in the Americas.

The introduction of same-sex marriage has varied by jurisdiction, being variously accomplished through legislative change to marriage law, a court ruling based on constitutional guarantees of equality, or by direct popular vote (via ballot initiative or referendum). The recognition of same-sex marriage is considered to be a human right and a civil right as well as a political, social, and religious issue. The most prominent supporters of same-sex marriage are human rights and civil rights organizations as well as the medical and scientific communities, while the most prominent opponents are religious groups. Various faith communities around the world support same-sex marriage, while many religious groups oppose it. Polls consistently show continually rising support for the recognition of same-sex marriage in all developed democracies and in some developing democracies.

The establishment of recognition in law for the marriages of same-sex couples is one of the most prominent objectives of the LGBT rights movement.

Polygyny is widely practiced in mostly Muslim and African countries. In the Middle Eastern region, Israel, Turkey and Tunisia are notable exceptions.

In most other jurisdictions, polygamy is illegal. For example, In the United States, polygamy is illegal in all 50 states.

In the late-19th century, citizens of the self-governing territory of what is present-day Utah were forced by the United States federal government to abandon the practice of polygamy through the vigorous enforcement of several Acts of Congress, and eventually complied. The Church of Jesus Christ of Latter-day Saints formally abolished the practice in 1890, in a document labeled 'The Manifesto' (see Latter Day Saint polygamy in the late-19th century). Among American Muslims, a small minority of around 50,000 to 100,000 people are estimated to live in families with a husband maintaining an illegal polygamous relationship.

Several countries such as India and Sri Lanka, permit only their Islamic citizens to practice polygamy. Some Indians have converted to Islam in order to bypass such legal restrictions. Predominantly Christian nations usually do not allow polygamous unions, with a handful of exceptions being the Republic of the Congo, Uganda, and Zambia. Myanmar (frequently referred to as Burma) is also the only predominantly Buddhist nation to allow for civil polygynous marriages, though such is rarely tolerated by the Burmese population.

In various jurisdictions, a civil marriage may take place as part of the religious marriage ceremony, although they are theoretically distinct. Some jurisdictions allow civil marriages in circumstances which are notably not allowed by particular religions, such as same-sex marriages or civil unions.

The opposite case may happen as well. Partners may not have full juridical acting capacity and churches may have less strict limits than the civil jurisdictions. This particularly applies to minimum age, or physical infirmities.

It is possible for two people to be recognised as married by a religious or other institution, but not by the state, and hence without the legal rights and obligations of marriage; or to have a civil marriage deemed invalid and sinful by a religion. Similarly, a couple may remain married in religious eyes after a civil divorce.

A marriage is usually formalized at a wedding or marriage ceremony. The ceremony may be officiated either by a religious official, by a government official or by a state approved celebrant. In various European and some Latin American countries, any religious ceremony must be held separately from the required civil ceremony. Some countries – such as Belgium, Bulgaria, France, the Netherlands, Romania and Turkey – require that a civil ceremony take place before any religious one. In some countries – notably the United States, Canada, the United Kingdom, the Republic of Ireland, Norway and Spain – both ceremonies can be held together; the officiant at the religious and civil ceremony also serving as agent of the state to perform the civil ceremony. To avoid any implication that the state is "recognizing" a religious marriage (which is prohibited in some countries) – the "civil" ceremony is said to be taking place at the same time as the religious ceremony. Often this involves simply signing a register during the religious ceremony. If the civil element of the religious ceremony is omitted, the marriage ceremony is not recognized as a marriage by government under the law.

Some countries, such as Australia, permit marriages to be held in private and at any location; others, including England and Wales, require that the civil ceremony be conducted in a place open to the public and specially sanctioned by law for the purpose. In England, the place of marriage formerly had to be a church or register office, but this was extended to any public venue with the necessary licence. An exception can be made in the case of marriage by special emergency license (UK: licence), which is normally granted only when one of the parties is terminally ill. Rules about where and when persons can marry vary from place to place. Some regulations require one of the parties to reside within the jurisdiction of the register office (formerly parish).

Each religious authority has rules for the manner in which marriages are to be conducted by their officials and members. Where religious marriages are recognised by the state, the officiator must also conform with the law of the jurisdiction.

In a small number of jurisdictions marriage relationships may be created by the operation of the law alone. Unlike the typical ceremonial marriage with legal contract, wedding ceremony, and other details, a common-law marriage may be called "marriage by habit and repute (cohabitation)." A de facto common-law marriage without a license or ceremony is legally binding in some jurisdictions but has no legal consequence in others.

A "civil union", also referred to as a "civil partnership", is a legally recognized form of partnership similar to marriage. Beginning with Denmark in 1989, civil unions under one name or another have been established by law in several countries in order to provide same-sex couples rights, benefits, and responsibilities similar (in some countries, identical) to opposite-sex civil marriage. In some jurisdictions, such as Brazil, New Zealand, Uruguay, Ecuador, France and the U.S. states of Hawaii and Illinois, civil unions are also open to opposite-sex couples.

Sometimes people marry to take advantage of a certain situation, sometimes called a marriage of convenience or a sham marriage. For example, according to one publisher of information about green card marriages, "Every year over 450,000 United States citizens marry foreign-born individuals and petition for them to obtain a permanent residency (Green Card) in the United States." While this is likely an overestimate, in 2003 alone 184,741 immigrants were admitted to the U.S. as spouses of U.S. citizens. More were admitted as fiancés of US citizens for the purpose of being married within 90 days. Regardless of the number of people entering the US to marry a US citizen, it does not indicate the number of these marriages that are convenience marriages, which number could include some of those with the motive of obtaining permanent residency, but also include people who are US citizens. One example would be to obtain an inheritance that has a marriage clause. Another example would be to save money on health insurance or to enter a health plan with preexisting conditions offered by the new spouse's employer. Other situations exist, and, in fact, all marriages have a complex combination of conveniences motivating the parties to marry. A marriage of convenience is one that is devoid of normal reasons to marry. In certain countries like Singapore sham marriages like these are punishable criminal offences.

People have proposed arguments against marriage for reasons that include political, philosophical and religious criticisms; concerns about the divorce rate; individual liberty and gender equality; questioning the necessity of having a personal relationship sanctioned by government or religious authorities; or the promotion of celibacy for religious or philosophical reasons.

Feminist theory approaches opposite-sex marriage as an institution traditionally rooted in patriarchy that promotes male superiority and power over women. This power dynamic conceptualizes men as "the provider operating in the public sphere" and women as "the caregivers operating within the private sphere". "Theoretically, women ... [were] defined as the property of their husbands ... The adultery of a woman was always treated with more severity than that of a man." "[F]eminist demands for a wife's control over her own property were not met [in parts of Britain] until ... [laws were passed in the late 19th century]."

Traditional heterosexual marriage imposed an obligation of the wife to be sexually available for her husband and an obligation of the husband to provide material/financial support for the wife. Numerous philosophers, feminists and other academic figures have commented on this throughout history, condemning the hypocrisy of legal and religious authorities in regard to sexual issues; pointing to the lack of choice of a woman in regard to controlling her own sexuality; and drawing parallels between marriage, an institution promoted as sacred, and prostitution, widely condemned and vilified (though often tolerated as a "necessary evil"). Mary Wollstonecraft, in the 18th century, described marriage as "legal prostitution". Emma Goldman wrote in 1910: "To the moralist prostitution does not consist so much in the fact that the woman sells her body, but rather that she sells it out of wedlock". Bertrand Russell in his book Marriage and Morals wrote that: "Marriage is for woman the commonest mode of livelihood, and the total amount of undesired sex endured by women is probably greater in marriage than in prostitution." Angela Carter in Nights at the Circus wrote: "What is marriage but prostitution to one man instead of many?"

Some critics object to what they see as propaganda in relation to marriage – from the government, religious organizations, the media – which aggressively promote marriage as a solution for all social problems; such propaganda includes, for instance, marriage promotion in schools, where children, especially girls, are bombarded with positive information about marriage, being presented only with the information prepared by authorities.

The performance of dominant gender roles by men and submissive gender roles by women influence the power dynamic of a heterosexual marriage. In some American households, women internalize gender role stereotypes and often assimilate into the role of "wife", "mother", and "caretaker" in conformity to societal norms and their male partner. Author bell hooks states "within the family structure, individuals learn to accept sexist oppression as 'natural' and are primed to support other forms of oppression, including heterosexist domination." "[T]he cultural, economic, political and legal supremacy of the husband" was "[t]raditional ... under English law". This patriarchal dynamic is contrasted with a conception of egalitarian or Peer Marriage in which power and labour are divided equally, and not according to gender roles.

In the US, studies have shown that, despite egalitarian ideals being common, less than half of respondents viewed their opposite-sex relationships as equal in power, with unequal relationships being more commonly dominated by the male partner. Studies also show that married couples find the highest level of satisfaction in egalitarian relationships and lowest levels of satisfaction in wife dominate relationships. In recent years, egalitarian or Peer Marriages have been receiving increasing focus and attention politically, economically and culturally in a number of countries, including the United States.

Different societies demonstrate variable tolerance of extramarital sex. The Standard Cross-Cultural Sample describes the occurrence of extramarital sex by gender in over 50 pre-industrial cultures. The occurrence of extramarital sex by men is described as "universal" in 6 cultures, "moderate" in 29 cultures, "occasional" in 6 cultures, and "uncommon" in 10 cultures. The occurrence of extramarital sex by women is described as "universal" in 6 cultures, "moderate" in 23 cultures, "occasional" in 9 cultures, and "uncommon" in 15 cultures. Three studies using nationally representative samples in the United States found that between 10–15% of women and 20–25% of men engage in extramarital sex.

Many of the world's major religions look with disfavor on sexual relations outside marriage. There are non-secular states that sanction criminal penalties for sexual intercourse before marriage. Sexual relations by a married person with someone other than his/her spouse is known as adultery. Adultery is considered in many jurisdictions to be a crime and grounds for divorce.

In some countries, such as Saudi Arabia, Pakistan, Afghanistan, Iran, Kuwait, Maldives, Morocco, Oman, Mauritania, United Arab Emirates, Sudan, Yemen, any form of sexual activity outside marriage is illegal.

In some parts of the world, women and girls accused of having sexual relations outside marriage are at risk of becoming victims of honor killings committed by their families. In 2011 several people were sentenced to death by stoning after being accused of adultery in Iran, Somalia, Afghanistan, Sudan, Mali and Pakistan. Practices such as honor killings and stoning continue to be supported by mainstream politicians and other officials in some countries. In Pakistan, after the 2008 Balochistan honour killings in which five women were killed by tribesmen of the Umrani Tribe of Balochistan, Pakistani Federal Minister for Postal Services Israr Ullah Zehri defended the practice; he said: "These are centuries-old traditions, and I will continue to defend them. Only those who indulge in immoral acts should be afraid."

An issue that is a serious concern regarding marriage and which has been the object of international scrutiny is that of sexual violence within marriage. Throughout much of the history, in most cultures, sex in marriage was considered a 'right', that could be taken by force (often by a man from a woman), if 'denied'. As the concept of human rights started to develop in the 20th century, and with the arrival of second-wave feminism, such views have become less widely held.

The legal and social concept of marital rape has developed in most industrialized countries in the mid- to late 20th century; in many other parts of the world it is not recognized as a form of abuse, socially or legally. Several countries in Eastern Europe and Scandinavia made marital rape illegal before 1970, and other countries in Western Europe and the English-speaking Western world outlawed it in the 1980s and 1990s. In England and Wales, marital rape was made illegal in 1991. Although marital rape is being increasingly criminalized in developing countries too, cultural, religious, and traditional ideologies about "conjugal rights" remain very strong in many parts of the world; and even in many countries that have adequate laws against rape in marriage these laws are rarely enforced.

Apart from the issue of rape committed against one's spouse, marriage is, in many parts of the world, closely connected with other forms of sexual violence: in some places, like Morocco, unmarried girls and women who are raped are often forced by their families to marry their rapist. Because being the victim of rape and losing virginity carry extreme social stigma, and the victims are deemed to have their "reputation" tarnished, a marriage with the rapist is arranged. This is claimed to be in the advantage of both the victim – who does not remain unmarried and doesn't lose social status – and of the rapist, who avoids punishment. In 2012, after a Moroccan 16-year-old girl committed suicide after having been forced by her family to marry her rapist and enduring further abuse by the rapist after they married, there have been protests from activists against this practice which is common in Morocco.

In some societies, the very high social and religious importance of marital fidelity, especially female fidelity, has as result the criminalization of adultery, often with harsh penalties such as stoning or flogging; as well as leniency towards punishment of violence related to infidelity (such as honor killings). In the 21st century, criminal laws against adultery have become controversial with international organizations calling for their abolition. Opponents of adultery laws argue that these laws are a major contributor to discrimination and violence against women, as they are enforced selectively mostly against women; that they prevent women from reporting sexual violence; and that they maintain social norms which justify violent crimes committed against women by husbands, families and communities. A Joint Statement by the United Nations Working Group on discrimination against women in law and in practice states that "Adultery as a criminal offence violates women's human rights". Some human rights organizations argue that the criminalization of adultery also violates internationally recognized protections for private life, as it represents an arbitrary interference with an individual's privacy, which is not permitted under international law.

The laws surrounding heterosexual marriage in many countries have come under international scrutiny because they contradict international standards of human rights; institutionalize violence against women, child marriage and forced marriage; require the permission of a husband for his wife to work in a paid job, sign legal documents, file criminal charges against someone, sue in civil court etc.; sanction the use by husbands of violence to "discipline" their wives; and discriminate against women in divorce.

Such things were legal even in many Western countries until recently: for instance, in France, married women obtained the right to work without their husband's permission in 1965, and in West Germany women obtained this right in 1977 (by comparison women in East Germany had many more rights). In Spain, during Franco's era, a married woman needed her husband's consent, referred to as the "permiso marital", for almost all economic activities, including employment, ownership of property, and even traveling away from home; the "permiso marital" was abolished in 1975.

An absolute submission of a wife to her husband is accepted as natural in many parts of the world, for instance surveys by UNICEF have shown that the percentage of women aged 15–49 who think that a husband is justified in hitting or beating his wife under certain circumstances is as high as 90% in Afghanistan and Jordan, 87% in Mali, 86% in Guinea and Timor-Leste, 81% in Laos, 80% in Central African Republic. Detailed results from Afghanistan show that 78% of women agree with a beating if the wife "goes out without telling him [the husband]" and 76% agree "if she argues with him".

Throughout history, and still today in many countries, laws have provided for extenuating circumstances, partial or complete defenses, for men who killed their wives due to adultery, with such acts often being seen as crimes of passion and being covered by legal defenses such as provocation or defense of family honor.

While international law and conventions recognize the need for consent for entering a marriage – namely that people cannot be forced to get married against their will – the right to obtain a divorce is not recognized; therefore holding a person in a marriage against their will (if such person has consented to entering in it) is not considered a violation of human rights, with the issue of divorce being left at the appreciation of individual states. The European Court of Human Rights has repeatedly ruled that under the European Convention on Human Rights there is neither a right to apply to divorce, nor a right to obtain the divorce if applied for it; in 2017, in "Babiarz v. Poland", the Court ruled that Poland was entitled to deny a divorce because the grounds for divorce were not met, even if the marriage in question was acknowledged both by Polish courts and by the ECHR as being a legal fiction involving a long-term separation where the husband lived with another woman with whom he had an 11-year-old child.

In the EU, the last country to allow divorce was Malta, in 2011. Around the world, the only countries to forbid divorce are Philippines and Vatican City, although in practice in many countries which use a fault-based divorce system obtaining a divorce is very difficult. The ability to divorce, in law and practice, has been and continues to be a controversial issue in many countries, and public discourse involves different ideologies such as feminism, social conservatism, religious interpretations.

In recent years, the customs of dowry and bride price have received international criticism for inciting conflicts between families and clans; contributing to violence against women; promoting materialism; increasing property crimes (where men steal goods such as cattle in order to be able to pay the bride price); and making it difficult for poor people to marry. African women's rights campaigners advocate the abolishing of bride price, which they argue is based on the idea that women are a form of property which can be bought. Bride price has also been criticized for contributing to child trafficking as impoverished parents sell their young daughters to rich older men. A senior Papua New Guinea police officer has called for the abolishing of bride price arguing that it is one of the main reasons for the mistreatment of women in that country. The opposite practice of dowry has been linked to a high level of violence (see Dowry death) and to crimes such as extortion.

Historically, and still in many countries, children born outside marriage suffered severe social stigma and discrimination. In England and Wales, such children were known as bastards and whoresons.

There are significant differences between world regions in regard to the social and legal position of non-marital births, ranging from being fully accepted and uncontroversial to being severely stigmatized and discriminated.

The 1975 European Convention on the Legal Status of Children Born out of Wedlock protects the rights of children born to unmarried parents. The convention states, among others, that: "The father and mother of a child born out of wedlock shall have the same obligation to maintain the child as if it were born in wedlock" and that "A child born out of wedlock shall have the same right of succession in the estate of its father and its mother and of a member of its father's or mother's family, as if it had been born in wedlock."

While in most Western countries legal inequalities between children born inside and outside marriage have largely been abolished, this is not the case in some parts of the world.

The legal status of an unmarried father differs greatly from country to country. Without voluntary formal recognition of the child by the father, in most cases there is a need of due process of law in order to establish paternity. In some countries however, unmarried cohabitation of a couple for a specific period of time does create a presumption of paternity similar to that of formal marriage. This is the case in Australia. Under what circumstances can a paternity action be initiated, the rights and responsibilities of a father once paternity has been established (whether he can obtain parental responsibility and whether he can be forced to support the child) as well as the legal position of a father who voluntarily acknowledges the child, vary widely by jurisdiction. A special situation arises when a married woman has a child by a man other than her husband. Some countries, such as Israel, refuse to accept a legal challenge of paternity in such a circumstance, in order to avoid the stigmatization of the child (see Mamzer, a concept under Jewish law). In 2010, the European Court of Human Rights ruled in favor of a German man who had fathered twins with a married woman, granting him right of contact with the twins, despite the fact that the mother and her husband had forbidden him to see the children.

The steps that an unmarried father must take in order to obtain rights to his child vary by country. In some countries (such as the UK – since 2003 in England and Wales, 2006 in Scotland, and 2002 in Northern Ireland) it is sufficient for the father to be listed on the birth certificate for him to have parental rights; in other countries, such as Ireland, simply being listed on the birth certificate does not offer any rights, additional legal steps must be taken (if the mother agrees, the parents can both sign a "statutory declaration", but if the mother does not agree, the father has to apply to court).

Children born outside marriage have become more common, and in some countries, the majority. Recent data from Latin America showed figures for non-marital childbearing to be 74% for Colombia, 69% for Peru, 68% for Chile, 66% for Brazil, 58% for Argentina, 55% for Mexico. In 2012, in the European Union, 40% of births were outside marriage, and in the United States, in 2013, the figure was similar, at 41%. In the United Kingdom 48% of births were to unmarried women in 2012; in Ireland the figure was 35%.

During the first half of the 20th century, unmarried women in some Western countries were coerced by authorities to give their children up for adoption. This was especially the case in Australia, through the forced adoptions in Australia, with most of these adoptions taking place between the 1950s and the 1970s. In 2013, Julia Gillard, then Prime Minister of Australia, offered a national apology to those affected by the forced adoptions.

Some married couples choose not to have children. Others are unable to have children because of infertility or other factors preventing conception or the bearing of children. In some cultures, marriage imposes an "obligation" on women to bear children. In northern Ghana, for example, payment of bridewealth signifies a woman's requirement to bear children, and women using birth control face substantial threats of physical abuse and reprisals.

Religions develop in specific geographic and social milieux.
Unsurprisingly, religious attitudes and practices relating to marriage can vary.
The precepts of mainstream religions include, as a rule, unequivocal prescriptions for marriage, establishing both rituals and rules of conduct.

The Bahá'í Faith encourages marriage and views it as a mutually strengthening bond, but it is not obligatory. A Bahá'í marriage requires the couple to choose each other, and then obtain the consent of all living parents.

Modern Christianity bases its views on marriage upon the teachings of Jesus and the Paul the Apostle. many Christian denominations regard marriage as a sacrament, sacred institution, or covenant. However, this was not the case in the Roman Catholic Church before the 1184 Council of Verona officially recognized it as such. Before then, no specific ritual was prescribed for celebrating a marriage: "Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime." The Church only formally recognized the union and it was finalized with the couple together partaking Holy Communion.

Decrees on marriage of the Roman Catholic Council of Trent (twenty-fourth session of 1563) made the validity of marriage dependent on the wedding occurring in the presence of a priest and two witnesses. The absence of a requirement of parental consent ended a debate that proceeded from the 12th century. In the case of a civil divorce, the innocent spouse had and has no right to marry again until the death of the other spouse terminates the still valid marriage, even if the other spouse was guilty of adultery.

The Christian Church performed marriages in the narthex of the church prior to the 16th century, when the emphasis was on the marital contract and betrothal. Subsequently, the ceremony moved inside the sacristy of the church.

Christians often marry for religious reasons, ranging from following the biblical injunction for a "man to leave his father and mother and cleave to his wife, and the two shall become one", to accessing the Divine grace of the Roman Catholic Sacrament.

Catholics, Eastern Orthodox, as well as many Anglicans and Methodists, consider marriage termed "holy matrimony" to be an expression of divine grace, termed a "sacrament" and "mystery" in the first two Christian traditions. In Western ritual, the ministers of the sacrament are the spouses themselves, with a bishop, priest, or deacon merely witnessing the union on behalf of the Church and blessing it. In Eastern ritual churches, the bishop or priest functions as the actual minister of the Sacred Mystery; Eastern Orthodox deacons may not perform marriages. Western Christians commonly refer to marriage as a vocation, while Eastern Christians consider it an ordination and a martyrdom, though the theological emphases indicated by the various names are not excluded by the teachings of either tradition. Marriage is commonly celebrated in the context of a Eucharistic service (a nuptial Mass or Divine Liturgy). The sacrament of marriage is indicative of the relationship between Christ and the Church.

The Roman Catholic tradition of the 12th and 13th centuries defined marriage as a sacrament ordained by God, signifying the mystical marriage of Christ to his Church.
The matrimonial covenant, by which a man and a woman establish between themselves a partnership of the whole of life, is by its nature ordered toward the good of the spouses and the procreation and education of offspring; this covenant between baptized persons has been raised by Christ the Lord to the dignity of a sacrament.
For Catholic and Methodist Christians, the mutual love between man and wife becomes an image of the eternal love with which God loves humankind. In the United Methodist Church, the celebration of Holy Matrimony ideally occurs in the context of a Service of Worship, which includes the celebration of the Eucharist. Likewise, the celebration of marriage between two Catholics normally takes place during the public liturgical celebration of the Holy Mass, because of its sacramental connection with the unity of the Paschal mystery of Christ (Communion). Sacramental marriage confers a perpetual and exclusive bond between the spouses. By its nature, the institution of marriage and conjugal love is ordered to the procreation and upbringing of offspring. Marriage creates rights and duties in the Church between the spouses and towards their children: "[e]ntering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment". According to current Roman Catholic legislation, progeny of annulled relationships are considered legitimate. Civilly remarried persons who civilly divorced a living and lawful spouse are not separated from the Church, but they cannot receive Eucharistic Communion.

Divorce and remarriage, while generally not encouraged, are regarded differently by each Christian denomination. Most Protestant Churches allow persons to marry again after a divorce, while other require an annulment. The Eastern Orthodox Church allows divorce for a limited number of reasons, and in theory, but usually not in practice, requires that a marriage after divorce be celebrated with a penitential overtone. With respect to marriage between a Christian and a pagan, the early Church "sometimes took a more lenient view, invoking the so-called Pauline privilege of permissible separation (1 Cor. 7) as legitimate grounds for allowing a convert to divorce a pagan spouse and then marry a Christian."

The Catholic Church adheres to the proscription of Jesus in "Matthew", 19: 6 that married spouses who have consummated their marriage "are no longer two, but one flesh. Therefore, what God has joined together, no human being must separate.” Consequently, the Catholic Church understands that it is wholly without authority to terminate a sacramentally valid and consummated marriage, and its "Codex Iuris Canonici" (1983 Code of Canon Law) confirms this in Canons 1055-7. Specifically, Canon 1056 declares that "the essential properties of marriage are unity and "indissolubility"; in [C]hristian marriage they acquire a distinctive "firmness" by reason of the sacrament." Canon 1057, §2 declares that marriage is "an "irrevocable" covenant". Therefore, divorce of such a marriage is a metaphysical, moral, and legal impossibility. However, the Church has the authority to annul a presumed "marriage" by declaring it to have been invalid from the beginning, i. e., declaring it not to be and never to have been a marriage, in an annulment procedure, which is basically a fact-finding and fact-declaring effort.

For Protestant denominations, the purposes of marriage include intimate companionship, rearing children, and mutual support for both spouses to fulfill their life callings. Most Reformed Christians did not regard marriage to the status of a sacrament "because they did not regard matrimony as a necessary means of grace for salvation"; nevertheless it is considered a covenant between spouses before God. In addition, some Protestant denominations (such as the Methodist Churches) affirmed that Holy Matrimony is a "means of grace, thus, sacramental in character".
Since the 16th century, five competing models have shaped marriage in the Western tradition, as described by John Witte, Jr.:

Members of The Church of Jesus Christ of Latter-day Saints (LDS Church) believe that "marriage between a man and a woman is ordained of God and that the family is central to the Creator's plan for the eternal destiny of His children." Their view of marriage is that family relationships can endure beyond the grave. This is known as 'eternal marriage' which can be eternal only when authorized priesthood holders perform the sealing ordinance in sacred temples.

Although many Christian denominations do not currently perform same-sex marriages, many do, such as the Presbyterian Church (USA), some dioceses of the Episcopal Church, the Metropolitan Community Church, Quakers, United Church of Canada, and United Church of Christ congregations, and some Anglican dioceses, for example. Same-sex marriage is recognized by various religious denominations.

Islam also commends marriage, with the age of marriage being whenever the individuals feel ready, financially and emotionally.

In Islam, polygyny is allowed while polyandry is not, with the specific limitation that a man can have no more than four legal wives at any one time and an unlimited number of female slaves as concubines who may have rights similar wives, with the exception of not being free unless the man has children with them, with the requirement that the man is able and willing to partition his time and wealth equally among the respective wives and concubines (this practice of concubinage, as in Judaism, is not applicable in contemporary times and has been deemed by scholars as invalid due to shifts in views about the role of slavery in the world).

For a Muslim wedding to take place, the bridegroom and the guardian of the bride ("wali") must both agree on the marriage. Should the guardian disagree on the marriage, it may not legally take place. If the "wali" of the girl her father or paternal grandfather, he has the right to force her into marriage even against her proclaimed will, if it is her first marriage. A guardian who is allowed to force the bride into marriage is called "wali mujbir".

From an Islamic (Sharia) law perspective, the minimum requirements and responsibilities in a Muslim marriage are that the groom provide living expenses (housing, clothing, food, maintenance) to the bride, and in return, the bride's main responsibility is raising children to be proper Muslims. All other rights and responsibilities are to be decided between the husband and wife, and may even be included as stipulations in the marriage contract before the marriage actually takes place, so long as they do not go against the minimum requirements of the marriage.

In Sunni Islam, marriage must take place in the presence of at least two reliable witnesses, with the consent of the guardian of the bride and the consent of the groom. Following the marriage, the couple may consummate the marriage. To create an 'urf marriage, it is sufficient that a man and a woman indicate an intention to marry each other and recite the requisite words in front of a suitable Muslim. The wedding party usually follows but can be held days, or months later, whenever the couple and their families want to; however, there can be no concealment of the marriage as it is regarded as public notification due to the requirement of witnesses.

In Shia Islam, marriage may take place without the presence of witnesses as is often the case in temporary Nikah mut‘ah (prohibited in Sunni Islam), but with the consent of both the bride and the groom. Following the marriage they may consummate their marriage.

In Judaism, marriage is based on the laws of the Torah and is a contractual bond between spouses in which the spouses dedicate to be exclusive to one another. This contract is called Kiddushin. Though procreation is not the sole purpose, a Jewish marriage is also expected to fulfill the commandment to have children. The main focus centers around the relationship between the spouses. Kabbalistically, marriage is understood to mean that the spouses are merging into a single soul. This is why a man is considered "incomplete" if he is not married, as his soul is only one part of a larger whole that remains to be unified.

The Hebrew Bible (Christian Old Testament) describes a number of marriages, including those of Isaac (), Jacob () and Samson (). Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Hebrew Bible; another is that of concubinage (pilegesh) which was often arranged by a man and a woman who generally enjoyed the same rights as a full legal wife (other means of concubinage can be seen in Judges 19-20 where mass marriage by abduction was practiced as a form of punishment on transgressors). Today Ashkenazi Jews are prohibited to take more than one wife because of a ban instituted on this by Gershom ben Judah (Died 1040).

Among ancient Hebrews, marriage was a domestic affair and not a religious ceremony; the participation of a priest or rabbi was not required.

Betrothal ("erusin"), which refers to the time that this binding contract is made, is distinct from marriage itself ("nissu'in"), with the time between these events varying substantially.
In biblical times, a wife was regarded as personal property, belonging to her husband; the descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and men with more than one wife were expected to ensure that they continue to give the first wife food, clothing, and marital rights.

Since a wife was regarded as property, her husband was originally free to divorce her for any reason, at any time. Divorcing a woman against her will was also banned by Gershom ben Judah for Ashkenazi Jews. A divorced couple were permitted to get back together, unless the wife had married someone else after her divorce.

Hinduism sees marriage as a sacred duty that entails both religious and social obligations. Old Hindu literature in Sanskrit gives many different types of marriages and their categorization ranging from "Gandharva Vivaha" (instant marriage by mutual consent of participants only, without any need for even a single third person as witness) to normal (present day) marriages, to "Rakshasa Vivaha" ("demoniac" marriage, performed by abduction of one participant by the other participant, usually, but not always, with the help of other persons). In the Indian subcontinent, arranged marriages, the spouse's parents or an older family member choose the partner, are still predominant in comparison with so-called love marriages until nowadays. The Hindu Widow's Remarriage Act 1856 empowers a Hindu widow to remarry.

The Buddhist view of marriage considers marriage a secular affair and thus not a sacrament. Buddhists are expected to follow the civil laws regarding marriage laid out by their respective governments. Gautama Buddha, being a kshatriya was required by Shakyan tradition to pass a series of tests to prove himself as a warrior, before he was allowed to marry.

In a Sikh marriage, the couple walks around the "Guru Granth Sahib" holy book four times, and a holy man recites from it in the kirtan style. The ceremony is known as 'Anand Karaj' and represents the holy union of two souls united as one.

Wiccan marriages are commonly known as handfastings. Although handfastings vary for each Wiccan they often involve honoring Wiccan gods. Sex is considered a pious and sacred activity.

Marriages are correlated with better outcomes for the couple and their children, including higher income for men, better health and lower mortality. Part of these effects is due to the fact that those with better expectations get married more often. According to a systematic review on research literature, a significant part of the effect seems to be due to a true causal effect. The reason may be that marriages make particularly men become more future-oriented and take an economic and other responsibility of the family. The studies eliminate the effect of selectivity in numerous ways. However, much of the research is of low quality in this sense. On the other hand, the causal effect might be even higher if money, working skills and parenting practises are endogenous. Married men have less drug abuse and alcohol use and are more often at home during nights.

Marriage, like other close relationships, exerts considerable influence on health. Married people experience lower morbidity and mortality across such diverse health threats as cancer, heart attacks, and surgery. Research on marriage and health is part of the broader study of the benefits of social relationships.

Social ties provide people with a sense of identity, purpose, belonging, and support. Simply being married, as well as the quality of one's marriage, have been linked to diverse measures of health.

The health-protective effect of marriage is stronger for men than women. Marital status—the simple fact of being married—confers more health benefits to men than women.

Women's health is more strongly impacted than men's by marital conflict or satisfaction, such that unhappily married women do not enjoy better health relative to their single counterparts. Most research on marriage and health has focused on heterosexual couples; more work is needed to clarify the health impacts of same-sex marriage.

In most societies, the death of one of the partners terminates the marriage, and in monogamous societies this allows the other partner to remarry, though sometimes after a waiting or mourning period.

In some societies, a marriage can be annulled, when an authority declares that a marriage never happened. Jurisdictions often have provisions for void marriages or voidable marriages.

A marriage may also be terminated through divorce. Countries that have relatively recently legalized divorce are Italy (1970), Portugal (1975), Brazil (1977), Spain (1981), Argentina (1987), Paraguay (1991), Colombia (1991), Ireland (1996), Chile (2004) and Malta (2011). As of 2012, the Philippines and the Vatican City are the only jurisdictions which do not allow divorce (this is currently under discussion in Philippines).)
After divorce, one spouse may have to pay alimony. Laws concerning divorce and the ease with which a divorce can be obtained vary widely around the world. After a divorce or an annulment, the people concerned are free to remarry (or marry).

A statutory right of two married partners to mutually consent to divorce was enacted in western nations in the mid-20th century. In the United States no-fault divorce was first enacted in California in 1969 and the final state to legalize it was New York in 1989.

About 45% of marriages in Britain and, according to a 2009 study, 46% of marriages in the U.S. end in divorce.

The history of marriage is often considered under History of the family or legal history.

Many cultures have legends concerning the origins of marriage. The way in which a marriage is conducted and its rules and ramifications has changed over time, as has the institution itself, depending on the culture or demographic of the time.

The first recorded evidence of marriage ceremonies uniting a man and a woman dates back to approximately 2350 BC, in ancient Mesopotamia. Wedding ceremonies, as well as dowry and divorce, can be traced back to Mesopotamia and Babylonia.
According to ancient Hebrew tradition, a wife was seen as being property of high value and was, therefore, usually, carefully looked after. Early nomadic communities in the middle east practised a form of marriage known as "beena", in which a wife would own a tent of her own, within which she retains complete independence from her husband; this principle appears to survive in parts of early Israelite society, as some early passages of the Bible appear to portray certain wives as each owning a tent as a personal possession (specifically, Jael, Sarah, and Jacob's wives).

The husband, too, is indirectly implied to have some responsibilities to his wife. The Covenant Code orders "If he take him another; her food, her clothing, and her duty of marriage, shall he not diminish(or lessen)". If the husband does not provide the first wife with these things, she is to be divorced, without cost to her. The Talmud interprets this as a requirement for a man to provide food and clothing to, and have sex with, each of his wives. However, "duty of marriage" is also interpreted as whatever one does as a married couple, which is more than just sexual activity. And the term diminish, which means to lessen, shows the man must treat her as if he was not married to another.

As a polygynous society, the Israelites did not have any laws that imposed marital fidelity on men. However, the prophet Malachi states that none should be faithless to the wife of his youth and that God hates divorce. Adulterous married women, adulterous betrothed women, and the men who slept with them however, were subject to the death penalty by the biblical laws against adultery According to the Priestly Code of the Book of Numbers, if a pregnant woman was suspected of adultery, she was to be subjected to the Ordeal of Bitter Water, a form of trial by ordeal, but one that took a miracle to convict. The literary prophets indicate that adultery was a frequent occurrence, despite their strong protests against it, and these legal strictnesses.

In ancient Greece, no specific civil ceremony was required for the creation of a heterosexual marriage – only mutual agreement and the fact that the couple must regard each other as husband and wife accordingly. Men usually married when they were in their 20s and women in their teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a teenage girl ensured ample time for her to bear children, as life expectancies were significantly lower. Married Greek women had few rights in ancient Greek society and were expected to take care of the house and children. Time was an important factor in Greek marriage. For example, there were superstitions that being married during a full moon was good luck and, according to Robert Flacelière, Greeks married in the winter. Inheritance was more important than feelings: a woman whose father dies without male heirs could be forced to marry her nearest male relative – even if she had to divorce her husband first.

There were several types of marriages in ancient Roman society. The traditional ("conventional") form called "conventio in manum" required a ceremony with witnesses and was also dissolved with a ceremony. In this type of marriage, a woman lost her family rights of inheritance of her old family and gained them with her new one. She now was subject to the authority of her husband. There was the free marriage known as "sine manu". In this arrangement, the wife remained a member of her original family; she stayed under the authority of her father, kept her family rights of inheritance with her old family and did not gain any with the new family. The minimum age of marriage for girls was 12.

Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, at least according to Tacitus:
The youths partake late of the pleasures of love, and hence pass the age of puberty unexhausted: nor are the virgins hurried into marriage; the same maturity, the same full growth is required: the sexes unite equally matched and robust; and the children inherit the vigor of their parents.
Where Aristotle had set the prime of life at 37 years for men and 18 for women, the Visigothic Code of law in the 7th century placed the prime of life at 20 years for both men and women, after which both presumably married. Tacitus states that ancient Germanic brides were on average about 20 and were roughly the same age as their husbands. Tacitus, however, had never visited the German-speaking lands and most of his information on Germania comes from secondary sources. In addition, Anglo-Saxon women, like those of other Germanic tribes, are marked as women from the age of 12 and older, based on archaeological finds, implying that the age of marriage coincided with puberty.

From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of Antioch writing around 110 to bishop Polycarp of Smyrna exhorts, "[I]t becomes both men and women who marry, to form their union with the approval of the bishop, that their marriage may be according to God, and not after their own lust."

In 12th-century Europe, women took the surname of their husbands and starting in the second half of the 16th century parental consent along with the church's consent was required for marriage.

With few local exceptions, until 1545, Christian marriages in Europe were by mutual consent, declaration of intention to marry and upon the subsequent physical union of the parties. The couple would promise verbally to each other that they would be married to each other; the presence of a priest or witnesses was not required. This promise was known as the "verbum." If freely given and made in the present tense (e.g., "I marry you"), it was unquestionably binding; if made in the future tense ("I will marry you"), it would constitute a betrothal.

In 1552 a wedding took place in Zufia, Navarre, between Diego de Zufia and Mari-Miguel following the custom as it was in the realm since the Middle Ages, but the man denounced the marriage on the grounds that its validity was conditioned to "riding" her (""si te cabalgo, lo cual dixo de bascuence (...) balvin yo baneça aren senar içateko""). The tribunal of the kingdom rejected the husband's claim, validating the wedding, but the husband appealed to the tribunal in Zaragoza, and this institution annulled the marriage. According to the Charter of Navarre, the basic union consisted of a civil marriage with no priest required and at least two witnesses, and the contract could be broken using the same formula. The Church in turn lashed out at those who got married twice or thrice in a row while their formers spouses were still alive. In 1563 the Council of Trent, twenty-fourth session, required that a valid marriage must be performed by a priest before two witnesses.

One of the functions of churches from the Middle Ages was to register marriages, which was not obligatory. There was no state involvement in marriage and personal status, with these issues being adjudicated in ecclesiastical courts. During the Middle Ages marriages were arranged, sometimes as early as birth, and these early pledges to marry were often used to ensure treaties between different royal families, nobles, and heirs of fiefdoms. The church resisted these imposed unions, and increased the number of causes for nullification of these arrangements. As Christianity spread during the Roman period and the Middle Ages, the idea of free choice in selecting marriage partners increased and spread with it.

In Medieval Western Europe, later marriage and higher rates of definitive celibacy (the so-called "European marriage pattern") helped to constrain patriarchy at its most extreme level. For example, Medieval England saw marriage age as variable depending on economic circumstances, with couples delaying marriage until the early twenties when times were bad and falling to the late teens after the Black Death, when there were labor shortages; by appearances, marriage of adolescents was not the norm in England. Where the strong influence of classical Celtic and Germanic cultures (which were not rigidly patriarchal) helped to offset the Judaeo-Roman patriarchal influence, in Eastern Europe the tradition of early and universal marriage (often in early adolescence) as well as traditional Slavic patrilocal custom led to a greatly inferior status of women at all levels of society.

The average age of marriage for most of Northwestern Europe from 1500 to 1800 was around 25 years of age; as the Church dictated that both parties had to be at least 21 years of age to marry without the consent of their parents, the bride and groom were roughly the same age, with most brides in their early twenties and most grooms two or three years older, and a substantial number of women married for the first time in their thirties and forties, particularly in urban areas, with the average age at first marriage rising and falling as circumstances dictated. In better times, more people could afford to marry earlier and thus fertility rose and conversely marriages were delayed or forgone when times were bad, thus restricting family size; after the Black Death, the greater availability of profitable jobs allowed more people to marry young and have more children, but the stabilization of the population in the 16th century meant fewer job opportunities and thus more people delaying marriages.

The age of marriage was not absolute, however, as child marriages occurred throughout the Middle Ages and later, with just some of them including:

As part of the Protestant Reformation, the role of recording marriages and setting the rules for marriage passed to the state, reflecting Martin Luther's view that marriage was a "worldly thing". By the 17th century, many of the Protestant European countries had a state involvement in marriage.

In England, under the Anglican Church, marriage by consent and cohabitation was valid until the passage of Lord Hardwicke's Act in 1753. This act instituted certain requirements for marriage, including the performance of a religious ceremony observed by witnesses.

As part of the Counter-Reformation, in 1563 the Council of Trent decreed that a Roman Catholic marriage would be recognized only if the marriage ceremony was officiated by a priest with two witnesses. The Council also authorized a Catechism, issued in 1566, which defined marriage as "The conjugal union of man and woman, contracted between two qualified persons, which obliges them to live together throughout life."

In the early modern period, John Calvin and his Protestant colleagues reformulated Christian marriage by enacting the Marriage Ordinance of Geneva, which imposed "The dual requirements of state registration and church consecration to constitute marriage" for recognition.

In England and Wales, Lord Hardwicke's Marriage Act 1753 required a formal ceremony of marriage, thereby curtailing the practice of Fleet Marriage, an irregular or a clandestine marriage. These were clandestine or irregular marriages performed at Fleet Prison, and at hundreds of other places. From the 1690s until the Marriage Act of 1753 as many as 300,000 clandestine marriages were performed at Fleet Prison alone. The Act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The Act did not apply to Jewish marriages or those of Quakers, whose marriages continued to be governed by their own customs.

In England and Wales, since 1837, civil marriages have been recognized as a legal alternative to church marriages under the Marriage Act 1836. In Germany, civil marriages were recognized in 1875. This law permitted a declaration of the marriage before an official clerk of the civil administration, when both spouses affirm their will to marry, to constitute a legally recognized valid and effective marriage, and allowed an optional private clerical marriage ceremony.

In contemporary English common law, a marriage is a voluntary contract by a man and a woman, in which by agreement they choose to become husband and wife. Edvard Westermarck proposed that "the institution of marriage has probably developed out of a primeval habit".

As of 2000, the average marriage age range was 25–44 years for men and 22–39 years for women.

The mythological origin of Chinese marriage is a story about Nüwa and Fu Xi who invented proper marriage procedures after becoming married. In ancient Chinese society, people of the same surname are supposed to consult with their family trees prior to marriage to reduce the potential risk of unintentional incest. Marrying one's maternal relatives was generally not thought of as incest. Families sometimes intermarried from one generation to another. Over time, Chinese people became more geographically mobile. Individuals remained members of their biological families. When a couple died, the husband and the wife were buried separately in the respective clan's graveyard. In a maternal marriage a male would become a son-in-law who lived in the wife's home.

The New Marriage Law of 1950 radically changed Chinese marriage traditions, enforcing monogamy, equality of men and women, and choice in marriage; arranged marriages were the most common type of marriage in China until then. Starting October 2003, it became legal to marry or divorce without authorization from the couple's work units. Although people with infectious diseases such as AIDS may now marry, marriage is still illegal for the mentally ill.




</doc>
<doc id="19731" url="https://en.wikipedia.org/wiki?curid=19731" title="Midgard">
Midgard

In Germanic cosmology, Midgard (an anglicised form of Old Norse ; Old English , Old Saxon , Old High German , and Gothic "Midjun-gards"; "middle yard") is the name for Earth (equivalent in meaning to the Greek term , "inhabited") inhabited by and known to humans in early Germanic cosmology. The Old Norse form plays a notable role in Norse cosmology.

This name occurs in Old Norse literature as . In Old Saxon "Heliand" it appears as and in Old High German poem "Muspilli" it appears as . The Gothic form is attested in the Gospel of Luke as a translation of the Greek word . The word is present in Old English epic and poetry as ; later transformed to or ("Middle-earth") in Middle English literature.

All these forms are from a Common Germanic "*midja-gardaz" ("*meddila-", "*medjan-"), a compound of "*midja-" "middle" and "*gardaz" "yard, enclosure".
In early Germanic cosmology, the term stands alongside "world" (Old English "weorold", Old Saxon "werold", Old High German "weralt", Old Frisian "warld" and Old Norse "verǫld"), from a Common Germanic compound "*wira-alđiz", the "age of men".

Midgard is a realm in Norse mythology. It is one of the Nine Worlds and the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim—the land of ice—to the north and Muspelheim—the land of fire—to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.

In Norse mythology, "Miðgarðr" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of "Manheimr", the "home of men", a word used to refer to the entire world. The gods slew the giant Ymir, the first created being, and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Ymir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.
According to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plane of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea only to rise again, fertile and green when the cycle repeats and the creation begins again.

Although most surviving instances of the word Midgard refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:

The Danish and Swedish form or , the Norwegian or , as well as the Icelandic and Faroese form , all derive from the Old Norse term.

The name "middangeard" occurs six times in the Old English epic poem "Beowulf", and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.

The concept of Midgard occurs many times in Middle English. The association with "earth" (OE "eorðe") in Middle English "middellærd", "middelerde" is by popular etymology; the continuation of "geard" "enclosure" is "yard". An early example of this transformation is from the Ormulum:

The usage of "Middle-earth" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his "The Lord of the Rings" and other fantasy works; he was originally inspired by the references to "middangeard" and "Éarendel" in the Old English poem "Crist".

"Mittilagart" is mentioned in the 9th-century Old High German "Muspilli" (v. 54) meaning "the world" as opposed to the sea and the heavens:


</doc>
<doc id="19732" url="https://en.wikipedia.org/wiki?curid=19732" title="Mage: The Ascension">
Mage: The Ascension

Mage: The Ascension is a role-playing game based in the World of Darkness, and was published by White Wolf Game Studio. The characters portrayed in the game are referred to as mages, and are capable of feats of magic. The idea of magic in "Mage" is broadly inclusive of diverse ideas about mystical practices as well as other belief systems, such as science and religion, so that most mages do not resemble typical fantasy wizards.

In 2005, White Wolf released a new version of the game, marketed as "", for the new World of Darkness series. The new game features some of the same game mechanics but uses a substantially different premise and setting.

Following the release of "", White Wolf put out a new roleplaying game every year, each set in "Vampire"'s World of Darkness and using its Storyteller rule system. The next four games were: "" (1992), "Mage: The Ascension" (1993), "" (1994) and "" (1995). "Mage" was the first World of Darkness game that Mark Rein•Hagen
was not explicitly involved with, although it featured the Order of Hermes from his "Ars Magica" as just a single tradition among many.

The basic premise of "Mage: The Ascension" is that everyone has the capacity, at some level, to shape reality. This capacity, personified as a mysterious alter ego called the Avatar, is dormant in most people, who are known as sleepers, whereas Magi (and/or their Avatars) are said to be Awakened. Because they're awakened, Magi can consciously effect changes to reality via willpower, beliefs, and specific magical techniques.

The beliefs and techniques of Magi vary enormously, and the ability to alter reality can only exist in the context of a coherent system of belief and technique, called a paradigm. A paradigm organizes a Mage's understanding of reality, how the universe works, and what things mean. It also provides the Mage with an understanding of how to change reality, through specific magical techniques. For example, an alchemical paradigm might describe the act of wood burning as the wood "releasing its essence of elemental Fire," while modern science would describe fire as "combustion resulting from a complex chemical reaction." Paradigms tend to be idiosyncratic to the individual Mage, but the vast majority belong to broad categories of paradigm, e.g., Shamanism, Medieval Sorcery, religious miracle working, and superscience.

In the Mage setting, everyday reality is governed by commonsense rules derived from the collective beliefs of sleepers. This is called the consensus. Most Magi's paradigms differ substantially from the consensus. When a mage performs an act of magic that does not seriously violate this commonsense version of reality, in game terms this is called coincidental magic. Magic that deviates wildly from consensus is called vulgar or dynamic magic. When it is performed ineptly, or is vulgar, and especially if it is vulgar and witnessed by sleepers, magic can cause Paradox, a phenomenon in which reality tries to resolve contradictions between the consensus and the Mage's efforts. Paradox is difficult to predict and almost always bad for the mage. The most common consequences of paradox include physical damage directly to the Mage's body, and paradox flaws, magical effects which can for example turn the mage's hair green, make him mute, make him incapable of leaving a certain location, and so on. In more extreme cases paradox can cause Quiet (madness that may leak into reality), Paradox Spirits (nebulous, often powerful beings which purposefully set about resolving the contradiction, usually by directly punishing the mage), or even the removal of the Mage to a paradox realm, a pocket dimension from which it may be difficult to escape.

In Mage, there is an underlying framework to reality called the Tapestry. The Tapestry is naturally divided into various sections, including the physical realm and various levels of the spirit world, or Umbra. At the most basic level, the Tapestry is composed of Quintessence, the essence of magic and what is real. Quintessence can have distinctive characteristics, called resonance, which are broken down into three categories: dynamic, static, and entropic.

In order to understand the metaphysics of the Mage setting, it is important to remember that many of the terms used to describe magic and Magi (e.g. Avatar, Quintessence, the Umbra, Paradox, Resonance, etc.) as well as the appearance, meaning, and understanding of a character's "Spheres," the areas of magic in which their character is proficient, vary depending on the Paradigm of the Mage in question, even though they are often, in the texts of the game, described from particular paradigmatic points-of-view. In-character, only a Mage's Paradigm can explain what each of these things are, what they mean, and why it's the way it is.

In the game, Mages have always existed, though there are legends of the Pure Ones who were shards of the original, divine One. Early mages cultivated their magical beliefs alone or in small groups, generally conforming to and influencing the belief systems of their societies. Obscure myths suggest that the precursors of the modern organizations of mages originally gathered in ancient Egypt. This period of historical uncertainty also saw the rise of the Nephandi in the Near East. This set the stage for what the game's history calls the Mythic Ages.

Until the late Middle Ages, mages' fortunes waxed and waned along with their native societies. Eventually, though, mages belonging to the Order of Hermes and the Messianic Voices attained great influence over European society. However, absorbed by their pursuit of occult power and esoteric knowledge, they often neglected and even abused humanity. Frequently, they were at odds with mainstream religions, envied by noble authorities and cursed by common folk.

Mages who believed in proto-scientific theories banded together under the banner of the , declaring their aim was to create a safe world with Man as its ruler. They won the support of Sleepers by developing the useful arts of manufacturing, economics, wayfaring, and medicine. They also championed many of the values that we now associate with the Renaissance. Masses of Sleepers embraced the gifts of early Technology and the Science that accompanied them. As the masses' beliefs shifted, the Consensus changed and wizards began to lose their position as their power and influence waned.

This was intentional. The Order of Reason perceived a safe world as one devoid of heretical beliefs, ungodly practices and supernatural creatures preying upon humanity. As the defenders of the common folk, they intended to replace the dominant magical groups with a society of philosopher-scientists as shepherds, protecting and guiding humanity. In response, non-scientific mages banded together to form the where mages of all the major magical paths gathered. They fought on battlefields and in universities trying to undermine as many discoveries as they could, but to no avail – technology made the march of Science unstoppable. The Traditions' power bases were crippled, their believers mainly converted, their beliefs ridiculed all around the world. Their final counteroffensives against the Order of Reason were foiled by internal dissent and treachery in their midst.

However, from the turn of the 17th century on, the goals of the Order of Reason began to change. As their scientific paradigm unfolded, they decided that the mystical beliefs of the common people were not only backward, but dangerous, and that they should be replaced by cold, measurable and predictable physical laws and respect for human genius. They replaced long-held theologies, pantheons, and mystical traditions with ideas like rational thought and the scientific method. As more and more sleepers began to use the Order's discoveries in their everyday lives, Reason and rationality came to govern their beliefs, and the old ways came to be regarded as misguided superstition. However, the Order of Reason became less and less focused on improving the daily lives of sleepers and more concerned with eliminating any resistance to their choke-hold on the minds of humanity. Ever since a reorganization performed under Queen Victoria in the late 1800s, they call themselves .

The Technocracy espouses an authoritarian rule over Sleepers' beliefs, while suppressing the Council of Nine's attempts to reintroduce magic. The Traditions replenished their numbers (which had been diminished by the withdrawal of two Traditions, the secretive Ahl-i-Batin, and the Solificati, alchemists plagued by scandal) with former Technocrats from the Sons of Ether and Virtual Adepts factions, vying for the beliefs of sleepers and with the Technocracy, and perpetually wary of the Nephandi (who consciously embrace evil and service to a demonic or alien master) and the Marauders (who resist Paradox with a magical form of madness). While the Technocracy's propaganda campaigns were effective in turning the Consensus against mystic and heterodox science, the Traditions maintained various resources, including magical nodes, hidden schools and fortresses called Chantries, and various realms outside of the Consensus in the Umbra.

Finally, from 1997–2000, a series of metaplot events destroyed the Council of Nine's Umbral steadings, killing many of their most powerful members. This also cut the Technocracy off from their leadership. Both sides called a truce in their struggle to assess their new situation, especially since these events implied that Armageddon was soon at hand. Chief among these signs was creation of a barrier between the physical world and spirit world. This barrier was called the Avatar Storm because it affected the Avatar of the Mage. This Avatar Storm was the result of a battle in India on the so-called "Week of Nightmares."

These changes were introduced in supplements for the second edition of the game and became core material in the third edition.

Aside from common changes introduced by the World of Darkness metaplot, mages dealt with renewed conflict when the hidden Rogue Council and the Technocracy's Panopticon encouraged the Traditions and Technocracy to struggle once again. The Rogue Council only made itself known through coded missives, while Panopticon was apparently created by the leaders of the Technocracy to counter it.

This struggle eventually led to the point on the timeline occupied by the book called "". While the entire metaplot has always been meant to be altered as each play group sees fit, "Ascension" provided multiple possible endings, with none of them being definitive (though one was meant to resolve the metaplot). Thus, there is no definitive canonical ending. Since the game is meant to be adapted to a group's tastes, the importance of this and the preceding storyline is largely a matter of personal preference.

The metaplot of the game involves a four-way struggle between the technological and authoritarian Technocracy, the insane Marauders, the cosmically evil Nephandi and the nine mystical Traditions (that tread the middle path), to which the player characters are assumed to belong. (This struggle has in every edition of the game been characterized both as primarily a covert, violent war directly between factions, and primarily as an effort to sway the imaginations and beliefs of sleepers.)

The Traditions (formally called the Nine Mystic Traditions) are a fictional alliance of secret societies in the "Mage: the Ascension" role-playing game. The Traditions exist to unify users of magic under a common banner to protect reality (particularly those parts of reality that are magical) against the growing disbelief of the modern world, the spreading dominance of the , and the predations of unstable mages such as Marauders and Nephandi. Each of the Traditions are largely independent organizations unified by a broadly accepted paradigm for practicing magic. The Traditions themselves vary substantially from one another. Some have almost no structure or rules, while others have rigid rules of protocol, etiquette, and rank. Though unified in their desire to keep magic alive, the magic practiced by different Traditions are often wildly different and entirely incompatible with one another. Understanding Traditions as a whole requires understanding each Tradition separately, and then assembling them into a somewhat cohesive whole.

The nine traditions are: the Akashic Brotherhood, Celestial Chorus, Cult of Ecstasy, Dreamspeakers, Euthanatos, Order of Hermes, Sons of Ether, Verbena and Virtual Adepts. 

The Technocracy is likewise divided into groups; unlike the Traditions, however, they share a single paradigm, and instead divide themselves based upon methodologies and areas of expertise.


The Marauders are a group of mages that embody Dynamism. Marauders are chaos mages. They are completely insane. To other mages, they appear immune to paradox effects, often using vulgar magic to accomplish their insane tasks. Marauders represent the other narrative extreme, the repellent and frightening corruption of unrestrained power, of dynamism unchecked. Marauders are insane mages whose Avatars have been warped by their mental instability, and who exist in a state of permanent Quiet. While the nature of a Marauder's power may make them seem invincible, they are still severely hampered by their madness. They cannot become Archmages, as they lack sufficient insight and are incapable of appreciating truths which do not suit their madness. In the second edition of "Mage: The Ascension", Marauders were much more cogent and likely to operate in groups, with the Umbral Underground using the Umbra to infiltrate any location and wreak havoc with the aid of bygones. They were also associated heavily with other perceived agents of Dynamism, particularly the s (who equate Dynamism with the Wyld) and sometimes Changelings. For example, the Marauders chapter in "The Book of Madness" is narrated by a Corax (were-raven) named Johnny Gore, who relates his experiences running with the Butcher Street Regulars. In the revised edition, Marauders were made darker and less coherent, in keeping with the more serious treatment of madness used for Malkavians in "Vampire: The Masquerade Revised Edition". The Avatar Storm was a very convenient explanation for the Underground's loss of power and influence, though they also became more vulnerable to Paradox. In this edition, the Regulars are a cell of the Underground, and like the other cells have highly compatible Quiets.

With the Technocracy representing Stasis and the Marauders acting on behalf of Dynamism, the third part of this trifecta is Entropy, as borne by the Nephandi. While other mages may be callous or cruel, the Nephandi are morally inverted and spiritually mutilated. While a Traditionalist or Technocrat may simply fall prey to human failings or excessive zeal in their ethos, while a Marauder may well commit some true atrocities in the depth of her incurable madness; a Nephandus retains a clear moral compass, and deliberately pursues actions to worsen the world and bring about its final end. To this end, the Technocracy and Traditions have been known to set aside the ongoing war for reality to temporarily join forces to oppose the Nephandi, and even the Marauders are known to attack the Nephandi on sight. Some of their members, called "barabbi", hail from the Technocracy and Traditions, but all Nephandi have experienced the Rebirth, wherein they embrace the antithesis of everything they know to be right, and are physically and spiritually torn apart and reassembled. This metamorphosis has a sort of terrible permanence to it: while each Mage's avatar will be reborn again and again, theirs is permanently twisted as a result of their rebirth: known as Widderslainte, these mages awaken as Nephandi. While some of the background stories detail a particular mage and her teacher trying—and succeeding—at keeping her from falling again, this is very rare.

Other mystical traditions that are not part of the nine exist, and are known as Crafts. Some examples of these are the mages of Ahl-i-Batin (also known as "The Subtle Ones") who are masters of the Correspondence Sphere and former holders of the seat now held by the Virtual Adepts, as well as the djinn binding magicians known as The Taftani and the eclectic nonconformist group of willworkers known as Hollow Ones, however they are far from the only ones.

The core rules of the game are similar to those in other World of Darkness games; see Storyteller System for an explanation.

Like other storytelling games Mage emphasizes personal creativity and that ultimately the game's powers and traits should be used to tell a satisfying story. One of Mage's highlights is its system for describing magic, based on spheres, a relatively open-ended 'toolkit' approach to using game mechanics to define the bounds of a given character's magical ability. Different Mages will have differing aptitudes for spheres, and player characters' magical expertise is described by allocation of points in the spheres.

There are nine known spheres:

Deals with spatial relations, giving the Mage power over space and distances. Correspondence magic allows powers such as teleportation, seeing into distant areas, and at higher levels the Mage may also co-locate herself or even stack different spaces within each other. Correspondence can be combined with almost any other sphere to create effects that span distances.

This sphere gives the Mage power over order, chaos, fate and fortune. A mage can sense where elements of chance influence the world and manipulate them to some degree. At simple levels machines can be made to fail, plans to go off without a hitch, and games of chance heavily influenced. Advanced mages can craft self-propagating memes or curse entire family lines with blights. The only requirement of the Entropy sphere is that all interventions work within the general flow of natural entropy.

Forces concerns energies and natural forces and their negative opposites (i.e. light and shadow can both be manipulated independently with this Sphere). Essentially, anything in the material world that can be seen or felt but is not material can be controlled: electricity, gravity, magnetism, friction, heat, motion, fire, etc. At low levels the mage can control forces on a small scale, changing their direction, converting one energy into another. At high levels, storms and explosions can be conjured. Obviously, this Sphere tends to do the most damage and is the most flashy and vulgar. Along with Life and Matter, Forces is one of the three 'Pattern Spheres' which together are able to mold all aspects of the physical world.

Life deals with understanding and influencing biological systems. Generally speaking, any material object with mostly living cells falls under the influence of this sphere. Simply, this allows the mage to heal herself or metamorphose simple life-forms at lower levels, working up to healing others and controlling more complex life at higher levels. Usually, seeking to improve a complex life-form beyond natural limits causes the condition of pattern bleeding: the affected life form begins to wither and die over time. Along with Matter and Forces, Life is one of the three Pattern Spheres.

Dealing with control over one's own mind, the reading and influencing of other minds, and a variety of subtler applications such as Astral Projection and psychometry. At high levels, Mages can create new complete minds or completely rework existing ones.

Matter deals with all inanimate material. Thus, being alive protects a thing from direct manipulation by the Matter sphere. Stone, dead wood, water, gold, and the corpses of once living things are only the beginning. With this Sphere, matter can be reshaped mentally, transmuted into another substance, or given altered properties. Along with Life and Forces, Matter is one of the three Pattern Spheres.

This sphere deals directly with Quintessence, the raw material of the tapestry, which is the metaphysical structure of reality. This sphere allows Quintessence to be channeled and/or funneled in any way at higher levels, and it is necessary if the mage ever wants to conjure something out of nothing (as opposed to transforming one pattern into another). Uses of Prime include general magic senses, counter-magic, and making magical effects permanent.

This sphere is an eclectic mixture of abilities relating to dealings with the spirit world or Umbra. It includes stepping into the Near Umbra right up to traveling through outer space, contacting and controlling spirits, communing with your own or others' avatars, returning a Mage into a sleeper, returning ghosts to life, creating magical fetish items, and so forth. Unlike other Spheres, the difficulty of Spirit magic is often a factor of the Gauntlet, making these spells more difficult for the most part. The Sphere is referred to as Dimensional Science by the Technocratic Union.

This sphere deals with dilating, slowing, stopping or traveling through time. Due to game mechanics, it is simpler to travel forward in time than backwards. Time can be used to install delays into spells, view the past or future, and even pull people and objects out of linear progression. Time magic offers one means to speed up a character to get multiple actions in a combat round, a much coveted power in turn-based role-playing.

One of the plot hooks that the second edition books put forth were persistent rumors of a "tenth sphere". Though there were hints, it was deliberately left vague. The final book in the line, "Ascension" implies that the tenth sphere is the sphere of Ascension (in as much as spheres are practically relevant at that point in the story). As the book presents alternative resolutions for the Mage line, Chapter Two also presents an alternative interpretation that the tenth sphere is "Judgement" or "Telos" and that Anthelios (the red star in the World of Darkness metaplot) is its planet (each sphere has an associated planet and Umbral realm).

The various sphere sigils are, in whole or in part, symbols taken from alchemical texts.


The third revision of the rules, "Mage: The Ascension Revised", made significant changes to the rules and setting, mainly to update Mage with respect to its own ongoing storyline, particularly in regards to events that occurred during the run of the game's second edition. (Like other World of Darkness games, Mage uses a continuing storyline across all of its books).

Adam Tinworth of "Arcane" gave "Mage: The Ascension" second edition a score of 8/10, calling it good for those who like involving and challenging games; he noted that it could be difficult for new players to grasp the entire background and how magic works, and to develop their own style of magic, but found the gameplay system itself to be easy to understand for newcomers.

"Mage: The Ascension", 2nd Edition won the Origins Award for "Best Roleplaying Rules" of 1995.

"Mage: The Ascension" was ranked 16th in the 1996 reader poll of "Arcane" magazine to determine the 50 most popular role-playing games of all time. The magazine's editor Paul Pettengale commented: "Mage is perfect for those of a philosophical bent. It's a hard game to get right, requiring a great deal of thought from players and referees alike, but its underlying theme – the nature of reality – makes it one of the most interesting and mature roleplaying games available."




</doc>
<doc id="19734" url="https://en.wikipedia.org/wiki?curid=19734" title="Malcolm Fraser">
Malcolm Fraser

John Malcolm Fraser (; 21 May 1930 – 20 March 2015) was an Australian politician who served as the 22nd Prime Minister of Australia, in office from 1975 to 1983 as leader of the Liberal Party.

Fraser was raised on his father's sheep stations, and after studying at Magdalen College, Oxford, returned to Australia to take over the family property in the Western District of Victoria. After an initial defeat in 1954, he was elected to the House of Representatives at the 1955 federal election, standing in the Division of Wannon. He was 25 at the time, making him one of the youngest people ever elected to parliament. When Harold Holt became prime minister in 1966, Fraser was appointed Minister for the Army. After Holt's disappearance and replacement by John Gorton, Fraser became Minister for Education and Science (1968–1969) and then Minister for Defence (1969–1971). In 1971, Fraser resigned from cabinet and denounced Gorton as "unfit to hold the great office of prime minister"; this precipitated the replacement of Gorton with William McMahon. He subsequently returned to his old education and science portfolio.

After the Coalition was defeated at the 1972 election, Fraser unsuccessfully stood for the Liberal leadership, losing to Billy Snedden. When the party lost the 1974 election, he began to move against Snedden, eventually mounting a successful challenge in March 1975. As Leader of the Opposition, Fraser used the Coalition's control of the Senate to block supply to the Whitlam Government, precipitating a constitutional crisis. This culminated with Gough Whitlam being dismissed as prime minister by Governor-General John Kerr, a unique occurrence in Australian history. The correctness of Fraser's actions in the crisis and the exact nature of his involvement in Kerr's decision have since been a topic of debate.

After Whitlam's dismissal, Fraser was sworn in as prime minister on an initial caretaker basis. The Coalition won a landslide victory at the 1975 election, and was re-elected in 1977 and 1980. Fraser took a keen interest in foreign affairs as prime minister, and was more active in the international sphere than many of his predecessors. He was a strong supporter of multiculturalism, and during his term in office Australia admitted significant numbers of non-white immigrants (including Vietnamese boat people) for the first time. His government also established the Special Broadcasting Service (SBS). Particularly in his final years in office, Fraser came into conflict with the economic rationalist faction of his party. His government made few major changes to economic policy.

Fraser and the Coalition lost power at the 1983 election, and he left politics a short time later. To date, he is the last Prime Minister from a country seat. In retirement, he held advisory positions with the UN and the Commonwealth of Nations, and was president of the aid agency CARE from 1990 to 1995. He resigned his membership of the Liberal Party in 2009, having been a critic of its policy direction for a number of years. Evaluations of Fraser's prime ministership have been mixed. He is generally credited with restoring stability to the country after a series of short-term leaders, but some have seen his government as a lost opportunity for economic reform. Only three Australian prime ministers have served longer terms in office – Robert Menzies, John Howard and Bob Hawke.

John Malcolm Fraser was born in Toorak, Melbourne, Victoria, on 21 May 1930. He was the second of two children born to Una Arnold (née Woolf) and John Neville Fraser; his older sister Lorraine had been born in 1928. Both he and his father were known exclusively by their middle names. His paternal grandfather, Sir Simon Fraser, was born in Nova Scotia, Canada, and arrived in Australia in 1853. He made his fortune as a railway contractor, and later acquired significant pastoral holdings, becoming a member of the "squattocracy". Fraser's maternal grandfather, Louis Woolf, was born in Dunedin, New Zealand, and arrived in Australia as a child. He was of Jewish origin, a fact which his grandson did not learn until he was an adult. A chartered accountant by trade, he married Amy Booth, who was related to the wealthy Hordern family of Sydney and was a first cousin of Sir Samuel Hordern.

Fraser had a political background on both sides of his family. His father served on the Wakool Shire Council, including as president for two years, and was an admirer of Billy Hughes and a friend of Richard Casey. Simon Fraser served in both houses of the colonial Parliament of Victoria, and represented Victoria at several of the constitutional conventions of the 1890s. He eventually become one of the inaugural members of the new federal Senate, serving from 1901 to 1913 as a member of the early conservative parties. Louis Woolf also ran for the Senate in 1901, standing as a Free Trader in Western Australia. He polled only 400 votes across the whole state, and was never again a candidate for public office.

Fraser spent most of his early life at "Balpool-Nyang", a sheep station of on the Edward River near Moulamein, New South Wales. His father had a law degree from Magdalen College, Oxford, but never practised law and preferred the life of a grazier. Fraser contracted a severe case of pneumonia when he was eight years old, which nearly proved fatal. He was home-schooled until the age of ten, when he was sent to board at Tudor House School in the Southern Highlands. He attended Tudor House from 1940 to 1943, and then completed his secondary education at Melbourne Grammar School from 1944 to 1948 where he was a member of Rusden House. While at Melbourne Grammar, he lived in a flat that his parents owned on Collins Street. In 1943, Fraser's father sold "Balpool-Nyang" – which had been prone to drought – and bought "Nareen", in the Western District of Victoria. He was devastated by the sale of his childhood home, and regarded the day he found out about it as the worst of his life.

In 1949, Fraser moved to England to study at Magdalen College, Oxford, which his father had also attended. He read Philosophy, Politics and Economics (PPE), graduating in 1952 with third-class honours. Although Fraser did not excel academically, he regarded his time at Oxford as his intellectual awakening, where he learned "how to think". His college tutor was Harry Weldon, who was a strong influence. His circle of friends at Oxford included Raymond Bonham Carter, Nicolas Browne-Wilkinson, and John Turner. In his second year, he had a relationship with Anne Reid, who as Anne Fairbairn later became a prominent poet. After graduating, Fraser considered taking a law degree or joining the British Army, but eventually decided to return to Australia and take over the running of the family property.

Fraser returned to Australia in mid-1952. He began attending meetings of the Young Liberals in Hamilton, and became acquainted with many of the local party officials. In November 1953, aged 23, Fraser unexpectedly won Liberal preselection for the Division of Wannon, which covered most of Victoria's Western District. The previous Liberal member, Dan Mackinnon, had been defeated in 1951 and moved to a different electorate. He was expected to be succeeded by Magnus Cormack, who had recently lost his place in the Senate. Fraser had put his name forward as a way of building a profile for future candidacies, but mounted a strong campaign and in the end won a narrow victory. In January 1954, he made the first of a series of weekly radio broadcasts on 3HA Hamilton and 3YB Warrnambool, titled "One Australia". His program – consisting of a pre-recorded 15-minute monologue – covered a wide range of topics, and was often reprinted in newspapers. It continued more or less uninterrupted until his retirement from politics in 1983, and helped him build a substantial personal following in his electorate.

At the 1954 election, Fraser lost to the sitting Labor member Don McLeod by just 17 votes (out of over 37,000 cast). However, he reprised his candidacy at the early 1955 election after a redistribution made Wannon notionally Liberal. McLeod concluded the reconfigured Wannon was unwinnable and retired. These factors, combined with the 1955 Labor Party split, allowed Fraser to win a landslide victory.

Fraser took his seat in parliament at the age of 25 – the youngest sitting MP by four years, and the first who had been too young to serve in World War II. He was re-elected at the 1958 election despite being restricted in his campaigning by a bout of hepatitis. Fraser was soon being touted as a future member of cabinet, but despite good relations with Robert Menzies never served in any of his ministries. This was probably due to a combination of his youth and the fact that the ministry already contained a disproportionately high number of Victorians.

Fraser spoke on a wide range of topics during his early years in parliament, but took a particular interest in foreign affairs. In 1964, he and Gough Whitlam were both awarded Leader Grants by the United States Department of State, allowing them to spend two months in Washington, D.C., getting to know American political and military leaders. The Vietnam War was the main topic of conversation, and on his return trip to Australia he spent two days in Saigon. Early in 1965, he also made a private seven-day visit to Jakarta, and with assistance from Ambassador Mick Shann secured meetings with various high-ranking officials.

After more than a decade on the backbench, Fraser was appointed to the Cabinet by the prime minister, Harold Holt, in 1966. As Minister for the Army he presided over the controversial Vietnam War conscription program.

Under the new prime minister, John Gorton, he became Minister for Education and Science and in 1969 was promoted to Minister for Defence, a particularly challenging post at the time, given the height of Australia's involvement in the Vietnam War and the protests against it.

In March 1971 Fraser abruptly resigned from the Cabinet in protest at what he called Gorton's "interference in (his) ministerial responsibilities".

This precipitated a series of events which eventually led to the downfall of Gorton and his replacement as prime minister by William McMahon. Gorton never forgave Fraser for the role he played in his downfall; to the day Gorton died in 2002, he could not bear to be in the same room with Fraser.

McMahon immediately reappointed Fraser to the Cabinet, returning him to his old position of Minister for Education and Science. When the Liberals were defeated at the 1972 election by the Labor Party under Gough Whitlam, McMahon resigned and Fraser became Shadow Minister for Labour under Billy Snedden.

After the Coalition lost the 1972 election, Fraser was one of five candidates for the Liberal leadership that had been vacated by McMahon. He outpolled John Gorton and James Killen, but was eliminated on the third ballot. Billy Snedden eventually defeated Nigel Bowen by a single vote on the fifth ballot. In the new shadow cabinet – which featured only Liberals – Fraser was given responsibility for primary industry. This was widely seen as a snub, as the new portfolio kept him mostly out of the public eye and was likely to be given to a member of the Country Party when the Coalition returned to government. In an August 1973 reshuffle, Snedden instead made him the Liberals' spokesman for industrial relations. He had hoped to be given responsibility for foreign affairs (in place of the retiring Nigel Bowen), but that role was given to Andrew Peacock. Fraser oversaw the development of the party's new industrial relations policy, which was released in April 1974. It was seen as more flexible and even-handed than the policy that the Coalition had pursued in government, and was received well by the media. According to Fraser's biographer Philip Ayres, by "putting a new policy in place, he managed to modify his public image and emerge as an excellent communicator across a traditionally hostile divide".

After the Liberals lost the 1974 election, Fraser unsuccessfully challenged Snedden for the leadership in November. Despite surviving the challenge, Snedden's position in opinion polls continued to decline and he was unable to get the better of Whitlam in the Parliament. Fraser again challenged Snedden on 21 March 1975, this time succeeding and becoming Leader of the Liberal Party and Leader of the Opposition.

Following a series of ministerial scandals engulfing the Whitlam Government later that year, Fraser began to instruct Coalition senators to delay the government's budget bills, with the objective of forcing an early election that he believed he would win. After several months of political deadlock, during which time the government secretly explored methods of obtaining supply funding outside the Parliament, the Governor-General, Sir John Kerr, controversially dismissed Whitlam as prime minister on 11 November 1975.

Fraser was immediately sworn in as caretaker prime minister on the condition that he end the political deadlock and call an immediate double dissolution election.

On 19 November 1975, shortly after the election had been called, a letter bomb was sent to Fraser, but it was intercepted and defused before it reached him. Similar devices were sent to the governor-general and the Premier of Queensland, Joh Bjelke-Petersen.

At the 1975 election, Fraser led the Liberal-Country Party Coalition to a landslide victory. The Coalition won 91 seats of a possible 127 in the election to gain a 55-seat majority, which remains to date the largest in Australian history. Fraser subsequently led the Coalition to a second victory in 1977, with only a very small decrease in their vote. The Liberals actually won a majority in their own right in both of these elections, something that Menzies and Holt had never achieved. Although Fraser thus had no need for the support of the (National) Country Party to govern, he retained the formal Coalition between the two parties.

Fraser quickly dismantled some of the programs of the Whitlam Government, such as the Ministry of the Media, and made major changes to the universal health insurance system Medibank. He initially maintained Whitlam's levels of tax and spending, but real per-person tax and spending soon began to increase. He did manage to rein in inflation, which had soared under Whitlam. His so-called "Razor Gang" implemented stringent budget cuts across many areas of the Commonwealth Public Sector, including the Australian Broadcasting Corporation (ABC).

Fraser practised Keynesian economics during his time as Prime Minister, in part demonstrated by running budget deficits throughout his term as Prime Minister. He was the Liberal Party's last Keynesian Prime Minister. Though he had long been identified with the Liberal Party's right wing, he did not carry out the radically conservative program that his political enemies had predicted, and that some of his followers wanted. Fraser's relatively moderate policies particularly disappointed the Treasurer, John Howard, as well as other ministers who were strong adherents of economic liberalism, and therefore detractors of Keynesian economics. The government's economic record was marred by rising double-digit unemployment and double-digit inflation, creating "stagflation", caused in part by the ongoing effects of the 1973 oil crisis.

Fraser was particularly active in foreign policy as prime minister. He supported the Commonwealth in campaigning to abolish apartheid in South Africa and refused permission for the aircraft carrying the Springbok rugby team to refuel on Australian territory en route to their controversial 1981 tour of New Zealand. However, an earlier tour by the South African ski boat angling team was allowed to pass through Australia on the way to New Zealand in 1977 and the transit records were suppressed by Cabinet order.

Fraser also strongly opposed white minority rule in Rhodesia. During the 1979 Commonwealth Conference, Fraser, together with his Nigerian counterpart, convinced the newly elected British prime minister, Margaret Thatcher, to withhold recognition of the internal settlement Zimbabwe Rhodesia government; Thatcher had earlier promised to recognise it. Subsequently, the Lancaster House Agreement was signed and Robert Mugabe was elected leader of an independent Zimbabwe at the inaugural 1980 election. Duncan Campbell, a former deputy secretary of the Department of Foreign Affairs and Trade has stated that Fraser was "the principal architect" in the ending of white minority rule. The President of Tanzania, Julius Nyerere, said that he considered Fraser's role "crucial in many parts" and the President of Zambia, Kenneth Kaunda, called his contribution "vital".

Under Fraser, Australia recognised Indonesia's annexation of East Timor, although many East Timorese refugees were granted asylum in Australia. Fraser was also a strong supporter of the United States and supported the boycott of the 1980 Summer Olympics in Moscow. However, although he persuaded some sporting bodies not to compete, Fraser did not try to prevent the Australian Olympic Committee sending a team to the Moscow Games.

Fraser also surprised his critics over immigration policy; according to 1977 Cabinet documents, the Fraser Government adopted a formal policy for "a humanitarian commitment to admit refugees for resettlement". Fraser's aim was to expand immigration from Asian countries and allow more refugees to enter Australia. He was a firm supporter of multiculturalism and established a government-funded multilingual radio and television network, the Special Broadcasting Service (SBS), building on their first radio stations which had been established under the Whitlam Government.

Despite Fraser's support for SBS, his government imposed stringent budget cuts on the national broadcaster, the ABC, which came under repeated attack from the Coalition for alleged "left-wing bias" and "unfair" coverage on their TV programs, including "This Day Tonight" and "Four Corners", and on the ABC's new youth-oriented radio station Double Jay. One result of the cuts was a plan to establish a national youth radio network, of which Double Jay was the first station. The network was delayed for many years and did not come to fruition until the 1990s. Fraser also legislated to give Indigenous Australians control of their traditional lands in the Northern Territory, but resisted imposing land rights laws on conservative state governments.
At the 1980 election, Fraser saw his majority more than halved, from 48 seats to 21. The Coalition also lost control of the Senate. Despite this, Fraser remained ahead of Labor leader Bill Hayden in opinion polls. However, the economy was hit by the early 1980s recession, and a protracted scandal over tax-avoidance schemes run by some high-profile Liberals also began to hurt the Government.

In April 1981, the Minister for Industrial Relations, Andrew Peacock, resigned from the Cabinet, accusing Fraser of "constant interference in his portfolio". Fraser, however, had accused former prime minister John Gorton of the same thing a decade earlier. Peacock subsequently challenged Fraser for the leadership; although Fraser defeated Peacock, these events left him politically weakened.

By early 1982, the popular former ACTU President, Bob Hawke, who had entered Parliament in 1980, was polling well ahead of both Fraser and the Labor Leader, Bill Hayden, on the question of who voters would rather see as prime minister. Fraser was well aware of the infighting this caused between Hayden and Hawke and had planned to call a snap election in autumn 1982, preventing the Labor Party changing leaders. These plans were derailed when Fraser suffered a severe back injury. Shortly after recovering from his injury, the Liberal Party narrowly won a by-election in the marginal seat of Flinders in December 1982. The failure of the Labor Party to win the seat convinced Fraser that he would be able to win an election against Hayden.

As leadership tensions began to grow in the Labor Party throughout January, Fraser subsequently resolved to call a double dissolution election at the earliest opportunity, hoping to capitalise on Labor's disunity. He knew that if the writs were issued soon enough, Labor would essentially be frozen into going into the subsequent election with Hayden as leader.

On 3 February 1983, Fraser arranged to visit the Governor-General of Australia, Ninian Stephen, intending to ask for a surprise election. However, Fraser made his run too late. Without any knowledge of Fraser's plans, Hayden resigned as Labor leader just two hours before Fraser travelled to Government House. This meant that the considerably more popular Hawke was able to replace him at almost exactly the same time that the writs were issued for the election. Although Fraser reacted to the move by saying he looked forward to "knock[ing] two Labor Leaders off in one go" at the forthcoming election, Labor immediately surged in the opinion polls.

At the election on 5 March the Coalition was heavily defeated, suffering a 24-seat swing, the worst defeat of a non-Labor government since Federation. Fraser immediately announced his resignation as Liberal leader and formally resigned as prime minister on 11 March 1983; he retired from Parliament two months later. To date, he is the last non-interim prime minister from a rural seat.

In retirement Fraser served as Chairman of the UN Panel of Eminent Persons on the Role of Transnational Corporations in South Africa 1985, as Co-Chairman of the Commonwealth Group of Eminent Persons on South Africa in 1985–86 (appointed by Prime Minister Hawke), and as Chairman of the UN Secretary-General's Expert Group on African Commodity Issues in 1989–90. He was a distinguished international fellow at the American Enterprise Institute from 1984 to 1986. Fraser helped to establish the foreign aid group CARE organisation in Australia and became the agency's international president in 1991, and worked with a number of other charitable organisations. In 2006, he was appointed Professorial Fellow at the Asia Pacific Centre for Military Law, and in October 2007 he presented his inaugural professorial lecture, "Finding Security in Terrorism's Shadow: The importance of the rule of law".

On 14 October 1986, Fraser, then the Chairman of the Commonwealth Eminent Persons Group, was found in the foyer of the Admiral Benbow Inn, a seedy Memphis hotel, wearing only a pair of underpants and confused as to where his trousers were. The hotel was an establishment popular with prostitutes and drug dealers. Though it was rumoured at the time that the former Prime Minister had been with a prostitute, his wife stated that Fraser had no recollection of the events and that she believes it more likely that he was the victim of a practical joke by his fellow delegates.

In 1993, Fraser made a bid for the Liberal Party presidency but withdrew at the last minute following opposition to his bid, which was raised due to him having been critical of then Liberal leader John Hewson for losing the election earlier that year.

After 1996, Fraser was critical of the Howard Coalition government over foreign policy issues, particularly John Howard's alignment with the foreign policy of the Bush administration, which Fraser saw as damaging Australian relationships in Asia. He opposed Howard's policy on asylum-seekers, campaigned in support of an Australian Republic and attacked what he perceived as a lack of integrity in Australian politics, together with former Labor prime minister Gough Whitlam, finding much common ground with his predecessor and his successor Bob Hawke, another republican.

The 2001 election continued his estrangement from the Liberal Party. Many Liberals criticised the Fraser years as "a decade of lost opportunity" on deregulation of the Australian economy and other issues. In early 2004, a Young Liberal convention in Hobart called for Fraser's life membership of the Liberal Party to be ended.

In 2006, Fraser criticised Howard Liberal government policies on areas such as refugees, terrorism and civil liberties, and that "if Australia continues to follow United States policies, it runs the risk of being embroiled in the conflict in Iraq for decades, and a fear of Islam in the Australian community will take years to eradicate". Fraser claimed that the way the Howard government handled the David Hicks, Cornelia Rau and Vivian Solon cases was questionable.

On 20 July 2007, Fraser sent an open letter to members of the large activist group GetUp!, encouraging members to support GetUp's campaign for a change in policy on Iraq including a clearly defined exit strategy. Fraser stated: "One of the things we should say to the Americans, quite simply, is that if the United States is not prepared to involve itself in high-level diplomacy concerning Iraq and other Middle East questions, our forces will be withdrawn before Christmas."

After the defeat of the Howard government at the 2007 federal election, Fraser claimed Howard approached him in a corridor, following a cabinet meeting in May 1977 regarding Vietnamese refugees, and said: "We don't want too many of these people. We're doing this just for show, aren't we?" The claims were made by Fraser in an interview to mark the release of the 1977 cabinet papers. Howard, through a spokesman, denied having made the comment.

In October 2007 Fraser gave a speech to Melbourne Law School on terrorism and "the importance of the rule of law," which Liberal MP Sophie Mirabella
condemned in January 2008, claiming errors and "either intellectual sloppiness or deliberate dishonesty", and claimed that he tacitly supported Islamic fundamentalism, that he should have no influence on foreign policy, and claimed his stance on the war on terror had left him open to caricature as a "frothing-at-the-mouth leftie".

Shortly after Tony Abbott won the 2009 Liberal Party leadership spill, Fraser ended his Liberal Party membership, stating the party was "no longer a liberal party but a conservative party".

In December 2011, Fraser was highly critical of the Australian government's decision (also supported by the Liberal Party Opposition) to permit the export of uranium to India, relaxing the Fraser government's policy of banning sales of uranium to countries that are not signatories of the Nuclear Non-Proliferation Treaty.

In 2012, Fraser criticised the basing of US military forces in Australia.

In late 2012, Fraser wrote a foreword for the journal "Jurisprudence" where he openly criticised the current state of human rights in Australia and the Western World. "It is a sobering thought that in recent times, freedoms hard won through centuries of struggle, in the United Kingdom and elsewhere have been whittled away. In Australia alone we have laws that allow the secret detention of the innocent. We have had a vast expansion of the power of intelligence agencies. In many cases the onus of proof has been reversed and the justice that once prevailed has been gravely diminished."

In July 2013, Fraser endorsed Australian Greens Senator Sarah Hanson-Young for re-election in a television advertisement, stating she had been a "reasonable and fair-minded voice".

Fraser's books include "Malcolm Fraser: The Political Memoirs" (with Margaret Simons – The Miegunyah Press, 2010) and "Dangerous Allies" (Melbourne University Press, 2014), which warns of "strategic dependence" on the United States. In the book and in talks promoting it, he criticised the concept of American exceptionalism and US foreign policy.

Fraser died on the early morning of 20 March 2015 at the age of 84 after a brief illness. An obituary noted that there had been "greater appreciation of the constructive and positive nature of his post-prime ministerial contribution" as his retirement years progressed.

Fraser was given a state funeral at Scots' Church in Melbourne on 27 March 2015. His ashes are interred within the 'Prime Ministers Garden' of Melbourne General Cemetery.

On 9 December 1956, Fraser married Tamara "Tamie" Beggs, who was almost six years his junior. They had met at a New Year's Eve party, and bonded over similar personal backgrounds and political views. The couple had four children together: Mark (b. 1958), Angela (b. 1959), Hugh (b. 1963), and Phoebe (b. 1966). Tamie frequently assisted her husband in campaigning, and her gregariousness was seen as complementing his more shy and reserved nature. She advised him on most of the important decisions in his career, and in retirement he observed that "if she had been prime minister in 1983, we would have won".

Fraser attended Anglican schools, although his parents were Presbyterian. In university he was inclined towards atheism, once writing that "the idea that God exists is a nonsense". However, his beliefs became less definite over time and tended towards agnosticism. During his political career, he occasionally self-described as Christian, such as in a 1975 interview with "The Catholic Weekly". Margaret Simons, the co-author of Fraser's memoirs, thought that he was "not religious, and yet thinks religion is a necessary thing". In a 2010 interview with her, he said: "I would probably like to be less logical and, you know, really able to believe there is a god, whether it is Allah, or the Christian god, or some other – but I think I studied too much philosophy ... you can never know".

In 2004, Fraser designated the University of Melbourne the official custodian of his personal papers and library to create the Malcolm Fraser Collection at the university.

Upon his death, Fraser's 1983 nemesis and often bitter opponent Hawke fondly described him as a "very significant figure in the history of Australian politics" who, in his post-Prime Ministerial years, "became an outstanding figure in the advancement of human rights issues in all respects", praised him for being "extraordinarily generous and welcoming to refugees from Indochina" and concluded that Fraser had "moved so far to the left he was almost out of sight". Andrew Peacock, who had challenged Fraser for the Liberal leadership and later succeeded him, said that he had "a deep respect and pleasurable memories of the first five years of the Fraser Government... I disagreed with him later on but during that period in the 1970s he was a very effective Prime Minister", and lamented that "despite all my arguments with him later on I am filled with admiration for his efforts on China".

In June 2018, he was honoured with the naming of the Australian Electoral Division of Fraser in the inner north-western suburbs of Melbourne.


Orders

Foreign honours

Organisations

Personal

Fellowships

Academic degrees







</doc>
<doc id="19735" url="https://en.wikipedia.org/wiki?curid=19735" title="Macquarie University">
Macquarie University

Macquarie University () is a public research university based in Sydney, Australia, in the suburb of Macquarie Park. Founded in 1964 by the New South Wales Government, it was the third university to be established in the metropolitan area of Sydney.

Established as a verdant university, Macquarie has five faculties, as well as the Macquarie University Hospital and the Macquarie Graduate School of Management, which are located on the university's main campus in suburban Sydney.

The university is the first in Australia to fully align its degree system with the Bologna Accord.

The idea of founding a third university in Sydney was flagged in the early 1960s when the New South Wales Government formed a committee of enquiry into higher education to deal with a perceived emergency in university enrollments in New South Wales. During this enquiry, the Senate of the University of Sydney put in a submission which highlighted 'the immediate need to establish a third university in the metropolitan area'. After much debate a future campus location was selected in what was then a semi-rural part of North Ryde, and it was decided that the future university be named after Lachlan Macquarie, an important early governor of the colony of New South Wales.

Macquarie University was formally established in 1964 with the passage of the Macquarie University Act 1964 by the New South Wales parliament.

The initial concept of the campus was to create a new high technology corridor, similar to the area surrounding Stanford University in Palo Alto, California, the goal being to provide for interaction between industry and the new university. The academic core was designed in the Brutalist style and developed by the renowned town planner Walter Abraham who also oversaw the next 20 years of planning and development for the university. A committee appointed to advise the state government on the establishment of the new university at North Ryde nominated Abraham as the architect-planner. The fledgling Macquarie University Council decided that planning for the campus would be done within the university, rather than by consultants, and this led to the establishment of the architect-planners office.

The first Vice-Chancellor of Macquarie University, Alexander George Mitchell, was selected by the University Council which met for the first time on 17 June 1964. Members of the first university council included: Colonel Sir Edward Ford OBE, David Paver Mellor, Rae Else-Mitchell QC and Sir Walter Scott.

The university first opened to students on 6 March 1967 with more students than anticipated. The Australian Universities Commission had allowed for 510 effective full-time students (EFTS) but Macquarie had 956 enrolments and 622 EFTS. Between 1968 and 1969, enrolment at Macquarie increased dramatically with an extra 1200 EFTS, with 100 new academic staff employed. 1969 also saw the establishment of the Macquarie Graduate School of Management (MGSM).
Macquarie grew during the seventies and eighties with rapid expansion in courses offered, student numbers and development of the site. In 1972, the university established the Macquarie Law School, the third law school in Sydney. In their book "Liberality of Opportunity", Bruce Mansfield and Mark Hutchinson describe the founding of Macquarie University as 'an act of faith and a great experiment'. An additional topic considered in this book is the science reform movement of the late 1970s that resulted in the introduction of a named science degree, thus facilitating the subsequent inclusion of other named degrees in addition to the traditional BA. An alternative view on this topic is given by theoretical physicist John Ward.

In 1973 the student union (MUSC) worked with the Builders Labourers Federation (BLF) to organise one of the first "pink bans". Similar in tactic to the green ban, the pink ban was recommended when one of the residential colleges at Macquarie University, Robert Menzies College, ordered a student to lead a celibate life and undertake therapy and confession to cure himself of his homosexuality. The BLF decided to stop all construction work at the college until the university and the college Master made statements committing to a non-discriminatory university environment. MUSC was successful in engaging with the BLF again in 1974 when a woman at Macquarie University had her NSW Department of Education scholarship cancelled on the basis that she was a lesbian and therefore unfit to be a teacher.

After over a decade of service, the first Vice Chancellor Mitchell was succeeded by Edwin Webb in December 1975. Webb was required to steer the university through one of its most difficult periods as the value of universities were debated and the governments introduced significant funding cuts. Webb left the university in 1986 and was succeeded by Di Yerbury, the first female Vice-Chancellor in Australia. Yerbury would go on to hold the position of Vice-Chancellor for nearly 20 years.

In 1990 the university absorbed the Institute of Early Childhood Studies of the Sydney College of Advanced Education, under the terms of the Higher Education (Amalgamation) Act 1989. l

Steven Schwartz replaced Di Yerbury at the beginning of 2006. Yerbury's departure was attended with much controversy, including a "bitter dispute" with Schwartz, disputed ownership of university artworks worth $13 million and Yerbury's salary package. In August 2006, Schwartz expressed concern about the actions of Yerbury in a letter to university auditors. Yerbury strongly denied any wrongdoing and claimed the artworks were hers.

During 2007, Macquarie University restructured its student organisation after an audit raised questions about management of hundreds of thousands of dollars in funds by student organisations At the centre of the investigation was Victor Ma, president of the Macquarie University Students' Council, who was previously involved in a high-profile case of student election fixing at the University of Sydney.
The university Council resolved to immediately remove Ma from his position. Vice-Chancellor Schwartz cited an urgent need to reform Macquarie's main student bodies.
However, Ma strongly denied any wrongdoing and labelled the controversy a case of 'character assassination'.
The Federal Court ordered on 23 May 2007 that Macquarie University Union Ltd be wound up.

Following the dissolution of Macquarie University Union Ltd, the outgoing student organisation was replaced with a new wholly owned subsidiary company of the university, known as U@MQ Ltd. The new student organisation originally lacked a true student representative union; however, following a complete review and authorisation from the university Council, a new student union known as Macquarie University Students Association (MUSRA) was established in 2009.

Within the first few hundred days of Schwartz's instatement as Vice-Chancellor, the 'Macquarie@50' strategic plan was launched, which positioned the university to enhance research, teaching, infrastructure and academic rankings by the university's 50th anniversary in 2014. Included in the university's plans for the future was the establishment of a sustainability office in order to more effectively manage environmental and social development at Macquarie. As part of this campaign, in 2009 Macquarie became the first Fair Trade accredited university in Australia. The beginning of 2009 also saw the introduction of a new logo for the university which retained the Sirius Star, present on both the old logo and the university crest, but now 'embedded in a stylised lotus flower'. In accordance with the university by-law, the crest continues to be used for formal purposes and is displayed on university testamurs. The by-law also prescribes the university's motto, taken from Chaucer: 'And gladly teche'.

In 2013, the university became the first in Australia to fully align its degree system with the Bologna Accord.

Macquarie's arms was assumed through a 1967 amendment of the Macquarie University Act 1964 (Confirmed by Letters Patent of the College of Arms, 16 August 1969). The escutcheon displays the Macquarie Lighthouse tower, the first major public building in the colony, as well as the Sirius star, the name of the flagship of the First Fleet. The university's founders originally wanted to base the university's arms on Lachlan Macquarie's family crest, but they decided to go for a more radical approach that represented Lachlan Macquarie as a builder and administrator. The motto chosen for the university was "And Gladly Teche." This is taken from the general Prologue of The Canterbury Tales, Geoffrey Chaucer c.1400 and symbolises the university's commitment to both learning and teaching. The coat of arms and the motto are used in a very limited number of formal communications.

Macquarie has had a number of logos in its history. In 2014, the university launched a new logo as part of its Shared Identity Project. The logo reintroduced the Macquarie Lighthouse, a popular symbol of the University within the University community and maintained the Sirus Star.

Macquarie University's main campus is located about north-west of the Sydney CBD and is set on 126 hectares of rolling lawns and natural bushland. Located within the high-technology corridor of Sydney's north-west and in close proximity to Macquarie Park and its surrounding industries, Macquarie's location has been crucial in its development as a relatively research intensive university.

Prior to the development of the campus, most of the site was cultivated with peach orchards, market gardens and poultry farms. The university's first architect-planner was Walter Abraham, one of the first six administrators appointed to Macquarie University. As the site adapted from its former rural use to a busy collegiate environment, he implemented carefully designed planting programs across the campus. Abraham established a grid design comprising lots of running north–south, with the aim of creating a compact academic core. The measure of was seen as one minute's walk, and grid design reflected the aim of having a maximum walk of 10 minutes between any two parts of the university. The main east–west walkway that runs from the Macquarie University Research Park through to the arts faculty buildings, was named Wally's Walk in recognition of Walter Abraham's contribution to the development of the university.

Apart from its centres of learning, the campus features the Macquarie University Research Park, museums, art galleries, a sculpture park, an observatory, a sport and aquatic centre and also the private Macquarie University Hospital. The campus has its own postcode, 2109.

Macquarie became the first university in Australia to own and operate a private medical facility in 2010 when it opened a $300 million hospital on its campus. The hospital is the first and only private not-for-profit teaching hospital on an Australian university campus. The Macquarie University Hospital is located to the north of the main campus area towards the university sports grounds. It comprises 183 beds, 12 operating theatres, 2 cardiac and vascular angiography suites. The hospital is co-located with the university's Australian School of Advanced Medicine.

The university hosts a number of high technology companies on its campus. Primarily designed to encourage interaction between the university and industry, commercialisation of its campus has also given the institution an additional revenue stream. Tenants are selected based off their potential to collaborate with the universities researches or their ability to provide opportunities for its students and graduates. Cochlear Limited, has its headquarters in close proximity to the Australian Hearing Hub on the southern edge of campus. Other companies that have office space at the campus include Dow Corning, Goodman Fielder, Nortel Networks, OPSM and Siemens.

The Macquarie University Observatory was originally constructed in 1978 as a research facility but, since 1997, has been accessible to the public through its Public Observing Program.

The library houses over 1.8 million items and uses the Library of Congress Classification System. The library features several collections including a Rare Book Collection, a Palaeontology Collection and the Brunner Collection of Egyptological materials. Macquarie University operated two libraries during the transition. The old library in building C7A closed at the end of July 2011 (which has since been repurposed as a student support and study space), and the new library in building C3C became fully operational on 1 August 2011. The new library was the first university library in Australia to possess an Automated Storage and Retrieval System (ASRS). The ASRS consists of an environmentally controlled vault with metal bins storing the items; robotic cranes retrieve an item on request and deliver it to the service desk for collection.

The Macquarie University Incubator is a space to research and develop ideas that can be commercialised. It was established in 2017 as a part of the Macquarie Park Innovation District (MPID) project. Macquarie University received $1 million grant from the New South Wales government to build the incubator. The University has also committed about $7 million to the incubator with financial support of the big businesses and the New South Wales government. It was officially opened by Prince Andrew, Duke of York on 25 September 2017.

Macquarie University has two residential colleges on its campus, Dunmore Lang College and Robert Menzies College, both founded in 1972. The colleges offer academic support and a wide range of social and sporting activities in a communal environment.

Separate to the colleges is the Macquarie University Village. The village has over 900 rooms in mostly town house style buildings to the north of the campus. The village encourages its students to interact in its communal spaces and has a number of social events throughout the year.

The museums and collections of Macquarie University are extensive and include nine museums and galleries. Each collection focuses on various historical, scientific or artistic interests. The most visible collection on campus is the sculpture park which is exhibited across the entire campus. At close to 100 sculptures on display, it is the largest park of its kind in the Southern Hemisphere. All museums and galleries are open to the public and offer educational programs for students at primary, secondary and tertiary levels.

Located on the western side of the campus is the Macquarie University Sport and Aquatic Centre. Previously a sports hall facility, the complex was renovated and reopened in 2007 with the addition of the new gym and aquatic centre. It houses a 50-metre FINA-compliant outdoor pool and a 25-metre indoor pool. The complex also contains a gymnasium and squash, badminton, basketball, volleyball and netball courts.

Macquarie also has seven hectares of high quality playing fields for football, cricket and tennis. Situated to the north of the campus, the playing fields are used by the university as well as a number of elite sporting teams such as Sydney FC and the Westfield Matildas.

Macquarie University is served by Macquarie University railway station on the Sydney Metro Northwest line. The station opened in 2009 as part of the Epping to Chatswood Rail Link on the Sydney Trains network. In 2018, Macquarie University station closed for six months for conversion to Sydney Metro on the Sydney Metro Northwest line. Platform screen doors were installed as part of the upgrade. Macquarie is the only university in Australia with a railway station on campus. The station is served by driverless Alstom Metropolis trains every ten minutes during the off peak and every four minutes during the peak. 

There is also a major bus interchange within the campus that provides close to 800 bus services daily. The M2 Motorway runs parallel to the northern boundary of the campus and is accessible to traffic from the university.

The university currently comprises 35 departments within five faculties:

Research centres, schools and institutes that are affiliated with the university:

Macquarie University's Australian Hearing Hub is partnered with Cochlear. Cochlear Headquarters are on campus. The Australian Hearing Hub includes the head office of Australian Hearing.

The Australian Research Institute for Environment and Sustainability is a research centre that promotes change for environmental sustainability, is affiliated with the University and is located on its campus.

Access Macquarie Limited was established in 1989 as the commercial arm of the university. It facilitates and supports the commercial needs of industry, business and government organisations seeking to utilise the academic expertise of the broader University community.

The university is governed by a 17-member Council.

The University Council is the governing authority of the university under the "Macquarie University Act 1989". The Council takes primary responsibility for the control and management of the affairs of the University, and is empowered to make by-laws and rules relating to how the University is managed. Members of the Council include the University Vice-Chancellor, Academic and non-academic staff, the Vice President of the Academic Senate and a student representative. The Council is chaired by The Chancellor of the University.

The Academic Senate is the primary academic body of the university. It has certain powers delegated to it by Council, such as the approving of examination results and the completion of requirements for the award of degrees. At the same time, it makes recommendations to the Council concerning all changes to degree rules, and all proposals for new awards. While the Academic Senate is an independent body, it is required to make recommendations to the university Council in relation to matters outside its delegated authority.

Macquarie's current Vice-Chancellor, Bruce Dowton, took over from Schwartz in September 2012. Prior to his appointment Dowton served as a senior medical executive having held a range of positions in university, healthcare and consulting organisations. He also served as a pediatrician at the Massachusetts General Hospital for Children, and as Clinical Professor of Pediatrics at Harvard Medical School. There have been five Vice-Chancellors in the university's history.

The Macquarie University International College offers Foundation Studies (Pre-University) and University-level Diplomas. Upon successful completion of a MUIC Diploma, students enter the appropriate bachelor's degree as a second year student.

The Centre for Macquarie English is the English-language centre that offers a range of specialised, direct entry English programmes that are approved by Macquarie University.

The university positions itself as being research intensive. In 2012, 85% of Macquarie's broad fields of research was rated 'at or above world standard' in the Excellence in Research for Australia 2012 National report. The university is within the top 3 universities in Australia for the number of peer reviewed publications produced per academic staff member.

Researchers at Macquarie University, David Skellern and Neil Weste, and the Commonwealth Scientific and Industrial Research Organisation helped develop Wi-Fi. David Skellern has been a major donor to the University through the Skellern Family Trust. Macquarie physicists Frank Duarte and Jim Piper pioneered the laser designs adopted by researchers worldwide, in various major national programs, for atomic vapor laser isotope separation.

Macquarie University's linguistics department developed the Macquarie Dictionary. The dictionary is regarded as the standard reference on Australian English.

Macquarie University has a research partnership with the University of Hamburg in Germany and Fudan University in China. They offer dual and joint degree programs and engage in joint research.

Macquarie University (MQ) world rankings includes it being number 237 on the QS rankings, number 251+ on Times (THE), number 151+ on ARWU, and number 267= with US News. This contributes to Macquarie being the number 8 ranked Australian university overall in the world ranking systems. Macquarie University rankings within Australia include being placed at number 8 on the ERA scale (2012) and being a 4 1/2 Star AEN rated university. Macquarie also has a student survey satisfaction rating of 77.4% for business, 90.3% for health, 91.4% for arts, and 93.8% for science. Macquarie is ranked in the top 40 universities in the Asia-Pacific region and within Australia's top 12 universities according to the Academic Ranking of World Universities, the U.S. News & World Report Rankings and the QS World University Rankings. Macquarie was the highest ranked university in Australia under the age of 50 and was ranked 18th in the world (prior to its golden jubilee in 2014), according to the QS World University Rankings.

Internationally, Macquarie was ranked 239th in the world (9th in Australia) in the Academic Ranking of World Universities of 2014. Macquarie University was ranked among the top 50 universities in the world for linguistics (43rd), psychology (48th) and earth and marine sciences (48th), and was ranked in the top 5 nationally for philosophy and earth and marine sciences, according to the 2014 QS World University Rankings.

Macquarie ranked 67th in the world for Arts and Humanities (equal 5th in Australia), according to the 2015 Times Higher Education rankings by subject and 54th in the world for arts and humanities, according to the 2017 USNWR rankings by subject. Arts and Humanities is Macquarie's best discipline area in rankings. Macquarie was one of four non-Group of Eight universities ranked in the top 100 universities in the world in particular discipline areas.

The Macquarie Graduate School of Management is one of the oldest business schools in Australia. In 2014, "The Economist" ranked MGSM 5th in the Asia-Pacific, 3rd in Australia, 1st in Sydney/New South Wales and 49th in the world. It was the highest ranked business school in Australia and was ranked 68th in the world in the 2015 "Financial Times" MBA ranking.

Macquarie is the fourth largest university in Sydney (38,753 students in 2013). The university has the largest student exchange programme in Australia.

In 2012, 9,802 students from Asia were enrolled at Macquarie University (Sydney campuses and offshore programs in China, Hong Kong, Korea and Singapore).

Campus Life manages the university's non-academic services: food and retail, sport and recreation, student groups, child care, and entertainment. From late 2017 onward its Campus Hub facility has been closed for reconstruction; a 'pop-up'-style replacement, the Campus Common, has been opened for the duration.

The Global Leadership Program (GLP) is a University-funded extracurricular program that is open to all students and can be undertaken alongside any degree at Macquarie University. The GLP aims to instil leadership and innovation skills, cross-cultural understanding and a sense of global citizenship in its graduates. Upon successful completion of the GLP, students receive a formal notation on their academic transcript and a certificate. 

Macquarie's GLP was the first of its kind when it launched in the Australian university sector in 2005 and is the country's flagship tertiary global leadership program with more than 4000 active participants in more than 200 academic disciplines. GLP is a voluntary, extra-curricular learning and engagement program that students design according to their own interests and complete at their own pace. Students are required to complete a workshop series, attend tailored keynote speaker and networking events and complete an experiential credit component. This ranges from short-term study abroad, volunteering (domestic and/or international), internships (domestic and/or international), learning a new language or attending internationally themed seminars and study tours.

The GLP won the Institute for International Education's 2017 Heiskell award for Innovation in International Education - Internationalising the Campus. Macquarie University is the first Southern Hemisphere university to receive the award in its 17-year history. 
The GLP was awarded the 2018 NSW International Student Community Engagement Award (Joint Winner) in the Education Provider category. This award recognises the innovative way in which the GLP facilitates connection and engagement with community for Macquarie University International GLP Students, and also recognises the contribution that the GLP makes to the International Student experience in New South Wales. In 2019, the GLP won the Global PIEoneer Award for International Education in the category of 'Progressive Education Delivery' in Guildhall, London. The PIEoneer Awards are the only global awards that celebrate innovation and achievement across the whole of the international education industry.

Macquarie University has its own community radio station on campus, 2SER FM. The station is jointly owned by Macquarie University and University of Technology, Sydney.

Macquarie University students celebrate Conception Day each year since 1969 to – according to legend – commemorate the date of conception of Lachlan Macquarie, as his birthday fell at the wrong time of year for a celebration. Conception Day is traditionally held on the last day of classes before the September mid-semester break.

Alumni include Rhodes and John Monash Scholars and several Fulbright Scholars.

Notable alumni include: Australian politician and former Lord Mayor of Brisbane, Jim Soorley; Australian politician, Tanya Plibersek; Australian basketball player, Lauren Jackson; Australian swimmer, Ian Thorpe; Australian water polo player, Holly Lincoln-Smith; three founding members of the Australian children's musical group The Wiggles (Murray Cook, Anthony Field, Greg Page); former Director-General of the National Library of Australia, Anne-Marie Schwirtlich AM; New Zealand conservationist, Pete Bethune.

Notable alumni in science include: Australian scientist Barry Brook, American physicist Frank Duarte, and Australian physicist Cathy Foley. Alumni notable in the business world include: Australian hedge fund manager Greg Coffey, Australian businesswoman Catherine Livingstone, founder of Freelancer.com Matt Barrie, businessman Napoleon Perdis and Australian venture capitalist Larry R. Marshall.

Notable faculty members include: Indian neurosurgeon, B. K. Misra
Australian writer and four time Miles Franklin Award winner, Thea Astley; Hungarian Australian mathematician, Esther Szekeres; Australian mathematician, Neil Trudinger; Australian composer, Phillip Wilcher; Australian environmentalist and activist, Tim Flannery; British physicist and author, Paul Davies; British-Australian physicist, John Clive Ward; Israeli-Australian mathematician, José Enrique Moyal; Australian linguist, Geoffrey Hull; Australian geologist, Fellow of the Australian Academy of Science, John Veevers; Australian climatologist, Ann Henderson-Sellers; Australian sociologist, Raewyn Connell.

Four Macquarie University academics were included in The World's Most Influential Minds 2014 report by Thomson Reuters, which identified the most highly cited researchers of the last 11 years.




</doc>
<doc id="19736" url="https://en.wikipedia.org/wiki?curid=19736" title="Muspelheim">
Muspelheim

In Norse cosmology, Muspelheim (), also called Muspell (), is a realm of fire.

The etymology of "Muspelheim" is uncertain, but may come from "Mund-spilli", "world-destroyers", "wreck of the world".

According to Norse myth there is said to be nine worlds. The Poetic Edda section Völuspá tells of them:

Muspelheim is described as a hot and glowing land of fire, home to the fire giants, and guarded by Surtr, with his flaming sword. It is featured in both the creation and destruction stories of Norse myth. According to the Prose Edda, A great time before the earth was made, Niflheim existed. Inside Niflheim was a well called Hvergelmer, from this well flowed numerous streams known as the Elivog. Their names were Svol, Gunnthro, Form, Finbul, Thul, Slid and Hrid, Sylg and Ylg, Vid, Leipt and Gjoll. After a time these streams had traveled far from their source at Niflheim. So far that the venom that flowed within them hardened and turned to ice. When this ice eventually settled, rain rose up from it, and froze into rime. This ice then began to layer itself over the primordial void, Ginungagap. This made the northern portion of Ginungagap thick with ice, and storms begin to form within. However, in the southern region of Ginungagap glowing sparks were flying out of Muspelheim. When the heat and sparks from Muspelheim met the ice, it began to melt. These sparks would go on to create the Sun, Moon, and stars, and the drops would form the primeval being Ymir. "by the might of him who sent the heat, the drops quickened into life and took the likeness of a man, who got the name Ymer. But the Frost giants call him Aurgelmer"

The "Prose Edda" section "Gylfaginning" foretells that the sons of Muspell will break the Bifröst bridge as part of the events of Ragnarök:



</doc>
<doc id="19737" url="https://en.wikipedia.org/wiki?curid=19737" title="Maxwell's equations">
Maxwell's equations

Maxwell's equations are a set of coupled partial differential equations that, together with the Lorentz force law, form the foundation of classical electromagnetism, classical optics, and electric circuits. The equations provide a mathematical model for electric, optical, and radio technologies, such as power generation, electric motors, wireless communication, lenses, radar etc. Maxwell's equations describe how electric and magnetic fields are generated by charges, currents, and changes of the fields. An important consequence of the equations is that they demonstrate how fluctuating electric and magnetic fields propagate at a constant speed ("c") in a vacuum. Known as electromagnetic radiation, these waves may occur at various wavelengths to produce a spectrum of light from radio waves to γ-rays. The equations are named after the physicist and mathematician James Clerk Maxwell, who published an early form of the equations that included the Lorentz force law between 1861 and 1862. Maxwell first used the equations to propose that light is an electromagnetic phenomenon.

The equations have two major variants. The microscopic Maxwell equations have universal applicability but are unwieldy for common calculations. They relate the electric and magnetic fields to total charge and total current, including the complicated charges and currents in materials at the atomic scale. The "macroscopic" Maxwell equations define two new auxiliary fields that describe the large-scale behaviour of matter without having to consider atomic scale charges and quantum phenomena like spins. However, their use requires experimentally determined parameters for a phenomenological description of the electromagnetic response of materials.

The term "Maxwell's equations" is often also used for equivalent alternative formulations. Versions of Maxwell's equations based on the electric and magnetic potentials are preferred for explicitly solving the equations as a boundary value problem, analytical mechanics, or for use in quantum mechanics. The covariant formulation (on spacetime rather than space and time separately) makes the compatibility of Maxwell's equations with special relativity manifest. Maxwell's equations in curved spacetime, commonly used in high energy and gravitational physics, are compatible with general relativity. In fact, Einstein developed special and general relativity to accommodate the invariant speed of light, a consequence of Maxwell's equations, with the principle that only relative movement has physical consequences.

Since the mid-20th century, it has been understood that Maxwell's equations are not exact, but a classical limit of the fundamental theory of quantum electrodynamics.

Gauss's law describes the relationship between a static electric field and the electric charges that cause it: a static electric field points away from positive charges and towards negative charges, and the net outflow of the electric field through any closed surface is proportional to the charge enclosed by the surface. Picturing the electric field by its field lines, this means the field lines begin at positive electric charges and end at negative electric charges. 'Counting' the number of field lines passing through a closed surface yields the total charge (including bound charge due to polarization of material) enclosed by that surface, divided by dielectricity of free space (the vacuum permittivity).

Gauss's law for magnetism states that there are no "magnetic charges" (also called magnetic monopoles), analogous to electric charges. Instead, the magnetic field due to materials is generated by a configuration called a dipole, and the net outflow of the magnetic field through any closed surface is zero. Magnetic dipoles are best represented as loops of current but resemble positive and negative 'magnetic charges', inseparably bound together, having no net 'magnetic charge'. In terms of field lines, this equation states that magnetic field lines neither begin nor end but make loops or extend to infinity and back. In other words, any magnetic field line that enters a given volume must somewhere exit that volume. Equivalent technical statements are that the sum total magnetic flux through any Gaussian surface is zero, or that the magnetic field is a solenoidal vector field.

The Maxwell–Faraday version of Faraday's law of induction describes how a time varying magnetic field creates ("induces") an electric field. In integral form, it states that the work per unit charge required to move a charge around a closed loop equals the rate of change of the magnetic flux through the enclosed surface.

The dynamically induced electric field has closed field lines similar to a magnetic field, unless superposed by a static (charge induced) electric field. This aspect of electromagnetic induction is the operating principle behind many electric generators: for example, a rotating bar magnet creates a changing magnetic field, which in turn generates an electric field in a nearby wire.

Ampère's law with Maxwell's addition states that magnetic fields can be generated in two ways: by electric current (this was the original "Ampère's law") and by changing electric fields (this was "Maxwell's addition", which he called displacement current). In integral form, the magnetic field induced around any closed loop is proportional to the electric current plus displacement current (proportional to the rate of change of electric flux) through the enclosed surface.

Maxwell's addition to Ampère's law is particularly important: it makes the set of equations mathematically consistent for non static fields, without changing the laws of Ampere and Gauss for static fields. However, as a consequence, it predicts that a changing magnetic field induces an electric field and vice versa. Therefore, these equations allow self-sustaining "electromagnetic waves" to travel through empty space (see electromagnetic wave equation).

The speed calculated for electromagnetic waves, which could be predicted from experiments on charges and currents, exactly matches the speed of light; indeed, light "is" one form of electromagnetic radiation (as are X-rays, radio waves, and others). Maxwell understood the connection between electromagnetic waves and light in 1861, thereby unifying the theories of electromagnetism and optics.

In the electric and magnetic field formulation there are four equations that determine the fields for given charge and current distribution. A separate law of nature, the Lorentz force law, describes how, conversely, the electric and magnetic fields act on charged particles and currents. A version of this law was included in the original equations by Maxwell but, by convention, is included no longer. The vector calculus formalism below, the work of Oliver Heaviside, has become standard. It is manifestly rotation invariant, and therefore mathematically much more transparent than Maxwell's original 20 equations in x,y,z components. The relativistic formulations are even more symmetric and manifestly Lorentz invariant. For the same equations expressed using tensor calculus or differential forms, see alternative formulations.

The differential and integral formulations are mathematically equivalent and are both useful. The integral formulation relates fields within a region of space to fields on the boundary and can often be used to simplify and directly calculate fields from symmetric distributions of charges and currents. On the other hand, the differential equations are purely "local" and are a more natural starting point for calculating the fields in more complicated (less symmetric) situations, for example using finite element analysis.

Symbols in bold represent vector quantities, and symbols in "italics" represent scalar quantities, unless otherwise indicated.
The equations introduce the electric field, , a vector field, and the magnetic field, , a pseudovector field, each generally having a time and location dependence.
The sources are

The universal constants appearing in the equations (the first two ones explicitly only in the SI units formulation) are:

In the differential equations, 

In the integral equations, 
Here a "fixed" volume or surface means that it does not change over time.
The equations are correct, complete, and a little easier to interpret with time-independent surfaces. For example, since the surface is time-independent, we can bring the differentiation under the integral sign in Faraday's law:
Maxwell's equations can be formulated with possibly time-dependent surfaces and volumes by using the differential version and using Gauss and Stokes formula appropriately. 

The definitions of charge, electric field, and magnetic field can be altered to simplify theoretical calculation, by absorbing dimensioned factors of and into the units of calculation, by convention. With a corresponding change in convention for the Lorentz force law this yields the same physics, i.e. trajectories of charged particles, or work done by an electric motor. These definitions are often preferred in theoretical and high energy physics where it is natural to take the electric and magnetic field with the same units, to simplify the appearance of the electromagnetic tensor: the Lorentz covariant object unifying electric and magnetic field would then contain components with uniform unit and dimension. Such modified definitions are conventionally used with the Gaussian (CGS) units. Using these definitions and conventions, colloquially "in Gaussian units",
the Maxwell equations become:

The equations are particularly readable when length and time are measured in compatible units like seconds and lightseconds i.e. in units such that c = 1 unit of length/unit of time. Ever since 1983 (see International System of Units), metres and seconds are compatible except for historical legacy since "by definition" c = 299 792 458 m/s (≈ 1.0 feet/nanosecond).

Further cosmetic changes, called rationalisations, are possible by absorbing factors of depending on whether we want Coulomb's law or Gauss's law to come out nicely, see Lorentz-Heaviside units (used mainly in particle physics). In theoretical physics it is often useful to choose units such that Planck's constant, the elementary charge, and even Newton's constant are 1. See Planck units.

The equivalence of the differential and integral formulations are a consequence of the Gauss divergence theorem and the Kelvin–Stokes theorem.

According to the (purely mathematical) Gauss divergence theorem, the electric flux through the 
boundary surface can be rewritten as
The integral version of Gauss's equation can thus be rewritten as 
Since is arbitrary (e.g. an arbitrary small ball with arbitrary center), this is satisfied iff the integrand is zero. This is 
the differential equations formulation of Gauss equation up to a trivial rearrangement.

Similarly rewriting the magnetic flux in Gauss's law for magnetism in integral form gives 
which is satisfied for all iff formula_9.

By the Kelvin–Stokes theorem we can rewrite the line integrals of the fields around the closed boundary curve to an integral of the "circulation of the fields" (i.e. their curls) over a surface it bounds, i.e.

Hence the modified Ampere law in integral form can be rewritten as 
Since can be chosen arbitrarily, e.g. as an arbitrary small, arbitrary oriented, and arbitrary centered disk, we conclude that the integrand is zero iff Ampere's modified law in differential equations form is satisfied.
The equivalence of Faraday's law in differential and integral form follows likewise.

The line integrals and curls are analogous to quantities in classical fluid dynamics: the circulation of a fluid is the line integral of the fluid's flow velocity field around a closed loop, and the vorticity of the fluid is the curl of the velocity field.

The invariance of charge can be derived as a corollary of Maxwell's equations. The left-hand side of the modified Ampere's Law has zero divergence by the div–curl identity. Expanding the divergence of the right-hand side, interchanging derivatives, and applying Gauss's law gives:

i.e.

By the Gauss Divergence Theorem, this means the rate of change of charge in a fixed volume equals the net current flowing through the boundary:
In particular, in an isolated system the total charge is conserved.

In a region with no charges () and no currents (), such as in a vacuum, Maxwell's equations reduce to:

Taking the curl of the curl equations, and using the curl of the curl identity we obtain

The quantity formula_16 has the dimension of (time/length). Defining
formula_17, the equations above have the form of the standard wave equations

Already during Maxwell's lifetime, it was found that the known values for formula_19 and formula_20 give formula_21, then already known to be the speed of light in free space. This led him to propose that light and radio waves were propagating electromagnetic waves, since amply confirmed. In the old SI system of units, the values of formula_22 and formula_23 are defined constants, (which means that by definition formula_24) that define the ampere and the metre. In the new SI system, only "c" keeps its defined value, and the electron charge gets a defined value. 
In materials with relative permittivity, , and relative permeability, , the phase velocity of light becomes

which is usually less than .

In addition, and are perpendicular to each other and to the direction of wave propagation, and are in phase with each other. A sinusoidal plane wave is one special solution of these equations. Maxwell's equations explain how these waves can physically propagate through space. The changing magnetic field creates a changing electric field through Faraday's law. In turn, that electric field creates a changing magnetic field through Maxwell's addition to Ampère's law. This perpetual cycle allows these waves, now known as electromagnetic radiation, to move through space at velocity .

The above equations are the "microscopic" version of Maxwell's equations, expressing the electric and the magnetic fields in terms of the (possibly atomic-level) charges and currents present. This is sometimes called the "general" form, but the macroscopic version below is equally general, the difference being one of bookkeeping.

The microscopic version is sometimes called "Maxwell's equations in a vacuum": this refers to the fact that the material medium is not built into the structure of the equations, but appears only in the charge and current terms. The microscopic version was introduced by Lorentz, who tried to use it to derive the macroscopic properties of bulk matter from its microscopic constituents.

"Maxwell's macroscopic equations", also known as Maxwell's equations in matter, are more similar to those that Maxwell introduced himself.

In the "macroscopic" equations, the influence of bound charge and bound current is incorporated into the displacement field and the magnetizing field , while the equations depend only on the free charges and free currents . This reflects a splitting of the total electric charge "Q" and current "I" (and their densities "ρ" and J) into free and bound parts:

The cost of this splitting is that the additional fields and need to be determined through phenomenological constituent equations relating these fields to the electric field and the magnetic field , together with the bound charge and current.

See below for a detailed description of the differences between the microscopic equations, dealing with "total" charge and current including material contributions, useful in air/vacuum;
and the macroscopic equations, dealing with "free" charge and current, practical to use within materials.

When an electric field is applied to a dielectric material its molecules respond by forming microscopic electric dipoles – their atomic nuclei move a tiny distance in the direction of the field, while their electrons move a tiny distance in the opposite direction. This produces a "macroscopic" "bound charge" in the material even though all of the charges involved are bound to individual molecules. For example, if every molecule responds the same, similar to that shown in the figure, these tiny movements of charge combine to produce a layer of positive bound charge on one side of the material and a layer of negative charge on the other side. The bound charge is most conveniently described in terms of the polarization of the material, its dipole moment per unit volume. If is uniform, a macroscopic separation of charge is produced only at the surfaces where enters and leaves the material. For non-uniform , a charge is also produced in the bulk.

Somewhat similarly, in all materials the constituent atoms exhibit magnetic moments that are intrinsically linked to the angular momentum of the components of the atoms, most notably their electrons. The connection to angular momentum suggests the picture of an assembly of microscopic current loops. Outside the material, an assembly of such microscopic current loops is not different from a macroscopic current circulating around the material's surface, despite the fact that no individual charge is traveling a large distance. These "bound currents" can be described using the magnetization .

The very complicated and granular bound charges and bound currents, therefore, can be represented on the macroscopic scale in terms of and , which average these charges and currents on a sufficiently large scale so as not to see the granularity of individual atoms, but also sufficiently small that they vary with location in the material. As such, "Maxwell's macroscopic equations" ignore many details on a fine scale that can be unimportant to understanding matters on a gross scale by calculating fields that are averaged over some suitable volume.

The "definitions" (not constitutive relations) of the auxiliary fields are:

where is the polarization field and is the magnetization field, which are defined in terms of microscopic bound charges and bound currents respectively. The macroscopic bound charge density and bound current density in terms of polarization and magnetization are then defined as

If we define the total, bound, and free charge and current density by

and use the defining relations above to eliminate , and , the "macroscopic" Maxwell's equations reproduce the "microscopic" equations.

In order to apply 'Maxwell's macroscopic equations', it is necessary to specify the relations between displacement field and the electric field , as well as the magnetizing field and the magnetic field . Equivalently, we have to specify the dependence of the polarization (hence the bound charge) and the magnetization (hence the bound current) on the applied electric and magnetic field. The equations specifying this response are called constitutive relations. For real-world materials, the constitutive relations are rarely simple, except approximately, and usually determined by experiment. See the main article on constitutive relations for a fuller description.

For materials without polarization and magnetization, the constitutive relations are (by definition)
where is the permittivity of free space and the permeability of free space. Since there is no bound charge, the total and the free charge and current are equal.

An alternative viewpoint on the microscopic equations is that they are the macroscopic equations "together" with the statement that vacuum behaves like a perfect linear "material" without additional polarization and magnetization.
More generally, for linear materials the constitutive relations are
where is the permittivity and the permeability of the material. For the displacement field the linear approximation is usually excellent because for all but the most extreme electric fields or temperatures obtainable in the laboratory (high power pulsed lasers) the interatomic electric fields of materials of the order of 10 V/m are much higher than the external field. For the magnetizing field formula_32, however, the linear approximation can break down in common materials like iron leading to phenomena like hysteresis. Even the linear case can have various complications, however.

Even more generally, in the case of non-linear materials (see for example nonlinear optics), and are not necessarily proportional to , similarly or is not necessarily proportional to . In general and depend on both and , on location and time, and possibly other physical quantities.

In applications one also has to describe how the free currents and charge density behave in terms of and possibly coupled to other physical quantities like pressure, and the mass, number density, and velocity of charge-carrying particles. E.g., the original equations given by Maxwell (see History of Maxwell's equations) included Ohm's law in the form

Following is a summary of some of the numerous other mathematical formalisms to write the microscopic Maxwell's equations, with the columns separating the two homogeneous Maxwell equations from the two inhomogeneous ones involving charge and current. Each formulation has versions directly in terms of the electric and magnetic fields, and indirectly in terms of the electrical potential and the vector potential . Potentials were introduced as a convenient way to solve the homogeneous equations, but it was thought that all observable physics was contained in the electric and magnetic fields (or relativistically, the Faraday tensor). The potentials play a central role in quantum mechanics, however, and act quantum mechanically with observable consequences even when the electric and magnetic fields vanish (Aharonov–Bohm effect).

Each table describes one formalism. See the main article for details of each formulation. SI units are used throughout.

The Maxwell equations can also be formulated on a spacetime-like Minkowski space where space and time are treated on equal footing. The direct spacetime formulations make manifest that the Maxwell equations are relativistically invariant. Because of this symmetry electric and magnetic field are treated on equal footing and are recognised as components of the Faraday tensor. This reduces the four Maxwell equations to two, which simplifies the equations, although we can no longer use the familiar vector formulation. In fact the Maxwell equations in the space + time formulation are not Galileo invariant and have Lorentz invariance as a hidden symmetry. This was a major source of inspiration for the development of relativity theory. Indeed, even the formulation that treats space and time separately is not a non-relativistic approximation and describes the same physics by simply renaming variables. For this reason the relativistic invariant equations are usually called the Maxwell equations as well.

Each table describes one formalism.


Other formalisms include the geometric algebra formulation and a matrix representation of Maxwell's equations. Historically, a quaternionic formulation was used.

Maxwell's equations are partial differential equations that relate the electric and magnetic fields to each other and to the electric charges and currents. Often, the charges and currents are themselves dependent on the electric and magnetic fields via the Lorentz force equation and the constitutive relations. These all form a set of coupled partial differential equations which are often very difficult to solve: the solutions encompass all the diverse phenomena of classical electromagnetism. Some general remarks follow.

As for any differential equation, boundary conditions and initial conditions are necessary for a unique solution. For example, even with no charges and no currents anywhere in spacetime, there are the obvious solutions for which E and B are zero or constant, but there are also non-trivial solutions corresponding to electromagnetic waves. In some cases, Maxwell's equations are solved over the whole of space, and boundary conditions are given as asymptotic limits at infinity. In other cases, Maxwell's equations are solved in a finite region of space, with appropriate conditions on the boundary of that region, for example an artificial absorbing boundary representing the rest of the universe, or periodic boundary conditions, or walls that isolate a small region from the outside world (as with a waveguide or cavity resonator).

Jefimenko's equations (or the closely related Liénard–Wiechert potentials) are the explicit solution to Maxwell's equations for the electric and magnetic fields created by any given distribution of charges and currents. It assumes specific initial conditions to obtain the so-called "retarded solution", where the only fields present are the ones created by the charges. However, Jefimenko's equations are unhelpful in situations when the charges and currents are themselves affected by the fields they create.

Numerical methods for differential equations can be used to compute approximate solutions of Maxwell's equations when exact solutions are impossible. These include the finite element method and finite-difference time-domain method. For more details, see Computational electromagnetics.

Maxwell's equations "seem" overdetermined, in that they involve six unknowns (the three components of and ) but eight equations (one for each of the two Gauss's laws, three vector components each for Faraday's and Ampere's laws). (The currents and charges are not unknowns, being freely specifiable subject to charge conservation.) This is related to a certain limited kind of redundancy in Maxwell's equations: It can be proven that any system satisfying Faraday's law and Ampere's law "automatically" also satisfies the two Gauss's laws, as long as the system's initial condition does. This explanation was first introduced by Julius Adams Stratton in 1941. Although it is possible to simply ignore the two Gauss's laws in a numerical algorithm (apart from the initial conditions), the imperfect precision of the calculations can lead to ever-increasing violations of those laws. By introducing dummy variables characterizing these violations, the four equations become not overdetermined after all. The resulting formulation can lead to more accurate algorithms that take all four laws into account.

Both identities formula_37, which reduce eight equations to six independent ones, are the true reason of overdetermination.

Maxwell's equations and the Lorentz force law (along with the rest of classical electromagnetism) are extraordinarily successful at explaining and predicting a variety of phenomena; however they are not exact, but a classical limit of quantum electrodynamics (QED).

Some observed electromagnetic phenomena are incompatible with Maxwell's equations. These include photon–photon scattering and many other phenomena related to photons or virtual photons, "nonclassical light" and quantum entanglement of electromagnetic fields (see quantum optics). E.g. quantum cryptography cannot be described by Maxwell theory, not even approximately. The approximate nature of Maxwell's equations becomes more and more apparent when going into the extremely strong field regime (see Euler–Heisenberg Lagrangian) or to extremely small distances.

Finally, Maxwell's equations cannot explain any phenomenon involving individual photons interacting with quantum matter, such as the photoelectric effect, Planck's law, the Duane–Hunt law, and single-photon light detectors. However, many such phenomena may be approximated using a halfway theory of quantum matter coupled to a classical electromagnetic field, either as external field or with the expected value of the charge current and density on the right hand side of Maxwell's equations.

Popular variations on the Maxwell equations as a classical theory of electromagnetic fields are relatively scarce because the standard equations have stood the test of time remarkably well.

Maxwell's equations posit that there is electric charge, but no magnetic charge (also called magnetic monopoles), in the universe. Indeed, magnetic charge has never been observed, despite extensive searches, and may not exist. If they did exist, both Gauss's law for magnetism and Faraday's law would need to be modified, and the resulting four equations would be fully symmetric under the interchange of electric and magnetic fields.


The developments before relativity:






</doc>
<doc id="19738" url="https://en.wikipedia.org/wiki?curid=19738" title="Metrization theorem">
Metrization theorem

In topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_1 is said to be metrizable if there is a metric 

such that the topology induced by "d" is formula_3. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.

Metrizable spaces inherit all topological properties from metric spaces. For example, they are Hausdorff paracompact spaces (and hence normal and Tychonoff) and first-countable. However, some properties of the metric, such as completeness, cannot be said to be inherited. This is also true of other structures linked to the metric. A metrizable uniform space, for example, may have a different set of contraction maps than a metric space to which it is homeomorphic.

One of the first widely recognized metrization theorems was Urysohn's metrization theorem. This states that every Hausdorff second-countable regular space is metrizable. So, for example, every second-countable manifold is metrizable. (Historical note: The form of the theorem shown here was in fact proved by Tychonoff in 1926. What Urysohn had shown, in a paper published posthumously in 1925, was that every second-countable "normal" Hausdorff space is metrizable). The converse does not hold: there exist metric spaces that are not second countable, for example, an uncountable set endowed with the discrete metric. The Nagata–Smirnov metrization theorem, described below, provides a more specific theorem where the converse does hold.

Several other metrization theorems follow as simple corollaries to Urysohn's theorem. For example, a compact Hausdorff space is metrizable if and only if it is second-countable.

Urysohn's Theorem can be restated as: A topological space is separable and metrizable if and only if it is regular, Hausdorff and second-countable. The Nagata–Smirnov metrization theorem extends this to the non-separable case. It states that a topological space is metrizable if and only if it is regular, Hausdorff and has a σ-locally finite base. A σ-locally finite base is a base which is a union of countably many locally finite collections of open sets. For a closely related theorem see the Bing metrization theorem.

Separable metrizable spaces can also be characterized as those spaces which are homeomorphic to a subspace of the Hilbert cube formula_4, i.e. the countably infinite product of the unit interval (with its natural subspace topology from the reals) with itself, endowed with the product topology.

A space is said to be locally metrizable if every point has a metrizable neighbourhood. Smirnov proved that a locally metrizable space is metrizable if and only if it is Hausdorff and paracompact. In particular, a manifold is metrizable if and only if it is paracompact.

The group of unitary operators formula_5 on a separable Hilbert space formula_6 endowed
with the strong operator topology is metrizable (see Proposition II.1 in ).

Non-normal spaces cannot be metrizable; important examples include

The real line with the lower limit topology is not metrizable. The usual distance function is not a metric on this space because the topology it determines is the usual topology, not the lower limit topology. This space is Hausdorff, paracompact and first countable.

The long line is locally metrizable but not metrizable; in a sense it is "too long".



</doc>
<doc id="19739" url="https://en.wikipedia.org/wiki?curid=19739" title="Martin Agricola">
Martin Agricola

Martin Agricola (6 January 1486 – 10 June 1556) was a German composer of Renaissance music and a music theorist.

Agricola was born in Schwiebus in Lebusz.

From 1524 until his death he lived at Magdeburg, where he occupied the post of teacher or cantor in the Protestant school. The senator and music-printer Georg Rhau, of Wittenberg, was a close friend of Agricola, whose theoretical works, providing valuable material concerning the change from the old to the new system of notation, he published.

Among Agricola's other theoretical works is "Musica instrumentalis deudsch" (1528 and 1545), a study of musical instruments, and one of the most important works in early organology; and one of the earliest books on the Rudiments of music.

Agricola was also the first to harmonize in four parts Martin Luther's chorale, "Ein feste Burg".




</doc>
<doc id="19740" url="https://en.wikipedia.org/wiki?curid=19740" title="Max August Zorn">
Max August Zorn

Max August Zorn (; June 6, 1906 – March 9, 1993) was a German mathematician. He was an algebraist, group theorist, and numerical analyst. He is best known for Zorn's lemma, a method used in set theory that is applicable to a wide range of mathematical constructs such as vector spaces, ordered sets and the like. Zorn's lemma was first postulated by Kazimierz Kuratowski in 1922, and then independently by Zorn in 1935.

Zorn was born in Krefeld, Germany. He attended the University of Hamburg. He received his Ph.D. in April 1930 for a thesis on alternative algebras. He published his findings in "Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg". Zorn showed that split-octonions could be represented by a mixed-style of matrices called Zorn's vector-matrix algebra.

Max Zorn was appointed as an assistant at the University of Halle. However, he did not have the opportunity to work there for long since he was forced to leave Germany in 1933 because of the Nazi policies. According to grandson Eric, "[Max] spoke with a raspy, airy voice most of his life. Few people knew why, because he only told the story after significant prodding, but he talked that way because pro-Hitler thugs who objected to his politics, had battered his throat in a 1933 street fight."

Zorn emigrated to the U.S. and was appointed a Sterling Fellow at Yale University. While at Yale, Zorn wrote his paper "A Remark on Method in Transfinite Algebra" that stated his Maximum Principle, later called Zorn's lemma. It requires a set that contains the union of any chain of subsets to have one chain not contained in any other, called the maximal element. He illustrated the principle with applications in ring theory and field extensions. Zorn’s lemma is an alternative expression of the axiom of choice, and thus a subject of interest in axiomatic set theory.

In 1936 he moved to UCLA and remained until 1946. While at UCLA Zorn revisited his study of alternative rings and proved the existence of the nilradical of certain alternative rings. According to Angus E. Taylor, Max was his most stimulating colleague at UCLA.

In 1946 Zorn became a professor at Indiana University where he taught until retiring in 1971. He was thesis advisor for Israel Nathan Herstein.

Zorn died in Bloomington, Indiana, United States, in March 1993, of congestive heart failure, according to his obituary in "The New York Times".

Max Zorn married Alice Schlottau and they had one son, Jens, and one daughter, Liz. Jens (born June 19, 1931) is an emeritus professor of physics at the University of Michigan and an accomplished sculptor. Max Zorn's grandson Eric Zorn is a columnist for the "Chicago Tribune".





</doc>
<doc id="19745" url="https://en.wikipedia.org/wiki?curid=19745" title="Main (river)">
Main (river)

The Main () is a river in Germany. With a length of (including its 52 km long source river White Main), it is the longest right tributary of the Rhine. It is also the longest river lying entirely in Germany (if the Weser and the Werra are considered as two separate rivers; together they are longer). The largest cities along the Main are Frankfurt am Main and Würzburg.

The mainspring of the Main River flows through the German states of Bavaria, Baden-Württemberg (forming the border with Bavaria for some distance) and Hesse. Its basin competes with the Danube for water; as a result, many of its boundaries are identical with those of the European Watershed.

The Main begins near Kulmbach in Franconia at the joining of its two headstreams, the Red Main ("Roter Main") and the White Main ("Weißer Main"). The Red Main originates in the Franconian Jura mountain range, in length, and runs through Creussen and Bayreuth. The White Main originates in the mountains of the Fichtelgebirge; it is long. In its upper and middle section, the Main runs through the valleys of the German Highlands. Its lower section crosses the Lower Main Lowlands (Hanau-Seligenstadt Basin and northern Upper Rhine Plain) to Wiesbaden, where it discharges into the Rhine. Major tributaries of the Main are the Regnitz, the Franconian Saale, the Tauber, and the Nidda.

The name ""Main"" derives from the Latin "Moenus" or "Menus". It is not related to the name of the city Mainz (Latin: "Moguntiacum").

The Main is navigable for shipping from its mouth at the Rhine close to Mainz for to Bamberg. Since 1992, the Main has been connected to the Danube via the Rhine-Main-Danube Canal and the highly regulated Altmühl river. The Main has been canalized with 34 large locks () to allow CEMT class V vessels () to navigate the total length of the river. The 16 locks in the adjacent Rhine-Main-Danube Canal and the Danube itself are of the same dimensions.

There are 34 dams and locks along the 380 km navigable portion of the Main, from the confluence with the Regnitz near Bamberg, to the Rhine.


Most of the dams along the Main also have turbines for power generation.

Tributaries from source to mouth:
Left

Right

Around Frankfurt are several large inland ports. Because the river is rather narrow on many of the upper reaches, navigation with larger vessels and push convoys requires great skill.

The largest cities along the Main are Frankfurt am Main and Würzburg. The Main also passes the following towns and cities: Burgkunstadt, Lichtenfels, Bad Staffelstein, Eltmann, Haßfurt, Schweinfurt, Volkach, Kitzingen, Marktbreit, Ochsenfurt, Karlstadt, Gemünden, Lohr, Marktheidenfeld, Wertheim, Miltenberg, Obernburg, Erlenbach/Main, Aschaffenburg, Seligenstadt, Hainburg, Hanau, Offenbach, Hattersheim, Flörsheim, and Rüsselsheim.

The river has gained enormous importance as a vital part of European "Corridor VII", the inland waterway link from the North Sea to the Black Sea.

In a historical and political sense, the Main line is referred to as the northern border of Southern Germany, with its predominantly Catholic population. The river roughly marked the southern border of the North German Federation, established in 1867 under Prussian leadership as the predecessor of the German Empire.

The river course also corresponds with the Speyer line isogloss between Central and Upper German dialects, sometimes mocked as "Weißwurstäquator".

The Main-Radweg is a major German bicycle path running along the Main River. It is approximately and was the first long-distance bicycle path to be awarded 5 stars by the General German Bicycle Club ADFC in 2008. It starts from either Creußen or Bischofsgrün and ends in Mainz.





</doc>
<doc id="19747" url="https://en.wikipedia.org/wiki?curid=19747" title="Marcus Vipsanius Agrippa">
Marcus Vipsanius Agrippa

Marcus Vipsanius Agrippa (; 64/62 BC – 12 BC) was a Roman consul, statesman, general and architect. He was a close friend, son-in-law, and lieutenant to Gaius Julius Caesar Octavianus and was responsible for the construction of some of the most notable buildings in the history of Rome and for important military victories, most notably at the Battle of Actium in 31 BC against the forces of Mark Antony and Cleopatra. As a result of these victories, Octavianus became the first Roman Emperor, adopting the name of Augustus. Agrippa assisted Augustus in making Rome "a city of marble" and renovating aqueducts to give all Romans, from every social class, access to the highest quality public services. He was responsible for the creation of many baths, porticoes and gardens, as well as the original Pantheon. Agrippa was also husband to Julia the Elder (who later married the second Emperor Tiberius), maternal grandfather to Caligula, and maternal great-grandfather to the Emperor Nero.

Agrippa was born between 64–62 BC, in an uncertain location. His father was called Lucius Vipsanius Agrippa. He had an elder brother whose name was also Lucius Vipsanius Agrippa, and a sister named Vipsania Polla. His family originated in the Italian countryside, and was of humble and plebeian origins. They had not been prominent in Roman public life. According to some scholars, including Victor Gardthausen, R. E. A. Palmer and David Ridgway, Agrippa's family was originally from Pisa in Etruria.

Agrippa was about the same age as Octavian (the future emperor Augustus), and the two were educated together and became close friends. Despite Agrippa's association with the family of Julius Caesar, his elder brother chose another side in the civil wars of the 40s BC, fighting under Cato against Caesar in Africa. When Cato's forces were defeated, Agrippa's brother was taken prisoner but freed after Octavian interceded on his behalf.

It is not known whether Agrippa fought against his brother in Africa, but he probably served in Caesar's campaign of 46–45 BC against Gnaeus Pompeius, which culminated in the Battle of Munda. Caesar regarded him highly enough to send him with Octavius in 45 BC to study in Apollonia (on the Illyrian coast) with the Macedonian legions, while Caesar consolidated his power in Rome. In the fourth month of their stay in Apollonia the news of Julius Caesar's assassination in March 44 BC reached them. Agrippa and another friend, Quintus Salvidienus Rufus, advised Octavius to march on Rome with the troops from Macedonia, but Octavius decided to sail to Italy with a small retinue. After his arrival, he learned that Caesar had adopted him as his legal heir. Octavius at this time took Caesar's name, but modern historians refer to him as "Octavian" during this period.

After Octavian's return to Rome, he and his supporters realised they needed the support of legions. Agrippa helped Octavian to levy troops in Campania. Once Octavian had his legions, he made a pact with Mark Antony and Lepidus, legally established in 43 BC as the Second Triumvirate. Octavian and his consular colleague Quintus Pedius arranged for Caesar's assassins to be prosecuted in their absence, and Agrippa was entrusted with the case against Gaius Cassius Longinus. It may have been in the same year that Agrippa began his political career, holding the position of Tribune of the Plebs, which granted him entry to the Senate.
In 42 BC, Agrippa probably fought alongside Octavian and Antony in the Battle of Philippi. After their return to Rome, he played a major role in Octavian's war against Lucius Antonius and Fulvia Antonia, respectively the brother and wife of Mark Antony, which began in 41 BC and ended in the capture of Perusia in 40 BC. However, Salvidienus remained Octavian's main general at this time. After the Perusine war, Octavian departed for Gaul, leaving Agrippa as urban praetor in Rome with instructions to defend Italy against Sextus Pompeius, an opponent of the Triumvirate who was now occupying Sicily. In July 40, while Agrippa was occupied with the Ludi Apollinares that were the praetor's responsibility, Sextus began a raid in southern Italy. Agrippa advanced on him, forcing him to withdraw. However, the Triumvirate proved unstable, and in August 40 both Sextus and Antony invaded Italy (but not in an organized alliance). Agrippa's success in retaking Sipontum from Antony helped bring an end to the conflict. Agrippa was among the intermediaries through whom Antony and Octavian agreed once more upon peace. During the discussions Octavian learned that Salvidienus had offered to betray him to Antony, with the result that Salvidienus was prosecuted and either executed or committed suicide. Agrippa was now Octavian's leading general.
In 39 or 38 BC, Octavian appointed Agrippa governor of Transalpine Gaul, where in 38 BC he put down a rising of the Aquitanians. He also fought the Germanic tribes, becoming the next Roman general to cross the Rhine after Julius Caesar. He was summoned back to Rome by Octavian to assume the consulship for 37 BC. He was well below the usual minimum age of 43, but Octavian had suffered a humiliating naval defeat against Sextus Pompey and needed his friend to oversee the preparations for further warfare. Agrippa refused the offer of a triumph for his exploits in Gaul – on the grounds, says Dio, that he thought it improper to celebrate during a time of trouble for Octavian. Since Sextus Pompeius had command of the sea on the coasts of Italy, Agrippa's first care was to provide a safe harbour for Octavian's ships. He accomplished this by cutting through the strips of land which separated the Lacus Lucrinus from the sea, thus forming an outer harbour, while joining the lake Avernus to the Lucrinus to serve as an inner harbor. The new harbor-complex was named Portus Julius in Octavian's honour. Agrippa was also responsible for technological improvements, including larger ships and an improved form of grappling hook. About this time, he married Caecilia Pomponia Attica, daughter of Cicero's friend Titus Pomponius Atticus.

In 36 BC, Octavian and Agrippa set sail against Sextus. The fleet was badly damaged by storms and had to withdraw; Agrippa was left in charge of the second attempt. Thanks to superior technology and training, Agrippa and his men won decisive victories at Mylae and Naulochus, destroying all but seventeen of Sextus' ships and compelling most of his forces to surrender. Octavian, with his power increased, forced the triumvir Lepidus into retirement and entered Rome in triumph. Agrippa received the unprecedented honour of a naval crown decorated with the beaks of ships; as Dio remarks, this was "a decoration given to nobody before or since".

Agrippa participated in smaller military campaigns in 35 and 34 BC, but by the autumn of 34 he had returned to Rome. He rapidly set out on a campaign of public repairs and improvements, including renovation of the aqueduct known as the Aqua Marcia and an extension of its pipes to cover more of the city. He became the first water commissioner of Rome in 33 BC. Through his actions after being elected in 33 BC as one of the aediles (officials responsible for Rome's buildings and festivals), the streets were repaired and the sewers were cleaned out, while lavish public spectacles were put on. Agrippa signalled his tenure of office by effecting great improvements in the city of Rome, restoring and building aqueducts, enlarging and cleansing the Cloaca Maxima, constructing baths and porticos, and laying out gardens. He also gave a stimulus to the public exhibition of works of art. It was unusual for an ex-consul to hold the lower-ranking position of aedile, but Agrippa's success bore out this break with tradition. As emperor, Augustus would later boast that "he had found the city of brick but left it of marble", thanks in part to the great services provided by Agrippa under his reign.

Agrippa was again called away to take command of the fleet when the war with Antony and Cleopatra broke out. He captured the strategically important city of Methone at the southwest of the Peloponnese, then sailed north, raiding the Greek coast and capturing Corcyra (modern Corfu). Octavian then brought his forces to Corcyra, occupying it as a naval base. Antony drew up his ships and troops at Actium, where Octavian moved to meet him. Agrippa meanwhile defeated Antony's supporter Quintus Nasidius in a naval battle at Patrae. Dio relates that as Agrippa moved to join Octavian near Actium, he encountered Gaius Sosius, one of Antony's lieutenants, who was making a surprise attack on the squadron of Lucius Tarius, a supporter of Octavian. Agrippa's unexpected arrival turned the battle around.

As the decisive battle approached, according to Dio, Octavian received intelligence that Antony and Cleopatra planned to break past his naval blockade and escape. At first he wished to allow the flagships past, arguing that he could overtake them with his lighter vessels and that the other opposing ships would surrender when they saw their leaders' cowardice. Agrippa objected, saying that Antony's ships, although larger, could outrun Octavian's if they hoisted sails, and that Octavian ought to fight now because Antony's fleet had just been struck by storms. Octavian followed his friend's advice.

On September 2, 31 BC, the Battle of Actium was fought. Octavian's victory, which gave him the mastery of Rome and the empire, was mainly due to Agrippa. Octavian then bestowed upon him the hand of his niece Claudia Marcella Major in 28 BC. He also served a second consulship with Octavian the same year. In 27 BC, Agrippa held a third consulship with Octavian, and in that year, the senate also bestowed upon Octavian the imperial title of Augustus.

In commemoration of the Battle of Actium, Agrippa built and dedicated the building that served as the Roman Pantheon before its destruction in 80 AD. Emperor Hadrian used Agrippa's design to build his own Pantheon, which survives in Rome. The inscription of the later building, which was built around 125, preserves the text of the inscription from Agrippa's building during his third consulship. The years following his third consulship, Agrippa spent in Gaul, reforming the provincial administration and taxation system, along with building an effective road system and aqueducts.

Agrippa's friendship with Augustus seems to have been clouded by the jealousy of Augustus' nephew and son-in-law Marcus Claudius Marcellus, which was probably instigated by the intrigues of Livia, the third wife of Augustus, who feared Agrippa's influence over her husband. Traditionally it is said the result of such jealousy was that Agrippa left Rome, ostensibly to take over the governorship of eastern provinces – a sort of honourable exile, but he only sent his legate to Syria, while he himself remained at Lesbos and governed by proxy, though he may have been on a secret mission to negotiate with the Parthians about the return of the Roman legions' standards which they held. On the death of Marcellus, which took place within a year of his exile, he was recalled to Rome by Augustus, who found he could not dispense with his services. However, if one places the events in the context of the crisis of 23 BC it seems unlikely that, when facing significant opposition and about to make a major political climb down, the emperor Augustus would place a man in exile in charge of the largest body of Roman troops. What is far more likely is that Agrippa's 'exile' was actually the careful political positioning of a loyal lieutenant in command of a significant army as a backup plan in case the settlement plans of 23 BC failed and Augustus needed military support. Moreover, after 23 BC as part of what became known as Augustus' "Second Constitutional Settlement", Agrippa's constitutional powers were greatly increased to provide the Principate of Augustus with greater constitutional stability by providing for a political heir or replacement for Augustus if he were to succumb to his habitual ill health or was assassinated. In the course of the year, proconsular imperium, similar to Augustus' power, was conferred upon Agrippa for five years. The exact nature of the grant is uncertain but it probably covered Augustus' imperial provinces, east and west, perhaps lacking authority over the provinces of the Senate. That was to come later, as was the jealously guarded tribunicia potestas, or powers of a tribune of the plebeians. These great powers of state are not usually heaped upon a former exile.
It is said that Maecenas advised Augustus to attach Agrippa still more closely to him by making him his son-in-law. He accordingly induced him to divorce Marcella and marry his daughter, Julia the Elder—the widow of Marcellus, equally celebrated for her beauty, abilities, and her shameless extravagance—by 21 BC. In 19 BC, Agrippa was employed in putting down a rising of the Cantabrians in Hispania (Cantabrian Wars).

In 18 BC, Agrippa's powers were even further increased to almost match those of Augustus. That year his proconsular imperium was augmented to cover the provinces of the Senate. More than that, he was finally granted tribunicia potestas, or powers of a tribune of the plebeians. As was the case with Augustus, Agrippa’s grant of tribunician powers was conferred without his having to actually hold that office. These powers were considerable, giving him veto power over the acts of the Senate or other magistracies, including those of other tribunes, and the power to present laws for approval by the People. Just as important, a tribune’s person was sacred, meaning that any person who harmfully touched them or impeded their actions, including political acts, could lawfully be killed. After the grant of these powers Agrippa was, on paper, almost as powerful as Augustus was. However, there was no doubt that Augustus was the man in charge.

Agrippa was appointed governor of the eastern provinces a second time in 17 BC, where his just and prudent administration won him the respect and good-will of the provincials, especially from the Jewish population. Agrippa also restored effective Roman control over the Cimmerian Chersonnese (Crimean Peninsula) during his governorship.

Agrippa’s last public service was his beginning of the conquest of the upper Danube River region, which would become the Roman province of Pannonia in 13 BC. He died at Campania in 12 BC at the age of 51. His posthumous son, Marcus Vipsanius Agrippa Postumus, was named in his honor. Augustus honoured his memory by a magnificent funeral and spent over a month in mourning. Augustus personally oversaw all of Agrippa's children’s educations. Although Agrippa had built a tomb for himself, Augustus had Agrippa's remains placed in Augustus' own mausoleum.

Agrippa was also known as a writer, especially on the subject of geography. Under his supervision, Julius Caesar's dream of having a complete survey of the Empire made was carried out. Agrippa constructed a circular chart, which was later engraved on marble by Augustus, and afterwards placed in the colonnade built by his sister Polla. Amongst his writings, an autobiography, now lost, is referenced.

Agrippa established a standard for Roman foot (Agrippa's own) in 29 BC, and thus a definition of a pace as 5 feet. An imperial Roman mile denotes 5,000 Roman feet.

The term Via Agrippa is used for any part of the network of roadways in Gaul built by Agrippa. Some of these still exist as paths or even as highways.

Agrippa had several children through his three marriages:

Through his numerous children, Agrippa would become ancestor to many subsequent members of the Julio-Claudian dynasty, whose position he helped to attain, as well as many other distinguished Romans.


There have been some attempts to assign further descendants to a number of the aforementioned figures, including two lines of Asinii descended from Gaius Asinius Pollio and Marcus Asinius Agrippa respectively. A daughter (and further descendants) named Rubellia Bassa to Julia, who may have been a daughter of Gaius Rubellius Blandus by an earlier marriage. And, finally, a series of descendants from Junia Lepida and her husband, Gaius Cassius Longinus. However, all of these lines of descent are extremely hypothetical and lack any evidence to support a connection to the descendants of Agrippa.
Agrippa is a character in William Shakespeare's play "Antony and Cleopatra".

A fictional version of Agrippa in his later life played a prominent role in the 1976 BBC Television series "I, Claudius". Agrippa was portrayed as a much older man, though he would have only been 39 years old at the time of the first episode (24/23 BC). He was played by John Paul.

Agrippa is the main character in Paul Naschy's 1980 film "Los cántabros", played by Naschy himself. It is a highly fictionalized version of the Cantabrian Wars in which Agrippa is depicted as the lover of the sister of Cantabrian leader Corocotta.

Agrippa appears in several film versions of the life of Cleopatra. He is normally portrayed as an old man rather than a young one. Among the people to portray him are Philip Locke, Alan Rowe and Andrew Keir.

Agrippa is also one of the principal characters in the British/Italian joint project "" (2003) featuring flashbacks between Augustus and Julia about Agrippa, which shows him in his youth on serving in Caesar's army up until his victory at Actium and the defeat of Cleopatra. He is portrayed by Ken Duken. In the 2005 series "Empire" the young Agrippa (played by Christopher Egan) becomes Octavian's sidekick after saving him from an attempted poisoning.

Marcus Agrippa, a highly fictional character based on Marcus Vipsanius Agrippa's early life, is part of the BBC-HBO-RAI television series "Rome". He is played by Allen Leech. He describes himself as the grandson of a slave. The series creates a romantic relationship between Agrippa and Octavian's sister Octavia Minor, for which there is no historical evidence.

Agrippa is mentioned by name in book VIII of Virgil's "The Aeneid", where Aeneas sees an image of Agrippa leading ships in the Battle of Actium on the shield forged for him by Vulcan and given to him by his mother, Venus.

Agrippa is a main character in the early part of Robert Graves' novel "I, Claudius". He is a main character in the later two novels of Colleen McCullough's Masters of Rome series. He is a featured character of prominence and importance in the historical fiction novel "Cleopatra's Daughter" by Michelle Moran. He also features prominently in John Edward Williams' historical novel "Augustus". In the backstory of "Gunpowder Empire", the first volume in Harry Turtledove's Crosstime Traffic alternate history series, Agrippa lived until AD 26, conquering all of Germania for the Empire and becoming the second Emperor when Augustus died in AD 14.

A heavily fictionalized version of Agrippa is one of the playable characters (the other being an equally fictionalized Augustus) in the video game "Shadow of Rome". There, Agrippa is sentenced to become a gladiator after his father was wrongly sentenced for assassinating Julius Caesar. Agrippa's goal is to stay alive as a gladiator for as long as possible, while Augustus acts as an infiltrator who slowly exposes the conspiracy against Caesar. Eventually, Augustus is able to prove Vipsanius' innocence and both of them are pardoned. Then a civil war breaks out, because the direct successor was outraged by exposure of the conspiracy. Agrippa and Augustus fight against Antonius. Agrippa appears as a Great Admiral in the computer game "Sid Meier's Civilization V". A fictionalized version of Agrippa also appears in the video game "Assassin's Creed Origins" as the commander of the Roman Citadel in the province of Kyrenaika where the player character has to kill him and retrieve a document from his body.





</doc>
<doc id="19757" url="https://en.wikipedia.org/wiki?curid=19757" title="Mariotto Albertinelli">
Mariotto Albertinelli

Mariotto di Bigio di Bindo Albertinelli (13 October 1474 – 5 November 1515) was an Italian Renaissance painter of the Florentine school. He was a close friend and collaborator of Fra Bartolomeo and their joint works appear as if they have been painted by one hand.

His work shows the influence of Perugino, Piero di Cosimo and Lorenzo di Credi as well as of Flemish painting. Some of Albertinelli's works reveal an eccentrically archaic tendency while others show a return to conventions of the early Renaissance.

Mariotto Albertinelli was born in Florence as the son of a gold beater. He was an only child and his mother died when he was just five years old. He was himself trained as a gold beater until the age of 12 when he became a pupil of Cosimo Rosselli and a fellow-pupil with Fra Bartolomeo. The two pupils formed such a close friendship that in 1494 they started a joint studio in Florence. After a while Albertinelli had mastered Fra Bartolomeo’s technique to such extent that he could paint in a style that blended with that of his partner. The closeness in style was such that for many years some doubts remained over who had painted certain works. For example, the Kress tondo, now in the Columbia Museum of Art was previously attributed to Fra Bartolomeo but is now thought to be the work of Albertinelli using the former's cartoon.
According to the early Italian biographer Vasari, Albertinelli was in the beginning of his career exclusively working for Alfonsina Orsini, the wife of Piero II de’ Medici and mother of Lorenzo II de' Medici. His early works were small paintings destined for the homes of sophisticated patrons. He produced these works independently of Fra Bartolommeo and as a result they are stylistically distinguishable. Piero di Cosimo who worked in Cosimo Rosselli’s workshop introduced Albertinelli to Flemish techniques.

From around 1500 Fra Bartolomeo renounced painting for a few years in the wake of Savonarola's morality campaign. He joined the Dominican order as Fra Bartolomeo. Albertinelli then worked as an independent painter. He received various commissions including in 1503 for the high altarpiece for the Chapel of Congregazione di San Martino (later Church of Santa Elisabetta) in Florence (now in the Uffizi). The central panel depicts the Visitation and the predella the Circumcision, the Adoration of the Child and the Annunciation. The subject depicts the story in the Gospel of Luke when Elizabeth, the cousin of the Virgin Mary visits Mary, who is, like herself, pregnant. Elizabeth features prominently in this episode and this subject was therefore chosen for a church dedicated to her. While the figural composition of the Visitation was derived from designs by Fra Bartolomeo, Albertinelli changed elements in these designs by replacing Fra Bartolomeo’s strong contrasts in light and dark with smoother gradations of tone. The work also shows the influence of Perugino in its use of soft highlights and its inclusion of a classical arcade.
In 1509 Albertinelli and Fra Bartolomeo, who had by then resumed painting, entered again into a partnership. Their partnership was on an equal footing and entitled each to half the profit of a shared commission. The partnership was dissolved in January 1513, as is evidenced by a surviving document.

According to Vasari, Albertinelli lived as a libertine and was fond of good living and women. He experienced financial problems and was unable to repay some of his loans, including one to Raphael, before he died. Albertinelli’s wife Antonia whom he had married in 1506 repaid some loans. Among his many students were Jacopo da Pontormo, Innocenzo di Pietro Francucci da Imola and Giuliano Bugiardini.

Albertinelli's paintings bear the imprint of varied influences: Perugino's sense of volume in space and perspective, Fra Bartolomeo's coloring, the landscape portrayal of Flemish masters like Memling and Leonardo's Sfumato technique. His chief paintings are in Florence, notably his masterpiece, the "Visitation" (1503), which is in the collection of the Uffizi.


</doc>
<doc id="19758" url="https://en.wikipedia.org/wiki?curid=19758" title="Beijing cuisine">
Beijing cuisine

Beijing cuisine, also known as Jing cuisine, Mandarin cuisine and Peking cuisine, and formerly as Beiping cuisine, is the local cuisine of Beijing, the national capital of China.

As Beijing has been the capital of China for centuries, its cuisine is influenced by culinary traditions from all over China, but the style that has the greatest influence on Beijing cuisine is that of the eastern coastal province of Shandong. Beijing cuisine has itself, in turn, also greatly influenced other Chinese cuisines, particularly the cuisine of Liaoning, the Chinese imperial cuisine, and the Chinese aristocrat cuisine.

Another tradition that influenced Beijing cuisine (as well as influenced by the latter itself) is the Chinese imperial cuisine that originated from the "Emperor's Kitchen" (), which referred to the cooking facilities inside the Forbidden City, where thousands of cooks from different parts of China showed their best culinary skills to please the imperial family and officials. Therefore, it is sometimes difficult to determine the actual origin of a dish as the term "Mandarin" is generalised and refers not only to Beijing, but other provinces as well. However, some generalisation of Beijing cuisine can be characterised as follows: Foods that originated in Beijing are often snacks rather than main courses, and they are typically sold by small shops or street vendors. There is emphasis on dark soy paste, sesame paste, sesame oil and scallions, and fermented tofu is often served as a condiment. In terms of cooking techniques, methods relating to different ways of frying are often used. There is less emphasis on rice as an accompaniment as compared to many other regions in China, as local rice production in Beijing is limited by the relatively dry climate.

Many dishes in Beijing cuisine that are served as main courses are derived from a variety of Chinese Halal foods, particularly lamb and beef dishes, as well as from Huaiyang cuisine.

Huaiyang cuisine has been praised since ancient times in China, and it was a general practice for an official travelling to Beijing to take up a new post to bring along with him a chef specialising in Huaiyang cuisine. When these officials had completed their terms in the capital and returned to their native provinces, most of the chefs they brought along often remained in Beijing. They opened their own restaurants or were hired by wealthy locals. The imperial clan of the Ming dynasty, the House of Zhu, who had ancestry from Jiangsu Province, also contributed greatly in introducing Huaiyang cuisine to Beijing when the capital was moved from Nanjing to Beijing in the 15th century, because the imperial kitchen was mainly Huaiyang style. The element of traditional Beijing culinary and gastronomical cultures of enjoying artistic performances such as Beijing opera while dining directly developed from the similar practice in the culture of Jiangsu and Huaiyang cuisines.

Chinese Islamic cuisine is another important component of Beijing cuisine, and was first prominently introduced when Beijing became the capital of the Yuan dynasty. However, the most significant contribution to the formation of Beijing cuisine came from Shandong cuisine, as most chefs from Shandong Province came to Beijing en masse during the Qing dynasty. Unlike the earlier two cuisines, which were brought by the ruling class such as nobles, aristocrats and bureaucrats, and then spread to the general populace, the introduction of Shandong cuisine begun with serving the general populace, with much wider market segment, from wealthy merchants to the working class.

The Qing dynasty was a major period in the formation of Beijing cuisine. Before the Boxer Rebellion, the foodservice establishments in Beijing were strictly stratified by the foodservice guild. Each category of the establishment was specifically based on its ability to provide for a particular segment of the market. The top ranking foodservice establishments served nobles, aristocrats, and wealthy merchants and landlords, while lower ranking foodservice establishments served the populace of lower financial and social status. It was during this period when Beijing cuisine gained fame and became recognised by the Chinese culinary society, and the stratification of the foodservice was one of its most obvious characteristics as part of its culinary and gastronomic cultures during this first peak of its formation.

The official stratification was an integral part of the local culture of Beijing and it was not finally abolished officially after the end of the Qing dynasty, which resulted in the second peak in the formation of Beijing cuisine. Meals previously offered to nobles and aristocrats were made available to anyone who could afford them instead of being restricted only to the upper class. As chefs freely switched between jobs offered by different foodservice establishments, they brought their skills that further enriched and developed Beijing cuisine. Though the stratification of food services in Beijing was no longer effected by imperial laws, the structure more or less remained despite continuous weakening due to the financial background of the local clientele. The different classes are listed in the following subsections.

Foodservice establishments with names ending with the Chinese character "zhuang" (), or "zhuang zihao" (), were the top-ranking foodservice establishments, not only in providing foods, but entertainment as well. The form of entertainment provided was usually Beijing opera, and foodservice establishments of this class always had long-term contracts with a Beijing opera troupe to perform onsite. Moreover, foodservice establishments of this class would always have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite, though not on a daily basis. Foodservice establishments of this category did not accept any different customers on a walk-in basis, but instead, only accepted customers who came as a group and ordered banquets by appointment, and the banquets provided by foodservice establishments of this category often included most, if not all tables, at the site. The bulk of the business of foodservice of this category, however, was catering at customers' homes or other locations, and such catering was often for birthdays, marriages, funerals, promotions and other important celebrations and festivals. When catering, these foodservice establishments not only provided what was on the menu, but fulfilled customers' requests.

Foodservice establishments categorised as "leng zhuangzi" () lacked any rooms to host banquets, and thus their business was purely catering.

Foodservice establishments with names ending with the Chinese character "tang" (), or "tang zihao" (), are similar to foodservice establishments with names ending with the Chinese character "zhuang", but the business of these second-class foodservice establishments were generally evenly divided among onsite banquet hosting and catering (at customers' homes). Foodservice establishments of this class would also have long-term contracts with Beijing opera troupes to perform onsite, but they did not have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite on regular basis; however these top performers would still perform at foodservice establishments of this category occasionally. In terms of catering at the customers' sites, foodservice establishments of this category often only provided dishes strictly according to their menu, and would not provide any dishes that were not on the menu.

Foodservice establishments with names ending with the Chinese character "ting" (), or "ting zihao" () are foodservice establishments which had more business in onsite banquet hosting than catering at customers' homes. For onsite banquet hosting, entertainment was still provided, but foodservice establishments of this category did not have long-term contracts with Beijing opera troupes, so that performers varied from time to time, and top performers usually did not perform here or at any lower-ranking foodservice establishments. For catering, different foodservice establishments of this category were incapable of handling significant catering on their own, but generally had to combine resources with other foodservice establishments of the same ranking (or lower) to do the job.

Foodservice establishments with names ending with the Chinese character "yuan" (), or "yuan zihao" () did nearly all their business in hosting banquets onsite. Entertainment was not provided on a regular basis, but there were stages built onsite for Beijing opera performers. Instead of being hired by the foodservice establishments like in the previous three categories, performers at foodservice establishments of this category were usually contractors who paid the foodservice establishment to perform and split the earnings according to a certain percentage. Occasionally, foodservice establishments of this category would be called upon to help cater at customers' homes, and like foodservice establishments with names ending with the Chinese character "ting", they could not do the job on their own but had to work with others, never taking the lead as foodservice establishments with names ending with the Chinese character "ting" could.

Foodservice establishments with names ending with the Chinese character "lou" (), or "lou zihao" () did the bulk of their business hosting banquets onsite by appointment. In addition, a smaller portion of the business was in serving different customers onsite on a walk-in basis. Occasionally, when catering at customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for.

Foodservice establishments with names ending with the Chinese character "ju" (), or "ju zihao" () generally divided their business evenly into two areas: serving different customers onsite on a walk-in basis, and hosting banquets by appointment for customers who came as one group. Occasionally, when catering at the customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for, just like foodservice establishments with names ending with the Chinese character "lou". However, unlike those establishments, which always cooked their specialty dishes on location, foodservice establishment of this category would either cook on location or simply bring the already-cooked food to the location.

Foodservice establishments with names ending with the Chinese character "zhai" (), or "zhai zihao" () were mainly in the business of serving different customers onsite on a walk-in basis, but a small portion of their income did come from hosting banquets by appointment for customers who came as one group. Just like foodservice establishments with names ending with the Chinese character "ju", when catering at customers’ homes, foodservice establishments of this category would also only provide the few specialty dishes they are famous for, but they would mostly bring the already-cooked dishes to the location, and would only cook on location occasionally.

Foodservice establishments with names ending with the Chinese character "fang" (), or "fang zihao" (). Foodservice establishments of this category generally did not offer the service of hosting banquets made by appointment for customers who came as one group, but instead, often only offered to serve different customers onsite on a walk-in basis. Foodservice establishments of this category or lower would not be called upon to perform catering at the customers' homes for special events.

Foodservice establishments with names ending with the Chinese character "guan" (), or "guan zihao" (). Foodservice establishments of this category mainly served different customers onsite on a walk-in basis, and in addition, a portion of the income would be earned from selling to-goes.

Foodservice establishments with names ending with the Chinese character "dian" (), or "dian zihao" (). Foodservice establishments of this category had their own place, like all previous categories, but serving different customers to dine onsite on a walk-in basis only provided half of the overall income, while the other half came from selling to-goes.

Foodservice establishments with name ending with the Chinese character "pu" (), or "pu zihao" (). Foodservice establishments of this category ranked next to the last, and they were often named after the owners' last names. Foodservice establishments of this category had fixed spots of business for having their own places, but not as large as those belonging to the category of "dian", and thus did not have tables, but only seats for customers. As a result, the bulk of the income of foodservice establishments of this category was from selling to-goes, while income earned from customers dining onsite only provided a small portion of the overall income.

Foodservice establishments with names ending with the Chinese character "tan" (), or "tan zihao" (). The lowest ranking foodservice establishments without any tables, and selling to-goes was the only form of business. In addition to name the food stand after the owners' last name or the food sold, these food stands were also often named after the owners' nicknames.

Numerous traditional restaurants in Beijing are credited with great contributions in the formation of Beijing cuisine, but many of them have gone out of business. However, some of them managed to survive until today, and some of them are:



</doc>
<doc id="19760" url="https://en.wikipedia.org/wiki?curid=19760" title="Manichaeism">
Manichaeism

Manichæism (;
in New Persian "Ãyīnⁱ Mānī"; ) was a major religion founded by the Iranian prophet Mani (Middle Persian "Mānī", New Persian: "Mānī", Syriac "Mānī", Greek , ) in the Sasanian Empire.

Manichaeism taught an elaborate dualistic cosmology describing the struggle between a good, spiritual world of light, and an evil, material world of darkness. Through an ongoing process that takes place in human history, light is gradually removed from the world of matter and returned to the world of light, whence it came. Its beliefs were based on local Mesopotamian religious movements and Gnosticism.

Manichaeism was quickly successful and spread far through the Aramaic-speaking regions. It thrived between the third and seventh centuries, and at its height was one of the most widespread religions in the world. Manichaean churches and scriptures existed as far east as China and as far west as the Roman Empire. It was briefly the main rival to Christianity before the spread of Islam in the competition to replace classical paganism. Manichaeism survived longer in the east than in the west, and it appears to have finally faded away after the 14th century in south China, contemporary to the decline of the Church of the East in Ming China. While most of Manichaeism's original writings have been lost, numerous translations and fragmentary texts have survived.

An adherent of Manichaeism was called a "Manichaean" or "Manichean", or "Manichee", especially in older sources.

Mani was an Iranian born in 216 in or near Seleucia-Ctesiphon (now al-Mada'in) in the Parthian Empire. According to the "Cologne Mani-Codex", Mani's parents were members of the Jewish Christian Gnostic sect known as the Elcesaites.

Mani composed seven works, six of which were written in the Syriac language, a late variety of Aramaic. The seventh, the "Shabuhragan", was written by Mani in Middle Persian and presented by him to the Sasanian emperor, Shapur I. Although there is no proof Shapur I was a Manichaean, he tolerated the spread of Manichaeism and refrained from persecuting it within his empire's boundaries.

According to one tradition, it was Mani himself who invented the unique version of the Syriac script known as the Manichaean alphabet, which was used in all of the Manichaean works written within the Sasanian Empire, whether they were in Syriac or Middle Persian, and also for most of the works written within the Uyghur Khaganate. The primary language of Babylon (and the administrative and cultural language of the Sassanid Empire) at that time was Eastern Middle Aramaic, which included three main dialects: Jewish Babylonian Aramaic (the language of the Babylonian Talmud), Mandaean (the language of Mandaeism), and Syriac, which was the language of Mani, as well as of the Syriac Christians.

While Manichaeism was spreading, existing religions such as Zoroastrianism were still popular and Christianity was gaining social and political influence. Although having fewer adherents, Manichaeism won the support of many high-ranking political figures. With the assistance of the Sasanian Empire, Mani began missionary expeditions. After failing to win the favour of the next generation of Persian royalty, and incurring the disapproval of the Zoroastrian clergy, Mani is reported to have died in prison awaiting execution by the Persian Emperor Bahram I. The date of his death is estimated at 276–277.

Mani believed that the teachings of Gautama Buddha, Zoroaster, and Jesus were incomplete, and that his revelations were for the entire world, calling his teachings the "Religion of Light". Manichaean writings indicate that Mani received revelations when he was 12 and again when he was 24, and over this time period he grew dissatisfied with the Elcesaite sect he was born into. Mani began preaching at an early age and was possibly influenced by contemporary Babylonian-Aramaic movements such as Mandaeism, and Aramaic translations of Jewish apocalyptic writings similar to those found at Qumran (such as the book of Enoch literature), and by the Syriac dualist-gnostic writer Bardaisan (who lived a generation before Mani). With the discovery of the Mani-Codex, it also became clear that he was raised in a Jewish-Christian baptism sect, the Elcesaites, and was influenced by their writings, as well. According to biographies preserved by Ibn al-Nadim and the Persian polymath al-Biruni, he received a revelation as a youth from a spirit, whom he would later call his Twin ( , from which is also derived the name of the Thomas the Apostle, the "twin"), his "Syzygos" ( "spouse, partner", in the "Cologne Mani-Codex"), his Double, his Protective Angel or Divine Self. It taught him truths that he developed into a religion. His divine Twin or true Self brought Mani to self-realization. He claimed to be the "Paraclete of the Truth", as promised by Jesus in the New Testament.
Manichaeism's views on Jesus are described by historians:
Augustine also noted that Mani declared himself to be an "apostle of Jesus Christ". Manichaean tradition is also noted to have claimed that Mani was the reincarnation of different religious figures such as Buddha, Krishna, Zoroaster, and Jesus.

Academics also note that since much of what is known about Manichaeism comes from later 10th- and 11th-century Muslim historians like Al-Biruni and especially ibn al-Nadim (and his "Fihrist"), "Islamic authors ascribed to Mani the claim to be the Seal of the Prophets." In reality, for Mani the expression "seal of prophecy" refers to his disciples, who testify for the veracity of his message, as a seal does.
Another source of Mani's scriptures was original Aramaic writings relating to the "Book of Enoch" literature (see the Book of Enoch and the Second Book of Enoch), as well as an otherwise unknown section of the Book of Enoch called "The Book of Giants". This book was quoted directly, and expanded on by Mani, becoming one of the original six Syriac writings of the Manichaean Church. Besides brief references by non-Manichaean authors through the centuries, no original sources of "The Book of Giants" (which is actually part six of the Book of Enoch) were available until the 20th century.

Scattered fragments of both the original Aramaic "Book of Giants" (which were analyzed and published by Józef Milik in 1976) and of the Manichaean version of the same name (analyzed and published by Walter Bruno Henning in 1943) were found with the discovery in the twentieth century of the Dead Sea Scrolls in the Judaean Desert and the Manichaean writings of the Uyghur Manichaean kingdom in Turpan. Henning wrote in his analysis of them:

By comparing the cosmology in the Book of Enoch literature and the Book of Giants, alongside the description of the Manichaean myth, scholars have observed that the Manichaean cosmology can be described as being based, in part, on the description of the cosmology developed in detail in the Book of Enoch literature. This literature describes the being that the prophets saw in their ascent to heaven, as a king who sits on a throne at the highest of the heavens. In the Manichaean description, this being, the "Great King of Honor", becomes a deity who guards the entrance to the world of light, placed at the seventh of ten heavens. In the Aramaic Book of Enoch, in the Qumran writings in general, and in the original Syriac section of Manichaean scriptures quoted by Theodore bar Konai, he is called "malka raba de-ikara" (the Great King of Honor).

Mani was also influenced by writings of the Assyrian gnostic Bardaisan (154–222), who, like Mani, wrote in Syriac, and presented a dualistic interpretation of the world in terms of light and darkness, in combination with elements from Christianity.
Noting Mani's travels to the Kushan Empire (several religious paintings in Bamyan are attributed to him) at the beginning of his proselytizing career, Richard Foltz postulates Buddhist influences in Manichaeism:

The Kushan monk Lokakṣema began translating Pure Land Buddhist texts into Chinese in the century prior to Mani arriving there, and the Chinese texts of Manichaeism are full of uniquely Buddhist terms taken directly from these Chinese Pure Land scriptures, including the term "pure land" (淨土 Jìngtǔ) itself. However, the central object of veneration in Pure Land Buddhism, Amitābha, the Buddha of Infinite Light, does not appear in Chinese Manichaeism, and seems to have been replaced by another deity.

Manichaeism spread with extraordinary speed through both the East and West. It reached Rome through the apostle Psattiq by 280, who was also in Egypt in 244 and 251. It was flourishing in the Faiyum in 290.

Manichaean monasteries existed in Rome in 312 during the time of Pope Miltiades.

In 291, persecution arose in the Sasanian Empire with the murder of the apostle Sisin by Emperor Bahram II, and the slaughter of many Manichaeans. In 296, Roman Emperor Diocletian decreed against the Manichaeans: "We order that their organizers and leaders be subject to the final penalties and condemned to the fire with their abominable scriptures." This resulted in martyrdom for many in Egypt and North Africa (see Diocletian Persecution). By 354, Hilary of Poitiers wrote that Manichaeism was a significant force in Roman Gaul. In 381, Christians requested Theodosius I to strip Manichaeans of their civil rights. Starting in 382, the emperor issued a series of edicts to suppress Manichaeism and punish its followers.
Augustine of Hippo (354–430) converted to Christianity from Manichaeism in the year 387. This was shortly after the Roman emperor Theodosius I had issued a decree of death for all Manichaean monks in 382 and shortly before he declared Christianity to be the only legitimate religion for the Roman Empire in 391. Due to the heavy persecution, the religion almost disappeared from western Europe in the fifth century and from the eastern portion of the empire in the sixth century. According to his "Confessions", after nine or ten years of adhering to the Manichaean faith as a member of the group of "hearers", Augustine became a Christian and a potent adversary of Manichaeism (which he expressed in writing against his Manichaean opponent Faustus of Mileve), seeing their beliefs that knowledge was the key to salvation as too passive and not able to effect any change in one's life.

Some modern scholars have suggested that Manichaean ways of thinking influenced the development of some of Augustine's ideas, such as the nature of good and evil, the idea of hell, the separation of groups into elect, hearers, and sinners, and the hostility to the flesh and sexual activity, and his dualistic theology. These influences of Manichaeism in Augustine's Christian thinking may well have been part of the conflict between Augustine and Pelagius, a British monk whose theology, being less influenced by the Latin Church, was non-dualistic, and one that saw the created order, and mankind in particular, as having a Divine core, rather than a 'darkness' at its core.
How Manichaeism might have influenced Christianity continues to be debated. Manichaeism could have influenced the Bogomils, Paulicians, and Cathars. However, these groups left few records, and the link between them and Manichaeans is tenuous. Regardless of its accuracy, the charge of Manichaeism was leveled at them by contemporary orthodox opponents, who often tried to make contemporary heresies conform to those combatted by the church fathers. Whether the dualism of the Paulicians, Bogomils, and Cathars and their belief that the world was created by a Satanic demiurge were due to influence from Manichaeism is impossible to determine. The Cathars apparently adopted the Manichaean principles of church organization. Priscillian and his followers may also have been influenced by Manichaeism. The Manichaeans preserved many apocryphal Christian works, such as the Acts of Thomas, that would otherwise have been lost.

Manichaeism maintained a sporadic and intermittent existence in the west (Mesopotamia, Africa, Spain, France, North Italy, the Balkans) for a thousand years, and flourished for a time in Persia and even further east in Northern India, Western China, and Tibet. While it had long been thought that Manichaeism arrived in China only at the end of the seventh century, a recent archaeological discovery demonstrated that it was already known there in the second half of the 6th century.
Some Sogdians in Central Asia believed in the religion. Uyghur khagan Boku Tekin (759–780) converted to the religion in 763 after a three-day discussion with its preachers, the Babylonian headquarters sent high rank clerics to Uyghur, and Manichaeism remained the state religion for about a century before the collapse of the Uyghur Khaganate in 840. In the east it spread along trade routes as far as Chang'an, the capital of Tang China. After the Tang Dynasty, some Manichaean groups participated in peasant movements. The religion was used by many rebel leaders to mobilise followers. In the Song and Yuan dynasties of China remnants of Manichaeism continued to leave a legacy contributing to sects such as the Red Turbans. During the Song Dynasty, the Manichaeans were derogatorily referred by the Chinese as "chicai simo" (meaning that they "abstain from meat and worship demons"). An account in "Fozu Tongji", an important historiography of Buddhism in China compiled by Buddhist scholars during 1258–1269, says that the Manichaeans worshipped the "white Buddha" and their leader wore a violet headgear, while the followers wore white costumes. Many Manichaeans took part in rebellions against the Song government and were eventually quelled. After that, all governments were suppressive against Manichaeism and its followers and the religion was banned by the Ming Dynasty in 1370.

Manichaeism spread to Tibet during the Tibetan Empire. There was likely a serious attempt to introduce the religion to the Tibetans as the text "Criteria of the Authentic Scriptures" (a text attributed to Tibetan Emperor Trisong Detsen) makes a great effort to attack Manichaeism by stating that Mani was a heretic who took ideas from all faiths and blended them together into a deviating and inauthentic form.

Manichaeans in Iran tried to assimilate their religion along with Islam in the Muslim caliphates. Relatively little is known about the religion during the first century of Islamic rule. During the early caliphates, Manichaeism attracted many followers. It had a significant appeal among the Muslim society, especially among the elites. Due to the appeal of its teachings, many Muslims adopted the ideas of its theology and some even became dualists. An apologia for Manichaeism ascribed to ibn al-Muqaffa' defended its phantasmagorical cosmogony and attacked the fideism of Islam and other monotheistic religions. The Manichaeans had sufficient structure to have a head of their community.

Under the eighth-century Abbasid Caliphate, Arabic "zindīq" and the adjectival term "zandaqa" could denote many different things, though it seems primarily (or at least initially) to have signified a follower of Manichaeism however its true meaning is not known. In the ninth century, it is reported that Caliph al-Ma'mun tolerated a community of Manichaeans.

During the early Abbasid period, the Manichaeans underwent persecution. The third Abbasid caliph, al-Mahdi, persecuted the Manichaeans, establishing an inquisition against dualists who if being found guilty of heresy refused to renounce their beliefs, were executed. Their persecution was finally ended in 780s by Harun al-Rashid. During the reign of the Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution and the base of the religion was later shifted to Samarkand.
Manichaeism claimed to present the complete version of teachings that were corrupted and misinterpreted by the followers of its predecessors Adam, Zoroaster, Buddha and Jesus. Accordingly, as it spread, it adapted new deities from other religions into forms it could use for its scriptures. Its original Aramaic texts already contained stories of Jesus. When they moved eastward and were translated into Iranian languages, the names of the Manichaean deities (or angels) were often transformed into the names of Zoroastrian yazatas. Thus "Abbā dəRabbūṯā" ("The Father of Greatness", the highest Manichaean deity of Light), in Middle Persian texts might either be translated literally as "pīd ī wuzurgīh", or substituted with the name of the deity "Zurwān". Similarly, the Manichaean primal figure "Nāšā Qaḏmāyā" "The Original Man" was rendered "Ohrmazd Bay", after the Zoroastrian god Ohrmazd. This process continued in Manichaeism's meeting with Chinese Buddhism, where, for example, the original Aramaic "qaryā" (the "call" from the World of Light to those seeking rescue from the World of Darkness), becomes identified in the Chinese scriptures with Guanyin ( or Avalokiteśvara in Sanskrit, literally, "watching/perceiving sounds [of the world]", the bodhisattva of Compassion).

Manichaeism was repressed by the Sasanian Empire. In 291, persecution arose in the Persian empire with the murder of the apostle Sisin by Bahram II, and the slaughter of many Manichaeans. In 296, the Roman emperor Diocletian decreed all the Manichaean leaders to be burnt alive along with the Manichaean scriptures and many Manichaeans in Europe and North Africa were killed. This policy of persecution was also followed by his successors. Theodosius I issued a decree of death for all Manichaean monks in 382 AD. The religion was vigorously attacked and persecuted by both the Christian Church and the Roman state, and the religion almost disappeared from western Europe in the fifth century and from the eastern portion of the empire in the sixth century.

In 732, Emperor Xuanzong of Tang banned any Chinese from converting to the religion, saying it was a heretic religion that was confusing people by claiming to be Buddhism. However, the foreigners who followed the religion were allowed to practice it without punishment. After the fall of the Uyghur Khaganate in 840, which was the chief patron of Manichaeism (which was also the state religion of the Khaganate) in China, all Manichaean temples in China except in the two capitals and Taiyuan were closed down and never reopened since these temples were viewed as a symbol of foreign arrogance by the Chinese (see Cao'an). Even those that were allowed to remain open did not for long. The Manichaean temples were attacked by Chinese people who burned the images and idols of these temples. Manichaean priests were ordered to wear hanfu instead of their traditional clothing, which was viewed as un-Chinese. In 843, Emperor Wuzong of Tang gave the order to kill all Manichaean clerics as part of his Great Anti-Buddhist Persecution, and over half died. They were made to look like Buddhists by the authorities, their heads were shaved, they were made to dress like Buddhist monks and then killed. Although the religion was mostly forbidden and its followers persecuted thereafter in China, it survived till the 14th century in the country. Under the Song dynasty, its followers were derogatorily referred to with the chengyu () "vegetarian demon-worshippers".

Many Manichaeans took part in rebellions against the Song dynasty. They were quelled by Song China and were suppressed and persecuted by all successive governments before the Mongol Yuan dynasty. In 1370, the religion was banned through an edict of the Ming dynasty, whose Hongwu Emperor had a personal dislike for the religion. Its core teaching influences many religious sects in China, including the White Lotus movement.

According to Wendy Doniger, Manichaeism may have continued to exist in the modern-East Turkestan region until the Mongol conquest in the 13th century.

Manicheans also suffered persecution for some time under the Abbasid Caliphate of Baghdad. In 780, the third Abbasid Caliph, al-Mahdi, started a campaign of inquisition against those who were "dualist heretics" or "Manichaeans" called the "zindīq". He appointed a "master of the heretics" ( "ṣāhib al-zanādiqa"), an official whose task was to pursue and investigate suspected dualists, who were then examined by the Caliph. Those found guilty who refused to abjure their beliefs were executed. This persecution continued under his successor, Caliph al-Hadi, and continued for some time during reign of Harun al-Rashid, who finally abolished it and ended it. During the reign of the 18th Abbassid Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution by him and about 500 of them assembled in Samarkand. The base of the religion was later shifted to this city, which became their new Patriarchate.

Manichaean pamphlets were still in circulation in Greek in 9th century Byzantine Constantinople, as the patriarch Photios summarizes and discusses one that he has read by Agapius in his "Bibliotheca".

During the Middle Ages, several movements emerged that were collectively described as "Manichaean" by the Catholic Church, and persecuted as Christian heresies through the establishment, in 1184, of the Inquisition. They included the Cathar churches of Western Europe. Other groups sometimes referred to as "neo-Manichaean" were the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria. An example of this usage can be found in the published edition of the Latin Cathar text, the "Liber de duobus principiis" ("Book of the Two Principles"), which was described as "Neo-Manichaean" by its publishers. As there is no presence of Manichaean mythology or church terminology in the writings of these groups, there has been some dispute among historians as to whether these groups were descendants of Manichaeism.

Some sites are preserved in Xinjiang and Fujian in China. The Cao'an temple is the only fully intact Manichaean building, though it later became associated with Buddhism. Several small groups claim to continue to practice this faith.

Mani's teaching dealt with the origin of evil, by addressing a theoretical part of the problem of evil by denying the omnipotence of God and postulating two opposite powers. Manichaean theology taught a dualistic view of good and evil. A key belief in Manichaeism is that the powerful, though not omnipotent good power (God), was opposed by the eternal evil power (devil). Humanity, the world and the soul are seen as the byproduct of the battle between God's proxy, Primal Man, and the devil. The human person is seen as a battleground for these powers: the soul defines the person, but it is under the influence of both light and dark. This contention plays out over the world as well as the human body—neither the Earth nor the flesh were seen as intrinsically evil, but rather possessed portions of both light and dark. Natural phenomena (such as rain) were seen as the physical manifestation of this spiritual contention. Therefore, the Manichaean worldview explained the existence of evil by positing a flawed creation in the formation of which God took no part and which constituted rather the product of a battle by the devil against God.

Manichaeism presented an elaborate description of the conflict between the spiritual world of light and the material world of darkness. The beings of both the world of darkness and the world of light have names. There are numerous sources for the details of the Manichaean belief. There are two portions of Manichaean scriptures that are probably the closest thing to the original Manichaean writings in their original languages that will ever be available. These are the Syriac-Aramaic quotation by the Nestorian Christian Theodore bar Konai, in his Syriac "Book of Scholia" ("Ketba de-Skolion"z, 8th century), and the Middle Persian sections of Mani's Shabuhragan discovered at Turpan (a summary of Mani's teachings prepared for Shapur I).

From these and other sources, it is possible to derive an almost complete description of the detailed Manichaean vision (a complete list of Manichaean deities is outlined below). According to Mani, the unfolding of the universe takes place with three "creations":

The First Creation: Originally, good and evil existed in two completely separate realms, one the "World of Light", ruled by the "Father of Greatness" together with his five "Shekhinas" (divine attributes of light), and the other the "World of Darkness", ruled by the "King of Darkness". At a certain point, the "Kingdom of Darkness" notices the "World of Light", becomes greedy for it and attacks it. The "Father of Greatness", in the first of three "creations" (or "calls"), calls to the "Mother of Life", who sends her son "Original Man" ("Nāšā Qaḏmāyā" in Aramaic), to battle with the attacking powers of Darkness, which include the "Demon of Greed". The "Original Man" is armed with five different shields of light (reflections of the five "Shekhinas"), which he loses to the forces of darkness in the ensuing battle, described as a kind of "bait" to trick the forces of darkness, as the forces of darkness greedily consume as much light as they can. When the "Original Man" comes to, he is trapped among the forces of darkness.

The Second Creation: Then the "Father of Greatness" begins the "Second Creation", calling to the "Living Spirit", who calls to his five sons, and sends a call to the "Original Man" ("Call" then becomes a Manichaean deity). An answer ("Answer" becomes another Manichaean deity) then returns from the "Original Man" to the "World of Light". The "Mother of Life", the "Living Spirit", and his five sons begin to create the universe from the bodies of the evil beings of the "World of Darkness", together with the light that they have swallowed. Ten heavens and eight earths are created, all consisting of various mixtures of the evil material beings from the "World of Darkness" and the swallowed light. The sun, moon, and stars are all created from light recovered from the "World of Darkness". The waxing and waning of the moon is described as the moon filling with light, which passes to the sun, then through the Milky Way, and eventually back to the "World of Light".
The Third Creation: Great demons (called "archons" in bar-Khonai's account) are hung out over the heavens, and then the "Father of Greatness" begins the "Third Creation". Light is recovered from out of the material bodies of the male and female evil beings and demons, by causing them to become sexually aroused in greed, towards beautiful images of the beings of light, such as the "Third Messenger" and the "Virgins of Light". However, as soon as the light is expelled from their bodies and falls to the earth (some in the form of abortions – the source of fallen angels in the Manichaean myth), the evil beings continue to swallow up as much of it as they can to keep the light inside of them. This results eventually in the evil beings swallowing huge quantities of light, copulating, and producing Adam and Eve. The "Father of Greatness" then sends the "Radiant Jesus" to awaken Adam, and to enlighten him to the true source of the light that is trapped in his material body. Adam and Eve, however, eventually copulate, and produce more human beings, trapping the light in bodies of mankind throughout human history. The appearance of the Prophet Mani was another attempt by the "World of Light" to reveal to mankind the true source of the spiritual light imprisoned within their material bodies.

Beginning with the time of its creation by Mani, the Manichaean religion had a detailed description of deities and events that took place within the Manichaean scheme of the universe. In every language and region that Manichaeism spread to, these same deities reappear, whether it is in the original Syriac quoted by Theodore bar Konai, or the Latin terminology given by Saint Augustine from Mani's "Epistola Fundamenti", or the Persian and Chinese translations found as Manichaeism spread eastward. While the original Syriac retained the original description that Mani created, the transformation of the deities through other languages and cultures produced incarnations of the deities not implied in the original Syriac writings.







The Manichaean Church was divided into the Elect, who had taken upon themselves the vows of Manicheaism, and the Hearers, those who had not, but still participated in the Church. The terms for these divisions were already common since the days of early Christianity. In Chinese writings, the Middle Persian and Parthian terms are transcribed phonetically (instead of being translated into Chinese). These were recorded by Augustine of Hippo.

Evidently from Manichaean sources, Manichaeans observed daily prayers, either four for the "hearers" or seven for the "elects". The sources differ about the exact time of prayer. The "Fihrist" by al-Nadim, points them after noon, mid-afternoon, just after sunset and at nightfall. Al-Biruni places the prayers at noon, nightfall, dawn and sunrise. The elect additionally pray at mid-afternoon, half an hour after nightfall and at midnight. Al-Nadim's account of daily prayers is probably adjusted to coincide with the public prayers for the Muslims, while Al-Birunis report may reflect an older tradition unaffected by Islam. When Al-Nadims account of daily prayers had been the only detailed source available, there was a concern, that these practises had been only adapted by Muslims during the Abbasid Caliphate. However, it is clear that the Arabic text provided by Al-Nadim corresponds with the descriptions of Egyptian texts from the fourth Century.

Every prayer started with an ablution with water or, if water is not available, with other substances comparable to Ablution in Islam and consisted of several blessings to the apostales and spirits. The prayer consisted of prostrating oneself to the ground and rising again twelve times during every prayer. During day, Manichaeans turned towards the sun and during night towards the moon. If the moon is not visible at night, when they turned towards north. Evident from Faustus of Mileve, Celestial bodies are not the subject of worship themselves, but "ships" carrying the light particles of the world to the supreme god, who can not be seen, since he exists beyond time and space, and also the dwelling places for emanations of the supreme deity, such as Jesus the Splendour. According to the writings of Augustine of Hippo, ten prayers were performed, the first devoted to the Father of Greatness, and the following to lesser deities, spirits and angels and finally towards the elect, in order to be freed from rebirth and pain and to attain peace in the realm of light. Comparable, in the Uighur confession, four prayers are directed to the supreme God ("Äzrua"), the God of the Sun and the Moon, and fivefold God and the buddhas.

Mani wrote either seven or eight books, which contained the teachings of the religion. Only scattered fragments and translations of the originals remain.

The original six Syriac writings are not preserved, although their Syriac names have been. There are also fragments and quotations from them. A long quotation, preserved by the eighth-century Nestorian Christian author Theodore Bar Konai, shows that in the original Syriac Aramaic writings of Mani there was no influence of Iranian or Zoroastrian terms. The terms for the Manichaean deities in the original Syriac writings are in Aramaic. The adaptation of Manichaeism to the Zoroastrian religion appears to have begun in Mani's lifetime however, with his writing of the Middle Persian "Shabuhragan", his book dedicated to the Sasanian emperor, Shapur I. In it, there are mentions of Zoroastrian divinities such as Ahura Mazda, Angra Mainyu, and Āz. Manichaeism is often presented as a Persian religion, mostly due to the vast number of Middle Persian, Parthian, and Sogdian (as well as Turkish) texts discovered by German researchers near Turpan in what is now Xinjiang, China, during the early 1900s. However, from the vantage point of its original Syriac descriptions (as quoted by Theodore Bar Khonai and outlined above), Manichaeism may be better described as a unique phenomenon of Aramaic Babylonia, occurring in proximity to two other new Aramaic religious phenomena, Talmudic Judaism and Mandaeism, which also appeared in Babylonia in roughly the third century.

The original, but now lost, six sacred books of Manichaeism were composed in Syriac Aramaic, and translated into other languages to help spread the religion. As they spread to the east, the Manichaean writings passed through Middle Persian, Parthian, Sogdian, Tocharian, and ultimately Uyghur and Chinese translations. As they spread to the west, they were translated into Greek, Coptic, and Latin.
Henning describes how this translation process evolved and influenced the Manichaeans of Central Asia:




In later centuries, as Manichaeism passed through eastern Persian-speaking lands and arrived at the Uyghur Khaganate (回鶻帝國), and eventually the Uyghur kingdom of Turpan (destroyed around 1335), Middle Persian and Parthian prayers ("āfrīwan" or "āfurišn") and the Parthian hymn-cycles (the "Huwīdagmān" and "Angad Rōšnan" created by Mar Ammo) were added to the Manichaean writings. A translation of a collection of these produced the "Manichaean Chinese Hymnscroll" (, which Lieu translates as "Hymns for the Lower Section [i.e. the Hearers] of the Manichaean Religion"). In addition to containing hymns attributed to Mani, it contains prayers attributed to Mani's earliest disciples, including Mār Zaku, Mār Ammo and Mār Sīsin. Another Chinese work is a complete translation of the "Sermon of the Light Nous", presented as a discussion between Mani and his disciple Adda.

Until discoveries in the 1900s of original sources, the only sources for Manichaeism were descriptions and quotations from non-Manichaean authors, either Christian, Muslim, Buddhist, or Zoroastrian. While often criticizing Manichaeism, they also quoted directly from Manichaean scriptures. This enabled Isaac de Beausobre, writing in the 18th century, to create a comprehensive work on Manichaeism, relying solely on anti-Manichaean sources. Thus quotations and descriptions in Greek and Arabic have long been known to scholars, as have the long quotations in Latin by Saint Augustine, and the extremely important quotation in Syriac by Theodore Bar Konai.

Eusebius commented as follows:
An example of how inaccurate some of these accounts could be is seen in the account of the origins of Manichaeism contained in the "Acta Archelai". This was a Greek anti-manichaean work written before 348, most well known in its Latin version, which was regarded as an accurate account of Manichaeism until refuted by Isaac de Beausobre in the 18th century:

In the time of the Apostles there lived a man named Scythianus, who is described as coming "from Scythia", and also as being "a Saracen by race" ("ex genere Saracenorum"). He settled in Egypt, where he became acquainted with "the wisdom of the Egyptians", and invented the religious system that was afterwards known as Manichaeism. Finally he emigrated to Palestine, and, when he died, his writings passed into the hands of his sole disciple, a certain Terebinthus. The latter betook himself to Babylonia, assumed the name of Budda, and endeavoured to propagate his master's teaching. But he, like Scythianus, gained only one disciple, who was an old woman. After a while he died, in consequence of a fall from the roof of a house, and the books that he had inherited from Scythianus became the property of the old woman, who, on her death, bequeathed them to a young man named Corbicius, who had been her slave. Corbicius thereupon changed his name to Manes, studied the writings of Scythianus, and began to teach the doctrines that they contained, with many additions of his own. He gained three disciples, named Thomas, Addas, and Hermas. About this time the son of the Persian king fell ill, and Manes undertook to cure him; the prince, however, died, whereupon Manes was thrown into prison. He succeeded in escaping, but eventually fell into the hands of the king, by whose order he was flayed, and his corpse was hung up at the city gate.

A. A. Bevan, who quoted this story, commented that it "has no claim to be considered historical".

According to Hegemonius' portrayal of Mani, the evil demiurge who created the world was the Jewish Jehovah. Hegemonius reports that Mani said,
In the early 1900s, original Manichaean writings started to come to light when German scholars led by Albert Grünwedel, and then by Albert von Le Coq, began excavating at Gaochang, the ancient site of the Manichaean Uyghur Kingdom near Turpan, in Chinese Turkestan (destroyed around AD 1300). While most of the writings they uncovered were in very poor condition, there were still hundreds of pages of Manichaean scriptures, written in three Iranian languages (Middle Persian, Parthian, and Sogdian) and old Uyghur. These writings were taken back to Germany, and were analyzed and published at the Preußische Akademie der Wissenschaften in Berlin, by Le Coq and others, such as Friedrich W. K. Müller and Walter Bruno Henning. While the vast majority of these writings were written in a version of the Syriac script known as Manichaean script, the German researchers, perhaps for lack of suitable fonts, published most of them using the Hebrew alphabet (which could easily be substituted for the 22 Syriac letters).

Perhaps the most comprehensive of these publications was "Manichaeische Dogmatik aus chinesischen und iranischen Texten" ("Manichaean Dogma from Chinese and Iranian texts"), by Ernst Waldschmidt and Wolfgang Lentz, published in Berlin in 1933. More than any other research work published before or since, this work printed, and then discussed, the original key Manichaean texts in the original scripts, and consists chiefly of sections from Chinese texts, and Middle Persian and Parthian texts transcribed with the Hebrew alphabet. After the Nazi Party gained power in Germany, the Manichaean writings continued to be published during the 1930s, but the publishers no longer used Hebrew letters, instead transliterating the texts into Latin letters.

Additionally, in 1930, German researchers in Egypt found a large body of Manichaean works in Coptic. Though these were also damaged, hundreds of complete pages survived and, beginning in 1933, were analyzed and published in Berlin before World War II, by German scholars such as Hans Jakob Polotsky. Some of these Coptic Manichaean writings were lost during the war.

After the success of the German researchers, French scholars visited China and discovered what is perhaps the most complete set of Manichaean writings, written in Chinese. These three Chinese writings, all found at the Mogao Caves among the Dunhuang manuscripts, and all written before the 9th century, are today kept in London, Paris, and Beijing. Some of the scholars involved with their initial discovery and publication were Édouard Chavannes, Paul Pelliot, and Aurel Stein. The original studies and analyses of these writings, along with their translations, first appeared in French, English, and German, before and after World War II. The complete Chinese texts themselves were first published in Tokyo, Japan in 1927, in the Taishō Tripiṭaka, volume 54. While in the last thirty years or so they have been republished in both Germany (with a complete translation into German, alongside the 1927 Japanese edition), and China, the Japanese publication remains the standard reference for the Chinese texts.

In Egypt, a small codex was found and became known through antique dealers in Cairo. It was purchased by the University of Cologne in 1969. Two of its scientists, Henrichs and Koenen, produced the first edition known since as the Cologne Mani-Codex, which was published in four articles in the "Zeitschrift für Papyrologie und Epigraphik". The ancient papyrus manuscript contained a Greek text describing the life of Mani. Thanks to this discovery, much more is known about the man who founded one of the most influential world religions of the past.

The terms "Manichaean" and "Manichaeism" are sometimes used figuratively as a synonym of the more general term "dualist" with respect to a philosophy, outlook or worldview. The terms are often used to suggest that the world view in question simplistically reduces the world to a struggle between good and evil. For example, Zbigniew Brzezinski used the phrase "Manichaean paranoia" in reference to U.S. President George W. Bush's world view (in "The Daily Show with Jon Stewart", 14 March 2007); Brzezinski elaborated that he meant "the notion that he [Bush] is leading the forces of good against the empire of evil". Author and journalist Glenn Greenwald followed up on the theme in describing Bush in his book "A Tragic Legacy" (2007).

The term is frequently used by critics to describe the attitudes and foreign policies of the United States and its leaders.

Philosopher Frantz Fanon frequently invoked the concept of Manicheanism in his discussions of violence between colonizers and the colonized.

In "My Secret History", author Paul Theroux's protagonist defines the word Manichaean for the protagonist's son. Prior to explaining the word to his son, the protagonist mentions Joseph Conrad's short story "The Secret Sharer" at least twice in the book, the plot of which also examines the idea of the duality of good and evil. The word in the novel can also be considered a metaphor for the last chapter of the book ("Two of Everything" [1984]) and possibly the novel itself.








</doc>
<doc id="19761" url="https://en.wikipedia.org/wiki?curid=19761" title="Moroccan cuisine">
Moroccan cuisine

Moroccan cuisine is influenced by Morocco's interactions and exchanges with other cultures and nations over the centuries. Moroccan cuisine is typically a mix of Berber, Arabic, Andalusian, and Mediterranean cuisines with slight European and sub-Saharan influences.

Morocco produces a large range of Mediterranean fruits, vegetables and even some tropical ones. Common meats include beef, goat, mutton and lamb, chicken and seafood, which serve as a base for the cuisine. Characteristic flavorings include lemon pickle, argan oil, cold-pressed, unrefined olive oil and dried fruits. As in Mediterranean cuisine in general, the staple ingredients include wheat, used for bread and couscous, and olive oil; the third Mediterranean staple, the grape, is eaten as a dessert, though a certain amount of wine is made in the country.

Spices are used extensively in Moroccan food. Although some spices have been imported to Morocco through the Arabs for thousands of years, many ingredients—like saffron from Talaouine, mint and olives from Meknes, and oranges and lemons from Fes—are home-grown, and are being exported internationally. Common spices include cinnamon, cumin, turmeric, ginger, paprika, coriander, saffron, mace, cloves, fennel, anise, nutmeg, cayenne pepper, fenugreek, caraway, black pepper and sesame seeds. Twenty-seven spices are combined for the famous Moroccan spice mixture "ras el hanout".

Common herbs in Moroccan cuisine include mint, parsley, coriander, oregano, peppermint, marjoram, verbena, sage and bay laurel.

A typical lunch meal begins with a series of hot and cold salads, followed by a tagine or Dwaz. Often, for a formal meal, a lamb or chicken dish is next, or couscous topped with meat and vegetables. Moroccans either eat with fork, knife and spoon or with their hands using bread as a utensil depending on the dish served. The consumption of pork and alcohol is uncommon due to religious restrictions.

The main Moroccan dish most people are familiar with is couscous, the old national delicacy. Beef is the most commonly eaten red meat in Morocco, usually eaten in a tagine with a wide selection of vegetables. Chicken is also very commonly used in tagines, or roasted.

Lamb is also heavily consumed, and since Moroccan sheep breeds store most of their fat in their tails, Moroccan lamb does not have the pungent flavour that Western lamb and mutton have.

Since Morocco lies on two coasts, the Atlantic and the Mediterranean, Moroccan cuisine has ample seafood dishes. European pilchard is caught in large but declining quantities. Other fish species include mackerel, anchovy, sardinella, and horse mackerel.

Other famous Moroccan dishes are Pastilla (also spelled Basteeya or Bestilla), Tanjia and Harira, a typical heavy soup, eaten during winter to warm up and is usually served for dinner. It is typically eaten with plain bread or with dates during the month of Ramadan. Bissara is a broad bean-based soup that is also consumed during the colder months of the year.

A big part of the daily meal is bread. Bread in Morocco is principally made from durum wheat semolina known as khobz. Bakeries are very common throughout Morocco and fresh bread is a staple in every city, town and village. The most common is whole grain coarse ground or white flour bread or baguettes. There are also a number of flat breads and pulled unleavened pan-fried breads.

In addition, there are dried salted meats and salted preserved meats such as khlea and g'did (basically sheep bacon), which are used to flavor tagines or used in "el rghaif", a folded savory Moroccan pancake.

Salads include both raw and cooked vegetables, served either hot or cold. Cold salads include "zaalouk," an aubergine and tomato mixture, and taktouka (a mixture of tomatoes, smoked green peppers, garlic and spices) characteristic of the cities of Taza and Fes, in the Atlas. Another cold salad is called Bakoula, or Khoubiza. It consists of braised mallow leaves, but can also be made with spinach or arugula, with parsley, cilantro, lemon, olive oil and olives.

Usually, seasonal fruits rather than cooked desserts are served at the close of a meal. A common dessert is "kaab el ghzal" (, "gazelle ankles"), a pastry stuffed with almond paste and topped with sugar. Another is "Halwa chebakia", pretzel-shaped dough deep-fried, soaked in honey and sprinkled with sesame seeds; it is eaten during the month of Ramadan. Coconut fudge cakes, 'Zucre Coco', are popular also.

Morocco is endowed with over 3000 km of coastline. There is an abundance of fish in these coastal waters with the sardine being commercially significant as Morocco is the world's largest exporter. Sardines were used in the production of garum in Lixus.

At Moroccan fish markets one can find sole, swordfish, tuna, tarbot, mackerel, shrimp, congre eel, skate, red snapper, spider crab, lobster and a variety of mollusks.

In Moroccan cuisine, seafood is incorporated into, among others: tajines, bastilla, briwat, and paella.

The most popular drink is green tea with mint. Traditionally, making good mint tea in Morocco is considered an art form and the drinking of it with friends and family is often a daily tradition. The pouring technique is as crucial as the quality of the tea itself. Moroccan tea pots have long, curved pouring spouts and this allows the tea to be poured evenly into tiny glasses from a height. For the best taste, glasses are filled in two stages. The Moroccans traditionally like tea with bubbles, so while pouring they hold the teapot high above the glasses. Finally, the tea is accompanied with hard sugar cones or lumps. Morocco has an abundance of oranges and tangerines, so fresh orange juice is easily found freshly squeezed and is cheap.

Selling fast food in the street has long been a tradition, and the best example is Djemaa el Fna square in Marrakech. Starting in the 1980s, new snack restaurants started serving "Bocadillo" (a Spanish word for a sandwich). Though the composition of a bocadillo varies by region, it is usually a baguette filled with salad and a choice of meats or sausage and french fries are placed inside the sandwich.

Dairy product shops locally called Mhlaba, are very prevalent all around the country. Those dairy stores generally offer all types of dairy products, juices, and local delicacies such as Bocadillos, Msemen and Harcha.

Another popular street food in Morocco is the snails, which are served in their stew in small bowls and eaten using a toothpick.

In the late 1990s, several multinational fast-food franchises opened restaurants in major cities.



</doc>
<doc id="19763" url="https://en.wikipedia.org/wiki?curid=19763" title="Martin Van Buren">
Martin Van Buren

Martin Van Buren ( ; born Maarten Van Buren; December 5, 1782 – July 24, 1862) was an American statesman who served as the eighth president of the United States from 1837 to 1841. He was the first president born speaking a language other than English (Dutch) and the first born after the United States had declared its independence from Great Britain. A founder of the Democratic Party, he had previously served as the ninth governor of New York, the tenth United States secretary of state, and the eighth vice president of the United States. He won the 1836 presidential election with the endorsement of popular outgoing President Andrew Jackson and the organizational strength of the Democratic Party. He lost his 1840 reelection bid to Whig Party nominee William Henry Harrison, thanks in part to the poor economic conditions surrounding the Panic of 1837. Later in his life, Van Buren emerged as an elder statesman and an important anti-slavery leader (abolitionist) who led the Free Soil Party ticket in the 1848 presidential election.

Van Buren was born in Kinderhook, New York, to a family of patroon Dutch Americans; his father was a Patriot during the American Revolution. He was raised speaking Dutch and learned English at school, making him the first and still only U.S. president to speak English as his second language. He trained as a lawyer and quickly became involved in politics as a member of the Democratic-Republican Party. He won election to the New York State Senate and became the leader of the Bucktails, the faction of Democratic-Republicans opposed to New York Governor DeWitt Clinton. Van Buren established a political machine known as the Albany Regency and in the 1820s emerged as the most influential politician in the Empire State. He was elected to the United States Senate in 1821 and supported William H. Crawford in the 1824 presidential election. John Quincy Adams won the 1824 election and Van Buren opposed his proposals for federally funded internal improvements and other measures. Van Buren's major political goal was to re-establish a two-party system with partisan differences based on ideology rather than personalities or sectional differences, and he supported Jackson's candidacy against Adams in the 1828 presidential election with this goal in mind. To support Jackson's candidacy, Van Buren ran for Governor of New York; he won, but resigned a few months after assuming the position to accept appointment as U.S. Secretary of State after Jackson took office in March 1829.

Van Buren was a key advisor during Jackson's eight years as President of the United States and he built the organizational structure for the coalescing Democratic Party, particularly in New York. He resigned from his position to help resolve the Petticoat affair, then briefly served as the U.S. ambassador to the United Kingdom. At Jackson's behest, the 1832 Democratic National Convention nominated Van Buren for Vice President of the United States, and he took office after the Democratic ticket won the 1832 presidential election. With Jackson's strong support, Van Buren faced little opposition for the presidential nomination at the 1835 Democratic National Convention, and he defeated several Whig opponents in the 1836 presidential election. Van Buren's response to the Panic of 1837 centered on his Independent Treasury system, a plan under which the Federal government of the United States would store its funds in vaults rather than in banks. He also continued Jackson's policy of Indian removal; he maintained peaceful relations with Britain but denied the application to admit Texas to the Union, seeking to avoid heightened sectional tensions. In the 1840 election, the Whigs rallied around Harrison's military record and ridiculed Van Buren as "Martin Van Ruin", and a surge of new voters helped turn him out of office.

At the opening of the Democratic convention in 1844, Van Buren was the leading candidate for the party's nomination for the presidency. Southern Democrats, however, were angered by his continued opposition to the annexation of Texas, and the party nominated James K. Polk. Van Buren grew increasingly opposed to slavery after he left office, and he agreed to lead a third party ticket in the 1848 presidential election, motivated additionally by intra-party differences at the state and national level. He finished in a distant third nationally, but his presence in the race most likely helped Whig nominee Zachary Taylor defeat Democrat Lewis Cass. Van Buren returned to the Democratic fold after the 1848 election, but he supported Abraham Lincoln's policies during the American Civil War. His health began to fail in 1861, and he died in July 1862, at age 79. He has been generally ranked as an average or below-average U.S. president by historians and political scientists.

Van Buren was born as Maarten Van Buren on December 5, 1782, in Kinderhook, New York about south of Albany on the Hudson River. By American law, he was the first U.S. president not born a British subject, nor of British ancestry. However, because he was born during the American Revolution and before the Peace of Paris, he was for the purposes of British law a British subject at birth.

His father, Abraham Van Buren, was a descendant of Cornelis Maessen of the village of Buurmalsen in Netherlands, who had come to North America in 1631, and purchased a plot of land on Manhattan Island. Abraham Van Buren had been a Patriot during the American Revolution, and he later joined the Democratic-Republican Party. He owned an inn and tavern in Kinderhook and served as Kinderhook's town clerk for several years. In 1776, he married Maria Hoes Van Alen in the town of Kinderhook, also of Dutch extraction and the widow of Johannes Van Alen. She had three children from her first marriage, including future U.S. Representative James I. Van Alen. Her second marriage produced five children, including Martin. Van Buren spoke English as a second language, unlike any other president; his primary language in his youth was Dutch.

Van Buren received a basic education at the village schoolhouse and briefly studied Latin at the Kinderhook Academy and at Washington Seminary in Claverack. His formal education ended in 1796, when he began reading law at the office of Peter Silvester and his son Francis, prominent Federalist Party attorneys in Kinderhook. At his father's inn, Van Buren learned early to interact with people from varied ethnic, income, and societal groups, which he used to his advantage as a political organizer.

Van Buren was small in stature at tall and affectionately nicknamed "Little Van". When he first began his legal studies, he wore rough, homespun clothing, causing the Silvesters to admonish him to pay greater heed to his clothing and personal appearance as an aspiring lawyer. He accepted their advice and subsequently emulated the Silvesters' clothing, appearance, bearing, and conduct.

Van Buren adopted the Democratic-Republican political leanings of his father, despite his association with the Silvesters and Kinderhook's strong affiliation with the Federalist Party. The Silvesters and Democratic-Republican political figure John Peter Van Ness suggested that Van Buren's political leanings constrained him to complete his education with a Democratic-Republican attorney, so he spent a final year of apprenticeship in the New York City office of John Van Ness's brother William P. Van Ness, a political lieutenant of Aaron Burr. Van Ness introduced Van Buren to the intricacies of New York state politics, and Van Buren observed Burr's battles for control of the state Democratic-Republican party against George Clinton and Robert R. Livingston. He returned to Kinderhook in 1803, after being admitted to the New York bar.

Van Buren married Hannah Hoes in Catskill, New York, on February 21, 1807, his childhood sweetheart and a daughter of his first cousin. Like Van Buren, she was raised in a Dutch home in Valatie; she spoke primarily Dutch, and spoke English with a distinct accent. The couple had five children, four of whom lived to adulthood: Abraham (1807–1873), John (1810–1866), Martin Jr. (1812–1855), Winfield Scott (born and died in 1814), and Smith Thompson (1817–1876). Hannah contracted tuberculosis and died on February 5, 1819, at age 35, and Van Buren never remarried.

Upon returning to Kinderhook in 1803, Van Buren formed a law partnership with his half-brother, James Van Alen, and became financially secure enough to increase his focus on politics. Van Buren had been active in politics from age 18, if not before. In 1801, he attended a Democratic-Republican Party convention in Troy, New York where he worked successfully to secure for John Peter Van Ness the party nomination in a special election for the 6th Congressional District seat. Upon returning to Kinderhook, Van Buren broke with the Burr faction, becoming an ally of both DeWitt Clinton and Daniel D. Tompkins. After the faction led by Clinton and Tompkins dominated the 1807 elections, Van Buren was appointed Surrogate of Columbia County, New York. Seeking to find a better base for his political and legal career, Van Buren and his family moved to the town of Hudson, the seat of Columbia County, in 1808. Van Buren's legal practice continued to flourish, and he traveled all over the state to represent various clients.

In 1812, Van Buren won his party's nomination for a seat in the New York State Senate. Though several Democratic-Republicans, including John Peter Van Ness, joined with the Federalists to oppose his candidacy, Van Buren won election to the state senate in mid-1812. Later in the year, the United States entered the War of 1812 against Great Britain, while Clinton launched an unsuccessful bid to defeat President James Madison in the 1812 presidential election. After the election, Van Buren became suspicious that Clinton was working with the Federalist Party, and he broke from his former political ally.

During the War of 1812, Van Buren worked with Clinton, Governor Tompkins, and Ambrose Spencer to support the Madison administration's prosecution of the war. In addition, he was a special judge advocate appointed to serve as a prosecutor of William Hull during Hull's court-martial following the surrender of Detroit. In the winter of 1814–1815, he collaborated with Winfield Scott on ways to reorganize the New York Militia in anticipation of another military campaign, but their work was halted by the end of the war in early 1815. Van Buren was so favorably impressed by Scott that he named his fourth son after him. Van Buren's strong support for the war boosted his standing, and in 1815, he was elected to the position of New York Attorney General. Van Buren moved from Hudson to the state capital of Albany, where he established a legal partnership with Benjamin Butler, and shared a house with political ally Roger Skinner. In 1816, Van Buren won re-election to the state senate, and he would continue to simultaneously serve as both state senator and as the state's attorney general. In 1819, he played an active part in prosecuting the accused murderers of Richard Jennings, the first murder-for-hire case in the state of New York. 

After Tompkins was elected as vice president in the 1816 presidential election, Clinton defeated Van Buren's preferred candidate, Peter Buell Porter, in the 1817 New York gubernatorial election. Clinton threw his influence behind the construction of the Erie Canal, an ambitious project designed to connect Lake Erie to the Atlantic Ocean. Though many of Van Buren's allies urged him to block Clinton's Erie Canal bill, Van Buren believed that the canal would benefit the state. His support for the bill helped it win approval from the New York legislature. Despite his support for the Erie Canal, Van Buren became the leader of an anti-Clintonian faction in New York known as the "Bucktails".

The Bucktails succeeded in emphasizing party loyalty and used it to capture and control many patronage posts throughout New York. Through his use of patronage, loyal newspapers, and connections with local party officials and leaders, Van Buren established what became known as the "Albany Regency", a political machine that emerged as an important factor in New York politics. The Regency relied on a coalition of small farmers, but also enjoyed support from the Tammany Hall machine in New York City. Van Buren largely determined Tammany Hall's political policy for the Democratic-Republicans in this era.

A New York state referendum that expanded state voting rights to all white men in 1821, and which further increased the power of Tammany Hall, was guided by Van Buren. Although Governor Clinton remained in office until late 1822, Van Buren emerged as the leader of the state's Democratic-Republicans after the 1820 elections. Van Buren was a member of the 1820 state constitutional convention, where he favored expanded voting rights, but opposed universal suffrage and tried to maintain property requirements for voting.

In February 1821, the state legislature elected Van Buren to represent New York in the United States Senate. Van Buren arrived in Washington during the "Era of Good Feelings", a period in which partisan distinctions at the national level had faded. Van Buren quickly became a prominent figure in Washington, D.C., befriending Secretary of the Treasury William H. Crawford, among others. Though not an exceptional orator, Van Buren frequently engaged in debate on the Senate floor, usually after extensively researching the subject at hand. Despite his commitments as a father and state party leader, Van Buren remained closely engaged in his legislative duties, and during his time in the Senate he served as the chairman of the Senate Finance Committee and the Senate Judiciary Committee. As he gained renown, Van Buren earned monikers like "Little Magician" and "Sly Fox".

Van Buren chose to back Crawford over John Quincy Adams, Andrew Jackson, and Henry Clay in the presidential election of 1824. Crawford shared Van Buren's affinity for Jeffersonian principles of states' rights and limited government, and Van Buren believed that Crawford was the ideal figure to lead a coalition of New York, Pennsylvania, and Virginia's "Richmond Junto". Van Buren's support for Crawford aroused strong opposition in New York in the form of the People's party, which drew support from Clintonians, Federalists, and others opposed to Van Buren. Nonetheless, Van Buren helped Crawford win the Democratic-Republican party's presidential nomination at the February 1824 congressional nominating caucus. The other Democratic-Republican candidates in the race refused to accept the poorly-attended caucus's decision, and as the Federalist Party had virtually ceased to function as a national party, the 1824 campaign became a competition among four candidates of the same party. Though Crawford suffered a severe stroke that left him in poor health, Van Buren continued to support his chosen candidate. Van Buren met with Thomas Jefferson in May 1824 an attempt to bolster Crawford's candidacy, and though he was unsuccessful in gaining a public endorsement for Crawford, he nonetheless cherished the chance to meet with his political hero.

The 1824 elections dealt a severe blow to the Albany Regency, as Clinton returned to the governorship with the support of the People's party. By the time the state legislature convened to choose the state's presidential electors, results from other states had made it clear that no individual would win a majority of the electoral vote, necessitating a contingent election in the United States House of Representatives. While Adams and Jackson were assured of finishing in the top three, and thus being eligible for selection in the contingent election, New York's electors would help determine whether Clay or Crawford would finish third. Though most of the state's electoral votes went to Adams, Crawford won one more electoral vote than Clay in the state, and Clay's defeat in Louisiana left Crawford in third place. With Crawford still in the running, Van Buren lobbied members of the House to support him. He hoped to engineer a Crawford victory on the second ballot of the contingent election, but Adams won on the first ballot with the help of Clay and Stephen Van Rensselaer, a Congressman from New York. Despite his close ties with Van Buren, Van Rensselaer cast his vote for Adams, thus giving Adams a narrow majority of New York's delegation and a victory in the contingent election.

After the House contest, Van Buren shrewdly kept out of the controversy which followed, and began looking forward to 1828. Jackson was angered to see the presidency go to Adams despite having won more popular votes than he had, and he eagerly looked forward to a rematch. Jackson's supporters accused Adams and Clay of having engaged in a "corrupt bargain" in which Clay helped Adams win the contingent election in return for Clay's appointment as Secretary of State. Always notably courteous in his treatment of opponents, Van Buren showed no bitterness toward either Adams or Clay, and he voted to confirm Clay's nomination to the cabinet. At the same time, Van Buren opposed the Adams-Clay plans for internal improvements like roads and canals and declined to support U.S. participation in the Congress of Panama. Van Buren considered Adams's proposals to represent a return to the Hamiltonian economic model favored by Federalists, which he strongly opposed. Despite his opposition to Adams's public policies, Van Buren was able to easily secure re-election in his own divided home state in 1827.

Van Buren's overarching goal at the national level was to restore a two-party system with party cleavages based on philosophical differences, and he viewed the old divide between Federalists and Democratic-Republicans as the best state of affairs for the nation. Van Buren believed that these national parties helped ensure that elections were decided on national, rather than sectional or local, issues; as he put it, "party attachment in former times furnished a complete antidote for sectional prejudices". After the 1824 election, Van Buren was initially somewhat skeptical of Jackson, who had not taken strong positions on most policy issues. Nonetheless, he settled on Jackson as the one candidate who could beat Adams in the 1828 presidential election, and he worked to bring Crawford's former backers into line behind Jackson.

He also forged alliances with other members of Congress opposed to Adams, including Vice President John C. Calhoun, Senator Thomas Hart Benton, and Senator John Randolph. Seeking to solidify his own standing in New York and bolster Jackson's campaign, Van Buren helped arrange the passage of the Tariff of 1828, which opponents labeled as the "Tariff of Abominations". The tariff satisfied many who sought protection from foreign competition, but angered Southern cotton interests and New Englanders. Because Van Buren believed that the South would never support Adams, and New England would never support Jackson, he was willing to alienate both regions through passage of the tariff.

Meanwhile, Clinton's death from a heart attack in 1828 dramatically shook up the politics of Van Buren's home state, while the Anti-Masonic Party emerged as an increasingly important factor. After some initial reluctance, Van Buren chose to run for Governor of New York in the 1828 election. Hoping that a Jackson victory would lead to his own elevation to Secretary of State or Secretary of the Treasury, Van Buren chose Enos T. Throop as his running mate and preferred successor. Van Buren's candidacy was aided by the split between supporters of Adams, who had adopted the label of National Republicans, and the Anti-Masonic Party.

Reflecting his public association with Jackson, Van Buren accepted the gubernatorial nomination on a ticket that called itself "Jacksonian-Democrat". He campaigned on local as well as national issues, emphasizing his opposition to the policies of the Adams administration. Van Buren ran ahead of Jackson, winning the state by 30,000 votes compared to a margin of 5,000 for Jackson. Nationally, Jackson defeated Adams by a wide margin, winning nearly every state outside of New England. After the election, Van Buren resigned from the Senate to start his term as governor, which began on January 1, 1829. While his term as governor was short, he did manage to pass the Bank Safety Fund Law, an early form of deposit insurance, through the legislature. He also appointed several key supporters, including William L. Marcy and Silas Wright, to important state positions.

In February 1829, Jackson wrote to Van Buren to ask him to become Secretary of State. Van Buren quickly agreed, and he resigned as governor the following month; his tenure of forty-three days is the shortest amount of time served by any Governor of New York. No serious diplomatic crises arose during Van Buren's tenure as Secretary of State, but he achieved several notable successes, such as settling long-standing claims against France and winning reparations for property that had been seized during the Napoleonic Wars. He reached an agreement with the British to open trade with the British West Indies colonies and concluded a treaty with the Ottoman Empire that gained American merchants access to the Black Sea. Items on which he did not achieve success included settling the Maine-New Brunswick boundary dispute with Great Britain, gaining settlement of the U.S. claim to the Oregon Country, concluding a commercial treaty with Russia, and persuading Mexico to sell Texas.

In addition to his foreign policy duties, Van Buren quickly emerged as an important adviser to Jackson on major domestic issues like the tariff and internal improvements. The Secretary of State was instrumental in convincing Jackson to issue the Maysville Road veto, which both reaffirmed limited government principles and also helped prevent the construction of infrastructure projects that could potentially compete with New York's Erie Canal. He also became involved in a power struggle with Calhoun over appointments and other issues, including the Petticoat Affair. The Petticoat Affair arose because Peggy Eaton, wife of Secretary of War John H. Eaton, was ostracized by the other cabinet wives due to circumstances surrounding her marriage.

Led by Floride Calhoun, wife of Vice President Calhoun, the other cabinet wives refused to pay courtesy calls to the Eatons, receive them as visitors, or invite them to social events. As a widower, Van Buren was unaffected by the position of the cabinet wives. Van Buren at first sought to conciliate the divide in the cabinet, but most of the leading citizens in Washington continued to snub the Eatons. Jackson was personally close to Eaton, and he came to the conclusion that the allegations against Eaton arose from a plot against his administration led by Henry Clay. The Petticoat Affair, combined with a contentious debate over the tariff and Calhoun's decade-old criticisms of Jackson's actions in the First Seminole War, contributed to a split between Jackson and Calhoun. As the debate over the tariff and the proposed ability of South Carolina to nullify federal law consumed Washington, Van Buren increasingly emerged as Jackson's likely successor.

The Petticoat affair was finally resolved when Van Buren offered to resign; in April 1831, Jackson accepted, and took the opportunity to reorganize his cabinet by asking for the resignations of the anti-Eaton cabinet members. Postmaster General William T. Barry, who had sided with the Eatons in the Petticoat Affair, was the lone cabinet member to remain in office. The cabinet reorganization removed Calhoun's allies from the Jackson administration, and Van Buren had a major role in shaping the new cabinet. After leaving office, Van Buren continued to play a part in the Kitchen Cabinet, Jackson's informal circle of advisers.

In August 1831, Jackson gave Van Buren a recess appointment as the ambassador to Britain, and Van Buren arrived in London in September. He was cordially received, but in February 1832, he learned his nomination had been rejected by the Senate. The rejection of Van Buren was essentially the work of Calhoun. When the vote on Van Buren's nomination was taken, enough pro-Calhoun Jacksonians refrained from voting to produce a tie, thus giving Calhoun, in his role as presiding officer, the ability to cast the deciding vote against Van Buren.

Calhoun was elated, convinced that he had ended Van Buren's career. "It will kill him dead, sir, kill him dead. He will never kick, sir, never kick", Calhoun exclaimed to a friend. Calhoun's move backfired; by making Van Buren appear the victim of petty politics, Calhoun raised Van Buren in both Jackson's regard and the esteem of others in the Democratic Party. Far from ending Van Buren's career, Calhoun's action gave greater impetus to Van Buren's candidacy for vice president.

Seeking to ensure that Van Buren would replace Calhoun as his running mate, Jackson had arranged for a national convention of his supporters. The May 1832 Democratic National Convention subsequently nominated Van Buren to serve as the party's vice presidential nominee. Van Buren won the nomination over Philip Pendleton Barbour (Calhoun's favored candidate) and Richard Mentor Johnson due to the support of Jackson and the strength of the Albany Regency. Upon Van Buren's return from Europe in July 1832, he became involved in the Bank War, a struggle over the re-charter of the Second Bank of the United States.

Van Buren had long been distrustful of banks, and he viewed the Bank as an extension of the Hamiltonian economic program, so he supported Jackson's veto of the Bank's re-charter. Henry Clay, the presidential nominee of the National Republicans, made the struggle over the Bank the key issue of the presidential election of 1832. The Jackson–Van Buren ticket won the 1832 election by a landslide, and Van Buren took office as vice president in March 1833. During the Nullification Crisis, Van Buren counseled Jackson to pursue a policy of conciliation with South Carolina leaders. He played little direct role in the passage of the Tariff of 1833, but he quietly hoped that the tariff would help bring an end to the Nullification Crisis, which it did.

During his time in office, Van Buren continued to be one of Jackson's primary advisors and confidants, and accompanied Jackson on his tour of the northeastern United States in 1833. Jackson's struggle with the Second Bank of the United States continued, as the president sought to remove federal funds from the Bank. Though at first apprehensive of the removal due to congressional support for the Bank, Van Buren eventually came to support Jackson's policy. He also helped undermine a fledgling alliance between Jackson and Daniel Webster, a senator from Massachusetts who could have potentially threatened Van Buren's project to create two parties separated by policy differences rather than personalities. During Jackson's second term, the president's supporters began to refer to themselves as members of the Democratic Party. Meanwhile, those opposed to Jackson, including Clay's National Republicans, followers of Calhoun, and many members of the Anti-Masonic Party, coalesced into the Whig Party.

President Andrew Jackson declined to seek another term in the 1836 presidential election, but he remained influential within the Democratic Party as his second term came to an end. Jackson was determined to help elect Van Buren in 1836 so that the latter could continue the Jackson administration's policies. the two men-–the charismatic "Old Hickory" and the super-efficient "Sly Fox"--had entirely different personalities but had become an effective team in eight years in office together. With Jackson's support, Van Buren won the presidential nomination of the 1835 Democratic National Convention without opposition. Two names were put forward for the vice-presidential nomination: Representative Richard M. Johnson of Kentucky, and former Senator William Cabell Rives of Virginia. Southern Democrats, and Van Buren himself, strongly preferred Rives. Jackson, on the other hand, strongly preferred Johnson. Again, Jackson's considerable influence prevailed, and Johnson received the required two-thirds vote after New York Senator Silas Wright prevailed upon non-delegate Edward Rucker to cast the 15 votes of the absent Tennessee delegation in Johnson's favor.

Van Buren's competitors in the election of 1836 were three members of the Whig Party, which remained a loose coalition bound by mutual opposition to Jackson's anti-bank policies. Lacking the party unity or organizational strength to field a single ticket or define a single platform, the Whigs ran several regional candidates in hopes of sending the election to the House of Representatives. The three candidates were: Hugh Lawson White of Tennessee, Daniel Webster of Massachusetts, and William Henry Harrison of Indiana. Besides endorsing internal improvements and a national bank, the Whigs tried to tie Democrats to abolitionism and sectional tension, and attacked Jackson for "acts of aggression and usurpation of power".

Southern voters represented the biggest potential impediment in Van Buren's quest for the presidency, as many were suspicious of a Northern president. Van Buren moved to obtain the support of southerners by assuring them that he opposed abolitionism and supported the maintaining of slavery in states where it had already existed. To demonstrate consistency regarding his opinions on slavery, Van Buren cast the tie-breaking Senate vote in favor of a bill to subject abolitionist mail to state laws, thus ensuring that its circulation would be prohibited in the South. Van Buren personally considered slavery to be immoral, but sanctioned by the Constitution.

Van Buren won the election with 764,198 popular votes, 50.9% of the total, and 170 electoral votes. Harrison led the Whigs with 73 electoral votes, White receiving 26, and Webster 14. Willie Person Mangum received South Carolina's 11 electoral votes, which were awarded by the state legislature. Van Buren's victory resulted from a combination of his own attractive political and personal qualities, Jackson's popularity and endorsement, the organizational power of the Democratic party, and the inability of the Whig Party to muster an effective candidate and campaign. Virginia's presidential electors voted for Van Buren for president, but voted for William Smith for vice president, leaving Johnson one electoral vote short of election. In accordance with the Twelfth Amendment, the Senate elected Johnson vice president in a contingent vote.

The election of 1836 marked an important turning point in American political history because it saw the establishment of the Second Party System. In the early 1830s, the political party structure was still changing, rapidly, and factional and personal leaders continued to play a major role in politics. By the end of the campaign of 1836, the new party system was almost complete, as nearly every faction had been absorbed by either the Democrats or the Whigs.

Van Buren retained much of Jackson's cabinet and lower-level appointees, as he hoped that the retention of Jackson's appointees would stop Whig momentum in the South and restore confidence in the Democrats as a party of sectional unity. The cabinet holdovers represented the different regions of the country: Secretary of the Treasury Levi Woodbury came from New England, Attorney General Benjamin F. Butler and Secretary of the Navy Mahlon Dickerson hailed from mid-Atlantic states, Secretary of State John Forsyth represented the South, and Postmaster General Amos Kendall of Kentucky represented the West.

For the lone open position of Secretary of War, Van Buren first approached William Cabell Rives, who had sought the vice presidency in 1836. After Rives declined to join the cabinet, Van Buren appointed Joel Roberts Poinsett, a South Carolinian who had opposed secession during the Nullification Crisis. Van Buren's cabinet choices were criticized by Pennsylvanians such as James Buchanan, who argued that their state deserved a cabinet position as well as some Democrats who argued that Van Buren should have used his patronage powers to augment his own power. However, Van Buren saw value in avoiding contentious patronage battles, and his decision to retain Jackson's cabinet made it clear that he intended to continue the policies of his predecessor. Additionally, Van Buren had helped select Jackson's cabinet appointees and enjoyed strong working relationships with them.

Van Buren held regular formal cabinet meetings and discontinued the informal gatherings of advisers that had attracted so much attention during Jackson's presidency. He solicited advice from department heads, tolerated open and even frank exchanges between cabinet members, perceiving himself as "a mediator, and to some extent an umpire between the conflicting opinions" of his counselors. Such detachment allowed the president to reserve judgment and protect his own prerogative for making final decisions. These open discussions gave cabinet members a sense of participation and made them feel part of a functioning entity, rather than isolated executive agents. Van Buren was closely involved in foreign affairs and matters pertaining to the Treasury Department, but the Post Office, War Department, and Navy Department all had possessed high levels of autonomy under their respective cabinet secretaries.

When Van Buren entered office, the nation's economic health had taken a turn for the worse and the prosperity of the early 1830s was over. Two months into his presidency, on May 10, 1837, some important state banks in New York, running out of hard currency reserves, refused to convert paper money into gold or silver, and other financial institutions throughout the nation quickly followed suit. This financial crisis would become known as the Panic of 1837. The Panic was followed by a five-year depression in which banks failed and unemployment reached record highs.

Van Buren blamed the economic collapse on greedy American and foreign business and financial institutions, as well as the over-extension of credit by U.S. banks. Whig leaders in Congress blamed the Democrats, along with Andrew Jackson's economic policies, specifically his 1836 Specie Circular. Cries of "rescind the circular!" went up and former president Jackson sent word to Van Buren asking him not to rescind the order, believing that it had to be given enough time to work. Others, like Nicholas Biddle, believed that Jackson's dismantling of the Bank of the United States was directly responsible for the irresponsible creation of paper money by the state banks which had precipitated this panic. The Panic of 1837 loomed large over the 1838 election cycle, as the carryover effects of the economic downturn led to Whig gains in both the U.S. House and Senate. The state elections in 1837 and 1838 were also disastrous for the Democrats, and the partial economic recovery in 1838 was offset by a second commercial crisis later that year.

To deal with the crisis, the Whigs proposed rechartering the national bank. The president countered by proposing the establishment of an independent U.S. treasury, which he contended would take the politics out of the nation's money supply. Under the plan, the government would hold all of its money balances in the form of gold or silver, and would be restricted from printing paper money at will; both measures were designed to prevent inflation. The plan would permanently separate the government from private banks by storing government funds in government vaults rather than in private banks. Van Buren announced his proposal in September 1837, but an alliance of conservative Democrats and Whigs prevented it from becoming law until 1840. As the debate continued, conservative Democrats like Rives defected to the Whig Party, which itself grew more unified in its opposition to Van Buren. The Whigs would abolish the Independent Treasury system in 1841, but it was revived in 1846, and remained in place until the passage of the Federal Reserve Act in 1913. More important for Van Buren's immediate future, the depression would be a major issue in his upcoming re-election campaign.

Federal policy under Jackson had sought to move Indian tribes to lands west of the Mississippi River through the Indian Removal Act of 1830, and the federal government negotiated 19 treaties with Indian tribes in the course of Van Buren's presidency. The 1835 Treaty of New Echota signed by government officials and representatives of the Cherokee tribe had established terms under which the Cherokees ceded their territory in the southeast and agreed to move west to Oklahoma. In 1838, Van Buren directed General Winfield Scott to forcibly move all those who had not yet complied with the treaty.

The Cherokees were herded violently into internment camps where they were kept for the summer of 1838. The actual transportation west was delayed by intense heat and drought, but they were forcibly marched west in the fall. Under the treaty, the government was supposed to provide wagons, rations, and even medical doctors, but it did not. Some 20,000 people were relocated against their will during the Cherokee removal, part of the Trail of Tears. Notably, Ralph Waldo Emerson, who would go on to become America's foremost man of letters, wrote Van Buren a letter protesting his treatment of the Cherokee.

The administration also contended with the Seminole Indians, who engaged the army in a prolonged conflict known as the Second Seminole War. Prior to leaving office, Jackson put General Thomas Jesup in command of all military troops in Florida to force Seminole emigration to the West. Forts were established throughout the Indian territory, and mobile columns of soldiers scoured the countryside, and many Seminoles offered to surrender, including Chief Micanopy. The Seminoles slowly gathered for emigration near Tampa, but in June they fled the detention camps, driven off by disease and the presence of slave catchers who were hoping to capture Black Seminoles.

In December 1837, Jesup began a massive offensive, culminating in the Battle of Lake Okeechobee, and the war entered a new phase of attrition. During this time, the government realized that it would be almost impossible to drive the remaining Seminoles from Florida, so Van Buren sent General Alexander Macomb to negotiate peace with them. It was the only time that an Indian tribe had forced the government to sue for peace. An agreement was reached allowing the Seminoles to remain in southwest Florida, but the peace was shattered in July 1839 and was not restored until 1842, after Van Buren had left office.

Just before leaving office in March 1837, Andrew Jackson extended diplomatic recognition to the Republic of Texas, which had gained de facto independence from Mexico in the Texas Revolution. By suggesting the prospect of quick annexation, Jackson raised the danger of war with Mexico and heightened sectional tensions at home. New England abolitionists charged that there was a "slaveholding conspiracy to acquire Texas", and Daniel Webster eloquently denounced annexation. Many Southern leaders, meanwhile, strongly desired the expansion of slave-holding territory in the United States.

Boldly reversing Jackson's policies, Van Buren sought peace abroad and harmony at home. He proposed a diplomatic solution to a long-standing financial dispute between American citizens and the Mexican government, rejecting Jackson's threat to settle it by force. Likewise, when the Texas minister at Washington, D.C., proposed annexation to the administration in August 1837, he was told that the proposition could not be entertained. Constitutional scruples and fear of war with Mexico were the reasons given for the rejection, but concern that it would precipitate a clash over the extension of slavery undoubtedly influenced Van Buren and continued to be the chief obstacle to annexation. Northern and Southern Democrats followed an unspoken rule in which Northerners helped quash anti-slavery proposals and Southerners refrained from agitating for the annexation of Texas. Texas withdrew the annexation offer in 1838.

British subjects in Lower Canada (now Quebec) and Upper Canada (now Ontario) rose in rebellion in 1837 and 1838, protesting their lack of responsible government. While the initial insurrection in Upper Canada ended quickly (following the December 1837 Battle of Montgomery's Tavern), many of the rebels fled across the Niagara River into New York, and Canadian leader William Lyon Mackenzie began recruiting volunteers in Buffalo. Mackenzie declared the establishment of the Republic of Canada and put into motion a plan whereby volunteers would invade Upper Canada from Navy Island on the Canadian side of the Niagara River. Several hundred volunteers traveled to Navy Island in the weeks that followed. They procured the steamboat "Caroline" to deliver supplies to Navy Island from Fort Schlosser. Seeking to deter an imminent invasion, British forces crossed to the American bank of the river in late December 1837, and they burned and sank the "Caroline". In the melee, one American was killed and others were wounded.

Considerable sentiment arose within the United States to declare war, and a British ship was burned in revenge. Van Buren, looking to avoid a war with Great Britain, sent General Winfield Scott to the Canada–United States border with large discretionary powers for its protection and its peace. Scott impressed upon American citizens the need for a peaceful resolution to the crisis, and made it clear that the U.S. government would not support adventuresome Americans attacking the British. Also, in early January 1838, the president proclaimed U.S. neutrality with regard to the Canadian independence issue, a declaration which Congress endorsed by passing a neutrality law designed to discourage the participation of American citizens in foreign conflicts.
During the Canadian rebellions, Charles Duncombe and Robert Nelson helped foment a largely American militia, the Hunters' Lodge/Frères chasseurs. This militia carried out several attacks in Upper Canada between December 1837 and December 1838, collectively known as the Patriot War. The administration followed through on its enforcement of the Neutrality Act, encouraged the prosecution of filibusters, and actively deterred U.S. citizens from subversive activities abroad. In the long term, Van Buren's opposition to the Patriot War contributed to the construction of healthy Anglo-American and Canada–United States relations in the 20th century; it also led, more immediately, to a backlash among citizens regarding the seeming overreach of federal authority, which hurt congressional Democrats in the 1838 midterm elections.

A new crisis surfaced in late 1838, in the disputed territory on the Maine–New Brunswick frontier, where Americans were settling on long-disputed land claimed by the United States and Great Britain. Jackson had been willing to drop American claims to the region in return for other concessions, but Maine was unwilling to drop its claims to the disputed territory. For their part, the British considered possession of the area vital to the defense of Canada. Both American and New Brunswick lumberjacks cut timber in the disputed territory during the winter of 1838–1839. On December 29, New Brunswick lumbermen were spotted cutting down trees on an American estate near the Aroostook River.

After American woodcutters rushed to stand guard, a shouting match, known as the Battle of Caribou, ensued. Tensions quickly boiled over into a near war with both Maine and New Brunswick arresting each other's citizens. The crisis seemed ready to turn into an armed conflict. British troops began to gather along the Saint John River. Governor John Fairfield mobilized the state militia to confront the British in the disputed territory and several forts were constructed. The American press clamored for war; "Maine and her soil, or BLOOD!" screamed one editorial. "Let the sword be drawn and the scabbard thrown away!" In June, Congress authorized 50,000 troops and a $10 million budget in the event foreign military troops crossed into United States territory.

Van Buren was unwilling to go to war over the disputed territory, though he assured Maine that he would respond to any attacks by the British. To settle the crisis, Van Buren met with the British minister to the United States, and Van Buren and the minister agreed to resolve the border issue diplomatically. Van Buren also sent General Scott to the northern border area, both to show military resolve, and more importantly, to lower the tensions. Scott successfully convinced all sides to submit the border issue to arbitration. The border dispute was put to rest a few years later, with the signing of the 1842 Webster–Ashburton Treaty.

The "Amistad" case was a freedom suit that involved international issues and parties, as well as United States law, resulting from the rebellion of Africans on board the Spanish schooner "La Amistad" in 1839. Van Buren viewed abolitionism as the greatest threat to the nation's unity, and he resisted the slightest interference with slavery in the states where it existed. His administration supported the Spanish government's demand that the ship and its cargo (including the Africans) be turned over to them. A federal district court judge ruled that the Africans were legally free and should be transported home, but Van Buren's administration appealed the case to the Supreme Court.

In February 1840, former president John Quincy Adams argued passionately for the Africans' right to freedom, and Attorney General Henry D. Gilpin presented the government's case. In March 1841, the Supreme Court issued its final verdict: the "Amistad" Africans were free people and should be allowed to return home. The unique nature of the case heightened public interest in the saga, including the participation of former president Adams, Africans testifying in federal court, and their being represented by prominent lawyers. The Amistad case drew attention to the personal tragedies of slavery and attracted new support for the growing abolition movement in the North. It also transformed the courts into the principal forum for a national debate on the legal foundations of slavery.

Van Buren appointed two Associate Justices to the Supreme Court: John McKinley, confirmed September 25, 1837, and Peter Vivian Daniel, confirmed March 2, 1841. He also appointed eight other federal judges, all to United States district courts.

For the first half of his presidency, Van Buren, who had been a widower for many years, did not have a specific person fill the role of White House hostess at administration social events, but tried to assume such duties himself. When his eldest son Abraham Van Buren married Angelica Singleton in 1838, he quickly acted to install his daughter-in-law as his hostess. She solicited the advice of her distant relative, Dolley Madison, who had moved back to Washington after her husband's death, and soon the president's parties livened up. After the 1839 New Year's Eve reception, the "Boston Post" raved: "[Angelica Van Buren is a] lady of rare accomplishments, very modest yet perfectly easy and graceful in her manners and free and vivacious in her conversation ... universally admired."

As the nation endured a deep economic depression, Angelica Van Buren's receiving style at receptions was influenced by her heavy reading on European court life (and her naive delight in being received as the "Queen of the United States" when she visited the royal courts of England and France after her marriage). Newspaper coverage of this, in addition to the claim that she intended to re-landscape the White House grounds to resemble the royal gardens of Europe, was used in a political attack on her father-in-law by a Pennsylvania Whig Congressman Charles Ogle. He referred obliquely to her as part of the presidential "household" in his famous Gold Spoon Oration. The attack was delivered in Congress and the depiction of the president as living a royal lifestyle was a primary factor in his defeat for re-election.

Van Buren easily won renomination for a second term at the 1840 Democratic National Convention, but he and his party faced a difficult election in 1840. Van Buren's presidency had been a difficult affair, with the U.S. economy mired in a severe downturn, and other divisive issues, such as slavery, western expansion, and tensions with Great Britain, providing opportunities for Van Buren's political opponents—including some of his fellow Democrats—to criticize his actions. Although Van Buren's renomination was never in doubt, Democratic strategists began to question the wisdom of keeping Johnson on the ticket. Even former president Jackson conceded that Johnson was a liability and insisted on former House Speaker James K. Polk of Tennessee as Van Buren's new running mate. Van Buren was reluctant to drop Johnson, who was popular with workers and radicals in the North and added military experience to the ticket, which might prove important against likely Whig nominee William Henry Harrison. Rather than re-nominating Johnson, the Democratic convention decided to allow state Democratic Party leaders to select the vice-presidential candidates for their states.

Van Buren hoped that the Whigs would nominate Clay for president, which would allow Van Buren to cast the 1840 campaign as a clash between Van Buren's Independent Treasury system and Clay's support for a national bank. However, rather than nominating longtime party spokesmen like Clay and Daniel Webster, the 1839 Whig National Convention nominated Harrison, who had served in various governmental positions during his career and had earned notoriety for his military leadership in the Battle of Tippecanoe and the War of 1812. Whig leaders like William Seward and Thaddeus Stevens believed that Harrison's war record would effectively counter the popular appeals of the Democratic Party. For vice president, the Whigs nominated former Senator John Tyler of Virginia. Clay was deeply disappointed by his defeat at the convention, but he nonetheless threw his support behind Harrison.

Whigs presented Harrison as the antithesis of the president, whom they derided as ineffective, corrupt, and effete. Whigs also depicted Van Buren as an aristocrat living in high style in the White House, while they used images of Harrison in a log cabin sipping cider to convince voters that he was a man of the people. They threw such jabs as "Van, Van, is a used-up man" and "Martin Van Ruin" and ridiculed him in newspapers and cartoons. Issues of policy were not absent from the campaign; the Whigs derided the alleged executive overreaches of Jackson and Van Buren, while also calling for a national bank and higher tariffs. Democrats attempted to campaign on the Independent Treasury system, but the onset of deflation undercut these arguments. The enthusiasm for "Tippecanoe and Tyler Too", coupled with the country's severe economic crisis, made it impossible for Van Buren to win a second term. Harrison won by a popular vote of 1,275,612 to 1,130,033, and an electoral vote margin of 234 to 60. An astonishing 80% of eligible voters went to the polls on election day. Van Buren actually won more votes than he had in 1836, but the Whig success in attracting new voters more than canceled out Democratic gains. Additionally, Whigs won majorities for the first time in both the House of Representatives and the Senate.

On the expiration of his term, Van Buren returned to his estate of Lindenwald in Kinderhook. He continued to closely watch political developments, including the battle between Clay and President Tyler, who took office after Harrison's death in April 1841. Though undecided on another presidential run, Van Buren made several moves calculated to maintain his support, including a trip to the South and West during which he met with Jackson, former Speaker of the House James K. Polk, and others. President Tyler, James Buchanan, Levi Woodbury, and others loomed as potential challengers for the 1844 Democratic nomination, but it was Calhoun who posed the most formidable obstacle.

Van Buren remained silent on major public issues like the debate over the Tariff of 1842, hoping to arrange for the appearance of a draft movement for his presidential candidacy. President John Tyler made annexation of Texas his chief foreign policy goal, and many Democrats, particularly in the South, were anxious to quickly complete the annexation of Texas. After an explosion on the killed Secretary of State Abel P. Upshur in February 1844, Tyler brought Calhoun into his cabinet to direct foreign affairs. Like Tyler, Calhoun pursued the annexation of Texas to upend the presidential race and to extend slavery into new territories.

Shortly after taking office, Secretary of State Calhoun negotiated an annexation treaty between the United States and Texas. Van Buren had hoped he would not have to take a public stand on annexation, but as the Texas question came to dominate U.S. politics, he decided to make his views on the issue public. Though he believed that his public acceptance of annexation would likely help him win the 1844 Democratic nomination, Van Buren thought that annexation would inevitably lead to an unjust war with Mexico. In a public letter published shortly after Henry Clay also announced his opposition to the annexation treaty, Van Buren articulated his views on the Texas question.

Van Buren's opposition to immediate annexation cost him the support of many pro-slavery Democrats. In the weeks before the 1844 Democratic National Convention, Van Buren's supporters anticipated that he would win a majority of the delegates on the first presidential ballot, but would not be able to win the support of the required two-thirds of delegates. Van Buren's supporters attempted to prevent the adoption of the two-thirds rule, but several Northern delegates joined with Southern delegates in implementing the two-thirds rule for the 1844 convention. Van Buren won 146 of the 266 votes on the first presidential ballot, with only 12 of his votes coming from Southern states.

Senator Lewis Cass won much of the remaining vote, and he gradually picked up support on subsequent ballots until the convention adjourned for the day. When the convention reconvened and held another ballot, James K. Polk, who shared many of Van Buren's views but favored immediate annexation, won 44 votes. On the ninth and final ballot of the convention, Van Buren's supporters withdrew the former president's name from consideration, and Polk won the Democratic presidential nomination. Though angered at the way in which his opponents had denied him in the Democratic nomination, Van Buren endorsed Polk in the interest of party unity. He also convinced Silas Wright to run for Governor of New York so that the popular Wright could help boost Polk in the state. Wright narrowly defeated Whig nominee Millard Fillmore in the 1844 gubernatorial election, and Wright's victory in the state helped Polk narrowly defeat Henry Clay in the 1844 presidential election.

After taking office, Polk used George Bancroft as an intermediary to offer Van Buren the ambassadorship to London. Van Buren declined, partly because he was upset with Polk over the treatment the Van Buren delegates had received at the 1844 convention, and partly because he was content in his retirement. Polk also consulted Van Buren in the formation of his cabinet, but offended Van Buren by offering to appoint a New Yorker only to the lesser post of Secretary of War, rather than as Secretary of State or Secretary of the Treasury. Other patronage decisions also angered Van Buren and Wright, and they became permanently alienated from the Polk administration.

Though he had previously helped maintain a balance between the Barnburners and Hunkers, the two factions of the New York Democratic Party, Van Buren moved closer to the Barnburners after the 1844 Democratic National Convention. The split in the state party worsened during the Polk's presidency, as his administration lavished patronage on the Hunkers. In his retirement, Van Buren also grew increasingly opposed to slavery.

As the Mexican–American War brought the debate over slavery in the territories to the forefront of American politics, Van Buren published an anti-slavery manifesto. In it, he refuted the notion that Congress did not have the power to regulate slavery in the territories, and argued the Founding Fathers had favored the eventual abolition of slavery. The document, which became known as the "Barnburner Manifesto," was edited at Van Buren's request by John Van Buren and Samuel Tilden, both of whom were leaders of the Barnburner faction. After the publication of the Barnburner Manifesto, many Barnburners urged the former president to seek his old office in the 1848 presidential election. The 1848 Democratic National Convention seated competing Barnburner and Hunker delegations from New York, but the Barnburners walked out of the convention when Lewis Cass, who opposed congressional regulation of slavery in the territories, was nominated on the fourth ballot.

In response to the nomination of Cass, the Barnburners began to organize as a third party. At a convention held in June 1848, in Utica, New York, the Barnburners nominated Van Buren for president. Though reluctant to bolt from the Democratic Party, Van Buren accepted the nomination to show the power of the anti-slavery movement, help defeat Cass, and weaken the Hunkers. At a convention held in Buffalo, New York in August 1848, a group of anti-slavery Democrats, Whigs, and members of the abolitionist Liberty Party met in the first national convention of what became known as the Free Soil Party.

The convention unanimously nominated Van Buren, and chose Charles Francis Adams as Van Buren's running mate. In a public message accepting the nomination, Van Buren gave his full support for the Wilmot Proviso, a proposed law that would ban slavery in all territories acquired from Mexico in the Mexican–American War. Van Buren won no electoral votes, but finished second to Whig nominee Zachary Taylor in New York, taking enough votes from Cass to give the state—and perhaps the election—to Taylor. Nationwide, Van Buren won 10.1% of the popular vote, the strongest showing by a third party presidential nominee up to that point in U.S. history.

Van Buren never sought public office again after the 1848 election, but he continued to closely follow national politics. He was deeply troubled by the stirrings of secessionism in the South and welcomed the Compromise of 1850 as a necessary conciliatory measure despite his opposition to the Fugitive Slave Act of 1850. Van Buren also worked on a history of American political parties and embarked on a tour of Europe, becoming the first former American head of state to visit Britain. Though still concerned about slavery, Van Buren and his followers returned to the Democratic fold, partly out of the fear that a continuing Democratic split would help the Whig Party. He also attempted to reconcile the Barnburners and the Hunkers, with mixed results.

Van Buren supported Franklin Pierce for president in 1852, James Buchanan in 1856, and Stephen A. Douglas in 1860. Van Buren viewed the fledgling Know Nothing movement with contempt and felt that the anti-slavery Republican Party exacerbated sectional tensions. He considered Chief Justice Roger Taney's decision in the 1857 case of "Dred Scott v. Sandford" to be a "grievous mistake" since it overturned the Missouri Compromise. He believed that the Buchanan administration handled the issue of Bleeding Kansas poorly, and saw the Lecompton Constitution as a sop to Southern extremists.

After the election of Abraham Lincoln and the secession of several Southern states in 1860, Van Buren unsuccessfully sought to call a constitutional convention. In April 1861, former president Pierce wrote to the other living former presidents and asked them to consider meeting to use their stature and influence to propose a negotiated end to the war. Pierce asked Van Buren to use his role as the senior living ex-president to issue a formal call. Van Buren's reply suggested that Buchanan should be the one to call the meeting, since he was the former president who had served most recently, or that Pierce should issue the call himself if he strongly believed in the merit of his proposal. Neither Buchanan nor Pierce was willing to make Pierce's proposal public, and nothing more resulted from it. Once the American Civil War began, Van Buren made public his support for the Union.

Van Buren's health began to fail later in 1861, and he was bedridden with pneumonia during the fall and winter of 1861–1862. He died of bronchial asthma and heart failure at his Lindenwald estate at 2:00 a.m. on July 24, 1862, at the age of 79. He is buried in the Kinderhook Reformed Dutch Church Cemetery, as are his wife Hannah, his parents, and his son Martin Van Buren Jr.

Van Buren outlived all four of his immediate successors: Harrison, Tyler, Polk, and Taylor. Despite this, he is not one of the longest lived presidents.

Van Buren's most lasting achievement was as a political organizer who built the Democratic Party and guided it to dominance in the Second Party System, and historians have come to regard Van Buren as integral to the development of the American political system. According to historian Robert Remini:

However, his presidency is considered to be average, at best, by historians. He was blamed for the economic troubles and was defeated for reelection. His time in office was dominated by the economic disaster of the Panic of 1837, and historians have split on the adequacy of the Independent Treasury as a response to that issue. Several writers have portrayed Van Buren as among the nation's most obscure presidents. As noted in a 2014 "Time" magazine article on the "Top 10 Forgettable Presidents":

Van Buren's home in Kinderhook, New York, which he called Lindenwald, is now the Martin Van Buren National Historic Site. Counties are named for Van Buren in Michigan, Iowa, Arkansas, and Tennessee. Mount Van Buren, , three state parks and numerous towns were named after him.

During the 1988 presidential campaign, George H. W. Bush, a Yale University graduate and member of the Skull and Bones secret society, was attempting to become the first incumbent vice president to win election to the presidency since Van Buren. In the comic strip "Doonesbury", artist Garry Trudeau depicted members of Skull and Bones as attempting to rob Van Buren's grave, apparently intending to use the relics in a ritual that would aid Bush in the election.

Van Buren is portrayed by Nigel Hawthorne in the 1997 film "Amistad". The film depicts the legal battle surrounding the status of slaves who in 1839 rebelled against their transporters on "La Amistad" slave ship. On the television show "Seinfeld", the episode "The Van Buren Boys" is about a fictional street gang that admires Van Buren and bases its rituals and symbols on him, including the hand sign of eight fingers pointing up.

Also, in an episode of "The Monkees", about the pre-formed pop band of the same name (the episode was titled "Dance, Monkee, Dance"), a dance instruction studio offers free lessons to anyone who can answer the question, "Who was the eighth president of the United States?" Martin Van Buren shows up at the studio to begin his free dance lessons.





</doc>
<doc id="19765" url="https://en.wikipedia.org/wiki?curid=19765" title="Melbourne Cricket Ground">
Melbourne Cricket Ground

The Melbourne Cricket Ground (MCG), also known simply as "The G", is an Australian sports stadium located in Yarra Park, Melbourne, Victoria. Founded and managed by the Melbourne Cricket Club, it is the largest stadium in the Southern Hemisphere, the 10th largest globally, and the largest cricket ground by capacity. The MCG is within walking distance of the city centre and is served by Richmond and Jolimont railway stations, as well as the route 70 tram. It is adjacent to Melbourne Park and is part of the Melbourne Sports and Entertainment Precinct.

Since it was built in 1853, the MCG has undergone numerous renovations. It served as the centrepiece stadium of the 1956 Summer Olympics, the 2006 Commonwealth Games and two Cricket World Cups: 1992 and 2015. Noted for its role in the development of international cricket, the MCG hosted both the first Test match and the first One Day International, played between Australia and England in 1877 and 1971 respectively. It has also maintained strong ties with Australian rules football since its codification in 1859, and has become the principal venue for Australian Football League (AFL) matches, including the AFL Grand Final, the world's highest attended league championship event.

Home to the National Sports Museum, the MCG has hosted other major sporting events, including international rules football matches between Australia and Ireland, international rugby union matches, State of Origin (rugby league) games, and FIFA World Cup qualifiers. Concerts and other cultural events are also held at the venue with the record attendance standing at 143,750 for a Billy Graham evangelistic crusade in 1959. Grandstand redevelopments and occupational health and safety legislation have limited the maximum seating capacity to approximately 95,000 with an additional 5,000 standing room capacity, bringing the total capacity to 100,024.

The MCG is listed on the Victorian Heritage Register and was included on the Australian National Heritage List in 2005. Journalist Greg Baum called it "a shrine, a citadel, a landmark, a totem" that "symbolises Melbourne to the world".

Founded in November 1838 the Melbourne Cricket Club (MCC) selected the current MCG site in 1853 after previously playing at several grounds around Melbourne. The club's first game was against a military team at the Old Mint site, at the corner of William and Latrobe Streets. Burial Hill (now Flagstaff Gardens) became its home ground in January 1839, but the area was already set aside for Botanical Gardens and the club was moved on in October 1846, to an area on the south bank of the Yarra about where the Herald and Weekly Times building is today. The area was subject to flooding, forcing the club to move again, this time to a ground in South Melbourne.

It was not long before the club was forced out again, this time because of the expansion of the railway. The South Melbourne ground was in the path of Victoria's first steam railway line from Melbourne to Sandridge (now Port Melbourne). Governor La Trobe offered the MCC a choice of three sites; an area adjacent to the existing ground, a site at the junction of Flinders and Spring Streets or a ten-acre (about 4 hectares) section of the Government Paddock at Richmond next to Richmond Park.

This last option, which is now Yarra Park, had been used by Aborigines until 1835. Between 1835 and the early 1860s it was known as the Government or Police Paddock and served as a large agistment area for the horses of the Mounted Police, Border Police and Native Police. The north-eastern section also housed the main barracks for the Mounted Police in the Port Phillip district. In 1850 it was part of a stretch set aside for public recreation extending from Governor La Trobe's Jolimont Estate to the Yarra River. By 1853 it had become a busy promenade for Melbourne residents.

An MCC sub-committee chose the Richmond Park option because it was level enough for cricket but sloped enough to prevent inundation. That ground was located where the Richmond, or outer, end of the current MCG is now.

At the same time the Richmond Cricket Club was given occupancy rights to six acres (2.4 hectares) for another cricket ground on the eastern side of the Government Paddock.

At the time of the land grant the Government stipulated that the ground was to be used for cricket and cricket only. This condition remained until 1933 when the State Government allowed the MCG's uses to be broadened to include other purposes when not being used for cricket.

In 1863 a corridor of land running diagonally across Yarra Park was granted to the Hobson's Bay Railway and divided Yarra Park from the river. The Mounted Police barracks were operational until the 1880s when it was subdivided into the current residential precinct bordered by Vale Street. The area closest to the river was also developed for sporting purposes in later years including Olympic venues in 1956.

The first grandstand at the MCG was the original wooden members’ stand built in 1854, while the first public grandstand was a 200-metre long 6000-seat temporary structure built in 1861. Another grandstand seating 2000, facing one way to the cricket ground and the other way to the park where football was played, was built in 1876 for the 1877 visit of James Lillywhite's English cricket team. It was during this tour that the MCG hosted the world's first Test match.

In 1881 the original members' stand was sold to the Richmond Cricket Club for £55. A new brick stand, considered at the time to be the world's finest cricket facility, was built in its place. The foundation stone was laid by Prince George of Wales and Prince Albert Victor on 4 July and the stand opened in December that year. It was also in 1881 that a telephone was installed at the ground, and the wickets and goal posts were changed from an east-west orientation to north-south. In 1882 a scoreboard was built which showed details of the batsman's name and how he was dismissed.

When the Lillywhite tour stand burnt down in 1884 it was replaced by a new stand which seated 450 members and 4500 public. In 1897, second-storey wings were added to ‘The Grandstand’, as it was known, increasing capacity to 9,000. In 1900 it was lit with electric light.

More stands were built in the early 20th century. An open wooden stand was on the south side of the ground in 1904 and the 2084-seat Grey Smith Stand (known as the New Stand until 1912) was erected for members in 1906. The 4000-seat Harrison Stand on the ground's southern side was built in 1908 followed by the 8000-seat Wardill Stand in 1912. In the 15 years after 1897 the stand capacity at the ground increased to nearly 20,000.

In 1927 the second brick members’ stand was replaced at a cost of £60,000. The Harrison and Wardill Stands were demolished in 1936 to make way for the Southern Stand which was completed in 1937. The Southern Stand seated 18,200 under cover and 13,000 in the open and was the main public area of the MCG. The maximum capacity of the ground under this configuration, as advised by the Health Department, was 84,000 seated and 94,000 standing.

The Northern Stand, also known as the Olympic Stand, was built to replace the old Grandstand for the 1956 Olympic Games. By Health Department regulations, this was to increase the stadium's capacity to 120,000; although this was revised down after the 1956 VFL Grand Final, which could not comfortably accommodate its crowd of 115,802. Ten years later, the Grey Smith Stand and the open concrete stand next to it were replaced by the Western Stand; the Duke of Edinburgh laid a foundation stone for the Western Stand on 3 March 1967, and it was completed in 1968; in 1986, it was renamed the Ponsford Stand in honour of Victorian batsman Bill Ponsford. This was the stadium's highest capacity configuration, and the all-time record crowd for a sporting event at the venue of 121,696 was set under this configuration in the 1970 VFL Grand Final.

The MCG was the home of Australia's first full colour video scoreboard, which replaced the old scoreboard in 1982, located on Level 4 of the Western Stand, which notably caught fire in 1999 and was replaced in 2000. A second video screen added in 1994 almost directly opposite, on Level 4 of the Olympic stand. In 1985, light towers were installed at the ground, allowing for night football and day-night cricket games.

In 1988 inspections of the old Southern Stand found concrete cancer and provided the opportunity to replace the increasingly run-down 50-year-old facility. The projected cost of $100 million was outside what the Melbourne Cricket Club could afford so the Victorian Football League took the opportunity to part fund the project in return for a 30-year deal to share the ground. The new Great Southern Stand was completed in 1992, in time for the 1992 Cricket World Cup, at a final cost of $150 million.
The 1928 Members' stand, the 1956 Olympic stand and the 1968 Ponsford stand were demolished one by one between late 2003 to 2005 and replaced with a new structure in time for the 2006 Commonwealth Games. Despite now standing as a single unbroken stand, the individual sections retain the names of Ponsford, Olympic and Members Stands. The redevelopment cost exceeded 400 million and pushed the ground's capacity to just above 100,000. Since redevelopment, the highest attendance was the 2018 Grand Final of the AFL with 100,022, followed by 100,021 in the 2017 Grand Final.

From 2011 until 2013, the Victorian Government and the Melbourne Cricket Club funded a $55 million refurbishment of the facilities of Great Southern Stand, including renovations to entrance gates and ticket outlets, food and beverage outlets, "etc.", without significantly modifying the stand. New scoreboards, more than twice the size of the original ones, were installed in the same positions in late 2013.

The first cricket match at the venue was played on 30 September 1854.

The first inter-colonial cricket match to be played at the MCG was between Victoria and New South Wales in March 1856. Victoria had played Tasmania (then known as Van Diemen's Land) as early as 1851 but the Victorians had included two professionals in the 1853 team upsetting the Tasmanians and causing a cooling of relations between the two colonies. To replace the disgruntled Tasmanians the Melbourne Cricket Club issued a challenge to play any team in the colonies for £1000. Sydney publican William Tunks accepted the challenge on behalf of New South Wales although the Victorians were criticised for playing for money. Ethics aside, New South Wales could not afford the £1000 and only managed to travel to Melbourne after half the team's travel cost of £181 was put up by Sydney barrister Richard Driver.

The game eventually got under way on 26 March 1856. The Victorians, stung by criticism over the £1000 stake, argued over just about everything; the toss, who should bat first, whether different pitches should be used for the different innings and even what the umpires should wear.

Victoria won the toss but New South Wales captain George Gilbert successfully argued that the visiting team should decide who bats first. The MCG was a grassless desert and Gilbert, considering players fielded without boots, promptly sent Victoria into bat. Needing only 16 to win in the final innings, New South Wales collapsed to be 5 for 5 before Gilbert's batting saved the game and the visitors won by three wickets.

In subsequent years conditions at the MCG improved but the ever-ambitious Melburnians were always on the lookout for more than the usual diet of club and inter-colonial games. In 1861, Felix William Spiers and Christopher Pond, the proprietors of the Cafe de Paris in Bourke Street and caterers to the MCC, sent their agent, W.B. Mallam, to England to arrange for a cricket team to visit Australia.

Mallam found a team and, captained by Heathfield Stephenson, it arrived in Australia on Christmas Eve 1861 to be met by a crowd of more than 3000 people. The team was taken on a parade through the streets wearing white-trimmed hats with blue ribbons given to them for the occasion. Wherever they went they were mobbed and cheered by crowds to the point where the tour sponsors had to take them out of Melbourne so that they could train undisturbed.

Their first game was at the MCG on New Year's Day 1862, against a Victorian XVIII. The Englishmen also wore coloured sashes around their waists to identify each player and were presented with hats to shade them from the sun. Some estimates put the crowd at the MCG that day at 25,000. It must have been quite a picture with a new 6000 seat grandstand, coloured marquees ringing the ground and a carnival outside. Stephenson said that the ground was better than any in England. The Victorians however, were no match for the English at cricket and the visitors won by an innings and 96 runs.

Over the four days of the ‘test’ more than 45,000 people attended and the profits for Speirs and Pond from this game alone was enough to fund the whole tour. At that time it was the largest number of people to ever watch a cricket match anywhere in the world. Local cricket authorities went out of their way to cater for the needs of the team and the sponsors. They provided grounds and sponsors booths without charge and let the sponsors keep the gate takings. The sponsors however, were not so generous in return. They quibbled with the Melbourne Cricket Club about paying £175 for damages to the MCG despite a prior arrangement to do so.

The last match of the tour was against a Victorian XXII at the MCG after which the English team planted an elm tree outside the ground.

Following the success of this tour, a number of other English teams also visited in subsequent years. George Parr's side came out in 1863–64 and there were two tours by sides led by W.G. Grace. The fourth tour was led by James Lillywhite.

On Boxing Day 1866 an Indigenous Australian cricket team played at the MCG with 11,000 spectators against an MCC team. A few players in that match were in a later team that toured England in 1868. Some also played in three other matches at the ground before 1869.

Up until the fourth tour in 1877, led by Lillywhite, touring teams had played first-class games against the individual colonial sides, but Lillywhite felt that his side had done well enough against New South Wales to warrant a game against an All Australian team.

When Lillywhite headed off to New Zealand he left Melbourne cricketer John Conway to arrange the match for their return. Conway ignored the cricket associations in each colony and selected his own Australian team, negotiating directly with the players. Not only was the team he selected of doubtful representation but it was also probably not the strongest available as some players had declined to take part for various reasons. Demon bowler Fred Spofforth refused to play because wicket-keeper Billy Murdoch was not selected. Paceman Frank Allan was at Warnambool Agricultural Show and Australia's best all-rounder Edwin Evans could not get away from work. In the end only five Australian-born players were selected.

The same could be said for Lillywhite's team which, being selected from only four counties, meant that some of England's best players did not take part. In addition, the team had a rough voyage back across the Tasman Sea and many members had been seasick. The game was due to be played on 15 March, the day after their arrival, but most had not yet fully recovered. On top of that, wicket-keeper Ted Pooley was still in a New Zealand prison after a brawl in a Christchurch pub.

England was nonetheless favourite to win the game and the first ever Test match began with a crowd of only 1000 watching. The Australians elected Dave Gregory from New South Wales as Australia's first ever captain and on winning the toss he decided to bat.

Charles Bannerman scored an unbeaten 165 before retiring hurt. Sydney Cricket Ground curator, Ned Gregory, playing in his one and only Test for Australia, scored Test cricket's first duck. Australia racked up 245 and 104 while England scored 196 and 108 giving Australia victory by 45 runs. The win hinged on Bannerman's century and a superb bowling performance by Tom Kendall who took 7 for 55 in England's second innings.

A fortnight later there was a return game, although it was really more of a benefit for the English team. Australia included Spofforth, Murdoch and T.J.D. Cooper in the side but this time the honours went to England who won by four wickets.

Two years later Lord Harris brought another England team out and during England's first innings in the Test at the MCG, Fred Spofforth took the first hat-trick in Test cricket. He bagged two hauls of 6 for 48 and 7 for 62 in Australia's ten wicket win.

Through most of the 20th century, the Melbourne Cricket Ground was one of the two major Test venues in Australia (along with the Sydney Cricket Ground), and it would host one or two Tests in each summer in which Tests were played; since 1982, the Melbourne Cricket Ground has hosted one Test match each summer. Until 1979, the ground almost always hosted its match or one of its matches over the New Year, with the first day's play falling somewhere between 29 December and 1 January; in most years since 1980 and every year since 1995, its test has begun on Boxing Day, and it is now a standard fixture in the Australian cricket calendar and is known as the Boxing Day Test. The venue also hosts one-day international matches each year, and Twenty20 international matches most years. No other venue in Melbourne has hosted a Test, and Docklands Stadium is the only other venue to have hosted a limited-overs international.

The Victorian first-class team plays Sheffield Shield cricket at the venue during the season. Prior to Test cricket being played on Boxing Day, it was a long-standing tradition for Victoria to host New South Wales in a first-class match on Boxing Day. Victoria also played its limited overs matches at the ground. Since the introduction of the domestic Twenty20 Big Bash League (BBL) in 2011, the Melbourne Stars club has played its home matches at the ground. It is also the home ground of the Melbourne Stars Women team, which plays in the Women's Big Bash League (WBBL).

By the 1980s, the integral MCG pitch – grown from Merri Creek black soil – was considered the worst in Australia, in some matches exhibiting wildly inconsistent bounce which could see balls pass through as grubbers or rear dangerously high – a phenomenon which was put down to damage caused by footballers in winter and increased use for cricket during the summers of the 1970s. The integral pitch has since been removed and drop-in pitches have been used since 1996, generally offering consistent bounce and a fair balance between bat and ball.

The highest first class team score in history was posted at the MCG in the Boxing Day match against New South Wales in 1926–27. Victoria scored 1107 in two days, with Bill Ponsford scoring 352 and Jack Ryder scoring 295.

One of the most sensational incidents in Test cricket occurred at the MCG during the Melbourne test of the 1954–55 England tour of Australia. Big cracks had appeared in the pitch during a very hot Saturday's play and on the rest day Sunday, groundsman Jack House watered the pitch to close them up. This was illegal and the story was leaked by "The Age" newspaper. The teams agreed to finish the match and England won by 128 runs after Frank Tyson took 7 for 27 in the final innings.

An incident in the second Test of the 1960–61 series involved the West Indies player Joe Solomon being given out after his hat fell on the stumps after being bowled at by Richie Benaud. The crowd sided with the West Indies over the Australians.

Not only was the first Test match played at the MCG, the first One Day International match was also played there, on 5 January 1971, between Australia and England. The match was played on what was originally scheduled to have been the fifth day of a Test match, but the Test was abandoned after the first three days were washed out. Australia won the 40-over match by 5 wickets. The next ODI was played on August 1972, some 19 months later.

In March 1977, the Australian Cricket Board assembled 218 of the surviving 224 Australia-England players for a Test match to celebrate 100 years of Test cricket between the two nations. The match was the idea of former Australian bowler and MCC committee member Hans Ebeling who had been responsible for developing the cricket museum at the MCG. The match had everything. England's Derek Randall scored 174, Australia's Rod Marsh also got a century, Lillee took 11 wickets, and David Hookes, in his first test, smacked five fours in a row off England captain Tony Greig's bowling. Rick McCosker who opened for Australia suffered a fractured jaw after being hit by a sharply rising delivery. He left the field but came back in the second innings with his head swathed in bandages. Australia won the match by 45 runs, exactly the same margin as the first Test in 1877.

Another incident occurred on 1 February 1981 at the end of a one-day match between Australia and New Zealand. New Zealand, batting second, needed six runs off the last ball of the day to tie the game. Australian captain, Greg Chappell instructed his brother Trevor, who was bowling the last over, to send the last ball down underarm to prevent the New Zealand batsman, Brian McKechnie, from hitting the ball for six. Although not entirely in the spirit of the game, an underarm delivery was quite legal, so long as the arm was kept straight. The Laws of cricket have since been changed to prevent such a thing happening again. The incident has long been a sore point between Australia and New Zealand.

In February and March 1985 the Benson & Hedges World Championship of Cricket was played at the MCG, a One Day International tournament involving all of the then Test match playing countries to celebrate 150 years of the Australian state of Victoria. Some matches were also played at Sydney Cricket Ground.

The MCG hosted the 1992 Cricket World Cup Final between Pakistan and England with a crowd of more than 87,000. Pakistan won the match after an all-round performance by Wasim Akram who scored 33 runs and picked up 3 crucial wickets to make Pakistan cricket world champions for the first and as yet only time.

During the 1995 Boxing Day Test at the MCG, Australian umpire Darrell Hair called Sri Lankan spin bowler Muttiah Muralitharan for throwing the ball, rather than bowling it, seven times during the match. The other umpires did not call him once and this caused a controversy, although he was later called for throwing by other umpires seven other times in different matches.

The MCG is known for its great atmosphere, much of which is generated in the infamous Bay 13, situated almost directly opposite to the members stand. In the late 1980s, the crowd at Bay 13 would often mimic the warm up stretches performed by Merv Hughes. In a 1999 One-Day International, the behaviour of Bay 13 was so bad that Shane Warne, donning a helmet for protection, had to enter the ground from his dressing rooms and tell the crowd to settle down at the request of opposing England captain Alec Stewart.

The MCG hosted three pool games as part of the 2015 ICC Cricket World Cup as well as a quarter-final, and then the final on 29 March. Australia comfortably defeated New Zealand by seven wickets in front of an Australian record cricket crowd of 93,013.

In 2017-18 Ashes series, Alastair Cook scored the highest score by an English batsman and second double century since Wally Hammond at the ground. Steve Smith scored his 4th consecutive century at the ground (2014-2017) in reply, being the only player since Don Bradman (1928–31) to do so. Smith also lasted 1093 days, or scored 455 runs, between two wickets fallen. The match ended in a draw, dashing hopes of Australia achieving the third Ashes sweep in the 21st Century. The wicket used for the Boxing Day test was the first Australian wicket ever to be rated 'poor' by the ICC.

Despite being called the Melbourne Cricket Ground, the stadium has been and continues to be used much more often for Australian rules football. Spectator numbers for football are larger than for any other sport in Australia, and it makes more money for the MCG than any of the other sports played there.

Although the Melbourne Cricket Club members were instrumental in founding Australian Rules Football, there were understandable concerns in the early days about the damage that might be done to the playing surface if football was allowed to be played at the MCG. Therefore, football games were often played in the parklands next to the cricket ground, and this was the case for the first documented football match to be played at the ground. The match which today is considered to be the first Australian rules football, played between Melbourne Grammar and Scotch College over three Saturdays beginning 7 August 1858 was played in this area.

It wasn't until 1869 that football was played on the MCG proper, a trial game involving a police team. It was not for another ten years, in 1879, after the formation of the Victorian Football Association, that the first official match was played on the MCG and the cricket ground itself became a regular venue for football. Two night matches were played on the ground during the year under the newly invented electric light.

In the early years, the MCG was the home ground of Melbourne Football Club, Australia's oldest club, established in 1858 by the founder of the game itself, Thomas Wills. Melbourne won five premierships during the 1870s using the MCG as its home ground.

The first of nearly 3000 Victorian Football League/Australian Football League games to be played at the MCG was on 15 May 1897, with beating 64 to 19.

Several Australian Football League (AFL) clubs later joined Melbourne in using the MCG as their home ground for matches: (1965), (1985), (1992), (started moving in 1994, became a full-time tenant in 2000) and (2000). Melbourne used the venue as its training base until 1984, before being required to move to preserve the venue's surface when North Melbourne began playing there.

The VFL/AFL Grand Final has been played at the MCG every season since 1902, except in 1924 when no Grand Final was held because of the season's round-robin finals format (it hosted three of the six games in the Finals Series) 1942–1945, when the ground was used by the military during World War II; and in 1991, as the construction of the Great Southern Stand had temporarily reduced the ground's capacity below that of Waverley Park. All three Grand Final Replays have been played at the MCG.

During the 1950s and 1960s crowds at finals and grand finals regularly approached 100,000, which left the VFL little choice but to stage the finals upon this huge ground with its superior spectator-space and facilities. However, in those days each team had its own home-ground, and the MCG was the exclusive home ground of the Melbourne “Demons”, then enjoying their glory days. They were to win six grand finals there, in 1955, 1956, 1957, 1959, 1960, and 1964; losing only twice, in 1954 and 1958. This home-ground advantage in the most crucial games of the VFL year was criticized by some opposing players and supporters. For instance, the Collingwood Football Club’s website records the arguments put forward by Mick Twomey, who played during those years in five Collingwood Grand Final teams: "Mick, like most of his teammates question[sic] Melbourne's dominance at the MCG as it was their exclusive home ground . . . Had a Grand Final been played at Victoria Park or any neutral ground the outcomes would have most certainly been vastly different."

The problem was not only the psychological advantage of playing on home ground, but also the different styles of play that the MCG enabled. Its playing area was a very broad oval (relative to the smaller suburban ovals on which many of the home-and-away matches were then played). Hence visiting teams needed to adjust their playing styles and also perhaps their selection of players to its wider spaces. Visiting teams were given the opportunity to train at the MCG, but would lack match-practice on this bigger ground; since under the old Final Four system they would play only one or at most two finals matches (except in the case of a drawn game) before reaching the grand final. The possible unfairness of this was in time reduced, as modern methods of turf management made it possible for several teams to share the MCG--a process whose history is described in the previous section. Even those teams that did not adopt the MCG as their shared home ground, now had the experience of playing fairly regularly upon it. Also, as smaller grounds were withdrawn from use in the VFL (and later AFL) and replaced by new larger shared grounds like Docklands Stadium, teams became more used to larger playing spaces. 

However the fairness of clubs with narrower or less developed home grounds being forced to play finals on the MCG was still a controversial issue as late as 2019. In that year's finals, 's AFL club, whose Kardinia Park Stadium is 31 metres narrower than the MCG (115 metres versus 146 metres), had earned the right to a home-ground qualifying final, but was required instead to accept the MCG as its secondary home ground, and to play there against a club whose sole home ground was the MCG. 

Before the MCG was fully seated, a Grand Final there could draw attendances above 110,000. The record for the highest attendance in the history of the sport was set in the 1970 VFL Grand Final, with 121,696 in attendance.

Since being fully seated, Grand Final attendances are typically between 95,000 and 100,000, with the record of 100,022 in the 2018 grand final, followed by 100,021 attending the 2017 AFL Grand Final.

In the modern era, most finals games held in Melbourne have been played at the MCG. Under the current contract, ten finals (excluding the Grand Final) must be played at the MCG over a five-year period. Under previous contracts, the MCG was entitled to host at least one match in each week of the finals, which on several occasions required non-Victorian clubs to play "home" finals in Victoria. On 12 April 2018, the AFL, Victorian Government and Melbourne Cricket Club (MCC) announced that the MCG would continue to host the Grand Final until at least 2057.

All Melbourne based teams (and most of the time Geelong) play their 'home' finals at the MCG unless if 4 Victorian teams win the right to host a final in the first week of the finals.

For many years the VFL had an uneasy relationship with the MCG trustees and the Melbourne Cricket Club. Both needed the other, but resented the dependence. The VFL made the first move which brought things to a head by beginning the development of VFL Park at Mulgrave in the 1960s as its own home ground and as a potential venue for future grand finals. Then in 1983, president of the VFL, Allen Aylett started to pressure the MCG Trust to give the VFL a greater share of the money it made from using the ground for football.

In March 1983 the MCG trustees met to consider a submission from Aylett. Aylett said he wanted the Melbourne Cricket Club's share of revenue cut from 15 per cent to 10 per cent. He threatened to take the following day's opening game of the season, Collingwood vs Melbourne, away from the MCG. The money was held aside until an agreement could be reached.

Different deals, half deals and possible deals were done over the years, with the Premier of Victoria, John Cain, Jr., even becoming involved. Cain was said to have promised the VFL it could use the MCG for six months of the year and then hand it back to the MCC, but this never eventuated, as the MCG Trust did not approve it. In the mid-1980s, a deal was done where the VFL was given its own members area in the Southern Stand.

Against this background of political manoeuvring, in 1985 became the third club to make the MCG its home ground. In the same year, North played in the first night football match at the MCG for almost 110 years, against Collingwood on 29 March 1985.

In 1986, only a month after Ross Oakley had taken over as VFL Commissioner, VFL executives met with the MCC and took a big step towards resolving their differences. Changes in the personnel at the MCC also helped. In 1983 John Lill was appointed secretary and Don Cordner its president.

Shortly after the Southern Stand opened in 1992, the Australian Football League moved its headquarters into the complex. The AFL assisted with financing the new stand and came to an agreement that ensures at least 45 AFL games are played at the MCG each year, including the Grand Final in September. Another 45 days of cricket are also played there each year and more than 3.5 million spectators come to watch every year.

As of the end of 2011, Matthew Richardson holds the records for having scored the most goals on the MCG, and Kevin Bartlett holds the record for playing the most matches at the MCG. Two players have scored 14 goals for an AFL or VFL game in one match at the MCG: Gary Ablett, Sr. in 1989 and 1993, and John Longmire in 1990.

Before an AFL match between and on 27 August 1999, the city end scoreboard caught on fire due to an electrical fault, causing the start of play to be delayed by half an hour.

During World War II, the government requisitioned the MCG for military use. From 1942 until 1945 it was occupied by (in order): the United States Army Air Forces, the Royal Australian Air Force, the United States Marine Corps and again the RAAF. Over the course of the war, more than 200,000 personnel were barracked at the MCG. From April to October 1942, the US Army's Fifth Air Force occupied the ground, naming it "Camp Murphy", in honor of officer Colonel William Murphy, a senior USAAF officer killed in Java. In 1943 the MCG was home to the legendary First Regiment of the First Division of the United States Marine Corps. The First Marine Division were the heroes of the Guadalcanal campaign and used the "cricket grounds", as the marines referred to it, to rest and recuperate. On 14 March 1943 the marines hosted a giant "get together" of American and Australian troops on the arena.

In 1977, Melbourne Cricket Club president Sir Albert Chadwick and Medal of Honor recipient, Colonel Mitchell Paige, unveiled a commemorative plaque recognizing the Americans' time at the ground.

In episode 3 of the 2010 TV miniseries, "The Pacific", members of the US Marines are shown to be camped in the war-era MCG.

The MCG's most famous moment in history was as the main stadium for the 1956 Olympic Games, hosting the opening and closing ceremonies, track and field events, and the finals in field hockey and soccer. The MCG was only one of seven possible venues, including the Melbourne Showgrounds, for the Games’ main arena. The MCG was the Federal Government's preferred venue but there was resistance from the MCC. The inability to decide on the central venue nearly caused the Games to be moved from Melbourne. Prime Minister Robert Menzies recognised the potential embarrassment to Australia if this happened and organised a three-day summit meeting to thrash things out. Attending was Victorian Premier John Cain, Sr., the Prime Minister, deputy opposition leader Arthur Calwell, all State political leaders, civic leaders, Olympic officials and trustees and officials of the MCC. Convening the meeting was no small effort considering the calibre of those attending and that many of the sports officials were only part-time amateurs.

As 22 November, the date of the opening ceremony, drew closer, Melbourne was gripped ever more tightly by Olympic fever. At 3 pm the day before the opening ceremony, people began to line up outside the MCG gates. That night the city was paralysed by a quarter of a million people who had come to celebrate.

The MCG's capacity was increased by the new Olympic (or Northern) Stand, and on the day itself 103,000 people filled the stadium to capacity. A young up and coming distance runner was chosen to carry the Olympic torch into the stadium for the opening ceremony.

Although Ron Clarke had a number of junior world records for distances of 1500 m, one mile (1.6 km) and two miles (3 km), he was relatively unknown in 1956. Perhaps the opportunity to carry the torch inspired him because he went on to have a career of exceptional brilliance and was without doubt the most outstanding runner of his day. At one stage he held the world record for every distance from two miles (3 km) to 20 km. His few failures came in Olympic and Commonwealth Games competition. Although favourite for the gold at Tokyo in 1964 he was placed ninth in the 5,000 metres race and the marathon and third in the 10,000 metres. He lost again in the 1966 Commonwealth Games and in 1968 at altitude in Mexico he collapsed at the end of the 10 km race.

On that famous day in Melbourne in 1956 the torch spluttered and sparked, showering Clarke with hot magnesium, burning holes in his shirt. When he dipped the torch into the cauldron it burst into flame singeing him further. In the centre of the ground, John Landy, the fastest miler in the world, took the Olympic oath and sculler Merv Wood carried the Australian flag.

The Melbourne Games also saw the high point of Australian female sprinting with Betty Cuthbert winning three gold medals at the MCG. She won the 100 m and 200 m and anchored the winning 4 x 100 m team. Born in Merrylands in Sydney's west she was a champion schoolgirl athlete and had already broken the world record for the 200 m just before the 1956 Games. She was to be overshadowed by her Western Suburbs club member, the Marlene Matthews. When they got to the Games, Matthews was the overwhelming favourite especially for the 100 m a distance over which Cuthbert had beaten her just once.

Both Matthews and Cuthbert won their heats with Matthews setting an Olympic record of 11.5 seconds in hers. Cuthbert broke that record in the following heat with a time of 11.4 seconds. The world record of 11.3 was held by another Australian, Shirley Strickland who was eliminated in her heat. In the final Matthews felt she got a bad start and was last at the 50 metre mark. Cuthbert sensed Isabella Daniels from the USA close behind her and pulled out a little extra to win Australia's first gold at the Games in a time of 11.5 seconds, Matthews was third. The result was repeated in the 200 m final. Cuthbert won her second gold breaking Marjorie Jackson's Olympic record. Mathews was third again.

By the time the 1956 Olympics came around, Shirley Strickland was a mother of 31 years of age but managed to defend her 80 m title, which she had won in Helsinki four years before, winning gold and setting a new Olympic record.

The sensational incident of the track events was the non-selection of Marlene Matthews in the 4 x 100 m relay. Matthews trained with the relay team up until the selection was made but Cuthbert, Strickland, Fleur Mellor and Norma Croker were picked for the team. There was outrage at the selection which increased when Matthews went on to run third in both the 100 m and 200 m finals. Personally she was devastated and felt that she had been overlooked for her poor baton change. Strickland was disappointed with the way Matthews was treated and maintained it was an opinion held in New South Wales that she had baton problems. One of the selectors, Doris Magee from NSW, said that selecting Matthews increased the risk of disqualification at the change. But Cuthbert maintained that the selectors made the right choice saying that Fleur Mellor was fresh, a specialist relay runner and was better around the curves than Matthews.

The men did not fare so well. The 4 x 400 m relay team, including later IOC Committee member Kevan Gosper, won silver. Charles Porter also won silver in the high jump. Hec Hogan won bronze in the 100 m to become the first Australian man to win a medal in a sprint since the turn of the century and despite injury John Landy won bronze in the 1500 m. Allan Lawrence won bronze in the 10,000 m event.

Apart from athletics, the stadium was also used for the soccer finals, the hockey finals, the Opening and Closing Ceremonies, and an exhibition game of baseball between the Australian National Team and a US armed services team at which an estimated crowd of 114,000 attended. This was the Guinness World Record for the largest attendance for any baseball game, which stood until a 29 March 2008 exhibition game between the Boston Red Sox and Los Angeles Dodgers at the Los Angeles Coliseum (also a former Olympic venue in 1932 and 1984) drawing 115,300.

The MCG was also used for another demonstration sport, Australian Rules. The Olympics being an amateur competition meant that only amateurs could play in the demonstration game. A combined team of amateurs from the VFL and VFA were selected to play a state team from the Victorian Amateur Football Association (VAFA). The game was played 7 December 1956 with the VAFA side, wearing white jumpers, green collars and the Olympic rings on their chests, winning easily 81 to 55. One of the players chosen for the VFA side was Lindsay Gaze (although he never got off the bench) who would go on to make his mark in another sport, basketball, rather than Australian Rules.

The MCG's link with its Olympic past continues to this day. Within its walls is the IOC-endorsed Australian Gallery of Sport and Olympic Museum.

Forty-four years later at the 2000 Summer Olympics in Sydney, the ground hosted several soccer preliminaries, making it one of a few venues ever used for more than one Olympics.

The Opening and Closing Ceremonies of the 2006 Commonwealth Games were held at the MCG, as well as athletics events during the games. The games began on 15 March and ended on 26 March.

The seating capacity of the stadium during the games was 80,000. A total of 47 events were contested, of which 24 by male and 23 by female athletes. Furthermore, three men's and three women's disability events were held within the programme. All athletics events took place within the Melbourne Cricket Ground, while the marathon and racewalking events took place on the streets of Melbourne and finished at the main stadium.

The hosts Australia easily won the medals table with 16 golds and 41 medals in total. Jamaica came second with 10 golds and 22 medals, while Kenya and England were the next best performers. A total of eleven Games records were broken over the course of the seven-day competition. Six of the records were broken by Australian athletes.

The first game of Rugby Union to be played on the ground was on Saturday, 29 June 1878, when the Waratah Club of Sydney played Carlton Football Club in a return of the previous year's contests in Sydney where the clubs had competed in both codes of football. The match, watched by a crowd of between 6,000 and 7,000 resulted in a draw; one goal and one try being awarded to each team.

The next Rugby match was held on Wednesday 29 June 1881, when the Wanderers, a team organised under the auspices of the Melbourne Cricket Club, played a team representing a detached Royal Navy squadron then visiting Melbourne. The squadron team won by one goal and one try to nil.
It was not until 19 August 1899 that the MCG was again the venue for a Union match, this time Victoria v the British Lions (as they were later to be called). During the preceding week the Victorians had held several trial and practice matches there, as well as several training sessions, despite which they were defeated
30–0 on the day before a crowd of some 7,000.

Nine years later, on Monday, 10 August 1908, Victoria was again the host, this time to the Australian team en route to Great Britain and soon to be dubbed the First Wallabies. Despite being held on a working day some 1,500 spectators attended to see the visitors win by 26–6.

On Saturday, 6 July 1912 the MCG was the venue, for the only time ever, of a match between two Victorian Rugby Union clubs, Melbourne and East Melbourne, the former winning 9–5 in what was reported to be ‘... one of the finest exhibitions of the Rugby game ever seen in Victoria.’ It was played before a large crowd as a curtain raiser to a State Rules match against South Australia.

On Saturday 18 June 1921, in another curtain raiser, this time to a Melbourne-Fitzroy League game, a team representing Victoria was soundly beaten 51–0 by the South African Springboks in front of a crowd of 11,214.

It was nine years later, on Saturday 13 September 1930, that the British Lions returned to play Victoria, again before a crowd of 7,000, this time defeating the home side 41–36, a surprisingly narrow winning margin. 
The first post war match at the MCG was on 21 May 1949 when the NZ Maoris outclassed a Southern States side 35–8 before a crowd of close to 10,000. A year later, on 29 July 1950, for the first and only time, Queensland travelled to Victoria to play an interstate match, defeating their hosts 31–12 before a crowd of 7,479. 
In the following year the MCG was the venue for a contest between the New Zealand All Blacks and an Australian XV . This was on 30 June 1951 before some 9,000 spectators and resulted in a convincing 56–11 win for the visitors.

Union did not return the MCG until the late 1990s, for several night time Test matches, both Australia v New Zealand All Blacks as part of the Tri Nations Series. The first, on Saturday 26 July 1997, being notable for an attendance of 90,119, the visitors winning 33–18 and the second, on Saturday 11 July 1998, for a decisive victory to Australia of 24–16. Australia and New Zealand met again at the MCG during the 2007 Tri Nations Series on 30 June, the hosts again winning, this time by 20 points to 15 in front of a crowd of 79,322.

Rugby league was first played at the ground on 15 August 1914, with the New South Wales team losing to England 15–21.

The first ever State of Origin match at the MCG (and second in Melbourne) was Game II of the 1994 series, and the attendance of 87,161 set a new record rugby league crowd in Australia. The MCG was also the venue for Game II of the 1995 State of Origin series and drew 52,994, the most of any game that series. The second game of the 1997 State of Origin series, which, due to the Super League war only featured Australian Rugby League-signed players, was played there too, but only attracted 25,105, the lowest in a series that failed to attract over 35,000 to any game.

The Melbourne Storm played two marquee games at the MCG in 2000. This was the first time that they had played outside of their normal home ground of Olympic Park Stadium which held 18,500 people. Their first game was held on 3 March 2000 against the St. George Illawarra Dragons in a rematch of the infamous 1999 NRL Grand Final. Dragons player Anthony Mundine said the Storm were 'not worthy premiers' and they responded by running in 12 tries to two, winning 70–10 in front of 23,239 fans. This was their biggest crowd they had played against until 33,427 turned up to the 2007 Preliminary Final at Docklands Stadium which saw Melbourne defeat the Parramatta Eels 26–10. The record home and away crowd record has also been overhauled, when a match at Docklands in 2010 against St George attracted 25,480 spectators. Their second game attracted only 15,535 spectators and was up against the Cronulla Sharks on 24 June 2000. Once again, the Storm won 22–16.

It was announced in June 2014 that the ground would host its first State of Origin match since 1997. Game II of the 2015 series was played at the venue, with an all-time record State of Origin crowd of 91,513 attending the match. The attendance is 19th on the all time rugby league attendance list and the 4th highest rugby league attendance in Australia.

On 9 February 2006 Victorian premier Steve Bracks and Football Federation Australia chairman Frank Lowy announced that the MCG would host a world class soccer event each year from 2006 until 2009 inclusive.

The agreement sees an annual fixture at the MCG, beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103, before Australia left to contest in the World Cup finals. Australia beat Greece 1–0. The Socceroos also hosted a match in 2007 against Argentina, losing 1–0, as well as 2010 FIFA World Cup qualification matches in 2009 against Japan, which attracted 81,872 fans as Australia beat Japan 2–1 via 2 Tim Cahill headers after falling behind 1–0 late in the 1st half. In 2010 it was announced that as a warm up to the 2010 FIFA World Cup which the Australians had qualified for, they would play fellow qualified nation New Zealand on 24 May at the MCG.
Other matches played at the MCG include the following:

In 1878 the Melbourne Cricket Club's Lawn Tennis Committee laid an asphalt court at the MCG and Victoria's first game of tennis was played there. A second court of grass was laid in 1879 and the first Victorian Championship played on it in 1880. The first inter-colonial championship was played in 1883 and the first formal inter-state match between NSW and Victoria played in 1884 with Victoria winning.

In 1889 the MCC arranged for tennis to be played at the Warehousemen's Cricket Ground (now known as the Albert Cricket Ground), at Albert Park, rather than at the MCG.

It was at the MCG in 1869 that one of Australia's first bicycle races was held. The event was for velocipedes, crude wooden machines with pedals on the front wheels. In 1898 the Austral Wheel Race was held at the MCG attracting a crowd of 30,000 to see cyclists race for a total of £200 in prize money.









The Tattersall's Parade of the Champions undertaking is a gift to the people of Australia by Tattersall's and is a focal point of the Yarra Park precinct.

The MCG is a magnet for tourists worldwide and the statues reinforce the association between the elite sportsmen and women who have competed here and the stadium that rejoiced in their performances.
In 2010, the Melbourne Cricket Club (MCC) announced an expansion to the list of sporting statues placed around the MCG precinct in partnership with Australia Post.

The Australia Post Avenue of Legends project aimed to place a minimum of five statues in Yarra Park, extending from the gate 2 MCC members entrance up the avenue towards Wellington Parade. The most recent addition of Kevin Bartlett was unveiled in March 2017.


 

 


</doc>
<doc id="19766" url="https://en.wikipedia.org/wiki?curid=19766" title="Marshall Plan">
Marshall Plan

The Marshall Plan (officially the European Recovery Program, ERP) was an American initiative passed in 1948 for foreign aid to Western Europe. The United States transferred over $12 billion (nearly $ billion in US dollars) in economic recovery programs to Western European economies after the end of World War II. Replacing an earlier proposal for a Morgenthau Plan, it operated for four years beginning on April 3, 1948. The goals of the United States were to rebuild war-torn regions, remove trade barriers, modernize industry, improve European prosperity, and prevent the spread of Communism. The Marshall Plan required a reduction of interstate barriers, a dropping of many regulations, and encouraged an increase in productivity, as well as the adoption of modern business procedures.

The Marshall Plan aid was divided amongst the participant states roughly on a per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The largest recipient of Marshall Plan money was the United Kingdom (receiving about 26% of the total), followed by France (18%) and West Germany (11%). Some eighteen European countries received Plan benefits. Although offered participation, the Soviet Union refused Plan benefits, and also blocked benefits to Eastern Bloc countries, such as Hungary and Poland. The United States provided similar aid programs in Asia, but they were not part of the Marshall Plan.

Its role in the rapid recovery has been debated. The Marshall Plan's accounting reflects that aid accounted for about 3% of the combined national income of the recipient countries between 1948 and 1951, which means an increase in GDP growth of less than half a percent.

After World War II, in 1947, industrialist Lewis H. Brown wrote (at the request of General Lucius D. Clay) "A Report on Germany", which served as a detailed recommendation for the reconstruction of post-war Germany, and served as a basis for the Marshall Plan. The initiative was named after United States Secretary of State George Marshall. The plan had bipartisan support in Washington, where the Republicans controlled Congress and the Democrats controlled the White House with Harry S. Truman as President. The Plan was largely the creation of State Department officials, especially William L. Clayton and George F. Kennan, with help from the Brookings Institution, as requested by Senator Arthur H. Vandenberg, chairman of the Senate Foreign Relations Committee. Marshall spoke of an urgent need to help the European recovery in his address at Harvard University in June 1947. The purpose of the Marshall Plan was to aid in the economic recovery of nations after World War II and to reduce the influence of Communist parties within them. To combat the effects of the Marshall Plan, the USSR developed its own economic plan, known as the Molotov Plan, in spite of the fact that large amounts of resources from the Eastern Bloc countries to the USSR were paid as reparations, for countries participating in the Axis Power during the war.

The phrase "equivalent of the Marshall Plan" is often used to describe a proposed large-scale economic rescue program.

In 1951 the Marshall Plan was largely replaced by the Mutual Security Act.

The reconstruction plan, developed at a meeting of the participating European states, was drafted on June 5, 1947. It offered the same aid to the Soviet Union and its allies, but they refused to accept it, as doing so would allow a degree of US control over the communist economies. In fact, the Soviet Union prevented its satellite states (i.e., East Germany, Poland, etc.) from accepting. Secretary Marshall became convinced Stalin had no interest in helping restore economic health in Western Europe.
President Harry Truman signed the Marshall Plan on April 3, 1948, granting $5 billion in aid to 16 European nations. During the four years the plan was in effect, the United States donated $17 billion (equivalent to $ billion in ) in economic and technical assistance to help the recovery of the European countries that joined the Organisation for European Economic Co-operation. The $17 billion was in the context of a US GDP of $258 billion in 1948, and on top of $17 billion in American aid to Europe between the end of the war and the start of the Plan that is counted separately from the Marshall Plan. The Marshall Plan was replaced by the Mutual Security Plan at the end of 1951; that new plan gave away about $7.5 billion annually until 1961 when it was replaced by another program.

The ERP addressed each of the obstacles to postwar recovery. The plan looked to the future and did not focus on the destruction caused by the war. Much more important were efforts to modernize European industrial and business practices using high-efficiency American models, reducing artificial trade barriers, and instilling a sense of hope and self-reliance.

By 1952, as the funding ended, the economy of every participant state had surpassed pre-war levels; for all Marshall Plan recipients, output in 1951 was at least 35% higher than in 1938. Over the next two decades, Western Europe enjoyed unprecedented growth and prosperity, but economists are not sure what proportion was due directly to the ERP, what proportion indirectly, and how much would have happened without it.
A common American interpretation of the program's role in European recovery was expressed by Paul Hoffman, head of the Economic Cooperation Administration, in 1949, when he told Congress Marshall aid had provided the "critical margin" on which other investment needed for European recovery depended. The Marshall Plan was one of the first elements of European integration, as it erased trade barriers and set up institutions to coordinate the economy on a continental level—that is, it stimulated the total political reconstruction of western Europe.

Belgian economic historian Herman Van der Wee concludes the Marshall Plan was a "great success":

By the end of World War II, much of Europe was devastated. Sustained aerial bombardment during the war had badly damaged most major cities, and industrial facilities were especially hard-hit. The region's trade flows had been thoroughly disrupted; millions were in refugee camps living on aid from the United States, which was provided in the guise of the United Nations Relief and Rehabilitation Administration and other agencies. The United Nations had been approved by a three-month conference that bookended Victory in Europe. Food shortages were severe, especially in the harsh winter of 1946–47. From July 1945 through June 1946, the United States shipped 16.5 million tons of food, primarily wheat, to Europe and Japan. It amounted to one-sixth of the American food supply and provided 35 trillion calories, enough to provide 400 calories a day for one year to 300 million people.

Especially damaged was transportation infrastructure, as railways, bridges, and docks had been specifically targeted by airstrikes, while much merchant shipping had been sunk. Although most small towns and villages had not suffered as much damage, the destruction of transportation left them economically isolated. None of these problems could be easily remedied, as most nations engaged in the war had exhausted their treasuries in the process.

The only major powers whose infrastructure had not been significantly harmed in World War II were the United States and Canada. They were much more prosperous than before the war but exports were a small factor in their economy. Much of the Marshall Plan aid would be used by the Europeans to buy manufactured goods and raw materials from the United States and Canada.

Along with the UN, many humanist ideas were circulating over the five-year period that ensued its formation. The World Bank and the International Monetary Fund (IMF) date from this time. One of the ideas proposed in 1947 at the United Nations Conference on Trade and Employment (UNCTE) was the International Trade Organization (ITO). The GATT was first conceived around that time too.

Most of Europe's economies were recovering slowly, as unemployment and food shortages led to strikes and unrest in several nations. Agricultural production was 83% of 1938 levels, industrial production was 88%, and exports 59%. Exceptions were the United Kingdom, the Netherlands and France, where by the end of 1947 production had already been restored to pre-war levels before the Marshall Plan. Italy and Belgium would follow by the end of 1948.

In Germany in 1945–46 housing and food conditions were bad, as the disruption of transport, markets, and finances slowed a return to normality. In the West, bombing had destroyed 5,000,000 houses and apartments, and 12,000,000 refugees from the east had crowded in. Food production was two-thirds of the pre-war level in 1946–48, while normal grain and meat shipments no longer arrived from the East. The drop in food production can be attributed to a drought that killed a major portion of the wheat crop while a severe winter destroyed the majority of the wheat crop the following year. This caused most Europeans to rely on a 1,500 calorie per day diet. Furthermore, the large shipments of food stolen from occupied nations during the war no longer reached Germany. Industrial production fell more than half and reached pre-war levels at the end of 1949.

While Germany struggled to recover from the destruction of the War, the recovery effort began in June 1948, moving on from emergency relief. The currency reform in 1948 was headed by the military government and helped Germany to restore stability by encouraging production. The reform revalued old currency and deposits and introduced new currency. Taxes were also reduced and Germany prepared to remove economic barriers.

During the first three years of occupation of Germany, the UK and US vigorously pursued a military disarmament program in Germany, partly by removal of equipment but mainly through an import embargo on raw materials, part of the Morgenthau Plan approved by President Franklin D. Roosevelt.

Nicholas Balabkins concludes that "as long as German industrial capacity was kept idle the economic recovery of Europe was delayed." By July 1947 Washington realized that economic recovery in Europe could not go forward without the reconstruction of the German industrial base, deciding that an "orderly, prosperous Europe requires the economic contributions of a stable and productive Germany." In addition, the strength of Moscow-controlled communist parties in France and Italy worried Washington.

In the view of the State Department under President Harry S Truman, the United States needed to adopt a definite position on the world scene or fear losing credibility. The emerging doctrine of containment (as opposed to rollback) argued that the United States needed to substantially aid non-communist countries to stop the spread of Soviet influence. There was also some hope that the Eastern Bloc nations would join the plan, and thus be pulled out of the emerging Soviet bloc, but that did not happen.

In January 1947, Truman appointed retired General George Marshall as Secretary of State. In July 1947 Marshall scrapped Joint Chiefs of Staff Directive 1067 implemented as part of the Morgenthau Plan under the personal supervision of Roosevelt's treasury secretary Henry Morgenthau, Jr., which had decreed "take no steps looking toward the economic rehabilitation of Germany [or] designed to maintain or strengthen the German economy." Thereafter, JCS 1067 was supplanted by JCS 1779, stating that "an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The restrictions placed on German heavy industry production were partly ameliorated; permitted steel production levels were raised from 25% of pre-war capacity to a new limit placed at 50% of pre-war capacity.

With a threatening Greece, and Britain financially unable to continue its aid, the President announced his Truman Doctrine on March 12, 1947, "to support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures", with an aid request for consideration and decision, concerning Greece and Turkey. Also in March 1947, former US President Herbert Hoover, in one of his reports from Germany, argued for a change in US occupation policy, among other things stating:

There is the illusion that the New Germany left after the annexations can be reduced to a 'pastoral state' (Morgenthau's vision). It cannot be done unless we exterminate or move 25,000,000 people out of it.

Hoover further noted that, "The whole economy of Europe is interlinked with German economy through the exchange of raw materials and manufactured goods. The productivity of Europe cannot be restored without the restoration of Germany as a contributor to that productivity." Hoover's report led to a realization in Washington that a new policy was needed; "almost any action would be an improvement on current policy." In Washington, the Joint Chiefs declared that the "complete revival of German industry, particularly coal mining" was now of "primary importance" to American security.

The United States was already spending a great deal to help Europe recover. Over $14 billion was spent or loaned during the postwar period through the end of 1947 and is not counted as part of the Marshall Plan. Much of this aid was designed to restore infrastructure and help refugees. Britain, for example, received an emergency loan of $3.75 billion.

The United Nations also launched a series of humanitarian and relief efforts almost wholly funded by the United States. These efforts had important effects, but they lacked any central organization and planning, and failed to meet many of Europe's more fundamental needs. Already in 1943, the United Nations Relief and Rehabilitation Administration (UNRRA) was founded to provide relief to areas liberated from Germany. UNRRA provided billions of dollars of rehabilitation aid and helped about 8 million refugees. It ceased operation of displaced persons camps in Europe in 1947; many of its functions were transferred to several UN agencies.

After Marshall's appointment in January 1947, administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets in their occupied zone. Molotov refrained from supplying accounts of Soviet assets. The Soviets took a punitive approach, pressing for a delay rather than an acceleration in economic rehabilitation, demanding unconditional fulfillment of all prior reparation claims, and pressing for progress toward nationwide socioeconomic transformation.

After six weeks of negotiations, Molotov rejected all of the American and British proposals. Molotov also rejected the counter-offer to scrap the British-American "Bizonia" and to include the Soviet zone within the newly constructed Germany. Marshall was particularly discouraged after personally meeting with Stalin to explain that the United States could not possibly abandon its position on Germany, while Stalin expressed little interest in a solution to German economic problems.

After the adjournment of the Moscow conference following six weeks of failed discussions with the Soviets regarding a potential German reconstruction, the United States concluded that a solution could not wait any longer. To clarify the American position, a major address by Secretary of State George Marshall was planned. Marshall gave the address at Harvard University on June 5, 1947. He offered American aid to promote European recovery and reconstruction. The speech described the dysfunction of the European economy and presented a rationale for US aid.

The modern system of the division of labor upon which the exchange of products is based is in danger of breaking down. ... Aside from the demoralizing effect on the world at large and the possibilities of disturbances arising as a result of the desperation of the people concerned, the consequences to the economy of the United States should be apparent to all. It is logical that the United States should do whatever it is able to do to assist in the return of normal economic health to the world, without which there can be no political stability and no assured peace. Our policy is not directed against any country, but against hunger, poverty, desperation and chaos. Any government that is willing to assist in recovery will find full co-operation on the part of the United States. Its purpose should be the revival of a working economy in the world so as to permit the emergence of political and social conditions in which free institutions can exist.

Marshall was convinced that economic stability would provide political stability in Europe. He offered aid, but the European countries had to organize the program themselves.

The speech, written by Charles Bohlen, contained virtually no details and no numbers. More a proposal than a plan, it was a challenge to European leaders to cooperate and coordinate. It asked Europeans to create their own plan for rebuilding Europe, indicating the United States would then fund this plan. The administration felt that the plan would likely be unpopular among many Americans, and the speech was mainly directed at a European audience. In an attempt to keep the speech out of American papers, journalists were not contacted, and on the same day, Truman called a press conference to take away headlines. In contrast, Dean Acheson, an Under Secretary of State, was dispatched to contact the European media, especially the British media, and the speech was read in its entirety on the BBC.

British Foreign Secretary Ernest Bevin heard Marshall's radio broadcast speech and immediately contacted French Foreign Minister Georges Bidault to begin preparing a quick European response to (and acceptance of) the offer, which led to the creation of the Committee of European Economic Co-operation. The two agreed that it would be necessary to invite the Soviets as the other major allied power. Marshall's speech had explicitly included an invitation to the Soviets, feeling that excluding them would have been a sign of distrust. State Department officials, however, knew that Stalin would almost certainly not participate and that any plan that would send large amounts of aid to the Soviets was unlikely to get Congressional approval.

Speaking at the Paris Peace Conference on October 10, 1946 Molotov had already stated Soviet fears: "If American capital was given a free hand in the small states ruined and enfeebled by the war [it] would buy up the local industries, appropriate the more attractive Rumanian, Yugoslav ... enterprises and would become the master in these small states." While the Soviet ambassador in Washington suspected that the Marshall Plan could lead to the creation of an anti-Soviet bloc, Stalin was open to the offer. He directed that—in negotiations to be held in Paris regarding the aid—countries in the Eastern Bloc should not reject economic conditions being placed upon them. Stalin only changed his outlook when he learned that (a) credit would only be extended under conditions of economic cooperation and, (b) aid would also be extended to Germany in total, an eventuality which Stalin thought would hamper the Soviets' ability to exercise influence in western Germany.

Initially, Stalin maneuvered to kill the Plan, or at least hamper it by means of destructive participation in the Paris talks regarding conditions. He quickly realized, however, that this would be impossible after Molotov reported—following his arrival in Paris in July 1947—that conditions for the credit were non-negotiable. Looming as just as large a concern was the Czechoslovak eagerness to accept the aid, as well as indications of a similar Polish attitude.

Stalin suspected a possibility that these Eastern Bloc countries might defy Soviet directives not to accept the aid, potentially causing a loss of control of the Eastern Bloc. In addition, the most important condition was that every country choosing to take advantage of the plan would need to have its economic situation independently assessed—a level of scrutiny to which the Soviets could not agree. Bevin and Bidault also insisted that any aid be accompanied by the creation of a unified European economy, something incompatible with the strict Soviet command economy.

Soviet Foreign Minister Vyacheslav Molotov left Paris, rejecting the plan. Thereafter, statements were made suggesting a future confrontation with the West, calling the United States both a "fascizing" power and the "center of worldwide reaction and anti-Soviet activity", with all U.S.-aligned countries branded as enemies. The Soviets also then blamed the United States for communist losses in elections in Belgium, France and Italy months earlier, in the spring of 1947. It claimed that "marshallization" must be resisted and prevented by any means, and that French and Italian communist parties were to take maximum efforts to sabotage the implementation of the Plan. In addition, Western embassies in Moscow were isolated, with their personnel being denied contact with Soviet officials.

On July 12, a larger meeting was convened in Paris. Every country of Europe was invited, with the exceptions of Spain (a World War II neutral that had sympathized with Axis powers) and the small states of Andorra, San Marino, Monaco, and Liechtenstein. The Soviet Union was invited with the understanding that it would likely refuse. The states of the future Eastern Bloc were also approached, and Czechoslovakia and Poland agreed to attend. In one of the clearest signs and reflections of tight Soviet control and domination over the region, Jan Masaryk, the foreign minister of Czechoslovakia, was summoned to Moscow and berated by Stalin for considering Czechoslovakia's possible involvement with and joining of the Marshall Plan. The prime minister of Poland, Józef Cyrankiewicz, was rewarded by Stalin for his country's rejection of the Plan, which came in the form of the Soviet Union's offer of a lucrative trade agreement lasting for a period of five years, a grant amounting to the approximate equivalent of $450 million (in 1948; the sum would have been $4.4 billion in 2014) in the form of long-term credit and loans and the provision of 200,000 tonnes of grain, heavy and manufacturing machinery and factories and heavy industries to Poland.

The Marshall Plan participants were not surprised when the Czechoslovakian and Polish delegations were prevented from attending the Paris meeting. The other Eastern Bloc states immediately rejected the offer. Finland also declined, to avoid antagonizing the Soviets (see also Finlandization). The Soviet Union's "alternative" to the Marshall plan, which was purported to involve Soviet subsidies and trade with western Europe, became known as the Molotov Plan, and later, the Comecon. In a 1947 speech to the United Nations, Soviet deputy foreign minister Andrei Vyshinsky said that the Marshall Plan violated the principles of the United Nations. He accused the United States of attempting to impose its will on other independent states, while at the same time using economic resources distributed as relief to needy nations as an instrument of political pressure.

Although all other Communist European Countries had deferred to Stalin and rejected the aid, the Yugoslavs, led by Josip Broz (Tito), at first went along and rejected the Marshall Plan. However, in 1948 Tito broke decisively with Stalin on other issues, making Yugoslavia an independent communist state. Yugoslavia requested American aid. American leaders were internally divided, but finally agreed and began sending money on a small scale in 1949, and on a much larger scale in 1950-53. The American aid was not part of the Marshall Plan.

In late September, the Soviet Union called a meeting of nine European Communist parties in southwest Poland. A Communist Party of the Soviet Union (CPSU) report was read at the outset to set the heavily anti-Western tone, stating now that "international politics is dominated by the ruling clique of the American imperialists" which have embarked upon the "enslavement of the weakened capitalist countries of Europe". Communist parties were to struggle against the US presence in Europe by any means necessary, including sabotage. The report further claimed that "reactionary imperialist elements throughout the world, particularly in the United States, in Britain and France, had put particular hope on Germany and Japan, primarily on Hitlerite Germany—first as a force most capable of striking a blow at the Soviet Union".

Referring to the Eastern Bloc, the report stated that "the Red Army's liberating role was complemented by an upsurge of the freedom-loving peoples' liberation struggle against the fascist predators and their hirelings." It argued that "the bosses of Wall Street" were "tak[ing] the place of Germany, Japan and Italy". The Marshall Plan was described as "the American plan for the enslavement of Europe". It described the world now breaking down "into basically two camps—the imperialist and antidemocratic camp on the one hand, and the antiimperialist and democratic camp on the other".

Although the Eastern Bloc countries except Czechoslovakia had immediately rejected Marshall Plan aid, Eastern Bloc communist parties were blamed for permitting even minor influence by non-communists in their respective countries during the run up to the Marshall Plan. The meeting's chair, Andrei Zhdanov, who was in permanent radio contact with the Kremlin from whom he received instructions, also castigated communist parties in France and Italy for collaboration with those countries' domestic agendas. Zhdanov warned that if they continued to fail to maintain international contact with Moscow to consult on all matters, "extremely harmful consequences for the development of the brother parties' work" would result.

Italian and French communist leaders were prevented by party rules from pointing out that it was actually Stalin who had directed them not to take opposition stances in 1944. The French communist party, as others, was then to redirect its mission to "destroy capitalist economy" and that the Soviet Communist Information Bureau (Cominform) would take control of the French Communist Party's activities to oppose the Marshall Plan. When they asked Zhdanov if they should prepare for armed revolt when they returned home, he did not answer. In a follow-up conversation with Stalin, he explained that an armed struggle would be impossible and that the struggle against the Marshall Plan was to be waged under the slogan of national independence.

Congress, under the control of conservative Republicans, agreed to the program for multiple reasons. The 20-member conservative isolationist Senate wing of the party, based in the rural Midwest and led by Senator Kenneth S. Wherry (R-Nebraska), was outmaneuvered by the emerging internationalist wing, led by Senator Arthur H. Vandenberg (R-Michigan). The opposition argued that it would be "a wasteful 'operation rat-hole'"; that it made no sense to oppose communism by supporting the socialist governments in Western Europe; and that American goods would reach Russia and increase its war potential. Vandenberg, assisted by Senator Henry Cabot Lodge, Jr. (R-Massachusetts) admitted there was no certainty that the plan would succeed, but said it would halt economic chaos, sustain Western civilization, and stop further Soviet expansion. Senator Robert A. Taft (R-Ohio) hedged on the issue. He said it was without economic justification; however, it was "absolutely necessary" in "the world battle against communism." In the end, only 17 senators voted against it on March 13, 1948 A bill granting an initial $5 billion passed Congress with strong bipartisan support. Congress would eventually allocate $12.4 billion in aid over the four years of the plan.

Congress reflected public opinion, which resonated with the ideological argument that communism flourishes in poverty. Truman's own prestige and power had been greatly enhanced by his stunning victory in the 1948 election. Across America, multiple interest groups, including business, labor, farming, philanthropy, ethnic groups, and religious groups, saw the Marshall Plan as an inexpensive solution to a massive problem, noting it would also help American exports and stimulate the American economy as well. Major newspapers were highly supportive, including such conservative outlets as "Time" magazine. Vandenberg made sure of bipartisan support on the Senate Foreign Relations Committee. The Solid Democratic South was highly supportive, the upper Midwest was dubious, but heavily outnumbered. The plan was opposed by conservatives in the rural Midwest, who opposed any major government spending program and were highly suspicious of Europeans. The plan also had some opponents on the left, led by Henry A. Wallace, the former Vice President. He said the Plan was hostile to the Soviet Union, a subsidy for American exporters, and sure to polarize the world between East and West. However, opposition against the Marshall Plan was greatly reduced by the shock of the Communist coup in Czechoslovakia in February 1948. The appointment of the prominent businessman Paul G. Hoffman as director reassured conservative businessmen that the gigantic sums of money would be handled efficiently.

Turning the plan into reality required negotiations among the participating nations. Sixteen nations met in Paris to determine what form the American aid would take, and how it would be divided. The negotiations were long and complex, with each nation having its own interests. France's major concern was that Germany not be rebuilt to its previous threatening power. The Benelux countries (Belgium, Netherlands, and Luxembourg), despite also suffering under the Nazis, had long been closely linked to the German economy and felt their prosperity depended on its revival. The Scandinavian nations, especially Sweden, insisted that their long-standing trading relationships with the Eastern Bloc nations not be disrupted and that their neutrality not be infringed.

The United Kingdom insisted on special status as a longstanding belligerent during the war, concerned that if it were treated equally with the devastated continental powers it would receive virtually no aid. The Americans were pushing the importance of free trade and European unity to form a bulwark against communism. The Truman administration, represented by William L. Clayton, promised the Europeans that they would be free to structure the plan themselves, but the administration also reminded the Europeans that implementation depended on the plan's passage through Congress. A majority of Congress members were committed to free trade and European integration, and were hesitant to spend too much of the money on Germany. However, before the Marshall Plan was in effect, France, Austria, and Italy needed immediate aid. On December 17, 1947, the United States agreed to give $40 million to France, Austria, China, and Italy.

Agreement was eventually reached and the Europeans sent a reconstruction plan to Washington, which was formulated and agreed upon by the Committee of European Economic Co-operation in 1947. In the document, the Europeans asked for $22 billion in aid. Truman cut this to $17 billion in the bill he put to Congress.
On March 17, 1948, Truman addressed European security and condemned the Soviet Union before a hastily convened Joint Session of Congress. Attempting to contain spreading Soviet influence in the Eastern Bloc, Truman asked Congress to restore a peacetime military draft and to swiftly pass the Economic Cooperation Act, the name given to the Marshall Plan. Of the Soviet Union Truman said, "The situation in the world today is not primarily the result of the natural difficulties which follow a great war. It is chiefly due to the fact that one nation has not only refused to cooperate in the establishment of a just and honorable peace but—even worse—has actively sought to prevent it."

Members of the Republican-controlled 80th Congress (1947–1949) were skeptical. "In effect, he told the Nation that we have lost the peace, that our whole war effort was in vain.", noted Representative Frederick Smith of Ohio. Others thought he had not been forceful enough to contain the USSR. "What [Truman] said fell short of being tough", noted Representative Eugene Cox, a Democrat from Georgia, "there is no prospect of ever winning Russian cooperation." Despite its reservations, the 80th Congress implemented Truman's requests, further escalating the Cold War with the USSR.

Truman signed the Economic Cooperation Act into law on April 3, 1948; the Act established the Economic Cooperation Administration (ECA) to administer the program. ECA was headed by economic cooperation administrator Paul G. Hoffman. In the same year, the participating countries (Austria, Belgium, Denmark, France, West Germany, the United Kingdom, Greece, Iceland, Ireland, Italy, Luxembourg, the Netherlands, Norway, Sweden, Switzerland, Turkey, and the United States) signed an accord establishing a master financial-aid-coordinating agency, the Organisation for European Economic Co-operation (later called the Organisation for Economic Co-operation and Development or OECD), which was headed by Frenchman Robert Marjolin.

The first substantial aid went to Greece and Turkey in January 1947, which were seen as the front line of the battle against communist expansion, and were already receiving aid under the Truman Doctrine. Initially, Britain had supported the anti-communist factions in those countries, but due to its dire economic condition it decided to pull out and in February 1947 requested the US to continue its efforts. The ECA formally began operation in July 1948.

The ECA's official mission statement was to give a boost to the European economy: to promote European production, to bolster European currency, and to facilitate international trade, especially with the United States, whose economic interest required Europe to become wealthy enough to import US goods. Another unofficial goal of ECA (and of the Marshall Plan) was the containment of growing Soviet influence in Europe, evident especially in the growing strength of communist parties in Czechoslovakia, France, and Italy.

The Marshall Plan money was transferred to the governments of the European nations. The funds were jointly administered by the local governments and the ECA. Each European capital had an ECA envoy, generally a prominent American businessman, who would advise on the process. The cooperative allocation of funds was encouraged, and panels of government, business, and labor leaders were convened to examine the economy and see where aid was needed.

The Marshall Plan aid was mostly used for the purchase of goods from the United States. The European nations had all but exhausted their foreign-exchange reserves during the war, and the Marshall Plan aid represented almost their sole means of importing goods from abroad. At the start of the plan, these imports were mainly much-needed staples such as food and fuel, but later the purchases turned towards reconstruction needs as was originally intended. In the latter years, under pressure from the United States Congress and with the outbreak of the Korean War, an increasing amount of the aid was spent on rebuilding the militaries of Western Europe. Of the some $13 billion allotted by mid-1951, $3.4 billion had been spent on imports of raw materials and semi-manufactured products; $3.2 billion on food, feed, and fertilizer; $1.9 billion on machines, vehicles, and equipment; and $1.6 billion on fuel.

Also established were counterpart funds, which used Marshall Plan aid to establish funds in the local currency. According to ECA rules, recipients had to invest 60% of these funds in industry. This was prominent in Germany, where these government-administered funds played a crucial role in lending money to private enterprises which would spend the money rebuilding. These funds played a central role in the reindustrialization of Germany. In 1949–50, for instance, 40% of the investment in the German coal industry was by these funds.

The companies were obligated to repay the loans to the government, and the money would then be lent out to another group of businesses. This process has continued to this day in the guise of the state-owned KfW bank, (Kreditanstalt für Wiederaufbau, meaning Reconstruction Credit Institute). The Special Fund, then supervised by the Federal Economics Ministry, was worth over DM 10 billion in 1971. In 1997 it was worth DM 23 billion. Through the revolving loan system, the Fund had by the end of 1995 made low-interest loans to German citizens amounting to around DM 140 billion. The other 40% of the counterpart funds were used to pay down the debt, stabilize the currency, or invest in non-industrial projects. France made the most extensive use of counterpart funds, using them to reduce the budget deficit. In France, and most other countries, the counterpart fund money was absorbed into general government revenues, and not recycled as in Germany.

The Netherlands received US aid for economic recovery in the Netherlands Indies. However, in January 1949, the American government suspended this aid in response to the Dutch efforts to restore colonial rule in Indonesia during the Indonesian National Revolution, and it implicitly threatened to suspend Marshall aid to the Netherlands if the Dutch government continued to oppose the independence of Indonesia.

At the time the United States was a significant oil producing nation — one of the goals of the Marshall Plan was for Europe to use oil in place of coal, but the Europeans wanted to buy crude oil and use the Marshall Plan funds to build refineries instead. However, when independent American oil companies complained, the ECA denied funds for European refinery construction.

A high priority was increasing industrial productivity in Europe, which proved one of the more successful aspects of the Marshall Plan. The US Bureau of Labor Statistics (BLS) contributed heavily to the success of the Technical Assistance Program. The United States Congress passed a law on June 7, 1940 that allowed the BLS to "make continuing studies of labor productivity" and appropriated funds for the creation of a Productivity and Technological Development Division. The BLS could then use its expertise in the field of productive efficiency to implement a productivity drive in each Western European country receiving Marshall Plan aid. Counterpart funds were used to finance large-scale tours of American industry. France, for example, sent 500 missions with 4700 businessmen and experts to tour American factories, farms, stores, and offices. They were especially impressed with the prosperity of American workers, and how they could purchase an inexpensive new automobile for nine months work, compared to 30 months in France.

By implementing technological literature surveys and organized plant visits, American economists, statisticians, and engineers were able to educate European manufacturers in statistical measurement. The goal of the statistical and technical assistance from the Americans was to increase productive efficiency of European manufacturers in all industries.

To conduct this analysis, the BLS performed two types of productivity calculations. First, they used existing data to calculate how much a worker produces per hour of work—the average output rate. Second, they compared the existing output rates in a particular country to output rates in other nations. By performing these calculations across all industries, the BLS was able to identify the strengths and weaknesses of each country's manufacturing and industrial production. From that, the BLS could recommend technologies (especially statistical) that each individual nation could implement. Often, these technologies came from the United States; by the time the Technical Assistance Program began, the United States used statistical technologies "more than a generation ahead of what [the Europeans] were using".

The BLS used these statistical technologies to create Factory Performance Reports for Western European nations. The American government sent hundreds of technical advisers to Europe to observe workers in the field. This on-site analysis made the Factory Performance Reports especially helpful to the manufacturers. In addition, the Technical Assistance Program funded 24,000 European engineers, leaders, and industrialists to visit America and tour America's factories, mines, and manufacturing plants. This way, the European visitors would be able to return to their home countries and implement the technologies used in the United States. The analyses in the Factory Performance Reports and the "hands-on" experience had by the European productivity teams effectively identified productivity deficiencies in European industries; from there, it became clearer how to make European production more effective.

Before the Technical Assistance Program even went into effect, United States Secretary of Labor Maurice Tobin expressed his confidence in American productivity and technology to both American and European economic leaders. He urged that the United States play a large role in improving European productive efficiency by providing four recommendations for the program's administrators:

The effects of the Technical Assistance Program were not limited to improvements in productive efficiency. While the thousands of European leaders took their work/study trips to the United States, they were able to observe a number of aspects of American society as well. The Europeans could watch local, state, and federal governments work together with citizens in a pluralist society. They observed a democratic society with open universities and civic societies in addition to more advanced factories and manufacturing plants. The Technical Assistance Program allowed Europeans to bring home many types of American ideas.

Another important aspect of the Technical Assistance Program was its low cost. While $19.4 billion was allocated for capital costs in the Marshall Plan, the Technical Assistance Program only required $300 million. Only one-third of that $300 million cost was paid by the United States.

Even while the Marshall Plan was being implemented, the dismantling of ostensibly German industry continued; and in 1949 Konrad Adenauer, an opponent to Hitler's regime and the head of the Christian Democratic Union, wrote to the Allies requesting the end of industrial dismantling, citing the inherent contradiction between encouraging industrial growth and removing factories, and also the unpopularity of the policy. Adenauer had been released from prison, only to discover that the Soviets had effectively divided Europe with Germany divided even further. Support for dismantling was by this time coming predominantly from the French, and the Petersberg Agreement of November 1949 greatly reduced the levels of deindustrialization, though dismantling of minor factories continued until 1951. The first "level of industry" plan, signed by the Allies on March 29, 1946, had stated that German heavy industry was to be lowered to 50% of its 1938 levels by the destruction of 1,500 listed manufacturing plants. Marshall Plan played a huge role in post-war recovery for Europe in general. 1948, conditions were improving, European workers exceeded by 20 percent from the earning from the west side. Thanks to the Plan, during 1952, it went up 35 percent of the industrial and agricultural.

In January 1946 the Allied Control Council set the foundation of the future German economy by putting a cap on German steel production. The maximum allowed was set at about 5,800,000 tons of steel a year, equivalent to 25% of the pre-war production level. The UK, in whose occupation zone most of the steel production was located, had argued for a more limited capacity reduction by placing the production ceiling at 12 million tons of steel per year, but had to submit to the will of the US, France and the Soviet Union (which had argued for a 3 million ton limit). Steel plants thus made redundant were to be dismantled. Germany was to be reduced to the standard of life it had known at the height of the Great Depression (1932). Consequently, car production was set to 10% of pre-war levels, and the manufacture of other commodities was reduced as well.

The first "German level of industry" plan was subsequently followed by a number of new ones, the last signed in 1949. By 1950, after the virtual completion of the by then much watered-down "level of industry" plans, equipment had been removed from 706 manufacturing plants in western Germany and steel production capacity had been reduced by 6,700,000 tons. Vladimir Petrov concludes that the Allies "delayed by several years the economic reconstruction of the war-torn continent, a reconstruction which subsequently cost the United States billions of dollars." In 1951 West Germany agreed to join the European Coal and Steel Community (ECSC) the following year. This meant that some of the economic restrictions on production capacity and on actual production that were imposed by the International Authority for the Ruhr were lifted, and that its role was taken over by the ECSC.

The Marshall Plan aid was divided among the participant states on a roughly per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The exception was Iceland, which had been neutral during the war, but received far more on a per capita basis than the second highest recipient. The table below shows Marshall Plan aid by country and year (in millions of dollars) from "The Marshall Plan Fifty Years Later." There is no clear consensus on exact amounts, as different scholars differ on exactly what elements of American aid during this period were part of the Marshall Plan.

The Marshall Plan, just as GARIOA, consisted of aid both in the form of grants and in the form of loans. Out of the total, US$1.2 billion were loan-aid.

Ireland which received US$146.2 million through the Marshall Plan, received US$128.2 million as loans, and the remaining US$18 million as grants. By 1969 the Irish Marshall Plan debt, which was still being repaid, amounted to 31 million pounds, out of a total Irish foreign debt of 50 million pounds.

The UK received US$385 million of its Marshall Plan aid in the form of loans. Unconnected to the Marshall Plan the UK also received direct loans from the US amounting to US$4.6 billion. The proportion of Marshall Plan loans versus Marshall Plan grants was roughly 15% to 85% for both the UK and France.

Germany, which up until the 1953 Debt agreement had to work on the assumption that all the Marshall Plan aid was to be repaid, spent its funds very carefully. Payment for Marshall Plan goods, "counterpart funds", were administered by the Reconstruction Credit Institute, which used the funds for loans inside Germany. In the 1953 Debt agreement, the amount of Marshall plan aid that Germany was to repay was reduced to less than US$1 billion. This made the proportion of loans versus grants to Germany similar to that of France and the UK. The final German loan repayment was made in 1971. Since Germany chose to repay the aid debt out of the German Federal budget, leaving the German ERP fund intact, the fund was able to continue its reconstruction work. By 1996 it had accumulated a value of 23 billion Deutsche Mark.

The Central Intelligence Agency received 5% of the Marshall Plan funds (about $685 million spread over six years), which it used to finance secret operations abroad. Through the Office of Policy Coordination money was directed towards support for labor unions, newspapers, student groups, artists and intellectuals, who were countering the anti-American counterparts subsidized by the Communists. The largest sum went to the Congress for Cultural Freedom. There were no agents working among the Soviets or their satellite states. The founding conference of the Congress for Cultural Freedom was held in Berlin in June 1950. Among the leading intellectuals from the US and Western Europe were writers, philosophers, critics and historians: Franz Borkenau, Karl Jaspers, John Dewey, Ignazio Silone, James Burnham, Hugh Trevor-Roper, Arthur Schlesinger, Jr., Bertrand Russell, Ernst Reuter, Raymond Aron, Alfred Ayer, Benedetto Croce, Arthur Koestler, Richard Löwenthal, Melvin J. Lasky, Tennessee Williams, Irving Brown, and Sidney Hook. There were conservatives among the participants, but non-Communist (or former Communist) left-wingers were more numerous.

The Marshall Plan was originally scheduled to end in 1953. Any effort to extend it was halted by the growing cost of the Korean War and rearmament. American Republicans hostile to the plan had also gained seats in the 1950 Congressional elections, and conservative opposition to the plan was revived. Thus the plan ended in 1951, though various other forms of American aid to Europe continued afterwards.

The years 1948 to 1952 saw the fastest period of growth in European history. Industrial production increased by 35%. Agricultural production substantially surpassed pre-war levels. The poverty and starvation of the immediate postwar years disappeared, and Western Europe embarked upon an unprecedented two decades of growth that saw standards of living increase dramatically. Additionally, the long-term effect of economic integration raised European income levels substantially, by nearly 20 percent by the mid-1970s. There is some debate among historians over how much this should be credited to the Marshall Plan. Most reject the idea that it alone miraculously revived Europe, as evidence shows that a general recovery was already underway. Most believe that the Marshall Plan sped this recovery, but did not initiate it. Many argue that the structural adjustments that it forced were of great importance. Economic historians J. Bradford DeLong and Barry Eichengreen call it "history's most successful structural adjustment program." One effect of the plan was that it subtly "Americanized" European countries, especially Austria, through new exposure to American popular culture, including the growth in influence of Hollywood movies and rock n' roll.

The political effects of the Marshall Plan may have been just as important as the economic ones. Marshall Plan aid allowed the nations of Western Europe to relax austerity measures and rationing, reducing discontent and bringing political stability. The communist influence on Western Europe was greatly reduced, and throughout the region, communist parties faded in popularity in the years after the Marshall Plan. The trade relations fostered by the Marshall Plan helped forge the North Atlantic alliance that would persist throughout the Cold War in the form of NATO. At the same time, the nonparticipation of the states of the Eastern Bloc was one of the first clear signs that the continent was now divided.

The Marshall Plan also played an important role in European integration. Both the Americans and many of the European leaders felt that European integration was necessary to secure the peace and prosperity of Europe, and thus used Marshall Plan guidelines to foster integration. In some ways, this effort failed, as the OEEC never grew to be more than an agent of economic cooperation. Rather, it was the separate European Coal and Steel Community, which notably excluded Britain, that would eventually grow into the European Union. However, the OEEC served as both a testing and training ground for the structures that would later be used by the European Economic Community. The Marshall Plan, linked into the Bretton Woods system, also mandated free trade throughout the region.

While some historians today feel some of the praise for the Marshall Plan is exaggerated, it is still viewed favorably and many thus feel that a similar project would help other areas of the world. After the fall of communism, several proposed a "Marshall Plan for Eastern Europe" that would help revive that region. Others have proposed a Marshall Plan for Africa to help that continent, and US Vice President Al Gore suggested a Global Marshall Plan. "Marshall Plan" has become a metaphor for any very large-scale government program that is designed to solve a specific social problem. It is usually used when calling for federal spending to correct a perceived failure of the private sector.

Nicholas Shaxson comments: “It is widely believed that the plan worked by offsetting European countries’ yawning deficits. But its real importance ... was simply to compensate for the US failure to institute controls on inflows of hot money from Europe. ... American post-war aid was less than the money flowing in the other direction.“ European hot money inflated the US dollar exchange rate, to the disadvantage of US exporters.

The Marshall Plan money was in the form of grants from the U.S. Treasury that did not have to be repaid. The Organisation for European Economic Co-operation took the leading role in allocating funds, and the OEEC arranged for the transfer of the goods. The American supplier was paid in dollars, which were credited against the appropriate European Recovery Program funds. The European recipient, however, was not given the goods as a gift but had to pay for them (usually on credit) in local currency. These payments were kept by the European government involved in a special counterpart fund. This counterpart money, in turn, could be used by the government for further investment projects. Five percent of the counterpart money was paid to the US to cover the administrative costs of the ERP. In addition to ERP grants, the Export-Import Bank (an agency of the US government) at the same time made long-term loans at low interest rates to finance major purchases in the US, all of which were repaid.

In the case of Germany, there also were 16 billion marks of debts from the 1920s which had defaulted in the 1930s, but which Germany decided to repay to restore its reputation. This money was owed to government and private banks in the US, France, and Britain. Another 16 billion marks represented postwar loans by the US. Under the London Debts Agreement of 1953, the repayable amount was reduced by 50% to about 15 billion marks and stretched out over 30 years, and compared to the fast-growing German economy were of minor impact.

Large parts of the world devastated by World War II did not benefit from the Marshall Plan. The only major Western European nation excluded was Francisco Franco's Spain, which was highly unpopular in Washington. With the escalation of the Cold War, the United States reconsidered its position, and in 1951 embraced Spain as an ally, encouraged by Franco's aggressive anti-communist policies. Over the next decade, a considerable amount of American aid would go to Spain, but less than its neighbors had received under the Marshall Plan.

The Soviet Union had been as badly affected as any part of the world by the war. The Soviets imposed large reparations payments on the Axis allies that were in its sphere of influence. Austria, Finland, Hungary, Romania, and especially East Germany were forced to pay vast sums and ship large amounts of supplies to the USSR. These reparation payments meant the Soviet Union itself received about the same as 16 European countries received in total from Marshall Plan aid.

In accordance with the agreements with the USSR, shipment of dismantled German industrial installations from the west began on March 31, 1946. Under the terms of the agreement, the Soviet Union would in return ship raw materials such as food and timber to the western zones. In view of the Soviet failure to do so, the western zones halted the shipments east, ostensibly on a temporary basis, although they were never resumed. It was later shown that the main reason for halting shipments east was not the behavior of the USSR but rather the recalcitrant behavior of France. Examples of material received by the USSR were equipment from the Kugel-Fischer ballbearing plant at Schweinfurt, the Daimler-Benz underground aircraft-engine plant at Obrigheim, the Deschimag shipyards at Bremen-Weser, and the Gendorf powerplant.

The USSR did establish COMECON as a riposte to the Marshall Plan to deliver aid for Eastern Bloc countries, but this was complicated by the Soviet efforts to manage their own recovery from the war. The members of Comecon looked to the Soviet Union for oil; in turn, they provided machinery, equipment, agricultural goods, industrial goods, and consumer goods to the Soviet Union. Economic recovery in the East was much slower than in the West, resulting in the formation of the shortage economies and a gap in wealth between East and West. Finland, which the USSR forbade to join the Marshall Plan and which was required to give large reparations to the USSR, saw its economy recover to pre-war levels in 1947. France, which received billions of dollars through the Marshall Plan, similarly saw its average income per person return to almost pre-war level by 1949. By mid-1948 industrial production in Poland, Hungary, Bulgaria, and Czechoslovakia had recovered to a level somewhat above pre-war level.

From the end of the war to the end of 1953, the US provided grants and credits amounting to $5.9 billion to Asian countries, especially China/Taiwan ($1.051 billion), India ($255 million), Indonesia ($215 million), Japan ($2.44 billion), South Korea ($894 million), Pakistan ($98 million) and the Philippines ($803 million). In addition, another $282 million went to Israel and $196 million to the rest of the Middle East. All this aid was separate from the Marshall Plan.

Canada, like the United States, was damaged little by the war and in 1945 was one of the world's richest economies. It operated its own aid program. In 1948, the US allowed ERP aid to be used in purchasing goods from Canada. Canada made over a billion dollars in sales in the first two years of operation.

The total of American grants and loans to the world from 1945 to 1953 came to $44.3 billion.

Bradford DeLong and Barry Eichengreen conclude it was "History's Most Successful Structural Adjustment Program." They state:

It was not large enough to have significantly accelerated recovery by financing investment, aiding the reconstruction of damaged infrastructure, or easing commodity bottlenecks. We argue, however, that the Marshall Plan did play a major role in setting the stage for post-World War II Western Europe's rapid growth. The conditions attached to Marshall Plan aid pushed European political economy in a direction that left its post World War II "mixed economies" with more "market" and less "controls" in the mix.

Prior to passing and enacting the Marshall Plan, President Truman and George Marshall started a domestic overhaul of public opinion from coast to coast. The purpose of this campaign was to sway public opinion in their direction and to inform the common person of what the Marshall Plan was and what the Plan would ultimately do. They spent months attempting to convince Americans that their cause was just and that they should embrace the higher taxes that would come in the foreseeable future.

A copious amount of propaganda ended up being highly effective in swaying public opinion towards supporting the Marshall Plan. During the nationwide campaign for support, "more than a million pieces of pro-Marshall Plan publications-booklets, leaflets, reprints, and fact sheets", were disseminated. Truman's and Marshall's efforts proved to be effective. A Gallup Poll taken between the months of July and December 1947 shows the percentage of Americans unaware of the Marshall Plan fell from 51% to 36% nationwide. By the time the Marshall Plan was ready to be implemented, there was a general consensus throughout the American public that this was the right policy for both America, and the countries who would be receiving aid.

During the period leading up to World War II, Americans were highly isolationist, and many called The Marshall Plan a "milestone" for American ideology. By looking at polling data over time from pre-World War II to post-World War II, one would find that there was a change in public opinion in regards to ideology. Americans swapped their isolationist ideals for a much more global internationalist ideology after World War II.

In a National Opinion Research Center (NORC) poll taken in April 1945, a cross-section of Americans were asked, "If our government keeps on sending lendlease materials, which we may not get paid for, to friendly countries for about three years after the war, do you think this will mean more jobs or fewer jobs for most Americans, or won't it make any difference?" 75% said the same or more jobs; 10% said fewer.

Before proposing anything to Congress in 1947, the Truman administration made an elaborate effort to organize public opinion in favor of the Marshall Plan spending, reaching out to numerous national organizations representing business, labor, farmers, women, and other interest groups. Political scientist Ralph Levering points out that:

Mounting large public relations campaigns and supporting private groups such as the Citizens Committee for the Marshall Plan, the administration carefully built public and bipartisan Congressional support before bringing these measures to a vote.

Public opinion polls in 1947 consistently showed strong support for the Marshall plan among Americans. Furthermore, Gallup polls in England, France, and Italy showed favorable majorities over 60% 

Laissez-faire criticism of the Marshall Plan came from a number of economists. Wilhelm Röpke, who influenced German Minister for Economy Ludwig Erhard in his economic recovery program, believed recovery would be found in eliminating central planning and restoring a market economy in Europe, especially in those countries which had adopted more fascist and corporatist economic policies. Röpke criticized the Marshall Plan for forestalling the transition to the free market by subsidizing the current, failing systems. Erhard put Röpke's theory into practice and would later credit Röpke's influence for West Germany's preeminent success.

Henry Hazlitt criticized the Marshall Plan in his 1947 book "Will Dollars Save the World?", arguing that economic recovery comes through savings, capital accumulation, and private enterprise, and not through large cash subsidies. Austrian School economist Ludwig von Mises criticized the Marshall Plan in 1951, believing that "the American subsidies make it possible for [Europe's] governments to conceal partially the disastrous effects of the various socialist measures they have adopted". Some critics and Congressmen at the time believed that America was giving too much aid to Europe. America had already given Europe $9 billion in other forms of help in previous years. The Marshall Plan gave another $13 billion, equivalent to about $100 billion in 2010 value.

However, its role in the rapid recovery has been debated. Most reject the idea that it alone miraculously revived Europe since the evidence shows that a general recovery was already underway. The Marshall Plan grants were provided at a rate that was not much higher in terms of flow than the previous UNRRA aid and represented less than 3% of the combined national income of the recipient countries between 1948 and 1951, which would mean an increase in GDP growth of only 0.3%. In addition, there is no correlation between the amount of aid received and the speed of recovery: both France and the United Kingdom received more aid, but West Germany recovered significantly faster.

Criticism of the Marshall Plan became prominent among historians of the revisionist school, such as Walter LaFeber, during the 1960s and 1970s. They argued that the plan was American economic imperialism and that it was an attempt to gain control over Western Europe just as the Soviets controlled Eastern Europe economically through the Comecon. In a review of West Germany's economy from 1945 to 1951, German analyst Werner Abelshauser concluded that "foreign aid was not crucial in starting the recovery or in keeping it going". The economic recoveries of France, Italy, and Belgium, Cowen argues, began a few months before the flow of US money. Belgium, the country that relied earliest and most heavily on free-market economic policies after its liberation in 1944, experienced swift recovery and avoided the severe housing and food shortages seen in the rest of continental Europe.

Former US Chairman of the Federal Reserve Bank Alan Greenspan gives most credit to German Chancellor Ludwig Erhard for Europe's economic recovery. Greenspan writes in his memoir "The Age of Turbulence" that Erhard's economic policies were the most important aspect of postwar Western European recovery, even outweighing the contributions of the Marshall Plan. He states that it was Erhard's reductions in economic regulations that permitted Germany's miraculous recovery, and that these policies also contributed to the recoveries of many other European countries. Its recovery is attributed to traditional economic stimuli, such as increases in investment, fueled by a high savings rate and low taxes. Japan saw a large infusion of US investment during the Korean War.
Noam Chomsky said the Marshall Plan "set the stage for large amounts of private U.S. investment in Europe, establishing the basis for modern transnational corporations".

Alfred Friendly, press aide to the US Secretary of Commerce W. Averell Harriman, wrote a humorous operetta about the Marshall Plan during its first year; one of the lines in the operetta was: "Wines for Sale; will you swap / A little bit of steel for Chateau Neuf du Pape?"

Spanish director Luis García Berlanga co-wrote and directed the movie "Welcome Mr. Marshall!", a comedy about the residents of a small Spanish village who dream about the life of wealth and self-fulfilment the Marshall Plan will bring them. The film highlights the stereotypes held by both the Spanish and the Americans regarding the culture of the other, as well as displays social criticism of 1950s Francoist Spain.






</doc>
<doc id="19769" url="https://en.wikipedia.org/wiki?curid=19769" title="Mariculture">
Mariculture

Mariculture is a specialized branch of aquaculture involving the cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater. An example of the latter is the farming of marine fish, including finfish and shellfish like prawns, or oysters and seaweed in saltwater ponds. Non-food products produced by mariculture include: fish meal, nutrient agar, jewellery (e.g. cultured pearls), and cosmetics.

Similar to algae cultivation, shellfish can be farmed in multiple ways: on ropes, in bags or cages, or directly on (or within) the intertidal substrate. Shellfish mariculture does not require feed or fertilizer inputs, nor insecticides or antibiotics, making shellfish aquaculture (or 'mariculture') a self-supporting system. Shellfish can also be used in multi-species cultivation techniques, where shellfish can utilize waste generated by higher trophic level organisms.

After trials in 2012, a commercial "sea ranch" was set up in Flinders Bay, Western Australia to raise abalone. The ranch is based on an artificial reef made up of 5000 () separate concrete units called "abitats" (abalone habitats). The abitats can host 400 abalone each. The reef is seeded with young abalone from an onshore hatchery.

The abalone feed on seaweed that has grown naturally on the habitats; with the ecosystem enrichment of the bay also resulting in growing numbers of dhufish, pink snapper, wrasse, Samson fish among other species.

Brad Adams, from the company, has emphasised the similarity to wild abalone and the difference from shore based aquaculture. "We're not aquaculture, we're ranching, because once they're in the water they look after themselves."

Raising marine organisms under controlled conditions in exposed, high-energy ocean environments beyond significant coastal influence, is a relatively new approach to mariculture. Open ocean aquaculture (OOA) uses cages, nets, or long-line arrays that are moored, towed or float freely. Research and commercial open ocean aquaculture facilities are in operation or under development in Panama, Australia, Chile, China, France, Ireland, Italy, Japan, Mexico, and Norway. As of 2004, two commercial open ocean facilities were operating in U.S. waters, raising Threadfin near Hawaii and cobia near Puerto Rico. An operation targeting bigeye tuna recently received final approval. All U.S. commercial facilities are currently sited in waters under state or territorial jurisdiction. The largest deep water open ocean farm in the world is raising cobia 12 km off the northern coast of Panama in highly exposed sites.

Enhanced Stocking (also known as sea ranching) is a Japanese principle based on operant conditioning and the migratory nature of certain species. The fishermen raise hatchlings in a closely knitted net in a harbor, sounding an underwater horn before each feeding. When the fish are old enough they are freed from the net to mature in the open sea. During spawning season, about 80% of these fish return to their birthplace. The fishermen sound the horn and then net those fish that respond.

In seawater pond mariculture, fish are raised in ponds which receive water from the sea. This has the benefit that the nutrition (e.g. microorganisms) present in the seawater can be used. This is a great advantage over traditional fish farms (e.g. sweet water farms) for which the farmers buy feed (which is expensive). Other advantages are that water purification plants may be planted in the ponds to eliminate the buildup of nitrogen, from fecal and other contamination. Also, the ponds can be left unprotected from natural predators, providing another kind of filtering.

Mariculture has rapidly expanded over the last two decades due to new technology, improvements in formulated feeds, greater biological understanding of farmed species, increased water quality within closed farm systems, greater demand for seafood products, site expansion and government interest. As a consequence, mariculture has been subject to some controversy regarding its social and environmental impacts. Commonly identified environmental impacts from marine farms are:


As with most farming practices, the degree of environmental impact depends on the size of the farm, the cultured species, stock density, type of feed, hydrography of the site, and husbandry methods. The adjacent diagram connects these causes and effects.

Mariculture of finfish can require a significant amount of fishmeal or other high protein food sources. Originally, a lot of fishmeal went to waste due to inefficient feeding regimes and poor digestibility of formulated feeds which resulted in poor feed conversion ratios.

In cage culture, several different methods are used for feeding farmed fish – from simple hand feeding to sophisticated computer-controlled systems with automated food dispensers coupled with "in situ" uptake sensors that detect consumption rates. In coastal fish farms, overfeeding primarily leads to increased disposition of detritus on the seafloor (potentially smothering seafloor dwelling invertebrates and altering the physical environment), while in hatcheries and land-based farms, excess food goes to waste and can potentially impact the surrounding catchment and local coastal environment. This impact is usually highly local, and depends significantly on the settling velocity of waste feed and the current velocity (which varies both spatially and temporally) and depth.

The impact of escapees from aquaculture operations depends on whether or not there are wild conspecifics or close relatives in the receiving environment, and whether or not the escapee is reproductively capable. Several different mitigation/prevention strategies are currently employed, from the development of infertile triploids to land-based farms which are completely isolated from any marine environment. Escapees can adversely impact local ecosystems through hybridization and loss of genetic diversity in native stocks, increase negative interactions within an ecosystem (such as predation and competition), disease transmission and habitat changes (from trophic cascades and ecosystem shifts to varying sediment regimes and thus turbidity).

The accidental introduction of invasive species is also of concern. Aquaculture is one of the main vectors for invasives following accidental releases of farmed stocks into the wild. One example is the Siberian sturgeon ("Acipenser baerii") which accidentally escaped from a fish farm into the Gironde Estuary (Southwest France) following a severe storm in December 1999 (5,000 individual fish escaped into the estuary which had never hosted this species before). Molluscan farming is another example whereby species can be introduced to new environments by ‘hitchhiking’ on farmed molluscs. Also, farmed molluscs themselves can become dominate predators and/or competitors, as well as potentially spread pathogens and parasites.

One of the primary concerns with mariculture is the potential for disease and parasite transfer. Farmed stocks are often selectively bred to increase disease and parasite resistance, as well as improving growth rates and quality of products. As a consequence, the genetic diversity within reared stocks decreases with every generation – meaning they can potentially reduce the genetic diversity within wild populations if they escape into those wild populations. Such genetic pollution from escaped aquaculture stock can reduce the wild population's ability to adjust to the changing natural environment. Species grown by mariculture can also harbour diseases and parasites (e.g., lice) which can be introduced to wild populations upon their escape. An example of this is the parasitic sea lice on wild and farmed Atlantic salmon in Canada. Also, non-indigenous species which are farmed may have resistance to, or carry, particular diseases (which they picked up in their native habitats) which could be spread through wild populations if they escape into those wild populations. Such ‘new’ diseases would be devastating for those wild populations because they would have no immunity to them.

With the exception of benthic habitats directly beneath marine farms, most mariculture causes minimal destruction to habitats. However, the destruction of mangrove forests from the farming of shrimps is of concern. Globally, shrimp farming activity is a small contributor to the destruction of mangrove forests; however, locally it can be devastating. Mangrove forests provide rich matrices which support a great deal of biodiversity – predominately juvenile fish and crustaceans. Furthermore, they act as buffering systems whereby they reduce coastal erosion, and improve water quality for in situ animals by processing material and ‘filtering’ sediments.

In addition, nitrogen and phosphorus compounds from food and waste may lead to blooms of phytoplankton, whose subsequent degradation can drastically reduce oxygen levels. If the algae are toxic, fish are killed and shellfish contaminated.

Mariculture development must be sustained by basic and applied research and development in major fields such as nutrition, genetics, system management, product handling, and socioeconomics. One approach is closed systems that have no direct interaction with the local environment. However, investment and operational cost are currently significantly higher than open cages, limiting them to their current role as hatcheries.

Sustainable mariculture promises economic and environmental benefits. Economies of scale imply that ranching can produce fish at lower cost than industrial fishing, leading to better human diets and the gradual elimination of unsustainable fisheries. Fish grown by mariculture are also perceived to be of higher quality than fish raised in ponds or tanks, and offer more diverse choice of species. Consistent supply and quality control has enabled integration in food market channels.






Scientific literature on mariculture can be found in the following journals:



</doc>
<doc id="19770" url="https://en.wikipedia.org/wiki?curid=19770" title="Memetics">
Memetics

Memetics is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual. Critics contend the theory is "untested, unsupported or incorrect".

The term meme was coined in Richard Dawkins' 1976 book "The Selfish Gene," but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a "unit of culture" (an idea, belief, pattern of behaviour, etc.) which is "hosted" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.

The Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The "Journal of Memetics" was published electronically from 1997 to 2005.

In his book "The Selfish Gene" (1976), the evolutionary biologist Richard Dawkins used the term "meme" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Bella Hiscock outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.

Dawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of "memetics" in "The Selfish Gene", but rather coined the term "meme" in a speculative spirit. Accordingly, different researchers came to define the term "unit of information" in different ways.

The modern memetics movement dates from the mid-1980s. A January 1983 "Metamagical Themas" column by Douglas Hofstadter, in "Scientific American", was influential – as was his 1985 book of the same name. "Memeticist" was coined as analogous to "geneticist" – originally in "The Selfish Gene." Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as "memetics" by analogy with "genetics". Dawkins' "The Selfish Gene" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of "Consciousness Explained" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay "Viruses of the Mind", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's "Snow Crash").

The idea of "language as a virus" had already been introduced by William S. Burroughs as early as 1962 in his book "The Ticket That Exploded", and later in "The Electronic Revolution", published in 1970 in "". Douglas Rushkoff explored the same concept in "Media Virus: Hidden Agendas in Popular Culture" in 1995.

However, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: "Virus of the Mind: The New Science of the Meme" by former Microsoft executive turned motivational speaker and professional poker-player Richard Brodie, and "Thought Contagion: How Belief Spreads Through Society" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not aware of "The Selfish Gene" until his book was very close to publication.

Around the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – "Evolutionary Models of Information Transmission" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the "Journal of Ideas" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published "The Meme Machine", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.

The term "meme" derives from the Ancient Greek μιμητής ("mimētḗs"), meaning "imitator, pretender". The similar term "mneme" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work "Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen", translated into English in 1921 as "The Mneme". Until Daniel Schacter published "Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word "meme" independently of Semon, writing this:
"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même."

In 2005, the "Journal of Memetics – Evolutionary Models of Information Transmission" ceased publication and published a set of articles on the future of memetics. The website states that although "there was to be a relaunch...after several years nothing has happened". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words "meme" and "memetics" (without disowning the ideas in his book), adopting the self-description "thought contagionist". He died in 2005.

Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.
That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.

The memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as "a unit of cultural transmission". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as "a unit of cultural information that can be copied, located in the brain". This thinking is more in line with Dawkins' second definition of the meme in his book "The Extended Phenotype". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.

These two schools became known as the "internalists" and the "externalists." Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, "i-memes", in response to external "e-memes".

An advanced statement of the internalist school came in 2002 with the publication of "The Electric Meme", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of "Darwinizing Culture: The Status of Memetics as a Science", edited by Aunger and with a foreword by Dennett, in 2001.

This evolutionary model of cultural information transfer is based on the concept that units of information, or "memes", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.

Critics contend that some proponents' assertions are "untested, unsupported or incorrect." Luis Benitez-Bribiesca, a critic of memetics, calls it "a pseudoscientific dogma" and "a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution" among other things. As factual criticism, he refers to the lack of a "code script" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in "Darwin's Dangerous Idea") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of "no code script" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)
Another criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.
Mary Midgley criticises memetics for at least two reasons: "One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in "Darwin's Dangerous Idea", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of "meme" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple."

Henry Jenkins, Joshua Green, and Sam Ford, in their book "Spreadable Media" (2013), criticize Dawkins' idea of the meme, writing that "while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture." The three authors also criticize other interpretations of memetics, especially those which describe memes as "self-replicating", because they ignore the fact that "culture is a human product and replicates through human agency."

Like other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is "heuristically trivial", being a mere redescription of what is already known without offering any useful novelty.

Dawkins in "A Devil's Chaplain" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.

Another definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, "memeplexes", and the "deme", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word "meme". For example, in the sense of computer simulation the term "memetic algorithm" is used to define a particular computational viewpoint.

The possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.

Velikovsky (2013) proposed the "holon" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.

Proponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – "Evolutionary Models of Information Transmission" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts.
Keith Henson in "Memetics and the Modular-Mind" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host.

DiCarlo (2010) has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In "How Problem Solving and Neurotransmission in the Upper Paleolithic led to The Emergence and Maintenance of Memetic Equilibrium in Contemporary World Religions", diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium.
But the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).

Houben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.
This renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).

Research methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.

The application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.

Another application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).

Ben Cullen, in his book "Contagious Ideas", brought the idea of the meme into the discipline of archaeology. He coined the term "Cultural Virus Theory", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.

Francis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls "memetic selection criteria". These criteria opened the way to a specialized field of "applied memetics" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.

In "Selfish Sounds and Linguistic Evolution", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.

Australian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.

Swedish political scientist Mikael Sandberg argues against "Lamarckian" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active ("Lamarckian") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the "Lamarckian" IT strategy.




</doc>
<doc id="19773" url="https://en.wikipedia.org/wiki?curid=19773" title="March 25">
March 25





</doc>
<doc id="19780" url="https://en.wikipedia.org/wiki?curid=19780" title="List of islands of Michigan">
List of islands of Michigan

The following is a list of islands of Michigan. Michigan has the second longest coastline of any state after Alaska. Being bordered by four of the five Great Lakes—Erie, Huron, Michigan, and Superior—Michigan also has 64,980 inland lakes and ponds, as well as innumerable rivers, that may contain their own islands included in this list. The majority of the islands are within the Great Lakes. Other islands can also be found within other waterways of the Great Lake system, including Lake St. Clair, St. Clair River, Detroit River, and St. Marys River.

The largest of all the islands is Isle Royale in Lake Superior, which, in addition to its waters and other surrounding islands, is organized as Isle Royale National Park. Isle Royale itself is . The most populated island is Grosse Ile with approximately 10,000 residents, located in the Detroit River about south of Detroit. The majority of Michigan's islands are uninhabited and very small. Some of these otherwise unusable islands have been used for the large number of Michigan's lighthouses to aid in shipping throughout the Great Lakes, while others have been set aside as nature reserves. Many islands in Michigan have the same name, even some that are in the same municipality and body of water, such as Gull, Long, or Round islands.

Only Monroe County has territory in the westernmost portion of Lake Erie, which has a surface elevation of . The islands in the southern portion of the county are part of the North Maumee Bay Archeological District of the Detroit River International Wildlife Refuge. Turtle Island is the only island in the state of Michigan that is shared by another state. This remote and tiny island is cut in half and shared with Ohio.

Lake Huron is the second largest of the Great Lakes (after Lake Superior) with a surface area of . Michigan is the only U.S. state to border Lake Huron, while the portion of the lake on the other side of the international border belongs to the Canadian province of Ontario. The vast majority of Michigan's islands in Lake Huron are centered around Drummond Island in the northernmost portion of the state's lake territory. Drummond Island is the largest of Michigan's islands in Lake Huron and is the second largest Michigan island after Lake Superior's Isle Royale. Another large group of islands is the Les Cheneaux Islands archipelago, which itself contains dozens of small islands. Many of the lake's islands are very small and uninhabited.

As the most popular tourist destination in the state, Mackinac Island is the most well known of Lake Huron's islands. Drummond Island is the most populous of Michigan's islands in Lake Huron, with a population of 992 at the 2000 census. While Mackinac Island had a population of only about 500, there are thousands more seasonal workers and tourists during the summer months.

Michigan only has islands in Lake Michigan in the northern portion of the lake. There are no islands in the southern half of Lake Michigan. The largest and most populated of Michigan's islands in Lake Michigan is Beaver Island at and 551 residents. Some of the smaller islands surrounding Beaver Island are part of the larger Michigan Islands National Wildlife Refuge.

Lake Superior is the largest of the Great Lakes, and the coastline is sparsely populated. At , Isle Royale is the largest Michigan island and is the center of Isle Royale National Park, which itself contains over 450 islands. The following is a list of islands in Lake Superior that are "not" part of Isle Royale National Park. For those islands, see the list of islands in Isle Royale National Park.

Lake St. Clair connects Lake Huron and Lake Erie through the St. Clair River in the north and the Detroit River in the south. At , it is one of the largest non-Great Lakes in the United States, but it only contains a small number of islands near the mouth of the St. Clair River, where all of the following islands are located. The largest of these islands is Harsens Island, and all the islands are in Clay Township in St. Clair County.

The Detroit River runs for and connects Lake St. Clair to Lake Erie. For its entire length, it carries the international border between the United States and Canada. Some islands belong to Ontario in Canada and are not included in the list below. All islands on the American side belong to Wayne County. Portions of the southern portion of the river serve as wildlife refuges as part of the Detroit River International Wildlife Refuge. The largest and most populous island is Grosse Ile at and a population of around 10,000. Most of the islands are around and closely connected to Grosse Ile.

The St. Marys River connects Lake Superior and Lake Huron at the easternmost point of the Upper Peninsula. It carries the international border throughout its length, and some of the islands belong to neighboring Ontario. The largest of Michigan's islands in the river are Sugar Island and Neebish Island. Wider portions of the river are designated as Lake George, Lake Nicolet, and the Munuscong Lake. The whole length of the Michigan portion of the river is part of Chippewa County.

Michigan has numerous inland lakes and rivers that also contain their own islands. The following also lists the body of water in which these islands are located. Five islands below (<nowiki>*</nowiki> and highlighted in green) are actually islands within an island; they are contained within inland lakes in Isle Royale.

Grand Lake is a large lake in Presque Isle County. While it is not the largest inland lake in Michigan, it does contain the most inland islands that are officially named. At its shortest distance, it is located less than from Lake Huron, but the two are not connected. Grand Lake contains 14 islands, of which Grand Island is by far the largest.




</doc>
<doc id="19808" url="https://en.wikipedia.org/wiki?curid=19808" title="List of governors of Michigan">
List of governors of Michigan

The Governor of Michigan is the head of the executive branch of Michigan's state government and serves as the commander-in-chief of the state's military forces. The governor has a duty to enforce state laws; the power to either approve or veto appropriation bills passed by the Michigan Legislature; the power to convene the legislature; and the power to grant pardons, except in cases of impeachment. He or she is also empowered to reorganize the executive branch of the state government.

Michigan was originally part of French and British holdings, and administered by their colonial governors. After becoming part of the United States, numerous areas of what is today Michigan were originally part of the Northwest Territory, Indiana Territory and Illinois Territory, and administered by territorial governors. In 1805, the Michigan Territory was created, and five men served as territorial governors, until Michigan was granted statehood in 1837. Forty-eight individuals have held the position of state governor. The first female governor, Jennifer Granholm, served from 2003 to 2011.

After Michigan gained statehood, governors held the office for a two-year term, until the 1963 Michigan Constitution changed the term to four years. The number of times an individual could hold the office was unlimited until a 1992 constitutional amendment imposed a lifetime term limit of two four-year governorships. The longest-serving governor in Michigan's history was William Milliken, who was promoted from lieutenant governor after Governor George W. Romney resigned, then was elected to three further successive terms.

Michigan was part of colonial New France until the Treaty of 1763 transferred ownership to the Kingdom of Great Britain. During this time, it was governed by the Lieutenants General of New France until 1627, the Governors of New France from 1627 to 1663, and the Governors General of New France until the transfer to Great Britain. The 1783 Treaty of Paris ceded the territory that is now Michigan to the United States as part of the end of the Revolutionary War, but British troops were not removed from the area until 1796. During the British ownership, their governors administrated the area as part of the Canadian territorial holdings.

Prior to becoming its own territory, parts of Michigan were administered by the governors of the Northwest Territory, the governors of the Indiana Territory and the governors of the Illinois Territory. On June 30, 1805, the Territory of Michigan was created, with General William Hull as the first territorial governor.

Michigan was admitted to the Union on January 26, 1837. The original 1835 Constitution of Michigan provided for the election of a governor and a lieutenant governor every 2 years. The fourth and current constitution of 1963 increased this term to four years. There was no term limit on governors until a constitutional amendment effective in 1993 limited governors to two terms.

Should the office of governor become vacant, the lieutenant governor becomes governor, followed in order of succession by the Secretary of State and the Attorney General. Prior to the current constitution, the duties of the office would devolve upon the lieutenant governor, without that person actually becoming governor. The term begins at noon on January 1 of the year following the election. Prior to the 1963 constitution, the governor and lieutenant governor were elected through separate votes, allowing them to be from different parties. In 1963, this was changed, so that votes are cast jointly for a governor and lieutenant governor of the same party.

Several governors also held other high positions within the state and federal governments. Eight governors served as U.S. House of Representatives members, while seven held positions in the U.S. Senate, all representing Michigan. Others have served as ambassadors, U.S. Cabinet members, and state and federal Supreme Court justices.

As of , there are four living former governors of Michigan. The most recent death of a former governor was that of William Milliken (served 1969-83) on October 18, 2019, aged 97. Milliken was also the most recently serving governor of Michigan to have died. The state's living former governors are:





</doc>
<doc id="19809" url="https://en.wikipedia.org/wiki?curid=19809" title="Moses Amyraut">
Moses Amyraut

Moïse Amyraut, Latin Moyses Amyraldus (Bourgueil, September 1596 – January 8, 1664), in English texts often Moses Amyraut, was a French Huguenot, Reformed theologian and metaphysician. He is perhaps most noted for his modifications to Calvinist theology regarding the nature of Christ's atonement, which is referred to as Amyraldism or Amyraldianism.

Born at Bourgueil, in the valley of the Changeon in the province of Anjou, his father was a lawyer, and, preparing Moses for his own profession, sent him, on the completion of his study of the humanities at Orléans to the university of Poitiers.

At the university he took the degree of licentiate (BA) of laws. On his way home from the university he passed through Saumur, and, having visited the pastor of the Protestant church there, was introduced by him to Philippe de Mornay, governor of the city. Struck with young Amyraut's ability and culture, they both urged him to change from law to theology. His father advised him to revise his philological and philosophical studies, and read over Calvin's "Institutions," before finally determining a course. He did so, and decided for theology.

He moved to the Academy of Saumur and studied under John Cameron, who ultimately regarded him as his greatest scholar. He had a brilliant course, and was in due time licensed as a minister of the French Protestant Church. The contemporary civil wars and excitements hindered his advancement. His first church was in Saint-Aignan, in the province of Maine. There he remained two years. Jean Daillé, who moved to Paris, advised the church at Saumur to secure Amyraut as his successor, praising him "as above himself." The university of Saumur at the same time had fixed its eyes on him as professor of theology. The great churches of Paris and Rouen also contended for him, and to win him sent their deputies to the provincial synod of Anjou.

Amyraut had left the choice to the synod. He was appointed to Saumur in 1633, and to the professor's chair along with the pastorate. On the occasion of his inauguration he maintained for thesis "De Sacerdotio Christi". His co-professors were Louis Cappel and Josué de la Place, who also were Cameron's pupils and lifelong friends, who collaborated in the "Theses Salmurienses", a collection of theses propounded by candidates in theology prefaced by the inaugural addresses of the three professors. Amyraut soon gave to French Protestantism a new direction.

In 1631 he published his "Traité des religions"; and from this year onward he was a foremost man in the church. Chosen to represent the provincial synod of Anjou, Touraine and Maine at the 1631 , he was appointed as orator to present to the king "The Copy of their Complaints and Grievances for the Infractions and Violations of the Edict of Nantes".

Previous deputies had addressed the king on their bent knees, whereas the representatives of the Catholics had been permitted to stand. Amyraut consented to be orator only if the assembly authorized him to stand. There was intense resistance. Cardinal Richelieu himself, preceded by lesser dignitaries, condescended to visit Amyraut privately, to persuade him to kneel; but Amyraut held resolutely to his point and carried it. His "oration" on this occasion, which was immediately published in the French "Mercure", remains a striking landmark in the history of French Protestantism. During his absence on this matter the assembly debated "whether the Lutherans who desired it, might be admitted into communion with the Reformed Churches of France at the Lord's Table." It was decided in the affirmative previous to his return; but he approved with astonishing eloquence, and thereafter was ever in the front rank in maintaining intercommunion between all churches holding the main doctrines of the Reformation.

Pierre Bayle recounts the title-pages of no fewer than thirty-two books of which Amyraut was the author. These show that he took part in all the great controversies on predestination and Arminianism which then so agitated and harassed all Europe. Substantially he held fast the Calvinism of his preceptor Cameron; but, like Richard Baxter in England, by his breadth and charity he exposed himself to all manner of misconstruction. In 1634 he published his "Traité de la predestination", in which he tried to mitigate the harsh features of predestination by his "Universalismus hypotheticus". God, he taught, predestines all men to happiness on condition of their having faith. This gave rise to a charge of heresy, of which he was acquitted at the national synod held at Alençon in 1637, and presided over by Benjamin Basnage (1580–1652). The charge was brought up again at the national synod of Charenton in 1644, when he was again acquitted. A third attack at the synod of Loudun in 1659 met with no better success. The university of Saumur became the university of French Protestantism.

Amyraut had as many as a hundred students in attendance upon his lectures. One of these was William Penn, who would later go on to found the Pennsylvania Colony in America based in part on Amyraut's notions of religious freedom . Another historic part filled by Amyraut was in the negotiations originated by Pierre le Gouz de la Berchère (1600–1653), first president of the "parlement" of Grenoble, when exiled to Saumur, for a reconciliation and reunion of the Catholics of France with the French Protestants. Very large were the concessions made by Richelieu in his personal interviews with Amyraut; but, as with the Worcester House negotiations in England between the Church of England and nonconformists, they inevitably fell through. On all sides the statesmanship and eloquence of Amyraut were conceded. His "De l'elevation de la foy et de l'abaissement de la raison en la creance des mysteres de la religion" (1641) gave him early a high place as a metaphysician. Exclusive of his controversial writings, he left behind him a very voluminous series of practical evangelical books, which have long remained the "fireside" favourites of the peasantry of French Protestantism. Amongst these are "Estat des fideles apres la mort"; "Sur l'oraison dominicale"; "Du merite des oeuvres"; "Traité de la justification"; and paraphrases of books of the Old and New Testament. His closing years were weakened by a severe fall he met with in 1657. He died on 18 January 1664.

There were a number of theologians who defended Calvinistic orthodoxy against Amyraut and Saumur, including Friedrich Spanheim (1600–1649) and Francis Turretin (1623–1687). Ultimately, the Helvetic Consensus was drafted to counteract the theology of Saumur and Amyraldism.




</doc>
<doc id="19811" url="https://en.wikipedia.org/wiki?curid=19811" title="Murray River">
Murray River

The Murray River (or River Murray) (Ngarrindjeri: "Millewa", Yorta Yorta: "Tongala") is Australia's longest river, at in length. The Murray rises in the Australian Alps, draining the western side of Australia's highest mountains, and then meanders across Australia's inland plains, forming the border between the states of New South Wales and Victoria as it flows to the northwest into South Australia. It turns south at Morgan for its final , reaching the ocean at Lake Alexandrina.

The water of the Murray flows through several terminal lakes that fluctuate in salinity (and were often fresh until recent decades) including Lake Alexandrina and The Coorong before emptying through the Murray Mouth into the southeastern portion of the Indian Ocean, often referenced on Australian maps as the Southern Ocean, near Goolwa. Despite discharging considerable volumes of water at times, particularly before the advent of large-scale river regulation, the mouth has always been comparatively small and shallow.

As of 2010, the Murray River system receives 58 percent of its natural flow. It is perhaps Australia's most important irrigated region, and it is widely known as the food bowl of the nation.

The Murray River forms part of the long combined Murray–Darling river system which drains most of inland Victoria, New South Wales, and southern Queensland. Overall the catchment area is one-seventh of Australia's total land mass. The Murray carries only a small fraction of the water of comparably-sized rivers in other parts of the world, and with a great annual variability of its flow. In its natural state it has even been known to dry up completely during extreme droughts, although that is extremely rare, with only two or three instances of this occurring since official record keeping began.

The Murray River makes up most of the border between the Australian states of Victoria and New South Wales. Where it does, the border is the top of the bank of the Victorian side of the river (i.e., none of the river itself is actually in Victoria). This was determined in a 1980 ruling by the High Court of Australia, which settled the question as to which state had jurisdiction in the unlawful death of a man who was fishing by the river's edge on the Victorian side of the river. This boundary definition can be ambiguous, since the river changes its course over time, and some of the river banks have been modified.

West of the line of longitude 141°E, the river continues as the border between Victoria and South Australia for approximately , where this is the only stretch where a state border runs down the middle of the river. This was due to a miscalculation during the 1840s, when the border was originally surveyed. Past this point, the Murray River is entirely within the state of South Australia.

The following major settlements are located along the course of the river, with population figures from the 2011 Census:

The Murray River (and associated tributaries) support a variety of river life adapted to its vagaries. This includes a variety of native fish such as the famous Murray cod, trout cod, golden perch, Macquarie perch, silver perch, eel-tailed catfish, Australian smelt, and western carp gudgeon, and other aquatic species like the Murray short-necked turtle, Murray River crayfish, broad-clawed yabbies, and the large clawed "Macrobrachium" shrimp, as well as aquatic species more widely distributed through southeastern Australia such as common longnecked turtles, common yabbies, the small claw-less "paratya" shrimp, water rats, and platypus. The Murray River also supports fringing corridors and forests of the river red gum.

The health of the Murray River has declined significantly since European settlement, particularly due to river regulation, and much of its aquatic life including native fish are now declining, rare or endangered. Recent extreme droughts (2000–07) have put significant stress on river red gum forests, with mounting concern over their long-term survival. The Murray has also flooded on occasion, the most significant of which was the flood of 1956, which inundated many towns on the lower Murray and which lasted for up to six months.

Introduced fish species such as carp, "gambusia", weather loach, redfin perch, brown trout, and rainbow trout have also had serious negative effects on native fish, while carp have contributed to environmental degradation of the Murray River and tributaries by destroying aquatic plants and permanently raising turbidity. In some segments of the Murray River, carp have become the only species found.

Between 2.5 and 0.5 million years ago the Murray River terminated in a vast freshwater lake called Lake Bungunnia. Lake Bungunnia was formed by earth movements that blocked the Murray River near Swan Reach during this period. At its maximum extent Lake Bungunnia covered , extending to near the Menindee Lakes in the north and to near Boundary Bend on the Murray in the south. The draining of Lake Bungunnia occurred approximately 600,000 years ago.

Deep clays deposited by the lake are evident in cliffs around Chowilla in South Australia. Considerably higher rainfall would have been required to keep such a lake full; the draining of Lake Bungunnia appears to mark the end of a wet phase in the history of the Murray-Darling Basin and the onset of widespread arid conditions similar to today. A species of "Neoceratodus" lungfish existed in Lake Bungunnia (McKay & Eastburn, 1990); today "Neoceratodus" lungfish are only found in several Queensland rivers.

The noted Barmah River Red Gum Forests owe their existence to the Cadell Fault. About 25,000 years ago, displacement occurred along the Cadell fault, raising the eastern edge of the fault, which runs north-south, above the floodplain. This created a complex series of events. A section of the original Murray River channel immediately behind the fault was abandoned, and it exists today as an empty channel known as Green Gully. The Goulburn River was dammed by the southern end of the fault to create a natural lake.

The Murray River flowed to the north around the Cadell Fault, creating the channel of the Edward River which exists today and through which much of the Murray River's waters still flow. Then the natural dam on the Goulburn River failed, the lake drained, and the Murray River avulsed to the south and started to flow through the smaller Goulburn River channel, creating "The Barmah Choke" and "The Narrows" (where the river channel is unusually narrow), before entering into the proper Murray River channel again.

This complex series of events, however, diverts attention from the primary result of the Cadell Fault – that the west-flowing water of the Murray River strikes the north-south fault and diverts both north and south around the fault in the two main channels (Edward and ancestral Goulburn) as well as a fan of small streams, and regularly floods a large amount of low-lying country in the area. These conditions are perfect for River Red Gums, which rapidly formed forests in the area. Thus the displacement of the Cadell Fault 25,000 BP led directly to the formation of the famous Barmah River Red Gum Forests.

The Barmah Choke and The Narrows mean the amount of water that can travel down this part of the Murray River is restricted. In times of flood and high irrigation flows the majority of the water, in addition to flooding the Red Gum forests, actually travels through the Edward River channel. The Murray River has not had enough flow power to naturally enlarge The Barmah Choke and The Narrows to increase the amount of water they can carry.

The Cadell Fault is quite noticeable as a continuous, low, earthen embankment as one drives into Barmah from the west, although to the untrained eye it may appear man-made.
The Murray Mouth is the point at which the Murray River empties into the sea, and the interaction between its shallow, shifting and variable currents and the open sea can be complex and unpredictable. During the peak period of Murray River commerce (roughly 1855 to 1920), it presented a major impediment to the passage of goods and produce between Adelaide and the Murray settlements, and many vessels foundered or were wrecked there.

Since the early 2000s, dredging machines have operated at the Murray Mouth, moving sand from the channel to maintain a minimal flow from the sea and into the Coorong's lagoon system. Without the 24-hour dredging, the mouth would silt up and close, cutting the supply of fresh sea-water into the Coorong, which would then warm up, stagnate and die.

Being one of the major river systems on one of the driest continents on Earth, the Murray has significant cultural relevance to Aboriginal Australians. According to the peoples of Lake Alexandrina, the Murray was created by the tracks of the Great Ancestor, Ngurunderi, as he pursued Pondi, the Murray Cod. The chase originated in the interior of New South Wales. Ngurunderi pursued the fish (who, like many totem animals in Aboriginal myths, is often portrayed as a man) on rafts (or "lala") made from red gums and continually launched spears at his target. But Pondi was a wily prey and carved a weaving path, carving out the river's various tributaries. Ngurunderi was forced to beach his rafts, and often create new ones as he changed from reach to reach of the river.

At Kobathatang, Ngurunderi finally got lucky and struck Pondi in the tail with a spear. However, the shock to the fish was so great it launched him forward in a straight line to a place called Peindjalang, near Tailem Bend. Eager to rectify his failure to catch his prey, the hunter and his two wives (sometimes the escaped sibling wives of Waku and Kanu) hurried on, and took positions high on the cliff on which Tailem Bend now stands. They sprung an ambush on Pondi only to fail again. Ngurunderi set off in pursuit again but lost his prey as Pondi dived into Lake Alexandrina. Ngurunderi and his women settled on the shore, only to suffer bad luck with fishing, being plagued by a water fiend known as Muldjewangk. They later moved to a more suitable spot at the site of present-day Ashville. The twin summits of Mount Misery are supposed to be the remnants of his rafts, they are known as "Lalangengall" or "the two watercraft".

Remarkably, this story of a hunter pursuing a Murray cod that carved out the Murray persists in numerous forms in various language groups that inhabit the enormous area spanned by the Murray system. The Wotojobaluk people of Victoria tell of Totyerguil from the area now known as Swan Hill who ran out of spears while chasing Otchtout the cod.

The first Europeans to encounter the river were Hamilton Hume and William Hovell, who crossed the river where Albury now stands in 1824: Hume named it the "Hume River" after his father. In 1830, Captain Charles Sturt reached the river after travelling down its tributary the Murrumbidgee River and named it the "Murray River" in honour of the then British Secretary of State for War and the Colonies, Sir George Murray, not realising it was the same river that Hume and Hovell had encountered further upstream.

Sturt continued down the remaining length of the Murray to finally reach Lake Alexandrina and the river's mouth. The area of the Murray Mouth was explored more thoroughly by Captain Collet Barker in 1831.

The first three settlers on the Murray River are known to have been James Collins Hawker (explorer and surveyor) along with E. J. Eyre (explorer and later Governor of Jamaica) plus E. B. Scott (onetime superintendent of Yatala Labour Prison). Hawker is known to have sold his share in the Bungaree Station which he founded with his brothers, and relocated alongside the Murray at a site near Moorundie

In 1852, Francis Cadell, in preparation for the launch of his steamer service, explored the river in a canvas boat, travelling downstream from Swan Hill.

In 1858, while acting as Minister of Land and Works for New South Wales, Irish nationalist and founder of Young Ireland, Charles Gavan Duffy, founded Carlyle Township on the Murray River, a township named after his close friend, Scottish historian and essayist Thomas Carlyle. Included in the township were "Jane Street," named in honor of Carlyle's wife Jane Carlyle and "Stuart-Mill Street" in honor of political philosopher John Stuart Mill (Duffy, 1892, "Conversations with Carlyle", pp. 202–203).

In 1858, the Government Zoologist, William Blandowski, along with Gerard Krefft, explored the lower reaches of the Murray and Darling rivers, compiling a list of birds and mammals.

George "Chinese" Morrison, then aged 18, navigated the river by canoe from Wodonga to its mouth, in 65 days, completing the 1,555-mile (2,503 km) journey in January 1881.

The lack of an estuary means that shipping cannot enter the Murray from the sea. However, in the 19th century the river supported a substantial commercial trade using shallow-draft paddle steamers, the first trips being made by two boats from South Australia on the spring flood of 1853. The "Lady Augusta", captained by Francis Cadell, reached Swan Hill while another, "Mary Ann", captained by William Randell, made it as far as Moama (near Echuca). In 1855 a steamer carrying gold-mining supplies reached Albury but Echuca was the usual turn-around point, though small boats continued to link with up-river ports such as Tocumwal, Wahgunya and Albury.

The arrival of steamboat transport was welcomed by pastoralists who had been suffering from a shortage of transport due to the demands of the gold fields. By 1860 a dozen steamers were operating in the high water season along the Murray and its tributaries. Once the railway reached Echuca in 1864, the bulk of the woolclip from the Riverina was transported via river to Echuca and then south to Melbourne.

The Murray was plagued by "snags", fallen trees submerged in the water, and considerable efforts were made to clear the river of these threats to shipping by using barges equipped with steam-driven winches. In recent times, efforts have been made to restore many of these snags by placing dead gum trees back into the river. The primary purpose of this is to provide habitat for fish species whose breeding grounds and shelter were eradicated by the removal of the snags.
The volume and value of river trade made Echuca Victoria's second port and in the decade from 1874 it underwent considerable expansion. By this time up to thirty steamers and a similar number of barges were working the river in season. River transport began to decline once the railways touched the Murray at numerous points. The unreliable levels made it impossible for boats to compete with the rail and later road transport. However, the river still carries pleasure boats along its entire length.

Today, most traffic on the river is recreational. Small private boats are used for water skiing and fishing. Houseboats are common, both commercial for hire and privately owned. There are a number of both historic paddle steamers and newer boats offering cruises ranging from half an hour to 5 days.

The Murray River has been a significant barrier to land-based travel and trade. Many of the ports for transport of goods along the Murray have also developed as places to cross the river, either by bridge or ferry. The first bridge to cross the Murray, which was built in 1869, is in the town of Murray Bridge, formerly called Edwards Crossing. Tolls applied on South Australian ferries until abolished in November 1961.

Small-scale pumping plants began drawing water from the Murray in the 1850s and the first high-volume plant was constructed at Mildura in 1887. The introduction of pumping stations along the river promoted an expansion of farming and led ultimately to the development of irrigation areas (including the Murrumbidgee Irrigation Area).

In 1915, the three Murray states – New South Wales, Victoria, and South Australia – signed the River Murray Agreement which proposed the construction of storage reservoirs in the river's headwaters as well as at Lake Victoria near the South Australian border. Along the intervening stretch of the river a series of locks and weirs were built. These were originally proposed to support navigation even in times of low water, but riverborne transport was already declining due to improved highway and railway systems.

Four large reservoirs were built along the Murray. In addition to Lake Victoria (completed late 1920s), these are Lake Hume near Albury–Wodonga (completed 1936), Lake Mulwala at Yarrawonga (completed 1939), and Lake Dartmouth, which is actually on the Mitta Mitta River upstream of Lake Hume (completed 1979). The Murray also receives water from the complex dam and pipeline system of the Snowy Mountains Scheme. An additional reservoir was proposed in the 1960s at Chowilla Dam which was to have been built in South Australia and would have flooded land mostly in Victoria and New South Wales. This reservoir was cancelled in favour of building Dartmouth Dam due to costs and concerns relating to increased salinity.

From 1935 to 1940 a series of barrages was built near the Murray Mouth to stop seawater egress into the lower part of the river during low flow periods. They are the Goolwa Barrage, located at , Mundoo Channel Barrage at , Boundary Creek Barrage at , Ewe Island Barrage at , and Tauwitchere Barrage at .
These dams inverted the patterns of the river's natural flow from the original winter-spring flood and summer-autumn dry to the present low level through winter and higher during summer. These changes ensured the availability of water for irrigation and made the Murray Valley Australia's most productive agricultural region, but have seriously disrupted the life cycles of many ecosystems both inside and outside the river, and the irrigation has led to dryland salinity that now threatens the agricultural industries.

The disruption of the river's natural flow, runoff from agriculture, and the introduction of pest species like the European carp has led to serious environmental damage along the river's length and to concerns that the river will be unusably salty in the medium to long term – a serious problem given that the Murray supplies 40 percent of the water supply for Adelaide. Efforts to alleviate the problems proceed but disagreement between various groups stalls progress.

In 2006, the state government of South Australia revealed its plan to investigate the construction of the controversial Wellington Weir.

Lock 1 was completed near Blanchetown in 1922. Torrumbarry weir downstream of Echuca began operating in December 1923. Of the numerous locks that were proposed, only thirteen were completed; Locks 1 to 11 on the stretch downstream of Mildura, Lock 15 at Euston and Lock 26 at Torrumbarry. Construction of the remaining weirs purely for navigation purposes was abandoned in 1934. The last lock to be completed was Lock 15, in 1937.
Lock 11, just downstream of Mildura, creates a long lock pool which aided irrigation pumping from Mildura and Red Cliffs.

Each lock has a navigable passage next to it through the weir, which is opened during periods of high river flow, when there is too much water for the lock. The weirs can be completely removed, and the locks completely covered by water during flood conditions. Lock 11 is unique in that the lock was built inside a bend of the river, with the weir in the bend itself. A channel was dug to the lock, creating an island between it and the weir. The weir is also of a different design, being dragged out of the river during high flow, rather than lifted out.

Major tributaries


Population centres





</doc>
