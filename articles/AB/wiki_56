<doc id="18866" url="https://en.wikipedia.org/wiki?curid=18866" title="Macbeth">
Macbeth

Macbeth (; full title The Tragedy of Macbeth) is a tragedy by William Shakespeare; it is thought to have been first performed in 1606. It dramatises the damaging physical and psychological effects of political ambition on those who seek power for its own sake. Of all the plays that Shakespeare wrote during the reign of James I, who was patron of Shakespeare's acting company, "Macbeth" most clearly reflects the playwright's relationship with his sovereign. It was first published in the Folio of 1623, possibly from a prompt book, and is Shakespeare's shortest tragedy.

A brave Scottish general named Macbeth receives a prophecy from a trio of witches that one day he will become King of Scotland. Consumed by ambition and spurred to action by his wife, Macbeth murders King Duncan and takes the Scottish throne for himself. He is then wracked with guilt and paranoia. Forced to commit more and more murders to protect himself from enmity and suspicion, he soon becomes a tyrannical ruler. The bloodbath and consequent civil war swiftly take Macbeth and Lady Macbeth into the realms of madness and death.

Shakespeare's source for the story is the account of Macbeth, King of Scotland, Macduff, and Duncan in "Holinshed's Chronicles" (1587), a history of England, Scotland, and Ireland familiar to Shakespeare and his contemporaries, although the events in the play differ extensively from the history of the real Macbeth. The events of the tragedy are usually associated with the execution of Henry Garnet for complicity in the Gunpowder Plot of 1605.

In the backstage world of theatre, some believe that the play is cursed, and will not mention its title aloud, referring to it instead as "The Scottish Play". Over the course of many centuries, the play has attracted some of the most renowned actors to the roles of Macbeth and Lady Macbeth. It has been adapted to film, television, opera, novels, comics, and other media.

The play opens amid thunder and lightning, and the Three Witches decide that their next meeting will be with Macbeth. In the following scene, a wounded sergeant reports to King Duncan of Scotland that his generals Macbeth, who is the Thane of Glamis, and Banquo have just defeated the allied forces of Norway and Ireland, who were led by the traitorous Macdonwald, and the Thane of Cawdor. Macbeth, the King's kinsman, is praised for his bravery and fighting prowess.

In the following scene, Macbeth and Banquo discuss the weather and their victory. As they wander onto a heath, the Three Witches enter and greet them with prophecies. Though Banquo challenges them first, they address Macbeth, hailing him as "Thane of Glamis," "Thane of Cawdor," and that he will "be King hereafter." Macbeth appears to be stunned to silence. When Banquo asks of his own fortunes, the witches respond paradoxically, saying that he will be less than Macbeth, yet happier, less successful, yet more. He will father a line of kings, though he himself will not be one. While the two men wonder at these pronouncements, the witches vanish, and another thane, Ross, arrives and informs Macbeth of his newly bestowed title: Thane of Cawdor. The first prophecy is thus fulfilled, and Macbeth, previously sceptical, immediately begins to harbour ambitions of becoming king.

King Duncan welcomes and praises Macbeth and Banquo, and declares that he will spend the night at Macbeth's castle at Inverness; he also names his son Malcolm as his heir. Macbeth sends a message ahead to his wife, Lady Macbeth, telling her about the witches' prophecies. Lady Macbeth suffers none of her husband's uncertainty and wishes him to murder Duncan in order to obtain kingship. When Macbeth arrives at Inverness, she overrides all of her husband's objections by challenging his manhood and successfully persuades him to kill the king that very night. He and Lady Macbeth plan to get Duncan's two chamberlains drunk so that they will black out; the next morning they will blame the chamberlains for the murder. They will be defenceless as they will remember nothing.

While Duncan is asleep, Macbeth stabs him, despite his doubts and a number of supernatural portents, including a hallucination of a bloody dagger. He is so shaken that Lady Macbeth has to take charge. In accordance with her plan, she frames Duncan's sleeping servants for the murder by placing bloody daggers on them. Early the next morning, Lennox, a Scottish nobleman, and Macduff, the loyal Thane of Fife, arrive. A porter opens the gate and Macbeth leads them to the king's chamber, where Macduff discovers Duncan's body. Macbeth murders the guards to prevent them from professing their innocence, but claims he did so in a fit of anger over their misdeeds. Duncan's sons Malcolm and Donalbain flee to England and Ireland, respectively, fearing that whoever killed Duncan desires their demise as well. The rightful heirs' flight makes them suspects and Macbeth assumes the throne as the new King of Scotland as a kinsman of the dead king. Banquo reveals this to the audience, and while sceptical of the new King Macbeth, he remembers the witches' prophecy about how his own descendants would inherit the throne; this makes him suspicious of Macbeth.

Despite his success, Macbeth, also aware of this part of the prophecy, remains uneasy. Macbeth invites Banquo to a royal banquet, where he discovers that Banquo and his young son, Fleance, will be riding out that night. Fearing Banquo's suspicions, Macbeth arranges to have him murdered, by hiring two men to kill them, later sending a Third Murderer. The assassins succeed in killing Banquo, but Fleance escapes. Macbeth becomes furious: he fears that his power remains insecure as long as an heir of Banquo remains alive.

At a banquet, Macbeth invites his lords and Lady Macbeth to a night of drinking and merriment. Banquo's ghost enters and sits in Macbeth's place. Macbeth raves fearfully, startling his guests, as the ghost is only visible to him. The others panic at the sight of Macbeth raging at an empty chair, until a desperate Lady Macbeth tells them that her husband is merely afflicted with a familiar and harmless malady. The ghost departs and returns once more, causing the same riotous anger and fear in Macbeth. This time, Lady Macbeth tells the lords to leave, and they do so.

Macbeth, disturbed, visits the three witches once more and asks them to reveal the truth of their prophecies to him. To answer his questions, they summon horrible apparitions, each of which offers predictions and further prophecies to put Macbeth's fears at rest. First, they conjure an armoured head, which tells him to beware of Macduff (IV.i.72). Second, a bloody child tells him that no one born of a woman will be able to harm him. Thirdly, a crowned child holding a tree states that Macbeth will be safe until Great Birnam Wood comes to Dunsinane Hill. Macbeth is relieved and feels secure because he knows that all men are born of women and forests cannot move. Macbeth also asks whether Banquo's sons will ever reign in Scotland: the witches conjure a procession of eight crowned kings, all similar in appearance to Banquo, and the last carrying a mirror that reflects even more kings. Macbeth realises that these are all Banquo's descendants having acquired kingship in numerous countries. After the witches perform a mad dance and leave, Lennox enters and tells Macbeth that Macduff has fled to England. Macbeth orders Macduff's castle be seized, and, most cruelly, sends murderers to slaughter Macduff, as well as Macduff's wife and children. Although Macduff is no longer in the castle, everyone in Macduff's castle is put to death, including Lady Macduff and their young son.

Meanwhile, Lady Macbeth becomes racked with guilt from the crimes she and her husband have committed. At night, in the king's palace at Dunsinane, a doctor and a gentlewoman discuss Lady Macbeth's strange habit of sleepwalking. Suddenly, Lady Macbeth enters in a trance with a candle in her hand. Bemoaning the murders of Duncan, Lady Macduff, and Banquo, she tries to wash off imaginary bloodstains from her hands, all the while speaking of the terrible things she knows she pressed her husband to do. She leaves, and the doctor and gentlewoman marvel at her descent into madness. Her belief that nothing can wash away the blood on her hands is an ironic reversal of her earlier claim to Macbeth that "[a] little water clears us of this deed" (II.ii.66).

In England, Macduff is informed by Ross that his "castle is surprised; wife and babes / Savagely slaughter'd" (IV.iii.204–05). When this news of his family's execution reaches him, Macduff is stricken with grief and vows revenge. Prince Malcolm, Duncan's son, has succeeded in raising an army in England, and Macduff joins him as he rides to Scotland to challenge Macbeth's forces. The invasion has the support of the Scottish nobles, who are appalled and frightened by Macbeth's tyrannical and murderous behaviour. Malcolm leads an army, along with Macduff and Englishmen Siward (the Elder), the Earl of Northumberland, against Dunsinane Castle. While encamped in Birnam Wood, the soldiers are ordered to cut down and carry tree limbs to camouflage their numbers.

Before Macbeth's opponents arrive, he receives news that Lady Macbeth has killed herself, causing him to sink into a deep and pessimistic despair and deliver his "Tomorrow, and tomorrow, and tomorrow" soliloquy (V.v.17–28). Though he reflects on the brevity and meaninglessness of life, he nevertheless awaits the English and fortifies Dunsinane. He is certain that the witches' prophecies guarantee his invincibility, but is struck with fear when he learns that the English army is advancing on Dunsinane shielded with boughs cut from Birnam Wood, in apparent fulfillment of one of the prophecies.

A battle culminates in Macduff's confrontation with Macbeth, who kills Young Siward in combat. The English forces overwhelm his army and castle. Macbeth boasts that he has no reason to fear Macduff, for he cannot be killed by any man born of woman. Macduff declares that he was "from his mother's womb / Untimely ripp'd" (V.8.15–16), (i.e., born by Caesarean section) and is not "of woman born" (an example of a literary quibble), fulfilling the second prophecy. Macbeth realises too late that he has misinterpreted the witches' words. Though he realises that he is doomed, he continues to fight. Macduff kills and beheads him, thus fulfilling the remaining prophecy.

Macduff carries Macbeth's head onstage and Malcolm discusses how order has been restored. His last reference to Lady Macbeth, however, reveals "'tis thought, by self and violent hands / Took off her life" (V.ix.71–72), but the method of her suicide is undisclosed. Malcolm, now the King of Scotland, declares his benevolent intentions for the country and invites all to see him crowned at Scone.

Although Malcolm, and not Fleance, is placed on the throne, the witches' prophecy concerning Banquo ("Thou shalt get kings") was known to the audience of Shakespeare's time to be true: James VI of Scotland (later also James I of England) was supposedly a descendant of Banquo.

A principal source comes from the "Daemonologie" of King James published in 1597 which included a news pamphlet titled "Newes from Scotland" that detailed the famous North Berwick Witch Trials of 1590. The publication of "Daemonologie" came just a few years before the tragedy of "Macbeth" with the themes and setting in a direct and comparative contrast with King James' personal experiences with witchcraft. Not only had this trial taken place in Scotland, the witches involved were recorded to have also conducted rituals with the same mannerisms as the three witches. One of the evidenced passages is referenced when the witches involved in the trial confessed to attempt the use of witchcraft to raise a tempest and sabotage the very boat King James and his queen were on board during their return trip from Denmark. This was significant as one ship sailing with King James' fleet actually sank in the storm. The three witches discuss the raising of winds at sea in the opening lines of Act 1 Scene 3.

"Macbeth" has been compared to Shakespeare's "Antony and Cleopatra." As characters, both Antony and Macbeth seek a new world, even at the cost of the old one. Both fight for a throne and have a 'nemesis' to face to achieve that throne. For Antony, the nemesis is Octavius; for Macbeth, it is Banquo. At one point Macbeth even compares himself to Antony, saying "under Banquo / My Genius is rebuk'd, as it is said / Mark Antony's was by Caesar." Lastly, both plays contain powerful and manipulative female figures: Cleopatra and Lady Macbeth.

Shakespeare borrowed the story from several tales in "Holinshed's Chronicles", a popular history of the British Isles well known to Shakespeare and his contemporaries. In "Chronicles", a man named Donwald finds several of his family put to death by his king, King Duff, for dealing with witches. After being pressured by his wife, he and four of his servants kill the King in his own house. In "Chronicles", Macbeth is portrayed as struggling to support the kingdom in the face of King Duncan's ineptitude. He and Banquo meet the three witches, who make exactly the same prophecies as in Shakespeare's version. Macbeth and Banquo then together plot the murder of Duncan, at Lady Macbeth's urging. Macbeth has a long, ten-year reign before eventually being overthrown by Macduff and Malcolm. The parallels between the two versions are clear. However, some scholars think that George Buchanan's "Rerum Scoticarum Historia" matches Shakespeare's version more closely. Buchanan's work was available in Latin in Shakespeare's day.

No medieval account of the reign of Macbeth mentions the Weird Sisters, Banquo, or Lady Macbeth, and with the exception of the latter none actually existed. The characters of Banquo, the Weird Sisters, and Lady Macbeth were first mentioned in 1527 by a Scottish historian Hector Boece in his book "Historia Gentis Scotorum" ("History of the Scottish People") who wanted to denigrate Macbeth in order to strengthen the claim of the House of Stewart to the Scottish throne. Boece portrayed Banquo as an ancestor of the Stewart kings of Scotland, adding in a "prophecy" that the descendants of Banquo would be the rightful kings of Scotland while the Weird Sisters served to give a picture of King Macbeth as gaining the throne via dark supernatural forces. Macbeth did have a wife, but it is not clear if she was as power-hungry and ambitious as Boece portrayed her, which served his purpose of having even Macbeth realise he lacked a proper claim to the throne, and only took it at the urging of his wife. Holinshed accepted Boece's version of Macbeth's reign at face value and included it in his "Chronicles". Shakespeare saw the dramatic possibilities in the story as related by Holinshed, and used it as the basis for the play.

No other version of the story has Macbeth kill the king in Macbeth's own castle. Scholars have seen this change of Shakespeare's as adding to the darkness of Macbeth's crime as the worst violation of hospitality. Versions of the story that were common at the time had Duncan being killed in an ambush at Inverness, not in a castle. Shakespeare conflated the story of Donwald and King Duff in what was a significant change to the story.

Shakespeare made another important change. In "Chronicles", Banquo is an accomplice in Macbeth's murder of King Duncan, and plays an important part in ensuring that Macbeth, not Malcolm, takes the throne in the coup that follows. In Shakespeare's day, Banquo was thought to be an ancestor of the Stuart King James I. (In the 19th century it was established that Banquo is an unhistorical character, the Stuarts are actually descended from a Breton family which migrated to Scotland slightly later than Macbeth's time.) The Banquo portrayed in earlier sources is significantly different from the Banquo created by Shakespeare. Critics have proposed several reasons for this change. First, to portray the king's ancestor as a murderer would have been risky. Other authors of the time who wrote about Banquo, such as Jean de Schelandre in his "Stuartide", also changed history by portraying Banquo as a noble man, not a murderer, probably for the same reasons. Second, Shakespeare may have altered Banquo's character simply because there was no dramatic need for another accomplice to the murder; there was, however, a need to give a dramatic contrast to Macbeth—a role which many scholars argue is filled by Banquo.

Other scholars maintain that a strong argument can be made for associating the tragedy with the Gunpowder Plot of 1605. As presented by Harold Bloom in 2008: "[S]cholars cite the existence of several topical references in "Macbeth" to the events of that year, namely the execution of the Father Henry Garnett for his alleged complicity in the Gunpowder Plot of 1605, as referenced in the porter's scene." Those arrested for their role in the Gunpowder Plot refused to give direct answers to the questions posed to them by their interrogators, which reflected the influence of the Jesuit practice of equivocation. Shakespeare, by having Macbeth say that demons "palter...in a double sense" and "keep the promise to our ear/And break it to our hope", confirmed James's belief that equivocation was a "wicked" practice, which reflected in turn the "wickedness" of the Catholic Church. Garnett had in his possession "A Treatise on Equivocation", and in the play the Weird Sisters often engage in equivocation, for instance telling Macbeth that he could never be overthrown until "Great Birnan wood to high Dunsinane hill/Shall Come". Macbeth interprets the prophecy as meaning never, but in fact, the Three Sisters refer only to branches of the trees of Great Birnan coming to Dunsinane hill.

"Macbeth" cannot be dated precisely but is usually taken as contemporaneous to the other canonical tragedies ("Hamlet", "Othello", and "King Lear"). While some scholars have placed the original writing of the play as early as 1599, most believe that the play is unlikely to have been composed earlier than 1603 as the play is widely seen to celebrate King James' ancestors and the Stuart accession to the throne in 1603 (James believed himself to be descended from Banquo), suggesting that the parade of eight kings—which the witches show Macbeth in a vision in Act IV—is a compliment to King James. Many scholars think the play was written in 1606 in the aftermath of the Gunpowder Plot, citing possible internal allusions to the 1605 plot and its ensuing trials. In fact, there are a great number of allusions and possible pieces of evidence alluding to the Plot, and, for this reason, a great many critics agree that "Macbeth" was written in the year 1606. Lady Macbeth's instructions to her husband, "Look like the innocent flower, but be the serpent under't" (1.5.74–75), may be an allusion to a medal that was struck in 1605 to commemorate King James' escape that depicted a serpent hiding among lilies and roses.

Particularly, the Porter's speech (2.3.1–21) in which he welcomes an "equivocator", a farmer, and a tailor to hell (2.3.8–13), has been argued to be an allusion to the 28 March 1606 trial and execution on 3 May 1606 of the Jesuit Henry Garnet, who used the alias "Farmer", with "equivocator" referring to Garnet's defence of "equivocation". The porter says that the equivocator "committed treason enough for God's sake" (2.3.9–10), which specifically connects equivocation and treason and ties it to the Jesuit belief that equivocation was only lawful when used "for God's sake", strengthening the allusion to Garnet. The porter goes on to say that the equivocator "yet could not equivocate to heaven" (2.3.10–11), echoing grim jokes that were current on the eve of Garnet's execution: i.e. that Garnet would be "hanged without equivocation" and at his execution he was asked "not to equivocate with his last breath." The "English tailor" the porter admits to hell (2.3.13), has been seen as an allusion to Hugh Griffin, a tailor who was questioned by the Archbishop of Canterbury on 27 November and 3 December 1607 for the part he played in Garnet's "miraculous straw", an infamous head of straw that was stained with Garnet's blood that had congealed into a form resembling Garnet's portrait, which was hailed by Catholics as a miracle. The tailor Griffin became notorious and the subject of verses published with his portrait on the title page.

When James became king of England, a feeling of uncertainty settled over the nation. James was a Scottish king and the son of Mary, Queen of Scots, a staunch Catholic and English traitor. In the words of critic Robert Crawford, ""Macbeth" was a play for a post-Elizabethan England facing up to what it might mean to have a Scottish king. England seems comparatively benign, while its northern neighbour is mired in a bloody, monarch-killing past. ... "Macbeth" may have been set in medieval Scotland, but it was filled with material of interest to England and England's ruler." Critics argue that the content of the play is clearly a message to James, the new Scottish King of England. Likewise, the critic Andrew Hadfield noted the contrast the play draws between the saintly King Edward the Confessor of England who has the power of the royal touch to cure scrofula and whose realm is portrayed as peaceful and prosperous vs. the bloody chaos of Scotland. James in his 1598 book "The Trew Law of Free Monarchies" had asserted that kings are always right, if not just, and his subjects owe him total loyalty at all times, writing that even if a king is a tyrant, his subjects must never rebel and just endure his tyranny for their own good. James had argued that the tyranny was preferable to the problems caused by rebellion which were even worse; Shakespeare by contrast in "Macbeth" argued for the right of the subjects to overthrow a tyrant king, in what appeared to be an implied criticism of James's theories if applied to England. Hadfield also noted a curious aspect of the play in that it implies that primogeniture is the norm in Scotland, but Duncan has to nominate his son Malcolm to be his successor while Macbeth is accepted without protest by the Scottish lairds as their king despite being an usurper. Hadfield argued this aspect of the play with the thanes apparently choosing their king was a reference to the Stuart claim to the English throne, and the attempts of the English parliament to block the succession of James's Catholic mother, Mary, Queen of Scots, from succeeding to the English throne. Hadfield argued that Shakespeare implied that James was indeed the rightful king of England, but owned his throne not to divine favour as James would have it, but rather due to the willingness of the English Parliament to accept the Protestant son of the Catholic Mary, Queen of Scots, as their king.

Garry Wills provides further evidence that "Macbeth" is a Gunpowder Play (a type of play that emerged immediately following the events of the Gunpowder Plot). He points out that every Gunpowder Play contains "a necromancy scene, regicide attempted or completed, references to equivocation, scenes that test loyalty by use of deceptive language, and a character who sees through plots—along with a vocabulary similar to the Plot in its immediate aftermath (words like "train, blow, vault") and an ironic recoil of the Plot upon the Plotters (who fall into the pit they dug)."

The play utilizes a few key words that the audience at the time would recognize as allusions to the Plot. In one sermon in 1605, Lancelot Andrewes stated, regarding the failure of the Plotters on God's day, "Be they fair or foul, glad or sad (as the poet calleth Him) the great Diespiter, 'the Father of days' hath made them both." Shakespeare begins the play by using the words "fair" and "foul" in the first speeches of the witches and Macbeth. In the words of Jonathan Gil Harris, the play expresses the "horror unleashed by a supposedly loyal subject who seeks to kill a king and the treasonous role of equivocation. The play even echoes certain keywords from the scandal—the 'vault' beneath the House of Parliament in which Guy Fawkes stored thirty kegs of gunpowder and the 'blow' about which one of the conspirators had secretly warned a relative who planned to attend the House of Parliament on 5 November...Even though the Plot is never alluded to directly, its presence is everywhere in the play, like a pervasive odor."
Scholars also cite an entertainment seen by King James at Oxford in the summer of 1605 that featured three "sibyls" like the weird sisters; Kermode surmises that Shakespeare could have heard about this and alluded to it with the weird sisters. However, A. R. Braunmuller in the New Cambridge edition finds the 1605–06 arguments inconclusive, and argues only for an earliest date of 1603.

One suggested allusion supporting a date in late 1606 is the first witch's dialogue about a sailor's wife: "'Aroint thee, witch!' the rump-fed ronyon cries./Her husband's to Aleppo gone, master o' the "Tiger"" (1.3.6–7). This has been thought to allude to the "Tiger", a ship that returned to England 27 June 1606 after a disastrous voyage in which many of the crew were killed by pirates. A few lines later the witch speaks of the sailor, "He shall live a man forbid:/Weary se'nnights nine times nine" (1.3.21–22). The real ship was at sea 567 days, the product of 7x9x9, which has been taken as a confirmation of the allusion, which if correct, confirms that the witch scenes were either written or amended later than July 1606.

The play is not considered to have been written any later than 1607, since, as Kermode notes, there are "fairly clear allusions to the play in 1607." One notable reference is in Francis Beaumont's "Knight of the Burning Pestle", first performed in 1607. The following lines (Act V, Scene 1, 24–30) are, according to scholars, a clear allusion to the scene in which Banquo's ghost haunts Macbeth at the dinner table:

<poem>
When thou art at thy table with thy friends,
Merry in heart, and filled with swelling wine,
I'll come in midst of all thy pride and mirth,
Invisible to all men but thyself,
And whisper such a sad tale in thine ear
Shall make thee let the cup fall from thy hand,
And stand as mute and pale as death itself.</poem>

"Macbeth" was first printed in the First Folio of 1623 and the Folio is the only source for the text. Some scholars contend that the Folio text was abridged and rearranged from an earlier manuscript or prompt book. Often cited as interpolation are stage cues for two songs, whose lyrics are not included in the Folio but are included in Thomas Middleton's play "The Witch", which was written between the accepted date for "Macbeth" (1606) and the printing of the Folio. Many scholars believe these songs were editorially inserted into the Folio, though whether they were Middleton's songs or preexisting songs is not certain. It is also widely believed that the character of Hecate, as well as some lines of the First Witch (4.1 124–31), were not part of Shakespeare's original play but were added by the Folio editors and possibly written by Middleton, though "there is no completely objective proof" of such interpolation.

The 'reconstructive movement' was concerned with the recreation of Elizabethan acting conditions, and would eventually lead to the creation of Shakespeare's Globe and similar replicas. One of the movement's offshoots was in the reconstruction of Elizabethan pronunciation: for example Bernard Miles' 1951 "Macbeth", for which linguists from University College London were employed to create a transcript of the play in Elizabethan English, then an audio recording of that transcription, from which the actors, in turn, learned their lines.

The pronunciation of many words evolves over time. In Shakespeare's day, for example, "heath" was pronounced as "heth" ("or a slightly elongated 'e' as in the modern 'get'"), so it rhymed with "Macbeth" in the sentences by the Witches at the beginning of the play:

Second Witch: Upon the heath.Third Witch: There to meet with Macbeth.

A scholar of antique pronunciation writes, ""Heath" would have made a close (if not exact) rhyme with the "-eth" of "Macbeth", which was pronounced with a short 'i' as in 'it'."

In the theatre programme notes, "much was made of how OP [Original Pronunciation] performance reintroduces lost rhymes such as the final couplet: 'So thanks to all at once, and each to one, / Whom we invite to see us crowned at Scone'" (5.11.40–41) where 'one' sounds like 'own'. The Witches, the play's great purveyors of rhyme, benefited most in this regard. So, 'babe' (4.1.30) sounded like 'bab' and rhymed with 'drab' (4.1.31)..."

Eoin Price wrote, "I found the OP rendition of Banquo's brilliant question 'Or have we eaten on the insane root / That takes the raison prisoner?' unduly amusing"; and he adds,

"Macbeth" is an anomaly among Shakespeare's tragedies in certain critical ways. It is short: more than a thousand lines shorter than "Othello" and "King Lear", and only slightly more than half as long as "Hamlet". This brevity has suggested to many critics that the received version is based on a heavily cut source, perhaps a prompt-book for a particular performance. This would reflect other Shakespearean plays existing in both Quarto and the Folio, where the Quarto versions are usually longer than the Folio versions. "Macbeth" was first printed in the First Folio, but has no Quarto version – if there were a Quarto, it would probably be longer than the Folio version. That brevity has also been connected to other unusual features: the fast pace of the first act, which has seemed to be "stripped for action"; the comparative flatness of the characters other than Macbeth; and the oddness of Macbeth himself compared with other Shakespearean tragic heroes. A. C. Bradley, in considering this question, concluded the play "always was an extremely short one", noting the witch scenes and battle scenes would have taken up some time in performance, remarking, "I do not think that, in reading, we "feel" Macbeth to be short: certainly we are astonished when we hear it is about half as long as "Hamlet". Perhaps in the Shakespearean theatre too it seemed to occupy a longer time than the clock recorded."

At least since the days of Alexander Pope and Samuel Johnson, analysis of the play has centred on the question of Macbeth's ambition, commonly seen as so dominant a trait that it defines the character. Johnson asserted that Macbeth, though esteemed for his military bravery, is wholly reviled.

This opinion recurs in critical literature, and, according to Caroline Spurgeon, is supported by Shakespeare himself, who apparently intended to degrade his hero by vesting him with clothes unsuited to him and to make Macbeth look ridiculous by several nimisms he applies: His garments seem either too big or too small for him – as his ambition is too big and his character too small for his new and unrightful role as king. When he feels as if "dressed in borrowed robes", after his new title as Thane of Cawdor, prophesied by the witches, has been confirmed by Ross (I, 3, ll. 108–09), Banquo comments: "New honours come upon him,<br>Like our strange garments, cleave not to their mould,<br>But with the aid of use" (I, 3, ll. 145–46). And, at the end, when the tyrant is at bay at Dunsinane, Caithness sees him as a man trying in vain to fasten a large garment on him with too small a belt: "He cannot buckle his distemper'd cause <br> Within the belt of rule" (V, 2, ll. 14–15) while Angus, in a similar nimism, sums up what everybody thinks ever since Macbeth's accession to power: "now does he feel his title <br> Hang loose about him, like a giant's robe <br> upon a dwarfish thief" (V, 2, ll. 18–20).

Like Richard III, but without that character's perversely appealing exuberance, Macbeth wades through blood until his inevitable fall. As Kenneth Muir writes, "Macbeth has not a predisposition to murder; he has merely an inordinate ambition that makes murder itself seem to be a lesser evil than failure to achieve the crown." Some critics, such as E. E. Stoll, explain this characterisation as a holdover from Senecan or medieval tradition. Shakespeare's audience, in this view, expected villains to be wholly bad, and Senecan style, far from prohibiting a villainous protagonist, all but demanded it.

Yet for other critics, it has not been so easy to resolve the question of Macbeth's motivation. Robert Bridges, for instance, perceived a paradox: a character able to express such convincing horror before Duncan's murder would likely be incapable of committing the crime. For many critics, Macbeth's motivations in the first act appear vague and insufficient. John Dover Wilson hypothesised that Shakespeare's original text had an extra scene or scenes where husband and wife discussed their plans. This interpretation is not fully provable; however, the motivating role of ambition for Macbeth is universally recognised. The evil actions motivated by his ambition seem to trap him in a cycle of increasing evil, as Macbeth himself recognises: "I am in blood<br>Stepp'd in so far that, should I wade no more,<br> Returning were as tedious as go o'er."

While working on Russian translations of Shakespeare's works, Boris Pasternak compared Macbeth to Raskolnikov, the protagonist of "Crime and Punishment" by Fyodor Dostoevsky. Pasternak argues that "neither Macbeth or Raskolnikov is a born criminal or a villain by nature. They are turned into criminals by faulty rationalizations, by deductions from false premises." He goes on to argue that Lady Macbeth is "feminine ... one of those active, insistent wives" who becomes her husband's "executive, more resolute and consistent than he is himself." According to Pasternak, she is only helping Macbeth carry out his own wishes, to her own detriment.

The disastrous consequences of Macbeth's ambition are not limited to him. Almost from the moment of the murder, the play depicts Scotland as a land shaken by inversions of the natural order. Shakespeare may have intended a reference to the great chain of being, although the play's images of disorder are mostly not specific enough to support detailed intellectual readings. He may also have intended an elaborate compliment to James's belief in the divine right of kings, although this hypothesis, outlined at greatest length by Henry N. Paul, is not universally accepted. As in "Julius Caesar", though, perturbations in the political sphere are echoed and even amplified by events in the material world. Among the most often depicted of the inversions of the natural order is sleep. Macbeth's announcement that he has "murdered sleep" is figuratively mirrored in Lady Macbeth's sleepwalking.

"Macbeth"s generally accepted indebtedness to medieval tragedy is often seen as significant in the play's treatment of moral order. Glynne Wickham connects the play, through the Porter, to a mystery play on the harrowing of hell. Howard Felperin argues that the play has a more complex attitude toward "orthodox Christian tragedy" than is often admitted; he sees a kinship between the play and the tyrant plays within the medieval liturgical drama.

The theme of androgyny is often seen as a special aspect of the theme of disorder. Inversion of normative gender roles is most famously associated with the witches and with Lady Macbeth as she appears in the first act. Whatever Shakespeare's degree of sympathy with such inversions, the play ends with a thorough return to normative gender values. Some feminist psychoanalytic critics, such as Janet Adelman, have connected the play's treatment of gender roles to its larger theme of inverted natural order. In this light, Macbeth is punished for his violation of the moral order by being removed from the cycles of nature (which are figured as female); nature itself (as embodied in the movement of Birnam Wood) is part of the restoration of moral order.

Critics in the early twentieth century reacted against what they saw as an excessive dependence on the study of character in criticism of the play. This dependence, though most closely associated with Andrew Cecil Bradley, is clear as early as the time of Mary Cowden Clarke, who offered precise, if fanciful, accounts of the predramatic lives of Shakespeare's female leads. She suggested, for instance, that the child Lady Macbeth refers to in the first act died during a foolish military action.

In the play, the Three Witches represent darkness, chaos, and conflict, while their role is as agents and witnesses. Their presence communicates treason and impending doom. During Shakespeare's day, witches were seen as worse than rebels, "the most notorious traytor and rebell that can be." They were not only political traitors, but spiritual traitors as well. Much of the confusion that springs from them comes from their ability to straddle the play's borders between reality and the supernatural. They are so deeply entrenched in both worlds that it is unclear whether they control fate, or whether they are merely its agents. They defy logic, not being subject to the rules of the real world. The witches' lines in the first act: "Fair is foul, and foul is fair: Hover through the fog and filthy air" are often said to set the tone for the rest of the play by establishing a sense of confusion. Indeed, the play is filled with situations where evil is depicted as good, while good is rendered evil. The line "Double, double toil and trouble," communicates the witches' intent clearly: they seek only trouble for the mortals around them. The witches' spells are remarkably similar to the spells of the witch Medusa in Anthony Munday's play "Fidele and Fortunio" published in 1584, and Shakespeare may have been influenced by these.

While the witches do not tell Macbeth directly to kill King Duncan, they use a subtle form of temptation when they tell Macbeth that he is destined to be king. By placing this thought in his mind, they effectively guide him on the path to his own destruction. This follows the pattern of temptation used at the time of Shakespeare. First, they argued, a thought is put in a man's mind, then the person may either indulge in the thought or reject it. Macbeth indulges in it, while Banquo rejects.

According to J. A. Bryant Jr., "Macbeth" also makes use of Biblical parallels, notably between King Duncan's murder and the murder of Christ:

While many today would say that any misfortune surrounding a production is mere coincidence, actors and others in the theatre industry often consider it bad luck to mention "Macbeth" by name while inside a theatre, and sometimes refer to it indirectly, for example as "The Scottish Play", or "MacBee", or when referring to the character and not the play, "Mr. and Mrs. M", or "The Scottish King".

This is because Shakespeare (or the play's revisers) are said to have used the spells of real witches in his text, purportedly angering the witches and causing them to curse the play. Thus, to say the name of the play inside a theatre is believed to doom the production to failure, and perhaps cause physical injury or death to cast members. There are stories of accidents, misfortunes and even deaths taking place during runs of "Macbeth".

According to the actor Sir Donald Sinden, in his Sky Arts TV series "Great West End Theatres", contrary to popular myth, Shakespeare's tragedy Macbeth is not the unluckiest play as superstition likes to portray it. Exactly the opposite! The origin of the unfortunate moniker dates back to repertory theatre days when each town and village had at least one theatre to entertain the public. If a play was not doing well, it would invariably get 'pulled' and replaced with a sure-fire audience pleaser – Macbeth guaranteed full-houses. So when the weekly theatre newspaper, "The Stage" was published, listing what was on in each theatre in the country, it was instantly noticed what shows had "not" worked the previous week, as they had been replaced by a definite crowd-pleaser. More actors have died during performances of Hamlet than in the "Scottish play" as the profession still calls it. It is forbidden to quote from it backstage as this could cause the current play to collapse and have to be replaced, causing possible unemployment.

Several methods exist to dispel the curse, depending on the actor. One, attributed to Michael York, is to immediately leave the building the stage is in with the person who uttered the name, walk around it three times, spit over their left shoulders, say an obscenity then wait to be invited back into the building. A related practice is to spin around three times as fast as possible on the spot, sometimes accompanied by spitting over their shoulder, and uttering an obscenity. Another popular "ritual" is to leave the room, knock three times, be invited in, and then quote a line from "Hamlet". Yet another is to recite lines from "The Merchant of Venice", thought to be a lucky play.

The only eyewitness account of "Macbeth" in Shakespeare's lifetime was recorded by Simon Forman, who saw a performance at the Globe on 20 April 1610. Scholars have noted discrepancies between Forman's account and the play as it appears in the Folio. For example, he makes no mention of the apparition scene, or of Hecate, of the man not of woman born, or of Birnam Wood. However, Clark observes that Forman's accounts were often inaccurate and incomplete (for instance omitting the statue scene from "The Winter's Tale") and his interest did not seem to be in "giving full accounts of the productions."

As mentioned above, the Folio text is thought by some to be an alteration of the original play. This has led to the theory that the play as we know it from the Folio was an adaptation for indoor performance at the Blackfriars Theatre (which was operated by the King's Men from 1608) – and even speculation that it represents a specific performance before King James. The play contains more musical cues than any other play in the canon as well as a significant use of sound effects.

All theatres were closed down by the Puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them. Sir William Davenant, founder of the Duke's Company, adapted Shakespeare's play to the tastes of the new era, and his version would dominate on stage for around eighty years. Among the changes he made were the expansion of the role of the witches, introducing new songs, dances and 'flying', and the expansion of the role of Lady Macduff as a foil to Lady Macbeth. There were, however, performances outside the patent companies: among the evasions of the Duke's Company's monopoly was a puppet version of "Macbeth".

"Macbeth" was a favourite of the seventeenth-century diarist Samuel Pepys, who saw the play on 5 November 1664 ("admirably acted"), 28 December 1666 ("most excellently acted"), ten days later on 7 January 1667 ("though I saw it lately, yet [it] appears a most excellent play in all respects"), on 19 April 1667 ("one of the best plays for a stage ... that ever I saw"), again on 16 October 1667 ("was vexed to see Young, who is but a bad actor at best, act Macbeth in the room of Betterton, who, poor man! is sick"), and again three weeks later on 6 November 1667 ("[at] "Macbeth", which we still like mightily"), yet again on 12 August 1668 ("saw "Macbeth", to our great content"), and finally on 21 December 1668, on which date the king and court were also present in the audience.

The first professional performances of "Macbeth" in North America were probably those of The Hallam Company.

In 1744, David Garrick revived the play, abandoning Davenant's version and instead advertising it "as written by Shakespeare". In fact this claim was largely false: he retained much of Davenant's more popular business for the witches, and himself wrote a lengthy death speech for Macbeth. And he cut more than 10% of Shakespeare's play, including the drunken porter, the murder of Lady Macduff's son, and Malcolm's testing of Macduff. Hannah Pritchard was his greatest stage partner, having her premiere as his Lady Macbeth in 1747. He would later drop the play from his repertoire upon her retirement from the stage. Mrs. Pritchard was the first actress to achieve acclaim in the role of Lady Macbeth – at least partly due to the removal of Davenant's material, which made irrelevant moral contrasts with Lady Macduff. Garrick's portrayal focused on the inner life of the character, endowing him with an innocence vacillating between good and evil, and betrayed by outside influences. He portrayed a man capable of observing himself, as if a part of him remained untouched by what he had done, the play moulding him into a man of sensibility, rather than him descending into a tyrant.

John Philip Kemble first played Macbeth in 1778. Although usually regarded as the antithesis of Garrick, Kemble nevertheless refined aspects of Garrick's portrayal into his own. However it was the "towering and majestic" Sarah Siddons (Kemble's sister) who became a legend in the role of Lady Macbeth. In contrast to Hannah Pritchard's savage, demonic portrayal, Siddons' Lady Macbeth, while terrifying, was nevertheless – in the scenes in which she expresses her regret and remorse – tenderly human. And in portraying her actions as done out of love for her husband, Siddons deflected from him some of the moral responsibility for the play's carnage. Audiences seem to have found the sleepwalking scene particularly mesmerising: Hazlitt said of it that "all her gestures were involuntary and mechanical ... She glided on and off the stage almost like an apparition."

In 1794, Kemble dispensed with the ghost of Banquo altogether, allowing the audience to see Macbeth's reaction as his wife and guests see it, and relying upon the fact that the play was so well known that his audience would already be aware that a ghost enters at that point.

Ferdinand Fleck, notable as the first German actor to present Shakespeare's tragic roles in their fullness, played Macbeth at the Berlin National Theatre from 1787. Unlike his English counterparts, he portrayed the character as achieving his stature after the murder of Duncan, growing in presence and confidence: thereby enabling stark contrasts, such as in the banquet scene, which he ended babbling like a child.

Performances outside the patent theatres were instrumental in bringing the monopoly to an end. Robert Elliston, for example, produced a popular adaptation of "Macbeth" in 1809 at the Royal Circus described in its publicity as "this matchless piece of pantomimic and choral performance", which circumvented the illegality of speaking Shakespeare's words through mimed action, singing, and doggerel verse written by J. C. Cross.

In 1809, in an unsuccessful attempt to take Covent Garden upmarket, Kemble installed private boxes, increasing admission prices to pay for the improvements. The inaugural run at the newly renovated theatre was "Macbeth", which was disrupted for over two months with cries of "Old prices!" and "No private boxes!" until Kemble capitulated to the protestors' demands.

Edmund Kean at Drury Lane gave a psychological portrayal of the central character, with a common touch, but was ultimately unsuccessful in the role. However he did pave the way for the most acclaimed performance of the nineteenth century, that of William Charles Macready. Macready played the role over a 30-year period, firstly at Covent Garden in 1820 and finally in his retirement performance. Although his playing evolved over the years, it was noted throughout for the tension between the idealistic aspects and the weaker, venal aspects of Macbeth's character. His staging was full of spectacle, including several elaborate royal processions.

In 1843 the Theatres Regulation Act finally brought the patent companies' monopoly to an end. From that time until the end of the Victorian era, London theatre was dominated by the actor-managers, and the style of presentation was "pictorial" – proscenium stages filled with spectacular stage-pictures, often featuring complex scenery, large casts in elaborate costumes, and frequent use of tableaux vivant. Charles Kean (son of Edmund), at London's Princess's Theatre from 1850 to 1859, took an antiquarian view of Shakespeare performance, setting his "Macbeth" in a historically accurate eleventh-century Scotland. His leading lady, Ellen Tree, created a sense of the character's inner life: "The Times" critic saying "The countenance which she assumed ... when luring on Macbeth in his course of crime, was actually appalling in intensity, as if it denoted a hunger after guilt." At the same time, special effects were becoming popular: for example in Samuel Phelps' "Macbeth" the witches performed behind green gauze, enabling them to appear and disappear using stage lighting.

In 1849, rival performances of the play sparked the Astor Place riot in Manhattan. The popular American actor Edwin Forrest, whose Macbeth was said to be like "the ferocious chief of a barbarous tribe" played the central role at the Broadway Theatre to popular acclaim, while the "cerebral and patrician" English actor Macready, playing the same role at the Astor Place Opera House, suffered constant heckling. The existing enmity between the two men (Forrest had openly hissed Macready at a recent performance of "Hamlet" in Britain) was taken up by Forrest's supporters – formed from the working class and lower middle class and anti-British agitators, keen to attack the upper-class pro-British patrons of the Opera House and the colonially-minded Macready. Nevertheless, Macready performed the role again three days later to a packed house while an angry mob gathered outside. The militia tasked with controlling the situation fired into the mob. In total, 31 rioters were killed and over 100 injured.

Charlotte Cushman is unique among nineteenth century interpreters of Shakespeare in achieving stardom in roles of both genders. Her New York debut was as Lady Macbeth in 1836, and she would later be admired in London in the same role in the mid-1840s. Helen Faucit was considered the embodiment of early-Victorian notions of femininity. But for this reason she largely failed when she eventually played Lady Macbeth in 1864: her serious attempt to embody the coarser aspects of Lady Macbeth's character jarred harshly with her public image. Adelaide Ristori, the great Italian actress, brought her Lady Macbeth to London in 1863 in Italian, and again in 1873 in an English translation cut in such a way as to be, in effect, Lady Macbeth's tragedy.

Henry Irving was the most successful of the late-Victorian actor-managers, but his "Macbeth" failed to curry favour with audiences. His desire for psychological credibility reduced certain aspects of the role: He described Macbeth as a brave soldier but a moral coward, and played him untroubled by conscience – clearly already contemplating the murder of Duncan before his encounter with the witches. Irving's leading lady was Ellen Terry, but her Lady Macbeth was unsuccessful with the public, for whom a century of performances influenced by Sarah Siddons had created expectations at odds with Terry's conception of the role.

Late nineteenth-century European Macbeths aimed for heroic stature, but at the expense of subtlety: Tommaso Salvini in Italy and Adalbert Matkowsky in Germany were said to inspire awe, but elicited little pity.

Two developments changed the nature of "Macbeth" performance in the 20th century: first, developments in the craft of acting itself, especially the ideas of Stanislavski and Brecht; and second, the rise of the dictator as a political icon. The latter has not always assisted the performance: it is difficult to sympathise with a Macbeth based on Hitler, Stalin, or Idi Amin.

Barry Jackson, at the Birmingham Repertory Theatre in 1923, was the first of the 20th-century directors to costume "Macbeth" in modern dress.

In 1936, a decade before his film adaptation of the play, Orson Welles directed "Macbeth" for the Negro Theatre Unit of the Federal Theatre Project at the Lafayette Theatre in Harlem, using black actors and setting the action in Haiti: with drums and Voodoo rituals to establish the Witches scenes. The production, dubbed "The Voodoo Macbeth", proved inflammatory in the aftermath of the Harlem riots, accused of making fun of black culture and as "a campaign to burlesque negroes" until Welles persuaded crowds that his use of black actors and voodoo made important cultural statements.
A performance which is frequently referenced as an example of the play's curse was the outdoor production directed by Burgess Meredith in 1953 in the British colony of Bermuda, starring Charlton Heston. Using the imposing spectacle of Fort St. Catherine as a key element of the set, the production was plagued by a host of mishaps, including Charlton Heston being burned when his tights caught fire.

The critical consensus is that there have been three great Macbeths on the English-speaking stage in the 20th century, all of them commencing at Stratford-upon-Avon: Laurence Olivier in 1955, Ian McKellen in 1976 and Antony Sher in 1999. Olivier's portrayal (directed by Glen Byam Shaw, with Vivien Leigh as Lady Macbeth) was immediately hailed as a masterpiece. Kenneth Tynan expressed the view that it succeeded because Olivier built the role to a climax at the end of the play, whereas most actors spend all they have in the first two acts.

The play caused grave difficulties for the Royal Shakespeare Company, especially at the (then) Shakespeare Memorial Theatre. Peter Hall's 1967 production was (in Michael Billington's words) "an acknowledged disaster" with the use of real leaves from Birnham Wood getting unsolicited first-night laughs, and Trevor Nunn's 1974 production was (Billington again) "an over-elaborate religious spectacle".

But Nunn achieved success for the RSC in his 1976 production at the intimate Other Place, with Ian McKellen and Judi Dench in the central roles. A small cast worked within a simple circle, and McKellen's Macbeth had nothing noble or likeable about him, being a manipulator in a world of manipulative characters. They were a young couple, physically passionate, "not monsters but recognisable human beings", but their relationship atrophied as the action progressed.

The RSC again achieved critical success in Gregory Doran's 1999 production at The Swan, with Antony Sher and Harriet Walter in the central roles, once again demonstrating the suitability of the play for smaller venues. Doran's witches spoke their lines to a theatre in absolute darkness, and the opening visual image was the entrance of Macbeth and Banquo in the berets and fatigues of modern warfare, carried on the shoulders of triumphant troops. In contrast to Nunn, Doran presented a world in which king Duncan and his soldiers were ultimately benign and honest, heightening the deviance of Macbeth (who seems genuinely surprised by the witches' prophesies) and Lady Macbeth in plotting to kill the king. The play said little about politics, instead powerfully presenting its central characters' psychological collapse.

"Macbeth" returned to the RSC in 2018, when Christopher Eccleston played the title role, with Niamh Cusack as his wife, Lady Macbeth. The play later transferred to the Barbican in London.

In Soviet-controlled Prague in 1977, faced with the illegality of working in theatres, Pavel Kohout adapted "Macbeth" into a 75-minute abridgement for five actors, suitable for "bringing a show in a suitcase to people's homes".

Spectacle was unfashionable in Western theatre throughout the 20th century. In East Asia, however, spectacular productions have achieved great success, including Yukio Ninagawa's 1980 production with Masane Tsukayama as Macbeth, set in the 16th century Japanese Civil War. The same director's tour of London in 1987 was widely praised by critics, even though (like most of their audience) they were unable to understand the significance of Macbeth's gestures, the huge Buddhist altar dominating the set, or the petals falling from the cherry trees.

Xu Xiaozhong's 1980 Central Academy of Drama production in Beijing made every effort to be unpolitical (necessary in the aftermath of the Cultural Revolution): yet audiences still perceived correspondences between the central character (whom the director had actually modelled on Louis Napoleon) and Mao Zedong. Shakespeare has often been adapted to indigenous theatre traditions, for example the "Kunju Macbeth" of Huang Zuolin performed at the inaugural Chinese Shakespeare Festival of 1986. Similarly, B. V. Karanth's "Barnam Vana" of 1979 had adapted "Macbeth" to the Yakshagana tradition of Karnataka, India. In 1997, Lokendra Arambam created "Stage of Blood", merging a range of martial arts, dance and gymnastic styles from Manipur, performed in Imphal and in England. The stage was literally a raft on a lake.

"Throne of Blood" (蜘蛛巣城 Kumonosu-jō, "Spider Web Castle") is a 1957 Japanese samurai film co-written and directed by Akira Kurosawa. The film transposes Macbeth from Medieval Scotland to feudal Japan, with stylistic elements drawn from Noh drama. Kurosawa was a fan of the play and planned his own adaptation for several years, postponing it after learning of Orson Welles' Macbeth (1948). The film won two Mainichi Film Awards.

The play has been translated and performed in various languages in different parts of the world, and "Media Artists" was the first to stage its Punjabi adaptation in India. The adaptation by Balram and the play directed by Samuel John have been universally acknowledged as a milestone in Punjabi theatre. The unique attempt involved trained theatre experts and the actors taken from a rural background in Punjab. Punjabi folk music imbued the play with the native ethos as the Scottish setting of Shakespeare's play was transposed into a Punjabi milieu.

In September, 2018, "Macbeth" was faithfully adapted into a fully illustrated Manga edition, by Manga Classics, an imprint of UDON Entertainment.


All references to "Macbeth", unless otherwise specified, are taken from the Arden Shakespeare, second series edition edited by Kenneth Muir. Under their referencing system, III.I.55 means act 3, scene 1, line 55. All references to other Shakespeare plays are to The Oxford Shakespeare "Complete Works of Shakespeare" edited by Stanley Wells and Gary Taylor.



</doc>
<doc id="18870" url="https://en.wikipedia.org/wiki?curid=18870" title="Minor Threat">
Minor Threat

Minor Threat was an American hardcore punk band, formed in 1980 in Washington, D.C. by vocalist Ian MacKaye and drummer Jeff Nelson. MacKaye and Nelson had played in several other bands together, and recruited bassist Brian Baker and guitarist Lyle Preslar to form Minor Threat. They added a fifth member, Steve Hansgen, in 1982, playing bass, while Baker switched to second guitar. 

The band was relatively short-lived, disbanding after only four years together, but had a strong influence on the punk scene, both stylistically and in establishing a "do it yourself" ethic for music distribution and concert promotion. Minor Threat's song "Straight Edge" became the eventual basis of the straight edge movement, which emphasized a lifestyle without alcohol or other drugs, or promiscuous sex. AllMusic described Minor Threat's music as "iconic" and noted that their groundbreaking music "has held up better than [that of] most of their contemporaries."

Along with the fellow Washington, D.C. hardcore band Bad Brains and California band Black Flag, Minor Threat set the standard for many hardcore punk bands in the 1980s and 1990s. All of Minor Threat's recordings were released on MacKaye's and Nelson's own label, Dischord Records. The "Minor Threat" EP and their only full-length studio album "Out of Step" have received a number of accolades and are cited as landmarks of the hardcore punk genre.

Prior to forming Minor Threat in 1980, vocalist Ian MacKaye and drummer Jeff Nelson had played bass and drums respectively in the Teen Idles while attending Wilson High School. During their two-year career within the flourishing Washington D.C. hardcore punk scene, the Teen Idles had gained a following of around one hundred fans (a sizable amount at the time), and were seen as only second within the scene to the contemporary Bad Brains. MacKaye and Nelson were strong believers in the DIY mentality and an independent, underground music scene. After the breakup of the Teen Idles, they used the money earned through the band to create Dischord Records, an independent record label that would host the releases of the Teen Idles, Minor Threat, and numerous other D.C. punk bands.

Eager to start a new band after the Teen Idles, MacKaye and Nelson recruited guitarist Lyle Preslar and bassist Brian Baker. They played their first performance in December 1980 to fifty people in a basement, opening for Bad Brains, The Untouchables, Black Market Baby and S.O.A., all D.C. bands.

The band's first 7" EPs, "Minor Threat" and "In My Eyes", were released in 1981. The group became popular regionally and toured the east coast and Midwest.

"Straight Edge," a song from the band's first EP, helped to inspire the straight edge movement. The lyrics of the song relay MacKaye's first-person perspective of his personal choice of abstinence from alcohol and other drugs, a novel ideology for rock musicians which initially found a small but dedicated following. Other prominent groups that subsequently advocated the straight edge stance include SS Decontrol and 7 Seconds. Although the original song was not written as a manifesto or a "set of rules," many later bands inspired by this idea used it as such, and over the years since its release, the song and the term "straight edge" became the zeitgeist for an entire subculture, and indeed the basis for a paradigm shift that has persisted and grown consistently throughout the world. The term comes as the point of the story -- he doesn't want to do drugs or drink, so therefore the writer has an edge over those who do -- a straight edge.

"Out of Step", A Minor Threat song from their second EP, further demonstrates the said belief: "Don't smoke/Don't drink/Don't fuck/At least I can fucking think/I can't keep up/I'm out of step with the world." The "I" in the lyrics was usually only implied, mainly because it did not quite fit the rhythm of the song. Some of the other members of Minor Threat, Jeff Nelson in particular, took exception to what they saw as MacKaye's imperious attitude on the song. The song was later re-recorded, and the updated version of the song on the 1983 album "Out of Step", which is slower so the first-person use of "I" would be clearer, included a bridge where MacKaye explains his personal beliefs, explaining that his ideals, which at the time were not yet known as what became a collective philosophy, or in fact, known as "straight edge," "is not a set of rules; I'm not telling you what to do. All I'm saying is there are three things, that are like so important to the whole world that I don't happen to find much importance in, whether it's fucking, or whether it's playing golf, because of that, I feel... I can't keep up... (full chorus)". 

Minor Threat's song "Guilty of Being White" led to some accusations of racism, but MacKaye has strongly denied such intentions and said that some listeners misinterpreted his words. He claims that his experiences attending Wilson High School, whose student population was 70 percent black, inspired the song. There, many students bullied MacKaye and his friends. Thrash metal band Slayer later covered the song, with the last iteration of the lyric "guilty of being white" changed to "guilty of being right." In an interview, MacKaye stated that he was offended that some perceived racist overtones in the lyrics, saying, "To me, at the time and now, it seemed clear it's an anti-racist song. Of course, it didn't occur to me at the time I wrote it that anybody outside of my twenty or thirty friends who I was singing to would ever have to actually ponder the lyrics or even consider them."

In the time between the release of the band's second seven-inch EP and the "Out of Step" record, the band briefly split when guitarist Lyle Preslar moved to Illinois to attend college for a semester at Northwestern University. Preslar was a member of Big Black for a few tempestuous rehearsals. During that period, MacKaye and Nelson put together a studio-only project called Skewbald/Grand Union; in a reflection of the slowly increasing disagreements between the two musicians, they were unable to decide on one name. The group recorded three untitled songs, which would be released posthumously as Dischord's 50th release. During Minor Threat's inactive period, Brian Baker also briefly played guitar for Government Issue and appeared on the "Make an Effort" EP.

In March 1982, at the urging of Bad Brains' H.R., Preslar left college to reform Minor Threat. The reunited band featured an expanded lineup: Steve Hansgen joined as the band's bassist and Baker switched to second guitar.

When "Out of Step" was rerecorded for the LP "Out of Step", MacKaye inserted a spoken section explaining, "This is not a set of rules..." An ideological door had already been opened, however, and by 1982, some straight-edge punks, such as followers of the band SS Decontrol, were swatting beers out of people's hands at clubs.

Minor Threat broke up in 1983. A contributing factor was disagreement over musical direction. MacKaye was allegedly skipping rehearsal sessions towards the end of the band's career, and he wrote the lyrics to the songs on the "Salad Days" EP in the studio. That was quite a contrast with the earlier recordings, as he had written and co-written the music for much of the band's early material. Minor Threat, which had returned to being a four-piece group with the departure of Hansgen, played its final show on September 23, 1983, at the Lansburgh Cultural Center in Washington, D.C., sharing the bill with go-go band Trouble Funk, and Austin, Texas punk funk act the Big Boys. In a meaningful way, Minor Threat ended their final set with "Last Song", a tune whose name was also the original title of the band's song "Salad Days".

Following the breakup, MacKaye stated that he did not "check out" on hardcore, but in fact hardcore "checked out." Explaining this, he stated that at a 1984 Minutemen show, a fan struck MacKaye's younger brother Alec in the face, and he punched the fan back, then realizing that the violence was "stupid," and that he saw his role in the stupidity. MacKaye claimed that immediately after this he decided to leave the hardcore scene.

In March 1984, six months after the band broke up, the EPs "Minor Threat" and "In My Eyes" were compiled together and re-released as the "Minor Threat" album.

MacKaye went on to found Embrace with former members of the Faith, Egg Hunt with Jeff Nelson, and later Fugazi and the Evens, as well as collaborating on Pailhead.

Baker went on to play in Junkyard, the Meatmen, Dag Nasty and Government Issue. He currently plays in Bad Religion.

Preslar was briefly a member of Glenn Danzig's Samhain, and his playing appears on a few songs on the band's first record. He joined The Meatmen in 1984, along with fellow Minor Threat member Brian Baker. He later ran Caroline Records, signing and working with (among others) Peter Gabriel, Ben Folds, Chemical Brothers, and Idaho, and ran marketing for Sire Records. He graduated from Rutgers University School of Law and lives in New Jersey.

Nelson played less-frantic alternative rock with Three and The High-Back Chairs before retiring from live performance. He runs his own label, Adult Swim Records, distributed by Dischord, and is a graphic artist and a political activist in Toledo, Ohio. The band's own Dischord Records released material by many bands from the Washington, D.C., area, such as Government Issue, Void, Scream, Fugazi, Artificial Peace, Rites of Spring, Gray Matter, and Dag Nasty, and has become a respected independent record label.

Hansgen formed Second Wind with Rich Moore, a former Minor Threat roadie and drummer for the Untouchables. He also worked with Tool in 1992 on the production of their first EP, "Opiate".

In 2005, a mock-up of the cover of Minor Threat's first EP (also used on the "Minor Threat" LP and "Complete Discography" CD) was copied by athletic footwear manufacturer Nike for use on a promotional poster for a skateboarding tour called "Major Threat". Nike also altered Minor Threat's logo (designed by Jeff Nelson) for the same campaign, as well as featuring Nike shoes in the new picture, rather than the combat boots worn by Ian MacKaye's younger brother Alec on the original.

MacKaye issued a press statement condemning Nike's actions and said that he would discuss legal options with the other members of the band. Meanwhile, fans, at the encouragement of Dischord, organized a letter-writing campaign protesting Nike's infringement. On June 27, 2005, Nike issued a statement apologizing to Minor Threat, Dischord Records, and their fans for the "Major Threat" campaign and said that all promotional artwork (print and digital) that they could acquire were destroyed.

On October 29, 2005, Fox played the first few seconds of Minor Threat's "Salad Days" during an NFL broadcast. Use of the song was not cleared by Dischord Records or any of the members of Minor Threat. Fox claimed that the clip was too short to have violated any copyrights.

In 2007, Brooklyn-based company Wheelhouse Pickles marketed a pepper sauce named "Minor Threat Sauce". Requesting only that the original label design (which was based on the "Bottled Violence" artwork) be amended, Ian MacKaye gave the product his endorsement. A small mention of this was made in music magazine "Revolver", where MacKaye commented "I don't really like hot sauce but I like the Minor Threat stuff".

In 2013, Minor Threat shirts began appearing in Urban Outfitters stores. Ian MacKaye confirmed that the shirts were officially licensed. Having spent what he described as "a complete waste of time" trying to track down bootlegged Minor Threat merchandise, MacKaye and Dischord made arrangements with a merchandise company in California to manage licensing of the band's shirts, as well as working to ensure that bootleg manufacturers of the shirts were curtailed. In comments that appeared in "Rolling Stone", MacKaye called it "absurd" for the shirts to be sold for $28 but concluded that "my time is better spent doing other things" than dealing with shirts. Dischord had previously taken action against Forever 21 in 2009 for marketing unlicensed Minor Threat shirts.








</doc>
<doc id="18875" url="https://en.wikipedia.org/wiki?curid=18875" title="Mental event">
Mental event

A mental event is anything which happens within the mind or mind substitute of a conscious individual. Examples include thoughts, feelings, decisions, dreams, and realizations.

Some believe that mental events are not limited to human thought but can be associated with animals and artificial intelligence as well. Whether mental events are identical to complex physical events, or whether such an identity even makes sense, is central to the mind-body problem.

Some state that the mental and the physical are the very same property which cause any event(s). This view is known as substance monism. An opposing view is substance dualism, which claims that the mental and physical are fundamentally different and can exist independently. A third approach is Donald Davidson's "anomalous monism".

Physicalism, a form of substance monism, states that everything that exists is either physical or depends on that which is physical. The existence of mental events has been used by philosophers as an argument against physicalism. For example, in his 1974 paper "What Is it Like to Be a Bat?", Thomas Nagel argues that physicalist theories of mind cannot explain an organism’s subjective experience because they cannot account for its mental events.





</doc>
<doc id="18878" url="https://en.wikipedia.org/wiki?curid=18878" title="Monopoly">
Monopoly

A monopoly (from Greek and ) exists when a specific person or enterprise is the only supplier of a particular commodity. This contrasts with a monopsony which relates to a single entity's control of a market to purchase a good or service, and with oligopoly which consists of a few sellers dominating a market. Monopolies are thus characterized by a lack of economic competition to produce the good or service, a lack of viable substitute goods, and the possibility of a high monopoly price well above the seller's marginal cost that leads to a high monopoly profit. The verb "monopolise" or "monopolize" refers to the "process" by which a company gains the ability to raise prices or exclude competitors. In economics, a monopoly is a single seller. In law, a monopoly is a business entity that has significant market power, that is, the power to charge overly high prices. Although monopolies may be big businesses, size is not a characteristic of a monopoly. A small business may still have the power to raise prices in a small industry (or market).

A monopoly is distinguished from a monopsony, in which there is only one "buyer" of a product or service; a monopoly may also have monopsony control of a sector of a market. Likewise, a monopoly should be distinguished from a cartel (a form of oligopoly), in which several providers act together to coordinate services, prices or sale of goods. Monopolies, monopsonies and oligopolies are all situations in which one or a few entities have market power and therefore interact with their customers (monopoly or oligopoly), or suppliers (monopsony) in ways that distort the market.

Monopolies can be established by a government, form naturally, or form by integration. In many jurisdictions, competition laws restrict monopolies due to government concerns over potential adverse effects. Holding a dominant position or a monopoly in a market is often not illegal in itself, however certain categories of behavior can be considered abusive and therefore incur legal sanctions when business is dominant. A government-granted monopoly or "legal monopoly", by contrast, is sanctioned by the state, often to provide an incentive to invest in a risky venture or enrich a domestic interest group. Patents, copyrights, and trademarks are sometimes used as examples of government-granted monopolies. The government may also reserve the venture for itself, thus forming a government monopoly, for example with a state-owned company.

Monopolies may be naturally occurring due to limited competition because the industry is resource intensive and requires substantial costs to operate (e.g., certain railroad systems).

In economics, the idea of monopoly is important in the study of management structures, which directly concerns normative aspects of economic competition, and provides the basis for topics such as industrial organization and economics of regulation. There are four basic types of market structures in traditional economic analysis: perfect competition, monopolistic competition, oligopoly and monopoly. A monopoly is a structure in which a single supplier produces and sells a given product or service. If there is a single seller in a certain market and there are no close substitutes for the product, then the market structure is that of a "pure monopoly". Sometimes, there are many sellers in an industry and/or there exist many close substitutes for the goods being produced, but nevertheless companies retain some market power. This is termed monopolistic competition, whereas in oligopoly the companies interact strategically.

In general, the main results from this theory compares the price-fixing methods across market structures, analyze the effect of a certain structure on welfare, and vary technological/demand assumptions in order to assess the consequences for an abstract model of society. Most economic textbooks follow the practice of carefully explaining the "perfect competition" model, mainly because this helps to understand "departures" from it (the so-called "imperfect competition" models).

The boundaries of what constitutes a market and what does not are relevant distinctions to make in economic analysis. In a general equilibrium context, a good is a specific concept including geographical and time-related characteristics. Most studies of market structure relax a little their definition of a good, allowing for more flexibility in the identification of substitute goods.

A monopoly has these characteristics:

Monopolies derive their market power from barriers to entry – circumstances that prevent or greatly impede a potential competitor's ability to compete in a market. There are three major types of barriers to entry: economic, legal and deliberate.
In addition to barriers to entry and competition, barriers to exit may be a source of market power. Barriers to exit are market conditions that make it difficult or expensive for a company to end its involvement with a market. High liquidation costs are a primary barrier to exiting. Market exit and shutdown are sometimes separate events. The decision whether to shut down or operate is not affected by exit barriers. A company will shut down if price falls below minimum average variable costs.

While monopoly and perfect competition mark the extremes of market structures there is some similarity. The cost functions are the same. Both monopolies and perfectly competitive (PC) companies minimize cost and maximize profit. The shutdown decisions are the same. Both are assumed to have perfectly competitive factors markets. There are distinctions, some of the most important distinctions are as follows:

The most significant distinction between a PC company and a monopoly is that the monopoly has a downward-sloping demand curve rather than the "perceived" perfectly elastic curve of the PC company. Practically all the variations mentioned above relate to this fact. If there is a downward-sloping demand curve then by necessity there is a distinct marginal revenue curve. The implications of this fact are best made manifest with a linear demand curve. Assume that the inverse demand curve is of the form x = a − by. Then the total revenue curve is TR = ay − by and the marginal revenue curve is thus MR = a − 2by. From this several things are evident. First the marginal revenue curve has the same y intercept as the inverse demand curve. Second the slope of the marginal revenue curve is twice that of the inverse demand curve. Third the x intercept of the marginal revenue curve is half that of the inverse demand curve. What is not quite so evident is that the marginal revenue curve is below the inverse demand curve at all points. Since all companies maximise profits by equating MR and MC it must be the case that at the profit-maximizing quantity MR and MC are less than price, which further implies that a monopoly produces less quantity at a higher price than if the market were perfectly competitive.

The fact that a monopoly has a downward-sloping demand curve means that the relationship between total revenue and output for a monopoly is much different than that of competitive companies. Total revenue equals price times quantity. A competitive company has a perfectly elastic demand curve meaning that total revenue is proportional to output. Thus the total revenue curve for a competitive company is a ray with a slope equal to the market price. A competitive company can sell all the output it desires at the market price. For a monopoly to increase sales it must reduce price. Thus the total revenue curve for a monopoly is a parabola that begins at the origin and reaches a maximum value then continuously decreases until total revenue is again zero. Total revenue has its maximum value when the slope of the total revenue function is zero. The slope of the total revenue function is marginal revenue. So the revenue maximizing quantity and price occur when MR = 0. For example, assume that the monopoly's demand function is P = 50 − 2Q. The total revenue function would be TR = 50Q − 2Q and marginal revenue would be 50 − 4Q. Setting marginal revenue equal to zero we have

So the revenue maximizing quantity for the monopoly is 12.5 units and the revenue maximizing price is 25.

A company with a monopoly does not experience price pressure from competitors, although it may experience pricing pressure from potential competition. If a company increases prices too much, then others may enter the market if they are able to provide the same good, or a substitute, at a lesser price. The idea that monopolies in markets with easy entry need not be regulated against is known as the "revolution in monopoly theory".

A monopolist can extract only one premium, and getting into complementary markets does not pay. That is, the total profits a monopolist could earn if it sought to leverage its monopoly in one market by monopolizing a complementary market are equal to the extra profits it could earn anyway by charging more for the monopoly product itself. However, the one monopoly profit theorem is not true if customers in the monopoly good are stranded or poorly informed, or if the tied good has high fixed costs.

A pure monopoly has the same economic rationality of perfectly competitive companies, i.e. to optimise a profit function given some constraints. By the assumptions of increasing marginal costs, exogenous inputs' prices, and control concentrated on a single agent or entrepreneur, the optimal decision is to equate the marginal cost and marginal revenue of production. Nonetheless, a pure monopoly can – unlike a competitive company – alter the market price for its own convenience: a decrease of production results in a higher price. In the economics' jargon, it is said that pure monopolies have "a downward-sloping demand". An important consequence of such behaviour is worth noticing: typically a monopoly selects a higher price and lesser quantity of output than a price-taking company; again, less is available at a higher price.

A monopoly chooses that price that maximizes the difference between total revenue and total cost. The basic markup rule (as measured by the Lerner index) can be expressed as
formula_4,
where formula_5 is the price elasticity of demand the firm faces. The markup rules indicate that the ratio between profit margin and the price is inversely proportional to the price elasticity of demand. The implication of the rule is that the more elastic the demand for the product the less pricing power the monopoly has.

Market power is the ability to increase the product's price above marginal cost without losing all customers. Perfectly competitive (PC) companies have zero market power when it comes to setting prices. All companies of a PC market are price takers. The price is set by the interaction of demand and supply at the market or aggregate level. Individual companies simply take the price determined by the market and produce that quantity of output that maximizes the company's profits. If a PC company attempted to increase prices above the market level all its customers would abandon the company and purchase at the market price from other companies. A monopoly has considerable although not unlimited market power. A monopoly has the power to set prices or quantities although not both. A monopoly is a price maker. The monopoly is the market and prices are set by the monopolist based on their circumstances and not the interaction of demand and supply. The two primary factors determining monopoly market power are the company's demand curve and its cost structure.

Market power is the ability to affect the terms and conditions of exchange so that the price of a product is set by a single company (price is not imposed by the market as in perfect competition). Although a monopoly's market power is great it is still limited by the demand side of the market. A monopoly has a negatively sloped demand curve, not a perfectly inelastic curve. Consequently, any price increase will result in the loss of some customers.

Price discrimination allows a monopolist to increase its profit by charging higher prices for identical goods to those who are willing or able to pay more. For example, most economic textbooks cost more in the United States than in developing countries like Ethiopia. In this case, the publisher is using its government-granted copyright monopoly to price discriminate between the generally wealthier American economics students and the generally poorer Ethiopian economics students. Similarly, most patented medications cost more in the U.S. than in other countries with a (presumed) poorer customer base. Typically, a high general price is listed, and various market segments get varying discounts. This is an example of framing to make the process of charging some people higher prices more socially acceptable. Perfect price discrimination would allow the monopolist to charge each customer the exact maximum amount they would be willing to pay. This would allow the monopolist to extract all the consumer surplus of the market. While such perfect price discrimination is a theoretical construct, advances in information technology and micromarketing may bring it closer to the realm of possibility.

It is very important to realize that partial price discrimination can cause some customers who are inappropriately pooled with high price customers to be excluded from the market. For example, a poor student in the U.S. might be excluded from purchasing an economics textbook at the U.S. price, which the student may have been able to purchase at the Ethiopian price'. Similarly, a wealthy student in Ethiopia may be able to or willing to buy at the U.S. price, though naturally would hide such a fact from the monopolist so as to pay the reduced third world price. These are deadweight losses and decrease a monopolist's profits. As such, monopolists have substantial economic interest in improving their market information and "market segmenting".

There is important information for one to remember when considering the monopoly model diagram (and its associated conclusions) displayed here. The result that monopoly prices are higher, and production output lesser, than a competitive company follow from a requirement that the monopoly not charge different prices for different customers. That is, the monopoly is restricted from engaging in price discrimination (this is termed first degree price discrimination, such that all customers are charged the same amount). If the monopoly were permitted to charge individualised prices (this is termed third degree price discrimination), the quantity produced, and the price charged to the "marginal" customer, would be identical to that of a competitive company, thus eliminating the deadweight loss; however, all gains from trade (social welfare) would accrue to the monopolist and none to the consumer. In essence, every consumer would be indifferent between (1) going completely without the product or service and (2) being able to purchase it from the monopolist.

As long as the price elasticity of demand for most customers is less than one in absolute value, it is advantageous for a company to increase its prices: it receives more money for fewer goods. With a price increase, price elasticity tends to increase, and in the optimum case above it will be greater than one for most customers.

A company maximizes profit by selling where marginal revenue equals marginal cost. A company that does not engage in price discrimination will charge the profit maximizing price, P*, to all its customers. In such circumstances there are customers who would be willing to pay a higher price than P* and those who will not pay P* but would buy at a lower price. A price discrimination strategy is to charge less price sensitive buyers a higher price and the more price sensitive buyers a lower price. Thus additional revenue is generated from two sources. The basic problem is to identify customers by their willingness to pay.

The purpose of price discrimination is to transfer consumer surplus to the producer. Consumer surplus is the difference between the value of a good to a consumer and the price the consumer must pay in the market to purchase it. Price discrimination is not limited to monopolies.

Market power is a company's ability to increase prices without losing all its customers. Any company that has market power can engage in price discrimination. Perfect competition is the only market form in which price discrimination would be impossible (a perfectly competitive company has a perfectly elastic demand curve and has zero market power).

There are three forms of price discrimination. First degree price discrimination charges each consumer the maximum price the consumer is willing to pay. Second degree price discrimination involves quantity discounts. Third degree price discrimination involves grouping consumers according to willingness to pay as measured by their price elasticities of demand and charging each group a different price. Third degree price discrimination is the most prevalent type.

There are three conditions that must be present for a company to engage in successful price discrimination. First, the company must have market power. Second, the company must be able to sort customers according to their willingness to pay for the good. Third, the firm must be able to prevent resell.

A company must have some degree of market power to practice price discrimination. Without market power a company cannot charge more than the market price. Any market structure characterized by a downward sloping demand curve has market power – monopoly, monopolistic competition and oligopoly. The only market structure that has no market power is perfect competition.

A company wishing to practice price discrimination must be able to prevent middlemen or brokers from acquiring the consumer surplus for themselves. The company accomplishes this by preventing or limiting resale. Many methods are used to prevent resale. For instance, persons are required to show photographic identification and a boarding pass before boarding an airplane. Most travelers assume that this practice is strictly a matter of security. However, a primary purpose in requesting photographic identification is to confirm that the ticket purchaser is the person about to board the airplane and not someone who has repurchased the ticket from a discount buyer.

The inability to prevent resale is the largest obstacle to successful price discrimination. Companies have however developed numerous methods to prevent resale. For example, universities require that students show identification before entering sporting events. Governments may make it illegal to resale tickets or products. In Boston, Red Sox baseball tickets can only be resold legally to the team.

The three basic forms of price discrimination are first, second and third degree price discrimination. In "first degree price discrimination" the company charges the maximum price each customer is willing to pay. The maximum price a consumer is willing to pay for a unit of the good is the reservation price. Thus for each unit the seller tries to set the price equal to the consumer's reservation price. Direct information about a consumer's willingness to pay is rarely available. Sellers tend to rely on secondary information such as where a person lives (postal codes); for example, catalog retailers can use mail high-priced catalogs to high-income postal codes. First degree price discrimination most frequently occurs in regard to professional services or in transactions involving direct buyer/seller negotiations. For example, an accountant who has prepared a consumer's tax return has information that can be used to charge customers based on an estimate of their ability to pay.

In "second degree price discrimination" or quantity discrimination customers are charged different prices based on how much they buy. There is a single price schedule for all consumers but the prices vary depending on the quantity of the good bought. The theory of second degree price discrimination is a consumer is willing to buy only a certain quantity of a good at a given price. Companies know that consumer's willingness to buy decreases as more units are purchased. The task for the seller is to identify these price points and to reduce the price once one is reached in the hope that a reduced price will trigger additional purchases from the consumer. For example, sell in unit blocks rather than individual units.

In "third degree price discrimination" or multi-market price discrimination the seller divides the consumers into different groups according to their willingness to pay as measured by their price elasticity of demand. Each group of consumers effectively becomes a separate market with its own demand curve and marginal revenue curve. The firm then attempts to maximize profits in each segment by equating MR and MC, Generally the company charges a higher price to the group with a more price inelastic demand and a relatively lesser price to the group with a more elastic demand. Examples of third degree price discrimination abound. Airlines charge higher prices to business travelers than to vacation travelers. The reasoning is that the demand curve for a vacation traveler is relatively elastic while the demand curve for a business traveler is relatively inelastic. Any determinant of price elasticity of demand can be used to segment markets. For example, seniors have a more elastic demand for movies than do young adults because they generally have more free time. Thus theaters will offer discount tickets to seniors.

Assume that by a uniform pricing system the monopolist would sell five units at a price of $10 per unit. Assume that his marginal cost is $5 per unit. Total revenue would be $50, total costs would be $25 and profits would be $25. If the monopolist practiced price discrimination he would sell the first unit for $50 the second unit for $40 and so on. Total revenue would be $150, his total cost would be $25 and his profit would be $125.00. Several things are worth noting. The monopolist acquires all the consumer surplus and eliminates practically all the deadweight loss because he is willing to sell to anyone who is willing to pay at least the marginal cost. Thus the price discrimination promotes efficiency. Secondly, by the pricing scheme price = average revenue and equals marginal revenue. That is the monopolist behaving like a perfectly competitive company. Thirdly, the discriminating monopolist produces a larger quantity than the monopolist operating by a uniform pricing scheme.
Successful price discrimination requires that companies separate consumers according to their willingness to buy. Determining a customer's willingness to buy a good is difficult. Asking consumers directly is fruitless: consumers don't know, and to the extent they do they are reluctant to share that information with marketers. The two main methods for determining willingness to buy are observation of personal characteristics and consumer actions. As noted information about where a person lives (postal codes), how the person dresses, what kind of car he or she drives, occupation, and income and spending patterns can be helpful in classifying.

According to the standard model, in which a monopolist sets a single price for all consumers, the monopolist will sell a lesser quantity of goods at a higher price than would companies by perfect competition. Because the monopolist ultimately forgoes transactions with consumers who value the product or service more than its price, monopoly pricing creates a deadweight loss referring to potential gains that went neither to the monopolist nor to consumers. Given the presence of this deadweight loss, the combined surplus (or wealth) for the monopolist and consumers is necessarily less than the total surplus obtained by consumers by perfect competition. Where efficiency is defined by the total gains from trade, the monopoly setting is less efficient than perfect competition.

It is often argued that monopolies tend to become less efficient and less innovative over time, becoming "complacent", because they do not have to be efficient or innovative to compete in the marketplace. Sometimes this very loss of psychological efficiency can increase a potential competitor's value enough to overcome market entry barriers, or provide incentive for research and investment into new alternatives. The theory of contestable markets argues that in some circumstances (private) monopolies are forced to behave "as if" there were competition because of the risk of losing their monopoly to new entrants. This is likely to happen when a market's barriers to entry are low. It might also be because of the availability in the longer term of substitutes in other markets. For example, a canal monopoly, while worth a great deal during the late 18th century United Kingdom, was worth much less during the late 19th century because of the introduction of railways as a substitute.

Contrary to common misconception, monopolists do not try to sell items for the highest possible price, nor do they try to maximize profit per unit, but rather they try to maximize total profit.

A natural monopoly is an organization that experiences increasing returns to scale over the relevant range of output and relatively high fixed costs. A natural monopoly occurs where the average cost of production "declines throughout the relevant range of product demand". The relevant range of product demand is where the average cost curve is below the demand curve. When this situation occurs, it is always cheaper for one large company to supply the market than multiple smaller companies; in fact, absent government intervention in such markets, will naturally evolve into a monopoly. An early market entrant that takes advantage of the cost structure and can expand rapidly can exclude smaller companies from entering and can drive or buy out other companies. A natural monopoly suffers from the same inefficiencies as any other monopoly. Left to its own devices, a profit-seeking natural monopoly will produce where marginal revenue equals marginal costs. Regulation of natural monopolies is problematic. Fragmenting such monopolies is by definition inefficient. The most frequently used methods dealing with natural monopolies are government regulations and public ownership. Government regulation generally consists of regulatory commissions charged with the principal duty of setting prices.

To reduce prices and increase output, regulators often use average cost pricing. By average cost pricing, the price and quantity are determined by the intersection of the average cost curve and the demand curve. This pricing scheme eliminates any positive economic profits since price equals average cost. Average-cost pricing is not perfect. Regulators must estimate average costs. Companies have a reduced incentive to lower costs. Regulation of this type has not been limited to natural monopolies. Average-cost pricing does also have some disadvantages. By setting price equal to the intersection of the demand curve and the average total cost curve, the firm's output is allocatively inefficient as the price is less than the marginal cost (which is the output quantity for a perfectly competitive and allocatively efficient market).

A government-granted monopoly (also called a ""de jure" monopoly") is a form of "coercive monopoly", in which a government grants exclusive privilege to a private individual or company to be the sole provider of a commodity. Monopoly may be granted explicitly, as when potential competitors are excluded from the market by a specific law, or implicitly, such as when the requirements of an administrative regulation can only be fulfilled by a single market player, or through some other legal or procedural mechanism, such as patents, trademarks, and copyright.

A monopolist should shut down when price is less than average variable cost for every output level – in other words where the demand curve is entirely below the average variable cost curve. Under these circumstances at the profit maximum level of output (MR = MC) average revenue would be less than average variable costs and the monopolists would be better off shutting down in the short term.

In a free market, monopolies can be ended at any time by new competition, breakaway businesses, or consumers seeking alternatives. In a highly regulated market environment a government will often either regulate the monopoly, convert it into a publicly owned monopoly environment, or forcibly fragment it (see Antitrust law and trust busting). Public utilities, often being naturally efficient with only one operator and therefore less susceptible to efficient breakup, are often strongly regulated or publicly owned. American Telephone & Telegraph (AT&T) and Standard Oil are often cited as examples of the breakup of a private monopoly by government. Standard Oil never achieved monopoly status, a consequence of existing in a market open to competition for the duration of its existence. The Bell System, later AT&T, was protected from competition first by the Kingsbury Commitment, and later by a series of agreements between AT&T and the Federal Government. In 1984, decades after having been granted monopoly power by force of law, AT&T was broken up into various components, MCI, Sprint, who were able to compete effectively in the long distance phone market.

The law regulating dominance in the European Union is governed by Article 102 of the "Treaty on the Functioning of the European Union" which aims at enhancing the consumer's welfare and also the efficiency of allocation of resources by protecting competition on the downstream market. The existence of a very high market share does not always mean consumers are paying excessive prices since the threat of new entrants to the market can restrain a high-market-share company's price increases. Competition law does not make merely having a monopoly illegal, but rather abusing the power a monopoly may confer, for instance through exclusionary practices (i.e. pricing high just because you are the only one around.) It may also be noted that it is illegal to try to obtain a monopoly, by practices of buying out the competition, or equal practices. If one occurs naturally, such as a competitor going out of business, or lack of competition, it is not illegal until such time as the monopoly holder abuses the power.

First it is necessary to determine whether a company is dominant, or whether it behaves "to an appreciable extent independently of its competitors, customers and ultimately of its consumer". Establishing dominance is a two-stage test. The first thing to consider is market definition which is one of the crucial factors of the test. It includes relevant product market and relevant geographic market.

As the definition of the market is of a matter of interchangeability, if the goods or services are regarded as interchangeable then they are within the same product market. For example, in the case of "United Brands v Commission", it was argued in this case that bananas and other fresh fruit were in the same product market and later on dominance was found because the special features of the banana made it could only be interchangeable with other fresh fruits in a limited extent and other and is only exposed to their competition in a way that is hardly perceptible. The demand substitutability of the goods and services will help in defining the product market and it can be access by the ‘hypothetical monopolist’ test or the ‘SSNIP’ test .

It is necessary to define it because some goods can only be supplied within a narrow area due to technical, practical or legal reasons and this may help to indicate which undertakings impose a competitive constraint on the other undertakings in question. Since some goods are too expensive to transport where it might not be economic to sell them to distant markets in relation to their value, therefore the cost of transporting is a crucial factor here. Other factors might be legal controls which restricts an undertaking in a Member States from exporting goods or services to another.

Market definition may be difficult to measure but is important because if it is defined too broadly, the undertaking may be more likely to be found dominant and if it is defined too narrowly, the less likely that it will be found dominant.

As with collusive conduct, market shares are determined with reference to the particular market in which the company and product in question is sold. It does not in itself determine whether an undertaking is dominant but work as an indicator of the states of the existing competition within the market. The Herfindahl-Hirschman Index (HHI) is sometimes used to assess how competitive an industry is. It sums up the squares of the individual market shares of all of the competitors within the market. The lower the total, the less concentrated the market and the higher the total, the more concentrated the market. In the US, the merger guidelines state that a post-merger HHI below 1000 is viewed as not concentrated while HHIs above that will provoke further review.

By European Union law, very large market shares raise a presumption that a company is dominant, which may be rebuttable. A market share of 100% may be very rare but it is still possible to be found and in fact it has been identified in some cases, for instance the "AAMS v Commission" case. Undertakings possessing market share that is lower than 100% but over 90% had also been found dominant, for example, Microsoft v Commission case. In the AKZO v Commission case, the undertaking is presumed to be dominant if it has a market share of 50%. There are also findings of dominance that are below a market share of 50%, for instance, United Brands v Commission, it only possessed a market share of 40% to 45% and still to be found dominant with other factors. The lowest yet market share of a company considered "dominant" in the EU was 39.7%.If a company has a dominant position, then there is a special responsibility not to allow its conduct to impair competition on the common market however these will all falls away if it is not dominant.

When considering whether an undertaking is dominant, it involves a combination of factors. Each of them cannot be taken separately as if they are, they will not be as determinative as they are when they are combined together. Also, in cases where an undertaking has previously been found dominant, it is still necessary to redefine the market and make a whole new analysis of the conditions of competition based on the available evidence at the appropriate time.

According to the Guidance, there are three more issues that must be examined. They are actual competitors that relates to the market position of the dominant undertaking and its competitors, potential competitors that concerns the expansion and entry and lastly the countervailing buyer power.

Market share may be a valuable source of information regarding the market structure and the market position when it comes to accessing it. The dynamics of the market and the extent to which the goods and services differentiated are relevant in this area.

It concerns with the competition that would come from other undertakings which are not yet operating in the market but will enter it in the future. So, market shares may not be useful in accessing the competitive pressure that is exerted on an undertaking in this area. The potential entry by new firms and expansions by an undertaking must be taken into account, therefore the barriers to entry and barriers to expansion is an important factor here.

Competitive constraints may not always come from actual or potential competitors. Sometimes, it may also come from powerful customers who have sufficient bargaining strength which come from its size or its commercial significance for a dominant firm.

There are three main types of abuses which are exploitative abuse, exclusionary abuse and single market abuse.

It arises when a monopolist has such significant market power that it can restrict its output while increasing the price above the competitive level without losing customers. This type is less concerned by the Commission than other types.

This is most concerned about by the Commissions because it is capable of causing long- term consumer damage and is more likely to prevent the development of competition. An example of it is exclusive dealing agreements.

It arises when a dominant undertaking carrying out excess pricing which would not only have an exploitative effect but also prevent parallel imports and limits intra- brand competition.


Despite wide agreement that the above constitute abusive practices, there is some debate about whether there needs to be a causal connection between the dominant position of a company and its actual abusive conduct. Furthermore, there has been some consideration of what happens when a company merely attempts to abuse its dominant position.

The term "monopoly" first appears in Aristotle's "Politics". Aristotle describes Thales of Miletus's cornering of the market in olive presses as a monopoly ("μονοπώλιον"). Another early reference to the concept of “monopoly” in a commercial sense appears in tractate Demai of the Mishna (2nd century C.E.), regarding the purchasing of agricultural goods from a dealer who has a monopoly on the produce (chapter 5; 4). The meaning and understanding of the English word 'monopoly' has changed over the years.

Vending of common salt (sodium chloride) was historically a natural monopoly. Until recently, a combination of strong sunshine and low humidity or an extension of peat marshes was necessary for producing salt from the sea, the most plentiful source. Changing sea levels periodically caused salt "famines" and communities were forced to depend upon those who controlled the scarce inland mines and salt springs, which were often in hostile areas (e.g. the Sahara desert) requiring well-organised security for transport, storage, and distribution.

The Salt Commission was a legal monopoly in China. Formed in 758, the Commission controlled salt production and sales in order to raise tax revenue for the Tang Dynasty.

The "Gabelle" was a notoriously high tax levied upon salt in the Kingdom of France. The much-hated levy had a role in the beginning of the French Revolution, when strict legal controls specified who was allowed to sell and distribute salt. First instituted in 1286, the Gabelle was not permanently abolished until 1945.

Robin Gollan argues in "The Coalminers of New South Wales" that anti-competitive practices developed in the coal industry of Australia's Newcastle as a result of the business cycle. The monopoly was generated by formal meetings of the local management of coal companies agreeing to fix a minimum price for sale at dock. This collusion was known as "The Vend". The Vend ended and was reformed repeatedly during the late 19th century, ending by recession in the business cycle. "The Vend" was able to maintain its monopoly due to trade union assistance, and material advantages (primarily coal geography). During the early 20th century, as a result of comparable monopolistic practices in the Australian coastal shipping business, the Vend developed as an informal and illegal collusion between the steamship owners and the coal industry, eventually resulting in the High Court case Adelaide Steamship Co. Ltd v. R. & AG.

Standard Oil was an American oil producing, transporting, refining, and marketing company. Established in 1870, it became the largest oil refiner in the world. John D. Rockefeller was a founder, chairman and major shareholder. The company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard was an illegal monopoly. The Standard Oil trust was dissolved into 33 smaller companies; two of its surviving "child" companies are ExxonMobil and the Chevron Corporation.

U.S. Steel has been accused of being a monopoly. J. P. Morgan and Elbert H. Gary founded U.S. Steel in 1901 by combining Andrew Carnegie's Carnegie Steel Company with Gary's Federal Steel Company and William Henry "Judge" Moore's National Steel Company. At one time, U.S. Steel was the largest steel producer and largest corporation in the world. In its first full year of operation, U.S. Steel made 67 percent of all the steel produced in the United States. However, U.S. Steel's share of the expanding market slipped to 50 percent by 1911, and antitrust prosecution that year failed.

De Beers settled charges of price fixing in the diamond trade in the 2000s. De Beers is well known for its monopoloid practices throughout the 20th century, whereby it used its dominant position to manipulate the international diamond market. The company used several methods to exercise this control over the market. Firstly, it convinced independent producers to join its single channel monopoly, it flooded the market with diamonds similar to those of producers who refused to join the cartel, and lastly, it purchased and stockpiled diamonds produced by other manufacturers in order to control prices through limiting supply.

In 2000, the De Beers business model changed due to factors such as the decision by producers in Russia, Canada and Australia to distribute diamonds outside the De Beers channel, as well as rising awareness of blood diamonds that forced De Beers to "avoid the risk of bad publicity" by limiting sales to its own mined products. De Beers' market share by value fell from as high as 90% in the 1980s to less than 40% in 2012, having resulted in a more fragmented diamond market with more transparency and greater liquidity.

In November 2011 the Oppenheimer family announced its intention to sell the entirety of its 40% stake in De Beers to Anglo American plc thereby increasing Anglo American's ownership of the company to 85%.[30] The transaction was worth £3.2 billion ($5.1 billion) in cash and ended the Oppenheimer dynasty's 80-year ownership of De Beers.

A public utility (or simply "utility") is an organization or company that maintains the infrastructure for a public service or provides a set of services for public consumption. Common examples of utilities are electricity, natural gas, water, sewage, cable television, and telephone. In the United States, public utilities are often natural monopolies because the infrastructure required to produce and deliver a product such as electricity or water is very expensive to build and maintain.

Western Union was criticized as a "price gouging" monopoly in the late 19th century. American Telephone & Telegraph was a telecommunications giant. AT&T was broken up in 1984. In the case of Telecom New Zealand, local loop unbundling was enforced by central government.

Telkom is a semi-privatised, part state-owned South African telecommunications company. Deutsche Telekom is a former state monopoly, still partially state owned. Deutsche Telekom currently monopolizes high-speed VDSL broadband network. The Long Island Power Authority (LIPA) provided electric service to over 1.1 million customers in Nassau and Suffolk counties of New York, and the Rockaway Peninsula in Queens.

The Comcast Corporation is the largest mass media and communications company in the world by revenue. It is the largest cable company and home Internet service provider in the United States, and the nation's third largest home telephone service provider. Comcast has a monopoly in Boston, Philadelphia, and many other small towns across the US.

The United Aircraft and Transport Corporation was an aircraft manufacturer holding company that was forced to divest itself of airlines in 1934.

Iarnród Éireann, the Irish Railway authority, is a current monopoly as Ireland does not have the size for more companies.

The Long Island Rail Road (LIRR) was founded in 1834, and since the mid-1800s has provided train service between Long Island and New York City. In the 1870s, LIRR became the sole railroad in that area through a series of acquisitions and consolidations. In 2013, the LIRR's commuter rail system is the busiest commuter railroad in North America, serving nearly 335,000 passengers daily.

Dutch East India Company was created as a legal trading monopoly in 1602. The "Vereenigde Oost-Indische Compagnie" enjoyed huge profits from its spice monopoly through most of the 17th century.

The British East India Company was created as a legal trading monopoly in 1600. The East India Company was formed for pursuing trade with the East Indies but ended up trading mainly with the Indian subcontinent, North-West Frontier Province, and Balochistan. The Company traded in basic commodities, which included cotton, silk, indigo dye, salt, saltpetre, tea and opium.

Major League Baseball survived U.S. antitrust litigation in 1922, though its special status is still in dispute as of 2009.

The National Football League survived antitrust lawsuit in the 1960s but was convicted of being an illegal monopoly in the 1980s.


According to professor Milton Friedman, laws against monopolies cause more harm than good, but unnecessary monopolies should be countered by removing tariffs and other regulation that upholds monopolies.

However, professor Steve H. Hanke believes that although private monopolies are more efficient than public ones, often by a factor of two, sometimes private natural monopolies, such as local water distribution, should be regulated (not prohibited) by, e.g., price auctions.

Thomas DiLorenzo asserts, however, that during the early days of utility companies where there was little regulation, there were no natural monopolies and there was competition. Only when companies realized that they could gain power through government did monopolies begin to form.

Baten, Bianchi and Moser find historical evidence that monopolies which are protected by patent laws may have adverse effects on the creation of innovation in an economy. They argue that under certain circumstances, compulsory licensing – which allows governments to license patents without the consent of patent-owners – may be effective in promoting invention by increasing the threat of competition in fields with low pre-existing levels of competition.


</doc>
<doc id="18879" url="https://en.wikipedia.org/wiki?curid=18879" title="Massachusetts Institute of Technology">
Massachusetts Institute of Technology

Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts. The Institute is a land-grant, sea-grant, and space-grant university, with an urban campus that extends more than a mile (1.6 km) alongside the Charles River. The Institute also encompasses a number of major off-campus facilities such as the MIT Lincoln Laboratory, the Bates Center, and the Haystack Observatory, as well as affiliated laboratories such as the Broad and Whitehead Institutes. Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. It has since played a key role in the development of many aspects of modern science, engineering, mathematics, and technology, and is widely known for its innovation and academic strength, making it one of the most prestigious institutions of higher learning in the world.

, 96 Nobel laureates, 26 Turing Award winners, and 8 Fields Medalists have been affiliated with MIT as alumni, faculty members, or researchers. In addition, 58 National Medal of Science recipients, 29 National Medals of Technology and Innovation recipients, 50 MacArthur Fellows, 73 Marshall Scholars, 48 Rhodes Scholars, 41 astronauts, and 16 Chief Scientists of the US Air Force have been affiliated with MIT. The school also has a strong entrepreneurial culture, and the aggregated annual revenues of companies founded by MIT alumni ($1.9 trillion) would rank roughly as the tenth-largest economy in the world (2014). MIT is a member of the Association of American Universities (AAU).

In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed. A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by John Albion Andrew, the governor of Massachusetts, on April 10, 1861.

Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances. He did not wish to found a professional school, but a combination with elements of both professional and liberal education, proposing that:

The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.

The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.<ref name="Angulo https://archive.org/details/williambartonrog00angu/page/155 155–156"></ref>

Two days after MIT was chartered, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes" and was a land-grant school. In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.

MIT was informally called "Boston Tech". The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date. Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker. Programs in electrical, chemical, marine, and sanitary engineering were introduced, new buildings were built, and the size of the student body increased to more than one thousand.

The curriculum drifted to a vocational emphasis, with less focus on theoretical science. The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School. There would be at least six attempts to absorb MIT into Harvard. In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually, the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni. However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.
In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge "Bucentaur" built for the occasion, to signify MIT's move to a spacious new campus largely consisting of filled land on a tract along the Cambridge side of the Charles River. The neoclassical "New Technology" campus was designed by William W. Bosworth and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($ million in 2015 dollars) in cash and Kodak stock to MIT.

In 1931 Stanislav Shumovsky enrolled. He later communicated much technical information on aviation to the Soviet Union. Other MIT spies sent other secrets about American technology, eventually including atomic bomb secrets.

In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering". Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding. The school was elected to the Association of American Universities in 1934.

Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.

MIT's involvement in military science surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT. Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area. Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory; the development of a digital computer for flight simulations under Project Whirlwind; and high-speed and high-altitude photography under Harold Edgerton. By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), employing nearly 4000 in the Radiation Laboratory alone and receiving in excess of $100 million ($ billion in 2015 dollars) before 1946. Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.

These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.

In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research. In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles. The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the MIT Lincoln Laboratory facility in 1973 in response to the protests. The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil. However six MIT students were sentenced to prison terms at this time and some former student leaders, such as Michael Albert and George Katsiaficas, are still indignant about MIT's role in military research and its suppression of these protests. (Richard Leacock's film, "November Actions", records some of these tumultuous events.)

In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research. More recently, MIT's research for the military has included work on robots, drones and 'battle suits'.

MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies, students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like "Spacewar!" and created much of modern hacker slang and culture. Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology; the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee; the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002; and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.

MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs. Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center. Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.

In 2001, inspired by the open source and open access movements, MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabi, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. While the cost of supporting and hosting the project is high, OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee. The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content. In March 2009 the MIT faculty adopted an open-access policy to make its scholarship publicly accessible online.

MIT has its own police force. Three days after the Boston Marathon bombing of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects Dzhokhar and Tamerlan Tsarnaev, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".

In September 2017, the school announced the creation of an artificial intelligence research lab called the MIT-IBM Watson AI Lab. IBM will spend $240 million over the next decade, and the lab will be staffed by MIT and IBM scientists. In October 2018 MIT announced that in September 2019, it would open a new College of Computing dedicated to the study of artificial intelligence, to be named after lead donor and The Blackstone Group CEO Stephen Schwarzman. The focus of the new college is to study not just AI, but interdisciplinary AI education, and how AI can be used in fields as diverse as history and biology. The cost of buildings and new faculty for the new college is expected to be $1 billion upon completion.

Over the course of 20 years, MIT received approximately $800,000 via foundations controlled by Jeffrey Epstein, convicted sex offender charged with the sex trafficking and sexual abuse of minors. All of those gifts went either to the MIT Media Lab or to Professor Seth Lloyd. Both Lloyd and former Media Lab Director Joi Ito have made public statements apologizing to Jeffrey Epstein's victims and others for judgments made over a series of years. Ito resigned from his position as Media Lab Director and professor, but Lloyd did not.

MIT's campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin. The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot.

The Kendall/MIT MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods. In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan. The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.

Each building at MIT has a number (possibly preceded by a "W", "N", "E", or "NW") designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. Many of the buildings are connected above ground as well as through an extensive network of tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.

MIT's on-campus nuclear reactor is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, but MIT maintains that it is well-secured. In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.

MIT Nano, also known as Building 12, is an interdisciplinary facility for nanoscale research. Its 100,000 square feet cleanroom and research space, visible through expansive glass facades, is the largest research facility of its kind in the nation. With a cost of US$400 million, it is also one of the costliest buildings on campus. The facility also provides state-of-the-art nanoimaging capabilities with vibration damped imaging and metrology suites sitting atop a 5-million-pound slab of concrete underground.

Other notable campus facilities include a pressurized wind tunnel for testing aerodynamic research, a towing tank for testing ship and ocean structure designs, and Alcator C-Mod, the largest fusion device operated by any university. MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering of campus.

In 2001, the Environmental Protection Agency sued MIT for violating the Clean Water Act and the Clean Air Act with regard to its hazardous waste storage and disposal procedures. MIT settled the suit by paying a $155,000 fine and launching three environmental projects. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.

The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.

MIT has substantial commercial real estate holdings in Cambridge on which it pays property taxes, plus an additional voluntary payment in lieu of taxes (PILOT) on academic buildings which are legally tax-exempt. , it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues. Holdings include Technology Square, parts of Kendall Square, and many properties in Cambridgeport and Area 4 neighboring the educational buildings. The land is held for investment purposes and potential long-term expansion.

MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States, and it has a history of commissioning progressive buildings. The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US. Bosworth's design was influenced by the City Beautiful Movement of the early 1900s and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.

Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture. More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture". These buildings have not always been well received; in 2010, "The Princeton Review" included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".

Undergraduates are guaranteed four-year housing in one of MIT's 10 undergraduate dormitories. Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the "Yale Daily News" staff's "The Insider's Guide to the Colleges, 2010", "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture." MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.

MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs). , 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities. Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin. After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year.

In 2013-2014, MIT abruptly closed and then demolished undergrad dorm Bexley Hall, citing extensive water damage that made repairs infeasible. In 2017, MIT shut down Senior House after a century of service as an undergrad dorm. That year, MIT administrators released data showing just 60% of Senior House residents had graduated in four years. Campus-wide, the four-year graduation rate is 84% (the cumulative graduation rate is significantly higher).

MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation. The current board consists of 43 members elected to five-year terms, 25 life members who vote until their 75th birthday, 3 elected officers (President, Treasurer, and Secretary), and 4 "ex officio" members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court). The board is chaired by Robert Millard, a co-founder of L-3 Communications Holdings. The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty. MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo). Valued at $16.4 billion in 2018, MIT's endowment was then the sixth-largest among American colleges and universities.

MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine. A second college, The Stephen A. Schwarzman College of Computing, is scheduled to open in September 2019. While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.

MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. The university has been accredited by the New England Association of Schools and Colleges since 1929. MIT operates on a 4-1-4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester commencing in early February and ceasing in late May.

MIT students refer to both their majors and classes using numbers or acronyms alone. Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is , while Linguistics and Philosophy is . Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; for instance, the introductory calculus-based classical mechanics course is simply "8.01" at MIT.

The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by "U.S. News", admitting few transfer students and 6.7% of its applicants in the 2017-2018 admissions cycle. MIT offers 44 undergraduate degrees across its five schools. In the 2017–2018 academic year, 1,045 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards. In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (), Computer Science and Engineering (), Mechanical Engineering (), Physics (), and Mathematics ().

All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive", including "substantial instruction and practice in oral presentation". Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.

Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose", the freshmen retention rate at MIT is similar to other research universities. The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.

In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. A substantial majority of undergraduates participate. Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.

In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published "The Hidden Curriculum," arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.

MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees. The Institute offers graduate programs leading to academic degrees such as the Master of Science (which is abbreviated as SM at MIT), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD) and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School) and a joint program in oceanography with Woods Hole Oceanographic Institution.

Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).

MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11. In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%), and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.

MIT also places among the top five in many overall rankings of universities (see right) and rankings based on students' revealed preferences. For several years, "U.S. News & World Report", the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.

Times Higher Education has recognized MIT as one of the world's "six super brands" on its "World Reputation Rankings", along with Berkeley, Cambridge, Harvard, Oxford and Stanford. In 2019, it ranked 3rd among the universities around the world by SCImago Institutions Rankings. In 2017, the Times Higher Education World University Rankings rated MIT the #2 university for arts and humanities. MIT was ranked #7 in 2015 and #6 in 2017 of the Nature Index Annual Tables, which measure the largest contributors to papers published in 82 leading journals.

The university historically pioneered research and training collaborations between academia, industry and government.  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.  In 1948, Compton established the MIT Industrial Liaison Program. Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese – firms that were competing with struggling American businesses. On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940. MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.

The US Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. While the Ivy League institutions settled, MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. MIT ultimately prevailed when the Justice Department dropped the case in 1994.

MIT's proximity to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute. In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees. A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge. MIT also has a long term partnership with Imperial College London, for both student exchanges and research collaboration. More modest cross-registration programs have been established with Boston University, Brandeis University, Tufts University, Massachusetts College of Art and the School of the Museum of Fine Arts, Boston.

MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute), Singapore-MIT Alliance, MIT-Politecnico di Milano, MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.

The mass-market magazine "Technology Review" is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine. The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events and social issues.

The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, the List Visual Arts Center's rotating exhibitions of contemporary art, and the Compton Gallery's cross-disciplinary exhibitions. MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.

The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".

MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity; research expenditures totaled $718.2 million in 2009. The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million. MIT employs approximately 1300 researchers in addition to faculty. In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.

In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers. Harold Eugene Edgerton was a pioneer in high speed photography and sonar. Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory. In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography. At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.

Current and previous physics faculty have won eight Nobel Prizes, four Dirac Medals, and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory. Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods. MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology. Professor Eric Lander was one of the principal leaders of the Human Genome Project. Positronium atoms, synthetic penicillin, synthetic self-replicating molecules, and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT. Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain". Researchers developed a system to convert MRI scans into 3D printed physical models.

In the domain of humanities, arts, and social sciences, as of October 2019 MIT economists have been awarded seven Nobel Prizes and nine John Bates Clark Medals. Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology. The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research, has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.

Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 50 people associated with MIT. Five Pulitzer Prize–winning writers currently work at or have retired from MIT. Four current or former faculty are members of the American Academy of Arts and Letters.

Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed. Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.



MIT alumni and faculty have founded numerous companies, some of which are shown below:


The faculty and student body place a high value on meritocracy and on technical proficiency. MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation. However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.

Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat". Originally created in 1929, the ring's official name is the "Standard Technology Ring". The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver. The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise", "Institute Has The Finest Professors", "Institute of Hacks, Tomfoolery and Pranks", "It's Hard to Fondle Penguins", and other variations, has occasionally been featured on the ring given its historical prominence in student culture.

MIT has over 500 recognized student activity groups, including a campus radio station, "The Tech" student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.

The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are Autonomous Robot Design (course 6.270), Robocraft Programming (6.370), and MasLab competitions, the annual "mystery hunt", and Charm School. More than 250 students pursue externships annually at companies in the US and abroad.

Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes. Recent high-profile hacks have included the abduction of Caltech's cannon, reconstructing a Wright Flyer atop the Great Dome, and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.

MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs. MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). The intercollegiate sports teams, called the MIT Engineers won 22 Team National Championships, 42 Individual National Championships. MIT is the all-time Division III leader in producing Academic All-Americas (302) and rank second across all NCAA Divisions only behind the University of Nebraska. MIT Athletes won 13 Elite 90 awards and ranks first among NCAA Division III programs, and third among all divisions. In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.

MIT enrolled 4,602 undergraduates and 6,972 graduate students in 2018–2019. Women constituted 45 percent of undergraduate students. Undergraduate and graduate students came from all 50 US states as well as from 115 foreign countries.

MIT received 20,247 applications for admission to the undergraduate Class of 2021: it admitted 1,452 (7.1 percent) and enrolled 1,102 (76 percent). 19,446 applications were received for graduate and advanced degree programs across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).

The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class. 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.

Undergraduate tuition and fees total $40,732 per student and annual expenses are estimated at $52,507 . 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student. Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million). The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".

MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry. Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963. Between 1993 and 2009 the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning and Biological Engineering.

A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention focussing on MIT's culture and student life. After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity, MIT began requiring all freshmen to live in the dormitory system. The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate. In late 2001 a task force's recommended improvements in student mental health services were implemented, including expanding staff and operating hours at the mental health center. These and later cases were significant as well because they sought to prove the negligence and liability of university administrators "in loco parentis".

, MIT had 1,030 faculty members. Faculty are responsible for lecturing classes, for advising both graduate and undergraduate students, and for sitting on academic committees, as well as for conducting original research. Between 1964 and 2009 a total of seventeen faculty and staff members affiliated with MIT won Nobel Prizes (thirteen of them in the latter 25 years). As of October 2019, 37 MIT faculty members, past or present, have won Nobel Prizes, the majority in Economics or Physics.

, current faculty and teaching staff included 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows. Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.

A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science, although the study's methods were controversial. Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice-presidents, although allegations of sexism continue. Susan Hockfield, a molecular neurobiologist, served as MIT's president from 2004 to 2012 - the first woman to hold the post.

Tenure issues have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble (a historian of technology) became a "cause célèbre" about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military. Former materials-science professor Gretchen Kalonji sued MIT in 1994, alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments and the establishment of a project to encourage women and minorities to seek faculty positions. In 1997 the Massachusetts Commission Against Discrimination issued a probable-cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.

In 2006–2007, MIT's denial of tenure to African-American stem-cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger-strike, and the resignation of Professor Frank L. Douglas in protest. "The Boston Globe" reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues later issued a statement saying that the professor was treated fairly in tenure review."

MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty-member Charles W. Eliot became president of Harvard University in 1869, a post he would hold for 40 years, during which he wielded considerable influence both on American higher education and on secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.

In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences, urban studies professor Xavier de Souza Briggs served as the associate director of the White House Office of Management and Budget, and biology professor Eric Lander was a co-chair of the President's Council of Advisors on Science and Technology. In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy. Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.

, MIT was the second-largest employer in the city of Cambridge. Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities . Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic" but complaining about "low pay" compared to an industry position.

Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. , 39 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.

Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, Representative Thomas Massie, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, former Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, former Iraqi Deputy Prime Minister Ahmed Chalabi, former Minister of Education and Culture of The Republic of Indonesia Yahya Muhaimin, former Jordanian Minister of Education, Higher Education and Scientific Research & former Jordanian Minister of Energy and Mineral Resources Khaled Toukan. Alumni in sports have included Olympic fencing champion Johan Harmenberg.

MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Apotex, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, "The Guardian", "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."

Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), New Jersey Institute of Technology, Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.

More than one third of the United States' manned spaceflights have included MIT-educated astronauts, more than any university excluding the United States service academies. Of the 12 people who have been on the Moon as of 2019, four graduated from MIT (among them Apollo 11 Lunar Module Pilot Buzz Aldrin). Alumnus and former faculty member Qian Xuesen led the Chinese nuclear weapons program and was instrumental in the PRC rocket program.

Noted alumni in non-scientific fields include author Hugh Lofting, sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British "BBC" and "ITN" correspondent and political advisor David Walter, "The New York Times" columnist and Nobel Prize Winning economist Paul Krugman, "The Bell Curve" author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.




</doc>
<doc id="18880" url="https://en.wikipedia.org/wiki?curid=18880" title="Monopolistic competition">
Monopolistic competition

Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. In the presence of coercive government, monopolistic competition will fall into government-granted monopoly. Unlike perfect competition, the firm maintains spare capacity. Models of monopolistic competition are often used to model industries. Textbook examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, "Theory of Monopolistic Competition" (1933). Joan Robinson published a book "The Economics of Imperfect Competition" with a comparable theme of distinguishing perfect from imperfect competition.

Monopolistically competitive markets have the following characteristics:


The long-run characteristics of a monopolistically competitive market are almost the same as a perfectly competitive market. Two differences between the two are that monopolistic competition produces heterogeneous products and that monopolistic competition involves a great deal of non-price competition, which is based on subtle product differentiation. A firm making profits in the short run will nonetheless only break even in the long run because demand will decrease and average total cost will increase. This means in the long run, a monopolistically competitive firm will make zero economic profit. This illustrates the amount of influence the firm has over the market; because of brand loyalty, it can raise its prices without losing all of its customers. This means that an individual firm's demand curve is downward sloping, in contrast to perfect competition, which has a perfectly elastic demand schedule.

There are six characteristics of monopolistic competition (MC):


MC firms sell products that have real or perceived non-price differences. However, the differences are not so great as to eliminate other goods as substitutes. Technically, the cross price elasticity of demand between goods in such a market is positive. In fact, the XED would be high. MC goods are best described as close but imperfect substitutes. The goods perform the same basic functions but have differences in qualities such as type, style, quality, reputation, appearance, and location that tend to distinguish them from each other. For example, the basic function of motor vehicles is the same—to move people and objects from point to point in reasonable comfort and safety. Yet there are many different types of motor vehicles such as motor scooters, motor cycles, trucks and cars, and many variations even within these categories.

There are many firms in each MC product group and many firms on the side lines prepared to enter the market. A product group is a "collection of similar products". The fact that there are "many firms" gives each MC firm the freedom to set prices without engaging in strategic decision making regarding the prices of other firms and each firm's actions have a negligible impact on the market. For example, a firm could cut prices and increase sales without fear that its actions will prompt retaliatory responses from competitors.

How many firms will an MC market structure support at market equilibrium? The answer depends on factors such as fixed costs, economies of scale and the degree of product differentiation. For example, the higher the fixed costs, the fewer firms the market will support.

Like perfect competition, under monopolistic competition also, the firms can enter or exit freely. The firms will enter when the existing firms are making super-normal profits. With the entry of new firms, the supply would increase which would reduce the price and hence the existing firms will be left only with normal profits. Similarly, if the existing firms are sustaining losses, some of the marginal firms will exit. It will reduce the supply due to which price would rise and the existing firms will be left only with normal profit.

Each MC firm independently sets the terms of exchange for its product. The firm gives no consideration to what effect its decision may have on competitors. The theory is that any action will have such a negligible effect on the overall market demand that an MC firm can act without fear of prompting heightened competition. In other words, each firm feels free to set prices as if it were a monopoly rather than an oligopoly.

MC firms have some degree of market power. Market power means that the firm has control over the terms and conditions of exchange. An MC firm can raise its prices without losing all its customers. The firm can also lower prices without triggering a potentially ruinous price war with competitors. The source of an MC firm's market power is not barriers to entry since they are low. Rather, an MC firm has market power because it has relatively few competitors, those competitors do not engage in strategic decision making and the firms sells differentiated product. Market power also means that an MC firm faces a downward sloping demand curve. The demand curve is highly elastic although not "flat".

No sellers or buyers have complete market information, like market demand or market supply.

There are two sources of inefficiency in the MC market structure. First, at its optimum output the firm charges a price that exceeds marginal costs, The MC firm maximizes profits where marginal revenue = marginal cost. Since the MC firm's demand curve is downward sloping this means that the firm will be charging a price that exceeds marginal costs. The monopoly power possessed by a MC firm means that at its profit maximizing level of production there will be a net loss of consumer (and producer) surplus. The second source of inefficiency is the fact that MC firms operate with excess capacity. That is, the MC firm's profit maximizing output is less than the output associated with minimum average cost. Both a PC and MC firm will operate at a point where demand or price equals average cost. For a PC firm this equilibrium condition occurs where the perfectly elastic demand curve equals minimum average cost. A MC firm's demand curve is not flat but is downward sloping. Thus in the long run the demand curve will be tangential to the long run average cost curve at a point to the left of its minimum. The result is excess capacity.


Monopolistically competitive firms are inefficient, it is usually the case that the costs of regulating prices for products sold in monopolistic competition exceed the benefits of such regulation. A monopolistically competitive firm might be said to be marginally inefficient because the firm produces at an output where average total cost is not a minimum. A monopolistically competitive market is productively inefficient market structure because marginal cost is less than price in the long run. Monopolistically competitive markets are also allocatively inefficient, as the price given is higher than Marginal cost. Product differentiation increases total utility by better meeting people's wants than homogenous products in a perfectly competitive market.

Another concern is that monopolistic competition fosters advertising and the creation of brand names. Advertising induces customers into spending more on products because of the name associated with them rather than because of rational factors. Defenders of advertising dispute this, arguing that brand names can represent a guarantee of quality and that advertising helps reduce the cost to consumers of weighing the tradeoffs of numerous competing brands. There are unique information and information processing costs associated with selecting a brand in a monopolistically competitive environment. In a monopoly market, the consumer is faced with a single brand, making information gathering relatively inexpensive. In a perfectly competitive industry, the consumer is faced with many brands, but because the brands are virtually identical information gathering is also relatively inexpensive. In a monopolistically competitive market, the consumer must collect and process information on a large number of different brands to be able to select the best of them. In many cases, the cost of gathering information necessary to selecting the best brand can exceed the benefit of consuming the best brand instead of a randomly selected brand. The result is that the consumer is confused. Some brands gain prestige value and can extract an additional price for that.

Evidence suggests that consumers use information obtained from advertising not only to assess the single brand advertised, but also to infer the possible existence of brands that the consumer has, heretofore, not observed, as well as to infer consumer satisfaction with brands similar to the advertised brand.

In many markets, such as toothpaste, soap, air conditioning, smartphones and toilet paper, producers practice product differentiation by altering the physical composition of products, using special packaging, or simply claiming to have superior products based on brand images or advertising.



</doc>
<doc id="18881" url="https://en.wikipedia.org/wiki?curid=18881" title="Mathematical induction">
Mathematical induction

Mathematical induction is a mathematical proof technique. It is essentially used to prove that a property "P"("n") holds for every natural number "n", i.e. for "n" = 0, 1, 2, 3, and so on. Metaphors can be informally used to understand the concept of mathematical induction, such as the metaphor of falling dominoes or climbing a ladder:
The method of induction requires two cases to be proved. The first case, called the base case (or, sometimes, the basis), proves that the property holds for the number 0. The second case, called the induction step, proves that if the property holds for one natural number formula_1, then it holds for the next natural number formula_2. These two steps establish the property formula_3 for every natural number formula_4 The base case does not necessarily begin with formula_5. In fact, it often begins with the number one, and it can begin with any natural number, establishing the truth of the property for all natural numbers greater than or equal to the starting number.

The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction, in some form, is the foundation of all correctness proofs for computer programs.

Although its name may suggest otherwise, mathematical induction should not be misconstrued as a form of inductive reasoning as used in philosophy (also see Problem of induction). Mathematical induction is an inference rule used in formal proofs. Proofs by mathematical induction are, in fact, examples of deductive reasoning.

In 370 BC, Plato's Parmenides may have contained an early example of an implicit inductive proof. The earliest implicit traces of mathematical induction may be found in Euclid's proof that the number of primes is infinite and in Bhaskara's "cyclic method". An opposite iterated technique, counting "down" rather than up, is found in the Sorites paradox, where it was argued that if 1,000,000 grains of sand formed a heap, and removing one grain from a heap left it a heap, then a single grain of sand (or even no grains) forms a heap.

An implicit proof by mathematical induction for arithmetic sequences was introduced in the al-Fakhri written by al-Karaji around 1000 AD, who used it to prove the binomial theorem and properties of Pascal's triangle.

None of these ancient mathematicians, however, explicitly stated the induction hypothesis. Another similar case (contrary to what Vacca has written, as Freudenthal carefully showed) was that of Francesco Maurolico in his "Arithmeticorum libri duo" (1575), who used the technique to prove that the sum of the first formula_1 odd integers is formula_7.

The earliest rigorous use of induction was by Gersonides (1288–1344). The first explicit formulation of the principle of induction was given by Pascal in his "Traité du triangle arithmétique" (1665). Another Frenchman, Fermat, made ample use of a related principle: indirect proof by infinite descent. 

The induction hypothesis was also employed by the Swiss Jakob Bernoulli, from then on it became more or less well known. The modern rigorous and systematic treatment of the principle came only in the 19th century, with George Boole, Augustus de Morgan, Charles Sanders Peirce, 
Giuseppe Peano, and Richard Dedekind.

The simplest and most common form of mathematical induction infers that a statement involving a natural number formula_1 (that is, an integer formula_9 or 1) holds for all values of formula_1. The proof consists of two steps:

The hypothesis in the inductive step, that the statement holds for a particular "formula_1", is called the induction hypothesis or inductive hypothesis. To prove the inductive step, one assumes the induction hypothesis for formula_1 and then uses this assumption, involving formula_1, to prove that the statement holds for formula_2.

Authors who prefer to define natural numbers to begin at 0 use that value in the base case.
Authors who prefer to define natural numbers to begin at 1 use that value in the base case.

Mathematical induction can be used to prove that the following statement, "P"("n"), holds for all natural numbers "n".

"P"("n") gives a formula for the sum of the natural numbers less than or equal to number "n". The proof that "P"("n") is true for each natural number "n" proceeds as follows:

Proposition. For any formula_21, formula_20

Proof. Let formula_3 be the statement formula_20 We give a proof by induction on "n".

"Base case:" Show that the statement holds for "n" = 0 (taking 0 as a natural).

"P"(0) is easily seen to be true:

"Inductive step:" Show that for any formula_26, if formula_27 holds, then formula_28 also holds. This can be done as follows.

Assume the induction hypothesis that formula_29 is true (for some arbitrary value of formula_30). It must then be shown that formula_28 is true, that is:

Using the induction hypothesis, the left-hand side can be equated to:

Algebraically, we have that:

which shows that formula_28 indeed holds.

Since both the base case and the inductive step have been performed, by mathematical induction the statement "P"("n") holds for all natural numbers "n". ∎

Induction is often used to prove inequalities. As an example, we prove that formula_36 for any real number formula_37 and natural number formula_1. 

At first glance, it may appear that a more general version, formula_36 for any "real" numbers formula_40, could be proven without induction, solely using trigonometry formulas holding for all real values of formula_1 and formula_37. However, formula_43 is essential: since formula_44 while formula_45 for negative values of formula_1, the statement is clearly false in general for negative formula_1. Moreover, taking formula_48 shows that it is also false in general for non-integral values of formula_1. This suggests we confine to proving the statement specifically for "natural" values of formula_1, and we turn to induction in order to pass from one value of formula_1 to the next in a relatively straightforward manner, starting from a trivially verifiable base case.

Proposition. For any formula_52, formula_36. 

Proof. Let formula_37 be a fixed, arbitrary real number and formula_3 be the statement formula_36. We induct on formula_1. 

"Base case:" The calculation formula_58 verifies the truth of the base case formula_59. 

"Inductive step:" We show that formula_60 for any natural number formula_61. We use the angle addition formula formula_62 and the triangle inequality formula_63, both of which hold for all real numbers formula_64. For every formula_30, assuming the truth of the induction hypothesis formula_29 gives us the following chain of equalities and inequalities:

The inequality implied by the first and last lines shows that formula_29 implies formula_69 for every formula_61, which completes the inductive step. Thus, the proposition holds by induction. ∎

In practice, proofs by induction are often structured differently, depending on the exact nature of the property to be proven.
All variants of induction are special cases of transfinite induction; see below.

If one wishes to prove a statement not for all natural numbers, but only for all numbers "n" greater than or equal to a certain number "b", then the proof by induction consists of:
This can be used, for example, to show that formula_74 for formula_75.

In this way, one can prove that some statement formula_3 holds for all formula_77, or even for all formula_78. This form of mathematical induction is actually a special case of the previous form, because if the statement to be proved is formula_3 then proving it with these two rules is equivalent with proving formula_80 for all natural numbers formula_1 with an induction base case formula_82.

Assume an infinite supply of 4- and 5-dollar coins. Induction can be used to prove that any whole amount of dollars greater than or equal to formula_83 can be formed by a combination of such coins. Let formula_84 denote the statement ""the amount of formula_61 dollars can be formed by a combination of 4- and 5-dollar coins"". The proof that formula_84 is true for all formula_87 can then be achieved by induction on formula_61 as follows:

"Base case": Showing that formula_84 holds for formula_90 is easy: take three 4-dollar coins.

"Induction step": Given that formula_84 holds for some value of formula_87 ("induction hypothesis"), prove that formula_93 holds, too: 

Therefore, by the principle of induction, formula_84 holds for all formula_95, and the proof is complete.

In this example, although formula_84 also holds for formula_104,
the above proof cannot be modified to replace the minimum amount of formula_83 dollar to any lower value formula_106.
For formula_107, the base case is actually false;
for formula_108, the second case in the induction step (replacing three 5- by four 4-dollar coins) will not work;
let alone for even lower formula_106.

It is sometimes desirable to prove a statement involving two natural numbers, "n" and "m", by iterating the induction process. That is, one proves a base case and an inductive step for "n", and in each of those proves a base case and an inductive step for "m". See, for example, the proof of commutativity accompanying "addition of natural numbers". More complicated arguments involving three or more counters are also possible.

The method of infinite descent is a variation of mathematical induction which was used by Pierre de Fermat. It is used to show that some statement "Q"("n") is false for all natural numbers "n". Its traditional form consists of showing that if "Q"("n") is true for some natural number "n", it also holds for some strictly smaller natural number "m". Because there are no infinite decreasing sequences of natural numbers, this situation would be impossible, thereby showing (by contradiction) that "Q"("n") cannot be true for any "n". 

The validity of this method can be verified from the usual principle of mathematical induction. Using mathematical induction on the statement "P"("n") defined as ""Q"("m") is false for all natural numbers "m" less than or equal to "n"", it follows that "P"("n") holds for all "n", which means that "Q"("n") is false for every natural number "n".

The most common form of proof by mathematical induction requires proving in the inductive step that

whereupon the induction principle "automates" "n" applications of this step in getting from "P"(0) to "P"("n"). This could be called "predecessor induction" because each step proves something about a number from something about that number's predecessor.

A variant of interest in computational complexity is "prefix induction", in which one proves the following statement in the inductive step:

or equivalently

The induction principle then "automates" log "n" applications of this inference in getting from "P"(0) to "P"("n"). In fact, it is called "prefix induction" because each step proves something about a number from something about the "prefix" of that number — as formed by truncating the low bit of its binary representation. It can also be viewed as an application of traditional induction on the length of that binary representation.

If traditional predecessor induction is interpreted computationally as an "n"-step loop, then prefix induction would correspond to a log-"n"-step loop. Because of that, proofs using prefix induction are "more feasibly constructive" than proofs using predecessor induction.

Predecessor induction can trivially simulate prefix induction on the same statement. Prefix induction can simulate predecessor induction, but only at the cost of making the statement more syntactically complex (adding a bounded universal quantifier), so the interesting results relating prefix induction to polynomial-time computation depend on excluding unbounded quantifiers entirely, and limiting the alternation of bounded universal and existential quantifiers allowed in the statement.

One can take the idea a step further: one must prove

whereupon the induction principle "automates" log log "n" applications of this inference in getting from "P"(0) to "P"("n"). This form of induction has been used, analogously, to study log-time parallel computation.

Another variant, called complete induction, course of values induction or strong induction (in contrast to which the basic form of induction is sometimes known as weak induction), makes the inductive step easier to prove by using a stronger hypothesis: one proves the statement under the assumption that "P"("n") holds for "all" natural "n" less than ; by contrast, the basic form only assumes "P"("m"). The name "strong induction" does not mean that this method can prove more than "weak induction", but merely refers to the stronger hypothesis used in the inductive step.

In fact, it can be shown that the two methods are actually equivalent, as explained below. In this form of complete induction one still has to prove the base case, "P"(0), and it may even be necessary to prove extra base cases such as "P"(1) before the general argument applies, as in the example below of the Fibonacci number "F".

Although the form just described requires one to prove the base case, this is unnecessary if one can prove "P"("m") (assuming "P"("n") for all lower "n") for all . This is a special case of transfinite induction as described below. In this form the base case is subsumed by the case , where "P"(0) is proved with no other "P"("n") assumed;
this case may need to be handled separately, but sometimes the same argument applies for "m" = 0 and , making the proof simpler and more elegant.
In this method, however, it is vital to ensure that the proof of "P"("m") does not implicitly assume that , e.g. by saying "choose an arbitrary ", or by assuming that a set of "m" elements has an element.

Complete induction is equivalent to ordinary mathematical induction as described above, in the sense that a proof by one method can be transformed into a proof by the other. Suppose there is a proof of "P"("n") by complete induction. Let Q("n") mean ""P"("m") holds for all "m" such that ". Then Q("n") holds for all "n" if and only if P("n") holds for all "n", and our proof of "P"("n") is easily transformed into a proof of Q("n") by (ordinary) induction. If, on the other hand, "P"("n") had been proven by ordinary induction, the proof would already effectively be one by complete induction: "P"(0) is proved in the base case, using no assumptions, and is proved in the inductive step, in which one may assume all earlier cases but need only use the case "P"("n").

Complete induction is most useful when several instances of the inductive hypothesis are required for each inductive step. For example, complete induction can be used to show that

where formula_115 is the "n"th Fibonacci number, <math display="inline">\varphi = 


</doc>
<doc id="18884" url="https://en.wikipedia.org/wiki?curid=18884" title="Matrix">
Matrix

Matrix or MATRIX may refer to:













</doc>
<doc id="18885" url="https://en.wikipedia.org/wiki?curid=18885" title="Morton Downey Jr.">
Morton Downey Jr.

Sean Morton Downey (December 9, 1932 – March 12, 2001), better known as Morton Downey Jr., was an American television talk show host of the late-1980s who pioneered the "trash TV" format on his program "The Morton Downey Jr. Show".

Downey's parents were also in show business; his father, Morton Downey, was a popular singer, and his mother, Barbara Bennett, was a stage and film actress and singer and dancer. Downey did not use his legal first name (Sean) in his stage name. His aunts included Hollywood film stars Constance and Joan Bennett, from whom he was estranged, and his maternal grandfather was the celebrated matinée idol Richard Bennett. Born into a wealthy family, he was raised during the summers next door to the Kennedy compound in Hyannis Port, Massachusetts. Downey attended New York University.

He was a program director and announcer at radio station WPOP in Hartford, Connecticut in the 1950s. He went on to work as a disc jockey, sometimes using the moniker "Doc" Downey, in various markets around the U.S., including Phoenix (KRIZ), Miami (WFUN), Kansas City (KUDL), San Diego (KDEO) and Seattle (KJR). He had to resign from WFUN after drawing ire from the FCC for announcing a competing disc jockey's home phone number on the air and insulting his wife. Like his father, Downey pursued a career in music, recording in both pop and country styles. He sang on a few records and then began to write songs, several of which were popular in the 1950s and 1960s. He joined ASCAP as a result. In 1958, he recorded "Boulevard of Broken Dreams", which he sang on national television on a set that resembled a dark street with one street light. In 1981, "Green Eyed Girl" charted on the "Billboard Magazine" country chart, peaking at #95.

In the 1980s, Downey was a talk show host at KFBK-AM in Sacramento, California, where he employed his abrasive style. He was fired in 1984. Downey also had a stint on WMAQ-AM in Chicago where he unsuccessfully tried to get other on air radio personalities to submit to drug testing. Downey's largest effect on American culture came from his popular, yet short-lived, syndicated late 1980s television talk show, "The Morton Downey Jr. Show".

On January 22, 1980, Downey, a devoted pro-life activist, hosted the California State Rally for Life at the invitation of the California ProLife Council and United Students for Life. At that time, he was also running for President of the United States, as a Democrat. The United Students for Life, at California State University, Sacramento helped organize his California presidential rallies. Downey worked to help promote pro-life candidates in California and around the country.

Downey headed to Secaucus, New Jersey, where his highly controversial television program "The Morton Downey Jr. Show" was taped. Starting as a local program on New York-New Jersey superstation WWOR-TV in October 1987, it expanded into national syndication in early 1988. The program featured screaming matches among Downey, his guests, and audience members. Using a large silver bowl for an ashtray, he would chainsmoke during the show and blow smoke in his guests' faces. Downey's fans became known as "Loudmouths," patterned after the studio lecterns decorated with gaping cartoon mouths, from which Downey's guests would go head-to-head against each other on their respective issues.

Downey's signature phrases "pablum puking liberal" (in reference to left-liberals) and "zip it!" briefly enjoyed some popularity in the contemporary vernacular. He particularly enjoyed making his guests angry with each other, which on a few occasions resulted in physical confrontations. One such incident occurred on a 1988 show taped at the Apollo Theater, involving Al Sharpton and CORE National Chairman Roy Innis. The exchange between the two men culminated in Innis shoving Sharpton into his chair, knocking him to the floor and Downey intervening to separate the pair.

Because of the controversial format and content of the show, distributor MCA Television had problems selling the show to a number of stations and advertisers. Even Downey's affiliates, many of which were low-rated independent television stations in small to medium markets, were so fearful of advertiser and viewer backlash that they would air one or even two local disclaimers during the broadcast.

During one controversial episode Downey introduced his gay brother, Tony Downey, to his studio audience and informed them Tony was HIV positive. During the episode Downey stated he was afraid his audience would abandon him if they knew he had a gay brother, but then said he did not care.

"The Washington Post" wrote about him, "Suppose a maniac got hold of a talk show. Or need we suppose?" David Letterman said, "I'm always amazed at what people will fall for. We see this every ten or twelve years, an attempt at this, and I guess from that standpoint I don't quite understand why everybody's falling over backwards over the guy."

The success of the show made Downey a pop culture celebrity, leading to appearances on "Saturday Night Live" in 1988, WrestleMania V in 1989 in which he traded insults with Roddy Piper and Brother Love on "Piper's Pit", and later roles in movies such as "Predator 2" and "". He was also cast in several television roles, often playing tabloid TV hosts or other obnoxious media types. Downey notably starred in the "Tales from the Crypt" episode "Television Terror" which utilized several scenes shot by characters within the story, a format which became popular in horror films a decade later with the found footage genre.

In 1989, Downey released an album of songs based on his show entitled "Morton Downey Jr. Sings". The album's single, "Zip It!" (a catch-phrase from the TV show, used to quiet an irate guest), became a surprise hit on some college radio stations. Over the course of the 1988–89 television season, his TV show suffered a decline in viewership, resulting from many markets downgrading its time slot; even flagship station WWOR moved Downey's program from its original 9:00 PM slot to 11:30 PM in the fall of 1988. Beginning in January 1989, the time slot immediately following Downey's program was given to the then-new "Arsenio Hall Show". Following Hall's strong early ratings, however, the two series swapped time slots several weeks later, thus relegating Downey to 12:30 AM in the number-one television market. 

In late April 1989, he was involved in an incident in a San Francisco International Airport restroom in which he claimed to have been attacked by neo-Nazis who painted a swastika on his face and attempted to shave his head. Some inconsistencies in Downey's account (e.g., the swastika was painted in reverse, suggesting that Downey had drawn it himself in a mirror), and the failure of the police to find supportive evidence, led many to suspect the incident was a hoax and a ploy for attention. In July 1989, his show was canceled, with the owners of the show announcing that the last episode had been taped on June 30, and that no new shows would air after September 15, 1989.

At the time of its cancellation, the show was airing on a total of 70 stations across the country, and its advertisers had been reduced primarily to "direct-response" ads (such as 900 chat line and phone sex numbers). In February 1990, Downey filed for bankruptcy in the US Bankruptcy Court for the District of New Jersey.

In 1990, Downey resurfaced on CNBC with an interview program called "Showdown", which was followed by three attempted talk radio comebacks: first in 1992 on Washington, D.C. radio station WWRC; then in 1993 on Dallas radio station KGBS, where he would scream insults at his callers. He was also hired as the station's VP of Operations. The following year, he returned to CNBC with a short-lived television show, "Downey"; in one episode, Downey claimed to have had a psychic communication with O.J. Simpson's murdered ex-wife, Nicole Brown Simpson.

His third – and final – attempt at a talk radio comeback occurred in 1997 on Cleveland radio station WTAM in a late evening time slot. It marked his return to the Cleveland market, where Downey had been a host for crosstown radio station WERE in the early 1980s prior to joining KFBK. This stint came shortly after the surgery for lung cancer that removed one of his lungs. At WTAM, Downey abandoned the confrontational schtick of his TV and previous radio shows, and conducted this program in a much more conversational and jovial manner.

On August 30, 1997, Downey quit his WTAM show to focus on pursuing legal action against Howard Stern. Downey had accused Stern of spreading rumors that he had resumed his smoking habit, to which publicist Les Schecter retorted, "He hasn't picked up a cigarette." His replacement was former WERE host Rick Gilmour.

Following his death, news reports and obituaries incorrectly (according to the "Orange County Register") credited him as the composer of "Wipe Out." As of 2008, Downey's official website (and others) continue to make this claim. Prior to Downey's death, "Spin" in April 1989 had identified the "Wipe Out" authorship as a myth.

In 1984, at KFBK radio, Downey used the word "Chinaman" while telling a joke. His use of the word upset portions of the sizable Asian community in Sacramento. One Asian-American city councilman called for an apology and pressured the station for Downey's resignation. Downey refused to apologize and was forced to resign.

Downey was sued for allegedly appropriating the words and music to his theme song from two songwriters. He was sued for $40 million after bringing then-stripper Kellie Everts onto the show and calling her a "slut," a "pig," a "hooker," and a "tramp," claiming that she had venereal diseases, and banging his pelvis against hers.

In April 1988, he was arraigned on criminal charges for allegedly attacking a gay guest on his show, in a never-aired segment. In another lawsuit, he was accused of slandering a newscaster (a former colleague), and of indecently exposing himself to her and slapping her. Downey punched Stuttering John during an interview done for "The Howard Stern Show", while also shouting verbal insults at John, referring to him as an "uneducated slob". The situation then began to evolve into a brawl between the two until Downey had to be pulled off of John by security; the entire incident was caught on camera. When an "Inside Edition" camera crew approached Downey in 1989 to question him about his involvement in an alleged business scam, Downey grabbed the boom mike and struck the soundman's head with it.

In his later years, Downey expressed remorse for some of the extreme theatrics of his TV show, as well as various incidents outside the studio, including the "Inside Edition" confrontation. However, he also claimed that his show was of a higher quality and not as "sleazy" as Jerry Springer's show.

Released in 2012, the documentary film "" touches upon Downey's upbringing and formative years in radio and politics before launching into the history of "The Morton Downey Jr. Show" and Downey's influence on trash TV. The film also looks at Downey's relationship with Al Sharpton and other important 80s figures, as well as Downey's role as a predecessor for commentators like Glenn Beck and Rush Limbaugh.

Downey was married four times and had four children from three of those marriages. With wife Helen, he had daughter Melissa; with Joan, he had daughters Tracey and Kelli; and, with fourth wife Lori, he had daughter Seanna Micaela. He and Lori met when she appeared as a dancer in a show he attended in Atlantic City. According to Terry Pluto's book, "Loose Balls", Downey was one of the owners of the New Orleans Buccaneers basketball team in the American Basketball Association in the late 1960s. Downey was also president and co-founder of the proposed World Baseball Association in 1974.

In 1998, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.

Morton Koopa Jr., a character from the "Super Mario" series and one of the Koopalings, was named after him.

In June 1996, Downey was diagnosed with lung cancer while being treated for pneumonia, and had one of his lungs removed. He did a complete about-face on the issue of tobacco use, going from a one-time member of the National Smokers Alliance to a staunch anti-smoking activist. He continued to speak against smoking until his death from lung cancer and pneumonia on March 12, 2001.

After being diagnosed with lung cancer, he commented, "I had spawned a generation of kids to think it was cool to smoke a cigarette. Kids walked up to me until a matter of weeks ago, they'd have a cigarette in their hand and they'd say, 'Hey, Mort,' or, 'Hey, Mouth, autograph my cigarette.' And I'd do it." He also blamed tobacco companies for lying to consumers about cigarettes.




</doc>
<doc id="18886" url="https://en.wikipedia.org/wiki?curid=18886" title="List of male singles tennis players">
List of male singles tennis players

This is a list of top international male singles tennis players, both past and present.

It includes players who have been officially ranked among the top 25 singles players in the world during the "Open Era"; been ranked in the top 10 prior to the Open Era; have been a singles quarterfinalist or better at a Grand Slam tournament; have reached the finals of or won the season-ending event; or have been singles medalists at the Olympics.

Players who have won more than one Grand Slam singles title or have been ranked world no. 1 in singles have been put in bold font. Players who are still active on the tour have been put in "italics".



</doc>
<doc id="18887" url="https://en.wikipedia.org/wiki?curid=18887" title="Metaphilosophy">
Metaphilosophy

Metaphilosophy (sometimes called philosophy of philosophy) is "the investigation of the nature of philosophy". Its subject matter includes the aims of philosophy, the boundaries of philosophy, and its methods. Thus, while philosophy characteristically inquires into the nature of being, the reality of objects, the possibility of knowledge, the nature of truth, and so on, metaphilosophy is the self-reflective inquiry into the nature, aims, and methods of the activity that makes these kinds of inquiries, by asking what "is" philosophy itself, what sorts of questions it should ask, how it might pose and answer them, and what it can achieve in doing so. It is considered by some to be a subject prior and preparatory to philosophy, while others see it as inherently a part of philosophy, or automatically a part of philosophy while others adopt some combination of these views. The interest in metaphilosophy led to the establishment of the journal "Metaphilosophy" in January 1970.

Although the "term" metaphilosophy and explicit attention to metaphilosophy as a specific domain within philosophy arose in the 20th century, the topic is likely as old as philosophy itself, and can be traced back at least as far as the works of Plato and Aristotle.

Some philosophers consider metaphilosophy to be a subject apart from philosophy, above or beyond it, while others object to that idea. Timothy Williamson argues that the philosophy of philosophy is "automatically part of philosophy", as is the philosophy of anything else. Nicholas Bunnin and Jiyuan Yu write that the separation of first- from second-order study has lost popularity as philosophers find it hard to observe the distinction. As evidenced by these contrasting opinions, debate persists as to whether the evaluation of the nature of philosophy is 'second-order philosophy' or simply 'plain philosophy'.

Many philosophers have expressed doubts over the value of metaphilosophy. Among them is Gilbert Ryle: "preoccupation with questions about methods tends to distract us from prosecuting the methods themselves. We run as a rule, worse, not better, if we think a lot about our feet. So let us ... not speak of it all but just do it."

The designations "metaphilosophy" and "philosophy of philosophy" have a variety of meanings, sometimes taken to be synonyms, and sometimes seen as distinct.

Morris Lazerowitz claims to have coined the term 'metaphilosophy' around 1940 and used it in print in 1942. Lazerowitz proposed that metaphilosophy is 'the investigation of the nature of philosophy'. Earlier uses have been found in translations from the French. The term is derived from Greek word "meta" μετά ("after", "beyond", "with") and "philosophía" φιλοσοφία ("love of wisdom").

The term 'metaphilosophy' is used by Paul Moser in the sense of a 'second-order' or more fundamental undertaking than philosophy itself, in the manner suggested by Charles Griswold:

This usage was considered nonsense by Ludwig Wittgenstein, who rejected the analogy between metalanguage and a metaphilosophy. As expressed by Martin Heidegger:
Some other philosophers treat the prefix "meta" as simply meaning '"about..."', rather than as referring to a metatheoretical 'second-order' form of philosophy, among them Rescher and Double. Others, such as Williamson, prefer the term "'philosophy of philosophy"' instead of 'metaphilosophy' as it avoids the connotation of a 'second-order' discipline that looks down on philosophy, and instead denotes something that is a part of it. Joll suggests that to take metaphilosophy as 'the application of the methods of philosophy to philosophy itself' is too vague, while the view that sees metaphilosophy as a 'second-order' or more abstract discipline, outside philosophy, "is narrow and tendentious".

In the analytical tradition, the term "metaphilosophy" is mostly used to tag commenting and research on previous works as opposed to original contributions towards solving philosophical problems.

Ludwig Wittgenstein wrote about the nature of philosophical puzzles and philosophical understanding. He suggested philosophical errors arose from confusions about the nature of philosophical inquiry. In the "Philosophical Investigations", Wittgenstein wrote that there is not a metaphilosophy in the sense of a metatheory of philosophy.

C. D. Broad distinguished Critical from Speculative philosophy in his "The Subject-matter of Philosophy, and its Relations to the special Sciences," in "Introduction to Scientific Thought", 1923. Curt Ducasse, in "Philosophy as a Science", examines several views of the nature of philosophy, and concludes that philosophy has a distinct subject matter: appraisals. Ducasse's view has been among the first to be described as 'metaphilosophy'.

Henri Lefebvre in "Métaphilosophie" (1965) argued, from a Marxian standpoint, in favor of an "ontological break", as a necessary methodological approach for critical social theory (whilst criticizing Louis Althusser's "epistemological break" with subjective Marxism, which represented a fundamental theoretical tool for the school of Marxist structuralism).

Paul Moser writes that typical metaphilosophical discussion includes determining the conditions under which a claim can be said to be a philosophical one. He regards meta-ethics, the study of ethics, to be a form of metaphilosophy, as well as meta-epistemology, the study of epistemology.

Many sub-disciplines of philosophy have their own branch of 'metaphilosophy', examples being meta-aesthetics, meta-epistemology, meta-ethics, meta-ontology, and so forth. However, some topics within 'metaphilosophy' cut across the various subdivisions of philosophy to consider fundamentals important to all its sub-disciplines. Some of these are mentioned below.

Some philosophers (e.g. existentialists, pragmatists) think philosophy is ultimately a practical discipline that should help us lead meaningful lives by showing us who we are, how we relate to the world around us and what we should do. Others (e.g. analytic philosophers) see philosophy as a technical, formal, and entirely theoretical discipline, with goals such as "the disinterested pursuit of knowledge for its own sake". Other proposed goals of philosophy include "discover[ing] the absolutely fundamental reason of everything it investigates", "making explicit the nature and significance of ordinary and scientific beliefs", and unifying and transcending the insights given by science and religion. Others proposed that philosophy is a complex discipline because it has 4 or 6 different dimensions.

Defining philosophy and its boundaries is itself problematic; Nigel Warburton has called it "notoriously difficult". There is no straightforward definition, and most interesting definitions are controversial. As Bertrand Russell wrote:

While there is some agreement that philosophy involves general or fundamental topics, there is no clear agreement about a series of demarcation issues, including:



Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.

Recently, some philosophers have cast doubt about intuition as a basic tool in philosophical inquiry, from Socrates up to contemporary philosophy of language. In "Rethinking Intuition" various thinkers discard intuition as a valid source of knowledge and thereby call into question 'a priori' philosophy. Experimental philosophy is a form of philosophical inquiry that makes at least partial use of empirical research—especially "opinion polling"—in order to address persistent philosophical questions. This is in contrast with the methods found in analytic philosophy, whereby some say a philosopher will sometimes begin by appealing to his or her intuitions on an issue and then form an argument with those intuitions as premises. However, disagreement about what experimental philosophy can accomplish is widespread and several philosophers have offered criticisms. One claim is that the empirical data gathered by experimental philosophers can have an indirect effect on philosophical questions by allowing for a better understanding of the underlying psychological processes which lead to philosophical intuitions. Some analytic philosophers like Timothy Williamson have rejected such a move against 'armchair' philosophy–i.e., philosophical inquiry that is undergirded by intuition–by construing 'intuition' (which they believe to be a misnomer) as merely referring to common cognitive faculties: If one is calling into question 'intuition', one is, they would say, harboring a skeptical attitude towards common cognitive faculties–a consequence that seems philosophically unappealing. For Williamson, instances of intuition are instances of our cognitive faculties processing counterfactuals (or subjunctive conditionals) that are specific to the thought experiment or example in question. 

A prominent question in metaphilosophy is that of whether or not philosophical progress occurs and more so, whether such progress in philosophy is even possible. It has even been disputed, most notably by Ludwig Wittgenstein, whether genuine philosophical problems actually exist. The opposite has also been claimed, for example by Karl Popper, who held that such problems do exist, that they are solvable, and that he had actually found definite solutions to some of them.

David Chalmers divides inquiry into philosophical progress in metaphilosophy into three questions.





</doc>
<doc id="18888" url="https://en.wikipedia.org/wiki?curid=18888" title="Mandolin">
Mandolin

A mandolin ( ; literally "small mandola") is a stringed musical instrument in the lute family and is usually plucked with a plectrum. It commonly has four courses of doubled metal strings tuned in unison (8 strings), although five (10 strings) and six (12 strings) course versions also exist. The courses are typically tuned in a succession of perfect fifths, with the same tuning as a violin. It is the soprano member of a family that includes the mandola, octave mandolin, mandocello and mandobass.

There are many styles of mandolin, but three are common, the "Neapolitan" or "round-backed" mandolin, the "carved-top" mandolin and the "flat-backed" mandolin. The round-back has a deep bottom, constructed of strips of wood, glued together into a bowl. The carved-top or "arch-top" mandolin has a much shallower, arched back, and an arched top—both carved out of wood. The flat-backed mandolin uses thin sheets of wood for the body, braced on the inside for strength in a similar manner to a guitar. Each style of instrument has its own sound quality and is associated with particular forms of music. Neapolitan mandolins feature prominently in European classical music and traditional music. Carved-top instruments are common in American folk music and bluegrass music. Flat-backed instruments are commonly used in Irish, British and Brazilian folk music. Some modern Brazilian instruments feature an extra fifth course tuned a fifth lower than the standard fourth course.

Other mandolin varieties differ primarily in the number of strings and include four-string models (tuned in fifths) such as the Brescian and Cremonese, six-string types (tuned in fourths) such as the Milanese, Lombard and the Sicilian and 6 course instruments of 12 strings (two strings per course) such as the Genoese. There has also been a twelve-string (three strings per course) type and an instrument with sixteen-strings (four strings per course).

Much of mandolin development revolved around the soundboard (the top). Pre-mandolin instruments were quiet instruments, strung with as many as six courses of gut strings, and were plucked with the fingers or with a quill. However, modern instruments are louder—using four courses of metal strings, which exert more pressure than the gut strings. The modern soundboard is designed to withstand the pressure of metal strings that would break earlier instruments. The soundboard comes in many shapes—but generally round or teardrop-shaped, sometimes with scrolls or other projections. There are usually one or more "sound holes" in the soundboard, either round, oval, or shaped like a calligraphic (f-hole). A round or oval sound hole may be covered or bordered with decorative rosettes or purfling.

See: History of the mandolin.

Mandolins evolved from lute family instruments in Europe. Predecessors include the gittern and mandore or mandola in Italy during the 17th and 18th centuries. There were a variety of regional variants, but two that were widespread included the Neapolitan mandolin and the Lombardic mandolin. The Neapolitan style has spread worldwide.

Mandolins have a body that acts as a resonator, attached to a neck. The resonating body may be shaped as a bowl () or a box (). Traditional Italian mandolins, such as the Neapolitan mandolin, meet the necked bowl description. The necked box instruments include the carved top mandolins and the flatback mandolins.

Strings run between mechanical tuning machines at the top of the neck to a tailpiece that anchors the other end of the strings. The strings are suspended over the neck and soundboard and pass over a floating bridge. The bridge is kept in contact with the soundboard by the downward pressure from the strings. The neck is either flat or has a slight radius, and is covered with a fingerboard with frets. The action of the strings on the bridge causes the soundboard to vibrate, producing sound.

Like any plucked instrument, mandolin notes decay to silence rather than sound out continuously as with a bowed note on a violin, and mandolin notes decay faster than larger stringed instruments like the guitar. This encourages the use of tremolo (rapid picking of one or more pairs of strings) to create sustained notes or chords. The mandolin's paired strings facilitate this technique: the plectrum (pick) strikes each of a pair of strings alternately, providing a more full and continuous sound than a single string would.

Various design variations and amplification techniques have been used to make mandolins comparable in volume with louder instruments and orchestras, including the creation of mandolin-banjo hybrid with the louder banjo, adding metal resonators (most notably by Dobro and the National String Instrument Corporation) to make a resonator mandolin, and amplifying electric mandolins through amplifiers.

A variety of different tunings are used. Usually, courses of 2 adjacent strings are tuned in unison. By far the most common tuning is the same as violin tuning, in scientific pitch notation G–D–A–E, or in Helmholtz pitch notation: g–d′–a′–e″.


Note that the numbers of Hz shown above assume a 440 Hz A, standard in most parts of the western world. Some players use an A up to 10 Hz above or below a 440, mainly outside the United States.

Other tunings exist, including "cross-tunings", in which the usually doubled string runs are tuned to different pitches. Additionally, guitarists may sometimes tune a mandolin to mimic a portion of the intervals on a standard guitar tuning to achieve familiar fretting patterns.

The mandolin is the soprano member of the mandolin family, as the violin is the soprano member of the violin family. Like the violin, its scale length is typically about . Modern American mandolins modelled after Gibsons have a longer scale, about . The strings in each of its double-strung courses are tuned in unison, and the courses use the same tuning as the violin: G–D–A–E.

The piccolo or sopranino mandolin is a rare member of the family, tuned one octave above the mandola and one fourth above the mandolin (C–G–D–A); the same relation as that of the piccolo or sopranino violin to the violin and viola. One model was manufactured by the Lyon & Healy company under the Leland brand. A handful of contemporary luthiers build piccolo mandolins. Its scale length is typically about .

The mandola (US and Canada), termed the tenor mandola in Britain and Ireland and liola or alto mandolin in continental Europe, which is tuned to a fifth below the mandolin, in the same relationship as that of the viola to the violin. Some also call this instrument the "alto mandola." Its scale length is typically about . It is normally tuned like a viola (fifth below the mandolin) and tenor banjo: C–G–D–A.

The octave mandolin (US and Canada), termed the octave mandola in Britain and Ireland and mandola in continental Europe, is tuned an octave below the mandolin: G–D–A–E. Its relationship to the mandolin is that of the tenor violin to the violin. Octave mandolin scale length is typically about , although instruments with scales as short as or as long as are not unknown. 

Bandol: The instrument has a variant off the coast of South America in Trinidad, where it is known as the bandol, a flat-backed instrument with four courses, the lower two strung with metal and nylon strings.

The Irish bouzouki, although not strictly a member of the mandolin family, has a resemblance and similar range to the octave mandolin. It was derived from the Greek bouzouki (a long-necked lute), constructed like a flat-backed mandolin and uses fifth-based tunings, most often G–D–A–E (an octave below the mandolin)—in which case it essentially functions as an octave mandolin. Common alternate tunings include: G–D–A–D, A–D–A–D or A–D–A–E. Although the Irish bouzouki's bass course pairs are most often tuned in unison, on some instruments one of each pair is replaced with a lighter string and tuned in octaves, in the fashion of the 12-string guitar. While occupying the same range as the octave mandolin/octave mandola, the Irish bouzouki is theoretically distinguished from the former instrument by its longer scale length, typically from , although scales as long as , which is the usual Greek bouzouki scale, are not unknown. In modern usage, however, the terms "octave mandolin" and "Irish bouzouki" are often used interchangeably to refer to the same instrument.

The modern cittern may also be loosely included in an "extended" mandolin family, based on resemblance to the flat-backed mandolins, which it predates. Its own lineage dates it back to the Renaissance. It is typically a five course (ten string) instrument having a scale length between . The instrument is most often tuned to either D–G–D–A–D or G–D–A–D–A, and is essentially an octave mandola with a fifth course at either the top or the bottom of its range. Some luthiers, such as Stefan Sobell also refer to the octave mandola or a shorter-scaled Irish bouzouki as a cittern, irrespective of whether it has four or five courses.

Other relatives of the cittern, which might also be loosely linked to the mandolins (and are sometimes tuned and played as such), include the 6-course/12-string Portuguese guitar and the 5-course/9-string waldzither.

The mandocello, which is classically tuned to an octave plus a fifth below the mandolin, in the same relationship as that of the cello to the violin: C–G–D–A. Its scale length is typically about . A typical violoncello scale is .
The mandolone was a Baroque member of the mandolin family in the bass range that was surpassed by the mandocello. Built as part of the Neapolitan mandolin family.

The Greek laouto or laghouto (long-necked lute) is similar to a mandocello, ordinarily tuned C/C–G/G–D/D–A/A with half of each pair of the lower two courses being tuned an octave high on a lighter gauge string. The body is a staved bowl, the saddle-less bridge glued to the flat face like most ouds and lutes, with mechanical tuners, steel strings, and tied gut frets. Modern laoutos, as played on Crete, have the entire lower course tuned to C, a reentrant octave above the expected low C. Its scale length is typically about .

The Algerian mandole was developed by an Italian luthier in the early 1930s, scaled up from a mandola until it reached a scale length of approximately 25-27 inches. It is a flatback instrument, with a wide neck and 4 courses (8 strings), 5 courses (10 strings) or 6 courses (12 strings). Used in music in Algeria and Morocco. The instrument can be tuned as a guitar, oud or mandocello, depending on the music it will be used to play and player preference. When tuning it as a guitar the strings will be tuned (E) (E) A A D D G G B B (E) (E). Strings in parenthesis are dropped for a five or four course instrument. Using a common Arabic oud tuning D D G G A A D D (G) (G) (C) (C). For a mandocello tuning using fifths C C G G D D A A (E) (E).

The mando-bass most frequently has 4 single strings, rather than double courses, and is typically tuned in fourths like a double bass or a bass guitar: E–A–D–G. These were made by the Gibson company in the early 20th century, but appear to have never been very common. A smaller scale four-string mandobass, usually tuned in fifths: G–D–A–E (two octaves below the mandolin), though not as resonant as the larger instrument, was often preferred by players as easier to handle and more portable. Reportedly, however, most mandolin orchestras preferred to use the ordinary double bass, rather than a specialised mandolin family instrument. Calace and other Italian makers predating Gibson also made mandolin-basses.

The relatively rare eight-string mandobass, or tremolo-bass also exists, with double courses like the rest of the mandolin family, and is tuned either G–D–A–E, two octaves lower than the mandolin, or C–G–D–A, two octaves below the mandola.

Bowlback mandolins (also known as roundbacks), are used worldwide. They are most commonly manufactured in Europe, where the long history of mandolin development has created local styles. However, Japanese luthiers also make them.

Owing to the shape and to the common construction from wood strips of alternating colors, in the United States these are sometimes colloquially referred to as the "potato bug" or "potato beetle" mandolin.

The Neapolitan style has an almond-shaped body resembling a bowl, constructed from curved strips of wood. It usually has a bent sound table, canted in two planes with the design to take the tension of the eight metal strings arranged in four courses. A hardwood fingerboard sits on top of or is flush with the sound table. Very old instruments may use wooden tuning pegs, while newer instruments tend to use geared metal tuners. The bridge is a movable length of hardwood. A pickguard is glued below the sound hole under the strings. European roundbacks commonly use a scale instead of the common on archtop Mandolins.

Intertwined with the Neapolitan style is the Roman style mandolin, which has influenced it. The Roman mandolin had a fingerboard that was more curved and narrow. The fingerboard was lengthened over the sound hole for the E strings, the high pitched strings. The shape of the back of the neck was different, less rounded with an edge, the bridge was curved making the G strings higher. The Roman mandolin had mechanical tuning gears before the Neapolitan.

Prominent Italian manufacturers include Vinaccia (Naples), Embergher (Rome) and Calace (Naples). Other modern manufacturers include Lorenzo Lippi (Milan), Hendrik van den Broek (Netherlands), Brian Dean (Canada), Salvatore Masiello and Michele Caiazza (La Bottega del Mandolino) and Ferrara, Gabriele Pandini.

In the United States, when the bowlback was being made in numbers, Lyon and Healy was a major manufacturer, especially under the "Washburn" brand. Other American manufacturers include Martin, Vega, and Larson Brothers.

In Canada, Brian Dean has manufactured instruments in Neapolitan, Roman, German and American styles but is also known for his original 'Grand Concert' design created for American virtuoso Joseph Brent.

German manufacturers include Albert & Mueller, Dietrich, Klaus Knorr, Reinhold Seiffert and Alfred Woll. The German bowlbacks use a style developed by Seiffert, with a larger and rounder body.

Japanese brands include Kunishima and Suzuki. Other Japanese manufacturers include Oona, Kawada, Noguchi, Toichiro Ishikawa, Rokutaro Nakade, Otiai Tadao, Yoshihiko Takusari, Nokuti Makoto, Watanabe, Kanou Kadama and Ochiai.

Another family of bowlback mandolins came from Milan and Lombardy. These mandolins are closer to the mandolino or mandore than other modern mandolins. They are shorter and wider than the standard Neapolitan mandolin, with a shallow back. The instruments have 6 strings, 3 wire treble-strings and 3 gut or wire-wrapped-silk bass-strings. The strings ran between the tuning pegs and a bridge that was glued to the soundboard, as a guitar's. The Lombardic mandolins were tuned g–b–e′–a′–d″–g″ (shown in Helmholtz pitch notation). A developer of the Milanese stye was Antonio Monzino (Milan) and his family who made them for 6 generations.

Samuel Adelstein described the Lombardi mandolin in 1893 as wider and shorter than the Neapolitan mandolin, with a shallower back and a shorter and wider neck, with six single strings to the regular mandolin's set of 4. The Lombardi was tuned C–D–A–E–B–G. The strings were fastened to the bridge like a guitar's. There were 20 frets, covering three octaves, with an additional 5 notes. When Adelstein wrote, there were no nylon strings, and the gut and single strings "do not vibrate so clearly and sweetly as the double steel string of the Neapolitan."

Brescian mandolins (also known as Cremonese) that have survived in museums have four gut strings instead of six and a fixed bridge. The mandolin was tuned in fifths, like the Neapolitan mandolin. In his 1805 mandolin method, "Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi", Bartolomeo Bortolazzi popularised the Cremonese mandolin, which had four single-strings and a fixed bridge, to which the strings were attached. Bortolazzi said in this book that the new wire strung mandolins were uncomfortable to play, when compared with the gut-string instruments. Also, he felt they had a "less pleasing...hard, zither-like tone" as compared to the gut string's "softer, full-singing tone."
He favored the four single strings of the Cremonese instrument, which were tuned the same as the Neapolitan.

Like the Lombardy mandolin, the Genoese mandolin was not tuned in fifths. Its 6 gut strings (or 6 courses of strings) were tuned as a guitar but one octave higher: e-a-d’-g’-b natural-e”. Like the Neapolitan and unlike the Lombardy mandolin, the Genoese does not have the bridge glued to the soundboard, but holds the bridge on with downward tension, from strings that run between the bottom and neck of the instrument. The neck was wider than the Neapolitan mandolin's neck. The peg-head is similar to the guitar's.

At the very end of the 19th century, a new style, with a carved top and back construction inspired by violin family instruments began to supplant the European-style bowl-back instruments in the United States. This new style is credited to mandolins designed and built by Orville Gibson, a Kalamazoo, Michigan luthier who founded the "Gibson Mandolin-Guitar Manufacturing Co., Limited" in 1902. Gibson mandolins evolved into two basic styles: the Florentine or F-style, which has a decorative scroll near the neck, two points on the lower body and usually a scroll carved into the headstock; and the A-style, which is pear shaped, has no points and usually has a simpler headstock.

These styles generally have either two f-shaped soundholes like a violin (F-5 and A-5), or an oval sound hole (F-4 and A-4 and lower models) directly under the strings. Much variation exists between makers working from these archetypes, and other variants have become increasingly common. Generally, in the United States, Gibson F-hole F-5 mandolins and mandolins influenced by that design are strongly associated with bluegrass, while the A-style is associated other types of music, although it too is most often used for and associated with bluegrass. The F-5's more complicated woodwork also translates into a more expensive instrument.

Internal bracing to support the top in the F-style mandolins is usually achieved with parallel tone bars, similar to the bass bar on a violin. Some makers instead employ "X-bracing," which is two tone bars mortised together to form an X. Some luthiers now using a "modified x-bracing" that incorporates both a tone bar and X-bracing.

Numerous modern mandolin makers build instruments that largely replicate the Gibson F-5 Artist models built in the early 1920s under the supervision of Gibson acoustician Lloyd Loar. Original Loar-signed instruments are sought after and extremely valuable. Other makers from the Loar period and earlier include Lyon and Healy, Vega and Larson Brothers.

Flatback mandolins use a thin sheet of wood with bracing for the back, as a guitar uses, rather than the bowl of the bowlback or the arched back of the carved mandolins.

Like the bowlback, the flatback has a round sound hole. This has been sometimes modified to an elongated hole, called a D-hole. The body has a rounded almond shape with flat or sometimes canted soundboard.

The type was developed in Europe in the 1850s. The French and Germans called it a Portuguese mandolin, although they also developed it locally. The Germans used it in Wandervogel.

The bandolim is commonly used wherever the Spanish and Portuguese took it: in South America, in Brazil (Choro) and in the Philippines.

In the early 1970s English luthier Stefan Sobell developed a large-bodied, flat-backed mandolin with a carved soundboard, based on his own cittern design; this is often called a 'Celtic' mandolin.

American forms include the Army-Navy mandolin, the flatiron and the pancake mandolins.

The tone of the flatback is described as warm or mellow, suitable for folk music and smaller audiences. The instrument sound does not punch through the other players' sound like a carved top does.

The double top is a feature that luthiers are experimenting with in the 21st century, to get better sound. 
However, mandolinists and luthiers have been experimenting with them since at least the early 1900s.

Back in the early 1900s, mandolinist Ginislao Paris approached Luigi Embergher to build custom mandolins. The sticker inside one of the four surviving instruments indicates the build was called after him, the "Sistema Ginislao Paris"). Paris' round-back double-top mandolins use a false back below the soundboard to create a second hollow space within the instrument.

Modern mandolinists such as Joseph Brent and Avi Avital use instruments customized, either by the luthier's choice or at the request of player. Joseph Brent's mandolin, made by Brian Dean also uses what Brent calls a false back. Brent's mandolin was the luthier's solution to Brent's request for a loud mandolin in which the wood was clearly audible, with less metallic sound from the strings. The type used by Avital is variation of the flatback, with a double top that encloses a resonating chamber, sound holes on the side, and a convex back. It is made by one manufacturer in Israel, luthier Arik Kerman. Other players of Kerman mandolins include Alon Sariel, Jacob Reuven, and Tom Cohen.

Other American-made variants include the mandolinetto or Howe-Orme guitar-shaped mandolin (manufactured by the Elias Howe Company between 1897 and roughly 1920), which featured a cylindrical bulge along the top from fingerboard end to tailpiece and the Vega mando-lute (more commonly called a cylinder-back mandolin manufactured by the Vega Company between 1913 and roughly 1927), which had a similar longitudinal bulge but on the back rather than the front of the instrument.

The mandolin was given a banjo body in an 1882 patent by Benjamin Bradbury of Brooklyn and given the name "banjolin" by John Farris in 1885. Today "banjolin" describes an instrument with four strings, while the version with the four courses of double strings is called a "mandolin-banjo".

A resonator mandolin or "resophonic mandolin" is a mandolin whose sound is produced by one or more metal cones (resonators) instead of the customary wooden soundboard (mandolin top/face). Historic brands include Dobro and National.

As with almost every other contemporary string instrument, another modern variant is the electric mandolin. These mandolins can have four or five individual or double courses of strings.

They have been around since the late 1920s or early 1930s depending on the brand. They come in solid body and acoustic electric forms.

Instruments have been designed that overcome the mandolin's lack of sustain with its plucked notes. Fender released a model in 1992 with an additional string (a high a, above the e string), a tremolo bridge and extra humbucker pickup (total of two). The result was an instrument capable of playing heavy metal style guitar riffs or violin-like passages with sustained notes that can be adjusted as with an electric guitar.

See Mandolin playing traditions worldwide and History of the mandolin
The international repertoire of music for mandolin is almost unlimited, and musicians use it to play various types of music. This is especially true of violin music, since the mandolin has the same tuning as the violin. Following its invention and early development in Italy the mandolin spread throughout the European continent. The instrument was primarily used in a classical tradition with Mandolin orchestras, so called "Estudiantinas" or in Germany "Zupforchestern" appearing in many cities. Following this continental popularity of the mandolin family local traditions appeared outside Europe in the Americas and in Japan. Travelling mandolin virtuosi like Giuseppe Pettine, Raffaele Calace and Silvio Ranieri contributed to the mandolin becoming a "fad" instrument in the early 20th century. This "mandolin craze" was fading by the 1930s, but just as this practice was falling into disuse, the mandolin found a new niche in American country, old-time music, bluegrass and folk music. More recently, the Baroque and Classical mandolin repertory and styles have benefited from the raised awareness of and interest in Early music, with media attention to classical players such as Israeli Avi Avital, Italian Carlo Aonzo and American Joseph Brent.

The tradition of so-called "classical music" for the mandolin has been somewhat spotty, due to its being widely perceived as a "folk" instrument. Significant composers did write music specifically for the mandolin, but few "large" works were composed for it by the most widely regarded composers. The total number of works these works is rather small in comparison to—say—those composed for violin. One result of this dearth being that there were few positions for mandolinists in regular orchestras. To fill this gap in the literature, mandolin orchestras have traditionally played many arrangements of music written for regular orchestras or other ensembles. Some players have sought out contemporary composers to solicit new works.

Furthermore, of the works that have been written for mandolin from the 18th century onward, many have been lost or forgotten. Some of these await discovery in museums and libraries and archives. One example of rediscovered 18th-century music for mandolin and ensembles with mandolins is the "Gimo collection", collected in the first half of 1762 by Jean Lefebure. Lefebure collected the music in Italy, and it was forgotten until manuscripts were rediscovered.

Vivaldi created some concertos for mandolinos and orchestra: one for 4-chord mandolino, string bass & continuous in C major, (RV 425), and one for two 5-chord mandolinos, bass strings & continuous in G major, (RV 532), and concerto for two mandolins, 2 violons "in Tromba"—2 flûtes à bec, 2 salmoe, 2 théorbes, violoncelle, cordes et basse continuein in C major (p. 16).

Beethoven composed mandolin music and enjoyed playing the mandolin. His 4 small pieces date from 1796: Sonatine WoO 43a; Adagio ma non troppo WoO 43b; Sonatine WoO 44a and Andante con Variazioni WoO 44b.

The opera "Don Giovanni" by Mozart (1787) includes mandolin parts, including the accompaniment to the famous aria "Deh vieni alla finestra", and Verdi's opera Otello calls for guzla accompaniment in the aria "Dove guardi splendono raggi", but the part is commonly performed on mandolin.

Gustav Mahler used the mandolin in his Symphony No. 7, Symphony No. 8 and Das Lied von der Erde.

Parts for mandolin are included in works by Schoenberg (Variations Op. 31), Stravinsky (Agon), Prokofiev (Romeo and Juliet) and Webern (opus Parts 10)

Some 20th century composers also used the mandolin as their instrument of choice (amongst these are: Schoenberg, Webern, Stravinsky and Prokofiev).

Among the most important European mandolin composers of the 20th century are Raffaele Calace (composer, performer and luthier) and Giuseppe Anedda (virtuoso concert pianist and professor of the first chair of the Conservatory of Italian Mandolin, Padua, 1975). Today representatives of Italian classical music and Italian classical-contemporary music include Ugo Orlandi, Carlo Aonzo, Dorina Frati, Mauro Squillante and Duilio Galfetti.

Japanese composers also produced orchestral music for mandolin in the 20th century, but these are not well known outside Japan.

Traditional mandolin orchestras remain especially popular in Japan and Germany, but also exist throughout the United States, Europe and the rest of the world. They perform works composed for mandolin family instruments, or re-orchestrations of traditional pieces. The structure of a contemporary traditional mandolin orchestra consists of: first and second mandolins, mandolas (either octave mandolas, tuned an octave below the mandolin, or tenor mandolas, tuned like the viola), mandocellos (tuned like the cello), and bass instruments (conventional string bass or, rarely, mandobasses). Smaller ensembles, such as quartets composed of two mandolins, mandola, and mandocello, may also be found.

































A duet or duo is a musical composition for two performers in which the performers have equal importance to the piece. A musical ensemble with more than two solo instruments or voices is called trio, quartet, quintet, sextet, septet, octet, etc.


































Concerto: a musical composition generally composed of three movements, in which, usually, one solo instrument (for instance, a piano, violin, cello or flute) is accompanied by an orchestra or concert band.




















Orchestral works in which the mandolin has a limited part.






















Chord dictionaries

Method and instructional guides



</doc>
<doc id="18889" url="https://en.wikipedia.org/wiki?curid=18889" title="Microphotonics">
Microphotonics

Microphotonics is a branch of technology that deals with directing light on a microscopic scale and is used in optical networking. Particularly, it refers to the branch of technology that deals with wafer-level integrated devices and systems that emit, transmit, detect, and process light along with other forms of radiant energy with photon as the quantum unit.

Microphotonics employs at least two different materials with a large differential index of refraction to squeeze the light down to a small size. Generally speaking, virtually all of microphotonics relies on Fresnel reflection to guide the light. If the photons reside mainly in the higher index material, the confinement is due to total internal reflection. If the confinement is due many distributed Fresnel reflections, the device is termed a photonic crystal. There are many different types of geometries used in microphotonics including optical waveguides, optical microcavities, and Arrayed waveguide gratings.

Photonic crystals are non-conducting materials that reflect various wavelengths of light almost perfectly. Such a crystal can be referred to as a perfect mirror. Other devices employed in microphotonics include micromirrors and photonic wire waveguides. These tools are used to "mold the flow of light", a famous phrase for describing the goal of microphotonics. The crystals serve as structures that allow the manipulation, confinement, and control of light in one, two, or three dimensions of space.

An optical microdisk, optical microtoroid, or optical microsphere uses internal reflection in a circular geometry to hold on to the photons. This type of circularly symmetric optical resonance is called a Whispering gallery mode, after Lord Rayleigh coined the term.

Microphotonics has biological applications and these can be demonstrated in the case of the "biophotonic chips", which are developed to increase efficiency in terms of "photonic yield" or the collected luminescent signal emitted by fluorescent markers used in biological chips.

Currently, microphotonics technology is also being developed to replace electronics devices and bio-compatible intracellular devices. For instance, the long-standing goal of an all-optical router would eliminate electronic bottlenecks, speeding up the network. Perfect mirrors are being developed for use in fiber optic cables.



</doc>
<doc id="18890" url="https://en.wikipedia.org/wiki?curid=18890" title="Microsoft Windows">
Microsoft Windows

Microsoft Windows, commonly referred to as Windows, is a group of several proprietary graphical operating system families, all of which are developed and marketed by Microsoft. Each family caters to a certain sector of the computing industry. Active Microsoft Windows families include Windows NT and Windows IoT; these may encompass subfamilies, e.g. Windows Server or Windows Embedded Compact (Windows CE). Defunct Microsoft Windows families include Windows 9x, Windows Mobile and Windows Phone.

Microsoft introduced an operating environment named "Windows" on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993). On PCs, Windows is still the most popular operating system. However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android, because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison, however, may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to that for end user use. 

, the most recent version of Windows for PCs, tablets, smartphones and embedded devices is Windows 10. The most recent version for server computers is Windows Server, version 1903. A specialized version of Windows also runs on the Xbox One video game console.

Microsoft, the developer of Windows, has registered several trademarks, each of which denote a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families were being actively developed:


The following Windows families are no longer being developed:


The term "Windows" collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:

The history of Windows dates back to 1981, when Microsoft started work on a program called "Interface Manager". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.

Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.

Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area. 

In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.

The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.

Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.

Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.

The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.

Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.

On September 14, 2000, Microsoft released Windows Me (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. "PC World" considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.

In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.

The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.

The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.

At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.

After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005, by Windows Server 2003 R2.

After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.

On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements.

Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels, effectively making it unfit for netbooks with 800x600-pixel screens.

Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 have been subject to some criticism, such as removal of the Start menu.

On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1, Windows 8.1 and Windows Phone 8.1 devices from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).

In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300 gigabyte repository. By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.

Multilingual support has been built into Windows since Windows 3. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.

Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).

The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.

Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.

Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.

With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.

On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.

An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from Windows 10 Fall Creators Update and later includes support for PCs with ARM architecture.

Windows CE (officially known as "Windows Embedded Compact"), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.

Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.

Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.

Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC. The Windows 10-based Core had replaced the Windows 8-based one in this update, and the new system is sometimes referred to as "Windows 10 on Xbox One" or "OneCore".
Xbox One's system also allows backward compatibility with Xbox 360, and the Xbox 360's system is backwards compatible with the original Xbox.

In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds. Microsoft has previously used a proprietary version control system called "Source Depot". Microsoft had begun to integrate Git into Team Foundation Server in 2013, but Windows continued to rely on Source Depot. Because of its large, decades-long history, the Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage. Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the repository takes several hours. Microsoft has been working on a new project called the Virtual File System for Git (VFSForGit) to address these challenges.

According to Net Applications, which tracks the use of operating systems in devices that are active on the Web, Windows was the most used operating-system family on personal computers in July 2017, with close to 90% usage share. Including personal computers of all kinds (e.g., desktops, laptops, mobile devices), Windows OSes accounted for 35.24% of usage share in July 2017, compared to Android (highest, at 41.24%), iOS's 13.22%, and macOS's 4.64%, according to StatCounter, which tracks use of operating systems by their use in devices active on the Web. Windows is used in less than half the market not only in developing countries, but also in developed ones—such as the United States, where use of Windows on desktops, on which it is the plurality operating system, has fallen to 46.18%, and the United Kingdom and Ireland. These numbers are easiest (monthly numbers) to find that track real use, but they may not mirror installed base or sales numbers (in recent years) of devices. They are consistent with server numbers in next section.

In terms of the number of devices shipped with the operating system installed, on smartphones, Windows Phone was the third-most-shipped OS (2.6%) after Android (82.8%) and iOS (13.9%) in the second quarter of 2015 according to IDC. Across both PCs and mobile devices, in 2014, Windows OSes were the second-most-shipped (333 million devices, or 14%) after Android (1.2 billion, 49%) and ahead of iOS and macOS combined (263 million, 11%).

Use of the latest version Windows 10 has exceeded Windows 7 globally since early 2018.

Usage share of Windows on serversthose running a web servers that is (there are also other kinds of servers) is at 33.6%.

Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.

These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's "Counterpane Internet Security" reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs—viruses, Trojans, back-doors, and exploits written for Windows.

Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.

While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.

The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.

Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.

Leaked documents published by WikiLeaks, codenamed Vault 7 and dated from 2013–2016, detail the capabilities of the CIA to perform electronic surveillance and cyber warfare, such as the ability to compromise operating systems such as Microsoft Windows.

In August 2019, computer experts reported that the BlueKeep security vulnerability, , that potentially affects older unpatched Microsoft Windows versions via the program's Remote Desktop Protocol, allowing for the possibility of remote code execution, may now include related flaws, collectively named "DejaBlue", affecting newer Windows versions (i.e., Windows 7 and all recent versions) as well. In addition, experts reported a Microsoft security vulnerability, , based on legacy code involving Microsoft CTF and ctfmon (ctfmon.exe), that affects all Windows versions from the older Windows XP version to the most recent Windows 10 versions; a patch to correct the flaw is currently available.

All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.

On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7. In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials, borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution. Windows Malicious Software Removal Tool and the optional Microsoft Safety Scanner are two other free security products offered by Microsoft. In the Windows 10 Anniversary Update, Microsoft introduced the Limited Periodic Scanning feature, which allows Windows Defender to scan, detect, and remove any threats that third-party anti-virus software missed. The Advanced Threat Protection service is introduced for enterprise users. The new service uses cloud service to detect and take actions on advanced network attacks.

In an article based on a report by Symantec, internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."

A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004, found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only four minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours. The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004, determined that 80% of Windows users were infected by at least one spyware/adware product. Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.

Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:





</doc>
<doc id="18892" url="https://en.wikipedia.org/wiki?curid=18892" title="Mojo (African-American culture)">
Mojo (African-American culture)

Mojo , in the African-American folk belief called hoodoo, is an amulet consisting of a flannel bag containing one or more magical items. It is a "prayer in a bag", or a spell that can be carried with or on the host's body.

Alternative American names for the mojo bag include hand, mojo hand, conjure hand, lucky hand, conjure bag, trick bag, root bag, toby, jomo, and gris-gris bag.

The term "mojo" is now commonly used in the English language to mean one's personal talent or gift. For example, a person might say that they are "getting their mojo on" when trying to get the attention of a possible mate.

The most common synonym for the word mojo is "gris-gris", which literally means "fetish" or "charm"; thus a gris-gris bag is a charm bag. In the Caribbean, an almost identical African-derived bag is called a "wanga" or "oanga" bag, but that term is uncommon in the United States. The word "conjure" is an ancient alternative to "hoodoo", which is a direct variation of African-American folklore. Because of this, a conjure hand is also considered a hoodoo bag, usually made by a respected community conjure doctor.

The word "hand" in this context is defined as a combination of ingredients. The term may derive from the use of finger and hand bones from the dead in mojo bags, or from ingredients such as the lucky hand root (favored by gamblers). The latter suggests an analogy between the varied bag ingredients and the several cards that make up a hand in card games. Mojo reaches as far back as West African culture, where it is said to drive away evil spirits, keep good luck in the household, manipulate a fortune, and lure and persuade lovers. The ideology of the ancestors and the descendants of the mojo hand used this "prayer in a bag" based on their belief of spiritual inheritance, by which the omniscient forefathers of their families would provide protection and favor, especially when they used the mojo. Through this, a strong belief was placed in the idealism of whomever used mojo, creating a spiritual trust in the magic itself.

Although most Southern-style conjure bags are made of red flannel material, most seasoned conjurers use color symbolism. This practice embodies itself in the practice of hoodoo, in which green flannel is used for a money mojo, white flannel is used for a baby-blessing mojo, red flannel is used for love mojo, and so on. West Indians also use mojo bags but often use leather instead of flannel.

The contents of each bag vary directly with the aim of the conjurer. For example, a mojo carried for love-drawing will contain different ingredients than one for gambling luck or magical protection. Ingredients can include roots, herbs, animal parts, minerals, coins, crystals, good luck tokens, and carved amulets. The more personalized objects are used to add extra power because of their symbolic value.

There is a process to fixing a proper mojo. A ritual must be put in place in order to successfully prepare a mojo by being filled and awakened to life. This can be done by smoking incense and candles, or it may be breathed upon to bring it to life. Prayers may be said, and other methods may be used to accomplish this essential step. Once prepared, the mojo is "dressed" or "fed" with a liquid such as alcohol, perfume, water, or bodily fluids. The reason it is said to feed the mojo to keep it working is that it is alive with spirit. One story from the work entitled "From My People" describes a slave who went out and sought a mojo conjurer that gave him a mojo to run away from home. The story describes the slave's mojo as fixing him into many formations, and he ultimately dies because he misuses its power. Had he fixed and believed in the specific mojo himself, he might have escaped the plantation alive.

Mojos are traditionally made for an individual and so must be concealed on the person at all times. Men usually keep the trinkets hidden in the pants pocket, while women are more prone to clip it to the bra. They are also commonly pinned to clothes below the waist. Depending on the type of mojo, the hiding place will be crucial to its success, as those who make conjure bags to carry love spells sometimes specify that the mojo must be worn next to the skin. A story from the book "From My People" described the story of Moses and the task he went through to bring his people out of slavery. It described how "Hoodoo Lost his Hand", as Moses's mojo was hidden through his staff. When he turned it into a snake, the pharaoh made his soothsayers and magicians create the same effect. As a result, the Pharaoh's snake was killed by Moses's snake, and that is how Hoodoo lost his hand.


</doc>
<doc id="18894" url="https://en.wikipedia.org/wiki?curid=18894" title="Matt Groening">
Matt Groening

Matthew Abraham Groening ( ; (born February 15, 1954) is an American cartoonist, writer, producer, animator, and voice actor. He is the creator of the comic strip "Life in Hell" (1977–2012) and the television series "The Simpsons" (1989–present), "Futurama" (1999–2003, 2008–2013), and "Disenchantment" (2018–present). "The Simpsons" is the longest-running U.S. primetime-television series in history and the longest-running U.S. animated series and sitcom.

Groening made his first professional cartoon sale of "Life in Hell" to the avant-garde "Wet" magazine in 1978. At its peak, the cartoon was carried in 250 weekly newspapers. "Life in Hell" caught the attention of James L. Brooks. In 1985, Brooks contacted Groening with the proposition of working in animation for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Fearing the loss of ownership rights, Groening decided to create something new and came up with a cartoon family, the Simpson family, and named the members after his own parents and sisters—while Bart was an anagram of the word "brat". The shorts would be spun off into their own series "The Simpsons", which has since aired episodes. In 1997, Groening and former "Simpsons" writer David X. Cohen developed "Futurama", an animated series about life in the year 3000, which premiered in 1999, running for four years on Fox, then picked up by Comedy Central for additional seasons. In 2016, Groening developed a new series for Netflix titled "Disenchantment", which premiered in August 2018.

Groening has won thirteen Primetime Emmy Awards, eleven for "The Simpsons" and two for "Futurama" as well as a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2002, he won the National Cartoonist Society Reuben Award for his work on "Life in Hell". He received a star on the Hollywood Walk of Fame on February 14, 2012.

Groening was born on February 15, 1954 in Portland, Oregon, the middle of five children (older brother Mark and sister Patty were born in 1950 and 1952, while the younger sisters Lisa and Maggie in 1956 and 1958, respectively). His Norwegian American mother, Margaret Ruth (née Wiggum; March 23, 1919 – April 22, 2013), was once a teacher, and his German Canadian father, Homer Philip Groening (December 30, 1919 – March 15, 1996), was a filmmaker, advertiser, writer and cartoonist. Homer, born in Main Centre, Saskatchewan, Canada, grew up in a Mennonite, Plautdietsch (German)-speaking family.

Matt's grandfather, Abraham Groening, was a professor at Tabor College, a Mennonite Brethren liberal arts college in Hillsboro, Kansas before moving to Albany College (now known as Lewis and Clark College) in Oregon in 1930.

Groening grew up in Portland, and attended Ainsworth Elementary School and Lincoln High School. From 1972 to 1977, Groening attended The Evergreen State College in Olympia, Washington, a liberal arts school that he described as "a hippie college, with no grades or required classes, that drew every weirdo in the Northwest." He served as the editor of the campus newspaper, "The Cooper Point Journal", for which he also wrote articles and drew cartoons. He befriended fellow cartoonist Lynda Barry after discovering that she had written a fan letter to Joseph Heller, one of Groening's favorite authors, and had received a reply. Groening has credited Barry with being "probably [his] biggest inspiration." He first became interested in cartoons after watching the Disney animated film "One Hundred and One Dalmatians", and he has also cited Robert Crumb, Ernie Bushmiller, Ronald Searle, Monty Python, and Charles M. Schulz as inspirations.

In 1977, at the age of 23, Groening moved to Los Angeles to become a writer. He went through what he described as "a series of lousy jobs," including being an extra in the television movie "When Every Day Was the Fourth of July", busing tables, washing dishes at a nursing home, clerking at the Hollywood Licorice Pizza record store, landscaping in a sewage treatment plant, and chauffeuring and ghostwriting for a retired Western director.

Groening described life in Los Angeles to his friends in the form of the self-published comic book "Life in Hell", which was loosely inspired by the chapter "How to Go to Hell" in Walter Kaufmann's book "Critique of Religion and Philosophy". Groening distributed the comic book in the book corner of Licorice Pizza, a record store in which he worked. He made his first professional cartoon sale to the avant-garde "Wet" magazine in 1978. The strip, titled "Forbidden Words," appeared in the September/October issue of that year.

Groening had gained employment at the "Los Angeles Reader", a newly formed alternative newspaper, delivering papers, typesetting, editing and answering phones. He showed his cartoons to the editor, James Vowell, who was impressed and eventually gave him a spot in the paper. "Life in Hell" made its official debut as a comic strip in the "Reader" on April 25, 1980. Vowell also gave Groening his own weekly music column, "Sound Mix," in 1982. However, the column would rarely actually be about music, as he would often write about his "various enthusiasms, obsessions, pet peeves and problems" instead. In an effort to add more music to the column, he "just made stuff up," concocting and reviewing fictional bands and nonexistent records. In the following week's column, he would confess to fabricating everything in the previous column and swear that everything in the new column was true. Eventually, he was finally asked to give up the "music" column. Among the fans of the column was Harry Shearer, who would later become a voice on "The Simpsons".

"Life in Hell" became popular almost immediately. In November 1984, Deborah Caplan, Groening's then-girlfriend and co-worker at the "Reader", offered to publish "Love is Hell", a series of relationship-themed "Life in Hell" strips, in book form. Released a month later, the book was an underground success, selling 22,000 copies in its first two printings. "Work is Hell" soon followed, also published by Caplan. Soon afterward, Caplan and Groening left and put together the Life in Hell Co., which handled merchandising for "Life in Hell". Groening also started Acme Features Syndicate, which initially syndicated "Life in Hell" as well as work by Lynda Barry and John Callahan, but would eventually only syndicate "Life in Hell". At the end of its run, "Life in Hell" was carried in 250 weekly newspapers and has been anthologized in a series of books, including "School is Hell", "Childhood is Hell", "The Big Book of Hell", and "The Huge Book of Hell". Although Groening previously stated, "I'll never give up the comic strip. It's my foundation," the June 16, 2012 strip marked "Life in Hell"s conclusion. After Groening ended the strip, the Center for Cartoon Studies commissioned a poster that was presented to Groening in honor of his work. The poster contained tribute cartoons by 22 of Groening's cartoonist friends who were influenced by "Life in Hell".

"Life in Hell" caught the attention of Hollywood writer-producer and Gracie Films founder James L. Brooks, who had been shown the strip by fellow producer Polly Platt. In 1985, Brooks contacted Groening with the proposition of working in animation on an undefined future project, which would turn out to be developing a series of short animated skits, called "bumpers," for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Groening feared that he would have to give up his ownership rights, and that the show would fail and would take down his comic strip with it. Groening conceived of the idea for the Simpsons in the lobby of James L. Brooks's office and hurriedly sketched out his version of a dysfunctional family: Homer, the overweight father; Marge, the slim mother; Bart, the bratty oldest child; Lisa, the intelligent middle child; and Maggie, the baby. Groening famously named the main Simpson characters after members of his own family: his parents, Homer and Marge (Margaret or Marjorie in full), and his younger sisters, Lisa and Margaret (Maggie). Claiming that it was a bit too obvious to name a character after himself, he chose the name "Bart," an anagram of brat. However, he stresses that aside from some of the sibling rivalry, his family is nothing like the Simpsons. Groening also has an older brother and sister, Mark and Patty, and in a 1995 interview Groening divulged that Mark "is the actual inspiration for Bart."

Maggie Groening has co-written a few "Simpsons" books featuring her cartoon namesake.

The family was crudely drawn, because Groening had submitted basic sketches to the animators, assuming they would clean them up; instead, they just traced over his drawings. The entire Simpson family was designed so that they would be recognizable in silhouette. When Groening originally designed Homer, he put his own initials into the character's hairline and ear: the hairline resembled an 'M', and the right ear resembled a 'G'. Groening decided that this would be too distracting though, and redesigned the ear to look normal. He still draws the ear as a 'G' when he draws pictures of Homer for fans. Marge's distinct beehive hairstyle was inspired by "Bride of Frankenstein" and the style that Margaret Groening wore during the 1960s, although her hair was never blue. Bart's original design, which appeared in the first shorts, had spikier hair, and the spikes were of different lengths. The number was later limited to nine spikes, all of the same size. At the time Groening was primarily drawing in black and "not thinking that [Bart] would eventually be drawn in color" gave him spikes that appear to be an extension of his head. Lisa's physical features are generally not used in other characters; for example, in the later seasons, no character other than Maggie shares her hairline. While designing Lisa, Groening "couldn't be bothered to even think about girls' hair styles". When designing Lisa and Maggie, he "just gave them this kind of spiky starfish hair style, not thinking that they would eventually be drawn in color". Groening storyboarded and scripted every short (now known as "The Simpsons shorts"), which were then animated by a team including David Silverman and Wes Archer, both of whom would later become directors on the series.

The Simpsons shorts first appeared in "The Tracey Ullman Show" on April 19, 1987. Another family member, Grampa Simpson, was introduced in the later shorts. Years later, during the early seasons of "The Simpsons", when it came time to give Grampa a first name, Groening says he refused to name him after his own grandfather, Abraham Groening, leaving it to other writers to choose a name. By coincidence, they chose "Abraham", unaware that it was the name of Groening's grandfather.

Although "The Tracey Ullman Show" was not a big hit, the popularity of the shorts led to a half-hour spin-off in 1989. A team of production companies adapted "The Simpsons" into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. James L. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content. Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching. The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special. "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.

The series quickly became a worldwide phenomenon, to the surprise of many. Groening said: "Nobody thought "The Simpsons" was going to be a big hit. It sneaked up on everybody." "The Simpsons" was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening and Simon, however, did not get along and were often in conflict over the show; Groening once described their relationship as "very contentious." Simon eventually left the show in 1993 over creative differences.

Like the main family members, several characters from the show have names that were inspired by people, locations or films. The name "Wiggum" for police chief Chief Wiggum is Groening's mother's maiden name. The names of a few other characters were taken from major street names in Groening's hometown of Portland, Oregon, including Flanders, Lovejoy, Powell, Quimby and Kearney. Despite common fan belief that Sideshow Bob Terwilliger was named after SW Terwilliger Boulevard in Portland, he was actually named after the character Dr. Terwilliker from the film "The 5,000 Fingers of Dr. T".

Although Groening has pitched a number of spin-offs from "The Simpsons", those attempts have been unsuccessful. In 1994, Groening and other "Simpsons" producers pitched a live-action spin-off about Krusty the Clown (with Dan Castellaneta playing the lead role), but were unsuccessful in getting it off the ground. Groening has also pitched "Young Homer" and a spin-off about the non-Simpsons citizens of Springfield.

In 1995, Groening got into a major disagreement with Brooks and other "Simpsons" producers over "A Star Is Burns", a crossover episode with "The Critic", an animated show also produced by Brooks and staffed with many former "Simpsons" crew members. Groening claimed that he feared viewers would "see it as nothing but a pathetic attempt to advertise "The Critic" at the expense of "The Simpsons"," and was concerned about the possible implication that he had created or produced "The Critic". He requested his name be taken off the episode.

Groening is credited with writing or co-writing the episodes "Some Enchanted Evening", "The Telltale Head", "Colonel Homer" and "22 Short Films About Springfield", as well as "The Simpsons Movie", released in 2007. He has had several cameo appearances in the show, with a speaking role in the episode "My Big Fat Geek Wedding". He currently serves at "The Simpsons" as an executive producer and creative consultant.

After spending a few years researching science fiction, Groening got together with "Simpsons" writer/producer David X. Cohen (known as David S. Cohen at the time) in 1997 and developed "Futurama", an animated series about life in the year 3000. By the time they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and storylines; Groening claimed they had gone "overboard" in their discussions. Groening described trying to get the show on the air as "by far the worst experience of [his] grown-up life." The show premiered on March 28, 1999. Groening's writing credits for the show are for the premiere episode, "Space Pilot 3000" (co-written with Cohen), "Rebirth" (story) and "In-A-Gadda-Da-Leela" (story).

After four years on the air, the show was canceled by Fox. In a situation similar to "Family Guy", however, strong DVD sales and very stable ratings on Adult Swim brought "Futurama" back to life. When Comedy Central began negotiating for the rights to air "Futurama" reruns, Fox suggested that there was a possibility of also creating new episodes. When Comedy Central committed to sixteen new episodes, it was decided that four straight-to-DVD films – "" (2007), "" (2008), "" (2008) and "" (2009) – would be produced.

Since no new "Futurama" projects were in production, the movie "Into the Wild Green Yonder" was designed to stand as the "Futurama" series finale. However, Groening had expressed a desire to continue the "Futurama" franchise in some form, including as a theatrical film. In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know... We're having discussions and there is some enthusiasm but I can't tell if it's just me". Comedy Central commissioned an additional 26 new episodes, and began airing them in 2010. The show continued in to 2013, before Comedy Central announced in April 2013 that they would not be renewing it beyond its seventh season. The final episode aired on September 4, 2013.

On January 15, 2016, it was announced that Groening was in talks with Netflix to develop a new animated series. On July 25, 2017 the series, "Disenchantment", was ordered by Netflix. The first ten episodes premiered on the streaming service in August 2018, with the remaining ten episodes of the initial order scheduled to air in September 2019. Netflix has renewed the series for twenty additional episodes, which are expected to debut in ten-episode batches in 2020 and 2021. 

Groening described the fantasy-oriented series as originating in a sketchbook full of "fantastic creatures we couldn't do on "The Simpsons"." The show's cast includes Abbi Jacobson, Eric Andre, and Nat Faxon. 

In 1994, Groening formed Bongo Comics (named after the character Bongo from "Life in Hell") with Steve Vance, Cindy Vance and Bill Morrison, which publishes comic books based on "The Simpsons" and "Futurama" (including "Futurama Simpsons Infinitely Secret Crossover Crisis", a crossover between the two), as well as a few original titles. According to Groening, the goal with Bongo is to "[try] to bring humor into the fairly grim comic book market." He also formed Zongo Comics in 1995, an imprint of Bongo that published comics for more mature readers, which included three issues of Mary Fleener's "Fleener" and seven issues of his close friend Gary Panter's "Jimbo" comics.

Groening is known for his eclectic taste in music. His favorite band is Frank Zappa and The Mothers of Invention and his favorite album is "Trout Mask Replica" by Captain Beefheart (which was produced by Zappa). He guest-edited Da Capo Press's "Best Music Writing 2003" and curated a US All Tomorrow's Parties music festival in 2003. He illustrated the cover of Frank Zappa's posthumous album "" (1996). In May 2010, he curated another edition of All Tomorrow's Parties in Minehead, England. He also plays the drums in the all-author rock and roll band The Rock Bottom Remainders (although he is listed as the cowbell player), whose other members include Dave Barry, Ridley Pearson, Scott Turow, Amy Tan, James McBride, Mitch Albom, Roy Blount Jr., Stephen King, Kathi Kamen Goldmark, Sam Barry and Greg Iles. In July 2013, Groening co-authored "Hard Listening" (2013) with the rest of the Rock Bottom Remainders (published by Coliloquy, LLC).
Groening and Deborah Caplan married in 1986 and had two sons together, Homer (who goes by Will) and Abe, both of whom Groening occasionally portrays as rabbits in "Life in Hell". The couple divorced in 1999 after thirteen years of marriage. In 2011, Groening married Argentine artist Agustina Picasso after a four-year relationship, and became stepfather to her daughter Camila Costantini. In May 2013, Picasso gave birth to Nathaniel Philip Picasso Groening, named after writer Nathanael West. She joked that "his godfather is SpongeBob's creator Stephen Hillenburg". In 2015, Groening's daughters Luna Margaret and India Mia were born. Matt is the brother-in-law of "Hey Arnold!" and "Dinosaur Train" creator, Craig Bartlett, who is married to Groening's sister, Lisa. Bartlett used to appear in "Simpsons Illustrated".

On June 16, 2018, he became the father of twins for a second time when his wife gave birth to Sol Matthew and Venus Ruth, announced via Instagram.
Groening is a self-identified agnostic and has often made campaign contributions to Democratic Party candidates. His first cousin, Laurie Monnes Anderson, is a member of the Oregon State Senate representing eastern Multnomah County.

Groening has been nominated for 41 Emmy Awards and has won thirteen, eleven for "The Simpsons" and two for "Futurama" in the "Outstanding Animated Program (for programming one hour or less)" category. Groening received the 2002 National Cartoonist Society Reuben Award, and had been nominated for the same award in 2000. He received a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2007, he was ranked fourth (and highest American by birth) in a list of the "top 100 living geniuses", published by British newspaper "The Daily Telegraph".

He received the 2,459th star on the Hollywood Walk of Fame on February 14, 2012.




</doc>
<doc id="18895" url="https://en.wikipedia.org/wiki?curid=18895" title="Metaphysics">
Metaphysics

Metaphysics is the branch of philosophy that examines the fundamental nature of reality, including the relationship between mind and matter, between substance and attribute, and between potentiality and actuality. The word "metaphysics" comes from two Greek words that, together, literally mean "after or behind or among [the study of] the natural". It has been suggested that the term might have been coined by a first century CE editor who assembled various small selections of Aristotle’s works into the treatise we now know by the name "Metaphysics" ("ta meta ta phusika", 'after the "Physics" ', another of Aristotle's works).

Metaphysics studies questions related to what it is for something to exist and what types of existence there are. Metaphysics seeks to answer, in an abstract and fully general manner, the questions:

Topics of metaphysical investigation include existence, objects and their properties, space and time, cause and effect, and possibility.

Metaphysical study is conducted using deduction from that which is known "a priori". Like foundational mathematics (which is sometimes considered a special case of metaphysics applied to the existence of number), it tries to give a coherent account of the structure of the world, capable of explaining our everyday and scientific perception of the world, and being free from contradictions. In mathematics, there are many different ways to define numbers; similarly in metaphysics there are many different ways to define objects, properties, concepts, and other entities which are claimed to make up the world. While metaphysics may, as a special case, study the entities postulated by fundamental science such as atoms and superstrings, its core topic is the set of categories such as object, property and causality which those scientific theories assume. For example: claiming that "electrons have charge" is a scientific theory; while exploring what it means for electrons to be (or at least, to be perceived as) "objects", charge to be a "property", and for both to exist in a topological entity called "space" is the task of metaphysics.

There are two broad stances about what is "the world" studied by metaphysics. The strong, classical view assumes that the objects studied by metaphysics exist independently of any observer, so that the subject is the most fundamental of all sciences. The weak, modern view assumes that the objects studied by metaphysics exist inside the mind of an observer, so the subject becomes a form of introspection and conceptual analysis. Some philosophers, notably Kant, discuss both of these "worlds" and what can be inferred about each one. Some, such as the logical positivists, and many scientists, reject the strong view of metaphysics as meaningless and unverifiable. Others reply that this criticism also applies to any type of knowledge, including hard science, which claims to describe anything other than the contents of human perception, and thus that the world of perception "is" the objective world in some sense. Metaphysics itself usually assumes that some stance has been taken on these questions and that it may proceed independently of the choice—the question of which stance to take belongs instead to another branch of philosophy, epistemology.

Ontology is the philosophical study of the nature of being, becoming, existence or reality, as well as the basic categories of being and their relations. Traditionally listed as the core of metaphysics, ontology often deals with questions concerning what entities exist or may be said to exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences.

Identity is a fundamental metaphysical issue. Metaphysicians investigating identity are tasked with the question of what, exactly, it means for something to be identical to itself, or — more controversially — to something else. Issues of identity arise in the context of time: what does it mean for something to be itself across two moments in time? How do we account for this? Another question of identity arises when we ask what our criteria ought to be for determining identity? And how does the reality of identity interface with linguistic expressions?

The metaphysical positions one takes on identity have far-reaching implications on issues such as the Mind–body problem, personal identity, ethics, and law.

The ancient Greeks took extreme positions on the nature of change. Parmenides denied change altogether, while Heraclitus argued that change was ubiquitous: "[Y]ou cannot step into the same river twice."

Identity, sometimes called Numerical Identity, is the relation that a "thing" bears to itself, and which no "thing" bears to anything other than itself (cf. sameness).

A modern philosopher who made a lasting impact on the philosophy of identity was Leibniz, whose "Law of the Indiscernibility of Identicals" is still in wide use today. It states that if some object x is identical to some object y, then any property that x has, y will have as well.

Put formally, it states

However, it seems, too, that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are perdurantism, which treats the tree as a series of tree-stages, and endurantism, which maintains that the organism—the same tree—is present at every stage in its history.

In virtue of intrinsic and extrinsic properties, endurantism finds a way to harmonize identity with change. Endurantists believe that objects persist by being strict numerical identity over time. However, if they utilize Leibniz’s Law ("Indiscernibility of Identicals)" to define numerical identity here, objects must be completely unchanged in order to persist. Discriminating between intrinsic properties and extrinsic properties, endurantists state that numerical identity means if some object x is identical to some object y, then any intrinsic property that x has, y will have as well. Thus, if an object persists (being numerical identity), intrinsic properties of it are unchanged, and extrinsic properties can change over time. This is reasonable—besides this object itself, environments and other objects can change over time; extrinsic properties, properties that relate to other objects, would change even if this object does not change. 

Perdurantism harmonizes identity with change through another way. Perdurantism, or four-dimensionalism, state that material objects persist by having temporal parts at different times. For perdurantists, what persist is a four-dimensional object and what keep numerical identity are temporal parts of this four-dimensional object. Thus, there is no contradiction between identity and change if one object persists and other objects are numerical identity over time. 

Objects appear to us in space and time, while abstract entities such as classes, properties, and relations do not. What then is meant by space and time such that it can serve this function as a ground for objects? Are space and time entities themselves, of some form, or must they exist prior to other entities? How exactly can they be defined? For example, if time is defined as a "rate of change" then must there always be something changing in order for time to exist?

Classical philosophy recognized a number of causes, including teleological future causes. In special relativity and quantum field theory the notions of space, time and causality become tangled together, with temporal orders of causations becoming dependent on who is observing them. The laws of physics are symmetrical in time, so could equally well be used to describe time as running backwards. Why then do we perceive it as flowing in one direction, the arrow of time, and as containing causation flowing in the same direction?

For that matter, can an effect precede its cause? This was the title of a 1954 paper by Michael Dummett, which sparked a discussion that continues today. Earlier, in 1947, C. S. Lewis had argued that one can meaningfully pray concerning the outcome of, e.g., a medical test while recognizing that the outcome is determined by past events: "My free act contributes to the cosmic shape." Likewise, some interpretations of quantum mechanics involve backward-in-time causal influences.

Causality is linked by many philosophers to the concept of counterfactuals. To say that A caused B means that if A had not happened then B would not have happened. This view was advanced by David Lewis in his 1973 paper "Causation". His subsequent papers further develop his theory of causation.

Causality is usually required as a foundation for philosophy of science, if science aims to understand causes and effects and make predictions about them.

Metaphysicians investigate questions about the ways the world could have been. David Lewis, in "On the Plurality of Worlds", endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds, just as in ours, in which things are different. Other philosophers, such as Gottfried Leibniz, have dealt with the idea of possible worlds as well. The idea of necessity is that any necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The particular example of analytic truth being necessary is not universally held among philosophers. A less controversial view might be that self-identity is necessary, as it seems fundamentally incoherent to claim that for any x, it is not identical to itself; this is known as the "law of identity", a putative "first principle". Aristotle describes the "principle of non-contradiction", "It is impossible that the same quality should both belong and not belong to the same thing ... This is the most certain of all principles ... Wherefore they who demonstrate refer to this as an ultimate opinion. For it is by nature the source of all the other axioms."

What is "central" and "peripheral" to metaphysics has varied over time and schools; however contemporary analytic philosophy as taught in USA and UK universities generally regards the above as "central" and the following as "applications" or "peripheral" topics; or in some cases as distinct subjects which have grown out of and depend upon metaphysics:

Metaphysical cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it formed a major part of the subject alongside Ontology, though its role is more peripheral in contemporary philosophy. It has had a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).

Cosmogony deals specifically with the origin of the universe. Modern metaphysical cosmology and cosmogony try to address questions such as:

Accounting for the existence of mind in a world otherwise composed of matter is a metaphysical problem which is so large and important as to have become a specialized subject of study in its own right, philosophy of mind.

Substance dualism is a classical theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, and which creates an immediate conceptual puzzle about how the two interact. This form of substance dualism differs from the dualism of some eastern philosophical traditions (like Nyāya), which also posit a soul; for the soul, under their view, is ontologically distinct from the mind. Idealism postulates that material objects do not exist unless perceived and only as perceptions. Panpsychism and panexperientialism, are property dualist theories in which everything "has" or "is" a mind rather than everything exists "in" a mind. Neutral monism postulates that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributesthus it implies a dual-aspect theory. For the last century, the dominant theories have been science-inspired including materialistic monism, Type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence.

Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.

The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as Incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as "Hard Determinism". Proponents range from Baruch Spinoza to Ted Honderich. Henri Bergson defended free will in his dissertation Time and Free Will from 1889.

Others, labeled Compatibilists (or "Soft Determinists"), believe that the two ideas can be reconciled coherently. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer, Gary Watson, Harry Frankfurt, and the like.

Incompatibilists who accept free will but reject determinism are called Libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.

The earliest type of classification of social construction traces back to Plato in his dialogue Phaedrus where he claims that the biological classification system seems to "carve nature at the joints". In contrast, later philosophers such as Michel Foucault and Jorge Luis Borges have challenged the capacity of natural and social classification. In his essay The Analytical Language of John Wilkins, Borges makes us imagine a certain encyclopedia where the animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained;... and so forth, in order to bring forward the ambiguity of natural and social kinds. According to metaphysics author Alyssa Ney: "the reason all this is interesting is that there seems to be a metaphysical difference between the Borgesian system and Platos". The difference is not obvious but one classification attempts to carve entities up according to objective distinction while the other does not. According to Quine this notion is closely related to the notion of similarity.

There are different ways to set up the notion of number in metaphysics theories. Platonist theories postulate number as a fundamental category itself. Others consider it to be a property of an entity called a "group" comprising other entities; or to be a relation held between several groups of entities, such as "the number four is the set of all sets of four things". Many of the debates around universals are applied to the study of number, and are of particular importance due to its status as a foundation for the philosophy of mathematics and for mathematics itself.

Although metaphysics as a philosophical enterprise is highly hypothetical, it also has practical application in most other branches of philosophy, science, and now also information technology. Such areas generally assume some basic ontology (such as a system of objects, properties, classes, and space time) as well as other metaphysical stances on topics such as causality and agency, then build their own particular theories upon these.

In science, for example, some theories are based on the ontological assumption of objects with properties (such as electrons having charge) while others may reject objects completely (such as quantum field theories, where spread-out "electronness" becomes a property of space time rather than an object).

"Social" branches of philosophy such as philosophy of morality, aesthetics and philosophy of religion (which in turn give rise to practical subjects such as ethics, politics, law, and art) all require metaphysical foundations, which may be considered as branches or applications of metaphysics. For example, they may postulate the existence of basic entities such as value, beauty, and God. Then they use these postulates to make their own arguments about consequences resulting from them. When philosophers in these subjects make their foundations they are doing applied metaphysics, and may draw upon its core topics and methods to guide them, including ontology and other core and peripheral topics. As in science, the foundations chosen will in turn depend on the underlying ontology used, so philosophers in these subjects may have to dig right down to the ontological layer of metaphysics to find what is possible for their theories. For example, a contradiction obtained in a theory of God or Beauty might be due to an assumption that it is an object rather than some other kind of ontological entity.

Prior to the modern history of science, scientific questions were addressed as a part of natural philosophy. Originally, the term "science" (Latin "scientia") simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment, unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from other branches of philosophy. Science and philosophy have been considered separated disciplines ever since. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence.

Metaphysics continues asking "why" where science leaves off. For example, any theory of fundamental physics is based on some set of axioms, which may postulate the existence of entities such as atoms, particles, forces, charges, mass, or fields. Stating such postulates is considered to be the "end" of a science theory. Metaphysics takes these postulates and explores what they mean as human concepts. For example, do all theories of physics require the existence of space and time, objects, and properties? Or can they be expressed using only objects, or only properties? Do the objects have to retain their identity over time or can they change? If they change, then are they still the same object? Can theories be reformulated by converting properties or predicates (such as "red") into entities (such as redness or redness fields) or processes ('there is some redding happening over there' appears in some human languages in place of the use of properties). Is the distinction between objects and properties fundamental to the physical world or to our perception of it?

Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book "Metaphysics and Measurement", "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress." That metaphysical propositions can influence scientific theorizing is John Watkins' most lasting contribution to philosophy. Since 1957 "he showed the ways in which some un-testable and hence, according to Popperian ideas, non-empirical propositions can nevertheless be influential in the development of properly testable and hence scientific theories. These profound results in applied elementary logic...represented an important corrective to positivist teachings about the meaninglessness of metaphysics and of normative claims". Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions. Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."

An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.

In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned. Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists such as Albert Einstein to propose alternative theories that retained determinism. A.N. Whitehead is famous for creating a process philosophy metaphysics inspired by electromagnetism and special relativity.

In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.

Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.

A number of individuals have suggested that much or all of metaphysics should be rejected. In the 16th century, Francis Bacon rejected scholastic metaphysics, and argued strongly for what is now called empiricism, being seen later as the father of modern empirical science. In the 18th century, David Hume took a strong position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his "Enquiry Concerning Human Understanding" with the statement:

If we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, "Does it contain any abstract reasoning concerning quantity or number?" No. "Does it contain any experimental reasoning concerning matter of fact and existence?" No. Commit it then to the flames: for it can contain nothing but sophistry and illusion.

Thirty-three years after Hume's "Enquiry" appeared, Immanuel Kant published his "Critique of Pure Reason". Although he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some "synthetic a priori" knowledge, concerned with matters of fact yet obtainable independent of experience. These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.

Wittgenstein introduced the concept that metaphysics could be influenced by theories of aesthetics, via logic, vis. a world composed of "atomical facts".

In the 1930s, A.J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above. They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning. Carnap took a similar line with the controversy over the reality of the external world. While the logical positivism movement is now considered dead, (with a major proponent AJ Ayer admitting in a TV interview that "it was a lot of fun ... but it was false") it has continued to influence philosophy development.

Arguing against such rejections, the Scholastic philosopher Edward Feser has observed that Hume's critique of metaphysics, and specifically Hume's fork, is "notoriously self-refuting". Feser argues that Hume's fork itself is not a conceptual truth and is not empirically testable.

Some living philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress toward answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.

The word "metaphysics" derives from the Greek words μετά ("metá","after") and φυσικά ("physiká", "physics"). It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix "meta-" ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books metaphysics: he referred to it as "first philosophy." The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, "Physics", and called them ("tà metà tà physikà biblía") or "the books [that come] after the [books on] physics".

However, once the name was given, the commentators sought to find other reasons for its appropriateness. For instance, Thomas Aquinas understood it to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world".

The term was misread by other medieval commentators, who thought it meant "the science of what is beyond the physical". Following this tradition, the prefix "meta-" has more recently been prefixed to the names of sciences to designate higher sciences dealing with ulterior and more fundamental problems: hence metamathematics, , etc.

A person who creates or develops metaphysical theories is called a "metaphysician".

Common parlance also uses the word "metaphysics" for a different referent from that of the present article, namely for beliefs in arbitrary non-physical or magical entities. For example, "Metaphysical healing" to refer to healing by means of remedies that are magical rather than scientific. This usage stemmed from the various historical schools of speculative metaphysics which operated by postulating all manner of physical, mental and spiritual entities as bases for particular metaphysical systems. Metaphysics as a subject does not preclude beliefs in such magical entities but neither does it promote them. Rather, it is the subject which provides the vocabulary and logic with which such beliefs might be analyzed and studied, for example to search for inconsistencies both within themselves and with other accepted systems such as Science.

Cognitive archeology such as analysis of cave paintings and other pre-historic art and customs suggests that a form of perennial philosophy or Shamanism metaphysics may stretch back to the birth of behavioral modernity, all around the world. Similar beliefs are found in present-day "stone age" cultures such as Australian aboriginals. Perennial philosophy postulates the existence of a spirit or concept world alongside the day-to-day world, and interactions between these worlds during dreaming and ritual, or on special days or at special places. It has been argued that perennial philosophy formed the basis for Platonism, with Plato articulating, rather than creating, much older widespread beliefs.

Bronze Age cultures such as ancient Mesopotamia and ancient Egypt (along with similarly structured but chronologically later cultures such as Mayans and Aztecs) developed belief systems based on mythology, anthropomorphic gods, mind-body dualism, and a spirit world, to explain causes and cosmology. These cultures appear to have been interested in astronomy and may have associated or identified the stars with some of these entities. In ancient Egypt, the ontological distinction between order (maat) and chaos (Isfet) seems to have been important.

The first named Greek philosopher, according to Aristotle, is Thales of Miletus, early 6th century BCE. He made use of purely physical explanations to explain the phenomena of the world rather than the mythological and divine explanations of tradition. He is thought to have posited water as the single underlying principle (or "Arche in later Aristotelian terminology) of the material world". His fellow, but younger Miletians, Anaximander and Anaximenes, also posited monistic underlying principles, namely apeiron (the indefinite or boundless) and air respectively.

Another school was the Eleatics, in southern Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used "reductio ad absurdum", to demonstrate the illusory nature of change and time in his paradoxes.

Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.

Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos. They are considered forerunners of the scientific method.

Metaphysics in Chinese philosophy can be traced back to the earliest Chinese philosophical concepts from the Zhou Dynasty such as Tian (Heaven) and Yin and Yang. The fourth century BCE saw a turn towards cosmogony with the rise of Taoism (in the Daodejing and Zhuangzi) and sees the natural world as dynamic and constantly changing processes which spontaneously arise from a single immanent metaphysical source or principle (Tao). Another philosophical school which arose around this time was the School of Naturalists which saw the ultimate metaphysical principle as the Taiji, the "supreme polarity" composed of the forces of Yin and Yang which were always in a state of change seeking balance. Another concern of Chinese metaphysics, especially Taoism, is the relationship and nature of Being and non-Being (you 有 and wu 無). The Taoists held that the ultimate, the Tao, was also non-being or no-presence. Other important concepts were those of spontaneous generation or natural vitality (Ziran) and "correlative resonance" (Ganying).

After the fall of the Han Dynasty (220 CE), China saw the rise of the Neo-Taoist Xuanxue school. This school was very influential in developing the concepts of later Chinese metaphysics. Buddhist philosophy entered China (c. 1st century) and was influenced by the native Chinese metaphysical concepts to develop new theories. The native Tiantai and Huayen schools of philosophy maintained and reinterpreted the Indian theories of shunyata (emptiness, kong 空) and Buddha-nature (Fo xing 佛性) into the theory of interpenetration of phenomena. Neo-Confucians like Zhang Zai under the influence of other schools developed the concepts of "principle" (li) and vital energy (qi).

Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.

His pupil, Plato is famous for his theory of forms (which he places in the mouth of Socrates in his dialogues). Platonic realism (also considered a form of idealism) is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.

The theory has a number of other aspects:

Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.

Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms are existentially apparent in the visible world, Aristotelian essences dwell in particulars.

Potentiality and Actuality are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.

The Aristotelian theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause "simplicity". Final causes are explicitly teleological, a concept now regarded as controversial in science. The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.

The opening arguments in Aristotle's "Metaphysics", Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect "advances from sensation through memory, experience, and art, to theoretical knowledge". Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.

"More on Indian philosophy: Hindu philosophy"

"Sāṃkhya" is an ancient system of Indian philosophy based on a dualism involving the ultimate principles of consciousness and matter. It is described as the rationalist school of Indian philosophy. It is most related to the Yoga school of Hinduism, and its method was most influential on the development of Early Buddhism.

The Sāmkhya is an enumerationist philosophy whose epistemology accepts three of six pramanas (proofs) as the only reliable means of gaining knowledge. These include "pratyakṣa" (perception), "anumāṇa" (inference) and "śabda" ("āptavacana", word/testimony of reliable sources).

Samkhya is strongly dualist. Sāmkhya philosophy regards the universe as consisting of two realities; puruṣa (consciousness) and prakṛti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakṛti in some form. This fusion, state the Samkhya scholars, led to the emergence of "buddhi" ("spiritual awareness") and "ahaṅkāra" (ego consciousness). The universe is described by this school as one created by purusa-prakṛti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind. During the state of imbalance, one of more constituents overwhelm the others, creating a form of bondage, particularly of the mind. The end of this imbalance, bondage is called liberation, or moksha, by the Samkhya school.

The existence of God or supreme being is not directly asserted, nor considered relevant by the Samkhya philosophers. Sāṃkhya denies the final cause of Ishvara (God). While the Samkhya school considers the Vedas as a reliable source of knowledge, it is an atheistic philosophy according to Paul Deussen and other scholars. A key difference between Samkhya and Yoga schools, state scholars, is that Yoga school accepts a "personal, yet essentially inactive, deity" or "personal god".

Samkhya is known for its theory of guṇas (qualities, innate tendencies). Guṇa, it states, are of three types: "sattva" being good, compassionate, illuminating, positive, and constructive; "rajas" is one of activity, chaotic, passion, impulsive, potentially good or bad; and "tamas" being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three guṇas, but in different proportions. The interplay of these guṇas defines the character of someone or something, of nature and determines the progress of life. The Samkhya theory of guṇas was widely discussed, developed and refined by various schools of Indian philosophies, including Buddhism. Samkhya's philosophical treatises also influenced the development of various theories of Hindu ethics.

Realization of the nature of Self-identity is the principal object of the Vedanta system of Indian metaphysics. In the Upanishads, self-consciousness is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification, and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness. It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.

The word "Self-consciousness" in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality. Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman. According to the Upanishads the Atman or Paramatman is phenomenally unknowable; it is the object of realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.

Metaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the "Dualistic school", b) the "Quasi-dualistic school" and c) the "Monistic school", as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad. Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.

Self-consciousness is the Fourth state of consciousness or "Turiya", the first three being "Vaisvanara", "Taijasa" and "Prajna". These are the four states of individual consciousness.

There are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the "I-within" with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising "I am the Absolute" – "Aham Brahman Asmi". The Fifth stage is in realising that Brahman is the "All" that exists, as also that which does not exist.

In Buddhist philosophy there are various metaphysical traditions that have proposed different questions about the nature of reality based on the teachings of the Buddha in the early Buddhist texts. The Buddha of the early texts does not focus on metaphysical questions but on ethical and spiritual training and in some cases, he dismisses certain metaphysical questions as unhelpful and indeterminate Avyakta, which he recommends should be set aside. The development of systematic metaphysics arose after the Buddha's death with the rise of the Abhidharma traditions. The Buddhist Abhidharma schools developed their analysis of reality based on the concept of "dharmas" which are the ultimate physical and mental events that make up experience and their relations to each other. Noa Ronkin has called their approach "phenomenological".

Later philosophical traditions include the Madhyamika school of Nagarjuna, which further developed the theory of the emptiness (shunyata) of all phenomena or dharmas which rejects any kind of substance. This has been interpreted as a form of anti-foundationalism and anti-realism which sees reality as having no ultimate essence or ground. The Yogacara school meanwhile promoted a theory called "awareness only" (vijnapti-matra) which has been interpreted as a form of Idealism or Phenomenology and denies the split between awareness itself and the objects of awareness.

Islamic metaphysics was highly active during Europe's 'Dark Ages', beginning with the arrival and translation of Aristotle into Arabic.

Major ideas in Sufi metaphysics have surrounded the concept of weḥdah (وحدة) meaning "unity", or in Arabic توحيد tawhid. Two main Sufi philosophies prevail on this topic. waḥdat al-wujūd literally means the "Unity of Existence" or "Unity of Being." The phrase has been translated "pantheism." Wujud (i.e. existence or presence) here refers to Allah's wujud (compare tawhid). On the other hand, waḥdat ash-shuhūd, meaning "Apparentism" or "Monotheism of Witness", holds that God and his creation are entirely separate.

Some Islamic reformers have claimed that the difference between the two philosophies differ only in semantics and that the entire debate is merely a collection of "verbal controversies" which have come about because of ambiguous language. However, the concept of the relationship between God and the universe is still actively debated both among Sufis and between Sufis and non-Sufi Muslims.

"More on medieval philosophy and metaphysics: Medieval Philosophy"

Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established
framework blending Christian theology with Aristotelian teachings. Although fundamental orthodoxies were not commonly challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.

In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.

British empiricism marked something of a reaction to rationalist and system-building metaphysics, or "speculative" metaphysics as it was pejoratively termed. The skeptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less skeptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.

Christian Wolff had theoretical philosophy divided into an ontology or "philosophia prima" as a general metaphysics, which arises as a preliminary to the distinction of the three "special metaphysics" on the soul, world and God: rational psychology, rational cosmology and rational theology. The three disciplines are called empirical and rational because they are independent of revelation. This scheme, which is the counterpart of religious tripartition in creature, creation, and Creator, is best known to philosophical students by Kant's treatment of it in the "Critique of Pure Reason". In the "Preface" of the 2nd edition of Kant's book, Wolff is defined "the greatest of all dogmatic philosophers."

Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. As did the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind.
Kant described his shift in metaphysics away from making claims about an objective noumenal world, towards exploring the subjective phenomenal world, as a Copernican Revolution, by analogy to (though opposite in direction to) Copernicus' shift from man (the subject) to the sun (an object) at the center of the universe.

Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the "synthetic apriori"—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it differs from abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There "is" a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of phenomenal God, morality, and free will "might" exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own, and as beginning the modern analytical conception of the subject.

Nineteenth century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early twentieth century with British idealists such as F.H. Bradley and J.M.E. McTaggart. Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.

During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.

Analytical philosophy was spearheaded by Bertrand Russell and G.E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.

The early to mid twentieth century philosophy saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of logical positivism as espoused by the Vienna Circle, which argued that the meaning of a statement was its prediction of observable results of an experiment, and thus that there is no need to postulate the existence of any objects other than these perceptual observations.

At around the same time, the American pragmatists were steering a middle course between materialism and idealism.
System-building metaphysics, with a fresh inspiration from science, was revived by A.N. Whitehead and Charles Hartshorne.

The forces that shaped analytical philosophy—the break with idealism, and the influence of science—were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.

The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytical philosophy.

Heidegger, author of "Being and Time", saw himself as re-focusing on Being-qua-being, introducing the novel concept of "Dasein" in the process. Classing himself an existentialist, Sartre wrote an extensive study of "Being and Nothingness".

The speculative realism movement marks a return to full blooded realism.

There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions, however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.

In a broad sense, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major "Continental" figures such as the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.

In a strict sense, process metaphysics may be limited to the works of a few founding fathers: G.W.F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A.N. Whitehead, and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars such as Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–2005), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–2007), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–2014), and Reiner Wiehl (1929–2010).

While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytical philosophy generally is away from the construction of all-encompassing systems and toward close analysis of individual ideas.

Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic–synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.

The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all come of relative obscurity into the limelight, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.

The analytic view is of metaphysics as studying phenomenal human concepts rather than making claims about the noumenal world, so its style often blurs into philosophy of language and introspective psychology. Compared to system-building, it can seem very dry, stylistically similar to computer programming, mathematics or even accountancy (as a common stated goal is to "account for" entities in the world).






</doc>
<doc id="18896" url="https://en.wikipedia.org/wiki?curid=18896" title="Human spaceflight">
Human spaceflight

Human spaceflight (also referred to as manned spaceflight) is space travel with a crew or passengers aboard the spacecraft. Spacecraft carrying people may be operated directly, by human crew, or it may be either remotely operated from ground stations on Earth or be autonomous, able to carry out a specific mission with no human involvement.

The first human in space was Yuri Gagarin, who flew the Vostok 1 spacecraft, launched by the Soviet Union on 12 April 1961 as part of the Vostok program. Humans have flown to the Moon nine times from 1968 to 1972 in the United States Apollo program, and have been continuously present in space for on the International Space Station. All human spaceflight has so far been human-piloted, with the first autonomous human-carrying spacecraft under design starting in 2015.

Russia and China have human spaceflight capability with the Soyuz program and Shenzhou program. In the United States, SpaceShipTwo reached the edge of space in 2018; this was the first crewed spaceflight from the US since the Space Shuttle retired in 2011. Currently, all expeditions to the International Space Station use Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States is developing commercial crew transportation to facilitate domestic access to ISS and low Earth orbit, as well as the Orion vehicle for beyond-low-Earth-orbit applications.

While spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both people and cargo transport to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches could begin by 2020.

Human spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR), which developed the first intercontinental ballistic missile rockets to deliver nuclear weapons. These rockets were large enough to be adapted to carry the first artificial satellites into low Earth orbit. After the first satellites were launched in 1957 and 1958, the US worked on Project Mercury to launch men singly into orbit, while the USSR secretly pursued the Vostok program to accomplish the same thing. The USSR launched the first human in space, Yuri Gagarin, into a single orbit in Vostok 1 on a Vostok 3KA rocket, on 12 April 1961. The US launched its first astronaut, Alan Shepard, on a suborbital flight aboard "Freedom 7" on a Mercury-Redstone rocket, on 5 May 1961. Unlike Gagarin, Shepard manually controlled his spacecraft's attitude, and landed inside it. The first American in orbit was John Glenn aboard "Friendship 7", launched 20 February 1962 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6 on 16 June 1963. The US launched a total of two astronauts in suborbital flight and four into orbit through 1963. The US also made two flights in the North American X-15 (90 and 91) piloted by Joseph A. Walker that exceeded the Kármán line, the internationally recognized 100 km altitude used by the FAI to denote the edge of space.

US President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely by the end of the 1960s. The US started the three-man Apollo program in 1961 to accomplish this, launched by the Saturn family of launch vehicles, and the interim two-man Project Gemini in 1962, which flew 10 missions launched by Titan II rockets in 1965 and 1966. Gemini's objective was to support Apollo by developing American orbital spaceflight experience and techniques to be used in the Moon mission.

Meanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule into a two- or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, made by Alexei Leonov on Voskhod 2 on 8 March 1965. But Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not accomplish the first spacewalk, but overcame the early Soviet lead by performing several spacewalks and solving the problem of astronaut fatigue caused by overcoming the lack of gravity, demonstrating up to two weeks endurance in a human spaceflight, and the first space rendezvous and dockings of spacecraft.

The US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon 21 July and returning them safely on 24 July along with Command Module pilot Michael Collins. A total of six Apollo missions landed 12 men to walk on the Moon through 1972, half of which drove electric powered vehicles on the surface. The crew of Apollo 13, Lovell, Jack Swigert, and Fred Haise, survived a catastrophic in-flight spacecraft failure and returned to Earth safely without landing on the Moon.
Meanwhile, the USSR secretly pursued crewed lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, and discontinued the lunar programs in 1974. On losing the Moon race, they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.

After the Apollo program, the US launched the Skylab sortie space station in 1973, inhabiting it for 171 days with three crews aboard Apollo spacecraft. President Richard Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as détente, an easing of Cold War tensions. As part of this, they negotiated the Apollo-Soyuz Test Project, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely diplomatic and symbolic.
Nixon appointed his Vice President Spiro Agnew to head a Space Task Group in 1969 to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched by a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent modular space station, reusable space tug and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986, or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support Congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted flyback booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.
The two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, dubbed "Freedom". 
The USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of "Mir", the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. "Mir" orbited at an altitude of , at a 51.6° inclination. It was occupied for 4,592 days, and made a controlled reentry in 2001.
The Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make "Freedom" a reality. A fleet of four shuttles was built: "Columbia", "Challenger", "Discovery", and "Atlantis". A fifth shuttle, "Endeavour", was built to replace "Challenger", which was destroyed in an accident during launch that killed 7 astronauts on 28 January 1986. Twenty-two Shuttle flights carried a European Space Agency sortie space station called Spacelab in the payload bay from 1983 to 1998.

The USSR copied the reusable Space Shuttle orbiter, which it called "Buran". It was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the US Shuttle, "Buran" had no main rocket engines, but like the Shuttle used its orbital maneuvering engines to perform its final orbital insertion. A single uncrewed orbital test flight was successfully made in November 1988. A second test flight was planned by 1993, but the program was cancelled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the first one was destroyed in a hangar roof collapse in May 2002.

The dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the "Mir" space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard "Mir".

In 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station "Freedom" into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of and an inclination of 51.65°.

The Space Shuttle was retired in 2011 after 135 orbital flights, several of which helped assemble, supply, and crew the ISS. "Columbia" was destroyed in another accident during reentry, which killed 7 astronauts on 1 February 2003.

Russia has continued cooperation though half of the International Space Station is its sole singular half.

After Russia's launch of Sputnik 1 in 1957, Chairman Mao Zedong intended to place a Chinese satellite in orbit by 1959 to celebrate the 10th anniversary of the founding of the People's Republic of China (PRC), However, China did not successfully launch its first satellite until 24 April 1970. Mao and Premier Zhou Enlai decided on 14 July 1967, that the PRC should not be left behind, and started China's own human spaceflight program. The first attempt, the Shuguang spacecraft copied from the US Gemini, was cancelled on 13 May 1972.
China later designed the Shenzhou spacecraft resembling the Russian Soyuz, and became the third nation to achieve independent human spaceflight capability by launching Yang Liwei on a 21-hour flight aboard Shenzhou 5 on 15 October 2003. China launched the Tiangong-1 space station on 29 September 2011, and two sortie missions to it: Shenzhou 9 16–29 June 2012, with China's first female astronaut Liu Yang; and Shenzhou 10, 13–26 June 2013. The station was retired on 21 March 2016 and reentered on 2 April 2018, burning up with smaller fragments impacting the ocean. Tiangong-1's successor Tiangong-2 was launched in September 2016 to then be derorbited in July 2019. Tiangong-2 hosted a crew of two (Jing Haipeng and Chen Dong) for 26 days. The Tianzhou 1 cargo spacecraft docked to the station on 22 April 2017.

The European Space Agency began development in 1987 of the Hermes spaceplane, to be launched on the Ariane 5 expendable launch vehicle. The project was cancelled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built.

Japan began development in the 1980s of the HOPE-X experimental spaceplane, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reduction, and the project's cancellation in 2003.

Under the Bush administration, the Constellation Program included plans for retiring the Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. For beyond low Earth orbit human spaceflight NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low Earth orbit, such as SpaceX's Falcon 9/Dragon V2, Sierra Nevada Corporation's Dream Chaser, or Boeing's CST-100. The period between the retirement of the shuttle in 2011 and the first launch to space of Spaceshiptwo Flight VP-03 on 13 December 2018 is similar to the gap between the end of Apollo in 1975 and the first Space Shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.

Since the early 2000s, a variety of private spaceflight ventures have been undertaken. Several of the companies, including Blue Origin, SpaceX, Virgin Galactic, and Sierra Nevada have explicit plans to advance human spaceflight. , all four of those companies have development programs underway to fly commercial passengers.

A commercial suborbital spacecraft aimed at the space tourism market is being developed by Virgin Galactic called SpaceshipTwo which reached space in December 2018.
Blue Origin has begun a multi-year test program of their New Shepard vehicle and carried out 11 successful uncrewed test flights in 2015–2019. Blue Origin plan to fly with humans in 2019.

SpaceX and Boeing are both developing passenger-capable orbital space capsules as of 2015, planning to fly NASA astronauts to the International Space Station by 2019. SpaceX will be carrying passengers on Dragon 2 launched on a Falcon 9 launch vehicle. Boeing will be doing it with their CST-100 launched on a United Launch Alliance Atlas V launch vehicle.
Development funding for these orbital-capable technologies has been provided by a mix of government and private funds, with SpaceX providing a greater portion of total development funding for this human-carrying capability from private investment.
There have been no public announcements of commercial offerings for orbital flights from either company, although both companies are planning some flights with their own private, not NASA, astronauts on board.


Sally Ride became the first American woman in space in 1983. Eileen Collins was the first female Shuttle pilot, and with Shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.

For many years, only the USSR (later Russia) and the United States had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978, in the Interkosmos programme. , citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.

Human spaceflight programs have been conducted by the former Soviet Union and currently Russia, the United States, Mainland China, and by the American private spaceflight company Scaled Composites.
Space vehicles are spacecraft used for transportation between the Earth's surface and outer space, or between locations in outer space. The following space vehicles and spaceports are currently used for launching human spaceflights:

The following space stations are currently maintained in Earth orbit for human occupation:

Numerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week.

Most of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.

NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions," though because of gender specificity this is no longer official parlance according to NASA style guides.

On 15 August 2018 the Prime Minister of India Narendra Modi, from the rampart of the Red Fort in New Delhi, formally announced the Indian Human Spaceflight Programme. Through this Programme, India is planning to send humans into space on its orbital vehicle Gaganyaan by the end of 2021. The Indian Space Research Organisation (ISRO) began work on this project in 2006. The objective is to carry a crew of three to low Earth orbit (LEO) and return them safely for a water-landing at a predefined landing zone. The program is proposed to be implemented in defined phases. Currently, the activities are progressing with a focus on the development of critical technologies for subsystems such as the Crew Module (CM), Environmental Control and Life Support System (ECLSS), Crew Escape System, etc. The department has initiated activities to study technical and managerial issues related to crewed missions. The program envisages the development of a fully autonomous orbital vehicle carrying 2 or 3 crew members to about low Earth orbit and to bring them safely back home. In June 2019, ISRO Chairman K. Sivan revealed plans for a space station by 2030, followed by a crewed lunar mission.

NASA is developing a plan to land humans on Mars by the 2030s. The first step will begin with Artemis 1 in 2020 or 2021, sending an uncrewed Orion spacecraft to a distant retrograde orbit around the Moon and return it to Earth after a 25-day mission.

Several other countries and space agencies have announced and begun human spaceflight programs utilizing natively developed equipment and technology, including Japan (JAXA), Iran (ISA), and Malaysia (MNSA).

A number of spacecraft have been proposed over the decades that might facilitate spaceliner passenger travel. Somewhat analogous to travel by airliner after the middle of the 20th century, these vehicles are proposed to transport a large number of passengers to destinations in space, or to destinations on Earth which travel through space. To date, none of these concepts have been built, although a few vehicles that carry fewer than 10 persons are currently in the flight testing phase of their development process.

One large spaceliner concept currently in early development is the SpaceX Starship which, in addition to replacing the Falcon 9 and Falcon Heavy launch vehicles in the legacy Earth-orbit market after 2020, has been proposed by SpaceX for long-distance commercial travel on Earth. This is to transport people on point-to-point suborbital flights between two points on Earth in under one hour, also known as "Earth-to-Earth," and carrying 100+ passengers.

Small spaceplane or small capsule suborbital spacecraft have been under development for the past decade or so and, , at least one of each type are under development. Both Virgin Galactic and Blue Origin are in active development, with the SpaceShipTwo spaceplane and the New Shepard capsule, respectively. Both would carry approximately a half-dozen passengers up to space for a brief time of zero gravity before returning to the same location from where the trip began. XCOR Aerospace had been developing the Lynx single-passenger spaceplane since the 2000s but development was halted in 2017.

There are two main sources of hazard in space flight: those due to the environment of space which make it hostile to the human body, and the potential for mechanical malfunctions of the equipment required to accomplish space flight.

Planners of human spaceflight missions face a number of safety concerns.

The basic needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.

Medical consequences such as possible blindness and bone loss have been associated with human space flight.

On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.

In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.

On 2 November 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.

Researchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five "Enterobacter bugandensis" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.

In March 2019, NASA reported that latent viruses in humans may be activated during space missions, adding possibly more risk to astronauts in future deep-space missions.

Medical data from astronauts in low Earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts' performance or increase their risk of injury.

In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes them to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon return to Earth from long-duration flights, astronauts are considerably weakened, and are not allowed to drive a car for twenty-one days.

Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.

In addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars.

Without proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares and associated solar particle events (SPEs). Lawrence Townsend of the University of Tennessee and others have studied the overall most powerful solar storm ever recorded. The flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type storm could cause acute radiation sickness and possibly even death. Another storm that could have incurred a lethal radiation dose if astronauts were outside the Earth's protective magnetosphere occurred during the Space Age, in fact, shortly after Apollo 16 landed and before Apollo 17 launched. This solar storm of August 1972 would likely at least have caused acute illness.

Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.

There is also some scientific concern that extended spaceflight might slow down the body's ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.

During long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.

Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.

During astronauts' stay in space, they may experience mental disorders (such as post-trauma, depression, anxiety, etc.), more than for an average person. NASA spends millions of dollars on psychological treatments for astronauts and former astronauts. To date, there is no way to prevent or reduce mental problems caused by extended periods of stay in space.

Due to these mental disorders, the efficiency of their work is impaired and sometimes they are forced to send the astronauts back to Earth, which is very expensive. A Russian expedition to space in 1976 was returned to Earth after the cosmonauts reported a strong odor that caused a fear of fluid leakage, but after a thorough investigation it became clear that there was no leakage or technical malfunction.  It was concluded by NASA that the cosmonauts most likely had hallucinations of the smell, which brought many unnecessary wasted expenses.

It is possible that the mental health of astronauts can be affected by the changes in the sensory systems while in prolonged space travel.

During astronauts' spaceflight, they are in a very extreme state where there is no gravity. This given state and the fact that no change is taking place in the environment will result in the weakening of sensory input to the astronauts in all seven senses.


Space flight requires much higher velocities than ground or air transportation, which in turn requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.

Since rockets carry the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landing. The escape tower is discarded at some point before the launch is complete, at a point where an abort can be performed using the spacecraft's engines.

Such a system is not always practical for multiple crew member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights, but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.

There have only been two in-flight launch aborts of a crewed flight. The first occurred on Soyuz 18a on 5 April 1975. The abort occurred after the launch escape system had been jettisoned, when the launch vehicle's spent second stage failed to separate before the third stage ignited. The vehicle strayed off course, and the crew separated the spacecraft and fired its engines to pull it away from the errant rocket. Both cosmonauts landed safely. The second occurred on 11 October 2018 with the launch of Soyuz MS-10. Again, both crew members survived.

In the first use of a launch escape system on a crewed flight, the planned Soyuz T-10a launch on 26 September 1983 was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.

The only crew fatality during launch occurred on 28 January 1986, when the Space Shuttle "Challenger" broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal which caused separation of the booster and failure of the external fuel tank, resulting in explosion of the fuel. All seven crew members were killed.

The single pilot of Soyuz 1, Vladimir Komarov was killed when his capsule's parachutes failed during an emergency landing on 24 April 1967, causing the capsule to crash.

The crew of seven aboard the were killed on reentry after completing a successful mission in space on 1 February 2003. A wing leading edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation which broke off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to breakup of the orbiter vehicle.

There are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen in an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen-oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.

Use of a gas mixture carries risk of decompression sickness (commonly known as "the bends") when transitioning to or from the pure oxygen space suit environment. There have also been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.

A pure oxygen atmosphere carries risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on 27 January 1967, and spread rapidly. The high pressure (increased even higher by the fire) prevented removal of the plug door hatch cover in time to rescue the crew. All three, Gus Grissom, Ed White, and Roger Chaffee, were killed. This led NASA to use a nitrogen/oxygen atmosphere before launch, and low pressure pure oxygen only in space.

The March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin which threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.

The third lunar landing expedition Apollo 13 in April 1970, was aborted and the lives of the crew, James Lovell, Jack Swigert and Fred Haise, were threatened by failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the loss of its remaining oxygen in a span of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a "life boat". The tank failure was determined to be caused by two mistakes. The tank's drain fitting had been damaged when it was dropped during factory testing. This necessitated use of its internal heaters to boil out the oxygen after a pre-launch test, which in turn damaged the fan wiring's electrical insulation, because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.

The crew of Soyuz 11 were killed on 30 June 1971 by a combination of mechanical malfunctions: they were asphyxiated due to cabin decompression following separation of their descent capsule from the service module. A cabin ventilation valve had been jolted open at an altitude of by the stronger than expected shock of explosive separation bolts which were designed to fire sequentially, but in fact had fired simultaneously. The loss of pressure became fatal within about 30 seconds.

, 22 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.





</doc>
<doc id="18899" url="https://en.wikipedia.org/wiki?curid=18899" title="Mendelevium">
Mendelevium

Mendelevium is a synthetic element with the symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element by atomic number that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the third-to-last actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of sixteen mendelevium isotopes are known, the most stable being Md with a half-life of 51 days; nevertheless, the shorter-lived Md (half-life 1.17 hours) is most commonly used in chemistry because it can be produced on a larger scale.

Mendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. All of the isotopes of mendelevium have relatively short half-lives, there are currently no uses for it outside basic scientific research, and only small amounts are produced.

Mendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory Robert Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced Md (half-life of 77 minutes) when they bombarded an Es target consisting of only a billion (10) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of known beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of Fm that thus constituted a hard limit to the success of the neutron capture process.

To predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 10 alpha particles per second; Seaborg applied for the necessary funds.
While Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 10 atoms of Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×10 particles per second over an area of 0.05 cm. The target was cooled by water or liquid helium, and the foil could be replaced.

Initial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.
On the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter Fm. The first one was identified with a "hooray" followed by a "double hooray" and a "triple hooray". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.

Being the first of the second hundred of the chemical elements, it was decided that the element would be named "mendelevium" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Because this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name "mendelevium" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol "Mv", which was changed to "Md" in the next IUPAC General Assembly (Paris, 1957).

In the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.

The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fds configurations, whereas the latter have fs configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ/mol. Divalent mendelevium metal should have a metallic radius of around . Like the other divalent late actinides (except the once again trivalent lawrencium), metallic mendelevium should assume a face-centered cubic crystal structure. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium. Its density is predicted to be around .

The chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.

Before mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).

In reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the "E"°(Md→Md) couple was variously estimated in 1967 as −0.10 V or −0.20 V: later 2013 experiments established the value as . In comparison, "E"°(Md→Md) should be around −1.74 V, and "E"°(Md→Md) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).

In 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.

Although "E"°(Md→Md) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).

A mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol F), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of . Md should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ/mol; Md should have ionic radius 117 pm.

Sixteen isotopes of mendelevium are known, with mass numbers from 245 to 260; all are radioactive. Additionally, five nuclear isomers are known: Md, Md, Md, Md, and Md. Of these, the longest-lived isotope is Md with a half-life of 51.5 days, and the longest-lived isomer is Md with a half-life of 58.0 minutes. Nevertheless, the shorter-lived Md (half-life 1.17 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After Md, the next most stable mendelevium isotopes are Md with a half-life of 31.8 days, Md with a half-life of 5.52 hours, Md with a half-life of 1.60 hours, and Md with a half-life of 1.17 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.

The half-lives of mendelevium isotopes mostly increase smoothly from Md onwards, reaching a maximum at Md. Experiments and predictions suggest that the half-lives will then decrease, apart from Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.

Mendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90% of the time and alpha decay 10% of the time. It is most easily detected through the spontaneous fission of its electron capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.

The lightest mendelevium isotopes (Md to Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (Md to Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from Md to Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. Md is produced as a daughter of No, and Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.

The recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.

Mendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.

Another possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.

Another method to isolate mendelevium exploits the distinct elution properties of Md from those of Es and Fm. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.

Although few people come in contact with mendelevium, the International Commission on Radiological Protection has set annual exposure limits for the most stable isotope. For mendelevium-258, the ingestion limit was set at 9×10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 6000 Bq.




</doc>
<doc id="18900" url="https://en.wikipedia.org/wiki?curid=18900" title="Modus ponens">
Modus ponens

In propositional logic, modus ponens (; MP; also modus ponendo ponens (Latin for "mode that by affirming affirms") or implication elimination) is a rule of inference. It can be summarized as ""P implies Q" and "P" is asserted to be true, therefore "Q" must be true."

"Modus ponens" is closely related to another valid form of argument, "modus tollens". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of "modus ponens". Hypothetical syllogism is closely related to "modus ponens" and sometimes thought of as "double "modus ponens"."

The history of "modus ponens" goes back to antiquity. The first to explicitly describe the argument form "modus ponens" was Theophrastus. It, along with "modus tollens", is one of the standard patterns of inference that can be applied to derive chains of conclusions that lead to the desired goal.

The "modus ponens" rule may be written in sequent notation as

where "P", "Q" and "P" → "Q" are statements (or propositions) in a formal language and ⊢ is a metalogical symbol meaning that "Q" is a syntactic consequence of "P" and "P" → "Q" in some logical system.

The argument form has two premises (hypothesis). The first premise is the "if–then" or conditional claim, namely that "P" implies "Q". The second premise is that "P", the antecedent of the conditional claim, is true. From these two premises it can be logically concluded that "Q", the consequent of the conditional claim, must be true as well. In artificial intelligence, "modus ponens" is often called forward chaining.

An example of an argument that fits the form "modus ponens":

This argument is valid, but this has no bearing on whether any of the statements in the argument are true; for "modus ponens" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid "and" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using "modus ponens" is said to be deductive.

In single-conclusion sequent calculi, "modus ponens" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.

The Curry–Howard correspondence between proofs and programs relates "modus ponens" to function application: if "f" is a function of type "P" → "Q" and "x" is of type "P", then "f x" is of type "Q".

The validity of "modus ponens" in classical two-valued logic can be clearly demonstrated by use of a truth table.
In instances of "modus ponens" we assume as premises that "p" → "q" is true and "p" is true. Only one line of the truth table—the first—satisfies these two conditions ("p" and "p" → "q"). On this line, "q" is also true. Therefore, whenever "p" → "q" is true and "p" is true, "q" must also be true.

While "modus ponens" is one of the most commonly used argument forms in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution". "Modus ponens" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment or the law of detachment. Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones", and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] ... an inference is the dropping of a true premise; it is the dissolution of an implication".

A justification for the "trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If "P" implies "Q" and "P" is true, then "Q" is true.

"Modus ponens" represents an instance of the Law of total probability which for a binary variable is expressed as:

formula_2,

where e.g. formula_3 denotes the probability of formula_4 and the conditional probability formula_5 generalizes the logical implication formula_6. Assume that formula_7 is equivalent to formula_4 being TRUE, and that formula_9 is equivalent to formula_4 being FALSE. It is then easy to see that formula_7 when formula_12 and formula_13. Hence, the law of total probability represents a generalization of "modus ponens" .

"Modus ponens" represents an instance of the binomial deduction operator in subjective logic expressed as:

formula_14,

where formula_15 denotes the subjective opinion about formula_16 as expressed by source formula_17, and the conditional opinion formula_18 generalizes the logical implication formula_6. The deduced marginal opinion about formula_4 is denoted by formula_21. The case where formula_15 is an absolute TRUE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is TRUE, and the case where formula_15 is an absolute FALSE opinion about formula_16 is equivalent to source formula_17 saying that formula_16 is FALSE. The deduction operator formula_30 of subjective logic produces an absolute TRUE deduced opinion formula_21 when the conditional opinion formula_18 is absolute TRUE and the antecedent opinion formula_15 is absolute TRUE. Hence, subjective logic deduction represents a generalization of both "modus ponens" and the Law of total probability .

The philosopher and logician Vann McGee has argued that "modus ponens" can fail to be valid when the consequent is itself a conditional sentence. Here is an example:

The first premise seems reasonable enough, because Shakespeare is generally credited with writing "Hamlet". The second premise seems reasonable, as well, because with the set of Hamlet's possible authors limited to just Shakespeare and Hobbes, eliminating one leaves only the other. But the conclusion, considered by itself and with the possible authors not limited to just Shakespeare and Hobbes, is dubious, because if Shakespeare is ruled out as "Hamlet"'s author, there are many more plausible alternatives than Hobbes.

The general form of McGee-type counterexamples to "modus ponens" is simply formula_34, therefore formula_35; it is not essential that formula_16 be a disjunction, as in the example given. That these kinds of cases constitute failures of "modus ponens" remains a minority view among logicians, but opinions vary on how the cases should be disposed of.

In deontic logic, some examples of conditional obligation also raise the possibility of modus ponens failure. These are cases where the conditional premise describes an obligation predicated on an immoral or imprudent action, e.g., “If Doe murders his mother, he ought to do so gently.” Here again, modus ponens failure is not a popular diagnosis but is sometimes argued for.

The fallacy of affirming the consequent is a common misinterpretation of the modus ponens.





</doc>
<doc id="18901" url="https://en.wikipedia.org/wiki?curid=18901" title="Modus tollens">
Modus tollens

In propositional logic, modus tollens (; MT; also modus tollendo tollens (Latin for "mode that by denying denies") or denying the consequent) is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contrapositive.

The inference rule "modus tollens" asserts that the inference from "P implies Q" to "the negation of Q implies the negation of P" is valid.

The "modus tollens" rule can be stated formally as:

where formula_2 stands for the statement "P implies Q". formula_3 stands for "it is not the case that Q" (or in brief "not Q"). Then, whenever "formula_2" and "formula_5" each appear by themselves as a line of a proof, then "formula_6" can validly be placed on a subsequent line. The history of the inference rule "modus tollens" goes back to antiquity.

"Modus tollens" is closely related to "modus ponens". There are two similar, but invalid, forms of argument: affirming the consequent and denying the antecedent. See also contraposition and proof by contrapositive.

The first to explicitly describe the argument form "modus tollens" was Theophrastus.

The "modus tollens" rule may be written in sequent notation:

where formula_8 is a metalogical symbol meaning that formula_6 is a syntactic consequence of formula_2 and formula_5 in some logical system;

or as the statement of a functional tautology or theorem of propositional logic:

where formula_13 and formula_14 are propositions expressed in some formal system;

or including assumptions:
though since the rule does not change the set of assumptions, this is not strictly necessary.

More complex rewritings involving "modus tollens" are often seen, for instance in set theory:

Also in first-order predicate logic:

Strictly speaking these are not instances of "modus tollens", but they may be derived from "modus tollens" using a few extra steps.

Requirements:

Consider an example:

Supposing that the premises are both true (the dog will bark if it detects an intruder, and does indeed not bark), it follows that no intruder has been detected. This is a valid argument since it is not possible for the conclusion to be false if the premises are true. (It is conceivable that there may have been an intruder that the dog did not detect, but that does not invalidate the argument; the first premise is "if the watch-dog detects an intruder." The thing of importance is that the dog detects or does not detect an intruder, not whether there is one.)

Another example:

Another example: 

Every use of "modus tollens" can be converted to a use of "modus ponens" and one use of transposition to the premise which is a material implication. For example:

Likewise, every use of "modus ponens" can be converted to a use of "modus tollens" and transposition.

The validity of "modus tollens" can be clearly demonstrated through a truth table.

In instances of "modus tollens" we assume as premises that p → q is true and q is false. There is only one line of the truth table—the fourth line—which satisfies these two conditions. In this line, p is false. Therefore, in every instance in which p → q is true and q is false, p must also be false.

"Modus tollens" represents an instance of the law of total probability combined with Bayes' theorem expressed as:

formula_22,

where the conditionals formula_23 and formula_24 are obtained with (the extended form of) Bayes' theorem expressed as:

formula_25 and formula_26.

In the equations above formula_27 denotes the probability of formula_14, and formula_29 denotes the base rate (aka. prior probability) of formula_13. The conditional probability formula_31 generalizes the logical statement formula_2, i.e. in addition to assigning TRUE or FALSE we can also assign any probability to the statement. Assume that formula_33 is equivalent to formula_14 being TRUE, and that formula_35 is equivalent to formula_14 being FALSE. It is then easy to see that formula_37 when formula_38 and formula_35. This is because formula_40 so that formula_41 in the last equation. Therefore, the product terms in the first equation always have a zero factor so that formula_37 which is equivalent to formula_13 being FALSE. Hence, the law of total probability combined with Bayes' theorem represents a generalization of "modus tollens" .

"Modus tollens" represents an instance of the abduction operator in subjective logic expressed as:

formula_44,

where formula_45 denotes the subjective opinion about formula_14, and formula_47 denotes a pair of binomial conditional opinions, as expressed by source formula_48. The parameter formula_49 denotes the base rate (aka. the prior probability) of formula_13. The abduced marginal opinion on formula_13 is denoted formula_52. The conditional opinion formula_53 generalizes the logical statement formula_2, i.e. in addition to assigning TRUE or FALSE the source formula_48 can assign any subjective opinion to the statement. The case where formula_45 is an absolute TRUE opinion is equivalent to source formula_48 saying that formula_14 is TRUE, and the case where formula_45 is an absolute FALSE opinion is equivalent to source formula_48 saying that formula_14 is FALSE. The abduction operator formula_62 of subjective logic produces an absolute FALSE abduced opinion formula_63 when the conditional opinion formula_53 is absolute TRUE and the consequent opinion formula_45 is absolute FALSE. Hence, subjective logic abduction represents a generalization of both "modus tollens" and of the Law of total probability combined with Bayes' theorem .





</doc>
<doc id="18902" url="https://en.wikipedia.org/wiki?curid=18902" title="Mathematician">
Mathematician

A mathematician is someone who uses an extensive knowledge of mathematics in their work, typically to solve mathematical problems.

Mathematics is concerned with numbers, data, quantity, structure, space, models, and change.

One of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.

The number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins.

The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).

Science and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.

The Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).

As time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve.

British universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt's idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”

Mathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.

Mathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.

The discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, "applied mathematicians" look into the "formulation, study, and use of mathematical models" in science, engineering, business, and other areas of mathematical practice.

Pure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as "speculative mathematics", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.

Another insightful view put forth is that "pure mathematics is not necessarily applied mathematics": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.

To develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be "pure" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.

Many professional mathematicians also engage in the teaching of mathematics. Duties may include:

Many careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.

As another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock ("see: Valuation of options; Financial modeling").

According to the Dictionary of Occupational Titles occupations in mathematics include the following.


The following are quotations about mathematicians, or by mathematicians.

There is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.

The American Mathematical Society, Association for Women in Mathematics, and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.

Several well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.




</doc>
<doc id="18906" url="https://en.wikipedia.org/wiki?curid=18906" title="Microfluidics">
Microfluidics

Microfluidics refers to the behaviour, precise control, and manipulation of fluids that are geometrically constrained to a small scale (typically sub-millimeter) at which capillary penetration governs mass transport. It is a multidisciplinary field that involves engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology. It has practical applications in the design of systems that process low volumes of fluids to achieve multiplexing, automation, and high-throughput screening. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies.

Typically, micro means one of the following features:


Typically microfluidic systems transport, mix, separate, or otherwise process fluids. Various applications rely on passive fluid control using capillary forces, in the form of capillary flow modifying elements, akin to flow resistors and flow accelerators. In some applications, external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or microvalves. Micropumps supply fluids in a continuous manner or are used for dosing. Microvalves determine the flow direction or the mode of movement of pumped liquids. Often, processes normally carried out in a lab are miniaturised on a single chip, which enhances efficiency and mobility, and reduces sample and reagent volumes.

The behaviour of fluids at the microscale can differ from "macrofluidic" behaviour in that factors such as surface tension, energy dissipation, and fluidic resistance start to dominate the system. Microfluidics studies how these behaviours change, and how they can be worked around, or exploited for new uses.

At small scales (channel size of around 100 nanometers to 500 micrometers) some interesting and sometimes unintuitive properties appear. In particular, the Reynolds number (which compares the effect of the momentum of a fluid to the effect of viscosity) can become very low. A key consequence is co-flowing fluids do not necessarily mix in the traditional sense, as flow becomes laminar rather than turbulent; molecular transport between them must often be through diffusion.

High specificity of chemical and physical properties (concentration, pH, temperature, shear force, etc.) can also be ensured resulting in more uniform reaction conditions and higher grade products in single and multi-step reactions.

Microfluidic structures include micropneumatic systems, i.e. microsystems for the handling of off-chip fluids (liquid pumps, gas valves, etc.), and microfluidic structures for the on-chip handling of nanoliter (nl) and picoliter (pl) volumes. To date, the most successful commercial application of microfluidics is the inkjet printhead. Additionally, microfluidic manufacturing advances mean that makers can produce the devices in low-cost plastics and automatically verify part quality. 

Advances in microfluidics technology are revolutionizing molecular biology procedures for enzymatic analysis (e.g., glucose and lactate assays), DNA analysis (e.g., polymerase chain reaction and high-throughput sequencing), proteomics, and in chemical synthesis. The basic idea of microfluidic biochips is to integrate assay operations such as detection, as well as sample pre-treatment and sample preparation on one chip.

An emerging application area for biochips is clinical pathology, especially the immediate point-of-care diagnosis of diseases. In addition, microfluidics-based devices, capable of continuous sampling and real-time testing of air/water samples for biochemical toxins and other dangerous pathogens, can serve as an always-on "bio-smoke alarm" for early warning.

Microfluidic technology has led to the creation of powerful tools for biologists to control the complete cellular environment, leading to new questions and discoveries. Many diverse advantages of this technology for microbiology are listed below:

Some of these areas are further elaborated in the sections below.

In open microfluidics, at least one boundary of the system is removed, exposing the fluid to air or another interface (i.e. liquid). Advantages of open microfluidics include accessibility to the flowing liquid for intervention, larger liquid-gas surface area, and minimized bubble formation. Another advantage of open microfluidics is the ability to integrate open systems with surface-tension driven fluid flow, which eliminates the need for external pumping methods such as peristaltic or syringe pumps. Open microfluidic devices are also easy and inexpensive to fabricate by milling, thermoforming, and hot embossing. In addition, open microfluidics eliminates the need to glue or bond a cover for devices, which could be detrimental to capillary flows. Examples of open microfluidics include open-channel microfluidics, rail-based microfluidics, paper-based, and thread-based microfluidics. Disadvantages to open systems include susceptibility to evaporation, contamination, and limited flow rate.

Continuous flow microfluidics rely on the control of a steady state liquid flow through narrow channels or porous media predominantly by accelerating or hindering fluid flow in capillary elements. In paper based microfluidics, capillary elements can be achieved through the simple variation of section geometry. In general, the actuation of liquid flow is implemented either by external pressure sources, external mechanical pumps, integrated mechanical micropumps, or by combinations of capillary forces and electrokinetic mechanisms. Continuous-flow microfluidic operation is the mainstream approach because it is easy to implement and less sensitive to protein fouling problems. Continuous-flow devices are adequate for many well-defined and simple biochemical applications, and for certain tasks such as chemical separation, but they are less suitable for tasks requiring a high degree of flexibility or fluid manipulations. These closed-channel systems are inherently difficult to integrate and scale because the parameters that govern flow field vary along the flow path making the fluid flow at any one location dependent on the properties of the entire system. Permanently etched microstructures also lead to limited reconfigurability and poor fault tolerance capability. Computer-aided design automation approaches for continuous-flow microfluidics have been proposed in recent years to alleviate the design effort and to solve the scalability problems.

Process monitoring capabilities in continuous-flow systems can be achieved with highly sensitive microfluidic flow sensors based on MEMS technology, which offers resolutions down to the nanoliter range.

Droplet-based microfluidics is a subcategory of microfluidics in contrast with continuous microfluidics; droplet-based microfluidics manipulates discrete volumes of fluids in immiscible phases with low Reynolds number and laminar flow regimes. Interest in droplet-based microfluidics systems has been growing substantially in past decades. Microdroplets allow for handling miniature volumes (μl to fl) of fluids conveniently, provide better mixing, encapsulation, sorting, and sensing, and suit high throughput experiments. Exploiting the benefits of droplet-based microfluidics efficiently requires a deep understanding of droplet generation to perform various logical operations such as droplet motion, droplet sorting, droplet merging, and droplet breakup.

Alternatives to the above closed-channel continuous-flow systems include novel open structures, where discrete, independently controllable droplets
are manipulated on a substrate using electrowetting. Following the analogy of digital microelectronics, this approach is referred to as digital microfluidics. Le Pesant et al. pioneered the use of electrocapillary forces to move droplets on a digital track. The "fluid transistor" pioneered by Cytonix also played a role. The technology was subsequently commercialised by Duke University. By using discrete unit-volume droplets, a microfluidic function can be reduced to a set of repeated basic operations, i.e., moving one unit of fluid over one unit of distance. This "digitisation" method facilitates the use of a hierarchical and cell-based approach for microfluidic biochip design. Therefore, digital microfluidics offers a flexible and scalable system architecture as well as high fault-tolerance capability. Moreover, because each droplet can be controlled independently, these systems also have dynamic reconfigurability, whereby groups of unit cells in a microfluidic array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. Although droplets are manipulated in confined microfluidic channels, since the control on droplets is not independent, it should not be confused as "digital microfluidics". One common actuation method for digital microfluidics is electrowetting-on-dielectric (EWOD). Many lab-on-a-chip applications have been demonstrated within the digital microfluidics paradigm using electrowetting. However, recently other techniques for droplet manipulation have also been demonstrated using magnetic force, surface acoustic waves, optoelectrowetting, mechanical actuation, etc.

Paper-based microfluidic devices fill a growing niche for portable, cheap, and user-friendly medical diagnostic systems. 
Paper based microfluidics rely on the phenomenon of capillary penetration in porous media. To tune fluid penetration in porous substrates such as paper in two and three dimensions, the pore structure, wettability and geometry of the microfluidic devices can be controlled while the viscosity and evaporation rate of the liquid play a further significant role. Many such devices feature hydrophobic barriers on hydrophilic paper that passively transport aqueous solutions to outlets where biological reactions take place. Current applications include portable glucose detection and environmental testing, with hopes of reaching areas that lack advanced medical diagnostic tools.

Early biochips were based on the idea of a DNA microarray, e.g., the GeneChip DNAarray from Affymetrix, which is a piece of glass, plastic or silicon substrate, on which pieces of DNA (probes) are affixed in a microscopic array. Similar to a DNA microarray, a protein array is a miniature array where a multitude of different capture agents, most frequently monoclonal antibodies, are deposited on a chip surface; they are used to determine the presence and/or amount of proteins in biological samples, e.g., blood. A drawback of DNA and protein arrays is that they are neither reconfigurable nor scalable after manufacture. Digital microfluidics has been described as a means for carrying out Digital PCR.

In addition to microarrays, biochips have been designed for two-dimensional electrophoresis, transcriptome analysis, and PCR amplification. Other applications include various electrophoresis and liquid chromatography applications for proteins and DNA, cell separation, in particular, blood cell separation, protein analysis, cell manipulation and analysis including cell viability analysis and microorganism capturing.

By combining microfluidics with landscape ecology and nanofluidics, a nano/micro fabricated fluidic landscape can be constructed by building local patches of bacterial habitat and connecting them by dispersal corridors. The resulting landscapes can be used as physical implementations of an adaptive landscape, by generating a spatial mosaic of patches of opportunity distributed in space and time. The patchy nature of these fluidic landscapes allows for the study of adapting bacterial cells in a metapopulation system. The evolutionary ecology of these bacterial systems in these synthetic ecosystems allows for using biophysics to address questions in evolutionary biology.

The ability to create precise and carefully controlled chemoattractant gradients makes microfluidics the ideal tool to study motility , chemotaxis and the ability to evolve / develop resistance to antibiotics in small populations of microorganisms and in a short period of time. These microorganisms including bacteria and the broad range of organisms that form the marine microbial loop, responsible for regulating much of the oceans' biogeochemistry.

Microfluidics has also greatly aided the study of durotaxis by facilitating the creation of durotactic (stiffness) gradients.

By rectifying the motion of individual swimming bacteria, microfluidic structures can be used to extract mechanical motion from a population of motile bacterial cells. This way, bacteria-powered rotors can be built.

The merger of microfluidics and optics is typical known as optofluidics. Examples of optofluidic devices are tunable microlens arrays and optofluidic microscopes.

Microfluidic flow enables fast sample throughput, automated imaging of large sample populations, as well as 3D capabilities. or superresolution.

Acoustic droplet ejection uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample to eject droplets as small as a millionth of a millionth of a litre (picoliter = 10 litre). ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.

Microfluidic fuel cells can use laminar flow to separate the fuel and its oxidant to control the interaction of the two fluids without the physical barrier that conventional fuel cells require.

To understand the prospects for life to exist elsewhere in the universe, astrobiologists are interested in measuring the chemical composition of extraplanetary bodies. Because of their small size and wide-ranging functionality, microfluidic devices are uniquely suited for these remote sample analyses. From an extraterrestrial sample, the organic content can be assessed using microchip capillary electrophoresis and selective fluorescent dyes. These devices are capable of detecting amino acids, peptides, fatty acids, and simple aldehydes, ketones, and thiols. These analyses coupled together could allow powerful detection of the key components of life, and hopefully inform our search for functioning extraterrestrial life.







</doc>
<doc id="18908" url="https://en.wikipedia.org/wiki?curid=18908" title="Mersenne prime">
Mersenne prime

In mathematics, a Mersenne prime is a prime number that is one less than a power of two. That is, it is a prime number of the form for some integer . They are named after Marin Mersenne, a French Minim friar, who studied them in the early 17th century.

The exponents which give Mersenne primes are 2, 3, 5, 7, 13, 17, 19, 31, ... and the resulting Mersenne primes are 3, 7, 31, 127, 8191, 131071, 524287, 2147483647, ... .

If is a composite number then so is . ( is divisible by both and .) This definition is therefore equivalent to the definition as a prime number of the form for some prime .

More generally, numbers of the form without the primality requirement may be called Mersenne numbers. Sometimes, however, Mersenne numbers are defined to have the additional requirement that be prime.
The smallest composite Mersenne number with prime exponent "n" is .

Mersenne primes are also noteworthy due to their connection to perfect numbers.

, 51 Mersenne primes are known. The largest known prime number, , is a Mersenne prime. Since 1997, all newly found Mersenne primes have been discovered by the Great Internet Mersenne Prime Search (GIMPS), a distributed computing project on the Internet.

Many fundamental questions about Mersenne primes remain unresolved. It is not even known whether the set of Mersenne primes is finite or infinite. The Lenstra–Pomerance–Wagstaff conjecture asserts that there are infinitely many Mersenne primes and predicts their order of growth. It is also not known whether infinitely many Mersenne numbers with prime exponents are composite, although this would follow from widely believed conjectures about prime numbers, for example, the infinitude of Sophie Germain primes congruent to 3 (mod 4). For these primes , (which is also prime) will divide , for example, , , , , , , , and . Since for these primes , is congruent to 7 mod 8, so 2 is a quadratic residue mod , and the multiplicative order of 2 mod must divide formula_1 = . Since is a prime, it must be or 1. However, it cannot be 1 since formula_2 and 1 has no prime factors, so it must be . Hence, divides formula_3 and formula_4 cannot be prime.

The first four Mersenne primes are , , and and because the first Mersenne prime starts at , all Mersenne primes are congruent to 3 (mod 4). Other than and , all other Mersenne numbers are also congruent to 3 (mod 4). Consequently, in the prime factorization of a Mersenne number (  ) there must be at least one prime factor congruent to 3 (mod 4).

A basic theorem about Mersenne numbers states that if is prime, then the exponent must also be prime. This follows from the identity

This rules out primality for Mersenne numbers with composite exponent, such as .

Though the above examples might suggest that is prime for all primes , this is not the case, and the smallest counterexample is the Mersenne number

The evidence at hand suggests that a randomly selected Mersenne number is much more likely to be prime than an arbitrary randomly selected odd integer of similar size. Nonetheless, prime values of appear to grow increasingly sparse as increases. For example, eight of the first 11 primes give rise to a Mersenne prime (the correct terms on Mersenne's original list), while is prime for only 43 of the first two million prime numbers (up to 32,452,843).

The lack of any simple test to determine whether a given Mersenne number is prime makes the search for Mersenne primes a difficult task, since Mersenne numbers grow very rapidly. The Lucas–Lehmer primality test (LLT) is an efficient primality test that greatly aids this task, making it much easier to test the primality of Mersenne numbers than that of most other numbers of the same size. The search for the largest known prime has somewhat of a cult following. Consequently, a lot of computer power has been expended searching for new Mersenne primes, much of which is now done using distributed computing.

Arithmetic modulo a Mersennne number is particularly efficient on a binary computer, making them popular choices when a prime modulus is desired, such as the Park–Miller random number generator. To find a primitive polynomial of Mersenne number order requires knowing the factorization of that number, so Mersenne primes allow one to find to primitive polynomials of very high order. Such primitive trinomials are used in pseudorandom number generators with very large periods such as the Mersenne twister, generalized shift register and Lagged Fibonacci generators.

Mersenne primes are also noteworthy due to their connection with perfect numbers. In the 4th century BC, Euclid proved that if is prime, then ) is a perfect number. This number, also expressible as , is the th triangular number and the th hexagonal number. In the 18th century, Leonhard Euler proved that, conversely, all even perfect numbers have this form. This is known as the Euclid–Euler theorem. It is unknown whether there are any odd perfect numbers.

Mersenne primes take their name from the 17th-century French scholar Marin Mersenne, who compiled what was supposed to be a list of Mersenne primes with exponents up to 257. The exponents listed by Mersenne were as follows:

His list replicated the known primes of his time with exponents up to 19. His next entry, 31, was correct, but the list then became largely incorrect, as Mersenne mistakenly included and (which are composite) and omitted , , and (which are prime). Mersenne gave little indication how he came up with his list.

Édouard Lucas proved in 1876 that is indeed prime, as Mersenne claimed. This was the largest known prime number for 75 years, and the largest ever found by hand. was determined to be prime in 1883 by Ivan Mikheevich Pervushin, though Mersenne claimed it was composite, and for this reason it is sometimes called Pervushin's number. This was the second-largest known prime number, and it remained so until 1911. Lucas had shown another error in Mersenne's list in 1876. Without finding a factor, Lucas demonstrated that is actually composite. No factor was found until a famous talk by Frank Nelson Cole in 1903. Without speaking a word, he went to a blackboard and raised 2 to the 67th power, then subtracted one. On the other side of the board, he multiplied and got the same number, then returned to his seat (to applause) without speaking. He later said that the result had taken him "three years of Sundays" to find. A correct list of all Mersenne primes in this number range was completed and rigorously verified only about three centuries after Mersenne published his list.

Fast algorithms for finding Mersenne primes are available, and the seven largest known prime numbers are Mersenne primes.

The first four Mersenne primes , , and were known in antiquity. The fifth, , was discovered anonymously before 1461; the next two ( and ) were found by Pietro Cataldi in 1588. After nearly two centuries, was verified to be prime by Leonhard Euler in 1772. The next (in historical, not numerical order) was , found by Édouard Lucas in 1876, then by Ivan Mikheevich Pervushin in 1883. Two more ( and ) were found early in the 20th century, by R. E. Powers in 1911 and 1914, respectively.

The best method presently known for testing the primality of Mersenne numbers is the Lucas–Lehmer primality test. Specifically, it can be shown that for prime , is prime if and only if divides , where and for .

During the era of manual calculation, all the exponents up to and including 257 were tested with the Lucas–Lehmer test and found to be composite. A notable contribution was made by retired Yale physics professor Horace Scudder Uhler, who did the calculations for exponents 157, 167, 193, 199, 227, and 229. Unfortunately for those investigators, the interval they were testing contains the largest known relative gap between Mersenne primes; in relative terms: the next Mersenne prime exponent, 521, would turn out to be more than four times larger than the previous record of 127.
The search for Mersenne primes was revolutionized by the introduction of the electronic digital computer. Alan Turing searched for them on the Manchester Mark 1 in 1949, but the first successful identification of a Mersenne prime, , by this means was achieved at 10:00 pm on January 30, 1952 using the U.S. National Bureau of Standards Western Automatic Computer (SWAC) at the Institute for Numerical Analysis at the University of California, Los Angeles, under the direction of Lehmer, with a computer search program written and run by Prof. R. M. Robinson. It was the first Mersenne prime to be identified in thirty-eight years; the next one, , was found by the computer a little less than two hours later. Three more — , ,  — were found by the same program in the next several months. is the first Mersenne prime that is titanic, is the first gigantic, and was the first megaprime to be discovered, being a prime with at least 1,000,000 digits. All three were the first known prime of any kind of that size. The number of digits in the decimal representation of equals , where denotes the floor function (or equivalently ).

In September 2008, mathematicians at UCLA participating in GIMPS won part of a $100,000 prize from the Electronic Frontier Foundation for their discovery of a very nearly 13-million-digit Mersenne prime. The prize, finally confirmed in October 2009, is for the first known prime with at least 10 million digits. The prime was found on a Dell OptiPlex 745 on August 23, 2008. This was the eighth Mersenne prime discovered at UCLA.

On April 12, 2009, a GIMPS server log reported that a 47th Mersenne prime had possibly been found. The find was first noticed on June 4, 2009, and verified a week later. The prime is . Although it is chronologically the 47th Mersenne prime to be discovered, it is smaller than the largest known at the time, which was the 45th to be discovered.

On January 25, 2013, Curtis Cooper, a mathematician at the University of Central Missouri, discovered a 48th Mersenne prime, (a number with 17,425,170 digits), as a result of a search executed by a GIMPS server network.

On January 19, 2016, Cooper published his discovery of a 49th Mersenne prime, (a number with 22,338,618 digits), as a result of a search executed by a GIMPS server network. This was the fourth Mersenne prime discovered by Cooper and his team in the past ten years.

On September 2, 2016, the Great Internet Mersenne Prime Search finished verifying all tests below M, thus officially confirming its position as the 45th Mersenne prime.

On January 3, 2018, it was announced that Jonathan Pace, a 51-year-old electrical engineer living in Germantown, Tennessee, had found a 50th Mersenne prime, (a number with 23,249,425 digits), as a result of a search executed by a GIMPS server network.

On December 21, 2018, it was announced that The Great Internet Mersenne Prime Search (GIMPS) discovered the largest known prime number, , having 24,862,048 digits. A computer volunteered by Patrick Laroche from Ocala, Florida made the find on December 7, 2018.


The table below lists all known Mersenne primes (sequence () and () in OEIS):

All Mersenne numbers below the 51st Mersenne prime () have been tested at least once but some have not been double-checked. Primes are not always discovered in increasing order. For example, the 29th Mersenne prime was discovered "after" the 30th and the 31st. Similarly, was followed by two smaller Mersenne primes, first 2 weeks later and then 9 months later. was the first discovered prime number with more than 10 million decimal digits.

The largest known Mersenne prime is also the largest known prime number.

The largest known prime has been a Mersenne prime since 1952, except between 1989 and 1992.

Since they are prime numbers, Mersenne primes are divisible only by 1 and by themselves. However, not all Mersenne numbers are Mersenne primes, and the composite Mersenne numbers may be factored non-trivially. Mersenne numbers are very good test cases for the special number field sieve algorithm, so often the largest number factorized with this algorithm has been a Mersenne number. , 2 − 1 is the record-holder, having been factored with a variant of the special number field sieve that allows the factorization of several numbers at once. See integer factorization records for links to more information. The special number field sieve can factorize numbers with more than one large factor. If a number has only one very large factor then other algorithms can factorize larger numbers by first finding small factors and then making a primality test on the cofactor. , the largest factorization with probable prime factors allowed is , where is a 2,201,714-digit probable prime. It was discovered by Oliver Kruse. , the Mersenne number "M" is the smallest composite Mersenne number with no known factors; it has no prime factors below 2.

The table below shows factorizations for the first 20 composite Mersenne numbers .

The number of factors for the first 500 Mersenne numbers can be found at .

In the mathematical problem Tower of Hanoi, solving a puzzle with an -disc tower requires steps, assuming no mistakes are made. The number of rice grains on the whole chessboard in the wheat and chessboard problem is .

The asteroid with minor planet number 8191 is named 8191 Mersenne after Marin Mersenne, because 8191 is a Mersenne prime (3 Juno, 7 Iris, 31 Euphrosyne and 127 Johanna having been discovered and named during the 19th century).

In geometry, an integer right triangle that is primitive and has its even leg a power of 2 (  ) generates a unique right triangle such that its inradius is always a Mersenne number. For example, if the even leg is then because it is primitive it constrains the odd leg to be , the hypotenuse to be and its inradius to be .

The Mersenne numbers were studied with respect to the total number of accepting paths of non-deterministic polynomial time Turing machines in 2018 and intriguing inclusions were discovered.

A Mersenne–Fermat number is defined as , with prime, natural number, and can be written as , when , it is a Mersenne number, and when , it is a Fermat number, the only known Mersenne–Fermat prime with are

In fact, , where is the cyclotomic polynomial.

The simplest generalized Mersenne primes are prime numbers of the form , where is a low-degree polynomial with small integer coefficients. An example is , in this case, , and ; another example is , in this case, , and .

It is also natural to try to generalize primes of the form to primes of the form (for and ). However (see also theorems above), is always divisible by , so unless the latter is a unit, the former is not a prime. This can be remedied by allowing "b" to be an algebraic integer instead of an integer:

In the ring of integers (on real numbers), if is a unit, then is either 2 or 0. But are the usual Mersenne primes, and the formula does not lead to anything interesting (since it is always −1 for all ). Thus, we can regard a ring of "integers" on complex numbers instead of real numbers, like Gaussian integers and Eisenstein integers.

If we regard the ring of Gaussian integers, we get the case and , and can ask (WLOG) for which the number is a Gaussian prime which will then be called a Gaussian Mersenne prime.

Like the sequence of exponents for usual Mersenne primes, this sequence contains only (rational) prime numbers.

As for all Gaussian primes, the norms (that is, squares of absolute values) of these numbers are rational primes:

We can also regard the ring of Eisenstein integers, we get the case and , and can ask for what the number is an "Eisenstein prime" which will then be called a Eisenstein Mersenne prime.

The norms (that is, squares of absolute values) of these Eisenstein primes are rational primes:

The other way to deal with the fact that is always divisible by , it is to simply take out this factor and ask which values of make
be prime. (The integer can be either positive or negative.) If, for example, we take , we get values of:
These primes are called repunit primes. Another example is when we take , we get values of:
It is a conjecture that for every integer which is not a perfect power, there are infinitely many values of such that is prime. (When is a perfect power, it can be shown that there is at most one value such that is prime)

Least such that is prime are (starting with , if no such exists)

For negative bases , they are (starting with , if no such exists)

Least base such that is prime are

For negative bases , they are

Another generalized Mersenne number is
with , any coprime integers, and . (Since is always divisible by , the division is necessary for there to be any chance of finding prime numbers. In fact, this number is the same as the Lucas number , since and are the roots of the quadratic equation , and this number equals 1 when ) We can ask which makes this number prime. It can be shown that such must be primes themselves or equal to 4, and can be 4 if and only if and is prime. (Since . Thus, in this case the pair must be and must be prime. That is, must be in .) It is a conjecture that for any pair such that for every natural number , and are not both perfect th powers, and is not a perfect fourth power. there are infinitely many values of such that is prime. (When and are both perfect th powers for an or when is a perfect fourth power, it can be shown that there are at most two values with this property, since if so, then can be factored algebraically) However, this has not been proved for any single value of .

Note: if and is even, then the numbers are not included in the corresponding OEIS sequence.

A conjecture related to the generalized Mersenne primes: (the conjecture predicts where is the next generalized Mersenne prime, if the conjecture is true, then there are infinitely many primes for all such pairs)

For any integers and which satisfy the conditions:

has prime numbers of the form

for prime , the prime numbers will be distributed near the best fit line

where

and there are about

prime numbers of this form less than .


We also have the following three properties:


If this conjecture is true, then for all such pairs, let be the th prime of the form , the graph of versus is almost linear. (See )

When , it is , a difference of two consecutive perfect th powers, and if is prime, then must be , because it is divisible by .

Least such that is prime are

Least such that is prime are




</doc>
<doc id="18909" url="https://en.wikipedia.org/wiki?curid=18909" title="Magnesium">
Magnesium

Magnesium is a chemical element with the symbol Mg and atomic number 12. It is a shiny gray solid which bears a close physical resemblance to the other five elements in the second column (group 2, or alkaline earth metals) of the periodic table: all group 2 elements have the same electron configuration in the outer electron shell and a similar crystal structure.

Magnesium is the ninth most abundant element in the universe. It is produced in large, aging stars from the sequential addition of three helium nuclei to a carbon nucleus. When such stars explode as supernovas, much of the magnesium is expelled into the interstellar medium where it may recycle into new star systems. Magnesium is the eighth most abundant element in the Earth's crust and the fourth most common element in the Earth (after iron, oxygen and silicon), making up 13% of the planet's mass and a large fraction of the planet's mantle. It is the third most abundant element dissolved in seawater, after sodium and chlorine.

Magnesium occurs naturally only in combination with other elements, where it invariably has a +2 oxidation state. The free element (metal) can be produced artificially, and is highly reactive (though in the atmosphere, it is soon coated in a thin layer of oxide that partly inhibits reactivity – see passivation). The free metal burns with a characteristic brilliant-white light. The metal is now obtained mainly by electrolysis of magnesium salts obtained from brine, and is used primarily as a component in aluminium-magnesium alloys, sometimes called "magnalium" or "magnelium". Magnesium is less dense than aluminium, and the alloy is prized for its combination of lightness and strength.

Magnesium is the eleventh most abundant element by mass in the human body and is essential to all cells and some 300 enzymes. Magnesium ions interact with polyphosphate compounds such as ATP, DNA, and RNA. Hundreds of enzymes require magnesium ions to function. Magnesium compounds are used medicinally as common laxatives, antacids (e.g., milk of magnesia), and to stabilize abnormal nerve excitation or blood vessel spasm in such conditions as eclampsia.
Elemental magnesium is a gray-white lightweight metal, two-thirds the density of aluminium. Magnesium has the lowest melting () and the lowest boiling point of all the alkaline earth metals.

Pure polycrystalline magnesium is brittle and easily fractures along shear bands. It becomes much more ductile when alloyed with small amount of other metals, such as 1% aluminium. Ductility of polycrystalline magnesium can also be significantly improved by reducing its grain size to ca. 1 micron or less.

It tarnishes slightly when exposed to air, although, unlike the heavier alkaline earth metals, an oxygen-free environment is unnecessary for storage because magnesium is protected by a thin layer of oxide that is fairly impermeable and difficult to remove.

Magnesium reacts with water at room temperature, though it reacts much more slowly than calcium, a similar group 2 metal. When submerged in water, hydrogen bubbles form slowly on the surface of the metal – though, if powdered, it reacts much more rapidly. The reaction occurs faster with higher temperatures (see safety precautions). Magnesium's reversible reaction with water can be harnessed to store energy and run a magnesium-based engine. Magnesium also reacts exothermically with most acids such as hydrochloric acid (HCl), producing the metal chloride and hydrogen gas, similar to the HCl reaction with aluminium, zinc, and many other metals.

Magnesium is highly flammable, especially when powdered or shaved into thin strips, though it is difficult to ignite in mass or bulk. Flame temperatures of magnesium and magnesium alloys can reach , although flame height above the burning metal is usually less than . Once ignited, such fires are difficult to extinguish, because combustion continues in nitrogen (forming magnesium nitride), carbon dioxide (forming magnesium oxide and carbon), and water (forming magnesium oxide and hydrogen, which also combusts due to heat in the presence of additional oxygen). This property was used in incendiary weapons during the firebombing of cities in World War II, where the only practical civil defense was to smother a burning flare under dry sand to exclude atmosphere from the combustion.

Magnesium may also be used as an igniter for thermite, a mixture of aluminium and iron oxide powder that ignites only at a very high temperature.

Organomagnesium compounds are widespread in organic chemistry. They are commonly found as Grignard reagents. Magnesium can react with haloalkanes to give Grignard reagents. Examples of Grignard reagents are phenylmagnesium bromide and ethylmagnesium bromide. The Grignard reagents function as a common nucleophile, attacking the electrophilic group such as the carbon atom that is present within the polar bond of a carbonyl group.

A prominent organomagnesium reagent beyond Grignard reagents is magnesium anthracene with magnesium forming a 1,4-bridge over the central ring. It is used as a source of highly active magnesium. The related butadiene-magnesium adduct serves as a source for the butadiene dianion.

When burning in air, magnesium produces a brilliant-white light that includes strong ultraviolet wavelengths. Magnesium powder (flash powder) was used for subject illumination in the early days of photography. Later, magnesium filament was used in electrically ignited single-use photography flashbulbs. Magnesium powder is used in fireworks and marine flares where a brilliant white light is required. It was also used for various theatrical effects, such as lightning, pistol flashes, and supernatural appearances.

Magnesium is the eighth-most-abundant element in the Earth's crust by mass and tied in seventh place with iron in molarity. It is found in large deposits of magnesite, dolomite, and other minerals, and in mineral waters, where magnesium ion is soluble.

Although magnesium is found in more than 60 minerals, only dolomite, magnesite, brucite, carnallite, talc, and olivine are of commercial importance.

The cation is the second-most-abundant cation in seawater (about ⅛ the mass of sodium ions in a given sample), which makes seawater and sea salt attractive commercial sources for Mg. To extract the magnesium, calcium hydroxide is added to seawater to form magnesium hydroxide precipitate.

Magnesium hydroxide (brucite) is insoluble in water and can be filtered out and reacted with hydrochloric acid to produced concentrated magnesium chloride.
From magnesium chloride, electrolysis produces magnesium.

As of 2013, magnesium alloys consumption was less than one million tonnes per year, compared with 50 million tonnes of aluminum alloys. Their use has been historically limited by the tendency of Mg alloys to corrode, creep at high temperatures, and combust.

The presence of iron, nickel, copper, and cobalt strongly activates corrosion. Greater than a very small percentage, these metals precipitate as intermetallic compounds, and the precipitate locales function as active cathodic sites that reduce water, causing the loss of magnesium. Controlling the quantity of these metals improves corrosion resistance. Sufficient manganese overcomes the corrosive effects of iron. This requires precise control over composition, increasing costs. Adding a cathodic poison captures atomic hydrogen within the structure of a metal. This prevents the formation of free hydrogen gas, an essential factor of corrosive chemical processes. The addition of about one in three hundred parts arsenic reduces its corrosion rate in a salt solution by a factor of nearly ten.

Research showed that magnesium's tendency to creep at high temperatures is eliminated by the addition of scandium and gadolinium. Flammability is greatly reduced by a small amount of calcium in the alloy.

Magnesium forms a variety of compounds important to industry and biology, including magnesium carbonate, magnesium chloride, magnesium citrate, magnesium hydroxide (milk of magnesia), magnesium oxide, magnesium sulfate, and magnesium sulfate heptahydrate (Epsom salts).

Magnesium has three stable isotopes: , and . All are present in significant amounts (see table of isotopes above). About 79% of Mg is . The isotope is radioactive and in the 1950s to 1970s was produced by several nuclear power plants for use in scientific experiments. This isotope has a relatively short half-life (21 hours) and its use was limited by shipping times.

The nuclide has found application in isotopic geology, similar to that of aluminium. is a radiogenic daughter product of , which has a half-life of 717,000 years. Excessive quantities of stable have been observed in the Ca-Al-rich inclusions of some carbonaceous chondrite meteorites. This anomalous abundance is attributed to the decay of its parent in the inclusions, and researchers conclude that such meteorites were formed in the solar nebula before the had decayed. These are among the oldest objects in the solar system and contain preserved information about its early history.

It is conventional to plot / against an Al/Mg ratio. In an isochron dating plot, the Al/Mg ratio plotted is/. The slope of the isochron has no age significance, but indicates the initial / ratio in the sample at the time when the systems were separated from a common reservoir.

World production was approximately 1,100 kt in 2017, with the bulk being produced in China (930 kt) and Russia (60 kt). China is almost completely reliant on the silicothermic Pidgeon process (the reduction of the oxide at high temperatures with silicon, often provided by a ferrosilicon alloy in which the iron is but a spectator in the reactions) to obtain the metal. The process can also be carried out with carbon at approx 2300 °C:

In the United States, magnesium is obtained principally with the Dow process, by electrolysis of fused magnesium chloride from brine and sea water. A saline solution containing ions is first treated with lime (calcium oxide) and the precipitated magnesium hydroxide is collected:

The hydroxide is then converted to a partial hydrate of magnesium chloride by treating the hydroxide with hydrochloric acid and heating of the product:

The salt is then electrolyzed in the molten state. At the cathode, the ion is reduced by two electrons to magnesium metal:

At the anode, each pair of ions is oxidized to chlorine gas, releasing two electrons to complete the circuit:

A new process, solid oxide membrane technology, involves the electrolytic reduction of MgO. At the cathode, ion is reduced by two electrons to magnesium metal. The electrolyte is yttria-stabilized zirconia (YSZ). The anode is a liquid metal. At the YSZ/liquid metal anode is oxidized. A layer of graphite borders the liquid metal anode, and at this interface carbon and oxygen react to form carbon monoxide. When silver is used as the liquid metal anode, there is no reductant carbon or hydrogen needed, and only oxygen gas is evolved at the anode. It has been reported that this method provides a 40% reduction in cost per pound over the electrolytic reduction method. This method is more environmentally sound than others because there is much less carbon dioxide emitted.

The United States has traditionally been the major world supplier of this metal, supplying 45% of world production even as recently as 1995. Today, the US market share is at 7%, with a single domestic producer left, US Magnesium, a Renco Group company in Utah born from now-defunct Magcorp.

The name magnesium originates from the Greek word for a district in Thessaly called Magnesia. It is related to magnetite and manganese, which also originated from this area, and required differentiation as separate substances. See manganese for this history.

In 1618, a farmer at Epsom in England attempted to give his cows water from a well there. The cows refused to drink because of the water's bitter taste, but the farmer noticed that the water seemed to heal scratches and rashes. The substance became known as Epsom salts and its fame spread. It was eventually recognized as hydrated magnesium sulfate, ·7.

The metal itself was first isolated by Sir Humphry Davy in England in 1808. He used electrolysis on a mixture of magnesia and mercuric oxide. Antoine Bussy prepared it in coherent form in 1831. Davy's first suggestion for a name was magnium, but the name magnesium is now used.

Magnesium is the third-most-commonly-used structural metal, following iron and aluminium. The main applications of magnesium are, in order: aluminium alloys, die-casting (alloyed with zinc), removing sulfur in the production of iron and steel, and the production of titanium in the Kroll process. Magnesium is used in super-strong, lightweight materials and alloys. For example, when infused with silicon carbide nanoparticles, it has extremely high specific strength.

Historically, magnesium was one of the main aerospace construction metals and was used for German military aircraft as early as World War I and extensively for German aircraft in World War II. The Germans coined the name "Elektron" for magnesium alloy, a term which is still used today. In the commercial aerospace industry, magnesium was generally restricted to engine-related components, due to fire and corrosion hazards. Currently, magnesium alloy use in aerospace is increasing, driven by the importance of fuel economy. Development and testing of new magnesium alloys continues, notably Elektron 21, which (in test) has proved suitable for aerospace engine, internal, and airframe components. The European Community runs three R&D magnesium projects in the Aerospace priority of Six Framework Program.

In the form of thin ribbons, magnesium is used to purify solvents; for example, preparing super-dry ethanol.


Both AJ62A and AE44 are recent developments in high-temperature low-creep magnesium alloys. The general strategy for such alloys is to form intermetallic precipitates at the grain boundaries, for example by adding mischmetal or calcium. New alloy development and lower costs that make magnesium competitive with aluminium will increase the number of automotive applications.

Because of low weight and good mechanical and electrical properties, magnesium is widely used for manufacturing of mobile phones, laptop and tablet computers, cameras, and other electronic components.
Magnesium, being readily available and relatively nontoxic, has a variety of uses:

Magnesium metal and its alloys can be explosive hazards; they are highly flammable in their pure form when molten or in powder or ribbon form. Burning or molten magnesium reacts violently with water. When working with powdered magnesium, safety glasses with eye protection and UV filters (such as welders use) are employed because burning magnesium produces ultraviolet light that can permanently damage the retina of a human eye.

Magnesium is capable of reducing water and releasing highly flammable hydrogen gas:

Therefore, water cannot extinguish magnesium fires. The hydrogen gas produced intensifies the fire. Dry sand is an effective smothering agent, but only on relatively level and flat surfaces.

Magnesium reacts with carbon dioxide exothermically to form magnesium oxide and carbon:

Hence, carbon dioxide fuels rather than extinguishes magnesium fires.

Burning magnesium can be quenched by using a Class D dry chemical fire extinguisher, or by covering the fire with sand or magnesium foundry flux to remove its air source.

Magnesium compounds, primarily magnesium oxide (MgO), are used as a refractory material in furnace linings for producing iron, steel, nonferrous metals, glass, and cement. Magnesium oxide and other magnesium compounds are also used in the agricultural, chemical, and construction industries. Magnesium oxide from calcination is used as an electrical insulator in fire-resistant cables.

Magnesium reacted with an alkyl halide gives a Grignard reagent, which is a very useful tool for preparing alcohols.

Magnesium salts are included in various foods, fertilizers (magnesium is a component of chlorophyll), and microbe culture media.

Magnesium sulfite is used in the manufacture of paper (sulfite process).

Magnesium phosphate is used to fireproof wood used in construction.

Magnesium hexafluorosilicate is used for moth-proofing textiles.

The important interaction between phosphate and magnesium ions makes magnesium essential to the basic nucleic acid chemistry of all cells of all known living organisms. More than 300 enzymes require magnesium ions for their catalytic action, including all enzymes using or synthesizing ATP and those that use other nucleotides to synthesize DNA and RNA. The ATP molecule is normally found in a chelate with a magnesium ion.

 Spices, nuts, cereals, cocoa and vegetables are rich sources of magnesium. Green leafy vegetables such as spinach are also rich in magnesium.

In the UK, the recommended daily values for magnesium are 300 mg for men and 270 mg for women. In the U.S. the Recommended Dietary Allowances (RDAs) are 400 mg for men ages 19–30 and 420 mg for older; for women 310 mg for ages 19–30 and 320 mg for older.

Numerous pharmaceutical preparations of magnesium and dietary supplements are available. In two human trials magnesium oxide, one of the most common forms in magnesium dietary supplements because of its high magnesium content per weight, was less bioavailable than magnesium citrate, chloride, lactate or aspartate.

An adult has 22–26 grams of magnesium, with 60% in the skeleton, 39% intracellular (20% in skeletal muscle), and 1% extracellular. Serum levels are typically 0.7–1.0 mmol/L or 1.8–2.4 mEq/L. Serum magnesium levels may be normal even when intracellular magnesium is deficient. The mechanisms for maintaining the magnesium level in the serum are varying gastrointestinal absorption and renal excretion. Intracellular magnesium is correlated with intracellular potassium. Increased magnesium lowers calcium and can either prevent hypercalcemia or cause hypocalcemia depending on the initial level. Both low and high protein intake conditions inhibit magnesium absorption, as does the amount of phosphate, phytate, and fat in the gut. Unabsorbed dietary magnesium is excreted in feces; absorbed magnesium is excreted in urine and sweat.

Magnesium status may be assessed by measuring serum and erythrocyte magnesium concentrations coupled with urinary and fecal magnesium content, but intravenous magnesium loading tests are more accurate and practical. A retention of 20% or more of the injected amount indicates deficiency. No biomarker has been established for magnesium.

Magnesium concentrations in plasma or serum may be monitored for efficacy and safety in those receiving the drug therapeutically, to confirm the diagnosis in potential poisoning victims, or to assist in the forensic investigation in a case of fatal overdose. The newborn children of mothers who received parenteral magnesium sulfate during labor may exhibit toxicity with normal serum magnesium levels.

Low plasma magnesium (hypomagnesemia) is common: it is found in 2.5–15% of the general population. From 2005 to 2006, 48 percent of the United States population consumed less magnesium than recommended in the Dietary Reference Intake. Other causes are increased renal or gastrointestinal loss, an increased intracellular shift, and proton-pump inhibitor antacid therapy. Most are asymptomatic, but symptoms referable to neuromuscular, cardiovascular, and metabolic dysfunction may occur. Alcoholism is often associated with magnesium deficiency. Chronically low serum magnesium levels are associated with metabolic syndrome, diabetes mellitus type 2, fasciculation, and hypertension.


Sorted by type of magnesium salt, other therapeutic applications include:

Overdose from dietary sources alone is unlikely because excess magnesium in the blood is promptly filtered by the kidneys, and overdose is more likely in the presence of impaired renal function. In spite of this, megadose therapy has caused death in a young child, and severe hypermagnesemia in a woman and a young girl who had healthy kidneys.
The most common symptoms of overdose are nausea, vomiting, and diarrhea; other symptoms include hypotension, confusion, slowed heart and respiratory rates, deficiencies of other minerals, coma, cardiac arrhythmia, and death from cardiac arrest.

Plants require magnesium to synthesize chlorophyll, essential for photosynthesis. Magnesium in the center of the porphyrin ring in chlorophyll functions in a manner similar to the iron in the center of the porphyrin ring in heme. Magnesium deficiency in plants causes late-season yellowing between leaf veins, especially in older leaves, and can be corrected by either applying epsom salts (which is rapidly leached), or crushed dolomitic limestone, to the soil.




</doc>
<doc id="18910" url="https://en.wikipedia.org/wiki?curid=18910" title="Markup language">
Markup language

In computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text. The idea and terminology evolved from the "marking up" of paper manuscripts (i.e., the revision instructions by editors), which is traditionally written with a red or blue pencil on authors' manuscripts. In digital media, this "blue pencil instruction text" was replaced by tags, which indicate what the parts of the document "are", rather than details of how they might be shown on some display. This lets authors avoid formatting every instance of the same kind of thing redundantly (and possibly inconsistently). It also avoids the specification of fonts and dimensions which may not apply to many users (such as those with different-size displays, impaired vision and screen-reading software).

Early markup systems typically included typesetting instructions, as troff, TeX and LaTeX do, while Scribe and most modern markup systems name components, and later process those names to apply formatting or other processing, as in the case of XML.

Some markup languages, such as the widely used HTML, have pre-defined presentation semantics—meaning that their specification prescribes generally how to present the structured data on particular media. Others, such as XML and its predecessor SGML, allow but do not impose such prescriptions—all of the while allowing users to define any custom document components as they wish.

HyperText Markup Language (HTML), one of the document formats of the World Wide Web, is an application of SGML and XML. Other applications, such as DocBook, Open eBook, JATS and others, are heavily used in the communication of work between authors, editors, and printers.

The term "markup" is derived from the traditional publishing practice of ""marking up"" a manuscript, which involves adding handwritten annotations in the form of conventional symbolic printer's instructions — in the margins and the text of a paper or a printed manuscript. It is a jargon used in coding proof. For centuries, this task was done primarily by skilled typographers known as "markup men" or "d markers" who marked up text to indicate what typeface, style, and size should be applied to each part, and then passed the manuscript to others for typesetting by hand or machine. Markup was also commonly applied by editors, proofreaders, publishers, and graphic designers, and indeed by document authors, all of whom might also mark other things, such as corrections, changes, etc.

There are three main general categories of electronic markup, articulated in Coombs, et al. (1987), and Bray (2003).




There is considerable blurring of the lines between the types of markup. In modern word-processing systems, presentational markup is often saved in descriptive-markup-oriented systems such as XML, and then processed procedurally by implementations. The programming in procedural-markup systems, such as TeX, may be used to create higher-level markup systems that are more descriptive in nature, such as LaTeX.

In the recent years, a number of small and largely unstandardized markup languages have been developed to allow authors to create formatted text via web browsers, such as the ones used in wikis and in web forums. These are sometimes called lightweight markup languages. Markdown and the markup language used by Wikipedia are examples of such wiki markup.

The first well-known public presentation of markup languages in computer text processing was made by William W. Tunnicliffe at a conference in 1967, although he preferred to call it "generic coding." It can be seen as a response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chairman of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970.

Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use. However, IBM researcher Charles Goldfarb is more commonly seen today as the "father" of markup languages. Goldfarb hit upon the basic idea while working on a primitive document management system intended for law firms in 1969, and helped invent IBM GML later that same year. GML was first publicly disclosed in 1973.

In 1975, Goldfarb moved from Cambridge, Massachusetts to Silicon Valley and became a product planner at the IBM Almaden Research Center. There, he convinced IBM's executives to deploy GML commercially in 1978 as part of IBM's Document Composition Facility product, and it was widely used in business within a few years.

SGML, which was based on both GML and GenCode, was an ISO project worked on by Goldfarb beginning in 1974. Goldfarb eventually became chair of the SGML committee. SGML was first released by ISO as the ISO 8879 standard in October 1986.

Some early examples of computer markup languages available outside the publishing industry can be found in typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error iterative process to get a document printed correctly. Availability of WYSIWYG ("what you see is what you get") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.

Another major publishing standard is TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating the art of typesetting. TeX is mainly used in academia, where it is a "de facto" standard in many scientific disciplines. A TeX macro package known as LaTeX provides a descriptive markup system on top of TeX, and is widely used both among the scientific community and the publishing industry.

The first language to make a clean distinction between structure and presentation was Scribe, developed by Brian Reid and described in his doctoral thesis in 1980. Scribe was revolutionary in a number of ways, not least that it introduced the idea of styles separated from the marked up document, and of a grammar controlling the usage of descriptive elements. Scribe influenced the development of Generalized Markup Language (later SGML), and is a direct ancestor to HTML and LaTeX.

In the early 1980s, the idea that markup should focus on the structural aspects of a document and leave the visual presentation of that structure to the interpreter led to the creation of SGML. The language was developed by a committee chaired by Goldfarb. It incorporated ideas from many different sources, including Tunnicliffe's project, GenCode. Sharon Adler, Anders Berglund, and James A. Marke were also key members of the SGML committee.

SGML specified a syntax for including the markup in documents, as well as one for separately describing "what" tags were allowed, and "where" (the Document Type Definition (DTD), later known as a schema). This allowed authors to create and use any markup they wished, selecting tags that made the most sense to them and were named in their own natural languages, while also allowing automated verification. Thus, SGML is properly a meta-language, and many particular markup languages are derived from it. From the late '80s onward, most substantial new markup languages have been based on the SGML system, including for example TEI and DocBook. SGML was promulgated as an International Standard by International Organization for Standardization, ISO 8879, in 1986.

SGML found wide acceptance and use in fields with very large-scale documentation requirements. However, many found it cumbersome and difficult to learn — a side effect of its design attempting to do too much and to be too flexible. For example, SGML made end tags (or start-tags, or even both) optional in certain contexts, because its developers thought markup would be done manually by overworked support staff who would appreciate saving keystrokes.

In 1989, computer scientist Sir Tim Berners-Lee wrote a memo proposing an Internet-based hypertext system, then specified HTML and wrote the browser and server software in the last part of 1990. The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Berners-Lee in late 1991. It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house SGML-based documentation format at CERN, and very similar to the sample schema in the SGML standard. Eleven of these elements still exist in HTML 4.

Berners-Lee considered HTML an SGML application. The Internet Engineering Task Force (IETF) formally defined it as such with the mid-1993 publication of the first proposal for an HTML specification: "Hypertext Markup Language (HTML)" Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements are found in the 1988 ISO technical report TR 9537 "Techniques for using SGML", which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of descriptive markup (and influence of SGML in particular) was a major factor in the success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.

XML (Extensible Markup Language) is a meta markup language that is now widely used. XML was developed by the World Wide Web Consortium, in a committee created and chaired by Jon Bosak. The main purpose of XML was to simplify SGML by focusing on a particular problem — documents on the Internet. XML remains a meta-language like SGML, allowing users to create any tags needed (hence "extensible") and then describing those tags and their permitted uses.

XML adoption was helped because every XML document can be written in such a way that it is also an SGML document, and existing SGML users and software could switch to XML fairly easily. However, XML eliminated many of the more complex features of SGML to simplify implementation environments such as documents and publications. It appeared to strike a happy medium between simplicity and flexibility, and was rapidly adopted for many other uses. XML is now widely used for communicating data between applications, for serializing program data, and many other uses as well as documents.

Since January 2000, all W3C Recommendations for HTML have been based on XML rather than SGML, using the abbreviation XHTML (Extensible HyperText Markup Language). The language specification requires that XHTML Web documents must be "well-formed" XML documents. This allows for more rigorous and robust documents while using tags familiar from HTML.

One of the most noticeable differences between HTML and XHTML is the rule that "all tags must be closed": empty HTML tags such as codice_2 must either be "closed" with a regular end-tag, or replaced by a special form: (the space before the 'codice_3' on the end tag is optional, but frequently used because it enables some pre-XML Web browsers, and SGML parsers, to accept the tag). Another is that all attribute values in tags must be quoted. Finally, all tag and attribute names within the XHTML namespace must be lowercase to be valid. HTML, on the other hand, was case-insensitive.

Many XML-based applications now exist, including the Resource Description Framework as RDF/XML, XForms, DocBook, SOAP, and the Web Ontology Language (OWL). For a partial list of these, see List of XML markup languages.

A common feature of many markup languages is that they intermix the text of a document with markup instructions in the same data stream or file. This is not necessary; it is possible to isolate markup from text content, using pointers, offsets, IDs, or other methods to co-ordinate the two. Such "standoff markup" is typical for the internal representations that programs use to work with marked-up documents. However, embedded or "inline" markup is much more common elsewhere. Here, for example, is a small section of text marked up in HTML:

The codes enclosed in angle-brackets codice_4 are markup instructions (known as tags), while the text between these instructions is the actual text of the document. The codes codice_5, codice_6, and codice_7 are examples of "semantic" markup, in that they describe the intended purpose or the meaning of the text they include. Specifically, codice_5 means "this is a first-level heading", codice_6 means "this is a paragraph", and codice_7 means "this is an emphasized word or phrase". A program interpreting such structural markup may apply its own rules or styles for presenting the various pieces of text, using different typefaces, boldness, font size, indentation, colour, or other styles, as desired.
For example, a tag such as "h1" (header level 1) might be presented in a large bold sans-serif typeface in an article, or it might be underscored in a monospaced (typewriter-style) document – or it might simply not change the presentation at all.

In contrast, the codice_11 tag in HTML is an example of "presentational" markup; it is generally used to specify a particular characteristic of the text (in this case, the use of an italic typeface) — without specifying the reason for that appearance.

The Text Encoding Initiative (TEI) has published extensive guidelines for how to encode texts of interest in the humanities and social sciences, developed through years of international cooperative work. These guidelines are used by projects encoding historical documents, the works of particular scholars, periods, or genres, and so on.

While the idea of markup language originated with text documents, there is increasing use of markup languages in the presentation of other types of information, including playlists, vector graphics, web services, content syndication, and user interfaces. Most of these are XML applications, because XML is a well-defined and extensible language.

The use of XML has also led to the possibility of combining multiple markup languages into a single profile, like XHTML+SMIL and XHTML+MathML+SVG.

Because markup languages, and more generally data description languages (not necessarily textual markup), are not programming languages (they are data without instructions), they are more easily manipulated than programming languages—for example, web pages are presented as HTML documents, not C code, and thus can be embedded within other web pages, displayed when only partially received, and so forth. This leads to the web design principle of the rule of least power, which advocates using the "least" (computationally) powerful language that satisfies a task to facilitate such manipulation and reuse.



</doc>
<doc id="18916" url="https://en.wikipedia.org/wiki?curid=18916" title="Meaning">
Meaning

Meaning may refer to:





</doc>
<doc id="18917" url="https://en.wikipedia.org/wiki?curid=18917" title="Meta-ethics">
Meta-ethics

Meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the three branches of ethics generally studied by philosophers, the others being normative ethics and applied ethics.

While normative ethics addresses such questions as "What should I do?", evaluating specific practices and principles of action, meta-ethics addresses questions such as "What "is" goodness?" and "How can we tell what is good from what is bad?", seeking to understand the nature of ethical properties and evaluations.

Some theorists argue that a metaphysical account of morality is necessary for the proper evaluation of actual moral theories and for making practical moral decisions; others reason from opposite premises and suggest that studying moral judgments about proper actions can guide us to a true account of the nature of morality.

According to Richard Garner and Bernard Rosen, there are three kinds of meta-ethical problems, or three general questions:
A question of the first type might be, "What do the words 'good', 'bad', 'right' and 'wrong' mean?" (see value theory). The second category includes questions of whether moral judgments are universal or relative, of one kind or many kinds, etc. Questions of the third kind ask, for example, how we can know if something is right or wrong, if at all. Garner and Rosen say that answers to the three basic questions "are not unrelated, and sometimes an answer to one will strongly suggest, or perhaps even entail, an answer to another."

A meta-ethical theory, unlike a normative ethical theory, does not attempt to evaluate specific choices as being better, worse, good, bad, or evil; although it may have profound implications as to the validity and meaning of normative ethical claims. An answer to any of the three example questions above would not itself be a normative ethical statement.

These theories focus on the first of the three questions above, "What is the meaning of moral terms or judgments?" They may however imply or even entail answers to the other two questions as well.


Yet another way of categorizing meta-ethical theories is to distinguish between centralist and non-centralist theories. The debate between centralism and non-centralism revolves around the relationship between the so-called "thin" and "thick" concepts of morality. Thin moral concepts are those such as good, bad, right, and wrong; thick moral concepts are those such as courageous, inequitable, just, or dishonest. While both sides agree that the thin concepts are more general and the thick more specific, centralists hold that the thin concepts are antecedent to the thick ones and that the latter are therefore dependent on the former. That is, centralists argue that one must understand words like "right" and "ought" before understanding words like "just" and "unkind." Non-centralism rejects this view, holding that thin and thick concepts are on par with one another and even that the thick concepts are a sufficient starting point for understanding the thin ones.

Non-centralism has been of particular importance to ethical naturalists in the late 20th and early 21st centuries as part of their argument that normativity is a non-excisable aspect of language and that there is no way of analyzing thick moral concepts into a purely descriptive element attached to a thin moral evaluation, thus undermining any fundamental division between facts and norms. Allan Gibbard, R. M. Hare, and Simon Blackburn have argued in favor of the fact/norm distinction, meanwhile, with Gibbard going so far as to argue that, even if conventional English has only mixed normative terms (that is, terms that are neither purely descriptive nor purely normative), we could develop a nominally English metalanguage that still allowed us to maintain the division between factual descriptions and normative evaluations.

These theories attempt to answer the second of the above questions: "What is the nature of moral judgments?"


These are theories that attempt to answer questions like, "How may moral judgments be supported or defended?" or "Why should I be moral?"

If one presupposes a cognitivist interpretation of moral sentences, morality is justified by the moralist's knowledge of moral facts, and the theories to justify moral judgements are epistemological theories.




</doc>
<doc id="18921" url="https://en.wikipedia.org/wiki?curid=18921" title="Montesquieu (disambiguation)">
Montesquieu (disambiguation)

Montesquieu (1689–1755) was a French lawyer, man of letters, and political philosopher.

Montesquieu may also refer to:


</doc>
<doc id="18925" url="https://en.wikipedia.org/wiki?curid=18925" title="Mormons">
Mormons

Mormons are a religious and cultural group related to Mormonism, the principal branch of the Latter Day Saint movement of Restorationist Christianity, initiated by Joseph Smith in upstate New York during the 1820s. After Smith's death in 1844, the Mormons followed Brigham Young to what would become the Utah Territory. Today, most Mormons are understood to be members of The Church of Jesus Christ of Latter-day Saints (LDS Church). Other Mormons may be independently religious, secular and non-practicing, or belong to another denomination. The center of Mormon cultural influence is in Utah, and North America has more Mormons than any other continent, though the majority of Mormons live outside the United States.

Mormons have developed a strong sense of commonality that stems from their doctrine and history. During the 19th century, Mormon converts tended to gather to a central geographic location, and between 1852 and 1890 a minority of Mormons openly practiced plural marriage, a form of religious polygamy. Mormons dedicate large amounts of time and resources to serving in their church, and many young Mormons choose to serve a full-time proselytizing mission. Mormons have a health code which eschews alcoholic beverages, tobacco, “hot drinks” (tea and coffee), and addictive substances. They tend to be very family-oriented and have strong connections across generations and with extended family, reflective of their belief that families can be sealed together beyond death. Mormons also have a strict law of chastity, requiring abstention from sexual relations outside heterosexual marriage and fidelity within marriage.

Mormons self-identify as Christian, although some non-Mormons consider Mormons non-Christian and some of their beliefs differ from those of mainstream Christianity. Mormons believe in the Bible, as well as other books of scripture, such as the Book of Mormon. They have a unique view of cosmology and believe that all people are spirit-children of God. Mormons believe that returning to God requires following the example of Jesus Christ, and accepting his atonement through ordinances such as baptism. They believe that Christ's church was restored through Joseph Smith and is guided by living prophets and apostles. Central to Mormon faith is the belief that God speaks to his children and answers their prayers.

In 1971, the LDS Church reported having 3,090,953 members, with 16,313,735 members worldwide, as of 2018.

The word "Mormons" most often refers to members of the LDS Church because of their belief in the Book of Mormon, though members often refer to themselves as "Latter-day Saints" or sometimes just "Saints". The term "Mormons" has been embraced by others, most notably Mormon fundamentalists, while other Latter Day Saint denominations, such as the Community of Christ, have rejected it. Both LDS Church members and members of fundamentalist groups commonly use the word "Mormon" in reference to themselves. LDS Church leaders have encouraged members to use the church's full name to emphasize its focus on Jesus Christ, and have discouraged the use of the shortened form "Church of the Latter Day Saints", as well as the acronym "LDS", and the nickname "Mormons".

The word "Mormon" is often associated with polygamy (or plural marriage), which was a distinguishing practice of many early Mormons; however, it was renounced by the LDS Church in 1890
and discontinued over the next 15 years.
Today, polygamy is practiced within Mormonism only by people that have broken with the LDS Church.

In the early days of the church the word "Mormon" or "Mormonite" was used to describe the religion in by those who were less familiar with the church. The term "Mormonite" was used several times in derogatory works.

The history of the Mormons has shaped them into a people with a strong sense of unity and commonality. From the start, Mormons have tried to establish what they call "Zion", a utopian society of the righteous.
Mormon history can be divided into three broad time periods: (1) the early history during the lifetime of Joseph Smith, (2) a "pioneer era" under the leadership of Brigham Young and his successors, and (3) a modern era beginning around the turn of the 20th century. In the first period, Smith attempted to build a city called Zion, in which converts could gather. During the pioneer era, Zion became a "landscape of villages" in Utah. In modern times, Zion is still an ideal, though Mormons gather together in their individual congregations rather than a central geographic location.

Mormons trace their origins to the visions that Joseph Smith reported he had in the early 1820s while living in Upstate New York . In 1823, Smith said an angel directed him to a buried book written on golden plates containing the religious history of an ancient people. Smith published what he said was a translation of these plates in March 1830 as the Book of Mormon, named after Mormon, the ancient prophet–historian who compiled the book. On April 6, 1830, Smith founded the Church of Christ. The early church grew westward as Smith sent missionaries to proselytize. In 1831, the church moved to Kirtland, Ohio where missionaries had made a large number of converts and Smith began establishing an outpost in Jackson County, Missouri, where he planned to eventually build the city of Zion (or the New Jerusalem). In 1833, Missouri settlers, alarmed by the rapid influx of Mormons, expelled them from Jackson County into the nearby Clay County, where local residents were more welcoming.
After Smith led a mission, known as Zion's Camp, to recover the land, he began building Kirtland Temple in Lake County, Ohio, where the church flourished. When the Missouri Mormons were later asked to leave Clay County in 1836, they secured land in what would become Caldwell County.

The Kirtland era ended in 1838, after the failure of a church-sponsored anti-bank caused widespread defections, and Smith regrouped with the remaining church in Far West, Missouri. During the fall of 1838, tensions escalated into the Mormon War with the old Missouri settlers. On October 27, the governor of Missouri ordered that the Mormons "must be treated as enemies" and be exterminated or driven from the state. Between November and April, some eight thousand displaced Mormons migrated east into Illinois.

In 1839, the Mormons purchased the small town of Commerce, converted swampland on the banks of the Mississippi River, and renamed the area Nauvoo, Illinois and began construction of the Nauvoo Temple. The city became the church's new headquarters and gathering place, and it grew rapidly, fueled in part by converts immigrating from Europe. Meanwhile, Smith introduced temple ceremonies meant to seal families together for eternity, as well as the doctrines of eternal progression or exaltation, and plural marriage.
Smith created a service organization for women called the Relief Society, as well as an organization called the Council of Fifty, representing a future theodemocratic "Kingdom of God" on the earth.
Smith also published the story of his First Vision, in which the Father and the Son appeared to him while he was about 14 years old.
This vision would come to be regarded by some Mormons as the most important event in human history after the birth, ministry, and resurrection of Jesus Christ.

In 1844, local prejudices and political tensions, fueled by Mormon peculiarity and internal dissent, escalated into conflicts between Mormons and "anti-Mormons". On June 27, 1844, Smith and his brother Hyrum were killed by a mob in Carthage, Illinois. Because Hyrum was Smith's logical successor, their deaths caused a succession crisis, and Brigham Young assumed leadership over the majority of Latter Day Saints. Young had been a close associate of Smith's and was senior apostle of the Quorum of the Twelve. Smaller groups of Latter Day Saints followed other leaders to form other denominations of the Latter Day Saint movement.

For two years after Smith's death, conflicts escalated between Mormons and other Illinois residents. To prevent war, Brigham Young led the Mormon pioneers (constituting most of the Latter Day Saints) to a temporary winter quarters in Nebraska and then, eventually (beginning in 1847), to what became the Utah Territory. Having failed to build Zion within the confines of American society, the Mormons began to construct a society in isolation, based on their beliefs and values. The cooperative ethic that Mormons had developed over the last decade and a half became important as settlers branched out and colonized a large desert region now known as the Mormon Corridor. Colonizing efforts were seen as religious duties, and the new villages were governed by the Mormon bishops (local lay religious leaders). The Mormons viewed land as commonwealth, devising and maintaining a co-operative system of irrigation that allowed them to build a farming community in the desert.

From 1849 to 1852, the Mormons greatly expanded their missionary efforts, establishing several missions in Europe, Latin America, and the South Pacific. Converts were expected to "gather" to Zion, and during Young's presidency (1847–77) over seventy thousand Mormon converts immigrated to America. Many of the converts came from England and Scandinavia, and were quickly assimilated into the Mormon community. Many of these immigrants crossed the Great Plains in wagons drawn by oxen, while some later groups pulled their possessions in small handcarts. During the 1860s, newcomers began using the new railroad that was under construction.

In 1852, church leaders publicized the previously secret practice of plural marriage, a form of polygamy. Over the next 50 years, many Mormons (between 20 and 30 percent of Mormon families) entered into plural marriages as a religious duty, with the number of plural marriages reaching a peak around 1860, and then declining through the rest of the century. Besides the doctrinal reasons for plural marriage, the practice made some economic sense, as many of the plural wives were single women who arrived in Utah without brothers or fathers to offer them societal support.

By 1857, tensions had again escalated between Mormons and other Americans, largely as a result of accusations involving polygamy and the theocratic rule of the Utah Territory by Brigham Young. In 1857, U.S. President James Buchanan sent an army to Utah, which Mormons interpreted as open aggression against them. Fearing a repeat of Missouri and Illinois, the Mormons prepared to defend themselves, determined to torch their own homes in the case that they were invaded. The relatively peaceful Utah War ensued from 1857 to 1858, in which the most notable instance of violence was the Mountain Meadows massacre, when leaders of a local Mormon militia ordered the killing of a civilian emigrant party that was traveling through Utah during the escalating tensions. In 1858, Young agreed to step down from his position as governor and was replaced by a non-Mormon, Alfred Cumming. Nevertheless, the LDS Church still wielded significant political power in the Utah Territory.

At Young's death in 1877, he was followed by other LDS Church presidents, who resisted efforts by the United States Congress to outlaw Mormon polygamous marriages. In 1878, the U.S. Supreme Court ruled in "Reynolds v. United States" that religious duty was not a suitable defense for practicing polygamy, and many Mormon polygamists went into hiding; later, Congress began seizing church assets. In September 1890, church president Wilford Woodruff issued a Manifesto that officially suspended the practice of polygamy. Although this Manifesto did not dissolve existing plural marriages, relations with the United States markedly improved after 1890, such that Utah was admitted as a U.S. state in 1896. After the Manifesto, some Mormons continued to enter into polygamous marriages, but these eventually stopped in 1904 when church president Joseph F. Smith disavowed polygamy before Congress and issued a "Second Manifesto" calling for all plural marriages in the church to cease. Eventually, the church adopted a policy of excommunicating members found practicing polygamy, and today seeks actively to distance itself from "fundamentalist" groups that continue the practice.

During the early 20th century, Mormons began to reintegrate into the American mainstream. In 1929, the Mormon Tabernacle Choir began broadcasting a weekly performance on national radio, becoming an asset for public relations. Mormons emphasized patriotism and industry, rising in socioeconomic status from the bottom among American religious denominations to middle-class.
In the 1920s and 1930s, Mormons began migrating out of Utah, a trend hurried by the Great Depression, as Mormons looked for work wherever they could find it. As Mormons spread out, church leaders created programs that would help preserve the tight-knit community feel of Mormon culture. In addition to weekly worship services, Mormons began participating in numerous programs such as Boy Scouting, a Young Women organization, church-sponsored dances, ward basketball, camping trips, plays, and religious education programs for youth and college students. During the Great Depression, the church started a welfare program to meet the needs of poor members, which has since grown to include a humanitarian branch that provides relief to disaster victims.

During the later half of the 20th century, there was a retrenchment movement in Mormonism in which Mormons became more conservative, attempting to regain their status as a "peculiar people".
Though the 1960s and 1970s brought changes such as Women's Liberation and the civil rights movement, Mormon leaders were alarmed by the erosion of traditional values, the sexual revolution, the widespread use of recreational drugs, moral relativism, and other forces they saw as damaging to the family.
Partly to counter this, Mormons put an even greater emphasis on family life, religious education, and missionary work, becoming more conservative in the process. As a result, Mormons today are probably less integrated with mainstream society than they were in the early 1960s.

Although black people have been members of Mormon congregations since Joseph Smith's time, before 1978, black membership was small. From 1852 to 1978, the LDS Church enforced a policy that restricted men of black African descent from being ordained to the church's lay priesthood. The church was sharply criticized for its policy during the civil rights movement, but the policy remained in force until a 1978 reversal that was prompted in part by questions about mixed-race converts in Brazil. In general, Mormons greeted the change with joy and relief. Since 1978, black membership has grown, and in 1997 there were approximately 500,000 black members of the church (about 5 percent of the total membership), mostly in Africa, Brazil and the Caribbean. Black membership has continued to grow substantially, especially in West Africa, where two temples have been built. Many black Mormons are members of the Genesis Group, an organization of black members that predates the priesthood ban, and is endorsed by the church.

The LDS Church grew rapidly after World War II and became a worldwide organization as missionaries were sent across the globe. The church doubled in size every 15 to 20 years, and by 1996, there were more Mormons outside the United States than inside. In 2012, there were an estimated 14.8 million Mormons, with roughly 57 percent living outside the United States. It is estimated that approximately 4.5 million Mormons – roughly 30% of the total membership – regularly attend services. A majority of U.S. Mormons are white and non-Hispanic (84 percent). Most Mormons are distributed in North and South America, the South Pacific, and Western Europe. The global distribution of Mormons resembles a contact diffusion model, radiating out from the organization's headquarters in Utah. The church enforces general doctrinal uniformity, and congregations on all continents teach the same doctrines, and international Mormons tend to absorb a good deal of Mormon culture, possibly because of the church's top-down hierarchy and a missionary presence. However, international Mormons often bring pieces of their own heritage into the church, adapting church practices to local cultures.

Chile, Uruguay, and several areas in the South Pacific have a higher percentage of Mormons than the United States (which is at about 2 percent). South Pacific countries and dependencies that are more than 10 percent Mormon include American Samoa, the Cook Islands, Kiribati, Niue, Samoa, and Tonga.

Isolation in Utah had allowed Mormons to create a culture of their own. As the faith spread around the world, many of its more distinctive practices followed. Mormon converts are urged to undergo lifestyle changes, repent of sins, and adopt sometimes atypical standards of conduct. Practices common to Mormons include studying scriptures, praying daily, fasting regularly, attending Sunday worship services, participating in church programs and activities on weekdays, and refraining from work on Sundays when possible. The most important part of the church services is considered to be the Lord's Supper (commonly called sacrament), in which church members renew covenants made at baptism. Mormons also emphasize standards they believe were taught by Jesus Christ, including personal honesty, integrity, obedience to law, chastity outside marriage and fidelity within marriage.

In 2010, around 13–14 percent of Mormons lived in Utah, the center of cultural influence for Mormonism. Utah Mormons (as well as Mormons living in the Intermountain West) are on average more culturally and/or politically conservative than those living in some cosmopolitan centers elsewhere in the U.S. Utahns self-identifying as Mormon also attend church somewhat more on average than Mormons living in other states. (Nonetheless, whether they live in Utah or elsewhere in the U.S., Mormons tend to be more culturally and/or politically conservative than members of other U.S. religious groups.) Utah Mormons often place a greater emphasis on pioneer heritage than international Mormons who generally are not descendants of the Mormon pioneers.

Mormons have a strong sense of communality that stems from their doctrine and history. LDS Church members have a responsibility to dedicate their time and talents to helping the poor and building the church. The church is divided by locality into congregations called "wards", with several wards or branches to create a "stake". The vast majority of church leadership positions are lay positions, and church leaders may work 10 to 15 hours a week in unpaid church service. Observant Mormons also contribute 10 percent of their income to the church as tithing, and are often involved in humanitarian efforts. Many LDS young men, women and elderly couples choose to serve a proselytizing mission, during which they dedicate all of their time to the church, without pay.

Mormons adhere to the Word of Wisdom, a health law or code that is interpreted as prohibiting the consumption of tobacco, alcohol, coffee and tea, while encouraging the use of herbs, grains, fruits, and a moderate consumption of meat. The Word of Wisdom is also understood to forbid other harmful and addictive substances and practices, such as the use of illegal drugs and abuse of prescription drugs. Mormons are encouraged to keep a year's supplies that include a food supply and a financial reserve. Mormons also oppose behaviors such as viewing pornography and gambling.

The concept of a united family that lives and progresses forever is at the core of Latter-day Saint doctrine, and Mormons place a high importance on family life. Many Mormons hold weekly Family Home Evenings, in which an evening is set aside for family bonding, study, prayer and other activities they consider to be wholesome. Latter-day Saint fathers who hold the priesthood typically name and bless their children shortly after birth to formally give the child a name. Mormon parents hope and pray that their children will gain testimonies of the "gospel" so they can grow up and marry in temples.

Mormons have a strict law of chastity, requiring abstention from sexual relations outside opposite-sex marriage and strict fidelity within marriage. All sexual activity (heterosexual and homosexual) outside marriage is considered a serious sin, with marriage recognized as only between a man and a woman. Same-sex marriages are not performed or supported by the LDS Church. Church members are encouraged to marry and have children, and Latter-day Saint families tend to be larger than average. Mormons are opposed to abortion, except in some exceptional circumstances, such as when pregnancy is the result of incest or rape, or when the life or health of the mother is in serious jeopardy. Many practicing adult Mormons wear religious undergarments that remind them of covenants and encourage them to dress modestly. Latter-day Saints are counseled not to partake of any form of media that is obscene or pornographic in any way, including media that depicts graphic representations of sex or violence. Tattoos and body piercings are also discouraged, with the exception of a single pair of earrings for LDS women.

LGBT Mormons, or Mormons who self-identify as gay, lesbian, or bisexual, remain in good standing in the church if they abstain from homosexual relations and obey the law of chastity. While there are no official numbers, LDS Family Services estimates that there are on average four or five members per LDS ward who experience same-sex attraction. Gary Watts, former president of Family Fellowship, estimates that only 10 percent of homosexuals stay in the church. Many of these individuals have come forward through different support groups or websites discussing their homosexual attractions and concurrent church membership.

Note that the categories below are not necessarily mutually exclusive.

Members of the LDS Church, also known as Latter-day Saints, constitute over 95 percent of Mormons. The beliefs and practices of LDS Mormons are generally guided by the teachings of LDS Church leaders. However, several smaller groups substantially differ from "mainstream" Mormonism in various ways.

LDS Church members who do not actively participate in worship services or church callings are often called "less-active" or "inactive" (akin to the qualifying expressions "non-observant" or "non-practicing" used in relation to members of other religious groups). The LDS Church does not release statistics on church activity, but it is likely that about 40 percent of Mormons in the United States and 30 percent worldwide regularly attend worship services. Reasons for inactivity can include rejection of the fundamental beliefs and/or history of the church, lifestyle incongruities with doctrinal teachings, and problems with social integration. Activity rates tend to vary with age, and disengagement occurs most frequently between age 16 and 25. In 1998, the church reported a majority of less active members returned to church activity later in life. As of 2017, the LDS Church was losing Millennial-age members, a phenomenon not unique to the LDS Church. Former Latter-day Saints who seek to disassociate themselves from the religion are often referred to as ex-Mormons.

Members of sects that broke with the LDS Church over the issue of polygamy have become known as fundamentalist Mormons; these groups differ from mainstream Mormonism primarily in their belief in and practice of plural marriage. There are thought to be between 20,000 and 60,000 members of fundamentalist sects, (0.1–0.4 percent of Mormons), with roughly half of them practicing polygamy. There are a number of fundamentalist sects, the largest two being the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS Church) and the Apostolic United Brethren (AUB). In addition to plural marriage, some of these groups also practice a form of Christian communalism known as the law of consecration or the United Order. The LDS Church seeks to distance itself from all such polygamous groups, excommunicating their members if discovered practicing or teaching it, and today a majority of Mormon fundamentalists have never been members of the LDS Church.

Liberal Mormons, also known as Progressive Mormons, take an interpretive approach to LDS teachings and scripture. They look to the scriptures for spiritual guidance, but may not necessarily believe the teachings to be literally or uniquely true. For liberal Mormons, revelation is a process through which God gradually brings fallible human beings to greater understanding. A person in this group is sometimes mistakenly regarded by others within the mainstream church as a Jack Mormon. Although this term is more commonly used to describe a different group with distinct motives to live the gospel in a non traditional manner. Liberal Mormons place doing good and loving fellow human beings above the importance of believing correctly. In a separate context, members of small progressive breakaway groups have also adopted the label.

Cultural Mormons are individuals who may not believe in certain doctrines or practices of the institutional LDS Church yet identify as members of the Mormon ethnic identity. Usually this is a result of having been raised in the LDS faith, or as having converted and spent a large portion of one's life as an active member of the LDS Church. Cultural Mormons may or may not be actively involved with the LDS Church. In some cases they may not be members of the LDS Church.

Mormons have a scriptural canon consisting of the Bible (both Old and New Testaments), the Book of Mormon, and a collection of revelations and writings by Joseph Smith known as the Doctrine and Covenants and Pearl of Great Price. Mormons, however, have a relatively open definition of scripture. As a general rule, anything spoken or written by a prophet, while under inspiration, is considered to be the word of God. Thus, the Bible, written by prophets and apostles, is the word of God, so far as it is translated correctly. The Book of Mormon is also believed to have been written by ancient prophets, and is viewed as a companion to the Bible. By this definition, the teachings of Smith's successors are also accepted as scripture, though they are always measured against, and draw heavily from the scriptural canon.

Mormons believe in "a friendly universe", governed by a God whose aim it is to bring his children to immortality and eternal life. Mormons have a unique perspective on the nature of God, the origin of man, and the purpose of life. For instance, Mormons believe in a pre-mortal existence where people were literal spirit children of God, and that God presented a plan of salvation that would allow his children to progress and become more like him. The plan involved the spirits receiving bodies on earth and going through trials in order to learn, progress, and receive a "fulness of joy". The most important part of the plan involved Jesus, the eldest of God's children, coming to earth as the literal Son of God, to conquer sin and death so that God's other children could return. According to Mormons, every person who lives on earth will be resurrected, and nearly all of them will be received into various kingdoms of glory. To be accepted into the highest kingdom, a person must fully accept Christ through faith, repentance, and through ordinances such as baptism and the laying on of hands.

According to Mormons, a deviation from the original principles of Christianity, known as the Great Apostasy, began not long after the ascension of Jesus Christ. It was marked with the corruption of Christian doctrine by Greek and other philosophies, with followers dividing into different ideological groups. Mormons claim the martyrdom of the Apostles led to a loss of Priesthood authority to administer the church and its ordinances.
Mormons believe that God restored the early Christian church through Joseph Smith. In particular, Mormons believe that angels such as Peter, James, John, John the Baptist, Moses, and Elijah appeared to Smith and others and bestowed various priesthood authorities on them. Mormons believe that their church is the "only true and living church" because of the divine authority restored through Smith. Mormons self-identify as being Christian, while many Christians, particularly evangelical Protestants, disagree with this view. Mormons view other religions as having portions of the truth, doing good works, and having genuine value.

The LDS Church has a top-down hierarchical structure with a president–prophet dictating revelations for the whole church. Lay Mormons are also believed to have access to inspiration, and are encouraged to seek their own personal revelations. Mormons see Joseph Smith's First Vision as proof that the heavens are open, and that God answers prayers. They place considerable emphasis on "asking God" to find out if something is true. Most Mormons do not claim to have had heavenly visions like Smith's in response to prayers, but feel that God talks to them in their hearts and minds through the Holy Ghost. Though Mormons have some beliefs that are considered strange in a modernized world, they continue to hold onto their beliefs because they feel God has spoken to them.



</doc>
<doc id="18926" url="https://en.wikipedia.org/wiki?curid=18926" title="Manitoba">
Manitoba

Manitoba () is a province at the longitudinal centre of Canada. It is often considered one of the three prairie provinces (with Alberta and Saskatchewan) and is Canada's fifth-most populous province with its estimated 1.369 million people. Manitoba covers with a widely varied landscape, stretching from the northern oceanic coastline to the southern border with the United States. The province is bordered by the provinces of Ontario to the east and Saskatchewan to the west, the territories of Nunavut to the north, and Northwest Territories to the northwest, and the U.S. states of North Dakota and Minnesota to the south.

Indigenous peoples have inhabited what is now Manitoba for thousands of years. In the late 17th century, fur traders arrived on two major river systems, what is now called the Nelson in northern Manitoba and in the southeast along the Winnipeg River system. A Royal Charter in 1670 granted all the lands draining into Hudson's Bay to the British company and they administered trade in what was then called Rupert's Land. During the next 200 years, communities continued to grow and evolve, with a significant settlement of Michif in what is now Winnipeg. The assertion of Métis identity and self-rule culminated in negotiations for the creation of the province of Manitoba. There are many factors that led to an armed uprising of the Métis people against the Government of Canada, a conflict known as the Red River Rebellion. The resolution of the assertion of the right to representation led to the Parliament of Canada passing the "Manitoba Act" in 1870 that created the province.

Manitoba's capital and largest city, Winnipeg, is the eighth-largest census metropolitan area in Canada. Other census agglomerations in the province are Brandon, Steinbach, Portage la Prairie, Winkler, and Thompson.

The name "Manitoba" is believed to be derived from the Cree, Ojibwe or Assiniboine languages. The name derives from Cree "manitou-wapow" or Ojibwa "manidoobaa", both meaning "straits of Manitou, the Great Spirit", a place referring to what are now called The Narrows in the centre of Lake Manitoba. It may also be from the Assiniboine for "Lake of the Prairie". which is rendered in the language as "minnetoba".

The lake was known to French explorers as "Lac des Prairies." Thomas Spence chose the name to refer to a new republic he proposed for the area south of the lake. Métis leader Louis Riel also chose the name, and it was accepted in Ottawa under the "Manitoba Act" of 1870.

Manitoba is bordered by the provinces of Ontario to the east and Saskatchewan to the west, the territories of Nunavut to the north, and the US states of North Dakota and Minnesota to the south. The province possibly meets the Northwest Territories at the four corners quadripoint to the extreme northwest, though surveys have not been completed and laws are unclear about the exact location of the Nunavut–NWT boundary. Manitoba adjoins Hudson Bay to the northeast, and is the only prairie province to have a saltwater coastline. The Port of Churchill is Canada's only Arctic deep-water port. Lake Winnipeg is the tenth-largest freshwater lake in the world. Hudson Bay is the world's second-largest bay by area. Manitoba is at the heart of the giant Hudson Bay watershed, once known as Rupert's Land. It was a vital area of the Hudson's Bay Company, with many rivers and lakes that provided excellent opportunities for the lucrative fur trade.

The province has a saltwater coastline bordering Hudson Bay and more than 110,000 lakes, covering approximately 15.6 percent or of its surface area. Manitoba's major lakes are Lake Manitoba, Lake Winnipegosis, and Lake Winnipeg, the tenth-largest freshwater lake in the world. Some traditional Native lands and boreal forest on Lake Winnipeg's east side are a proposed UNESCO World Heritage Site.

Manitoba is at the center of the Hudson Bay drainage basin, with a high volume of the water draining into Lake Winnipeg and then north down the Nelson River into Hudson Bay. This basin's rivers reach far west to the mountains, far south into the United States, and east into Ontario. Major watercourses include the Red, Assiniboine, Nelson, Winnipeg, Hayes, Whiteshell and Churchill rivers. Most of Manitoba's inhabited south has developed in the prehistoric bed of Glacial Lake Agassiz. This region, particularly the Red River Valley, is flat and fertile; receding glaciers left hilly and rocky areas throughout the province.

Baldy Mountain is the province's highest point at above sea level, and the Hudson Bay coast is the lowest at sea level. Riding Mountain, the Pembina Hills, Sandilands Provincial Forest, and the Canadian Shield are also upland regions. Much of the province's sparsely inhabited north and east lie on the irregular granite Canadian Shield, including Whiteshell, Atikaki, and Nopiming Provincial Parks.

Extensive agriculture is found only in the province's southern areas, although there is grain farming in the Carrot Valley Region (near The Pas). The most common agricultural activity is cattle husbandry (34.6%), followed by assorted grains (19.0%) and oilseed (7.9%). Around 12 percent of Canada's farmland is in Manitoba.

Manitoba has an extreme continental climate. Temperatures and precipitation generally decrease from south to north and increase from east to west. Manitoba is far from the moderating influences of mountain ranges or large bodies of water. Because of the generally flat landscape, it is exposed to cold Arctic high-pressure air masses from the northwest during January and February. In the summer, air masses sometimes come out of the Southern United States, as warm humid air is drawn northward from the Gulf of Mexico. Temperatures exceed numerous times each summer, and the combination of heat and humidity can bring the humidex value to the mid-40s. Carman, Manitoba recorded the second-highest humidex ever in Canada in 2007, with 53.0. According to Environment Canada, Manitoba ranked first for clearest skies year round, and ranked second for clearest skies in the summer and for the sunniest province in the winter and spring.

Southern Manitoba (including the city of Winnipeg), falls into the humid continental climate zone (Köppen Dfb). This area is cold and windy in the winter and often has blizzards because of the open landscape. Summers are warm with a moderate length. This region is the most humid area in the prairie provinces, with moderate precipitation. Southwestern Manitoba, though under the same climate classification as the rest of Southern Manitoba, is closer to the semi-arid interior of Palliser's Triangle. The area is drier and more prone to droughts than other parts of southern Manitoba. This area is cold and windy in the winter and has frequent blizzards due to the openness of the Canadian Prairie landscape. Summers are generally warm to hot, with low to moderate humidity.

Southern parts of the province just north of Tornado Alley, experience tornadoes, with 16 confirmed touchdowns in 2016. In 2007, on 22 and 23 June, numerous tornadoes touched down, the largest an F5 tornado that devastated parts of Elie (the strongest recorded tornado in Canada).

The province's northern sections (including the city of Thompson) fall in the subarctic climate zone (Köppen climate classification "Dfc"). This region features long and extremely cold winters and brief, warm summers with little precipitation. Overnight temperatures as low as occur on several days each winter.

Manitoba natural communities may be grouped within five ecozones: boreal plains, prairie, taiga shield, boreal shield and Hudson plains. Three of these—taiga shield, boreal shield and Hudson plain—contain part of the Boreal forest of Canada which covers the province's eastern, southeastern, and northern reaches.

Forests make up about , or 48 percent, of the province's land area. The forests consist of pines (Jack Pine, Red Pine, Eastern White Pine), spruces (White Spruce, Black Spruce), Balsam Fir, Tamarack (larch), poplars (Trembling Aspen, Balsam Poplar), birches (White Birch, Swamp Birch) and small pockets of Eastern White Cedar.

Two sections of the province are not dominated by forest. The province's northeast corner bordering Hudson Bay is above the treeline and is considered tundra. The tallgrass prairie once dominated the south central and southeastern parts including the Red River Valley. Mixed grass prairie is found in the southwestern region. Agriculture has replaced much of the natural prairie but prairie still can be found in parks and protected areas; some are notable for the presence of the endangered western prairie fringed orchid.

Manitoba is especially noted for its northern polar bear population; Churchill is commonly referred to as the "Polar Bear Capital". Other large animals, including moose, white-tailed deer, black bears, cougars, lynx, and wolves, are common throughout the province, especially in the provincial and national parks. There is a large population of red sided garter snakes near Narcisse; the dens there are home to the world's largest concentration of snakes.

Manitoba's bird diversity is enhanced by its position on two major migration routes, with 392 confirmed identified species; 287 of these nesting within the province. These include the great grey owl, the province's official bird, and the endangered peregrine falcon.

Manitoba's lakes host 18 species of game fish, particularly species of trout, pike, and goldeye, as well as many smaller fish.

Modern-day Manitoba was inhabited by the First Nations people shortly after the last ice age glaciers retreated in the southwest about 10,000 years ago; the first exposed land was the Turtle Mountain area. The Ojibwe, Cree, Dene, Sioux, Mandan, and Assiniboine peoples founded settlements, and other tribes entered the area to trade. In Northern Manitoba, quartz was mined to make arrowheads. The first farming in Manitoba was along the Red River, where corn and other seed crops were planted before contact with Europeans.
In 1611, Henry Hudson was one of the first Europeans to sail into what is now known as Hudson Bay, where he was abandoned by his crew. The first European to reach present-day central and southern Manitoba was Sir Thomas Button, who travelled upstream along the Nelson River to Lake Winnipeg in 1612 in an unsuccessful attempt to find and rescue Hudson. When the British ship "Nonsuch" sailed into Hudson Bay in 1668–1669, she became the first trading vessel to reach the area; that voyage led to the formation of the Hudson's Bay Company, to which the British government gave absolute control of the entire Hudson Bay watershed. This watershed was named Rupert's Land, after Prince Rupert, who helped to subsidize the Hudson's Bay Company. York Factory was founded in 1684 after the original fort of the Hudson's Bay Company, Fort Nelson (built in 1682), was destroyed by rival French traders.

Pierre Gaultier de Varennes, sieur de La Vérendrye, visited the Red River Valley in the 1730s to help open the area for French exploration and trade. As French explorers entered the area, a Montreal-based company, the North West Company, began trading with the local Indigenous people. Both the North West Company and the Hudson's Bay Company built fur-trading forts; the two companies competed in southern Manitoba, occasionally resulting in violence, until they merged in 1821 (the Hudson's Bay Company Archives in Winnipeg preserve the history of this era).

Great Britain secured the territory in 1763 after their victory over France in the North American theatre of the Seven Years' War, better known as the French and Indian War in North America; lasting from 1754 to 1763. The founding of the first agricultural community and settlements in 1812 by Lord Selkirk, north of the area which is now downtown Winnipeg, led to conflict between British colonists and the Métis. Twenty colonists, including the governor, and one Métis were killed in the Battle of Seven Oaks in 1816. Thomas Spence attempted to be President of the Republic of Manitobah in 1867, that he and his council named.

Rupert's Land was ceded to Canada by the Hudson's Bay Company in 1869 and incorporated into the Northwest Territories; a lack of attention to Métis concerns caused Métis leader Louis Riel to establish a local provisional government which formed into the Convention of Forty and the subsequent elected Legislative Assembly of Assiniboia on 9 March 1870. This assembly subsequently sent three delegates to Ottawa to negotiate with the Canadian government. This resulted in the "Manitoba Act" and that province's entry into the Canadian Confederation. Prime Minister John A. Macdonald introduced the "Manitoba Act" in the House of Commons of Canada, the bill was given Royal Assent and Manitoba was brought into Canada as a province in 1870. Louis Riel was pursued by British army officer Garnet Wolseley because of the rebellion, and Riel fled into exile. The Canadian government blocked the Métis' attempts to obtain land promised to them as part of Manitoba's entry into confederation. Facing racism from the new flood of white settlers from Ontario, large numbers of Métis moved to what would become Saskatchewan and Alberta.

Numbered Treaties were signed in the late 19th century with the chiefs of First Nations that lived in the area. They made specific promises of land for every family. As a result, a reserve system was established under the jurisdiction of the Federal Government. The prescribed amount of land promised to the native peoples was not always given; this led Indigenous groups to assert rights to the land through land claims, many of which are still ongoing.

The original province of Manitoba was a square one-eighteenth of its current size, and was known colloquially as the "postage stamp province". Its borders were expanded in 1881, taking land from the Northwest Territories and the District of Keewatin, but Ontario claimed a large portion of the land; the disputed portion was awarded to Ontario in 1889. Manitoba grew to its current size in 1912, absorbing land from the Northwest Territories to reach 60°N, uniform with the northern reach of its western neighbours Saskatchewan, Alberta and British Columbia.

The Manitoba Schools Question showed the deep divergence of cultural values in the territory. The Catholic Franco-Manitobans had been guaranteed a state-supported separate school system in the original constitution of Manitoba, but a grassroots political movement among English Protestants from 1888 to 1890 demanded the end of French schools. In 1890, the Manitoba legislature passed a law removing funding for French Catholic schools. The French Catholic minority asked the federal government for support; however, the Orange Order and other anti-Catholic forces mobilized nationwide to oppose them.

The federal Conservatives proposed remedial legislation to override Manitoba, but they were blocked by the Liberals, led by Wilfrid Laurier, who opposed the remedial legislation because of his belief in provincial rights. Once elected Prime Minister in 1896, Laurier implemented a compromise stating Catholics in Manitoba could have their own religious instruction for 30 minutes at the end of the day if there were enough students to warrant it, implemented on a school-by-school basis.

By 1911, Winnipeg was the third largest city in Canada, and remained so until overtaken by Vancouver in the 1920s. A boomtown, it grew quickly around the start of the 20th century, with outside investors and immigrants contributing to its success. The drop in growth in the second half of the decade was a result of the opening of the Panama Canal in 1914, which reduced reliance on transcontinental railways for trade, as well as a decrease in immigration due to the outbreak of the First World War. Over 18,000 Manitoba residents enlisted in the first year of the war; by the end of the war, 14 Manitobans had received the Victoria Cross.

During World War 1, Nellie McClung started the campaign for women's votes. On January 28, 1916, the vote for women was legalized. Manitoba was the first province to allow women to vote, obviously only in provincial elections. This was two years before Canada as a country granted women the right to vote.

After the First World War ended, severe discontent among farmers (over wheat prices) and union members (over wage rates) resulted in an upsurge of radicalism, coupled with a polarization over the rise of Bolshevism in Russia. The most dramatic result was the Winnipeg general strike of 1919. It began on 15 May and collapsed on 25 June 1919; as the workers gradually returned to their jobs, the Central Strike Committee decided to end the movement.

Government efforts to violently crush the strike, including a Royal Northwest Mounted Police charge into a crowd of protesters that resulted in multiple casualties and one death, had led to the arrest of the movement's leaders. In the aftermath, eight leaders went on trial, and most were convicted on charges of seditious conspiracy, illegal combinations, and seditious libel; four were aliens who were deported under the "Canadian Immigration Act".

The Great Depression (1929–c. 1939) hit especially hard in Western Canada, including Manitoba. The collapse of the world market combined with a steep drop in agricultural production due to drought led to economic diversification, moving away from a reliance on wheat production. The Manitoba Co-operative Commonwealth Federation, forerunner to the New Democratic Party of Manitoba (NDP), was founded in 1932.

Canada entered the Second World War in 1939. Winnipeg was one of the major commands for the British Commonwealth Air Training Plan to train fighter pilots, and there were air training schools throughout Manitoba. Several Manitoba-based regiments were deployed overseas, including Princess Patricia's Canadian Light Infantry. In an effort to raise money for the war effort, the Victory Loan campaign organized "If Day" in 1942. The event featured a simulated Nazi invasion and occupation of Manitoba, and eventually raised over C$65 million.
Winnipeg was inundated during the 1950 Red River Flood and had to be partially evacuated. In that year, the Red River reached its highest level since 1861 and flooded most of the Red River Valley. The damage caused by the flood led then-Premier Duff Roblin to advocate for the construction of the Red River Floodway; it was completed in 1968 after six years of excavation. Permanent dikes were erected in eight towns south of Winnipeg, and clay dikes and diversion dams were built in the Winnipeg area. In 1997, the "Flood of the Century" caused over in damages in Manitoba, but the floodway prevented Winnipeg from flooding.

In 1990, Prime Minister Brian Mulroney attempted to pass the Meech Lake Accord, a series of constitutional amendments to persuade Quebec to endorse the "Canada Act 1982". Unanimous support in the legislature was needed to bypass public consultation. Manitoba politician Elijah Harper, a Cree, opposed because he did not believe First Nations had been adequately involved in the Accord's process, and thus the Accord failed.

In 2013, Manitoba was the second province to make accessibility legislation law, protecting the rights of persons with disabilities.

At the 2011 census, Manitoba had a population of 1,208,268, more than half of which is in the Winnipeg Capital Region; Winnipeg is Canada's eighth-largest Census Metropolitan Area, with a population of 730,018 (2011 Census). Although initial colonization of the province revolved mostly around homesteading, the last century has seen a shift towards urbanization; Manitoba is the only Canadian province with over fifty-five percent of its population in a single city.

According to the 2006 Canadian census, the largest ethnic group in Manitoba is English (22.9%), followed by German (19.1%), Scottish (18.5%), Ukrainian (14.7%), Irish (13.4%), North American Indian (10.6%), Polish (7.3%), Métis (6.4%), French (5.6%), Dutch (4.9%), Russian (4.0%), and Icelandic (2.4%). Almost one-fifth of respondents also identified their ethnicity as "Canadian". Indigenous peoples (including Métis) are Manitoba's fastest-growing ethnic group, representing 13.6 percent of Manitoba's population as of 2001 (some reserves refused to allow census-takers to enumerate their populations or were otherwise incompletely counted). There is a significant Franco-Manitoban minority (148,370). Gimli, Manitoba is home to the largest Icelandic community outside of Iceland.

Most Manitobans belong to a Christian denomination: on the 2001 census, 758,760 Manitobans (68.7%) reported being Christian, followed by 13,040 (1.2%) Jewish, 5,745 (0.5%) Buddhist, 5,485 (0.5%) Sikh, 5,095 (0.5%) Muslim, 3,840 (0.3%) Hindu, 3,415 (0.3%) Indigenous spirituality and 995 (0.1%) pagan. 201,825 Manitobans (18.3%) reported no religious affiliation. The largest Christian denominations by number of adherents were the Roman Catholic Church with 292,970 (27%); the United Church of Canada with 176,820 (16%); and the Anglican Church of Canada with 85,890 (8%).

Manitoba has a moderately strong economy based largely on natural resources. Its Gross Domestic Product was C$50.834 billion in 2008. The province's economy grew 2.4 percent in 2008, the third consecutive year of growth. The average individual income in Manitoba in 2006 was C$25,100 (compared to a national average of C$26,500), ranking fifth-highest among the provinces. As of October 2009, Manitoba's unemployment rate was 5.8 percent.

Manitoba's economy relies heavily on agriculture, tourism, electricity, oil, mining, and forestry. Agriculture is vital and is found mostly in the southern half of the province, although grain farming occurs as far north as The Pas. Around 12 percent of Canadian farmland is in Manitoba. The most common type of farm found in rural areas is cattle farming (34.6%), followed by assorted grains (19.0%) and oilseed (7.9%).

Manitoba is the nation's largest producer of sunflower seed and dry beans, and one of the leading sources of potatoes. Portage la Prairie is a major potato processing centre, and is home to the McCain Foods and Simplot plants, which provide French fries for McDonald's, Wendy's, and other commercial restaurant chains. Richardson International, one of the largest oat mills in the world, also has a plant in the municipality.

Manitoba's largest employers are government and government-funded institutions, including crown corporations and services like hospitals and universities. Major private-sector employers are The Great-West Life Assurance Company, Cargill Ltd., and James Richardson and Sons Ltd. Manitoba also has large manufacturing and tourism sectors. Churchill's Arctic wildlife is a major tourist attraction; the town is a world capital for polar bear and beluga whale watchers. Manitoba is the only province with an Arctic deep-water seaport, at Churchill.

In January 2018, the Canadian Federation of Independent Business claimed Manitoba was the most improved province for tackling red tape.

Manitoba's early economy depended on mobility and living off the land. Indigenous Nations (Cree, Ojibwa, Dene, Sioux and Assiniboine) followed herds of bison and congregated to trade among themselves at key meeting places throughout the province. After the arrival of the first European traders in the 17th century, the economy centred on the trade of beaver pelts and other furs. Diversification of the economy came when Lord Selkirk brought the first agricultural settlers in 1811, though the triumph of the Hudson's Bay Company (HBC) over its competitors ensured the primacy of the fur trade over widespread agricultural colonization.

HBC control of Rupert's Land ended in 1868; when Manitoba became a province in 1870, all land became the property of the federal government, with homesteads granted to settlers for farming. Transcontinental railways were constructed to simplify trade. Manitoba's economy depended mainly on farming, which persisted until drought and the Great Depression led to further diversification.

CFB Winnipeg is a Canadian Forces Base at the Winnipeg International Airport. The base is home to flight operations support divisions and several training schools, as well as the 1 Canadian Air Division and Canadian NORAD Region Headquarters. 17 Wing of the Canadian Forces is based at CFB Winnipeg; the Wing has three squadrons and six schools. It supports 113 units from Thunder Bay to the Saskatchewan/Alberta border, and from the 49th parallel north to the high Arctic. 17 Wing acts as a deployed operating base for CF-18 Hornet fighter–bombers assigned to the Canadian NORAD Region.

The two 17 Wing squadrons based in the city are: the 402 ("City of Winnipeg" Squadron), which flies the Canadian designed and produced de Havilland Canada CT-142 Dash 8 navigation trainer in support of the 1 Canadian Forces Flight Training School's Air Combat Systems Officer and Airborne Electronic Sensor Operator training programs (which trains all Canadian Air Combat Systems Officer); and the 435 ("Chinthe" Transport and Rescue Squadron), which flies the Lockheed C-130 Hercules tanker/transport in airlift search and rescue roles, and is the only Air Force squadron equipped and trained to conduct air-to-air refuelling of fighter aircraft.

Canadian Forces Base Shilo (CFB Shilo) is an Operations and Training base of the Canadian Forces east of Brandon. During the 1990s, Canadian Forces Base Shilo was designated as an Area Support Unit, acting as a local base of operations for Southwest Manitoba in times of military and civil emergency. CFB Shilo is the home of the 1st Regiment, Royal Canadian Horse Artillery, both battalions of the 1 Canadian Mechanized Brigade Group, and the Royal Canadian Artillery. The Second Battalion of Princess Patricia's Canadian Light Infantry (2 PPCLI), which was originally stationed in Winnipeg (first at Fort Osborne, then in Kapyong Barracks), has operated out of CFB Shilo since 2004. CFB Shilo hosts a training unit, 3rd Canadian Division Training Centre. It serves as a base for support units of 3rd Canadian Division, also including 3 CDSG Signals Squadron, Shared Services Unit (West), 11 CF Health Services Centre, 1 Dental Unit, 1 Military Police Regiment, and an Integrated Personnel Support Centre. The base houses 1,700 soldiers.

After the control of Rupert's Land was passed from Great Britain to the Government of Canada in 1869, Manitoba attained full-fledged rights and responsibilities of self-government as the first Canadian province carved out of the Northwest Territories. The Legislative Assembly of Manitoba was established on 14 July 1870. Political parties first emerged between 1878 and 1883, with a two-party system (Liberals and Conservatives). The United Farmers of Manitoba appeared in 1922, and later merged with the Liberals in 1932. Other parties, including the Co-operative Commonwealth Federation (CCF), appeared during the Great Depression; in the 1950s, Manitoban politics became a three-party system, and the Liberals gradually declined in power. The CCF became the New Democratic Party of Manitoba (NDP), which came to power in 1969. Since then, the Progressive Conservatives and the NDP have been the dominant parties.

Like all Canadian provinces, Manitoba is governed by a unicameral legislative assembly. The executive branch is formed by the governing party; the party leader is the premier of Manitoba, the head of the executive branch. The head of state, Queen Elizabeth II, is represented by the Lieutenant Governor of Manitoba, who is appointed by the Governor General of Canada on advice of the Prime Minister. The head of state is primarily a ceremonial role, although the Lieutenant Governor has the official responsibility of ensuring Manitoba has a duly constituted government.

The Legislative Assembly consists of the 57 Members elected to represent the people of Manitoba. The premier of Manitoba is Brian Pallister of the PC Party. The PCs were elected with a majority government of 40 seats. The NDP holds 14 seats, and the Liberal Party have three seats but does not have official party status in the Manitoba Legislature. The last provincial general election was held on 19 April 2016. The province is represented in federal politics by 14 Members of Parliament and six Senators.

Manitoba's judiciary consists of the Court of Appeal, the Court of Queen's Bench, and the Provincial Court. The Provincial Court is primarily for criminal law; 95 per cent of criminal cases in Manitoba are heard here. The Court of Queen's Bench is the highest trial court in the province. It has four jurisdictions: family law (child and family services cases), civil law, criminal law (for indictable offences), and appeals. The Court of Appeal hears appeals from both benches; its decisions can only be appealed to the Supreme Court of Canada.

English and French are the official languages of the legislature and courts of Manitoba, according to §23 of the "Manitoba Act, 1870" (part of the Constitution of Canada). In April 1890, the Manitoba legislature attempted to abolish the official status of French and ceased to publish bilingual legislation. However, in 1985 the Supreme Court of Canada ruled in the Reference re Manitoba Language Rights that §23 still applied, and that legislation published only in English was invalid (unilingual legislation was declared valid for a temporary period to allow time for translation).

Although French is an official language for the purposes of the legislature, legislation, and the courts, the "Manitoba Act" does not require it to be an official language for the purpose of the executive branch (except when performing legislative or judicial functions). Hence, Manitoba's government is not completely bilingual. The Manitoba French Language Services Policy of 1999 is intended to provide a comparable level of provincial government services in both official languages. According to the 2006 Census, 82.8 percent of Manitoba's population spoke only English, 3.2 percent spoke only French, 15.1 percent spoke both, and 0.9 percent spoke neither.

In 2010, the provincial government of Manitoba passed the "Aboriginal Languages Recognition Act", which gives official recognition to seven indigenous languages: Cree, Dakota, Dene, Inuktitut, Michif, Ojibway and Oji-Cree.

Transportation and warehousing contribute approximately to Manitoba's GDP. Total employment in the industry is estimated at 34,500, or around 5 percent of Manitoba's population. Trucks haul 95 percent of land freight in Manitoba, and trucking companies account for 80 percent of Manitoba's merchandise trade to the United States. Five of Canada's twenty-five largest employers in for-hire trucking are headquartered in Manitoba. of Manitoba's GDP comes directly or indirectly from trucking.

Greyhound Canada and Grey Goose Bus Lines offer domestic bus service from the Winnipeg Bus Terminal. The terminal was relocated from downtown Winnipeg to the airport in 2009, and is a Greyhound hub. Municipalities also operate localized transit bus systems.

Manitoba has two Class I railways: Canadian National Railway (CN) and Canadian Pacific Railway (CPR). Winnipeg is centrally located on the main lines of both carriers, and both maintain large inter-modal terminals in the city. CN and CPR operate a combined of track in Manitoba. Via Rail offers transcontinental and Northern Manitoba passenger service from Winnipeg's Union Station. Numerous small regional and short-line railways also run trains within Manitoba: the Hudson Bay Railway, the Southern Manitoba Railway, Burlington Northern Santa Fe Manitoba, Greater Winnipeg Water District Railway, and Central Manitoba Railway. Together, these smaller lines operate approximately of track in the province.
Winnipeg James Armstrong Richardson International Airport, Manitoba's largest airport, is one of only a few 24-hour unrestricted airports in Canada and is part of the National Airports System. A new, larger terminal opened in October 2011. The airport handles approximately of cargo annually, making it the third largest cargo airport in the country.

Eleven regional passenger airlines and nine smaller and charter carriers operate out of the airport, as well as eleven air cargo carriers and seven freight forwarders. Winnipeg is a major sorting facility for both FedEx and Purolator, and receives daily trans-border service from UPS. Air Canada Cargo and Cargojet Airways use the airport as a major hub for national traffic.

The Port of Churchill, owned by Arctic Gateway Group, is the only Arctic deep-water port in Canada. It is nautically closer to ports in Northern Europe and Russia than any other port in Canada. It has four deep-sea berths for the loading and unloading of grain, general cargo and tanker vessels. The port is served by the Hudson Bay Railway (also owned by Arctic Gateway Group). Grain represented 90 percent of the port's traffic in the 2004 shipping season. In that year, over of agricultural products were shipped through the port.

The first school in Manitoba was founded in 1818 by Roman Catholic missionaries in present-day Winnipeg; the first Protestant school was established in 1820. A provincial board of education was established in 1871; it was responsible for public schools and curriculum, and represented both Catholics and Protestants. The Manitoba Schools Question led to funding for French Catholic schools largely being withdrawn in favour of the English Protestant majority. Legislation making education compulsory for children between seven and fourteen was first enacted in 1916, and the leaving age was raised to sixteen in 1962.

Public schools in Manitoba fall under the regulation of one of thirty-seven school divisions within the provincial education system (except for the Manitoba Band Operated Schools, which are administered by the federal government). Public schools follow a provincially mandated curriculum in either French or English. There are sixty-five funded independent schools in Manitoba, including three boarding schools. These schools must follow the Manitoban curriculum and meet other provincial requirements. There are forty-four non-funded independent schools, which are not required to meet those standards.

There are five universities in Manitoba, regulated by the Ministry of Advanced Education and Literacy. Four of these universities are in Winnipeg: the University of Manitoba, the largest and most comprehensive; the University of Winnipeg, a liberal arts school primarily focused on undergrad studies downtown; Université de Saint-Boniface, the province's only French-language university; and the Canadian Mennonite University, a religious-based institution. The Université de Saint-Boniface, established in 1818 and now affiliated with the University of Manitoba, is the oldest university in Western Canada. Brandon University, formed in 1899 and in Brandon, is the province's only university not in Winnipeg.

Manitoba has thirty-eight public libraries; of these, twelve have French-language collections and eight have significant collections in other languages. Twenty-one of these are part of the Winnipeg Public Library system. The first lending library in Manitoba was founded in 1848.

The Minister of Culture, Heritage, Tourism and Sport is responsible for promoting and, to some extent, financing Manitoban culture. Manitoba is the birthplace of the Red River Jig, a combination of Indigenous pow-wows and European reels popular among early settlers. Manitoba's traditional music has strong roots in Métis and First Nations culture, in particular the old-time fiddling of the Métis. Manitoba's cultural scene also incorporates classical European traditions. The Winnipeg-based Royal Winnipeg Ballet (RWB), is Canada's oldest ballet and North America's longest continuously operating ballet company; it was granted its royal title in 1953 under Queen Elizabeth II. The Winnipeg Symphony Orchestra (WSO) performs classical music and new compositions at the Centennial Concert Hall. Manitoba Opera, founded in 1969, also performs out of the Centennial Concert Hall.
Le Cercle Molière (founded 1925) is the oldest French-language theatre in Canada, and Royal Manitoba Theatre Centre (founded 1958) is Canada's oldest English-language regional theatre. Manitoba Theatre for Young People was the first English-language theatre to win the Canadian Institute of the Arts for Young Audiences Award, and offers plays for children and teenagers as well as a theatre school. The Winnipeg Art Gallery (WAG), Manitoba's largest art gallery and the sixth largest in the country, hosts an art school for children; the WAG's permanent collection comprises over twenty thousand works, with a particular emphasis on Manitoban and Canadian art.

The 1960s pop group The Guess Who was formed in Manitoba, and later became the first Canadian band to have a No. 1 hit in the United States; Guess Who guitarist Randy Bachman later created Bachman–Turner Overdrive (BTO) with fellow Winnipeg-based musician Fred Turner. Fellow rocker Neil Young, lived for a time in Manitoba, played with Stephen Stills in Buffalo Springfield, and again in supergroup Crosby, Stills, Nash & Young. Soft-rock band Crash Test Dummies formed in the late 1980s in Winnipeg and were the 1992 Juno Awards Group of the Year.

Several prominent Canadian films were produced in Manitoba, such as "The Stone Angel", based on the Margaret Laurence book of the same title, "The Saddest Music in the World", "Foodland", "For Angela", and "My Winnipeg". Major films shot in Manitoba include "The Assassination of Jesse James by the Coward Robert Ford" and "Capote", both of which received Academy Award nominations. "Falcon Beach", an internationally broadcast television drama, was filmed at Winnipeg Beach, Manitoba.

Manitoba has a strong literary tradition. Manitoban writer Bertram Brooker won the first-ever Governor General's Award for Fiction in 1936. Cartoonist Lynn Johnston, author of the comic strip "For Better or For Worse", was a finalist for a 1994 Pulitzer Prize and inducted into the Canadian Cartoonist Hall of Fame. Margaret Laurence's "The Stone Angel" and "A Jest of God" were set in Manawaka, a fictional town representing Neepawa; the latter title won the Governor General's Award in 1966. Carol Shields won both the Governor General's Award and the Pulitzer Prize for "The Stone Diaries". Gabrielle Roy, a Franco-Manitoban writer, won the Governor General's Award three times. A quote from her writings is featured on the Canadian $20 bill.

Festivals take place throughout the province, with the largest centred in Winnipeg. The inaugural Winnipeg Folk Festival was held in 1974 as a one-time celebration to mark Winnipeg's 100th anniversary. Today, the five-day festival is one of the largest folk festivals in North America with over 70 acts from around the world and an annual attendance of over 80,000. The Winnipeg Folk Festival's home – Birds Hill Provincial Park – is 34 kilometres outside of Winnipeg and for the five days of the festival, it becomes Manitoba's third largest "city." The Festival du Voyageur is an annual ten-day event held in Winnipeg's French Quarter, and is Western Canada's largest winter festival. It celebrates Canada's fur-trading past and French-Canadian heritage and culture. Folklorama, a multicultural festival run by the Folk Arts Council, receives around 400,000 pavilion visits each year, of which about thirty percent are from non-Winnipeg residents. The Winnipeg Fringe Theatre Festival is an annual alternative theatre festival, the second-largest festival of its kind in North America (after the Edmonton International Fringe Festival).

Manitoban museums document different aspects of the province's heritage. The Manitoba Museum is the largest museum in Manitoba and focuses on Manitoban history from prehistory to the 1920s. The full-size replica of the Nonsuch is the museum's showcase piece. The Manitoba Children's Museum at The Forks presents exhibits for children. There are two museums dedicated to the native flora and fauna of Manitoba: the Living Prairie Museum, a tall grass prairie preserve featuring 160 species of grasses and wildflowers, and FortWhyte Alive, a park encompassing prairie, lake, forest and wetland habitats, home to a large herd of bison. The Canadian Fossil Discovery Centre houses the largest collection of marine reptile fossils in Canada. Other museums feature the history of aviation, marine transport, and railways in the area. The Canadian Museum for Human Rights is the first Canadian national museum outside of the National Capital Region.

Winnipeg has two daily newspapers: the "Winnipeg Free Press", a broadsheet with the highest circulation numbers in Manitoba, as well as the "Winnipeg Sun", a smaller tabloid-style paper. There are several ethnic weekly newspapers, including the weekly French-language "La Liberté", and regional and national magazines based in the city. Brandon has two newspapers: the daily "Brandon Sun" and the weekly "Wheat City Journal". Many small towns have local newspapers.

There are five English-language television stations and one French-language station based in Winnipeg. The Global Television Network (owned by Canwest) is headquartered in the city. Winnipeg is home to twenty-one AM and FM radio stations, two of which are French-language stations. Brandon's five local radio stations are provided by Astral Media and Westman Communications Group. In addition to the Brandon and Winnipeg stations, radio service is provided in rural areas and smaller towns by Golden West Broadcasting, Corus Entertainment, and local broadcasters. CBC Radio broadcasts local and national programming throughout the province. Native Communications is devoted to indigenous programming and broadcasts to many of the isolated native communities as well as to larger cities.
Manitoba has five professional sports teams: the Winnipeg Blue Bombers (Canadian Football League), the Winnipeg Jets (National Hockey League), the Manitoba Moose (American Hockey League), the Winnipeg Goldeyes (American Association), and Valour FC (Canadian Premier League). The province was previously home to another team called the Winnipeg Jets, which played in the World Hockey Association and National Hockey League from 1972 until 1996, when financial troubles prompted a sale and move of the team, renamed the Phoenix Coyotes. A second incarnation of the Winnipeg Jets returned, after True North Sports & Entertainment bought the Atlanta Thrashers and moved the team to Winnipeg in time for the 2011 hockey season. Manitoba has two major junior-level hockey teams, the Western Hockey League's Brandon Wheat Kings and Winnipeg Ice, and one junior football team, the Winnipeg Rifles of the Canadian Junior Football League.

The province is represented in university athletics by the University of Manitoba Bisons, the University of Winnipeg Wesmen, and the Brandon University Bobcats. All three teams compete in the Canada West Universities Athletic Association, a regional division of U Sports.

Curling is an important winter sport in the province with Manitoba producing more men's national champions than any other province, while additionally in the top 3 women's national champions, as well as multiple world champions in the sport. The province also hosts the world's largest curling tournament in the MCA Bonspiel. The province is regular host to Grand Slam events which feature as the largest cash events in the sport such as the annual Manitoba Lotteries Women's Curling Classic as well as other rotating events.

Though not as prominent as hockey and curling, long track speed skating also features as a notable and top winter sport in Manitoba. The province has produced some of the world's best female speed skaters including Susan Auch and the country's top Olympic medal earners Cindy Klassen and Clara Hughes.





</doc>
<doc id="18929" url="https://en.wikipedia.org/wiki?curid=18929" title="Mount Logan">
Mount Logan

Mount Logan () is the highest mountain in Canada and the second-highest peak in North America, after Denali. The mountain was named after Sir William Edmond Logan, a Canadian geologist and founder of the Geological Survey of Canada (GSC). Mount Logan is located within Kluane National Park Reserve in southwestern Yukon, less than north of the Yukon–Alaska border. Mount Logan is the source of the Hubbard and Logan glaciers. Logan is believed to have the largest base circumference of any non-volcanic mountain on Earth (many shield volcanoes are much larger in size and mass), including a massif with eleven peaks over .

Due to active tectonic uplifting, Mount Logan is still rising in height. Before 1992, the exact elevation of Mount Logan was unknown and measurements ranged from . In May 1992, a GSC expedition climbed Mount Logan and fixed the current height of using GPS.

Temperatures are extremely low on and near Mount Logan. On the plateau, air temperature hovers around in the winter and reaches near freezing in summer with the median temperature for the year around . Minimal snow melt leads to a significant ice cap, reaching almost in certain spots. A temperature of was unofficially recorded on the summit on 26 May 1991.

The Mount Logan massif is considered to contain all the surrounding peaks with less than of prominence, as listed below:

In 1922, a geologist approached the Alpine Club of Canada with the suggestion that the club send a team to the mountain to reach the summit for the first time. An international team of Canadian, British and American climbers was assembled and initially they had planned their attempt in 1924 but funding and preparation delays postponed the trip until 1925. The international team of climbers began their journey in early May, crossing the mainland from the Pacific coast by train. They then walked the remaining to within of the Logan Glacier where they established base camp. In the early evening of June 23, 1925, Albert H. MacCarthy (leader), H.F. Lambart, Allen Carpé, W.W. Foster, Norman H. Read and Andy Taylor stood on top for the first time. It had taken them 65 days to approach the mountain from the nearest town, McCarthy, summit and return, with all climbers intact.


Following the death of former Prime Minister of Canada Pierre Trudeau in 2000, then Prime Minister Jean Chrétien, a close friend of Trudeau, proposed renaming the mountain Mount Trudeau.
However opposition from Yukoners, mountaineers, geologists, Trudeau's political critics, and many other Canadians forced the plan to be dropped. A mountain in British Columbia's Premier Range was named Mount Pierre Elliott Trudeau instead.




</doc>
<doc id="18930" url="https://en.wikipedia.org/wiki?curid=18930" title="Subject (philosophy)">
Subject (philosophy)

A subject is a being who has a unique consciousness and/or unique personal experiences, or an entity that has a relationship with another entity that exists outside itself (called an "object").

A "subject" is an observer and an "object" is a thing observed. This concept is especially important in Continental philosophy, where 'the subject' is a central term in debates over the nature of the self. The nature of the subject is also central in debates over the nature of subjective experience within the Anglo-American tradition of analytical philosophy.

The sharp distinction between subject and object corresponds to the distinction, in the philosophy of René Descartes, between thought and extension. Descartes believed that thought (subjectivity) was the essence of the mind, and that extension (the occupation of space) was the essence of matter.

"Subject" as a key-term in thinking about human consciousness began its career with the German Idealists, in response to David Hume's radical skepticism. The idealists' starting point was Hume's conclusion that there is nothing to the self over and above a big, fleeting bundle of perceptions. The next step was to ask how this undifferentiated bundle comes to be experienced as a unity – as a single "subject". Hume had offered the following proposal:

Kant, Hegel and their successors sought to flesh out the process by which the subject is constituted out of the flow of sense impressions. Hegel, for example, stated in his Preface to the "Phenomenology of Spirit" that a subject is constituted by "the process of reflectively mediating itself with itself."

Hegel begins his definition of the subject at a standpoint derived from Aristotelian physics: "the unmoved which is also "self-moving"" (Preface, para. 22). That is, what is not moved by an outside force, but which propels itself, has a "prima facie" case for subjectivity. Hegel's next step, however, is to identify this power to move, this unrest that is the subject, as "pure negativity". Subjective self-motion, for Hegel, comes not from any pure or simple kernel of authentic individuality, but rather, it is

The Hegelian subject's "modus operandi" is therefore cutting, splitting and introducing distinctions by injecting negation into the flow of sense-perceptions. Subjectivity is thus a kind of structural effect – what happens when Nature is diffused, refracted around a field of negativity and the "unity of the subject" for Hegel, is in fact a second-order effect, a "negation of negation". The subject experiences itself as a unity only by purposively negating the very diversity it itself had produced. The Hegelian subject may therefore be characterized either as "self-restoring sameness" or else as "reflection in otherness within itself" (Preface, para. 18).

The thinking of Karl Marx and Sigmund Freud provided a point of departure for questioning the notion of a unitary, autonomous Subject, which for many thinkers in the Continental tradition is seen as the foundation of the liberal theory of the social contract. These thinkers opened up the way for the deconstruction of the subject as a core-concept of metaphysics.

Sigmund Freud's explorations of the unconscious mind added up to a wholesale indictment of Enlightenment notions of subjectivity.

Among the most radical re-thinkers of human self-consciousness was Martin Heidegger, whose concept of "Dasein" or "Being-there" displaces traditional notions of the personal subject altogether. With Heidegger, phenomenology tries to go beyond the classical dichotomy between subject and object, because they are linked by an inseparable and original relationship, in the sense that there can be no world without a subject, nor the subject without world.

Jacques Lacan, inspired by Heidegger and Ferdinand de Saussure, built on Freud's psychoanalytic model of the subject, in which the "split subject" is constituted by a double bind: alienated from jouissance when he or she leaves the Real, enters into the Imaginary (during the mirror stage), and separates from the Other when he or she comes into the realm of language, difference, and demand in the Symbolic or the Name of the Father..

Thinkers such as structural Marxist Louis Althusser and poststructuralist Michel Foucault theorize the subject as a social construction, the so-called poststructuralist subject. According to Althusser, the "subject" is an ideological construction (more exactly, constructed by the "Ideological State Apparatuses"). One's subjectivity exists, "always already" and is discovered through the process of interpellation. Ideology inaugurates one into being a subject, and every ideology is intended to maintain and glorify its idealized subject, as well as the metaphysical category of the subject itself (see antihumanism). 

According to Foucault, it is the "effect" of power and "disciplines" (see "Discipline and Punish": construction of the subject (subjectivation or subjectification, ) as student, soldier, "criminal", etc.). Foucault believed it was possible to transform oneself; he used the word ethopoiein from the word "ethos" to describe the process. Subjectification was a central concept in Gilles Deleuze and Félix Guattari's work as well.

In contemporary analytic philosophy, the issue of subject—and more specifically the "point of view" of the subject, or "subjectivity"—has received attention as one of the major intractable problems in philosophy of mind (a related issue being the mind–body problem). In the essay "What is it like to be a bat?", Thomas Nagel famously argued that explaining subjective experience—the "what it is like" to be something—is currently beyond the reach of scientific inquiry, because scientific understanding by definition requires an objective perspective, which, according to Nagel, is diametrically opposed to the subjective first-person point of view. Furthermore, one cannot have a definition of objectivity without being connected to subjectivity in the first place since they are mutual and interlocked.

In Nagel's book "The View from Nowhere", he asks: "What kind of fact is it that I am Thomas Nagel?". Subjects have a perspective but each subject has a unique perspective and this seems to be a fact in Nagel's view from nowhere (i.e. the birds-eye view of the objective description in the universe). The Indian view of "Brahman" suggests that the ultimate and fundamental subject is existence itself, through which each of us as it were "looks out" as an aspect of a frozen and timeless everything, experienced subjectively due to our separated sensory and memory apparati. These additional features of subjective experience are often referred to as "qualia" (see Frank Cameron Jackson and Mary's room).



</doc>
<doc id="18932" url="https://en.wikipedia.org/wiki?curid=18932" title="Media bias">
Media bias

Media bias is the bias or perceived bias of journalists and news producers within the mass media in the selection of many events and stories that are reported and how they are covered. The term "media bias" implies a pervasive or widespread bias contravening the standards of journalism, rather than the perspective of an individual journalist or article. The direction and degree of media bias in various countries is widely disputed.

Practical limitations to media neutrality include the inability of journalists to report all available stories and facts, and the requirement that selected facts be linked into a coherent narrative. Government influence, including overt and covert censorship, biases the media in some countries, for example China, North Korea and Myanmar. Market forces that result in a biased presentation include the ownership of the news source, concentration of media ownership, the selection of staff, the preferences of an intended audience,

There are a number of national and international watchdog groups that report on bias in the media.

The most commonly discussed types of bias occur when the (allegedly partisan) media support or attack a particular political party, candidate, or ideology.

D'Alessio and Allen list three forms of media bias as the most widely studied:
Other common forms of political and non-political media bias include:

Other forms of bias include reporting that favors or attacks a particular race, religion, gender, age, sexual orientation, ethnic group, or even person.

Media bias in the United States occurs when the media in the United States systematically emphasizes one particular point of view in a manner that contravenes the standards of professional journalism. Claims of media bias in the United States include claims of liberal bias, conservative bias, mainstream bias, and corporate bias and activist/cause bias. To combat this, a variety of watchdog groups that attempt to find the facts behind both biased reporting and unfounded claims of bias have been founded. These include:

Research about media bias is now a subject of systematic scholarship in a variety of disciplines.

Media bias is studied at schools of journalism, university departments (including Media studies, Cultural studies and Peace studies) and by independent watchdog groups from various parts of the political spectrum. In the United States, many of these studies focus on issues of a conservative/liberal balance in the media. Other focuses include international differences in reporting, as well as bias in reporting of particular issues such as economic class or environmental interests. Currently, most of these analyses are performed manually, requiring exacting and time-consuming effort. However, an interdisciplinary literature review from 2018 found that automated methods, mostly from computer science and computational linguistics, are available or could with comparably low effort be adapted for the analysis of the various forms of media bias. Employing or adapting such techniques would help to further automate the analyses in the social sciences, such as content analysis and frame analysis.

Martin Harrison's "TV News: Whose Bias?" (1985) criticized the methodology of the Glasgow Media Group, arguing that the GMG identified bias selectively, via their own preconceptions about what phrases qualify as biased descriptions. For example, the GMG sees the word "idle" to describe striking workers as pejorative, despite the word being used by strikers themselves.

Herman and Chomsky (1988) proposed a propaganda model hypothesizing systematic biases of U.S. media from structural economic causes. They hypothesize media ownership by corporations, funding from advertising, the use of official sources, efforts to discredit independent media ("flak"), and "anti-communist" ideology as the filters that bias news in favor of U.S. corporate interests.

Many of the positions in the preceding study are supported by a 2002 study by Jim A. Kuypers: "Press Bias and Politics: How the Media Frame Controversial Issues". In this study of 116 mainstream US papers (including "The New York Times", "the Washington Post", "Los Angeles Times", and the "San Francisco Chronicle"), Kuypers found that the mainstream print press in America operate within a narrow range of liberal beliefs. Those who expressed points of view further to the left were generally ignored, whereas those who expressed moderate or conservative points of view were often actively denigrated or labeled as holding a minority point of view. In short, if a political leader, regardless of party, spoke within the press-supported range of acceptable discourse, he or she would receive positive press coverage. If a politician, again regardless of party, were to speak outside of this range, he or she would receive negative press or be ignored. Kuypers also found that the liberal points of view expressed in editorial and opinion pages were found in hard news coverage of the same issues. Although focusing primarily on the issues of race and homosexuality, Kuypers found that the press injected opinion into its news coverage of other issues such as welfare reform, environmental protection, and gun control; in all cases favoring a liberal point of view.

Studies reporting perceptions of bias in the media are not limited to studies of print media. A joint study by the Joan Shorenstein Center on Press, Politics and Public Policy at Harvard University and the Project for Excellence in Journalism found that people see media bias in television news media such as CNN. Although both CNN and Fox were perceived in the study as not being centrist, CNN was perceived as being more liberal than Fox. Moreover, the study's findings concerning CNN's perceived bias are echoed in other studies. There is also a growing economics literature on mass media bias, both on the theoretical and the empirical side. On the theoretical side the focus is on understanding to what extent the political positioning of mass media outlets is mainly driven by demand or supply factors. This literature is surveyed by Andrea Prat of Columbia University and David Stromberg of Stockholm University.

According to Dan Sutter of the University of Oklahoma, a systematic liberal bias in the U.S. media could depend on the fact that owners and/or journalists typically lean to the left.

Along the same lines, David Baron of Stanford GSB presents a game-theoretic model of mass media behaviour in which, given that the pool of journalists systematically leans towards the left or the right, mass media outlets maximise their profits by providing content that is biased in the same direction. They can do so, because it is cheaper to hire journalists who write stories that are consistent with their political position. A concurrent theory would be that supply and demand would cause media to attain a neutral balance because consumers would of course gravitate towards the media they agreed with. This argument fails in considering the imbalance in self-reported political allegiances by journalists themselves, that distort any market analogy as regards offer: (..) "Indeed, in 1982, 85 percent of Columbia Graduate School of Journalism students identified themselves as liberal, versus 11 percent conservative"" (Lichter, Rothman, and Lichter 1986: 48), quoted in Sutter, 2001.

This same argument would have news outlets in equal numbers increasing profits of a more balanced media far more than the slight increase in costs to hire unbiased journalists, notwithstanding the extreme rarity of self-reported conservative journalists (Sutton, 2001).

As mentioned above, Tim Groseclose of UCLA and Jeff Milyo of the University of Missouri at Columbia use think tank quotes, in order to estimate the relative position of mass media outlets in the political spectrum. The idea is to trace out which think tanks are quoted by various mass media outlets within news stories, and to match these think tanks with the political position of members of the U.S. Congress who quote them in a non-negative way. Using this procedure, Groseclose and Milyo obtain the stark result that all sampled news providers -except Fox News' Special Report and the Washington Times- are located to the left of the average Congress member, i.e. there are signs of a liberal bias in the US news media. 

The methods Groseclose and Milyo used to calculate this bias have been criticized by Mark Liberman, a professor of Linguistics at the University of Pennsylvania. Liberman concludes by saying he thinks "that many if not most of the complaints directed against G&M are motivated in part by ideological disagreement – just as much of the praise for their work is motivated by ideological agreement. It would be nice if there were a less politically fraught body of data on which such modeling exercises could be explored."

Sendhil Mullainathan and Andrei Shleifer of Harvard University construct a behavioural model, which is built around the assumption that readers and viewers hold beliefs that they would like to see confirmed by news providers. When news customers share common beliefs, profit-maximizing media outlets find it optimal to select and/or frame stories in order to pander to those beliefs. On the other hand, when beliefs are heterogeneous, news providers differentiate their offer and segment the market, by providing news stories that are slanted towards the two extreme positions in the spectrum of beliefs.

Matthew Gentzkow and Jesse Shapiro of Chicago GSB present another demand-driven theory of mass media bias. If readers and viewers have a priori views on the current state of affairs and are uncertain about the quality of the information about it being provided by media outlets, then the latter have an incentive to slant stories towards their customers' prior beliefs, in order to build and keep a reputation for high-quality journalism. The reason for this is that rational agents would tend to believe that pieces of information that go against their prior beliefs in fact originate from low-quality news providers.

Given that different groups in society have different beliefs, priorities, and interests, to which group would the media tailor its bias? David Stromberg constructs a demand-driven model where media bias arises because different audiences have different effects on media profits. Advertisers pay more for affluent audiences and media may tailor content to attract this audience, perhaps producing a right-wing bias. On the other hand, urban audiences are more profitable to newspapers because of lower delivery costs. Newspapers may for this reason tailor their content to attract the profitable predominantly liberal urban audiences. Finally, because of the increasing returns to scale in news production, small groups such as minorities are less profitable. This biases media content against the interest of minorities.

Steve Ansolabehere, Rebecca Lessem and Jim Snyder of the Massachusetts Institute of Technology analyze the political orientation of endorsements by U.S. newspapers. They find an upward trend in the average propensity to endorse a candidate, and in particular an incumbent one. There are also some changes in the average ideological slant of endorsements: while in the 1940s and in the 1950s there was a clear advantage to Republican candidates, this advantage continuously eroded in subsequent decades, to the extent that in the 1990s the authors find a slight Democratic lead in the average endorsement choice.

John Lott and Kevin Hassett of the American Enterprise Institute study the coverage of economic news by looking at a panel of 389 U.S. newspapers from 1991 to 2004, and from 1985 to 2004 for a subsample comprising the top 10 newspapers and the Associated Press. For each release of official data about a set of economic indicators, the authors analyze how newspapers decide to report on them, as reflected by the tone of the related headlines. The idea is to check whether newspapers display some kind of partisan bias, by giving more positive or negative coverage to the same economic figure, as a function of the political affiliation of the incumbent president. Controlling for the economic data being released, the authors find that there are between 9.6 and 14.7 percent fewer positive stories when the incumbent president is a Republican.

Riccardo Puglisi of the Massachusetts Institute of Technology looks at the editorial choices of the "New York Times" from 1946 to 1997. He finds that the "Times" displays Democratic partisanship, with some watchdog aspects. This is the case, because during presidential campaigns the "Times" systematically gives more coverage to Democratic topics of civil rights, health care, labor and social welfare, but only when the incumbent president is a Republican. These topics are classified as Democratic ones, because Gallup polls show that on average U.S. citizens think that Democratic candidates would be better at handling problems related to them. According to Puglisi, in the post-1960 period the "Times" displays a more symmetric type of watchdog behaviour, just because during presidential campaigns it also gives more coverage to the typically Republican issue of Defense when the incumbent president is a Democrat, and less so when the incumbent is a Republican.

Alan Gerber and Dean Karlan of Yale University use an experimental approach to examine not whether the media are biased, but whether the media influence political decisions and attitudes. They conduct a randomized control trial just prior to the November 2005 gubernatorial election in Virginia and randomly assign individuals in Northern Virginia to (a) a treatment group that receives a free subscription to the Washington Post, (b) a treatment group that receives a free subscription to the Washington Times, or (c) a control group. They find that those who are assigned to the Washington Post treatment group are eight percentage points more likely to vote for the Democrat in the elections. The report also found that "exposure to either newspaper was weakly linked to a movement away from the Bush administration and Republicans."

Another unaffiliated group, Media Study Group, established seven categories of poor journalistic practice: for example, the journalist stating personal opinion in a report, asserting incorrect facts, applying unequal space or treatment to two sides of a controversial issue; then analyzed The Age Newspaper (Melbourne Australia) for the frequency of infraction of this code of practice. The resultant instances were then analyzed statistically with respect to the frequency they supported one or other side of the two-sided controversial issue under consideration. The goal of this group was to establish a quantitative methodology for the study of bias.

A self-described "progressive" media watchdog group, Fairness and Accuracy in Reporting (FAIR), in consultation with the Survey and Evaluation Research Laboratory at Virginia Commonwealth University, sponsored a 1998 survey in which 141 Washington bureau chiefs and Washington-based journalists were asked a range of questions about how they did their work and about how they viewed the quality of media coverage in the broad area of politics and economic policy. "They were asked for their opinions and views about a range of recent policy issues and debates. Finally, they were asked for demographic and identifying information, including their political orientation". They then compared to the same or similar questions posed with "the public" based on Gallup, and Pew Trust polls. Their study concluded that a majority of journalists, although relatively liberal on social policies, were significantly to the right of the public on economic, labor, health care and foreign policy issues.

This study continues: "we learn much more about the political orientation of news content by looking at sourcing patterns rather than journalists' personal views. As this survey shows, it is government officials and business representatives to whom journalists "nearly always" turn when covering economic policy. Labor representatives and consumer advocates were at the bottom of the list. This is consistent with earlier research on sources. For example, analysts from the non-partisan Brookings Institution and from conservative think tanks such as the Heritage Foundation and the American Enterprise Institute are those most quoted in mainstream news accounts.

In direct contrast to the FAIR survey, in 2014, media communication researcher Jim A. Kuypers published a 40-year longitudinal, aggregate study of the political beliefs and actions of American journalists. In every single category (for instance, social, economic, unions, health care, and foreign policy) he found that nationwide, print and broadcast journalists and editors as a group were "considerably" to the political left of the majority of Americans, and that these political beliefs found their way into news stories. Kuypers concluded, "Do the political proclivities of journalists influence their interpretation of the news? I answer that with a resounding, yes. As part of my evidence, I consider testimony from journalists themselves. ... [A] solid majority of journalists do allow their political ideology to influence their reporting."

Jonathan M. Ladd, who has conducted intensive studies of media trust and media bias, concluded that the primary cause of belief in media bias is media telling their audience that particular media are biased. People who are told that a medium is biased tend to believe that it is biased, and this belief is unrelated to whether that medium is actually biased or not. The only other factor with as strong an influence on belief that media is biased is extensive coverage of celebrities. A majority of people see such media as biased, while at the same time preferring media with extensive coverage of celebrities.

A major problem in studies is confirmation bias. Research into studies of media bias in the United States shows that liberal experimenters tend to get results that say the media has a conservative bias, while conservatives experimenters tend to get results that say the media has a liberal bias, and those who do not identify themselves as either liberal or conservative get results indicating little bias, or mixed bias.

The study "A Measure of Media Bias", by political scientist Timothy J. Groseclose of UCLA and economist Jeffrey D. Milyo of the University of Missouri-Columbia, purports to rank news organizations in terms of identifying with liberal or conservative values relative to each other. They used the Americans for Democratic Action (ADA) scores as a quantitative proxy for political leanings of the referential organizations. Thus their definition of "liberal" includes the RAND Corporation, a nonprofit research organization with strong ties to the Defense Department. Their work claims to detect a bias towards liberalism in the American media.

A technique used to avoid bias is the "point/counterpoint" or "round table", an adversarial format in which representatives of opposing views comment on an issue. This approach theoretically allows diverse views to appear in the media. However, the person organizing the report still has the responsibility to choose people who really represent the breadth of opinion, to ask them non-prejudicial questions, and to edit or arbitrate their comments fairly. When done carelessly, a point/counterpoint can be as unfair as a simple biased report, by suggesting that the "losing" side lost on its merits.

Using this format can also lead to accusations that the reporter has created a misleading appearance that viewpoints have equal validity (sometimes called "false balance"). This may happen when a taboo exists around one of the viewpoints, or when one of the representatives habitually makes claims that are easily shown to be inaccurate.

One such allegation of misleading balance came from Mark Halperin, political director of ABC News. He stated in an internal e-mail message that reporters should not "artificially hold George W. Bush and John Kerry 'equally' accountable" to the public interest, and that complaints from Bush supporters were an attempt to "get away with ... renewed efforts to win the election by destroying Senator Kerry." When the conservative web site the Drudge Report published this message, many Bush supporters viewed it as "smoking gun" evidence that Halperin was using ABC to propagandize against Bush to Kerry's benefit, by interfering with reporters' attempts to avoid bias. An academic content analysis of election news later found that coverage at ABC, CBS, and NBC was more favorable toward Kerry than Bush, while coverage at Fox News Channel was more favorable toward Bush.

Scott Norvell, the London bureau chief for Fox News, stated in a May 20, 2005 interview with the "Wall Street Journal" that:
"Even we at Fox News manage to get some lefties on the air occasionally, and often let them finish their sentences before we club them to death and feed the scraps to Karl Rove and Bill O'Reilly. And those who hate us can take solace in the fact that they aren't subsidizing Bill's bombast; we payers of the BBC license fee don't enjoy that peace of mind.<br>
Fox News is, after all, a private channel and our presenters are quite open about where they stand on particular stories. That's our appeal. People watch us because they know what they are getting. The Beeb's (British Broadcasting Corporation) (BBC) institutionalized leftism would be easier to tolerate if the corporation was a little more honest about it".

Another technique used to avoid bias is disclosure of affiliations that may be considered a possible conflict of interest. This is especially apparent when a news organization is reporting a story with some relevancy to the news organization itself or to its ownership individuals or conglomerate. Often this disclosure is mandated by the laws or regulations pertaining to stocks and securities. Commentators on news stories involving stocks are often required to disclose any ownership interest in those corporations or in its competitors.

In rare cases, a news organization may dismiss or reassign staff members who appear biased. This approach was used in the Killian documents affair and after Peter Arnett's interview with the Iraqi press. This approach is presumed to have been employed in the case of Dan Rather over a story that he ran on "60 Minutes" in the month prior to the 2004 election that attempted to impugn the military record of George W. Bush by relying on allegedly fake documents that were provided by Bill Burkett, a retired Lieutenant Colonel in the Texas Army National Guard.

Finally, some countries have laws enforcing balance in state-owned media. Since 1991, the CBC and Radio Canada, its French language counterpart, are governed by the Broadcasting Act. This act states, among other things:
the programming provided by the Canadian broadcasting system should
(i) be varied and comprehensive, providing a balance of information, enlightenment and entertainment for men, women and children of all ages, interests and tastes,
(iv) provide a reasonable opportunity for the public to be exposed to the expression of differing views on matters of public concern

Besides these manual approaches, several (semi-)automated approaches have been developed by social scientists and computer scientists. These approaches identify differences in news coverage, which potentially resulted from media bias, by analyzing the text and meta data, such as author and publishing date. For instance, NewsCube is a news aggregator that extracts key phrases that describe a topic differently. Other approaches make use of text- and meta-data, e.g., matrix-based news aggregation spans a matrix over two dimensions, such as "publisher countries" (in which articles have been published) and "mentioned countries" (on which country an article reports). As a result, each cell contains only articles that have been published in one country and that report on another country. Particularly in international news topics, matrix-based news aggregation helps to reveal differences in media coverage between the involved countries.

Political bias has been a feature of the mass media since its birth with the invention of the printing press. The expense of early printing equipment restricted media production to a limited number of people. Historians have found that publishers often served the interests of powerful social groups.

John Milton's pamphlet "Areopagitica, a Speech for the Liberty of Unlicensed Printing", published in 1644, was one of the first publications advocating freedom of the press.

In the 19th century, journalists began to recognize the concept of unbiased reporting as an integral part of journalistic ethics. This coincided with the rise of journalism as a powerful social force. Even today, though, the most conscientiously objective journalists cannot avoid accusations of bias.

Like newspapers, the broadcast media (radio and television) have been used as a mechanism for propaganda from their earliest days, a tendency made more pronounced by the initial ownership of broadcast spectrum by national governments. Although a process of media deregulation has placed the majority of the western broadcast media in private hands, there still exists a strong government presence, or even monopoly, in the broadcast media of many countries across the globe. At the same time, the concentration of media ownership in private hands, and frequently amongst a comparatively small number of individuals, has also led to accusations of media bias.

There are many examples of accusations of bias being used as a political tool, sometimes resulting in government censorship.

Not all accusations of bias are political. Science writer Martin Gardner has accused the entertainment media of anti-science bias. He claims that television programs such as "The X-Files" promote superstition. In contrast, the Competitive Enterprise Institute, which is funded by businesses, accuses the media of being biased in favor of science and against business interests, and of credulously reporting science that shows that greenhouse gasses cause global warming.

Mass media, despite its ability to project worldwide, is limited in its cross-ethnic compatibility by one simple attribute – language. Ethnicity, being largely developed by a divergence in geography, language, culture, genes and similarly, point of view, has the potential to be countered by a common source of information. Therefore, language, in the absence of translation, comprises a barrier to a worldwide community of debate and opinion, although it is also true that media within any given society may be split along class, political or regional lines.
Furthermore, if the language is translated, the translator has room to shift a bias by choosing weighed words for translation.

Language may also be seen as a political factor in mass media, particularly in instances where a society is characterized by a large number of languages spoken by its populace. The choice of language of mass media may represent a bias towards the group most likely to speak that language, and can limit the public participation by those who do not speak the language. On the other hand, there have also been attempts to use a common-language mass media to reach out to a large, geographically dispersed population, such as in the use of Arabic language by news channel Al Jazeera.

Many media theorists concerned with language and media bias point towards the media of the United States, a large country where English is spoken by the majority of the population. Some theorists argue that the common language is not homogenizing; and that there still remain strong differences expressed within the mass media. This viewpoint asserts that moderate views are bolstered by drawing influences from the extremes of the political spectrum. In the United States, the national news therefore contributes to a sense of cohesion within the society, proceeding from a similarly informed population. According to this model, most views within society are freely expressed, and the mass media are accountable to the people and tends to reflect the spectrum of opinion.

Language may also introduce a more subtle form of bias. The selection of metaphors and analogies, or the inclusion of personal information in one situation but not another can introduce bias, such as a gender bias. Use of a word with positive or negative connotations rather than a more neutral synonym can form a biased picture in the audience's mind. For example, it makes a difference whether the media calls a group "terrorists" or "freedom fighters" or "insurgents". A 2005 memo to the staff of the CBC states:

In a widely criticized episode, initial online BBC reports of the 7 July 2005 London bombings identified the perpetrators as terrorists, in contradiction to the BBC's internal policy. But by the next day, journalist Tom Gross noted that the online articles had been edited, replacing "terrorists" by "bombers". In another case, March 28, 2007, the BBC paid almost $400,000 in legal fees in a London court to keep an internal memo dealing with alleged anti-Israeli bias from becoming public. The BBC has been accused of having a pro-Israel bias, which it has partially admitted to in a case in 2013.

Many news organizations reflect, or are perceived to reflect in some way, the viewpoint of the geographic, ethnic, and national population that they primarily serve. Media within countries are sometimes seen as being sycophantic or unquestioning about the country's government.

Western media are often criticized in the rest of the world (including eastern Europe, Asia, Africa, and the Middle East) as being pro-Western with regard to a variety of political, cultural and economic issues. Al Jazeera is frequently criticized both in the West and in the Arab world.

The Israeli–Palestinian conflict and wider Arab–Israeli issues are a particularly controversial area, and nearly all coverage of any kind generates accusation of bias from one or both sides. This topic is covered in a separate article.

It has been observed that the world's principal suppliers of news, the news agencies, and the main buyers of news are Anglophone corporations and this gives an Anglophone bias to the selection and depiction of events. Anglophone definitions of what constitutes news are paramount; the news provided originates in Anglophone capitals and responds first to their own rich domestic markets.

Despite the plethora of news services, most news printed and broadcast throughout the world each day comes from only a few major agencies, the three largest of which are the Associated Press, Reuters and Agence France-Presse. Although these agencies are 'global' in the sense of their activities, they each retain significant associations with particular nations, namely the United States (AP), the United Kingdom (Reuters) and France (AFP). Chambers and Tinckell suggest that the so-called global media are agents of Anglophone values which privilege norms of 'competitive individualism, "laissez-faire" capitalism, parliamentary democracy and consumerism.' They see the presentation of the English language as international as a further feature of Anglophone dominance.

The media are often accused of bias favoring a particular religion or of bias against a particular religion. In some countries, only reporting approved by a state religion is permitted. In other countries, derogatory statements about any belief system are considered hate crimes and are illegal.

According to the Encyclopedia of Social Work (19th edition), the news media play an influential role in the general public's perception of cults. As reported in several studies, the media have depicted cults as problematic, controversial, and threatening from the beginning, tending to favor sensationalistic stories over balanced public debates. It furthers the analysis that media reports on cults rely heavily on police officials and cult "experts" who portray cult activity as dangerous and destructive, and when divergent views are presented, they are often overshadowed by horrific stories of ritualistic torture, sexual abuse, mind control, and other such practices. Furthermore, unfounded allegations, when proved untrue, receive little or no media attention.

In 2012, "Huffington Post", columnist Jacques Berlinerblau argued that secularism has often been misinterpreted in the media as another word for atheism, stating that: "Secularism must be the most misunderstood and mangled ism in the American political lexicon. Commentators on the right and the left routinely equate it with Stalinism, Nazism and Socialism, among other dreaded isms. In the United States, of late, another false equation has emerged. That would be the groundless association of secularism with atheism. The religious right has profitably promulgated this misconception at least since the 1970s."

According to Stuart A. Wright, there are six factors that contribute to media bias against minority religions: first, the knowledge and familiarity of journalists with the subject matter; second, the degree of cultural accommodation of the targeted religious group; third, limited economic resources available to journalists; fourth, time constraints; fifth, sources of information used by journalists; and finally, the front-end/back-end disproportionality of reporting. According to Yale Law professor Stephen Carter, "it has long been the American habit to be more suspicious of—and more repressive toward—religions that stand outside the mainline Protestant-Roman Catholic-Jewish troika that dominates America's spiritual life." As for front-end/back-end disproportionality, Wright says: "news stories on unpopular or marginal religions frequently are predicated on unsubstantiated allegations or government actions based on faulty or weak evidence occurring at the front-end of an event. As the charges weighed in against material evidence, these cases often disintegrate. Yet rarely is there equal space and attention in the mass media given to the resolution or outcome of the incident. If the accused are innocent, often the public is not made aware."

The apparent bias of media is not always specifically political in nature. The news media tend to appeal to a specific audience, which means that stories that affect a large number of people on a global scale often receive less coverage in some markets than local stories, such as a public school shooting, a celebrity wedding, a plane crash, a "missing white woman", or similarly glamorous or shocking stories. For example, the deaths of millions of people in an ethnic conflict in Africa might be afforded scant mention in American media, while the shooting of five people in a high school is analyzed in depth. Bias is also known to exist in sports broadcasting; in the United States, broadcasters tend to favor teams on the East Coast, teams in major markets, older and more established teams and leagues, teams based in their respective country (in international sport) and teams that include high-profile celebrity athletes. The reason for these types of bias is a function of what the public wants to watch and/or what producers and publishers believe the public wants to watch.

Bias has also been claimed in instances referred to as conflict of interest, whereby the owners of media outlets have vested interests in other commercial enterprises or political parties. In such cases in the United States, the media outlet is required to disclose the conflict of interest.

However, the decisions of the editorial department of a newspaper and the corporate parent frequently are not connected, as the editorial staff retains freedom to decide what is covered as well as what is not. Biases, real or implied, frequently arise when it comes to deciding what stories will be covered and who will be called for those stories.

Accusations that a source is biased, if accepted, may cause media consumers to distrust certain kinds of statements, and place added confidence on others.

In 1997, two-thirds (67%) said agreed with the statement: "In dealing with political and social issues, news organizations tend to favor one side." That was up 14 points from 53 percent who gave that answer in 1985. Those who believed the media "deal fairly with all sides" fell from 34 percent to 27 percent. "In one of the most telling complaints, a majority (54%) of Americans believe the news media gets in the way of society solving its problems," Pew reported. Republicans "are more likely to say news organizations favor one side than are Democrats or independents (77 percent vs. 58 percent and 69 percent, respectively)."
The percentage who felt "news organizations get the facts straight" fell from 55 percent to 37 percent.




</doc>
<doc id="18934" url="https://en.wikipedia.org/wiki?curid=18934" title="Muhammad">
Muhammad

Muhammad (, ; c. 570 CE – 8 June 632 CE) was an Arab religious, social, and political leader and the founder of Islam. According to Islamic doctrine, he was a prophet, sent to present and confirm the monotheistic teachings preached previously by Adam, Abraham, Moses, Jesus, and other prophets. He is viewed as the final prophet of God in all the main branches of Islam, though some modern denominations diverge from this belief. Muhammad united Arabia into a single Muslim polity, with the Quran as well as his teachings and practices forming the basis of Islamic religious belief. He is referred to by many appellations, including Messenger of Allah, The Prophet Muhammad, Allah's Apostle, Last Prophet of Islam, and others; there are also many variant spellings of Muhammad, such as Mohamet, Mahamad, Muhamad, and many others.

Born approximately 570CE (Year of the Elephant) in the Arabian city of Mecca, Muhammad was orphaned at the age of six. He was raised under the care of his paternal grandfather Abd al-Muttalib, and upon his death, by his uncle Abu Talib. In later years, he would periodically seclude himself in a mountain cave named Hira for several nights of prayer. When he was 40, Muhammad reported being visited by Gabriel in the cave, and receiving his first revelation from God. Three years later, in 610, Muhammad started preaching these revelations publicly, proclaiming that "God is One", that complete "submission" ("islām") to God is the right way of life ("dīn"), and that he was a prophet and messenger of God, similar to the other prophets in Islam.

The followers of Muhammad were initially few in number, and experienced hostility from Meccan polytheists. He sent some of his followers to Abyssinia in 615 to shield them from prosecution, before he and his followers migrated from Mecca to Medina (then known as Yathrib) in 622. This event, the "Hijra", marks the beginning of the Islamic calendar, also known as the Hijri Calendar. In Medina, Muhammad united the tribes under the Constitution of Medina. In December 629, after eight years of intermittent fighting with Meccan tribes, Muhammad gathered an army of 10,000 Muslim converts and marched on the city of Mecca. The conquest went largely uncontested and Muhammad seized the city with little bloodshed. In 632, a few months after returning from the Farewell Pilgrimage, he fell ill and died. By the time of his death, most of the Arabian Peninsula had converted to Islam.

The revelations (each known as "Ayah" — literally, "Sign [of God]") that Muhammad reported receiving until his death form the verses of the Quran, regarded by Muslims as the verbatim "Word of God" on which the religion is based. Besides the Quran, Muhammad's teachings and practices ("sunnah"), found in the Hadith and "sira" (biography) literature, are also upheld and used as sources of Islamic law (see Sharia).

The name "Muhammad" () means "praiseworthy" and appears four times in the Quran. The Quran also addresses Muhammad in the second person by various appellations; prophet, messenger, servant of God ("'abd"), announcer ("bashir"), witness ("shahid"), bearer of good tidings ("mubashshir"), warner ("nathir"), reminder ("mudhakkir"), one who calls [unto God] ("dā'ī"), light personified ("noor"), and the light-giving lamp ("siraj munir").

The Quran is the central religious text of Islam. Muslims believe it represents the words of God revealed by the archangel Gabriel to Muhammad. The Quran, however, provides minimal assistance for Muhammad's chronological biography; most Quranic verses do not provide significant historical context.

Important sources regarding Muhammad's life may be found in the historic works by writers of the 2nd and 3rd centuries of the Muslim era (AH – 8th and 9th century CE). These include traditional Muslim biographies of Muhammad, which provide additional information about Muhammad's life.

The earliest surviving written "sira" (biographies of Muhammad and quotes attributed to him) is Ibn Ishaq's "Life of God's Messenger" written c. 767 CE (150 AH). Although the work was lost, this sira was used at great length by Ibn Hisham and to a lesser extent by Al-Tabari. However, Ibn Hisham admits in the preface to his biography of Muhammad that he omitted matters from Ibn Ishaq's biography that "would distress certain people". Another early history source is the history of Muhammad's campaigns by al-Waqidi (death 207 of Muslim era), and the work of his secretary Ibn Sa'd al-Baghdadi (death 230 of Muslim era).

Many scholars accept these early biographies as authentic, though their accuracy is unascertainable. Recent studies have led scholars to distinguish between traditions touching legal matters and purely historical events. In the legal group, traditions could have been subject to invention while historic events, aside from exceptional cases, may have been only subject to "tendential shaping".

Other important sources include the hadith collections, accounts of the verbal and physical teachings and traditions of Muhammad. Hadiths were compiled several generations after his death by followers including Muhammad al-Bukhari, Muslim ibn al-Hajjaj, Muhammad ibn Isa at-Tirmidhi, Abd ar-Rahman al-Nasai, Abu Dawood, Ibn Majah, Malik ibn Anas, al-Daraqutni.

Some Western academics cautiously view the hadith collections as accurate historical sources. Scholars such as Madelung do not reject the narrations which have been compiled in later periods, but judge them in the context of history and on the basis of their compatibility with the events and figures. Muslim scholars on the other hand typically place a greater emphasis on the hadith literature instead of the biographical literature, since hadiths maintain a verifiable chain of transmission (isnad); the lack of such a chain for the biographical literature makes it less verifiable in their eyes.

The Arabian Peninsula was, and still is, largely arid with volcanic soil, making agriculture difficult except near oases or springs. Towns and cities dotted the landscape; two of the most prominent being Mecca and Medina. Medina was a large flourishing agricultural settlement, while Mecca was an important financial center for many surrounding tribes. Communal life was essential for survival in the desert conditions, supporting indigenous tribes against the harsh environment and lifestyle. Tribal affiliation, whether based on kinship or alliances, was an important source of social cohesion. Indigenous Arabs were either nomadic or sedentary. Nomadic groups constantly traveled seeking water and pasture for their flocks, while the sedentary settled and focused on trade and agriculture. Nomadic survival also depended on raiding caravans or oases; nomads did not view this as a crime.

In pre-Islamic Arabia, gods or goddesses were viewed as protectors of individual tribes, their spirits being associated with sacred trees, stones, springs and wells. As well as being the site of an annual pilgrimage, the Kaaba shrine in Mecca housed 360 idols of tribal patron deities. Three goddesses were revered as God's daughters: Allāt, Manāt and al-'Uzzá. Monotheistic communities existed in Arabia, including Christians and Jews. Hanifs – native pre-Islamic Arabs who "professed a rigid monotheism" – are also sometimes listed alongside Jews and Christians in pre-Islamic Arabia, although their historicity is disputed among scholars. According to Muslim tradition, Muhammad himself was a Hanif and one of the descendants of Ishmael, son of Abraham.

The second half of the sixth century was a period of political disorder in Arabia and communication routes were no longer secure. Religious divisions were an important cause of the crisis. Judaism became the dominant religion in Yemen while Christianity took root in the Persian Gulf area. In line with broader trends of the ancient world, the region witnessed a decline in the practice of polytheistic cults and a growing interest in a more spiritual form of religion. While many were reluctant to convert to a foreign faith, those faiths provided intellectual and spiritual reference points.

During the early years of Muhammad's life, the Quraysh tribe he belonged to became a dominant force in western Arabia. They formed the cult association of "hums", which tied members of many tribes in western Arabia to the Kaaba and reinforced the prestige of the Meccan sanctuary. To counter the effects of anarchy, Quraysh upheld the institution of sacred months during which all violence was forbidden, and it was possible to participate in pilgrimages and fairs without danger. Thus, although the association of "hums" was primarily religious, it also had important economic consequences for the city.

Abū al-Qāsim Muḥammad ibn ʿAbd Allāh ibn ʿAbd al-Muṭṭalib ibn Hāshim, was born in Mecca about the year 570 and his birthday is believed to be in the month of Rabi' al-awwal. He belonged to the Banu Hashim clan, part of the Quraysh tribe, and was one of Mecca's prominent families, although it appears less prosperous during Muhammad's early lifetime. Tradition places the year of Muhammad's birth as corresponding with the Year of the Elephant, which is named after the failed destruction of Mecca that year by the Abraha, Yemen's king, who supplemented his army with elephants.
Alternatively some 20th century scholars have suggested different years, such as 568 or 569.

Muhammad's father, Abdullah, died almost six months before he was born. According to Islamic tradition, soon after birth he was sent to live with a Bedouin family in the desert, as desert life was considered healthier for infants; some western scholars reject this tradition's historicity. Muhammad stayed with his foster-mother, Halimah bint Abi Dhuayb, and her husband until he was two years old. At the age of six, Muhammad lost his biological mother Amina to illness and became an orphan. For the next two years, until he was eight years old, Muhammad was under the guardianship of his paternal grandfather Abdul-Muttalib, of the Banu Hashim clan until his death. He then came under the care of his uncle Abu Talib, the new leader of the Banu Hashim. According to Islamic historian William Montgomery Watt there was a general disregard by guardians in taking care of weaker members of the tribes in Mecca during the 6th century, "Muhammad's guardians saw that he did not starve to death, but it was hard for them to do more for him, especially as the fortunes of the clan of Hashim seem to have been declining at that time."

In his teens, Muhammad accompanied his uncle on Syrian trading journeys to gain experience in commercial trade. Islamic tradition states that when Muhammad was either nine or twelve while accompanying the Meccans' caravan to Syria, he met a Christian monk or hermit named Bahira who is said to have foreseen Muhammad's career as a prophet of God.

Little is known of Muhammad during his later youth, available information is fragmented, making it difficult to separate history from legend. It is known that he became a merchant and "was involved in trade between the Indian Ocean and the Mediterranean Sea." Due to his upright character he acquired the nickname "al-Amin" (Arabic: الامين), meaning "faithful, trustworthy" and "al-Sadiq" meaning "truthful" and was sought out as an impartial arbitrator. His reputation attracted a proposal in 595 from Khadijah, a 40-year-old widow. Muhammad consented to the marriage, which by all accounts was a happy one.

Several years later, according to a narration collected by historian Ibn Ishaq, Muhammad was involved with a well-known story about setting the Black Stone in place in the wall of the Kaaba in 605 CE. The Black Stone, a sacred object, was removed during renovations to the Kaaba. The Meccan leaders could not agree which clan should return the Black Stone to its place. They decided to ask the next man who comes through the gate to make that decision; that man was the 35-year-old Muhammad. This event happened five years before the first revelation by Gabriel to him. He asked for a cloth and laid the Black Stone in its center. The clan leaders held the corners of the cloth and together carried the Black Stone to the right spot, then Muhammad laid the stone, satisfying the honour of all.

Muhammad began to pray alone in a cave named Hira on Mount Jabal al-Nour, near Mecca for several weeks every year. Islamic tradition holds that during one of his visits to that cave, in the year 610 the angel Gabriel appeared to him and commanded Muhammad to recite verses that would be included in the Quran. Consensus exists that the first Quranic words revealed were the beginning of Surah .
Muhammad was deeply distressed upon receiving his first revelations. After returning home, Muhammad was consoled and reassured by Khadijah and her Christian cousin, Waraka ibn Nawfal. He also feared that others would dismiss his claims as being possessed. Shi'a tradition states Muhammad was not surprised or frightened at Gabriel's appearance; rather he welcomed the angel, as if he was expected. The initial revelation was followed by a three-year pause (a period known as "fatra") during which Muhammad felt depressed and further gave himself to prayers and spiritual practices. When the revelations resumed he was reassured and commanded to begin preaching: "Thy Guardian-Lord hath not forsaken thee, nor is He displeased."

Sahih Bukhari narrates Muhammad describing his revelations as "sometimes it is (revealed) like the ringing of a bell". Aisha reported, "I saw the Prophet being inspired Divinely on a very cold day and noticed the sweat dropping from his forehead (as the Inspiration was over)". According to Welch these descriptions may be considered genuine, since they are unlikely to have been forged by later Muslims. Muhammad was confident that he could distinguish his own thoughts from these messages. According to the Quran, one of the main roles of Muhammad is to warn the unbelievers of their eschatological punishment (Quran , Quran ). Occasionally the Quran did not explicitly refer to Judgment day but provided examples from the history of extinct communities and warns Muhammad's contemporaries of similar calamities (Quran ). Muhammad did not only warn those who rejected God's revelation, but also dispensed good news for those who abandoned evil, listening to the divine words and serving God. Muhammad's mission also involves preaching monotheism: The Quran commands Muhammad to proclaim and praise the name of his Lord and instructs him not to worship idols or associate other deities with God. 

The key themes of the early Quranic verses included the responsibility of man towards his creator; the resurrection of the dead, God's final judgment followed by vivid descriptions of the tortures in Hell and pleasures in Paradise, and the signs of God in all aspects of life. Religious duties required of the believers at this time were few: belief in God, asking for forgiveness of sins, offering frequent prayers, assisting others particularly those in need, rejecting cheating and the love of wealth (considered to be significant in the commercial life of Mecca), being chaste and not committing female infanticide.

According to Muslim tradition, Muhammad's wife Khadija was the first to believe he was a prophet. She was followed by Muhammad's ten-year-old cousin Ali ibn Abi Talib, close friend Abu Bakr, and adopted son Zaid. Around 613, Muhammad began to preach to the public (Quran ). Most Meccans ignored and mocked him, though a few became his followers. There were three main groups of early converts to Islam: younger brothers and sons of great merchants; people who had fallen out of the first rank in their tribe or failed to attain it; and the weak, mostly unprotected foreigners.

According to Ibn Saad, opposition in Mecca started when Muhammad delivered verses that condemned idol worship and the polytheism practiced by the Meccan forefathers. However, the Quranic exegesis maintains that it began as Muhammad started public preaching. As his followers increased, Muhammad became a threat to the local tribes and rulers of the city, whose wealth rested upon the Ka'aba, the focal point of Meccan religious life that Muhammad threatened to overthrow. Muhammad's denunciation of the Meccan traditional religion was especially offensive to his own tribe, the Quraysh, as they were the guardians of the Ka'aba. Powerful merchants attempted to convince Muhammad to abandon his preaching; he was offered admission to the inner circle of merchants, as well as an advantageous marriage. He refused both of these offers.

Tradition records at great length the persecution and ill-treatment towards Muhammad and his followers. Sumayyah bint Khayyat, a slave of a prominent Meccan leader Abu Jahl, is famous as the first martyr of Islam; killed with a spear by her master when she refused to give up her faith. Bilal, another Muslim slave, was tortured by Umayyah ibn Khalaf who placed a heavy rock on his chest to force his conversion.

In 615, some of Muhammad's followers emigrated to the Ethiopian Kingdom of Aksum and founded a small colony under the protection of the Christian Ethiopian emperor Aṣḥama ibn Abjar. Ibn Sa'ad mentions two separate migrations. According to him, most of the Muslims returned to Mecca prior to Hijra, while a second group rejoined them in Medina. Ibn Hisham and Tabari, however, only talk about one migration to Ethiopia. These accounts agree that Meccan persecution played a major role in Muḥammad's decision to suggest that a number of his followers seek refuge among the Christians in Abyssinia. According to the famous letter of ʿUrwa preserved in al-Tabari, the majority of Muslims returned to their native town as Islam gained strength and high ranking Meccans, such as Umar and Hamzah converted.

However, there is a completely different story on the reason why the Muslims returned from Ethiopia to Mecca. According to this account—initially mentioned by Al-Waqidi then rehashed by Ibn Sa'ad and Tabari, but not by Ibn Hisham and not by Ibn Ishaq—Muhammad, desperately hoping for an accommodation with his tribe, pronounced a verse acknowledging the existence of three Meccan goddesses considered to be the daughters of Allah. Muhammad retracted the verses the next day at the behest of Gabriel, claiming that the verses were whispered by the devil himself. Instead, a ridicule of these gods was offered. This episode, known as "The Story of the Cranes," is also known as "Satanic Verses". According to the story, this led to a general reconciliation between Muḥammad and the Meccans, and the Abyssinia Muslims began to return home. When they arrived Gabriel had informed Muḥammad the two verses were not part of the revelation, but had been inserted by Satan. Notable scholars at the time argued against the historic authenticity of these verses and the story itself on various grounds. Al-Waqidi was severely criticized by Islamic scholars such as Malik ibn Anas, al-Shafi'i, Ahmad ibn Hanbal, Al-Nasa'i, al-Bukhari, Abu Dawood, Al-Nawawi and others as a liar and forger. Later, the incident received some acceptance among certain groups, though strong objections to it continued onwards past the tenth century. The objections continued until rejection of these verses and the story itself eventually became the only acceptable orthodox Muslim position.

In 617, the leaders of Makhzum and Banu Abd-Shams, two important Quraysh clans, declared a public boycott against Banu Hashim, their commercial rival, to pressure it into withdrawing its protection of Muhammad. The boycott lasted three years but eventually collapsed as it failed in its objective. During this time, Muhammad was only able to preach during the holy pilgrimage months in which all hostilities between Arabs was suspended.

Islamic tradition states that in 620, Muhammad experienced the "Isra and Mi'raj", a miraculous night-long journey said to have occurred with the angel Gabriel. At the journey's beginning, the "Isra", he is said to have traveled from Mecca on a winged steed to "the farthest mosque." Later, during the "Mi'raj", Muhammad is said to have toured heaven and hell, and spoke with earlier prophets, such as Abraham, Moses, and Jesus. Ibn Ishaq, author of the first biography of Muhammad, presents the event as a spiritual experience; later historians, such as Al-Tabari and Ibn Kathir, present it as a physical journey.

Some western scholars hold that the Isra and Mi'raj journey traveled through the heavens from the sacred enclosure at Mecca to the celestial "al-Baytu l-Maʿmur" (heavenly prototype of the Kaaba); later traditions indicate Muhammad's journey as having been from Mecca to Jerusalem.

Muhammad's wife Khadijah and uncle Abu Talib both died in 619, the year thus being known as the "Year of Sorrow". With the death of Abu Talib, leadership of the Banu Hashim clan passed to Abu Lahab, a tenacious enemy of Muhammad. Soon afterward, Abu Lahab withdrew the clan's protection over Muhammad. This placed Muhammad in danger; the withdrawal of clan protection implied that blood revenge for his killing would not be exacted. Muhammad then visited Ta'if, another important city in Arabia, and tried to find a protector, but his effort failed and further brought him into physical danger. Muhammad was forced to return to Mecca. A Meccan man named Mut'im ibn Adi (and the protection of the tribe of Banu Nawfal) made it possible for him to safely re-enter his native city.
Many people visited Mecca on business or as pilgrims to the Kaaba. Muhammad took this opportunity to look for a new home for himself and his followers. After several unsuccessful negotiations, he found hope with some men from Yathrib (later called Medina). The Arab population of Yathrib were familiar with monotheism and were prepared for the appearance of a prophet because a Jewish community existed there. They also hoped, by the means of Muhammad and the new faith, to gain supremacy over Mecca; the Yathrib were jealous of its importance as the place of pilgrimage. Converts to Islam came from nearly all Arab tribes in Medina; by June of the subsequent year, seventy-five Muslims came to Mecca for pilgrimage and to meet Muhammad. Meeting him secretly by night, the group made what is known as the ""Second Pledge of al-'Aqaba"", or, in Orientalists' view, the ""Pledge of War"". Following the pledges at Aqabah, Muhammad encouraged his followers to emigrate to Yathrib. As with the migration to Abyssinia, the Quraysh attempted to stop the emigration. However, almost all Muslims managed to leave.

The Hijra is the migration of Muhammad and his followers from Mecca to Medina in 622 CE. In June 622, warned of a plot to assassinate him, Muhammad secretly slipped out of Mecca and moved his followers to Medina, north of Mecca.

A delegation, consisting of the representatives of the twelve important clans of Medina, invited Muhammad to serve as chief arbitrator for the entire community; due to his status as a neutral outsider. There was fighting in Yathrib: primarily the dispute involved its Arab and Jewish inhabitants, and was estimated to have lasted for around a hundred years before 620. The recurring slaughters and disagreements over the resulting claims, especially after the Battle of Bu'ath in which all clans were involved, made it obvious to them that the tribal concept of blood-feud and an eye for an eye were no longer workable unless there was one man with authority to adjudicate in disputed cases. The delegation from Medina pledged themselves and their fellow-citizens to accept Muhammad into their community and physically protect him as one of themselves.

Muhammad instructed his followers to emigrate to Medina, until nearly all his followers left Mecca. Being alarmed at the departure, according to tradition, the Meccans plotted to assassinate Muhammad. With the help of Ali, Muhammad fooled the Meccans watching him, and secretly slipped away from the town with Abu Bakr. By 622, Muhammad emigrated to Medina, a large agricultural oasis. Those who migrated from Mecca along with Muhammad became known as "muhajirun" (emigrants).

Among the first things Muhammad did to ease the longstanding grievances among the tribes of Medina was to draft a document known as the Constitution of Medina, "establishing a kind of alliance or federation" among the eight Medinan tribes and Muslim emigrants from Mecca; this specified rights and duties of all citizens, and the relationship of the different communities in Medina (including the Muslim community to other communities, specifically the Jews and other "Peoples of the Book"). The community defined in the Constitution of Medina, "Ummah", had a religious outlook, also shaped by practical considerations and substantially preserved the legal forms of the old Arab tribes.

The first group of converts to Islam in Medina were the clans without great leaders; these clans had been subjugated by hostile leaders from outside. This was followed by the general acceptance of Islam by the pagan population of Medina, with some exceptions. According to Ibn Ishaq, this was influenced by the conversion of Sa'd ibn Mu'adh (a prominent Medinan leader) to Islam. Medinans who converted to Islam and helped the Muslim emigrants find shelter became known as the "ansar" (supporters). Then Muhammad instituted brotherhood between the emigrants and the supporters and he chose Ali as his own brother.

Following the emigration, the people of Mecca seized property of Muslim emigrants to Medina. War would later break out between the people of Mecca and the Muslims. Muhammad delivered Quranic verses permitting Muslims to fight the Meccans (see sura Al-Hajj, Quran ). According to the traditional account, on 11 February 624, while praying in the Masjid al-Qiblatayn in Medina, Muhammad received revelations from God that he should be facing Mecca rather than Jerusalem during prayer. Muhammad adjusted to the new direction, and his companions praying with him followed his lead, beginning the tradition of facing Mecca during prayer.

In March 624, Muhammad led some three hundred warriors in a raid on a Meccan merchant caravan. The Muslims set an ambush for the caravan at Badr. Aware of the plan, the Meccan caravan eluded the Muslims. A Meccan force was sent to protect the caravan and went on to confront the Muslims upon receiving word that the caravan was safe. The Battle of Badr commenced. Though outnumbered more than three to one, the Muslims won the battle, killing at least forty-five Meccans with fourteen Muslims dead. They also succeeded in killing many Meccan leaders, including Abu Jahl. Seventy prisoners had been acquired, many of whom were ransomed. Muhammad and his followers saw the victory as confirmation of their faith and Muhammad ascribed the victory as assisted from an invisible host of angels. The Quranic verses of this period, unlike the Meccan verses, dealt with practical problems of government and issues like the distribution of spoils.

The victory strengthened Muhammad's position in Medina and dispelled earlier doubts among his followers. As a result, the opposition to him became less vocal. Pagans who had not yet converted were very bitter about the advance of Islam. Two pagans, Asma bint Marwan of the Aws Manat tribe and Abu 'Afak of the 'Amr b. 'Awf tribe, had composed verses taunting and insulting the Muslims. They were killed by people belonging to their own or related clans, and Muhammad did not disapprove of the killings. This report, however, is considered by some to be a fabrication. Most members of those tribes converted to Islam, and little pagan opposition remained.

Muhammad expelled from Medina the Banu Qaynuqa, one of three main Jewish tribes, but some historians contend that the expulsion happened after Muhammad's death. According to al-Waqidi, after Abd-Allah ibn Ubaiy spoke for them, Muhammad refrained from executing them and commanded that they be exiled from Medina. Following the Battle of Badr, Muhammad also made mutual-aid alliances with a number of Bedouin tribes to protect his community from attacks from the northern part of Hejaz.

The Meccans were eager to avenge their defeat. To maintain economic prosperity, the Meccans needed to restore their prestige, which had been reduced at Badr. In the ensuing months, the Meccans sent ambush parties to Medina while Muhammad led expeditions against tribes allied with Mecca and sent raiders onto a Meccan caravan. Abu Sufyan gathered an army of 3000 men and set out for an attack on Medina.

A scout alerted Muhammad of the Meccan army's presence and numbers a day later. The next morning, at the Muslim conference of war, a dispute arose over how best to repel the Meccans. Muhammad and many senior figures suggested it would be safer to fight within Medina and take advantage of the heavily fortified strongholds. Younger Muslims argued that the Meccans were destroying crops, and huddling in the strongholds would destroy Muslim prestige. Muhammad eventually conceded to the younger Muslims and readied the Muslim force for battle. Muhammad led his force outside to the mountain of Uhud (the location of the Meccan camp) and fought the Battle of Uhud on 23 March 625. Although the Muslim army had the advantage in early encounters, lack of discipline on the part of strategically placed archers led to a Muslim defeat; 75 Muslims were killed including Hamza, Muhammad's uncle who became one of the best known martyrs in the Muslim tradition. The Meccans did not pursue the Muslims, instead, they marched back to Mecca declaring victory. The announcement is probably because Muhammad was wounded and thought dead. When they discovered that Muhammad lived, the Meccans did not return due to false information about new forces coming to his aid. The attack had failed to achieve their aim of completely destroying the Muslims. The Muslims buried the dead and returned to Medina that evening. Questions accumulated about the reasons for the loss; Muhammad delivered Quranic verses indicating that the defeat was twofold: partly a punishment for disobedience, partly a test for steadfastness.

Abu Sufyan directed his effort towards another attack on Medina. He gained support from the nomadic tribes to the north and east of Medina; using propaganda about Muhammad's weakness, promises of booty, memories of Quraysh prestige and through bribery. Muhammad's new policy was to prevent alliances against him. Whenever alliances against Medina were formed, he sent out expeditions to break them up. Muhammad heard of men massing with hostile intentions against Medina, and reacted in a severe manner. One example is the assassination of Ka'b ibn al-Ashraf, a chieftain of the Jewish tribe of Banu Nadir. Al-Ashraf went to Mecca and wrote poems that roused the Meccans' grief, anger and desire for revenge after the Battle of Badr. Around a year later, Muhammad expelled the Banu Nadir from Medina forcing their emigration to Syria; he allowed them to take some possessions, as he was unable to subdue the Banu Nadir in their strongholds. The rest of their property was claimed by Muhammad in the name of God as it was not gained with bloodshed. Muhammad surprised various Arab tribes, individually, with overwhelming force, causing his enemies to unite to annihilate him. Muhammad's attempts to prevent a confederation against him were unsuccessful, though he was able to increase his own forces and stopped many potential tribes from joining his enemies.

With the help of the exiled Banu Nadir, the Quraysh military leader Abu Sufyan mustered a force of 10,000 men. Muhammad prepared a force of about 3,000 men and adopted a form of defense unknown in Arabia at that time; the Muslims dug a trench wherever Medina lay open to cavalry attack. The idea is credited to a Persian convert to Islam, Salman the Persian. The siege of Medina began on 31 March 627 and lasted two weeks. Abu Sufyan's troops were unprepared for the fortifications, and after an ineffectual siege, the coalition decided to return home. The Quran discusses this battle in sura Al-Ahzab, in verses .
During the battle, the Jewish tribe of Banu Qurayza, located to the south of Medina, entered into negotiations with Meccan forces to revolt against Muhammad. Although the Meccan forces were swayed by suggestions that Muhammad was sure to be overwhelmed, they desired reassurance in case the confederacy was unable to destroy him. No agreement was reached after prolonged negotiations, partly due to sabotage attempts by Muhammad's scouts. After the coalition's retreat, the Muslims accused the Banu Qurayza of treachery and besieged them in their forts for 25 days. The Banu Qurayza eventually surrendered; according to Ibn Ishaq, all the men apart from a few converts to Islam were beheaded, while the women and children were enslaved. Walid N. Arafat and Barakat Ahmad have disputed the accuracy of Ibn Ishaq's narrative. Arafat believes that Ibn Ishaq's Jewish sources, speaking over 100 years after the event, conflated this account with memories of earlier massacres in Jewish history; he notes that Ibn Ishaq was considered an unreliable historian by his contemporary Malik ibn Anas, and a transmitter of "odd tales" by the later Ibn Hajar. Ahmad argues that only some of the tribe were killed, while some of the fighters were merely enslaved. Watt finds Arafat's arguments "not entirely convincing", while Meir J. Kister has contradicted the arguments of Arafat and Ahmad.

In the siege of Medina, the Meccans exerted the available strength to destroy the Muslim community. The failure resulted in a significant loss of prestige; their trade with Syria vanished. Following the Battle of the Trench, Muhammad made two expeditions to the north, both ended without any fighting. While returning from one of these journeys (or some years earlier according to other early accounts), an accusation of adultery was made against Aisha, Muhammad's wife. Aisha was exonerated from accusations when Muhammad announced he had received a revelation confirming Aisha's innocence and directing that charges of adultery be supported by four eyewitnesses (sura 24, An-Nur).

Although Muhammad had delivered Quranic verses commanding the Hajj, the Muslims had not performed it due to Quraysh enmity. In the month of Shawwal 628, Muhammad ordered his followers to obtain sacrificial animals and to prepare for a pilgrimage ("umrah") to Mecca, saying that God had promised him the fulfillment of this goal in a vision when he was shaving his head after completion of the Hajj. Upon hearing of the approaching 1,400 Muslims, the Quraysh dispatched 200 cavalry to halt them. Muhammad evaded them by taking a more difficult route, enabling his followers to reach al-Hudaybiyya just outside Mecca. According to Watt, although Muhammad's decision to make the pilgrimage was based on his dream, he was also demonstrating to the pagan Meccans that Islam did not threaten the prestige of the sanctuaries, that Islam was an Arabian religion. 

Negotiations commenced with emissaries traveling to and from Mecca. While these continued, rumors spread that one of the Muslim negotiators, Uthman bin al-Affan, had been killed by the Quraysh. Muhammad called upon the pilgrims to make a pledge not to flee (or to stick with Muhammad, whatever decision he made) if the situation descended into war with Mecca. This pledge became known as the "Pledge of Acceptance" or the "Pledge under the Tree". News of Uthman's safety allowed for negotiations to continue, and a treaty scheduled to last ten years was eventually signed between the Muslims and Quraysh. The main points of the treaty included: cessation of hostilities, the deferral of Muhammad's pilgrimage to the following year, and agreement to send back any Meccan who emigrated to Medina without permission from their protector.

Many Muslims were not satisfied with the treaty. However, the Quranic sura "Al-Fath" (The Victory) (Quran ) assured them that the expedition must be considered a victorious one. It was later that Muhammad's followers realized the benefit behind the treaty. These benefits included the requirement of the Meccans to identify Muhammad as an equal, cessation of military activity allowing Medina to gain strength, and the admiration of Meccans who were impressed by the pilgrimage rituals.

After signing the truce, Muhammad assembled an expedition against the Jewish oasis of Khaybar, known as the Battle of Khaybar. This was possibly due to housing the Banu Nadir who were inciting hostilities against Muhammad, or to regain prestige from what appeared as the inconclusive result of the truce of Hudaybiyya. According to Muslim tradition, Muhammad also sent letters to many rulers, asking them to convert to Islam (the exact date is given variously in the sources). He sent messengers (with letters) to Heraclius of the Byzantine Empire (the eastern Roman Empire), Khosrau of Persia, the chief of Yemen and to some others. In the years following the truce of Hudaybiyya, Muhammad directed his forces against the Arabs on Transjordanian Byzantine soil in the Battle of Mu'tah.

The truce of Hudaybiyyah was enforced for two years. The tribe of Banu Khuza'a had good relations with Muhammad, whereas their enemies, the Banu Bakr, had allied with the Meccans. A clan of the Bakr made a night raid against the Khuza'a, killing a few of them. The Meccans helped the Banu Bakr with weapons and, according to some sources, a few Meccans also took part in the fighting. After this event, Muhammad sent a message to Mecca with three conditions, asking them to accept one of them. These were: either the Meccans would pay blood money for the slain among the Khuza'ah tribe, they disavow themselves of the Banu Bakr, or they should declare the truce of Hudaybiyyah null.

The Meccans replied that they accepted the last condition. Soon they realized their mistake and sent Abu Sufyan to renew the Hudaybiyyah treaty, a request that was declined by Muhammad.

Muhammad began to prepare for a campaign. In 630, Muhammad marched on Mecca with 10,000 Muslim converts. With minimal casualties, Muhammad seized control of Mecca. He declared an amnesty for past offences, except for ten men and women who were "guilty of murder or other offences or had sparked off the war and disrupted the peace". Some of these were later pardoned. Most Meccans converted to Islam and Muhammad proceeded to destroy all the statues of Arabian gods in and around the Kaaba. According to reports collected by Ibn Ishaq and al-Azraqi, Muhammad personally spared paintings or frescos of Mary and Jesus, but other traditions suggest that all pictures were erased. The Quran discusses the conquest of Mecca.

Following the conquest of Mecca, Muhammad was alarmed by a military threat from the confederate tribes of Hawazin who were raising an army double the size of Muhammad's. The Banu Hawazin were old enemies of the Meccans. They were joined by the Banu Thaqif (inhabiting the city of Ta'if) who adopted an anti-Meccan policy due to the decline of the prestige of Meccans. Muhammad defeated the Hawazin and Thaqif tribes in the Battle of Hunayn.

In the same year, Muhammad organized an attack against northern Arabia because of their previous defeat at the Battle of Mu'tah and reports of hostility adopted against Muslims. With great difficulty he assembled 30,000 men; half of whom on the second day returned with Abd-Allah ibn Ubayy, untroubled by the damning verses which Muhammad hurled at them. Although Muhammad did not engage with hostile forces at Tabuk, he received the submission of some local chiefs of the region.

He also ordered the destruction of any remaining pagan idols in Eastern Arabia. The last city to hold out against the Muslims in Western Arabia was Taif. Muhammad refused to accept the city's surrender until they agreed to convert to Islam and allowed men to destroy the statue of their goddess Al-Lat.

A year after the Battle of Tabuk, the Banu Thaqif sent emissaries to surrender to Muhammad and adopt Islam. Many bedouins submitted to Muhammad to safeguard against his attacks and to benefit from the spoils of war. However, the bedouins were alien to the system of Islam and wanted to maintain independence: namely their code of virtue and ancestral traditions. Muhammad required a military and political agreement according to which they "acknowledge the suzerainty of Medina, to refrain from attack on the Muslims and their allies, and to pay the Zakat, the Muslim religious levy."

In 632, at the end of the tenth year after migration to Medina, Muhammad completed his first true Islamic pilgrimage, setting precedent for the annual Great Pilgrimage, known as "Hajj". On the 9th of Dhu al-Hijjah Muhammad delivered his Farewell Sermon, at Mount Arafat east of Mecca. In this sermon, Muhammad advised his followers not to follow certain pre-Islamic customs. For instance, he said a white has no superiority over a black, nor a black any superiority over a white except by piety and good action. He abolished old blood feuds and disputes based on the former tribal system and asked for old pledges to be returned as implications of the creation of the new Islamic community. Commenting on the vulnerability of women in his society, Muhammad asked his male followers to "be good to women, for they are powerless captives ("awan") in your households. You took them in God's trust, and legitimated your sexual relations with the Word of God, so come to your senses people, and hear my words ..." He told them that they were entitled to discipline their wives but should do so with kindness. He addressed the issue of inheritance by forbidding false claims of paternity or of a client relationship to the deceased and forbade his followers to leave their wealth to a testamentary heir. He also upheld the sacredness of four lunar months in each year. According to Sunni tafsir, the following Quranic verse was delivered during this event: "Today I have perfected your religion, and completed my favours for you and chosen Islam as a religion for you" (Quran ). According to Shia tafsir, it refers to the appointment of Ali ibn Abi Talib at the pond of Khumm as Muhammad's successor, this occurring a few days later when Muslims were returning from Mecca to Medina.

A few months after the farewell pilgrimage, Muhammad fell ill and suffered for several days with fever, head pain, and weakness. He died on Monday, 8 June 632, in Medina, at the age of 62 or 63, in the house of his wife Aisha. With his head resting on Aisha's lap, he asked her to dispose of his last worldly goods (seven coins), then spoke his final words: 

According to the "Encyclopaedia of Islam", Muhammad's death may be presumed to have been caused by Medinan fever exacerbated by physical and mental fatigue.

Academics Reşit Haylamaz and Fatih Harpci say that "Ar-Rafiq Al-A'la" is referring to God. He was buried where he died in Aisha's house. During the reign of the Umayyad caliph al-Walid I, al-Masjid an-Nabawi (the Mosque of the Prophet) was expanded to include the site of Muhammad's tomb. The Green Dome above the tomb was built by the Mamluk sultan Al Mansur Qalawun in the 13th century, although the green color was added in the 16th century, under the reign of Ottoman sultan Suleiman the Magnificent. Among tombs adjacent to that of Muhammad are those of his companions (Sahabah), the first two Muslim caliphs Abu Bakr and Umar, and an empty one that Muslims believe awaits Jesus.
When Saud bin Abdul-Aziz took Medina in 1805, Muhammad's tomb was stripped of its gold and jewel ornamentation. Adherents to Wahhabism, Saud's followers, destroyed nearly every tomb dome in Medina in order to prevent their veneration, and the one of Muhammad is reported to have narrowly escaped. Similar events took place in 1925, when the Saudi militias retook—and this time managed to keep—the city. In the Wahhabi interpretation of Islam, burial is to take place in unmarked graves. Although frowned upon by the Saudis, many pilgrims continue to practice a ziyarat—a ritual visit—to the tomb.

Muhammad united several of the tribes of Arabia into a single Arab Muslim religious polity in the last years of his life. With Muhammad's death, disagreement broke out over who his successor would be. Umar ibn al-Khattab, a prominent companion of Muhammad, nominated Abu Bakr, Muhammad's friend and collaborator. With additional support Abu Bakr was confirmed as the first caliph. This choice was disputed by some of Muhammad's companions, who held that Ali ibn Abi Talib, his cousin and son-in-law, had been designated the successor by Muhammad at Ghadir Khumm. Abu Bakr immediately moved to strike against the Byzantine (or Eastern Roman Empire) forces because of the previous defeat, although he first had to put down a rebellion by Arab tribes in an event that Muslim historians later referred to as the Ridda wars, or "Wars of Apostasy".

The pre-Islamic Middle East was dominated by the Byzantine and Sassanian empires. The Roman–Persian Wars between the two had devastated the region, making the empires unpopular amongst local tribes. Furthermore, in the lands that would be conquered by Muslims many Christians (Nestorians, Monophysites, Jacobites and Copts) were disaffected from the Eastern Orthodox Church which deemed them heretics. Within a decade Muslims conquered Mesopotamia, Byzantine Syria, Byzantine Egypt, large parts of Persia, and established the Rashidun Caliphate.

According to William Montgomery Watt, religion for Muhammad was not a private and individual matter but "the total response of his personality to the total situation in which he found himself. He was responding [not only]... to the religious and intellectual aspects of the situation but also to the economic, social, and political pressures to which contemporary Mecca was subject." Bernard Lewis says there are two important political traditions in Islam—Muhammad as a statesman in Medina, and Muhammad as a rebel in Mecca. In his view, Islam is a great change, akin to a revolution, when introduced to new societies.

Historians generally agree that Islamic social changes in areas such as social security, family structure, slavery and the rights of women and children improved on the "status quo" of Arab society. For example, according to Lewis, Islam "from the first denounced aristocratic privilege, rejected hierarchy, and adopted a formula of the career open to the talents". Muhammad's message transformed society and moral orders of life in the Arabian Peninsula; society focused on the changes to perceived identity, world view, and the hierarchy of values.
Economic reforms addressed the plight of the poor, which was becoming an issue in pre-Islamic Mecca. The Quran requires payment of an alms tax (zakat) for the benefit of the poor; as Muhammad's power grew he demanded that tribes who wished to ally with him implement the zakat in particular.

In Muhammad al-Bukhari's book Sahih al-Bukhari, in Chapter 61, Hadith 57 & Hadith 60, Muhammad is depicted by two of his companions thus:

The description given in Muhammad ibn Isa at-Tirmidhi's book Shama'il al-Mustafa, attributed to Ali ibn Abi Talib and Hind ibn Abi Hala is as follows:

The "seal of prophecy" between Muhammad's shoulders is generally described as having been a type of raised mole the size of a pigeon's egg. Another description of Muhammad was provided by Umm Ma'bad, a woman he met on his journey to Medina:

Descriptions like these were often reproduced in calligraphic panels ("hilya" or, in Turkish, "hilye"), which in the 17th century developed into an art form of their own in the Ottoman Empire.

Muhammad's life is traditionally defined into two periods: pre-hijra (emigration) in Mecca (from 570 to 622), and post-hijra in Medina (from 622 until 632). Muhammad is said to have had thirteen wives in total (although two have ambiguous accounts, Rayhana bint Zayd and Maria al-Qibtiyya, as wife or concubine). Eleven of the thirteen marriages occurred after the migration to Medina.

At the age of 25, Muhammad married the wealthy Khadijah bint Khuwaylid who was 40 years old. The marriage lasted for 25 years and was a happy one. Muhammad did not enter into marriage with another woman during this marriage. After Khadijah's death, Khawla bint Hakim suggested to Muhammad that he should marry Sawda bint Zama, a Muslim widow, or Aisha, daughter of Um Ruman and Abu Bakr of Mecca. Muhammad is said to have asked for arrangements to marry both. Muhammad's marriages after the death of Khadijah were contracted mostly for political or humanitarian reasons. The women were either widows of Muslims killed in battle and had been left without a protector, or belonged to important families or clans with whom it was necessary to honor and strengthen alliances.

According to traditional sources Aisha was six or seven years old when betrothed to Muhammad, with the marriage not being consummated until she had reached puberty at the age of nine or ten years old. She was therefore a virgin at marriage. Modern Muslim authors who calculate Aisha's age based on other sources of information, such as a hadith about the age difference between Aisha and her sister Asma, estimate that she was over thirteen and perhaps in her late teens at the time of her marriage.

After migration to Medina, Muhammad, who was then in his fifties, married several more women.

Muhammad performed household chores such as preparing food, sewing clothes, and repairing shoes. He is also said to have had accustomed his wives to dialogue; he listened to their advice, and the wives debated and even argued with him.

Khadijah is said to have had four daughters with Muhammad (Ruqayyah bint Muhammad, Umm Kulthum bint Muhammad, Zainab bint Muhammad, Fatimah Zahra) and two sons (Abd-Allah ibn Muhammad and Qasim ibn Muhammad, who both died in childhood). All but one of his daughters, Fatimah, died before him. Some Shi'a scholars contend that Fatimah was Muhammad's only daughter. Maria al-Qibtiyya bore him a son named Ibrahim ibn Muhammad, but the child died when he was two years old.

Nine of Muhammad's wives survived him. Aisha, who became known as Muhammad's favourite wife in Sunni tradition, survived him by decades and was instrumental in helping assemble the scattered sayings of Muhammad that form the Hadith literature for the Sunni branch of Islam.

Muhammad's descendants through Fatimah are known as "sharifs", "syeds" or "sayyids". These are honorific titles in Arabic, "sharif" meaning 'noble' and "sayed" or "sayyid" meaning 'lord' or 'sir'. As Muhammad's only descendants, they are respected by both Sunni and Shi'a, though the Shi'a place much more emphasis and value on their distinction.

Zayd ibn Haritha was a slave that Muhammad bought, freed, and then adopted as his son. He also had a wetnurse. According to a BBC summary, "the Prophet Muhammad did not try to abolish slavery, and bought, sold, captured, and owned slaves himself. But he insisted that slave owners treat their slaves well and stressed the virtue of freeing slaves. Muhammad treated slaves as human beings and clearly held some in the highest esteem".

Following the attestation to the oneness of God, the belief in Muhammad's prophethood is the main aspect of the Islamic faith. Every Muslim proclaims in "Shahadah": "I testify that there is no god but God, and I testify that Muhammad is a Messenger of God." The Shahadah is the basic creed or tenet of Islam. Islamic belief is that ideally the Shahadah is the first words a newborn will hear; children are taught it immediately and it will be recited upon death. Muslims repeat the shahadah in the call to prayer ("adhan") and the prayer itself. Non-Muslims wishing to convert to Islam are required to recite the creed.

In Islamic belief, Muhammad is regarded as the last prophet sent by God. states that "...it (the Quran) is a confirmation of (revelations) that went before it, and a fuller explanation of the Book—wherein there is no doubt—from The Lord of the Worlds.". Similarly states "...And before this was the book of Moses, as a guide and a mercy. And this Book confirms (it)...", while commands the believers of Islam to "Say: we believe in God and that which is revealed unto us, and that which was revealed unto Abraham and Ishmael and Isaac and Jacob and the tribes, and that which Moses and Jesus received, and which the prophets received from their Lord. We make no distinction between any of them, and unto Him we have surrendered."

Muslim tradition credits Muhammad with several miracles or supernatural events. For example, many Muslim commentators and some Western scholars have interpreted the Surah as referring to Muhammad splitting the Moon in view of the Quraysh when they began persecuting his followers. Western historian of Islam Denis Gril believes the Quran does not overtly describe Muhammad performing miracles, and the supreme miracle of Muhammad is identified with the Quran itself.

According to Islamic tradition, Muhammad was attacked by the people of Ta'if and was badly injured. The tradition also describes an angel appearing to him and offering retribution against the assailants. It is said that Muhammad rejected the offer and prayed for the guidance of the people of Ta'if.

The Sunnah represents actions and sayings of Muhammad (preserved in reports known as Hadith), and covers a broad array of activities and beliefs ranging from religious rituals, personal hygiene, and burial of the dead to the mystical questions involving the love between humans and God. The Sunnah is considered a model of emulation for pious Muslims and has to a great degree influenced the Muslim culture. The greeting that Muhammad taught Muslims to offer each other, "may peace be upon you" (Arabic: "as-salamu 'alaykum") is used by Muslims throughout the world. Many details of major Islamic rituals such as daily prayers, the fasting and the annual pilgrimage are only found in the Sunnah and not the Quran.

The Sunnah contributed much to the development of Islamic law, particularly from the end of the first Islamic century. Muslim mystics, known as sufis, who were seeking for the inner meaning of the Quran and the inner nature of Muhammad, viewed the prophet of Islam not only as a prophet but also as a perfect human being. All Sufi orders trace their chain of spiritual descent back to Muhammad.

Muslims have traditionally expressed love and veneration for Muhammad. Stories of Muhammad's life, his intercession and of his miracles (particularly "Splitting of the moon") have permeated popular Muslim thought and poetry. Among Arabic odes to Muhammad, Qasidat al-Burda ("Poem of the Mantle") by the Egyptian Sufi al-Busiri (1211–1294) is particularly well-known, and widely held to possess a healing, spiritual power. The Quran refers to Muhammad as "a mercy ("rahmat") to the worlds" (Quran ). The association of rain with mercy in Oriental countries has led to imagining Muhammad as a rain cloud dispensing blessings and stretching over lands, reviving the dead hearts, just as rain revives the seemingly dead earth (see, for example, the Sindhi poem of Shah ʿAbd al-Latif). Muhammad's birthday is celebrated as a major feast throughout the Islamic world, excluding Wahhabi-dominated Saudi Arabia where these public celebrations are discouraged. When Muslims say or write the name of Muhammad, they usually follow it with the Arabic phrase "ṣallā llahu ʿalayhi wa-sallam" ("may God honor him and grant him peace") or the English phrase "peace be upon him". In casual writing, the abbreviations SAW (for the Arabic phrase) or PBUH (for the English phrase) are sometimes used; in printed matter, a small calligraphic rendition is commonly used ().

In line with the hadith's prohibition against creating images of sentient living beings, which is particularly strictly observed with respect to God and Muhammad, Islamic religious art is focused on the word. Muslims generally avoid depictions of Muhammad, and mosques are decorated with calligraphy and Quranic inscriptions or geometrical designs, not images or sculptures. Today, the interdiction against images of Muhammad—designed to prevent worship of Muhammad, rather than God—is much more strictly observed in Sunni Islam (85%–90% of Muslims) and Ahmadiyya Islam (1%) than among Shias (10%–15%). While both Sunnis and Shias have created images of Muhammad in the past, Islamic depictions of Muhammad are rare. They have mostly been limited to the private and elite medium of the miniature, and since about 1500 most depictions show Muhammad with his face veiled, or symbolically represent him as a flame.

The earliest extant depictions come from 13th century Anatolian Seljuk and Ilkhanid Persian miniatures, typically in literary genres describing the life and deeds of Muhammad. During the Ilkhanid period, when Persia's Mongol rulers converted to Islam, competing Sunni and Shi'a groups used visual imagery, including images of Muhammad, to promote their particular interpretation of Islam's key events. Influenced by the Buddhist tradition of representational religious art predating the Mongol elite's conversion, this innovation was unprecedented in the Islamic world, and accompanied by a "broader shift in Islamic artistic culture away from abstraction toward representation" in "mosques, on tapestries, silks, ceramics, and in glass and metalwork" besides books. In the Persian lands, this tradition of realistic depictions lasted through the Timurid dynasty until the Safavids took power in the early 16th century. The Safavaids, who made Shi'i Islam the state religion, initiated a departure from the traditional Ilkhanid and Timurid artistic style by covering Muhammad's face with a veil to obscure his features and at the same time represent his luminous essence. Concomitantly, some of the unveiled images from earlier periods were defaced. Later images were produced in Ottoman Turkey and elsewhere, but mosques were never decorated with images of Muhammad. Illustrated accounts of the night journey ("mi'raj") were particularly popular from the Ilkhanid period through the Safavid era. During the 19th century, Iran saw a boom of printed and illustrated "mi'raj" books, with Muhammad's face veiled, aimed in particular at illiterates and children in the manner of graphic novels. Reproduced through lithography, these were essentially "printed manuscripts". Today, millions of historical reproductions and modern images are available in some Muslim-majority countries, especially Turkey and Iran, on posters, postcards, and even in coffee-table books, but are unknown in most other parts of the Islamic world, and when encountered by Muslims from other countries, they can cause considerable consternation and offense.

The earliest documented Christian knowledge of Muhammad stems from Byzantine sources. They indicate that both Jews and Christians saw Muhammad as a false prophet. Another Greek source for Muhammad is Theophanes the Confessor, a 9th-century writer. The earliest Syriac source is the 7th-century writer John bar Penkaye.

According to Hossein Nasr, the earliest European literature often refers to Muhammad unfavorably. A few learned circles of Middle Ages Europeprimarily Latin-literate scholarshad access to fairly extensive biographical material about Muhammad. They interpreted the biography through a Christian religious filter, one that viewed Muhammad as a person who seduced the Saracens into his submission under religious guise. Popular European literature of the time portrayed Muhammad as though he were worshipped by Muslims, similar to an idol or a heathen god.

In later ages, Muhammad came to be seen as a schismatic: Brunetto Latini's 13th century "Li livres dou tresor" represents him as a former monk and cardinal, and Dante's "Divine Comedy" (Inferno, Canto 28), written in the early 1300s, puts Muhammad and his son-in-law, Ali, in Hell "among the sowers of discord and the schismatics, being lacerated by devils again and again."

After the Reformation, Muhammad was often portrayed in a similar way. Guillaume Postel was among the first to present a more positive view of Muhammad when he argued that Muhammad should be esteemed by Christians as a valid prophet. Gottfried Leibniz praised Muhammad because "he did not deviate from the natural religion". Henri de Boulainvilliers, in his "Vie de Mahomed" which was published posthumously in 1730, described Muhammad as a gifted political leader and a just lawmaker. He presents him as a divinely inspired messenger whom God employed to confound the bickering Oriental Christians, to liberate the Orient from the despotic rule of the Romans and Persians, and to spread the knowledge of the unity of God from India to Spain. Voltaire had a somewhat mixed opinion on Muhammad: in his play "Le fanatisme, ou Mahomet le Prophète" he vilifies Muhammad as a symbol of fanaticism, and in a published essay in 1748 he calls him "a sublime and hearty charlatan", but in his historical survey "Essai sur les mœurs", he presents him as legislator and a conqueror and calls him an "enthusiast." Jean-Jacques Rousseau, in his "Social Contract" (1762), "brushing aside hostile legends of Muhammad as a trickster and impostor, presents him as a sage legislator who wisely fused religious and political powers." Emmanuel Pastoret published in 1787 his "Zoroaster, Confucius and Muhammad", in which he presents the lives of these three "great men", "the greatest legislators of the universe", and compares their careers as religious reformers and lawgivers. He rejects the common view that Muhammad is an impostor and argues that the Quran proffers "the most sublime truths of cult and morals"; it defines the unity of God with an "admirable concision." Pastoret writes that the common accusations of his immorality are unfounded: on the contrary, his law enjoins sobriety, generosity, and compassion on his followers: the "legislator of Arabia" was "a great man." Napoleon Bonaparte admired Muhammad and Islam, and described him as a model lawmaker and a great man. Thomas Carlyle in his book "Heroes and Hero Worship and the Heroic in History" (1840) describes Muhammad as "[a] silent great soul; [...] one of those who cannot "but" be in earnest". Carlyle's interpretation has been widely cited by Muslim scholars as a demonstration that Western scholarship validates Muhammad's status as a great man in history.

Ian Almond says that German Romantic writers generally held positive views of Muhammad: "Goethe’s 'extraordinary' poet-prophet, Herder’s nation builder (...) Schlegel’s admiration for Islam as an aesthetic product, enviably authentic, radiantly holistic, played such a central role in his view of Mohammed as an exemplary world-fashioner that he even used it as a scale of judgement for the classical (the dithyramb, we are told, has to radiate pure beauty if it is to resemble 'a Koran of poetry')." After quoting Heinrich Heine, who said in a letter to some friend that "I must admit that you, great prophet of Mecca, are the greatest poet and that your Quran... will not easily escape my memory", John Tolan goes on to show how Jews in Europe in particular held more nuanced views about Muhammad and Islam, being an ethnoreligious minority feeling discriminated, they specifically lauded Al-Andalus, and thus, "writing about Islam was for Jews a way of indulging in a fantasy world, far from the persecution and pogroms of nineteenth-century Europe, where Jews could live in harmony with their non-Jewish neighbors."

Recent writers such as William Montgomery Watt and Richard Bell dismiss the idea that Muhammad deliberately deceived his followers, arguing that Muhammad "was absolutely sincere and acted in complete good faith" and Muhammad's readiness to endure hardship for his cause, with what seemed to be no rational basis for hope, shows his sincerity. Watt, however, says that sincerity does not directly imply correctness: in contemporary terms, Muhammad might have mistaken his subconscious for divine revelation. Watt and Bernard Lewis argue that viewing Muhammad as a self-seeking impostor makes it impossible to understand Islam's development. Alford T. Welch holds that Muhammad was able to be so influential and successful because of his firm belief in his vocation.

Bahá'ís venerate Muhammad as one of a number of prophets or "Manifestations of God". He is thought to be the final manifestation, or seal of the Adamic cycle, but consider his teachings to have been superseded by those of Bahá'u'lláh, the founder of the Bahai faith, and the first Manifestation of the current cycle.

Criticism of Muhammad has existed since the 7th century. According to "The Jewish Encyclopedia" (1906), Muhammad was decried by his non-Muslim Arab contemporaries for preaching monotheism, and by the Jewish tribes of Arabia for his unwarranted appropriation of Biblical narratives and figures and attacks on the Jewish faith. According to modern critics Norman Stillman (1979), Andrew G. Bostom (2008) and Ibn Warraq (pseudonym, 2007), Muhammad was also criticised for proclaiming himself as "the last prophet" without performing any miracle nor showing any personal requirement demanded in the Hebrew Bible to distinguish a true prophet chosen by the God of Israel from a false claimant; for these reasons, they gave him the derogatory nickname "ha-Meshuggah" (, "the Madman" or "the Possessed").

During the Dark and Middle Ages various Western and Byzantine Christian thinkers considered Muhammad to be a perverted, deplorable man, a false prophet, and even the Antichrist, as he was frequently seen in Christendom as a heretic or possessed by the demons. Some of them, like Thomas Aquinas, criticised Muhammad's promises of carnal pleasure in the afterlife.

Modern religious and secular criticism of Islam has questioned Muhammad's sincerity in claiming to be a prophet, his morality, his ownership of slaves, his treatment of enemies, his polygynous marriages and his treatment of doctrinal matters. Muhammad has been accused of mercilessness (such as during the invasion of the tribe in Medina) and his marriage to Aisha, whose age at marriage has been variously reported as between six and nineteen years old.




</doc>
<doc id="18935" url="https://en.wikipedia.org/wiki?curid=18935" title="Morse code">
Morse code

Morse code is a method used in telecommunication to encode text characters as standardized sequences of two different signal durations, called "dots" and "dashes" or "dits" and "dahs". Morse code is named for Samuel F. B. Morse, an inventor of the telegraph.

The International Morse Code encodes the 26 English letters A through Z, some non-English letters, the Arabic numerals and a small set of punctuation and procedural signals (prosigns). There is no distinction between upper and lower case letters. Each Morse code symbol is formed by a sequence of dots and dashes. The dot duration is the basic unit of time measurement in Morse code transmission. The duration of a dash is three times the duration of a dot. Each dot or dash within a character is followed by period of signal absence, called a "space", equal to the dot duration. The letters of a word are separated by a space of duration equal to three dots, and the words are separated by a space equal to seven dots. To increase the efficiency of encoding, Morse code was designed so that the length of each symbol is approximately inverse to the frequency of occurrence in text of the English language character that it represents. Thus the most common letter in English, the letter "E", has the shortest code: a single dot. Because the Morse code elements are specified by proportion rather than specific time durations, the code is usually transmitted at the highest rate that the receiver is capable of decoding. The Morse code transmission rate ("speed") is specified in "groups per minute", commonly referred to as "words per minute".

Morse code is usually transmitted by on-off keying of an information-carrying medium such as electric current, radio waves, visible light, or sound waves. The current or wave is present during the time period of the dot or dash and absent during the time between dots and dashes.

Morse code can be memorized, and Morse code signalling in a form perceptible to the human senses, such as sound waves or visible light, can be directly interpreted by persons trained in the skill.

Because many non-English natural languages use other than the 26 Roman letters, Morse alphabets have been developed for those languages.
In an emergency, Morse code can be generated by improvised methods such as turning a light on and off, tapping on an object or sounding a horn or whistle, making it one of the simplest and most versatile methods of telecommunication. The most common distress signal is SOS – three dots, three dashes, and three dots – internationally recognized by treaty.

Early in the nineteenth century, European experimenters made progress with electrical signaling systems, using a variety of techniques including static electricity and electricity from Voltaic piles producing electrochemical and electromagnetic changes. These numerous ingenious experimental designs were precursors to practical telegraphic applications.

Following the discovery of electromagnetism by Hans Christian Ørsted in 1820 and the invention of the electromagnet by William Sturgeon in 1824, there were developments in electromagnetic telegraphy in Europe and America. Pulses of electric current were sent along wires to control an electromagnet in the receiving instrument. Many of the earliest telegraph systems used a single-needle system which gave a very simple and robust instrument. However, it was slow, as the receiving operator had to alternate between looking at the needle and writing down the message. In Morse code, a deflection of the needle to the left corresponded to a dot and a deflection to the right to a dash. By making the two clicks sound different with one ivory and one metal stop, the single needle device became an audible instrument, which led in turn to the Double Plate Sounder System.
The American artist Samuel F. B. Morse, the American physicist Joseph Henry, and Alfred Vail developed an electrical telegraph system. It needed a method to transmit natural language using only electrical pulses and the silence between them. Around 1837, Morse, therefore, developed an early forerunner to the modern International Morse code. William Cooke and Charles Wheatstone in Britain developed an electrical telegraph that used electromagnets in its receivers. They obtained an English patent in June 1837 and demonstrated it on the London and Birmingham Railway, making it the first commercial telegraph. Carl Friedrich Gauss and Wilhelm Eduard Weber (1833) as well as Carl August von Steinheil (1837) used codes with varying word lengths for their telegraphs. In 1841, Cooke and Wheatstone built a telegraph that printed the letters from a wheel of typefaces struck by a hammer.

The Morse system for telegraphy, which was first used in about 1844, was designed to make indentations on a paper tape when electric currents were received. Morse's original telegraph receiver used a mechanical clockwork to move a paper tape. When an electrical current was received, an electromagnet engaged an armature that pushed a stylus onto the moving paper tape, making an indentation on the tape. When the current was interrupted, a spring retracted the stylus and that portion of the moving tape remained unmarked. Morse code was developed so that operators could translate the indentations marked on the paper tape into text messages. In his earliest code, Morse had planned to transmit only numerals and to use a codebook to look up each word according to the number which had been sent. However, the code was soon expanded by Alfred Vail in 1840 to include letters and special characters so it could be used more generally. Vail estimated the frequency of use of letters in the English language by counting the movable type he found in the type-cases of a local newspaper in Morristown. The shorter marks were called "dots" and the longer ones "dashes", and the letters most commonly used were assigned the shorter sequences of dots and dashes. This code was used since 1844 and became known as "Morse landline code" or "American Morse code".

In the original Morse telegraphs, the receiver's armature made a clicking noise as it moved in and out of position to mark the paper tape. The telegraph operators soon learned that they could translate the clicks directly into dots and dashes, and write these down by hand, thus making the paper tape unnecessary. When Morse code was adapted to radio communication, the dots and dashes were sent as short and long tone pulses. It was later found that people become more proficient at receiving Morse code when it is taught as a language that is heard, instead of one read from a page.

To reflect the sounds of Morse code receivers, the operators began to vocalize a dot as "dit", and a dash as "dah". Dots which are not the final element of a character became vocalized as "di". For example, the letter "c" was then vocalized as "dah-di-dah-dit". Morse code was sometimes facetiously known as "iddy-umpty" and a dash as "umpty", leading to the word "umpteen".

The Morse code, as it is used internationally today, was derived from a much refined proposal by Friedrich Clemens Gerke in 1848 that became known as the "Hamburg alphabet". Gerke changed many of the codepoints, in the process doing away with the different length dashes and different inter-element spaces of American Morse, leaving only two coding elements, the dot and the dash. Codes for German umlauted vowels and "ch" were introduced. Gerke's code was adopted by the Deutsch-Österreichischer Telegraphenverein (German-Austrian Telegraph Society) in 1851. This finally led to the International Morse code in 1865. The International Morse code adopted most of Gerke's codepoints. The codepoints for "O" and "P" were taken from Steinheil's code. A new codepoint was added for "J" since Gerke did not distinguish between "I" and "J". Changes were also made to "Q", "X", "Y", "Z". This left only four codepoints identical to the original Morse code, namely "E", "H", "K" and "N", and the latter two have had their dashes lengthened. The original code being compared dates to 1838, not the code shown in the table which was developed in the 1840s.

In the 1890s, Morse code began to be used extensively for early radio communication before it was possible to transmit voice. In the late 19th and early 20th centuries, most high-speed international communication used Morse code on telegraph lines, undersea cables and radio circuits. In aviation, Morse code in radio systems started to be used on a regular basis in the 1920s. Although previous transmitters were bulky and the spark gap system of transmission was difficult to use, there had been some earlier attempts. In 1910, the US Navy experimented with sending Morse from an airplane. That same year, a radio on the airship "America" had been instrumental in coordinating the rescue of its crew. Zeppelin airships equipped with radio were used for bombing and naval scouting during World War I, and ground-based radio direction finders were used for airship navigation. Allied airships and military aircraft also made some use of radiotelegraphy. However, there was little aeronautical radio in general use during World War I, and in the 1920s, there was no radio system used by such important flights as that of Charles Lindbergh from New York to Paris in 1927. Once he and the "Spirit of St. Louis" were off the ground, Lindbergh was truly alone and incommunicado. On the other hand, when the first airplane flight was made from California to Australia in 1928 on the "Southern Cross", one of its four crewmen was its radio operator who communicated with ground stations via radio telegraph.

Beginning in the 1930s, both civilian and military pilots were required to be able to use Morse code, both for use with early communications systems and for identification of navigational beacons which transmitted continuous two- or three-letter identifiers in Morse code. Aeronautical charts show the identifier of each navigational aid next to its location on the map.

Radiotelegraphy using Morse code was vital during World War II, especially in carrying messages between the warships and the naval bases of the belligerents. Long-range ship-to-ship communication was by radio telegraphy, using encrypted messages because the voice radio systems on ships then were quite limited in both their range and their security. Radiotelegraphy was also extensively used by warplanes, especially by long-range patrol planes that were sent out by those navies to scout for enemy warships, cargo ships, and troop ships.

In addition, rapidly moving armies in the field could not have fought effectively without radiotelegraphy because they moved more rapidly than telegraph and telephone lines could be erected. This was seen especially in the blitzkrieg offensives of the Nazi German Wehrmacht in Poland, Belgium, France (in 1940), the Soviet Union, and in North Africa; by the British Army in North Africa, Italy, and the Netherlands; and by the U.S. Army in France and Belgium (in 1944), and in southern Germany in 1945.
Morse code was used as an international standard for maritime distress until 1999 when it was replaced by the Global Maritime Distress Safety System. When the French Navy ceased using Morse code on January 31, 1997, the final message transmitted was "Calling all. This is our last cry before our eternal silence." In the United States the final commercial Morse code transmission was on July 12, 1999, signing off with Samuel Morse's original 1844 message, "What hath God wrought", and the prosign "SK".

As of 2015, the United States Air Force still trains ten people a year in Morse. The United States Coast Guard has ceased all use of Morse code on the radio, and no longer monitors any radio frequencies for Morse code transmissions, including the international medium frequency (MF) distress frequency of 500 kHz. However, the Federal Communications Commission still grants commercial radiotelegraph operator licenses to applicants who pass its code and written tests. Licensees have reactivated the old California coastal Morse station KPH and regularly transmit from the site under either this Call sign or as KSM. Similarly, a few US Museum ship stations are operated by Morse enthusiasts.

Morse code speed is measured in words per minute (wpm) or characters per minute (cpm). Characters have differing lengths because they contain differing numbers of dots and dashes. Consequently, words also have different lengths in terms of dot duration, even when they contain the same number of characters. For this reason, a standard word is helpful to measure operator transmission speed. "PARIS" and "CODEX" are two such standard words. Operators skilled in Morse code can often understand ("copy") code in their heads at rates in excess of 40 wpm.

In addition to knowing, understanding, and being able to copy the standard written alpha-numeric and punctuation characters or symbols at high speeds, skilled high speed operators must also be fully knowledgeable of all of the special unwritten Morse code symbols for the standard Prosigns for Morse code and the meanings of these special procedural signals in standard Morse code communications protocol.

International contests in code copying are still occasionally held. In July 1939 at a contest in Asheville, North Carolina in the United States Ted R. McElroy W1JYN set a still-standing record for Morse copying, 75.2 wpm. William Pierpont N0HFF also notes that some operators may have passed 100 wpm. By this time, they are "hearing" phrases and sentences rather than words. The fastest speed ever sent by a straight key was achieved in 1942 by Harry Turner W9YZE (d. 1992) who reached 35 wpm in a demonstration at a U.S. Army base. To accurately compare code copying speed records of different eras it is useful to keep in mind that different standard words (50 dot durations versus 60 dot durations) and different interword gaps (5 dot durations versus 7 dot durations) may have been used when determining such speed records. For example, speeds run with the CODEX standard word and the PARIS standard may differ by up to 20%.

Today among amateur operators there are several organizations that recognize high-speed code ability, one group consisting of those who can copy Morse at 60 wpm. Also, Certificates of Code Proficiency are issued by several amateur radio societies, including the American Radio Relay League. Their basic award starts at 10 wpm with endorsements as high as 40 wpm, and are available to anyone who can copy the transmitted text. Members of the Boy Scouts of America may put a Morse interpreter's strip on their uniforms if they meet the standards for translating code at 5 wpm.

Morse code has been in use for more than 160 years—longer than any other electrical coding system. What is called Morse code today is actually somewhat different from what was originally developed by Vail and Morse. The Modern International Morse code, or "continental code", was created by Friedrich Clemens Gerke in 1848 and initially used for telegraphy between Hamburg and Cuxhaven in Germany. Gerke changed nearly half of the alphabet and all of the numerals, providing the foundation for the modern form of the code. After some minor changes, International Morse Code was standardized at the International Telegraphy Congress in 1865 in Paris and was later made the standard by the International Telecommunication Union (ITU). Morse's original code specification, largely limited to use in the United States and Canada, became known as American Morse code or "railroad code". American Morse code is now seldom used except in historical re-enactments.

In aviation, pilots use radio navigation aids. To ensure that the stations the pilots are using are serviceable, the stations transmit a set of identification letters (usually a two-to-five-letter version of the station name) in Morse code. Station identification letters are shown on air navigation charts. For example, the VOR-DME based at Vilo Acuña Airport in Cayo Largo del Sur, Cuba is coded as "UCL", and UCL in Morse code is transmitted on its radio frequency. In some countries, during periods of maintenance, the facility may radiate a T-E-S-T code () or the code may be removed which tells pilots and navigators that the station is unreliable. In Canada, the identification is removed entirely to signify the navigation aid is not to be used. In the aviation service, Morse is typically sent at a very slow speed of about 5 words per minute. In the U.S., pilots do not actually have to know Morse to identify the transmitter because the dot/dash sequence is written out next to the transmitter's symbol on aeronautical charts. Some modern navigation receivers automatically translate the code into displayed letters.

International Morse code today is most popular among amateur radio operators, in the mode commonly referred to as "continuous wave" or "CW". (This name was chosen to distinguish it from the damped wave emissions from spark transmitters, not because the transmission is continuous.) Other keying methods are available in radio telegraphy, such as frequency shift keying.

The original amateur radio operators used Morse code exclusively since voice-capable radio transmitters did not become commonly available until around 1920. Until 2003, the International Telecommunication Union mandated Morse code proficiency as part of the amateur radio licensing procedure worldwide. However, the World Radiocommunication Conference of 2003 made the Morse code requirement for amateur radio licensing optional. Many countries subsequently removed the Morse requirement from their licence requirements.

Until 1991, a demonstration of the ability to send and receive Morse code at a minimum of five words per minute (wpm) was required to receive an amateur radio license for use in the United States from the Federal Communications Commission. Demonstration of this ability was still required for the privilege to use the HF bands. Until 2000, proficiency at the 20 wpm level was required to receive the highest level of amateur license (Amateur Extra Class); effective April 15, 2000, the FCC reduced the Extra Class requirement to five wpm. Finally, effective on February 23, 2007, the FCC eliminated the Morse code proficiency requirements from all amateur radio licenses.

While voice and data transmissions are limited to specific amateur radio bands under U.S. rules, Morse code is permitted on all amateur bands—LF, MF, HF, VHF, and UHF. In some countries, certain portions of the amateur radio bands are reserved for transmission of Morse code signals only.

The relatively limited speed at which Morse code can be sent led to the development of an extensive number of abbreviations to speed communication. These include prosigns, Q codes, and a set of Morse code abbreviations for typical message components. For example, CQ is broadcast to be interpreted as "seek you" (I'd like to converse with anyone who can hear my signal). OM (old man), YL (young lady) and XYL ("ex-YL" – wife) are common abbreviations. YL or OM is used by an operator when referring to the other operator, XYL or OM is used by an operator when referring to his or her spouse. QTH is "location" ("My QTH" is "My location"). The use of abbreviations for common terms permits conversation even when the operators speak different languages.

Although the traditional telegraph key (straight key) is still used by some amateurs, the use of mechanical semi-automatic keyers (known as "bugs") and of fully automatic electronic keyers is prevalent today. Software is also frequently employed to produce and decode Morse code radio signals.

Many amateur radio repeaters identify with Morse, even though they are used for voice communications.

Through May 2013, the First, Second, and Third Class (commercial) Radiotelegraph Licenses using code tests based upon the CODEX standard word were still being issued in the United States by the Federal Communications Commission. The First Class license required 20 WPM code group and 25 WPM text code proficiency, the others 16 WPM code group test (five letter blocks sent as simulation of receiving encrypted text) and 20 WPM code text (plain language) test. It was also necessary to pass written tests on operating practice and electronics theory. A unique additional demand for the First Class was a requirement of a year of experience for operators of shipboard and coast stations using Morse. This allowed the holder to be chief operator on board a passenger ship. However, since 1999 the use of satellite and very high-frequency maritime communications systems (GMDSS) has made them obsolete. (By that point meeting experience requirement for the First was very difficult.) Currently, only one class of license, the Radiotelegraph Operator License, is issued. This is granted either when the tests are passed or as the Second and First are renewed and become this lifetime license. For new applicants, it requires passing a written examination on electronic theory and radiotelegraphy practices, as well as 16 WPM codegroup and 20 WPM text tests. However, the code exams are currently waived for holders of Amateur Extra Class licenses who obtained their operating privileges under the old 20 WPM test requirement.

Radio navigation aids such as VORs and NDBs for aeronautical use broadcast identifying information in the form of Morse Code, though many VOR stations now also provide voice identification. Warships, including those of the U.S. Navy, have long used signal lamps to exchange messages in Morse code. Modern use continues, in part, as a way to communicate while maintaining radio silence.

ATIS (Automatic Transmitter Identification System) uses Morse code to identify uplink sources of analog satellite transmissions.
An important application is signalling for help through SOS, "". This can be sent many ways: keying a radio on and off, flashing a mirror, toggling a flashlight, and similar methods. SOS is not three separate characters, rather, it is a prosign SOS, and is keyed without gaps between characters.

Some Nokia mobile phones offer an option to alert the user of an incoming text message with the Morse tone "" (representing SMS or Short Message Service). In addition, applications are now available for mobile phones that enable short messages to be input in Morse Code.

Morse code has been employed as an assistive technology, helping people with a variety of disabilities to communicate. For example, the Android operating system versions 5.0 and higher allow users to input text using Morse Code as an alternative to a keypad or handwriting recognition.

Morse can be sent by persons with severe motion disabilities, as long as they have some minimal motor control. An original solution to the problem that caretakers have to learn to decode has been an electronic typewriter with the codes written on the keys. Codes were sung by users; see the voice typewriter employing morse or votem, Newell and Nabarro, 1968.

Morse code can also be translated by computer and used in a speaking communication aid. In some cases, this means alternately blowing into and sucking on a plastic tube ("sip-and-puff" interface). An important advantage of Morse code over row column scanning is that once learned, it does not require looking at a display. Also, it appears faster than scanning.

In one case reported in the radio amateur magazine "QST", an old shipboard radio operator who had a stroke and lost the ability to speak or write could communicate with his physician (a radio amateur) by blinking his eyes in Morse. Two examples of communication in intensive care units were also published in "QST", Another example occurred in 1966 when prisoner of war Jeremiah Denton, brought on television by his North Vietnamese captors, Morse-blinked the word "TORTURE". In these two cases, interpreters were available to understand those series of eye-blinks.

International Morse code is composed of five elements:


Morse code can be transmitted in a number of ways: originally as electrical pulses along a telegraph wire, but also as an audio tone, a radio signal with short and long tones, or as a mechanical, audible, or visual signal (e.g. a flashing light) using devices like an Aldis lamp or a heliograph, a common flashlight, or even a car horn. Some mine rescues have used pulling on a rope - a short pull for a dot and a long pull for a dash. 

Morse code is transmitted using just two states (on and off). Historians have called it the first digital code. Morse code may be represented as a binary code, and that is what telegraph operators do when transmitting messages. Working from the above ITU definition and further defining a bit as a dot time, a Morse code sequence may be made from a combination of the following five bit-strings:


Note that the marks and gaps alternate: dots and dashes are always separated by one of the gaps, and that the gaps are always separated by a dot or a dash.

Morse messages are generally transmitted by a hand-operated device such as a telegraph key, so there are variations introduced by the skill of the sender and receiver — more experienced operators can send and receive at faster speeds. In addition, individual operators differ slightly, for example, using slightly longer or shorter dashes or gaps, perhaps only for particular characters. This is called their "fist", and experienced operators can recognize specific individuals by it alone. A good operator who sends clearly and is easy to copy is said to have a "good fist". A "poor fist" is a characteristic of sloppy or hard to copy Morse code.

The very long time constants of 19th and early 20th century submarine communications cables required a different form of Morse signalling. Instead of keying a voltage on and off for varying times, the dits and dahs were represented by two polarities of voltage impressed on the cable, for a uniform time.

Below is an illustration of timing conventions. The phrase "MORSE CODE", in Morse code format, would normally be written something like this, where represents dahs and represents dits:

Next is the exact conventional timing for this phrase, with representing "signal on", and representing "signal off", each for the time length of exactly one dit:

Morse code is often spoken or written with "dah" for dashes, "dit" for dots located at the end of a character, and "di" for dots located at the beginning or internally within the character. Thus, the following Morse code sequence:
is orally:

"Dah-dah dah-dah-dah di-dah-dit di-di-dit dit, Dah-di-dah-dit dah-dah-dah dah-di-dit dit".

There is little point in learning to read Morse as above; rather, the of all of the letters and symbols need to be learned, for both sending and receiving.

All Morse code elements depend on the dot length. A dash is the length of 3 dots, and spacings are specified in number of dot lengths. An unambiguous method of specifying the transmission speed is to specify the dot duration as, for example, 50 milliseconds.

Specifying the dot duration is, however, not the common practice. Usually, speeds are stated in words per minute. That introduces ambiguity because words have different numbers of characters, and characters have different dot lengths. It is not immediately clear how a specific word rate determines the dot duration in milliseconds.

Some method to standardize the transformation of a word rate to a dot duration is useful. A simple way to do this is to choose a dot duration that would send a typical word the desired number of times in one minute. If, for example, the operator wanted a character speed of 13 words per minute, the operator would choose a dot rate that would send the typical word 13 times in exactly one minute.

The typical word thus determines the dot length. It is common to assume that a word is 5 characters long. There are two common typical words: "PARIS" and "CODEX". PARIS mimics a word rate that is typical of natural language words and reflects the benefits of Morse code's shorter code durations for common characters such as "e" and "t". CODEX offers a word rate that is typical of 5-letter code groups (sequences of random letters). Using the word PARIS as a standard, the number of dot units is 50 and a simple calculation shows that the dot length at 20 words per minute is 60 milliseconds. Using the word CODEX with 60 dot units, the dot length at 20 words per minute is 50 milliseconds.

Because Morse code is usually sent by hand, it is unlikely that an operator could be that precise with the dot length, and the individual characteristics and preferences of the operators usually override the standards.

For commercial radiotelegraph licenses in the United States, the Federal Communications Commission specifies tests for Morse code proficiency in words per minute and in code groups per minute. The Commission specifies that a word is 5 characters long. The Commission specifies Morse code test elements at 16 code groups per minute, 20 words per minute, 20 code groups per minute, and 25 words per minute. The word per minute rate would be close to the PARIS standard, and the code groups per minute would be close to the CODEX standard.

While the Federal Communications Commission no longer requires Morse code for amateur radio licenses, the old requirements were similar to the requirements for commercial radiotelegraph licenses.

A difference between amateur radio licenses and commercial radiotelegraph licenses is that commercial operators must be able to receive code groups of random characters along with plain language text. For each class of license, the code group speed requirement is slower than the plain language text requirement. For example, for the Radiotelegraph Operator License, the examinee must pass a 20 word per minute plain text test and a 16 word per minute code group test.

Based upon a 50 dot duration standard word such as PARIS, the time for one dot duration or one unit can be computed by the formula:

Where: "T" is the unit time, or dot duration in milliseconds, and "W" is the speed in wpm.

High-speed telegraphy contests are held; according to the "Guinness Book of Records" in June 2005 at the International Amateur Radio Union's 6th World Championship in High Speed Telegraphy in Primorsko, Bulgaria, Andrei Bindasov of Belarus transmitted 230 morse code marks of mixed text in one minute.

Sometimes, especially while teaching Morse code, the timing rules above are changed so two different speeds are used: a character speed and a text speed. The character speed is how fast each individual letter is sent. The text speed is how fast the entire message is sent. For example, individual characters may be sent at a 13 words-per-minute rate, but the intercharacter and interword gaps may be lengthened so the word rate is only 5 words per minute.

Using different character and text speeds is, in fact, a common practice, and is used in the Farnsworth method of learning Morse code.

Some methods of teaching Morse code use a dichotomic search table.
Morse Code cannot be treated as a classical radioteletype (RTTY) signal when it comes to calculating a link margin or a link budget for the simple reason of it possessing variable length dots and dashes as well as variant timing between letters and words. For the purposes of Information Theory and Channel Coding comparisons, the word "PARIS" is used to determine Morse Code's properties because it has an even number of dots and dashes.

Morse Code, when transmitted, essentially creates an AM signal (even in on/off keying mode), assumptions about signal can be made with respect to similarly timed RTTY signalling. Because Morse code transmissions employ an on-off keyed radio signal, it requires less complex transmission equipment than other forms of radio communication.

Morse code also requires less signal bandwidth than voice communication, typically , compared to the roughly 2400 Hz used by single-sideband voice, although at a lower data rate.

Morse code is usually heard at the receiver as a medium-pitched on/off audio tone (600–1000 Hz), so transmissions are easier to copy than voice through the noise on congested frequencies, and it can be used in very high noise / low signal environments. The transmitted power is concentrated into a limited bandwidth so narrow receiver filters can be used to suppress interference from adjacent frequencies. The audio tone is usually created by use of a beat frequency oscillator.

The narrow signal bandwidth also takes advantage of the natural aural selectivity of the human brain, further enhancing weak signal readability. This efficiency makes CW extremely useful for DX (distance) transmissions, as well as for low-power transmissions (commonly called "QRP operation", from the Q-code for "reduce power").

The ARRL has a readability standard for robot encoders called ARRL Farnsworth Spacing that is supposed to have higher readability for both robot and human decoders. Some programs like WinMorse have implemented the standard.

People learning Morse code using the Farnsworth method are taught to send and receive letters and other symbols at their full target speed, that is with normal relative timing of the dots, dashes, and spaces within each symbol for that speed. The Farnsworth method is named for Donald R. "Russ" Farnsworth, also known by his call sign, W6TTB. However, initially exaggerated spaces between symbols and words are used, to give "thinking time" to make the sound "shape" of the letters and symbols easier to learn. The spacing can then be reduced with practice and familiarity.

Another popular teaching method is the Koch method, named after German psychologist Ludwig Koch, which uses the full target speed from the outset but begins with just two characters. Once strings containing those two characters can be copied with 90% accuracy, an additional character is added, and so on until the full character set is mastered.

In North America, many thousands of individuals have increased their code recognition speed (after initial memorization of the characters) by listening to the regularly scheduled code practice transmissions broadcast by W1AW, the American Radio Relay League's headquarters station.

Visual mnemonic charts have been devised over the ages. Baden-Powell included one in the Girl Guides handbook in 1918.

In the United Kingdom, many people learned the Morse code by means of a series of words or phrases that have the same rhythm as a Morse character. For instance, "Q" in Morse is dah-dah-di-dah, which can be memorized by the phrase "God save the Queen", and the Morse for "F" is di-di-dah-dit, which can be memorized as "Did she like it."

A well-known Morse code rhythm from the Second World War period derives from Beethoven's Fifth Symphony, the opening phrase of which was regularly played at the beginning of BBC broadcasts. The timing of the notes corresponds to the Morse for "V", di-di-di-dah, understood as "V for Victory" (as well as the Roman numeral for the number five).

Prosigns for Morse code are special (usually) unwritten procedural signals or symbols that are used to indicate changes in communications protocol status or white space text formatting actions.

The symbols !, $ and & are not defined inside the ITU recommendation on Morse code, but conventions for them exist. The @ symbol was formally added in 2004.


There is no standard representation for the exclamation mark (!), although the KW digraph () was proposed in the 1980s by the Heathkit Company (a vendor of assembly kits for amateur radio equipment).

While Morse code translation software prefers the Heathkit version, on-air use is not yet universal as some amateur radio operators in North America and the Caribbean continue to prefer the older MN digraph () carried over from American landline telegraphy code.




For Chinese, Chinese telegraph code is used to map Chinese characters to four-digit codes and send these digits out using standard Morse code. Korean Morse code uses the SKATS mapping, originally developed to allow Korean to be typed on western typewriters. SKATS maps hangul characters to arbitrary letters of the Latin script and has no relationship to pronunciation in Korean. For Russian and Bulgarian, Russian Morse code is used to map the Cyrillic characters to four-element codes. Many of the characters are encoded the same way (A, O, E, I, T, M, N, R, K, etc.). Bulgarian alphabet contains 30 characters, which exactly match all possible combinations of 1, 2, 3, and 4 dots and dashes (Russian Ы is used as Bulgarian Ь, Russian Ь is used as Bulgarian Ъ). Russian requires 2 extra characters, "Э" and "Ъ" which are encoded with 5 elements.

During early World War I (1914–1916), Germany briefly experimented with 'dotty' and 'dashy' Morse, in essence adding a dot or a dash at the end of each Morse symbol. Each one was quickly broken by Allied SIGINT, and standard Morse was restored by Spring 1916. Only a small percentage of Western Front (North Atlantic and Mediterranean Sea) traffic was in 'dotty' or 'dashy' Morse during the entire war. In popular culture, this is mostly remembered in the book "The Codebreakers" by Kahn and in the national archives of the UK and Australia (whose SIGINT operators copied most of this Morse variant). Kahn's cited sources come from the popular press and wireless magazines of the time.

Other forms of 'Fractional Morse' or 'Fractionated Morse' have emerged.

Decoding software for Morse code ranges from software-defined wide-band radio receivers coupled to the Reverse Beacon Network, which decodes signals and detects CQ messages on ham bands, to smartphone applications.





</doc>
<doc id="18937" url="https://en.wikipedia.org/wiki?curid=18937" title="Mapping">
Mapping

Mapping can mean:



</doc>
<doc id="18939" url="https://en.wikipedia.org/wiki?curid=18939" title="Emergency contraception">
Emergency contraception

Emergency contraception (EC) are birth control measures that may be used after sexual intercourse to prevent pregnancy. Emergency contraception has not been shown to affect the rates of abortion within a country.

There are different forms of EC. Emergency contraceptive pills (ECPs)—sometimes simply referred to as emergency contraceptives (ECs) or the morning-after pill—are medications intended to disrupt or delay ovulation or fertilization, which are necessary for pregnancy. ECPs and abortion pills are not the same. ECPs work by preventing or delaying ovulation and therefore preventing pregnancy, not by abortion. Intrauterine devices (IUDs)—usually used as a primary contraceptive method—are sometimes used as the most effective form of emergency contraception. However, use of IUDs for emergency contraception is relatively rare.
Emergency contraceptive pills (ECPs) (sometimes referred to as emergency hormonal contraception, EHC) are taken after unprotected sexual intercourse or breakage of a condom.

A variety of types of emergency contraceptive pills are available: combined estrogen and progestin pills, progestin-only (levonorgestrel, LNG) pills, and antiprogestin (ulipristal acetate or mifepristone) pills. Progestin-only and antiprogestin pills are available specifically packaged for use as emergency contraceptive pills. Emergency contraceptive pills originally contained higher doses of the same hormones (estrogens, progestins, or both) found in regular combined oral contraceptive pills. Combined estrogen and progestin pills are no longer recommended as dedicated emergency contraceptive pills (because this regimen is less effective and caused more nausea), but certain regular combined oral contraceptive pills (taken 2-5 at a time in what was called "the Yuzpe regimen") have also been shown to be effective as emergency contraceptive pills.

Progestin-only emergency contraceptive pills contain levonorgestrel, either as a single tablet (or historically, as a split dose of two tablets taken 12 hours apart), effective up to 72 hours after intercourse. Progestin-only ECPs are sold under many different brand names. Progestin-only ECPs are available over-the-counter (OTC) in many countries (e.g. Australia, Bangladesh, Bulgaria, Canada, Cyprus, Czech Republic, Denmark, Estonia, India, Malta, Netherlands, Norway, Portugal, Romania, Slovakia, South Africa, Sweden, United States), from a pharmacist without a prescription, and available with a prescription in some other countries.

The antiprogestin ulipristal acetate is available as a micronized emergency contraceptive tablet, effective up to 120 hours after intercourse. Ulipristal acetate ECPs developed by HRA Pharma are available over the counter in Europe and by prescription in over 50 countries under the brand names "ellaOne", "ella" (marketed by Watson Pharmaceuticals in the United States), "Duprisal 30", "Ulipristal 30", and "UPRIS".

The antiprogestin mifepristone (also known as RU-486) is available in five countries as a low-dose or mid-dose emergency contraceptive tablet, effective up to 120 hours after intercourse. Low-dose mifepristone ECPs are available by prescription in Armenia, Russia, Ukraine, and Vietnam and from a pharmacist without a prescription in China. Mid-dose mifepristone ECPs are available by prescription in China and Vietnam.

Combined estrogen (ethinylestradiol) and progestin (levonorgestrel or norgestrel) pills used to be available as dedicated emergency contraceptive pills under several brand names: "Schering PC4", "Tetragynon", "Neoprimavlar", and "Preven" (in the United States) but were withdrawn after more effective dedicated progestin-only (levonorgestrel) emergency contraceptive pills with fewer side effects became available. If other more effective dedicated emergency contraceptive pills (levonorgestrel, ulipristal acetate, or mifepristone) are not available, specific combinations of regular combined oral contraceptive pills can be taken in split doses 12 hours apart (the Yuzpe regimen), effective up to 72 hours after intercourse. 

The U.S. Food and Drug Administration (FDA) approved this off-label use of certain brands of regular combined oral contraceptive pills in 1997. As of 2014, there are 26 brands of regular combined oral contraceptive pills containing levonorgestrel or norgestrel available in the United States that can be used in the emergency contraceptive Yuzpe regimen, when none of the more effective and better tolerated options are available.

Ulipristal acetate, and mid-dose mifepristone are both more effective than levonorgestrel, which is more effective than the Yuzpe method.

The effectiveness of emergency contraception is expressed as a percentage reduction in pregnancy rate for a single use of EC. Using an example of "75% effective", the effectiveness calculation thus: ... these numbers do not translate into a pregnancy rate of 25 percent. Rather, they mean that if 1,000 women have unprotected intercourse in the middle two weeks of their menstrual cycles, approximately 80 will become pregnant. Use of emergency contraceptive pills would reduce this number by 75 percent, to 20 women.

The progestin-only regimen (using levonorgestrel) has an 89% effectiveness. , the labeling on the U.S. brand Plan B explained this effectiveness rate by stating, "Seven out of every eight women who would have gotten pregnant will not become pregnant."

In 1999, a meta-analysis of eight studies of the combined (Yuzpe) regimen concluded that the best point estimate of effectiveness was 74%. A 2003 analysis of two of the largest combined (Yuzpe) regimen studies, using a different calculation method, found effectiveness estimates of 47% and 53%.

For both the progestin-only and Yuzpe regimens, the effectiveness of emergency contraception is highest when taken within 12 hours of intercourse and declines over time.
The World Health Organization (WHO) suggested that reasonable effectiveness may continue for up to 120 hours (5 days) after intercourse.

For 10 mg of mifepristone taken up to 120 hours (5 days) after intercourse, the combined estimate from three trials was an effectiveness of 83%. A review found that a moderate dose of mifepristone is better than LNG or Yuzpe, with delayed return of menstruation being the main adverse effect of most regimes.

HRA Pharma changed its packaging information for Norlevo (levonorgesterel 1.5 mg, which is identical to many other EHCs) in November 2013 warning that the drug loses effectiveness in women who weigh more than 165 pounds and is completely ineffective for women who weigh over 176 pounds.

The most common side effect reported by users of emergency contraceptive pills was nausea (14 to 23% of levonorgestrel-only users and 50.5% of Yuzpe regimen users; vomiting is much less common and unusual with levonorgestrel-only ECPs (5.6% of levonorgestrel-only users vs 18.8% of 979 Yuzpe regimen users in the 1998 WHO trial; 1.4% of 2,720 levonorgestrel-only users in the 2002 WHO trial). Anti-emetics are not routinely recommended with levonorgestrel-only ECPs. If a woman vomits within 2 hours of taking a levonorgestrel-only ECP, she should take a further dose as soon as possible.

Other common side effects (each reported by less than 20% of levonorgestrel-only users in both the 1998 and 2002 WHO trials) were abdominal pain, fatigue, headache, dizziness, and breast tenderness. Side effects generally resolve within 24 hours, although temporary disruption of the menstrual cycle is commonly experienced. If taken before ovulation, the high doses of progestogen in levonorgestrel treatments may induce progestogen withdrawal bleeding a few days after the pills are taken. 

One study found that about half of women who used levonorgestrel ECPs experienced bleeding within 7 days of taking the pills. If levonorgestrel is taken after ovulation, it may increase the length of the luteal phase, thus delaying menstruation by a few days. Mifepristone, if taken before ovulation, may delay ovulation by 3–4 days (delayed ovulation may result in a delayed menstruation). These disruptions only occur in the cycle in which ECPs were taken; subsequent cycle length is not significantly affected. If a woman's menstrual period is delayed by two weeks or more, it is advised that she take a pregnancy test. (Earlier testing may not give accurate results.)

Existing pregnancy is not a contraindication in terms of safety, as there is no known harm to the woman, the course of her pregnancy, or the fetus if progestin-only or combined emergency contraception pills are accidentally used, but EC is not indicated for a woman with a known or suspected pregnancy because it is not effective in women who are already pregnant.

The World Health Organization (WHO) lists no medical condition for which the risks of emergency contraceptive pills outweigh the benefits. The American Academy of Pediatrics (AAP) and experts on emergency contraception have concluded that progestin-only ECPs are preferable to combined ECPs containing estrogen for all women, and particularly those with a history of blood clots, stroke, or migraine.

There are no medical conditions in which progestin-only ECPs are contraindicated. Current venous thromboembolism, current or past history of breast cancer, inflammatory bowel disease, and acute intermittent porphyria are conditions where the advantages of using emergency contraceptive pills generally outweigh the theoretical or proven risks.

ECPs, like all other contraceptives, reduce the absolute risk of ectopic pregnancy by preventing pregnancies and there is no increase in the relative risk of ectopic pregnancy in women who become pregnant after using progestin-only ECPs.

The herbal preparation of St John's wort and some enzyme-inducing drugs (e.g. anticonvulsants or rifampicin) may reduce the effectiveness of ECP, and a larger dose may be required, especially in women who weigh more than 165 lbs.

A more effective alternative to emergency contraceptive pills is the copper-T intrauterine device (IUD) which is generally recommended up to 5 days after unprotected intercourse (however some studies have found effectiveness up to 10 days) to prevent pregnancy. Insertion of an IUD is more effective than use of Emergency Contraceptive Pills - pregnancy rates when used as emergency contraception are the same as with normal IUD use. Unlike emergency contraceptive pills, which work by delaying ovulation, the copper-T IUD works by interfering with sperm motility. Therefore, the copper IUD is equally effective as emergency contraception at all weight ranges. IUDs may be left in place following the subsequent menstruation to provide ongoing contraception for as long as desired (12+ years).

One brand of levonorgestrel pills was marketed as an ongoing method of postcoital contraception. However, with typical use, failure rates are expected to be higher than with use of other birth control methods.


ECPs are generally recommended for backup or "emergency" use—for example, if a woman has forgotten to take a birth control pill or when a condom is torn during sex. However, for individuals facing reproductive coercion, who are not able to use regular birth control, repeated use of EC pills may be the most viable option available.

Making ECPs more widely available does not increase sexual risk-taking. While they are effective for individuals who use them in a timely fashion, availability of EC pills does not appear to decrease abortion rates at the population level.

In 2012 the American Academy of Pediatrics (AAP) stated: "Despite multiple studies showing no increased risk behavior and evidence that hormonal emergency contraception will not disrupt an established pregnancy, public and medical discourse reflects that personal values of physicians and pharmacists continue to affect emergency-contraception access, particularly for adolescents."

Beginning in the 1960s, women who had been sexually assaulted were offered DES. Currently, the standard of care is to offer ulipristal or prompt placement of a copper IUD which is the most effective forms of EC. However, adherence to these best practices varies by emergency department. Before these EC options were available (in 1996), pregnancy rates among females of child-bearing age who had been raped were around 5%. Although EC is recommended following sexual assault, room for improvement in clinical practice remains.

The primary mechanism of action of progestogen-only emergency contraceptive pills is to prevent fertilization by inhibition of ovulation. The best available evidence is that they do not have any post-fertilization effects such as the prevention of implantation. The U.S. FDA-approved labels and European EMA-approved labels (except for HRA Pharma's "NorLevo") levonorgestrel emergency contraceptive pills (based on labels for regular oral contraceptive pills) say they may cause endometrial changes that discourage implantation. Daily use of regular oral contraceptive pills can alter the endometrium (although this has not been proven to interfere with implantation), but the isolated use of a levonorgestrel emergency contraceptive pill does not have time to alter the endometrium. 

In March 2011, the International Federation of Gynecology and Obstetrics (FIGO) issued a statement that: "review of the evidence suggests that LNG [levonorgestreol] ECPs cannot prevent implantation of a fertilized egg. Language on implantation should not be included in LNG ECP product labeling." In June 2012, a "New York Times" editorial called on the FDA to remove from the label the unsupported suggestion that levonorgestrel emergency contraceptive pills inhibit implantation. In November 2013, the European Medicines Agency (EMA) approved a change to the label for HRA Pharma's "NorLevo" saying it cannot prevent implantation of a fertilized egg.

Progestogen-only emergency contraceptive does not appear to affect the function of the Fallopian tubes or increase the rate of ectopic pregnancies.

The primary mechanism of action of progesterone receptor modulator emergency contraceptive pills like low-dose and mid-dose mifepristone and ulipristal acetate is to prevent fertilization by inhibition or delay of ovulation. One clinical study found that post-ovulatory administration of ulipristal acetate altered the endometrium, but whether the changes would inhibit implantation is unknown. The European EMA-approved labels for ulipristal acetate emergency contraceptive pills do not mention an effect on implantation, but the U.S. FDA-approved label says: "alterations to the endometrium that may affect implantation may also contribute to efficacy."

The primary mechanism of action of copper-releasing intrauterine devices (IUDs) as emergency contraceptives is to prevent fertilization because of copper toxicity to sperm and ova. The very high effectiveness of copper-releasing IUDs as emergency contraceptives implies that they must also prevent some pregnancies by post-fertilization effects such as prevention of implantation.

In 1966, gynecologist John McLean Morris and biologist Gertrude Van Wagenen at the Yale School of Medicine reported the successful use of oral high-dose estrogen pills as post-coital contraceptives in women and rhesus macaque monkeys, respectively. A few different drugs were studied, with a focus on high-dose estrogens, and it was originally hoped that postcoital contraception would prove viable as an ongoing contraceptive method.

The first widely used methods were five-day treatments with high-dose estrogens, using diethylstilbestrol (DES) in the US and ethinylestradiol in the Netherlands by Haspels.

In the early 1970s, the Yuzpe regimen was developed by A. Albert Yuzpe in 1974; progestin-only postcoital contraception was investigated (1975); and the copper IUD was first studied for use as emergency contraception (1975). Danazol was tested in the early 1980s in the hopes that it would have fewer side effects than Yuzpe, but was found to be ineffective.

The Yuzpe regimen became the standard course of treatment for postcoital contraception in many countries in the 1980s. The first prescription-only combined estrogen-progestin dedicated product, Schering PC4 (ethinylestradiol and norgestrel), was approved in the UK in January 1984 and first marketed in October 1984. Schering introduced a second prescription-only combined product, Tetragynon (ethinylestradiol and levonorgestrel) in Germany in 1985. By 1997, Schering AG dedicated prescription-only combined products had been approved in only 9 countries: the UK (Schering PC4), New Zealand (Schering PC4), South Africa (E-Gen-C), Germany (Tetragynon), Switzerland (Tetragynon), Denmark (Tetragynon), Norway (Tetragynon), Sweden (Tetragynon) and Finland (Neoprimavlar); and had been withdrawn from marketing in New Zealand in 1997 to prevent it being sold over-the-counter. Regular combined oral contraceptive pills (which were less expensive and more widely available) were more commonly used for the Yuzpe regimen even in countries where dedicated products were available.

Over time, interest in progestin-only treatments increased. The Special Program on Human Reproduction (HRP), an international organization whose members include the World Bank and World Health Organization, "played a pioneering role in emergency contraception" by "confirming the effectiveness of levonorgestrel." After the WHO conducted a large trial comparing Yuzpe and levonorgestrel in 1998, combined estrogen-progestin products were gradually withdrawn from some markets ("Preven" in the United States discontinued May 2004, "Schering PC4" in the UK discontinued October 2001, and "Tetragynon" in France) in favor of progestin-only EC, although prescription-only dedicated Yuzpe regimen products are still available in some countries.

In 2002, China became the first country in which mifepristone was registered for use as EC.

Early studies of emergency contraceptives did not attempt to calculate a failure rate; they simply reported the number of women who became pregnant after using an emergency contraceptive. Since 1980, clinical trials of emergency contraception have first calculated probable pregnancies in the study group if no treatment were given. The effectiveness is calculated by dividing observed pregnancies by the estimated number of pregnancies without treatment.

Placebo-controlled trials that could give a precise measure of the pregnancy rate without treatment would be unethical, so the effectiveness percentage is based on estimated pregnancy rates. These are currently estimated using variants of the calendar method.
Women with irregular cycles for any reason (including recent hormone use such as oral contraceptives and breastfeeding) must be excluded from such calculations. Even for women included in the calculation, the limitations of calendar methods of fertility determination have long been recognized. In their February 2014 emergency review article, Trussell and Raymond note:
Calculation of effectiveness, and particularly the denominator of the fraction, involves many assumptions that are difficult to validate...The risk of pregnancy for women requesting ECPs appears to be lower than assumed in the estimates of ECP efficacy, which are consequently likely to be overestimates. Yet, precise estimates of efficacy may not be highly relevant to many women who have had unprotected intercourse, since ECPs are often the only available treatment.

In 1999, hormonal assay was suggested as a more accurate method of estimating fertility for EC studies.







</doc>
<doc id="18940" url="https://en.wikipedia.org/wiki?curid=18940" title="Meat">
Meat

Meat is animal flesh that is eaten as food. Humans have hunted and killed animals for meat since prehistoric times. The advent of civilization allowed the domestication of animals such as chickens, sheep, rabbits, pigs and cattle. This eventually led to their use in meat production on an industrial scale with the aid of slaughterhouses.

Meat is mainly composed of water, protein, and fat. It is edible raw, but is normally eaten after it has been cooked and seasoned or processed in a variety of ways. Unprocessed meat will spoil or rot within hours or days as a result of infection with and decomposition by bacteria and fungi.

Meat is important in economy and culture, even though its mass production and consumption has been determined to pose risks for human health and the environment. Many religions have rules about which meat may or may not be eaten. Vegetarians and vegans may abstain from eating meat because of concerns about the ethics of eating meat, environmental effects of meat production or nutritional effects of consumption.

The word "meat" comes from the Old English word "mete", which referred to food in general. The term is related to "mad" in Danish, "mat" in Swedish and Norwegian, and "matur" in Icelandic and Faroese, which also mean 'food'. The word "mete" also exists in Old Frisian (and to a lesser extent, modern West Frisian) to denote important food, differentiating it from "swiets" (sweets) and "dierfied" (animal feed).

Most often, "meat" refers to skeletal muscle and associated fat and other tissues, but it may also describe other edible tissues such as offal. "Meat" is sometimes also used in a more restrictive sense to mean the flesh of mammalian species (pigs, cattle, lambs, etc.) raised and prepared for human consumption, to the exclusion of fish, other seafood, insects, poultry, or other animals.

In the context of food, "meat" can also refer to "the edible part of something as distinguished from its covering (such as a husk or shell)", for example, "coconut meat".

Paleontological evidence suggests that meat constituted a substantial proportion of the diet of the earliest humans. Early hunter-gatherers depended on the organized hunting of large animals such as bison and deer.

The domestication of animals, of which we have evidence dating back to the end of the last glacial period (c. 10,000 BCE), allowed the systematic production of meat and the breeding of animals with a view to improving meat production. Animals that are now principal sources of meat were domesticated in conjunction with the development of early civilizations:


Other animals are or have been raised or hunted for their flesh. The type of meat consumed varies much between different cultures, changes over time, depending on factors such as tradition and the availability of the animals. The amount and kind of meat consumed also varies by income, both between countries and within a given country.


Modern agriculture employs a number of techniques, such as progeny testing, to speed artificial selection by breeding animals to rapidly acquire the qualities desired by meat producers. For instance, in the wake of well-publicised health concerns associated with saturated fats in the 1980s, the fat content of United Kingdom beef, pork and lamb fell from 20–26 percent to 4–8 percent within a few decades, due to both selective breeding for leanness and changed methods of butchery. Methods of genetic engineering aimed at improving the meat production qualities of animals are now also becoming available.
Even though it is a very old industry, meat production continues to be shaped strongly by the evolving demands of customers. The trend towards selling meat in pre-packaged cuts has increased the demand for larger breeds of cattle, which are better suited to producing such cuts. Even more animals not previously exploited for their meat are now being farmed, especially the more agile and mobile species, whose muscles tend to be developed better than those of cattle, sheep or pigs. Examples are the various antelope species, the zebra, water buffalo and camel, as well as non-mammals, such as the crocodile, emu and ostrich. Another important trend in contemporary meat production is organic farming which, while providing no organoleptic benefit to meat so produced, meets an increasing demand for organic meat.

For most of human history, meat was a largely unquestioned part of the human diet. Only in the 20th century did it begin to become a topic of discourse and contention in society, politics and wider culture.

The founders of Western philosophy disagreed about the ethics of eating meat. Plato's "Republic" has Socrates describe the ideal state as vegetarian. Pythagoras believed that humans and animals were equal and therefore disapproved of meat consumption, as did Plutarch, whereas Zeno and Epicurus were vegetarian but allowed meat-eating in their philosophy. Conversely, Aristotle's "Politics" assert that animals, as inferior beings, exist to serve humans, including as food. Augustine drew on Aristotle to argue that the universe's natural hierarchy allows humans to eat animals, and animals to eat plants. Enlightenment philosophers were likewise divided. Descartes wrote that animals are merely animated machines, and Kant considered them inferior beings for lack of discernment; means rather than ends. But Voltaire and Rousseau disagreed. The latter argued that meat-eating is a social rather than a natural act, because children are not interested in meat.

Later philosophers examined the changing practices of eating meat in the modern age as part of a process of detachment from animals as living beings. Norbert Elias, for instance, noted that in medieval times cooked animals were brought to the table whole, but that since the Renaissance only the edible parts are served, which are no longer recognizably part of an animal. Modern eaters, according to , demand an "ellipsis" between meat and dead animals; for instance, calves' eyes are no longer considered a delicacy as in the Middle Ages, but provoke disgust. Even in the English language, distinctions emerged between animals and their meat, such as between cattle and beef, pigs and pork. Fernand Braudel wrote that since the European diet of the 15th and 16th century was particularly heavy in meat, European colonialism helped export meat-eating across the globe, as colonized peoples took up the culinary habits of their colonizers, which they associated with wealth and power.

Meat consumption varies worldwide, depending on cultural or religious preferences, as well as economic conditions. Vegetarians and vegans choose not to eat meat because of ethical, economic, environmental, religious or health concerns that are associated with meat production and consumption.

According to the analysis of the FAO the overall consumption for white meat between 1990 and 2009 has dramatically increased. Poultry meat has increased by 76.6% per kilo per capita and pig meat by 19.7%. Bovine meat has decreased from per capita in 1990 to per capita in 2009. 

Overall, diets that include meat are the most common worldwide according to the results of a 2018 Ipsos MORI study of 16–64 years olds in 28 different countries. Ipsos states “"An omnivorous diet is the most common diet globally, with non-meat diets (which can include fish) followed by over a tenth of the global population."” Approximately 87% of people include meat in their diet in some frequency. 73% of meat eaters included it in their diet regularly and 14% consumed meat only occasionally or infrequently. Estimates of the non-meat diets were also broken down. About 3% of people followed vegan diets; where consumption of meat, eggs, and dairy are abstained from. About 5% of people followed vegetarian diets; where consumption of meat is abstained from, but egg and/or dairy consumption is not strictly restricted. About 3% of people followed pescetarian diets; where consumption of the meat of land animals is abstained from, fish meat and other seafood is consumed, and egg and/or dairy consumption may or may not be strictly restricted.

Agricultural science has identified several factors bearing on the growth and development of meat in animals.

Several economically important traits in meat animals are heritable to some degree (see the adjacent table) and can thus be selected for by animal breeding. In cattle, certain growth features are controlled by recessive genes which have not so far been controlled, complicating breeding. One such trait is dwarfism; another is the doppelender or "double muscling" condition, which causes muscle hypertrophy and thereby increases the animal's commercial value. Genetic analysis continues to reveal the genetic mechanisms that control numerous aspects of the endocrine system and, through it, meat growth and quality.

Genetic engineering techniques can shorten breeding programs significantly because they allow for the identification and isolation of genes coding for desired traits, and for the reincorporation of these genes into the animal genome. To enable such manipulation, research is ongoing () to map the entire genome of sheep, cattle and pigs. Some research has already seen commercial application. For instance, a recombinant bacterium has been developed which improves the digestion of grass in the rumen of cattle, and some specific features of muscle fibres have been genetically altered.

Experimental reproductive cloning of commercially important meat animals such as sheep, pig or cattle has been successful. Multiple asexual reproduction of animals bearing desirable traits is anticipated, although this is not yet practical on a commercial scale.

Heat regulation in livestock is of great economic significance, because mammals attempt to maintain a constant optimal body temperature. Low temperatures tend to prolong animal development and high temperatures tend to retard it. Depending on their size, body shape and insulation through tissue and fur, some animals have a relatively narrow zone of temperature tolerance and others (e.g. cattle) a broad one. Static magnetic fields, for reasons still unknown, also retard animal development.

The quality and quantity of usable meat depends on the animal's "plane of nutrition", i.e., whether it is over- or underfed. Scientists disagree about how exactly the plane of nutrition influences carcass composition.

The composition of the diet, especially the amount of protein provided, is also an important factor regulating animal growth. Ruminants, which may digest cellulose, are better adapted to poor-quality diets, but their ruminal microorganisms degrade high-quality protein if supplied in excess. Because producing high-quality protein animal feed is expensive (see also "Environmental impact" below), several techniques are employed or experimented with to ensure maximum utilization of protein. These include the treatment of feed with formalin to protect amino acids during their passage through the rumen, the recycling of manure by feeding it back to cattle mixed with feed concentrates, or the partial conversion of petroleum hydrocarbons to protein through microbial action.

In plant feed, environmental factors influence the availability of crucial nutrients or micronutrients, a lack or excess of which can cause a great many ailments. In Australia, for instance, where the soil contains limited phosphate, cattle are being fed additional phosphate to increase the efficiency of beef production. Also in Australia, cattle and sheep in certain areas were often found losing their appetite and dying in the midst of rich pasture; this was at length found to be a result of cobalt deficiency in the soil. Plant toxins are also a risk to grazing animals; for instance, sodium fluoroacetate, found in some African and Australian plants, kills by disrupting the cellular metabolism. Certain man-made pollutants such as methylmercury and some pesticide residues present a particular hazard due to their tendency to bioaccumulate in meat, potentially poisoning consumers.

Meat producers may seek to improve the fertility of female animals through the administration of gonadotrophic or ovulation-inducing hormones. In pig production, sow infertility is a common problem — possibly due to excessive fatness. No methods currently exist to augment the fertility of male animals. Artificial insemination is now routinely used to produce animals of the best possible genetic quality, and the efficiency of this method is improved through the administration of hormones that synchronize the ovulation cycles within groups of females.

Growth hormones, particularly anabolic agents such as steroids, are used in some countries to accelerate muscle growth in animals. This practice has given rise to the beef hormone controversy, an international trade dispute. It may also decrease the tenderness of meat, although research on this is inconclusive, and have other effects on the composition of the muscle flesh. Where castration is used to improve control over male animals, its side effects are also counteracted by the administration of hormones.

Sedatives may be administered to animals to counteract stress factors and increase weight gain. The feeding of antibiotics to certain animals has been shown to improve growth rates also. This practice is particularly prevalent in the USA, but has been banned in the EU, partly because it causes antimicrobial resistance in pathogenic microorganisms.

Numerous aspects of the biochemical composition of meat vary in complex ways depending on the species, breed, sex, age, plane of nutrition, training and exercise of the animal, as well as on the anatomical location of the musculature involved. Even between animals of the same litter and sex there are considerable differences in such parameters as the percentage of intramuscular fat.

Adult mammalian muscle flesh consists of roughly 75 percent water, 19 percent protein, 2.5 percent intramuscular fat, 1.2 percent carbohydrates and 2.3 percent other soluble non-protein substances. These include nitrogenous compounds, such as amino acids, and inorganic substances such as minerals.

Muscle proteins are either soluble in water (sarcoplasmic proteins, about 11.5 percent of total muscle mass) or in concentrated salt solutions (myofibrillar proteins, about 5.5 percent of mass). There are several hundred sarcoplasmic proteins. Most of them – the glycolytic enzymes – are involved in the glycolytic pathway, i.e., the conversion of stored energy into muscle power. The two most abundant myofibrillar proteins, myosin and actin, are responsible for the muscle's overall structure. The remaining protein mass consists of connective tissue (collagen and elastin) as well as organelle tissue.

Fat in meat can be either adipose tissue, used by the animal to store energy and consisting of "true fats" (esters of glycerol with fatty acids), or intramuscular fat, which contains considerable quantities of phospholipids and of unsaponifiable constituents such as cholesterol.

Meat can be broadly classified as "red" or "white" depending on the concentration of myoglobin in muscle fibre. When myoglobin is exposed to oxygen, reddish oxymyoglobin develops, making myoglobin-rich meat appear red. The redness of meat depends on species, animal age, and fibre type: Red meat contains more narrow muscle fibres that tend to operate over long periods without rest, while white meat contains more broad fibres that tend to work in short fast bursts.

Generally, the meat of adult mammals such as cows, sheep, and horses is considered red, while chicken and turkey breast meat is considered white.

All muscle tissue is very high in protein, containing all of the essential amino acids, and in most cases is a good source of zinc, vitamin B, selenium, phosphorus, niacin, vitamin B, choline, riboflavin and iron. Several forms of meat are also high in vitamin K. Muscle tissue is very low in carbohydrates and does not contain dietary fiber. While taste quality may vary between meats, the proteins, vitamins, and minerals available from meats are generally consistent.

The fat content of meat can vary widely depending on the species and breed of animal, the way in which the animal was raised, including what it was fed, the anatomical part of the body, and the methods of butchering and cooking. Wild animals such as deer are typically leaner than farm animals, leading those concerned about fat content to choose game such as venison. Decades of breeding meat animals for fatness is being reversed by consumer demand for meat with less fat. The fatty deposits that exist with the muscle fibers in meats soften meat when it is cooked and improve the flavor through chemical changes initiated through heat that allow the protein and fat molecules to interact. The fat, when cooked with meat, also makes the meat seem juicier. The nutritional contribution of the fat is mainly calories as opposed to protein. As fat content rises, the meat's contribution to nutrition declines. In addition, there is cholesterol associated with fat surrounding the meat. The cholesterol is a lipid associated with the kind of saturated fat found in meat. The increase in meat consumption after 1960 is associated with, though not definitively the cause of, significant imbalances of fat and cholesterol in the human diet.

The table in this section compares the nutritional content of several types of meat. While each kind of meat has about the same content of protein and carbohydrates, there is a very wide range of fat content.
Meat is produced by killing an animal and cutting flesh out of it. These procedures are called slaughter and butchery, respectively. There is ongoing research into producing meat "in vitro"; that is, outside of animals.

Upon reaching a predetermined age or weight, livestock are usually transported "en masse" to the slaughterhouse. Depending on its length and circumstances, this may exert stress and injuries on the animals, and some may die "en route". Unnecessary stress in transport may adversely affect the quality of the meat. In particular, the muscles of stressed animals are low in water and glycogen, and their pH fails to attain acidic values, all of which results in poor meat quality. Consequently, and also due to campaigning by animal welfare groups, laws and industry practices in several countries tend to become more restrictive with respect to the duration and other circumstances of livestock transports.

Animals are usually slaughtered by being first stunned and then exsanguinated (bled out). Death results from the one or the other procedure, depending on the methods employed. Stunning can be effected through asphyxiating the animals with carbon dioxide, shooting them with a gun or a captive bolt pistol, or shocking them with electric current. In most forms of ritual slaughter, stunning is not allowed.

Draining as much blood as possible from the carcass is necessary because blood causes the meat to have an unappealing appearance and is a breeding ground for microorganisms. The exsanguination is accomplished by severing the carotid artery and the jugular vein in cattle and sheep, and the anterior vena cava in pigs.
The act of slaughtering animals for meat, or of raising or transporting animals for slaughter, may engender both psychological stress and physical trauma in the people involved. Additionally, slaughterhouse workers are exposed to noise of between 76 and 100 dB from the screams of animals being killed. 80 dB is the threshold at which the wearing of ear protection is recommended.

After exsanguination, the carcass is dressed; that is, the head, feet, hide (except hogs and some veal), excess fat, viscera and offal are removed, leaving only bones and edible muscle. Cattle and pig carcases, but not those
of sheep, are then split in half along the mid ventral axis, and the carcase is cut into wholesale pieces. The dressing and cutting sequence, long a province of manual labor, is progressively being fully automated.

Under hygienic conditions and without other treatment, meat can be stored at above its freezing point (–1.5 °C) for about six weeks without spoilage, during which time it undergoes an aging process that increases its tenderness and flavor.

During the first day after death, glycolysis continues until the accumulation of lactic acid causes the pH to reach about 5.5. The remaining glycogen, about 18 g per kg, is believed to increase the water-holding capacity and tenderness of the flesh when cooked. "Rigor mortis" sets in a few hours after death as ATP is used up, causing actin and myosin to combine into rigid actomyosin and lowering the meat's water-holding capacity, causing it to lose water ("weep"). In muscles that enter "rigor" in a contracted position, actin and myosin filaments overlap and cross-bond, resulting in meat that is tough on cooking – hence again the need to prevent pre-slaughter stress in the animal.

Over time, the muscle proteins denature in varying degree, with the exception of the collagen and elastin of connective tissue, and "rigor mortis" resolves. Because of these changes, the meat is tender and pliable when cooked just after death or after the resolution of "rigor", but tough when cooked during "rigor." As the muscle pigment myoglobin denatures, its iron oxidates, which may cause a brown discoloration near the surface of the meat. Ongoing proteolysis also contributes to conditioning. Hypoxanthine, a breakdown product of ATP, contributes to the meat's flavor and odor, as do other products of the decomposition of muscle fat and protein.

When meat is industrially processed in preparation of consumption, it may be enriched with additives to protect or modify its flavor or color, to improve its tenderness, juiciness or cohesiveness, or to aid with its preservation. Meat additives include the following:

With the rise of complex supply chains, including cold chains, in developed economies, the distance between the farmer or fisherman and customer has grown, increasing the possibility for intentional and unintentional misidentification of meat at various points in the supply chain.

In 2013, reports emerged across Europe that products labelled as containing beef actually contained horse meat. In February 2013 a study was published showing that about one-third of raw fish are misidentified across the United States.

Various forms of imitation meat have been created for people who wish not to eat meat but still want to taste its flavor and texture. Meat imitates are typically some form of processed soybean (tofu, tempeh), but they can also be based on wheat gluten, pea protein isolate, or even fungi (quorn).

Various environmental effects are associated with meat production. Among these are greenhouse gas emissions, fossil energy use, water use, water quality changes, and effects on grazed ecosystems.

The livestock sector may be the largest source of water pollution (due to animal wastes, fertilizers, pesticides), and it contributes to emergence of antibiotic resistance. It accounts for over 8% of global human water use. It is by far the biggest cause of land use, as it accounts for nearly 40% of the global land surface. It is a significant driver of biodiversity loss, as it causes deforestation, ocean dead zones, land degradation, pollution, and overfishing.

The occurrence, nature and significance of environmental effects varies among livestock production systems. Grazing of livestock can be beneficial for some wildlife species, but not for others. Targeted grazing of livestock is used as a food-producing alternative to herbicide use in some vegetation management.

Meat production is responsible for 14.5% and possibly up to 51% of the world's anthropogenic greenhouse gas emissions. Greenhouse gas emission depends on the economy and country: animal products (meat, fish, and dairy) account for 22%, 65%, and 70% of emissions in the diets of lower-middle–, upper-middle–, and high-income nations, respectively. Some nations show very different impacts to counterparts within the same group, with Brazil and Australia having emissions over 200% higher than the average of their respective income groups and driven by meat consumption.

According to the "Assessing the Environmental Impacts of Consumption and Production" report produced by United Nations Environment Programme's (UNEP) international panel for sustainable resource management, a worldwide transition in the direction of a meat and dairy free diet is indispensable if adverse global climate change were to be prevented. A 2019 report in "The Lancet" recommended that global meat (and sugar) consumption be reduced by 50 percent to mitigate climate change. Meat consumption in Western societies needs to be reduced by up to 90% according to a 2018 study published in "Nature". The 2019 special report by the Intergovernmental Panel on Climate Change advocated for significantly reducing meat consumption, particularly in wealthy countries, in order to mitigate and adapt to climate change.

Meat consumption is considered one of the primary contributors of the sixth mass extinction. A 2017 study by the World Wildlife Fund found that 60% of global biodiversity loss is attributable to meat-based diets, in particular from the vast scale of feed crop cultivation needed to rear tens of billions of farm animals for human consumption puts an enormous strain on natural resources resulting in a wide-scale loss of lands and species. Currently, livestock make up 60% of the biomass of all mammals on earth, followed by humans (36%) and wild mammals (4%). In November 2017, 15,364 world scientists signed a Warning to Humanity calling for, among other things, drastically diminishing our per capita consumption of meat and "dietary shifts towards mostly plant-based foods". The 2019 "Global Assessment Report on Biodiversity and Ecosystem Services", released by IPBES, also recommended reductions in meat consumption in order to mitigate biodiversity loss.

A July 2018 study in "Science" says that meat consumption is set to rise as the human population increases along with affluence, which will increase greenhouse gas emissions and further reduce biodiversity.

Meat-producing livestock can provide environmental benefits through waste reduction, e.g. conversion of human-inedible residues of food crops. Manure from meat-producing livestock is used as fertilizer; it may be composted before application to food crops. Substitution of animal manures for synthetic fertilizers in crop production can be environmentally significant, as between 43 and 88 MJ of fossil fuel energy are used per kg of nitrogen in manufacture of synthetic nitrogenous fertilizers.

The spoilage of meat occurs, if untreated, in a matter of hours or days and results in the meat becoming unappetizing, poisonous or infectious. Spoilage is caused by the practically unavoidable infection and subsequent decomposition of meat by bacteria and fungi, which are borne by the animal itself, by the people handling the meat, and by their implements. Meat can be kept edible for a much longer time – though not indefinitely – if proper hygiene is observed during production and processing, and if appropriate food safety, food preservation and food storage procedures are applied. Without the application of preservatives and stabilizers, the fats in meat may also begin to rapidly decompose after cooking or processing, leading to an objectionable taste known as warmed over flavor.

Fresh meat can be cooked for immediate consumption, or be processed, that is, treated for longer-term preservation and later consumption, possibly after further preparation. Fresh meat cuts or processed cuts may produce iridescence, commonly thought to be due to spoilage but actually caused by structural coloration and diffraction of the light. A common additive to processed meats for both preservation and the prevention of discoloration is sodium nitrite. This substance is a source of health concerns because it may form carcinogenic nitrosamines when heated.

Meat is prepared in many ways, as steaks, in stews, fondue, or as dried meat like beef jerky. It may be ground then formed into patties (as hamburgers or croquettes), loaves, or sausages, or used in loose form (as in "sloppy joe" or Bolognese sauce).

Some meat is cured by smoking, which is the process of flavoring, cooking, or preserving food by exposing it to the smoke from burning or smoldering plant materials, most often wood. In Europe, alder is the traditional smoking wood, but oak is more often used now, and beech to a lesser extent. In North America, hickory, mesquite, oak, pecan, alder, maple, and fruit-tree woods are commonly used for smoking. Meat can also be cured by pickling, preserving in salt or brine (see salted meat and other curing methods). Other kinds of meat are marinated and barbecued, or simply boiled, roasted, or fried.

Meat is generally eaten cooked, but many recipes call for raw beef, veal or fish (tartare). Steak tartare is a meat dish made from finely chopped or minced raw beef or horse meat. Meat is often spiced or seasoned, particularly with meat products such as sausages. Meat dishes are usually described by their source (animal and part of body) and method of preparation (e.g., a beef rib).

Meat is a typical base for making sandwiches. Popular varieties of sandwich meat include ham, pork, salami and other sausages, and beef, such as steak, roast beef, corned beef, pepperoni, and pastrami. Meat can also be molded or pressed (common for products that include offal, such as haggis and scrapple) and canned.

There is concern and debate regarding the potential association of meat, in particular red and processed meat, with a variety of health risks. A study of 400,000 subjects conducted by the European Prospective Investigation into Cancer and Nutrition and published in 2013 showed "a moderate positive association between processed meat consumption and mortality, in particular due to cardiovascular diseases, but also to cancer."

A 1999 metastudy combined data from five studies from western countries. The metastudy reported mortality ratios, where lower numbers indicated fewer deaths, for fish eaters to be 0.82, vegetarians to be 0.84, occasional meat eaters to be 0.84. Regular meat eaters and vegans shared the highest mortality ratio of 1.00.

In response to changing prices as well as health concerns about saturated fat and cholesterol (see lipid hypothesis), consumers have altered their consumption of various meats. A USDA report points out that consumption of beef in the United States between 1970–1974 and 1990–1994 dropped by 21%, while consumption of chicken increased by 90%. During the same period of time, the price of chicken dropped by 14% relative to the price of beef. From 1995–1996, beef consumption increased due to higher supplies and lower prices.

The "2015–2020 Dietary Guidelines for Americans" asked men and teenage boys to increase their consumption of vegetables or other underconsumed foods (fruits, whole grains, and dairy) while reducing intake of protein foods (meats, poultry, and eggs) that they currently overconsume.

The health effects of red meat are unclear as of 2019.

Various toxic compounds can contaminate meat, including heavy metals, mycotoxins, pesticide residues, dioxins, polychlorinated biphenyl (PCBs). Processed, smoked and cooked meat may contain carcinogens such as polycyclic aromatic hydrocarbons.

Toxins may be introduced to meat as part of animal feed, as veterinary drug residues, or during processing and cooking. Often, these compounds can be metabolized in the body to form harmful by-products. Negative effects depend on the individual genome, diet, and history of the consumer. Any chemical's toxicity is also dependent on the dose and timing of exposure.

There are concerns about a relationship between the consumption of meat, in particular processed and red meat, and increased cancer risk. The International Agency for Research on Cancer (IARC), a specialized agency of the World Health Organization (WHO), classified processed meat (e.g., bacon, ham, hot dogs, sausages) as, ""carcinogenic to humans" (Group 1), based on "sufficient evidence" in humans that the consumption of processed meat causes colorectal cancer." IARC also classified red meat as ""probably carcinogenic to humans" (Group 2A), based on "limited evidence" that the consumption of red meat causes cancer in humans and "strong" mechanistic evidence supporting a carcinogenic effect."

The correlation of consumption to increased risk of heart disease is controversial. Some studies fail to find a link between red meat consumption and heart disease (although the same study found statistically significant correlation between the consumption of processed meat and coronary heart disease). A large cohort study of Seventh-Day Adventists in California found that the risk of heart disease is three times greater for 45-64-year-old men who eat meat daily, versus those who did not eat meat. This study compared adventists to the general population and not other Seventh Day Adventists who ate meat and did not specifically distinguish red and processed meat in its assessment.

A major Harvard University study in 2010 involving over one million people who ate meat found that only processed meat had an adverse risk in relation to coronary heart disease. The study suggests that eating 50 g (less than 2 ounces) of processed meat per day increases risk of coronary heart disease by 42%, and diabetes by 19%. Equivalent levels of fat, including saturated fats, in unprocessed meat (even when eating twice as much per day) did not show any deleterious effects, leading the researchers to suggest that "differences in salt and preservatives, rather than fats, might explain the higher risk of heart disease and diabetes seen with processed meats, but not with unprocessed red meats."
A 2017 meta-analyses of randomized controlled trials found that eating more than 0.5 servings of meat per-day does not increase lipids, blood pressure, lipoproteins, or other heart disease risk factors.

Prospective analysis suggests that meat consumption is positively associated with weight gain in men and women. The National Cattlemen's Beef Association countered by stating that meat consumption may not be associated with fat gain. In response, the authors of the original study controlled for just abdominal fat across a sample of 91,214 people and found that even when controlling for calories and lifestyle factors, meat consumption is linked with obesity. Additional studies and reviews have confirmed the finding that greater meat consumption is positively linked with greater weight gain even when controlling for calories, and lifestyle factors.

Bacterial contamination has been seen with meat products. A 2011 study by the Translational Genomics Research Institute showed that nearly half (47%) of the meat and poultry in U.S. grocery stores were contaminated with "S. aureus", with more than half (52%) of those bacteria resistant to antibiotics. A 2018 investigation by the Bureau of Investigative Journalism and "The Guardian" found that around 15 percent of the US population suffers from foodborne illnesses every year. The investigation also highlighted unsanitary conditions in US-based meat plants, which included meat products covered in excrement and abscesses "filled with pus".

Meat can transmit certain diseases, but complete cooking and avoiding recontamination reduces this possibility.

Several studies published since 1990 indicate that cooking muscle meat creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute published results of a study which found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While eating muscle meat raw may be the only way to avoid HCAs fully, the National Cancer Institute states that cooking meat below creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90%.

Nitrosamines, present in processed and cooked foods, have been noted as being carcinogenic, being linked to colon cancer. Also, toxic compounds called PAHs, or polycyclic aromatic hydrocarbons, present in processed, smoked and cooked foods, are known to be carcinogenic.

Meat is part of the human diet in most cultures, where it often has symbolic meaning and important social functions. People choose not to eat meat (vegetarianism) or any food made from animals (veganism). The reasons for not eating all or some meat may include ethical objections to killing animals for food, health concerns, environmental concerns or religious dietary laws.

Ethical issues regarding the consumption of meat include objecting to the act of killing animals or to the agricultural practices used in meat production. Reasons for objecting to killing animals for consumption may include animal rights, environmental ethics, or an aversion to inflicting pain or harm on other sentient creatures. Some people, while not vegetarians, refuse to eat the flesh of certain animals (such as cows, pigs, cats, dogs, horses, or rabbits) due to cultural or religious traditions.

Some people eat only the flesh of animals that they believe have not been mistreated, and abstain from the flesh of animals raised in factory farms or else abstain from particular products, such as foie gras and veal.

Some techniques of intensive agriculture may be cruel to animals: foie gras is a food product made from the liver of ducks or geese that have been force fed corn to fatten the organ; veal is criticised because the veal calves may be highly restricted in movement, have unsuitable flooring, spend their entire lives indoors, experience prolonged deprivation (sensory, social, and exploratory), and be more susceptible to high amounts of stress and disease.

The religion of Jainism has always opposed eating meat, and there are also schools of Buddhism and Hinduism that condemn the eating of meat. Jewish dietary rules ("Kashrut") allow certain ("kosher") meat and forbid other ("treif"). The rules include prohibitions on the consumption of unclean animals (such as pork, shellfish including mollusca and crustacea, and most insects), and mixtures of meat and milk. Similar rules apply in Islamic dietary laws: The Quran explicitly forbids meat from animals that die naturally, blood, the meat of swine (porcine animals, pigs), and animals dedicated to other than Allah (either undedicated or dedicated to idols) which are haram as opposed to halal. Sikhism forbids meat of slowly slaughtered animals ("kutha") and prescribes killing animals with a single strike ("jhatka"), but some Sikh groups oppose eating any meat.

Research in applied psychology has investigated practices of meat eating in relation to morality, emotions, cognition, and personality characteristics. Psychological research suggests meat eating is correlated with masculinity, support for social hierarchy, and reduced openness to experience. Research into the consumer psychology of meat is relevant both to meat industry marketing and to advocates of reduced meat consumption.

Unlike most other food, meat is not perceived as gender-neutral, and is particularly associated with men and masculinity. Sociological research, ranging from African tribal societies to contemporary barbecues, indicates that men are much more likely to participate in preparing meat than other food. This has been attributed to the influence of traditional male gender roles, in view of a "male familiarity with killing" or roasting being more violent as opposed to boiling. By and large, at least in modern societies, men also tend to consume more meat than women, and men often prefer red meat whereas women tend to prefer chicken and fish.



</doc>
<doc id="18942" url="https://en.wikipedia.org/wiki?curid=18942" title="Monty Python">
Monty Python

Monty Python (also collectively known as the Pythons) were a British surreal comedy group who created the sketch comedy television show "Monty Python's Flying Circus", which first aired on the BBC in 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series into something larger in scope and impact, including touring stage shows, films, numerous albums, several books and musicals. The Pythons' influence on comedy has been compared to the Beatles' influence on music. Regarded as an enduring icon of 1970s pop culture, their sketch show has been referred to as being “an important moment in the evolution of television comedy".

Broadcast by the BBC between 1969 and 1974, "Monty Python's Flying Circus" was conceived, written and performed by its members Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. Loosely structured as a sketch show, but with an innovative stream-of-consciousness approach aided by Gilliam's animation, it pushed the boundaries of what was acceptable in style and content. A self-contained comedy team responsible for both writing and performing their work, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. Following their television work, they began making films, including "Monty Python and the Holy Grail" (1975), "Life of Brian" (1979) and "The Meaning of Life" (1983). Their influence on British comedy has been apparent for years, while in North America, it has coloured the work of cult performers from the early editions of "Saturday Night Live" through to more recent absurdist trends in television comedy. "Pythonesque" has entered the English lexicon as a result.

At the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution To Cinema. In 1998 they were awarded the AFI Star Award by the American Film Institute. Many sketches from their TV show and films are well-known and widely quoted. Both "Holy Grail" and "Life of Brian" are frequently ranked in lists of greatest comedy films. In a 2005 poll of over 300 comics, comedy writers, producers and directors throughout the English-speaking world to find "The Comedian's Comedian", three of the six Pythons members were voted to be among the top 50 greatest comedians ever: Cleese at No. 2, Idle at No. 21, and Palin at No. 30.

Jones and Palin met at Oxford University, where they performed together with the Oxford Revue. Chapman and Cleese met at Cambridge University. Idle was also at Cambridge, but started a year after Chapman and Cleese. Cleese met Gilliam in New York City while on tour with the Cambridge University Footlights revue "Cambridge Circus" (originally entitled "A Clump of Plinths"). Chapman, Cleese, and Idle were members of the Footlights, which at that time also included the future "Goodies" (Tim Brooke-Taylor, Bill Oddie, and Graeme Garden), and Jonathan Lynn (co-writer of "Yes Minister" and "Yes, Prime Minister"). During Idle's presidency of the club, feminist writer Germaine Greer and broadcaster Clive James were members. Recordings of Footlights' revues (called "Smokers") at Pembroke College include sketches and performances by Cleese and Idle, which, along with tapes of Idle's performances in some of the drama society's theatrical productions, are kept in the archives of the Pembroke Players.

The six Python members appeared in or wrote these shows before "Flying Circus":

The BBC's satirical television show "The Frost Report", broadcast from March 1966 to December 1967, is credited as first uniting the British Pythons and providing an environment in which they could develop their particular styles.
Following the success of "Do Not Adjust Your Set", broadcast on ITV from December 1967 to May 1969, Thames Television offered Gilliam, Idle, Jones, and Palin their own late-night adult comedy series together. At the same time, Chapman and Cleese were offered a show by the BBC, which had been impressed by their work on "The Frost Report" and "At Last the 1948 Show". Cleese was reluctant to do a two-man show for various reasons, including Chapman's supposedly difficult and erratic personality. Cleese had fond memories of working with Palin on "How to Irritate People" and invited him to join the team. With no studio available at Thames until summer 1970 for the late-night show, Palin agreed to join Cleese and Chapman, and suggested the involvement of his writing partner Jones and colleague Idle—who in turn wanted Gilliam to provide animations for the projected series. Much has been made of the fact that the Monty Python troupe is the result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.

By contrast, according to John Cleese's autobiography, the origins of "Monty Python" lay in the admiration that writing partners Cleese and Chapman had for the new type of comedy being done on "Do Not Adjust Your Set"; as a result, a meeting was initiated by Cleese between Chapman, Idle, Jones, Palin, and himself at which it was agreed to pool their writing and performing efforts and jointly seek production sponsorship. According to their official website, the group was born from a Kashmir tandoori restaurant in Hampstead on 11 May 1969, following a taping of "Do Not Adjust Your Set" which Cleese and Chapman attended. It was the first time all six got together, reportedly going back to Cleese's apartment on nearby Basil Street afterwards to continue discussions.

The Pythons had a definite idea about what they wanted to do with the series. They were admirers of the work of Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore on "Beyond the Fringe"—seminal to the British "satire boom"—and had worked on "Frost", which was similar in style. They enjoyed Cook and Moore's sketch show "Not Only... But Also". One problem the Pythons perceived with these programmes was that though the body of the sketch would be strong, the writers would often struggle to then find a punchline funny enough to end on, and this would detract from the overall sketch quality. They decided that they would simply not bother to "cap" their sketches in the traditional manner, and early episodes of the "Flying Circus" series make great play of this abandonment of the punchline (one scene has Cleese turn to Idle, as the sketch descends into chaos, and remark that "This is the silliest sketch I've ever been in"—they all resolve not to carry on and simply walk off the set). However, as they began assembling material for the show, the Pythons watched one of their collective heroes, Spike Milligan, whom they had admired on "The Goon Show" (a show the Pythons regard as their biggest influence, which also featured Peter Sellers, whom Cleese called “the greatest voice man of all time”) recording his groundbreaking BBC series "Q..." (1969). Not only was "Q..." more irreverent and anarchic than any previous television comedy, but Milligan also would often "give up" on sketches halfway through and wander off set (often muttering "Did I write this?"). It was clear that their new series would now seem less original, and Jones in particular became determined the Pythons should innovate. Michael Palin recalls "Terry Jones and I adored the "Q..." shows...[Milligan] was the first writer to play with the conventions of television."
After much debate, Jones remembered an animation Gilliam had created for "Do Not Adjust Your Set" called "Beware of the Elephants", which had intrigued him with its stream-of-consciousness style. Jones felt it would be a good concept to apply to the series: allowing sketches to blend into one another. Palin had been equally fascinated by another of Gilliam's efforts, entitled "Christmas Cards", and agreed that it represented "a way of doing things differently". Since Cleese, Chapman, and Idle were less concerned with the overall flow of the programme, Jones, Palin, and Gilliam became largely responsible for the presentation style of the "Flying Circus" series, in which disparate sketches are linked to give each episode the appearance of a single stream-of-consciousness (often using a Gilliam animation to move from the closing image of one sketch to the opening scene of another). The BBC states, “Gilliam's unique animation style became crucial, segueing seamlessly between any two completely unrelated ideas and making the stream-of-consciousness work.”

Writing started at 9 am and finished at 5 pm. Typically, Cleese and Chapman worked as one pair isolated from the others, as did Jones and Palin, while Idle wrote alone. After a few days, they would join together with Gilliam, critique their scripts, and exchange ideas. Their approach to writing was democratic. If the majority found an idea humorous, it was included in the show. The casting of roles for the sketches was a similarly unselfish process, since each member viewed himself primarily as a "writer", rather than an actor eager for screen time. When the themes for sketches were chosen, Gilliam had a free hand in bridging them with animations, using a camera, scissors, and airbrush.
While the show was a collaborative process, different factions within Python were responsible for elements of the team's humour. In general, the work of the Oxford-educated members (Jones and Palin) was more visual, and more fanciful conceptually (e.g., the arrival of the Spanish Inquisition in a suburban front room), while the Cambridge graduates' sketches tended to be more verbal and more aggressive (for example, Cleese and Chapman's many "confrontation" sketches, where one character intimidates or hurls abuse, or Idle's characters with bizarre verbal quirks, such as "The Man Who Speaks In Anagrams"). Cleese confirmed that "most of the sketches with heavy abuse were Graham's and mine, anything that started with a slow pan across countryside and impressive music was Mike and Terry's, and anything that got utterly involved with words and disappeared up any personal orifice was Eric's". Gilliam's animations ranged from the whimsical to the savage (the cartoon format allowing him to create some astonishingly violent scenes without fear of censorship).

Several names for the show were considered before "Monty Python's Flying Circus" was settled upon. Some were "Owl Stretching Time", "The Toad Elevating Moment", "A Horse, a Spoon and a Bucket", "Vaseline Review", and "Bun, Wackett, Buzzard, Stubble and Boot". "Flying Circus" stuck when the BBC explained it had printed that name in its schedules and was not prepared to amend it. Many variations on the name in front of this title then came and went (popular legend holds that the BBC considered "Monty Python's Flying Circus" to be a ridiculous name, at which point the group threatened to change their name every week until the BBC relented). "Gwen Dibley's Flying Circus" was named after a woman Palin had read about in the newspaper, thinking it would be amusing if she were to discover she had her own TV show. "Baron Von Took's Flying Circus" was considered as an affectionate tribute to Barry Took, the man who had brought them together. "Arthur Megapode's Flying Circus" was suggested, then discarded. The name "Baron Von Took's Flying Circus" had the form of "Baron Manfred von Richthofen's Flying Circus" of WWI fame, and the new group was forming in a time when the Royal Guardsmen's 1966 song "Snoopy vs. the Red Baron" had peaked. The term 'flying circus' was also another name for the popular entertainment of the 1920s known as barnstorming, where multiple performers collaborated with their stunts to perform a combined set of acts.

Differing, somewhat confusing accounts are given of the origins of the Python name, although the members agree that its only "significance" was that they thought it sounded funny. In the 1998 documentary "Live at Aspen" during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the group implied that "Monty" was selected (Eric Idle's idea) as a gently mocking tribute to Field Marshal Lord Montgomery, a British general of World War II; requiring a "slippery-sounding" surname, they settled on "Python". On other occasions, Idle has claimed that the name "Monty" was that of a popular and rotund fellow who drank in his local pub; people would often walk in and ask the barman, "Has Monty been in yet?", forcing the name to become stuck in his mind. The name Monty Python was later described by the BBC as being "envisaged by the team as the perfect name for a sleazy entertainment agent".

"Flying Circus" popularised innovative formal techniques, such as the cold open, in which an episode began without the traditional opening titles or announcements. An example of this is the "It's" man: Palin, outfitted in Robinson Crusoe garb, making a tortuous journey across various terrains, before finally approaching the camera to state, "It's ...", only to be then cut off by the title sequence and theme music. On several occasions, the cold open lasted until mid-show, after which the regular opening titles ran. Occasionally, the Pythons tricked viewers by rolling the closing credits halfway through the show, usually continuing the joke by fading to the familiar globe logo used for BBC continuity, over which Cleese would parody the clipped tones of a BBC announcer. On one occasion, the credits ran directly after the opening titles. On the subversive nature of the show (and their subsequent films), Cleese states “anti-authoritarianism was deeply ingrained in Python".
Because of their dislike of finishing with punchlines, they experimented with ending the sketches by cutting abruptly to another scene or animation, walking offstage, addressing the camera (breaking the fourth wall), or introducing a totally unrelated event or character. A classic example of this approach was the use of Chapman's "anti-silliness" character of "the Colonel", who walked into several sketches and ordered them to be stopped because things were becoming "far too silly".

Another favourite way of ending sketches was to drop a cartoonish "16-ton weight" prop on one of the characters when the sketch seemed to be losing momentum, or a knight in full armour (played by Terry Gilliam) would wander on-set and hit characters over the head with a rubber chicken, before cutting to the next scene. Yet another way of changing scenes was when John Cleese, usually outfitted in a dinner suit, would come in as a radio commentator and, in a rather pompous manner, make the formal and determined announcement "And now for something completely different.", which later became the title of the first Monty Python film.

The Python theme music is the Band of the Grenadier Guards' rendition of John Philip Sousa's "The Liberty Bell" which was first published in 1893. Under the Berne Convention's "country of origin" concept, the composition was subject to United States copyright law which states that any work first published prior to 1924 was in the public domain, owing to copyright expiration. This enabled Gilliam to co-opt the march for the series without having to make any royalty payments.
The use of Gilliam's surreal, collage stop motion animations was another innovative intertextual element of the Python style. Many of the images Gilliam used were lifted from famous works of art, and from Victorian illustrations and engravings. The giant foot which crushes the show's title at the end of the opening credits is in fact the foot of Cupid, cut from a reproduction of the Renaissance masterpiece "Venus, Cupid, Folly and Time" by Bronzino. This foot, and Gilliam's style in general, are visual trademarks of the programme.

The Pythons used the British tradition of cross-dressing comedy by donning frocks and makeup and playing female roles themselves while speaking in falsetto. Jones specialised in playing the working-class housewife, or “ratbag old women” as termed by the BBC. Palin and Idle generally played the role more posh, with Idle playing more feminine women. The other members played female roles more sparsely. Generally speaking, female roles were played by women only when the scene specifically required that the character be sexually attractive (although sometimes they used Idle for this). The troupe later turned to Carol Cleveland, who co-starred in numerous episodes after 1970. In some episodes, and later in "Monty Python's Life of Brian", they took the idea one step further by playing women who impersonated men (in the stoning scene).

Many sketches are well-known and widely quoted. "Dead Parrot sketch", "The Lumberjack Song", "Spam" (which led to the coining of the term email spam), "Nudge Nudge", "The Spanish Inquisition", "Upper Class Twit of the Year", "Cheese Shop", "The Ministry of Silly Walks", “Argument Clinic”, “The Funniest Joke in the World” (a sketch referenced in Google Translate), and Four Yorkshiremen sketch” are just a few examples. Most of the show’s sketches satirise areas of public life, such as, Dead Parrot (poor customer service), Silly Walks (bureaucratic inefficiency), Spam (ubiquity of Spam post World War II), Four Yorkshiremen (nostalgic conversations). Featuring regularly in skits, Gumbys (characters of limited intelligence and vocabulary) were part of the Pythons' satirical view of television of the 1970s which condescendingly encouraged more involvement from the “man on the street”.

The Canadian Broadcasting Corporation (CBC) added "Monty Python's Flying Circus" to its national September 1970 fall line-up. They aired the 13 episodes of series 1, which had first run on the BBC the previous autumn (October 1969 to January 1970), as well as the first six episodes of series 2 only a few weeks after they first appeared on the BBC (September to November 1970). The CBC dropped the show when it returned to regular programming after the Christmas 1970 break, choosing to not place the remaining seven episodes of series 2 on the January 1971 CBC schedule. Within a week, the CBC received hundreds of calls complaining of the cancellation, and more than 100 people staged a demonstration at the CBC's Montreal studios. The show eventually returned, becoming a fixture on the network during the first half of the 1970s.
Sketches from "Monty Python's Flying Circus" were introduced to American audiences in August 1972, with the release of the Python film "And Now for Something Completely Different", featuring sketches from series 1 and 2 of the television show. This 1972 release met with limited box office success.

Through the efforts of Python's American manager Nancy Lewis, during the summer of 1974, Ron Devillier, the programme director for nonprofit PBS television station KERA in Dallas, Texas, started airing episodes of "Monty Python's Flying Circus". Ratings shot through the roof, providing an encouraging sign to the other 100 PBS stations that had signed up to begin airing the show in October 1974—exactly five years after their BBC debut. There was also cross-promotion from FM radio stations across the US, whose airing of tracks from the Python LPs had already introduced American audiences to this bizarre brand of comedy. The popularity on PBS resulted in the 1974 re-release of the 1972 "...Completely Different" film, with much greater box office success. The success of the show was captured by a March 1975 article headline in "The New York Times", “Monty Python's Flying Circus Is Barnstorming Here”. Asked what challenges were left, now that they had made TV shows, films, written books, and produced records, Chapman responded, “Well, actually world supremacy would be very nice”, before Idle cautioned, “Yes, but that sort of thing has got to be done properly”.

The ability to show "Monty Python's Flying Circus" under the American NTSC standard had been made possible by the commercial actions of American television producer Greg Garrison. Garrison produced the NBC series "The Dean Martin Comedy World", which ran during the summer of 1974. The concept was to show clips from comedy shows produced in other countries, including tape of the Python sketches "Bicycle Repairman" and "The Dull Life of a Stockbroker". Payment for use of these two sketches was enough to allow Time-Life Films to convert the entire Python library to NTSC standard, allowing for the sale to the PBS network stations which then brought the entire show to US audiences.

In 1975, ABC broadcast two 90-minute "Monty Python" specials, each with three shows, but cut out a total of 24 minutes from each, in part to make time for commercials, and in part to avoid upsetting their audience. As the judge observed in "Gilliam v. American Broadcasting Companies, Inc.", where Monty Python sued for damages caused by broadcast of the mutilated version, "According to the network, appellants should have anticipated that most of the excised material contained scatological references inappropriate for American television and that these scenes would be replaced with commercials, which presumably are more palatable to the American public." Monty Python won the case.

With the popularity of Python throughout the rest of the 1970s and through most of the 1980s, PBS stations looked at other British comedies, leading to UK shows such as "Are You Being Served?" gaining a US audience, and leading, over time, to many PBS stations having a "British Comedy Night" which airs many popular UK comedies.

In 1976, Monty Python became the top rated show in Japan. The literal translation of the Japanese title was "The Gay Dragon Boys Show". The popularity of the show in the Netherlands saw the town of Spijkenisse near Rotterdam open a 'silly walks' road crossing in 2018. Believed to be a world first, the official sign asks pedestrians to cross the road in a comical manner.

Having considered the possibility at the end of the second season, Cleese left the "Flying Circus" at the end of the third. He later explained that he felt he no longer had anything fresh to offer the show, and claimed that only two Cleese- and Chapman-penned sketches in the third series ("Dennis Moore" and the "Cheese Shop") were truly original, and that the others were bits and pieces from previous work cobbled together in slightly different contexts. He was also finding Chapman, who was at that point in the full throes of alcoholism, difficult to work with. According to an interview with Idle, "It was on an Air Canada flight on the way to Toronto, when John (Cleese) turned to all of us and said 'I want out.' Why? I don't know. He gets bored more easily than the rest of us. He's a difficult man, not easy to be friendly with. He's so funny because he never wanted to be liked. That gives him a certain fascinating, arrogant freedom." Jones noted his reticence in 2012, "He was good at it, when he did it he was professional, but he’d rather not have done it. The others all loved it, but he got more and more pissed off about having to come out and do filming, and the one that really swung it, in my view, was when we had to do the day on the Newhaven lifeboat."

The rest of the group carried on for one more "half" season before calling a halt to the programme in 1974. While the first three seasons contained 13 episodes each, the fourth ended after just six. The name "Monty Python's Flying Circus" appears in the opening animation for season four, but in the end credits, the show is listed as simply "Monty Python". Although Cleese left the show, he was credited as a writer for three of the six episodes, largely concentrated in the "Michael Ellis" episode, which had begun life as one of the many drafts of the "Holy Grail" motion picture. When a new direction for "Grail" was decided upon, the subplot of Arthur and his knights wandering around a strange department store in modern times was lifted out and recycled as the aforementioned TV episode. Songwriter Neil Innes contributed to some sketches, including "Appeal on Behalf of Very Rich People".

The Pythons' first feature film was directed by Ian MacNaughton, reprising his role from the television series. It consisted of sketches from the first two seasons of the "Flying Circus", reshot on a low budget (and often slightly edited) for cinema release. Material selected for the film includes: "Dead Parrot", "The Lumberjack Song", "Upper Class Twit of the Year", "Hell's Grannies", "Self-Defence Class", "How Not to Be Seen", and "Nudge Nudge". Financed by "Playboy"s UK executive Victor Lownes, it was intended as a way of breaking Monty Python into America, and although it was ultimately unsuccessful in this, the film did good business in the UK, and later in the US on the "Midnight movie" circuit after their breakthrough television and film success, this being in the era before home video would make the original material much more accessible. The group did not consider the film a success.

In 1974, between production on the third and fourth seasons, the group decided to embark on their first "proper" feature film, containing entirely new material. "Monty Python and the Holy Grail" was based on Arthurian legend and was directed by Jones and Gilliam. Again, the latter also contributed linking animations (and put together the opening credits). Along with the rest of the Pythons, Jones and Gilliam performed several roles in the film, but Chapman took the lead as King Arthur. Cleese returned to the group for the film, feeling that they were once again breaking new ground. "Holy Grail" was filmed on location, in picturesque rural areas of Scotland, with a budget of only £229,000; the money was raised in part with investments from rock groups such as Pink Floyd, Jethro Tull, and Led Zeppelin, as well as UK music industry entrepreneur Tony Stratton-Smith (founder and owner of the Charisma Records label, for which the Pythons recorded their comedy albums).

The backers of the film wanted to cut the famous Black Knight scene (a Sam Peckinpah send-up in which the Black Knight loses his limbs in a duel), but it was eventually kept in the movie. "Tis but a scratch" and "It's just a flesh wound…" are often quoted. "Holy Grail" was selected as the second-best comedy of all time in the ABC special "". and viewers in a Channel 4 poll placed it sixth.

Following the success of "Holy Grail", reporters asked for the title of the next Python film, despite the fact that the team had not even begun to consider a third one. Eventually, Idle flippantly replied "Jesus Christ – Lust for Glory", which became the group's stock answer once they realised that it shut reporters up. However, they soon began to seriously consider a film lampooning the New Testament era in the same way "Holy Grail" had lampooned Arthurian legend. Despite sharing a distrust of organised religion, they agreed not to mock Jesus or his teachings directly. They also mentioned that they could not think of anything legitimate to make fun of about him. Instead, they decided to write a satire on credulity and hypocrisy among the followers of someone [Brian] who had been mistaken for the "Messiah", but who had no desire to be followed as such. Terry Jones adds it was a satire on those who for the next 2,000 years "couldn't agree on what Jesus was saying about peace and love".
The focus therefore shifted to a separate individual, Brian Cohen, born at the same time, and in a neighbouring stable, to Jesus. When Jesus appears in the film (first, as a baby in the stable, and then later on the Mount, speaking the Beatitudes), he is played straight (by actor Kenneth Colley) and portrayed with respect. The comedy begins when members of the crowd mishear his statements of peace, love, and tolerance ("I think he said, 'Blessed are the cheesemakers).

Directing duties were handled solely by Jones, having amicably agreed with Gilliam that Jones' approach to film-making was better suited for Python's general performing style. "Holy Grail's" production had often been stilted by their differences behind the camera. Gilliam again contributed two animated sequences (one being the opening credits) and took charge of set design. The film was shot on location in Tunisia, the finances being provided this time by The Beatles' George Harrison, who together with Denis O'Brien formed the production company Hand-Made Films for the movie. Harrison had a cameo role as the "owner of the Mount".

Despite its subject matter attracting controversy, particularly upon its initial release, it has (together with its predecessor) been ranked among the greatest comedy films. In 2006 it was ranked first on a Channel 4 list of the 50 Greatest Comedy Films. In 2013, Richard Burridge, a theologian decorated by Pope Francis, called "Life of Brian" an "extraordinary tribute to the life and work and teaching of Jesus—that they couldn't actually blaspheme or make a joke out of it. They did a great satire on closed minds and people who follow blindly. Then you have them splitting into factions...it is a wonderful satire on the way that Jesus's own teaching has been used to persecute others. They were satirising fundamentalism and persecution of others and at the same time saying the one person who rises above all this was Jesus".

Filmed at the Hollywood Bowl in Los Angeles during preparations for "The Meaning of Life", this was a concert film (directed by Terry Hughes) in which the Pythons performed sketches from the television series in front of an audience. The released film also incorporated footage from the German television specials (the inclusion of which gives Ian MacNaughton his first on-screen credit for Python since the end of "Flying Circus") and live performances of several songs from the troupe's then-current "Monty Python's Contractual Obligation Album".

The Pythons' final film returned to something structurally closer to the style of "Flying Circus". A series of sketches loosely follows the ages of man from birth to death. Directed again by Jones solo, "The Meaning of Life" is embellished with some of the group's most bizarre and disturbing moments, as well as various elaborate musical numbers, which include "Galaxy Song" (performed by Idle) and "Every Sperm Is Sacred" (performed by Palin and Jones). The film is by far their darkest work, containing a great deal of black humour, garnished by some spectacular violence (including an operation to remove a liver from a living patient without anaesthetic and the morbidly obese Mr. Creosote exploding over several restaurant patrons after finally giving in to the smooth maître d' telling him to eat a mint – "It's only a wafer-thin mint..."). At the time of its release, the Pythons confessed their aim was to offend "absolutely everyone", adding "It is guaranteed to offend".

The Liver Donor scene (which sees someone come to a man's door to take his liver, to which he says: "No, no, I'm not dead", before being told: "Oooh, it doesn't say that on the form"), is a satire on bureaucracy, a common Python trope. Besides the opening credits and the fish sequence, Gilliam, by now an established live-action director, no longer wanted to produce any linking cartoons, offering instead to direct one sketch, "The Crimson Permanent Assurance". Under his helm, though, the segment grew so ambitious and tangential that it was cut from the movie and used as a supporting feature in its own right. (Television screenings also use it as a prologue.) This was the last project on which all six Pythons collaborated, except for the 1989 compilation "Parrot Sketch Not Included," where they are all seen sitting in a closet for four seconds. This was the last time Chapman appeared on screen with the Pythons.

Members of Python contributed their services to charitable endeavours and causes—sometimes as an ensemble, at other times as individuals. The cause that has been the most frequent and consistent beneficiary has been the human rights work of Amnesty International. Between 1976 and 1981, the troupe or its members appeared in four major fund-raisers for Amnesty—known collectively as the "Secret Policeman's Ball" shows—which were turned into multiple films, TV shows, videos, record albums, and books. The brainchild of John Cleese, these benefit shows in London and their many spin-offs raised considerable sums of money for Amnesty, raised public and media awareness of the human rights cause, and influenced many other members of the entertainment community (especially rock musicians) to become involved in political and social issues. Among the many musicians who have publicly attributed their activism—and the organisation of their own benefit events—to the inspiration of the work in this field of Monty Python are Bob Geldof (organiser of Live Aid), U2, Pete Townshend, and Sting. Bono told "Rolling Stone" in 1986, “I saw "The Secret Policeman’s Ball" and it became a part of me. It sowed a seed..." Sting states, “before [the Ball] I did not know about Amnesty, I did not know about its work, I did not know about torture in the world." On the impact of the Ball on Geldof, Sting states, “he took the ‘Ball’ and ran with it.”

"Ball" co-founder Cleese and Jones had an involvement (as performer, writer or director) in all four Amnesty benefit shows, Palin in three, Chapman in two, and Gilliam in one. Idle did not participate in the Amnesty shows. Notwithstanding Idle's lack of participation, the other five members (together with "Associate Pythons" Carol Cleveland and Neil Innes) all appeared together in the first "Secret Policeman's Ball" benefit—the 1976 "A Poke in the Eye" held at Her Majesty's Theatre in London's West End—where they performed several Python sketches. In this first show, they were collectively billed as "Monty Python". Peter Cook deputised for the errant Idle in a courtroom sketch. In the next three shows, the participating Python members performed many Python sketches, but were billed under their individual names rather than under the collective Python banner. The second show featured newcomer Rowan Atkinson and Scottish comedian Billy Connolly. The "Secret Policeman's Ball" were the first stage shows in the UK to present comedic performers (such as Monty Python and Rowan Atkinson) in the same setting and shows as their contemporaries in rock music (which included Eric Clapton, Sting and Phil Collins). After a six-year break, Amnesty resumed producing "Secret Policeman's Ball" benefit shows which were held at the London Palladium in 1987 (sometimes with, and sometimes without, variants of the iconic title) and by 2006 had presented a total of 12 such shows. The shows since 1987 have featured newer generations of British comedic performers, such as Stephen Fry, Hugh Laurie, and puppets from the satirical TV show "Spitting Image", with many attributing their participation in the show to their desire to emulate the Python's pioneering work for Amnesty. Cleese and Palin made a brief cameo appearance in the 1989 Amnesty show; apart from that, the Pythons have not appeared in shows after the first four."

Each member has pursued various film, television, and stage projects since the break-up of the group, but often continued to work with one another. Many of these collaborations were very successful, most notably "A Fish Called Wanda" (1988), written by Cleese, in which he starred along with Palin. The pair also appeared in "Time Bandits" (1981), a film directed by Gilliam, who wrote it together with Palin. Gilliam directed "Jabberwocky" (1977), and also directed and co-wrote "Brazil" (1985), which featured Palin, and "The Adventures of Baron Munchausen" (1988), which featured Idle. "Yellowbeard" (1983) was co-written by Chapman and featured Chapman, Idle, and Cleese, as well as many other English comedians including Peter Cook, Spike Milligan, and Marty Feldman.

Palin and Jones wrote the comedic TV series "Ripping Yarns" (1976–79), starring Palin. Jones also appeared in the pilot episode and Cleese appeared in a nonspeaking part in the episode "Golden Gordon". Jones' film "Erik the Viking" also has Cleese playing a small part. In 1996, Terry Jones wrote and directed an adaptation of Kenneth Grahame's novel "The Wind in the Willows". It featured four members of Monty Python: Jones as Mr. Toad, Idle as Ratty, Cleese as Mr. Toad's lawyer, and Palin as the Sun. Gilliam was considered for the voice of the river. The film included Steve Coogan who played Mole.

In terms of numbers of productions, Cleese has the most prolific solo career, having appeared in dozens of films, several TV shows or series (including "Cheers", "3rd Rock from the Sun", Q's assistant in the James Bond movies, and "Will & Grace"), many direct-to-video productions, some video games, and a number of commercials. His BBC sitcom "Fawlty Towers" (written by and starring Cleese together with his then-wife Connie Booth) is the only comedy series to rank higher than the "Flying Circus" on the BFI TV 100's list, topping the whole poll. Cleese's character, Basil Fawlty, was ranked second (to Homer Simpson) on Channel 4's 2001 list of the 100 Greatest TV Characters.

Idle enjoyed critical success with "Rutland Weekend Television" in the mid-1970s, out of which came the Beatles parody the Rutles (responsible for the cult mockumentary "All You Need Is Cash"), and as an actor in "Nuns on the Run" (1990) with Robbie Coltrane. In 1976, Idle directed music videos for George Harrison songs "This Song" and "Crackerbox Palace", the latter of which also featured cameo appearances from Neil Innes and John Cleese. Idle has had success with Python songs: "Always Look on the Bright Side of Life" went to no. 3 in the UK singles chart in 1991. The song had been revived by Simon Mayo on BBC Radio 1, and was consequently released as a single that year. The theatrical phenomenon of the Python musical "Spamalot" has made Idle the most financially successful of the troupe after Python. Written by Idle (and featuring a pre-recorded cameo of Cleese as the voice of God), it has proved to be an enormous hit on Broadway, London's West End, and Las Vegas. This was followed by "Not the Messiah", which repurposes "The Life of Brian" as an oratorio. For the work's 2007 premiere at the Luminato festival in Toronto (which commissioned the work), Idle himself sang the "baritone-ish" part.

Since "The Meaning of Life", their last project as a team, the Pythons have often been the subject of reunion rumours. In 1988, Monty Python won the BAFTA Award for Outstanding British Contribution To Cinema, with four of the six Pythons (Jones, Palin, Gilliam and Chapman) collecting the award. The final appearance of all six together occurred during the 1989 "Parrot Sketch Not Included – 20 Years of Monty Python" TV special. The death of Chapman in October 1989 put an end to the speculation of any further reunions. Several occasions since 1989 have occurred when the surviving five members have gathered together for appearances—albeit not formal reunions. In 1996, Jones, Idle, Cleese, and Palin were featured in a film adaptation of "The Wind in the Willows", which was later renamed "Mr. Toad's Wild Ride". In 1997, Palin and Cleese rolled out a new version of the “Dead Parrot sketch” for "Saturday Night Live".

Monty Python were the inaugural recipients of the Empire Inspiration Award in 1997. Palin, Jones and Gilliam received the award on stage in London from Elton John while Cleese and Idle appeared via satellite from Los Angeles. In 1998 during the US Comedy Arts Festival, where the troupe were awarded the AFI Star Award by the American Film Institute, the five remaining members, along with what was purported to be Chapman's ashes, were reunited on stage for the first time in 18 years. The occasion was in the form of an interview called "Monty Python Live at Aspen", (hosted by Robert Klein, with an appearance by Eddie Izzard) in which the team looked back at some of their work and performed a few new sketches. On 9 October 1999, to commemorate 30 years since the first "Flying Circus" television broadcast, BBC2 devoted an evening to Python programmes, including a documentary charting the history of the team, interspersed with new sketches by the Monty Python team filmed especially for the event.

The surviving Pythons had agreed in principle to perform a live tour of America in 1999. Several shows were to be linked with Q&A meetings in various cities. Although all had said yes, Palin later changed his mind, much to the annoyance of Idle, who had begun work organising the tour. This led to Idle refusing to take part in the new material shot for the BBC anniversary evening. In 2002, four of the surviving members, bar Cleese, performed "The Lumberjack Song" and "Sit on My Face" for George Harrison's memorial concert. The reunion also included regular supporting contributors Neil Innes and Carol Cleveland, with a special appearance from Tom Hanks.
In an interview to publicise the DVD release of "The Meaning of Life", Cleese said a further reunion was unlikely. "It is absolutely impossible to get even a majority of us together in a room, and I'm not joking," Cleese said. He said that the problem was one of busyness rather than one of bad feelings. A sketch appears on the same DVD spoofing the impossibility of a full reunion, bringing the members "together" in a deliberately unconvincing fashion with modern bluescreen/greenscreen techniques.

Idle responded to queries about a Python reunion by adapting a line used by George Harrison in response to queries about a possible Beatles reunion. When asked in November 1989 about such a possibility, Harrison responded: "As far as I'm concerned, there won't be a Beatles reunion as long as John Lennon remains dead." Idle's version of this was that he expected to see a proper Python reunion, "just as soon as Graham Chapman comes back from the dead", but added, "we're talking to his agent about terms."

"The Pythons Autobiography by The Pythons" (2003), compiled from interviews with the surviving members, reveals that a series of disputes in 1998, over a possible sequel to "Holy Grail" that had been conceived by Idle, may have resulted in the group's split. Cleese's feeling was that "The Meaning of Life" had been personally difficult and ultimately mediocre, and did not wish to be involved in another Python project for a variety of reasons (not least amongst them was the absence of Chapman, whose straight man-like central roles in the "Grail" and "Brian" films had been considered to be an essential anchoring performance). The book also reveals that Cleese saw Chapman as his “greatest sounding board. If Graham thought something was funny, then it almost certainly was funny. You cannot believe how invaluable that is.' Ultimately it was Cleese who ended the possibility of another Python movie.

A full, if nonperforming, reunion of the surviving Python members appeared at the March 2005 premiere of Idle's musical "Spamalot", based on "Monty Python and the Holy Grail". It opened in Chicago and has since played in New York on Broadway, London, and numerous other major cities across the world. In 2004, it was nominated for 14 Tony Awards and won three: Best Musical, Best Direction of a Musical for Mike Nichols, and Best Performance by a Featured Actress in a Musical for Sara Ramirez, who played the Lady of the Lake, a character specially added for the musical. The original Broadway cast included Tim Curry as King Arthur, Michael McGrath as Patsy, David Hyde Pierce as Sir Robin, Hank Azaria as Sir Lancelot and other roles (e.g., the French Taunter, Knight of Ni, and Tim the Enchanter), Christopher Sieber as Sir Galahad and other roles (e.g., the Black Knight and Prince Herbert's Father). Cleese played the voice of God, a role played in the film by Chapman.

Owing in part to the success of "Spamalot", PBS announced on 13 July 2005 that it would begin to re-air the entire run of "Monty Python's Flying Circus" and new one-hour specials focusing on each member of the group, called "Monty Python's Personal Best". Each episode was written and produced by the individual being honoured, with the five remaining Pythons collaborating on Chapman's programme, the only one of the editions to take on a serious tone with its new material.

In 2009, to commemorate the 40th anniversary of the first episode of "Monty Python's Flying Circus", a six-part documentary entitled "" was released, featuring interviews with the surviving members of the team, as well as archive interviews with Graham Chapman and numerous excerpts from the television series and films. Each episode opens with a different re-recording of the theme song from "Life of Brian", with Iron Maiden vocalist and Python fan Bruce Dickinson performing the sixth.

Also in commemoration of the 40th anniversary, Idle, Palin, Jones, and Gilliam appeared in a production of "Not the Messiah" at the Royal Albert Hall. The European premiere was held on 23 October 2009. An official 40th anniversary Monty Python reunion event took place in New York City on 15 October 2009, where the team received a Special Award from the British Academy of Film and Television Arts.

In June 2011, it was announced that "", an animated 3D movie based on the memoir of Graham Chapman, was in the making. The memoir "A Liar's Autobiography" was published in 1980 and details Chapman's journey through medical school, alcoholism, acknowledgement of his gay identity, and the tolls of surreal comedy. Asked what was true in a deliberately fanciful account by Chapman of his life, Terry Jones joked: "Nothing ... it's all a downright, absolute, blackguardly lie." The film uses Chapman's own voice—from a reading of his autobiography shortly before he died of cancer—and entertainment channel Epix announced the film's release in early 2012 in both 2D and 3D formats. Produced and directed by London-based Bill Jones, Ben Timlett, and Jeff Simpson, the new film has 15 animation companies working on chapters that will range from three to 12 minutes in length, each in a different style. John Cleese recorded dialogue which was matched with Chapman's voice. Michael Palin voiced Chapman's father and Terry Jones voiced his mother. Terry Gilliam voiced Graham's psychiatrist. They all play various other roles. Among the original Python group, only Eric Idle was not involved.

On 26 January 2012, Terry Jones announced that the five surviving Pythons would reunite in a sci-fi comedy film called "Absolutely Anything". The film would combine computer-generated imagery and live action. It would be directed by Jones based on a script by Jones and Gavin Scott, and in addition to the Python members it would also star Simon Pegg, Kate Beckinsale and Robin Williams (in his final film role). The plot revolves around a teacher who discovers aliens (voiced by the Pythons) have given him magical powers to do "absolutely anything". Eric Idle responded via Twitter that he would not, in fact, be participating, although he was later added to the cast.

In 2013, the Pythons lost a legal case to Mark Forstater, the film producer of "Monty Python and the Holy Grail", over royalties for the derivative work "Spamalot". They owed a combined £800,000 in legal fees and back royalties to Forstater. They proposed a reunion show to pay their legal bill.

On 19 November 2013, a new reunion was reported, following months of "secret talks". The original plan was for a live, one-off stage show at the O Arena in London on 1 July 2014, with "some of Monty Python's greatest hits, with modern, topical, Pythonesque twists" according to a press release. The tickets for this show went on sale in November 2013 and sold out in just 43 seconds. Nine additional shows were added, all of them at the O, the last on 20 July. They have said that their reunion was inspired by "South Park" creators Trey Parker and Matt Stone, who are massive Monty Python fans.

Mick Jagger featured in a promotional video for the shows: “Who wants to see that again, really? It's a bunch of wrinkly old men trying to relive their youth and make a load of money—the best one died years ago!" Michael Palin stated that the final reunion show on 20 July 2014 would be the last time that the troupe would perform together. It was screened to 2,000 cinemas around the world. Prior to the final night, Idle stated, “It is a world event and that’s really quite exciting. It means we’re actually going to say goodbye publicly on one show. Nobody ever has the chance to do that. The Beatles didn’t get a last good night.” The last show was broadcast in the UK on Gold TV and internationally in cinemas by Fathom Events through a Dish Network satellite link.

Graham Chapman was originally a medical student, joining the Footlights at Cambridge. He completed his medical training and was legally entitled to practise as a doctor. Chapman is best remembered for the lead roles in "Holy Grail", as King Arthur, and "Life of Brian", as Brian Cohen. He died of metastatic throat cancer on 4 October 1989. At Chapman's memorial service, Cleese delivered an irreverent eulogy that included all the euphemisms for being dead from the "Dead Parrot" sketch, which they had written. Chapman's comedic fictional memoir, "A Liar's Autobiography", was adapted into an animated 3D movie in 2012.

John Cleese is the oldest Python. He met his future Python writing partner, Chapman, in Cambridge. Outside of Python, he is best known for setting up the Video Arts group and for the sitcom "Fawlty Towers" (co-written with Connie Booth, whom Cleese met during work on Python and to whom he was married for a decade). Cleese has also co-authored several books on psychology and wrote the screenplay for the award-winning "A Fish Called Wanda", in which he starred with Michael Palin.

Terry Gilliam, an American by birth, is the only member of the troupe of non-British origin. He started off as an animator and strip cartoonist for Harvey Kurtzman's "Help!" magazine, one issue of which featured Cleese. Moving from the US to England, he animated features for "Do Not Adjust Your Set" and was then asked by its makers to join them on their next project: "Monty Python's Flying Circus". He co-directed "Monty Python and the Holy Grail" and directed short segments of other Python films (for instance "The Crimson Permanent Assurance", the short film that appears before "The Meaning of Life").
When Monty Python was first formed, two writing partnerships were already in place: Cleese and Chapman, Jones and Palin. That left two in their own corners: Gilliam, operating solo due to the nature of his work, and Eric Idle. Regular themes in Idle's contributions were elaborate wordplay and musical numbers. After "Flying Circus", he hosted "Saturday Night Live" four times in the first five seasons. Idle's initially successful solo career faltered in the 1990s with the failures of his 1993 film "Splitting Heirs" (written, produced by, and starring him) and 1998's "" (in which he starred). He revived his career by returning to the source of his worldwide fame, adapting Monty Python material for other media. Idle wrote the Tony Award-winning musical "Spamalot", based on "Holy Grail". Following the success of the musical he wrote "Not the Messiah", an oratorio derived from the "Life of Brian". Representing Monty Python, Idle featured in a one-hour symphony of British Music when he performed at the London 2012 Olympic Games closing ceremony.

Terry Jones has been described by other members of the team as the "heart" of the operation. Jones had a lead role in maintaining the group's unity and creative independence. Python biographer George Perry has commented that should "[you] speak to him on subjects as diverse as fossil fuels, or Rupert Bear, or mercenaries in the Middle Ages or Modern China ... in a moment you will find yourself hopelessly out of your depth, floored by his knowledge." Many others agree that Jones is characterised by his irrepressible, good-natured enthusiasm. However, Jones' passion often led to prolonged arguments with other group members—in particular Cleese—with Jones often unwilling to back down. Since his major contributions were largely behind the scenes (direction, writing), and he often deferred to the other members of the group as an actor, Jones' importance to Python was often under-rated. However, he does have the legacy of delivering possibly the most famous line in all of Python, as Brian's mother Mandy in "Life of Brian", "He's not the Messiah, he's a very naughty boy!", a line voted the funniest in film history on two occasions. Jones died on 21 January 2020 from complications of dementia.

Sir Michael Palin attended Oxford, where he met his Python writing partner Jones. The two also wrote the series "Ripping Yarns" together. Palin and Jones originally wrote face-to-face, but soon found it was more productive to write apart and then come together to review what the other had written. Therefore, Jones and Palin's sketches tended to be more focused than that of the others, taking one bizarre situation, sticking to it, and building on it. After "Flying Circus", Palin hosted "Saturday Night Live" four times in the first 10 seasons. His comedy output began to decrease in amount following the increasing success of his travel documentaries for the BBC. Palin released a book of diaries from the Python years entitled "Michael Palin Diaries 1969–1979", published in 2007. Palin was awarded a knighthood in the 2019 New Year Honours, which was announced by Buckingham Palace in December 2018.

Several people have been accorded unofficial "associate Python" status over the years. Occasionally such people have been referred to as the 'seventh Python', in a style reminiscent of George Martin (or other associates of the Beatles) being dubbed "the Fifth Beatle". The two collaborators with the most meaningful and plentiful contributions have been Neil Innes and Carol Cleveland. Both were present and presented as Associate Pythons at the official Monty Python 25th-anniversary celebrations held in Los Angeles in July 1994.

Neil Innes is the only non-Python besides Douglas Adams to be credited with writing material for "Flying Circus". He appeared in sketches and the Python films, as well as performing some of his songs in "Monty Python Live at the Hollywood Bowl". He was also a regular stand-in for absent team members on the rare occasions when they recreated sketches. For example, he took the place of Cleese at the Concert for George. Gilliam once noted that if anyone qualified for the title of the seventh Python, it would be Innes. He was one of the creative talents in the off-beat Bonzo Dog Band. He would later portray Ron Nasty of the Rutles and write all of the Rutles' compositions for "All You Need Is Cash" (1978), a mockumentary film co-directed by Idle. By 2005, a falling out had occurred between Idle and Innes over additional Rutles projects, the results being Innes' critically acclaimed Rutles "reunion" album "The Rutles: Archaeology" and Idle's straight-to-DVD "The Rutles 2: Can't Buy Me Lunch", each undertaken without the other's participation. According to an interview with Idle in the "Chicago Tribune" in May 2005, his attitude is that Innes and he go back "too far. And no further." Innes died of a heart attack on 29 December 2019 near Toulouse, where he had lived for several years.

Carol Cleveland was the most important female performer in the Monty Python ensemble, commonly referred to as "the female Python". She was originally hired by producer/director John Howard Davies for just the first five episodes of the "Flying Circus". The Pythons then pushed to make Cleveland a permanent recurring performer after producer/director Ian MacNaughton brought in several other actresses who were not as good as she was. Cleveland went on to appear in about two-thirds of the episodes, as well as in all of the Python films, and in most of their stage shows, as well. According to "Time", her most recognisable film roles are playing Zoot and Dingo, two maidens in the Castle Anthrax in "Holy Grail".

Cleese's first wife, Connie Booth, appeared as various characters in all four series of "Flying Circus". Her most significant role was the "best girl" of the eponymous Lumberjack in "The Lumberjack Song", though this role was sometimes played by Carol Cleveland. Booth appeared in a total of six sketches and also played one-off characters in Python feature films "And Now for Something Completely Different" and "Monty Python and the Holy Grail".

Douglas Adams was "discovered" by Chapman when a version of "Footlights Revue" (a 1974 BBC2 television show featuring some of Adams' early work) was performed live in London's West End. In Cleese's absence from the final TV series, the two formed a brief writing partnership, with Adams earning a writing credit in one episode for a sketch called "Patient Abuse". In the sketch—a satire on mind-boggling bureaucracy—a man who had been stabbed by a nurse arrives at his doctor's office bleeding profusely from the stomach, when the doctor makes him fill in numerous senseless forms before he can administer treatment. He also had two cameo appearances in this season. Firstly, in the episode "The Light Entertainment War", Adams shows up in a surgeon's mask (as Dr. Emile Koning, according to the on-screen captions), pulling on gloves, while Palin narrates a sketch that introduces one person after another, and never actually gets started. Secondly, at the beginning of "Mr. Neutron", Adams is dressed in a "pepperpot" outfit and loads a missile onto a cart being driven by Terry Jones, who is calling out for scrap metal ("Any old iron ..."). Adams and Chapman also subsequently attempted a few non-Python projects, including "Out of the Trees". He also contributed to a sketch on the soundtrack album for "Monty Python and the Holy Grail".

Other than Carol Cleveland, the only other non-Python to make a significant number of appearances in the "Flying Circus" was Ian Davidson. He appeared in the first two series of the show, and played over 10 roles. While Davidson is primarily known as a scriptwriter, it is not known if he had any contribution toward the writing of the sketches, as he is only credited as a performer. In total, Davidson is credited as appearing in eight episodes of the show, which is more than any other male actor who was not a Python. Despite this, Davidson did not appear in any Python-related media subsequent to series 2, though footage of him was shown on the documentary "Python Night – 30 Years of Monty Python".

Stand-up comedian Eddie Izzard, a devoted fan of the group, has occasionally stood in for absent members. When the BBC held a "Python Night" in 1999 to celebrate 30 years of the first broadcast of "Flying Circus", the Pythons recorded some new material with Izzard standing in for Idle, who had declined to partake in person (he taped a solo contribution from the US). Izzard hosted "The Life of Python" (1999), a history of the group that was part of Python Night and appeared with them at a festival/tribute in Aspen, Colorado, in 1998 (released on DVD as "Live at Aspen"). Izzard has said that Monty Python was a significant influence on his style of comedy and Cleese has referred to him as "the lost Python".

Series director of "Flying Circus", Ian MacNaughton, is also regularly associated with the group and made a few on-screen appearances in the show and in the film "And Now for Something Completely Different". Apart from Neil Innes, others to contribute musically included Fred Tomlinson and the Fred Tomlinson Singers. They made appearances in songs such as "The Lumberjack Song" as a backup choir. Other contributors and performers for the Pythons included John Howard Davies, John Hughman, Lyn Ashley, Bob Raymond, John Young, Rita Davies, Stanley Mason, Maureen Flanagan, and David Ballantyne.

Monty Python in films

Monty Python live

Monty Python reunions

By the time of Monty Python's 25th anniversary, in 1994, the point was already being made that "the five surviving members had with the passing years begun to occupy an institutional position in the edifice of British social culture that they had once had so much fun trying to demolish". A similar point is made in a 2006 book on the relationship between Python and philosophy: "It is remarkable, after all, not only that the utterly bizarre "Monty Python's Flying Circus" was sponsored by the BBC in the first place, but that Monty Python itself grew into an institution of enormous cultural influence."

A self-contained comedy unit responsible for both writing and performing their work, Monty Python's influence on comedy has been compared to the Beatles' influence on music. Author Neil Gaiman writes, “A strange combination of individuals gave us Python. And you needed those people, just in the same way that with the Beatles you had four talented people, but together you had the Beatles. And I think that's so incredibly true when it comes to Python.”

Monty Python have been named as being influential to the comedy stylings of a great many people including: Sacha Baron Cohen, David Cross, Rowan Atkinson, Seth MacFarlane, Seth Meyers, Trey Parker, Matt Stone, Vic and Bob, Mike Myers, and "Weird Al" Yankovic. Matt Groening, creator of "The Simpsons", was influenced by Python's "high velocity sense of the absurd and not stopping to explain yourself", and pays tribute through a couch gag used in seasons five and six. Appearing on "Monty Python's Best Bits (Mostly)", Jim Carrey—who refers to Monty Python as the "Super Justice League of comedy"—recalled the effect on him of Ernest Scribbler (played by Palin) laughing himself to death in "The Funniest Joke in the World" sketch.

Comedian John Oliver states, "Writing about the importance of Monty Python is basically pointless. Citing them as an influence is almost redundant. It's assumed. This strange group of wildly talented, appropriately disrespectful, hugely imaginative and massively inspirational idiots changed what comedy could be for their generation and for those that followed." On how Python's freeform style influenced sketch comedy, Tina Fey of the US television show "Saturday Night Live" states, "Sketch endings are overrated. Their key was to do something as long as it was funny and then just stop and do something else."


Among the more visible cultural influences of Monty Python is the inclusion of terms either directly from, or derived from, Monty Python, into the lexicon of the English language.

The Japanese anime series, "Girls und Panzer", featured the special episode, "Survival War!", which referenced the 'Spam' sketch.

Beyond a dictionary definition, Python terms have entered the lexicon in other ways.


On St George's Day, 23 April 2007, the cast and creators of "Spamalot" gathered in Trafalgar Square under the tutelage of the two Terrys (Jones and Gilliam) to set a new record for the world's largest coconut orchestra. They led 5,567 people "clip-clopping" in time to the Python classic, "Always Look on the Bright Side of Life", for the "Guinness World Records" attempt.

On 5 October 2019, to mark the 50th anniversary of Monty Python's first show, the "first official Monty Python Guinness world record attempt" tried to break the record for "the largest gathering of people dressed as Gumbys." A recurring character on the show, a Gumby wears a handkerchief on their head, has spectacles, braces, a knitted tank top, and wellington boots. The shirt sleeves and trouser legs are always rolled up, exposing their socks and knees. Dimwitted, their most famous catchphrases are "My brain hurts!" and repeated shouts of "Hello!" and "Sorry!".


Five Monty Python productions were released as theatrical films:









</doc>
<doc id="18943" url="https://en.wikipedia.org/wiki?curid=18943" title="Married... with Children">
Married... with Children

Married... with Children is an American television sitcom that aired on Fox, created by Michael G. Moye and Ron Leavitt. Originally broadcast from April 5, 1987 to June 9, 1997, it is the longest-lasting live-action sitcom on Fox and the first to be broadcast in the network's primetime slot. One episode was broadcast on FX on June 18, 2002.

The show follows the suburban Chicago lives of Al Bundy, a once-glorious high school football player-turned-hard-luck women's shoe salesman; his lazy wife, Peggy; their attractive, dumb and popular daughter, Kelly; and their smart, horny and unpopular son, Bud. Their neighbors are the upwardly mobile Steve Rhoades and his feminist wife Marcy, who later gets remarried to Jefferson D'Arcy, a white-collar criminal who becomes her "trophy husband" and Al's sidekick. Most storylines involve Al's schemes being foiled by his own cartoonish dim wit and bad luck.

The series comprises 259 episodes and 11 seasons. Its theme song is "Love and Marriage" by Sammy Cahn and Jimmy Van Heusen, performed by Frank Sinatra from the 1955 television production "Our Town".

The first two seasons of the series were videotaped at ABC Television Center in Hollywood. From season three to season eight, the show was taped at Sunset Gower Studios in Hollywood, and the remaining three seasons were taped at Sony Pictures Studios in Culver City. The series was produced by Embassy Communications during its first season and half of its second season and the remaining seasons by ELP Communications under the studio Columbia Pictures Television.

In 2008, the show placed number 94 on "Entertainment Weekly" "New TV Classics" list.


In the show's pilot episode, Tina Caspary played the role of Kelly Bundy, while Hunter Carson played Bud. Before the series aired publicly and at the behest of Ed O'Neill the roles for the two Bundy children were re-cast. O'Neill felt a lack of chemistry with the original actors cast as the children. He requested a re-cast, which the producers approved. All of the scenes in the original pilot were re-shot with the replacement actors, Christina Applegate and David Faustino.


On April 22, 2012, Fox reaired the series premiere in commemoration of its 25th anniversary.

During its 11-season run on the Fox network, "Married... with Children" aired 258 episodes. A 259th episode, "I'll See You in Court" from season 3, never aired on Fox (see below), but premiered on FX and has since been included on DVD and in syndication packages. The episode counts in the chart below. Three specials also aired following the series' cancellation, including a cast reunion.

Despite the show's enduring popularity and fanbase, "Married... with Children" was never a huge ratings success. Part of the reason was the fact that Fox, being a new startup network, did not have the affiliate base of the Big Three television networks, thus preventing the series from reaching the entire country. In an interview for a special commemorating the series' 20-year anniversary in 2007, Katey Sagal stated that part of the problem the series faced was that many areas of the country were able to get Fox only through low-quality UHF channels well into the early 1990s, while some areas of the country did not receive the new network at all, a problem not largely rectified until the launch of Foxnet in June 1991 and later the 1994 United States broadcast TV realignment which brought the NFC football rights to the network.

Another problem lays in the fact that many of the newly-developed series on Fox were unsuccessful, which kept the network from building a popular lineup to draw in a larger audience. In its original airing debut, "Married... with Children" was part of a Sunday lineup that competed with the popular "Murder, She Wrote" and Sunday-night movie on CBS. Fellow freshman series included "Duet", cancelled in 1989, along with "It's Garry Shandling's Show" and "The Tracey Ullman Show", both of which were canceled in 1990. The success of "The Simpsons", which debuted on "The Tracey Ullman Show" in 1987, helped draw some viewers over to Fox, allowing "Married... with Children" to sneak into the top 50 of television shows for seasons 3 through 9 doing its best overall rating at number 8 for its third and tenth season. Although these ratings were somewhat small in comparison with the other three networks, they were good enough for Fox to keep renewing the show.

Ratings data for some seasons courtesy of TVTango.com.

In 1989, Terry Rakolta, from Bloomfield Hills, Michigan, led a boycott of the show after viewing the episode "Her Cups Runneth Over". Offended by the images of an old man wearing a woman's garter and stockings, the scene where Steve touches the pasties of a mannequin dressed in S&M gear, a homosexual man wearing a tiara on his head (and Al's line "...and they wonder why we call them 'queens'"), and a half-nude woman who takes off her bra in front of Al (and is shown with her arms covering her bare chest in the next shot), Rakolta began a letter-writing campaign to advertisers, demanding they boycott the show.

After advertisers began dropping their support for the show and while Rakolta made several appearances on television talk shows demanding the show's cancellation, Fox executives refused to air the episode titled "I'll See You in Court". This episode would become known as the "Lost Episode" and was aired on FX on June 18, 2002, with some parts cut. The episode was packaged with the rest of the third season in the January 2005 DVD release (and in the first volume of the "Married ... With Children Most Outrageous Episode" DVD set) with the parts cut from syndication restored.

Ironically, viewers' curiosity over the boycott and over the show itself led to a drastic ratings boost in an example of the Streisand Effect, which Rakolta has since acknowledged. She has been referenced twice on the show: "Rock and Roll Girl" when a newscaster mentioned the city Bloomfield Hills, and "No Pot to Pease In", when a television show was made about the Bundy family and then was cancelled because (according to Marcy) "some woman in Michigan didn't like it".

The conservative Parents Television Council named "Married... with Children" the worst show of both the 1995–96 and 1996–97 television seasons in its first two years in operation. In 1996, the organization called the show the "crudest comedy on prime time television...peppered with lewd punch lines about sex, masturbation, the gay lifestyle and the lead character's fondness for pornographic magazines and strip clubs."

Sony Pictures Home Entertainment has released all 11 seasons of "Married... with Children" on DVD in Regions 1, 2, & 4. On December 12, 2010, Sony released a complete series set on DVD in Region 1.

In December 2007, the Big Bundy Box—a special collection box with all seasons plus new interviews with Sagal and David Faustino—was released. This boxset was released in Australia (Region 4) on November 23, 2009.

The Sony DVD box sets from season 3 onward do not feature the original "Love and Marriage" theme song in the opening sequence. This was done because Sony was unable to obtain the licensing rights to the song for later sets. Despite this, the end credits on the DVDs for season 3 still include a credit for "Love and Marriage."

On August 27, 2013, it was announced that Mill Creek Entertainment had acquired the home media rights to various television series from the Sony Pictures library including "Married... with Children" with the original theme song "Love and Marriage" sung by Frank Sinatra. They have subsequently re-released the 11 seasons on DVD. The Mill Creek Entertainment version (along with the versions available for streaming and downloading) include scenes that are normally edited in syndication and most of the licensed music that's dubbed over or deleted due to copyright issues. A complete series DVD set was re-released on July 7, 2015. All seasons of Married with Children are now available for online download and streaming through Amazon, Apple iTunes, Hulu, and Vudu.

"The Complete "Married... with Children" Book: TV's Dysfunctional Family Phenomenon", Bear Manor Media, August 2017, 

"Married... with Children" was adapted into a comic book series by NOW Comics in 1990.


Two series (10 in all) of 8" action figures were produced by Classic TV Toys in 2005 and 2006.
In 2018, Funko produced figures of Al, Kelly, Bud and Peggy as apart of their Funko POP! line.
That same year, Funko also released a "Married... with Children" box set as a Comic Con Exclusive. It included retro-styled Al, Peggy, Kelly and Bud action figures.

In 2018 and 2019, Mego released Target exclusives of Al, Peggy and Kelly in 1/9 scale.

An Armenian remake was made in 2016, called "The Azizyans". The Azizyans is an Armenian sitcom television series developed by Robert Martirosyan and Van Grigoryan. The series premiered on Armenia TV on October 31, 2016. However, the series was not available to the public until Armenia TV started airing the sitcom from October 10, 2017. The series takes place in Yerevan, Armenia. The Azizyans sitcom is starred by Hayk Marutyan. He embodies the character of Garnik Azizyan – a clothes store seller, who is the only one working in the family. Mrs. Ruzan Azizyan is lazy enough to perform the duties of a housewife. The problems of the father of the family don’t bother his 3 children – his daughter, who is internet-addicted and is active in all social networks; his unemployed eldest son, who is a complete loser, and his youngest son, who is a schoolboy. The roles in this sitcom, created for family watching, are played by Ani Lupe, Satenik Hazaryan, Ishkhan Gharibyan, Suren Arustamyan and other popular Armenian actors. The project is directed by Arman Marutyan. In the second season of the sitcom, the Azizyan family continues to survive thanks to the meager salary of Garnik. The wife of Garnik - Ruzan, remains in the status of a housewife, without even thinking about finding a job. The elder son of Garnik and Ruzan - Azat, continues to look for a new job, a young man appears in the life of Marie, who is trying to win the girl's heart. Their younger son Levon, continues to live his own life and does not understand what he has in common with this family. And their neighbors Irina and Alik continue to be friends with the family, which Azizyans do not quite approve. The only bright spot in the life of the family is their house, which Garnik inherited from his grandfather.

An Argentine remake was made in 2005, called "Casados con Hijos". The series was also shown by local channels in Uruguay, Paraguay, and Peru. Only two seasons were made (2005 and 2006), but it is still aired Monday through Friday at 2 pm and Saturday at 11:30 pm by Telefe.

The character names are: José "Pepe" Argento (based on Al, played by Guillermo Francella), Mónica "Moni" Argento (based on Peggy, played by Florencia Peña), Paola Argento (based on Kelly, played by Luisana Lopilato), Alfio "Coqui" Argento (based on Bud, played by Darío Lopilato), Dardo and María Elena Fuseneco (based on Marcy and Jefferson D'Arcy, played by Marcelo de Bellis and Érica Rivas).

In Brazil Rede Bandeirantes made a remake in 1999 with the name "A Guerra dos Pintos" (The War of The Pintos). 52 episodes were recorded but only 22 aired before cancelation.

In Bulgaria a remake is aired from March 26, 2012 with the name "Женени с деца в България" (Zheneni s detsa v Bulgaria) (Married with children in Bulgaria).

In Croatia a remake called "Bračne vode" was broadcast from September 2008 until November 2009 on Nova TV channel. The characters based on the Bundys were called Zvonimir, Sunčica, Kristina and Boris Bandić while the ones based on Marcy and Steve were called Marica and Ivan Kumarica.

In Germany, the 1992 remake "Hilfe, meine Familie spinnt", broadcast in the prime time, reached double the audience than the original (broadcast in the early fringe time). This, however, was not enough to maintain the series, so it was cancelled after one season. The remake used the exact translated scripts of the original series (which already substituted localised humour and in-jokes for incomprehensible references to American TV shows not shown in Germany, as well as some totally different jokes) and just renamed placed and people according to the new setting.

" was aired from March to December 1993 for 26 episodes.

In 2006, Hungarian TV network TV2 purchased the license rights including scripts and hired the original producers from Sony Pictures for a remake of the show placed in a Hungarian environment. It was entitled " (in English: "Married with children in Budapest", loan translation: "A gruesomely decent family in Budapest"). The main story began with the new family called the Bándis inheriting an outskirt house from their American relatives the Bundys. They filmed a whole season of 26 episodes, all of them being remade versions of the plots of the original first seasons. It was the highest budget sitcom ever made in Hungary. First it was aired on Tuesday nights, but was beaten by a new season of "ER", then placed to Wednesday nights. The remake lost its viewers, but stayed on the air due to the contract between Sony and TV2.

The complete American series aired in Israel in the 1990s, with reruns of it ever since. There has also been an Israeli remake to the show titled "Nesuim Plus" (Married Plus) that aired its two seasons from 2012 to 2017.


The Original "Married... With Children" ran on TV-6 Russia in the late 1990s and early 2000s (before the closing of the channel) in prime-time basis, broadcasting the episodes from seasons 1–10. The show later aired on DTV and Domashniy TV. However, for unknown reasons, most episodes from season 11 were not shown. A Russian adaptation, titled "Happy Together" (Schastlivy Vmeste; "Happy Together"), was broadcast on TNT across the country.

The character names are: Gena Bukin (based on Al, played by Viktor Loginov), Dasha Bukina (based on Peggy, played by Natalya Bochkareva), Sveta Bukina (based on Kelly, played by Darya Sagalova), Roma Bukin (based on Bud, played by Alexander Yakin), Elena and Anatoliy Poleno (based on Marcy and Jefferson D'Arcy, played by Yulia Zaharova and Pavel Savinkov), Evgeniy Stepanov (based on Steve Rhoades, played by Aleksey Sekirin), Sema Bukin (based on Seven, played by Ilya Butkovskiy), and Baron Bukin (based on Buck and Lucky, played by Bayra).


ITV had been screening the original "Married... With Children" since 1988. In 1996, the UK production company Central Television and Columbia Pictures Television (Columbia TriStar Central Productions) produced a UK version called Married for Life, which lasted for one series with seven episodes.

"Top of the Heap" was a sitcom starring Matt LeBlanc. The show was about Vinnie Verducci (played by LeBlanc) and his father Charlie (played by Joseph Bologna) always trying get rich quick schemes. The Verduccis were introduced in an earlier episode where Vinnie dated Kelly Bundy, and Charlie was introduced as an old friend of Al Bundy's. The end of the pilot episode shows Al breaking into their apartment and stealing their TV to replace the one he lost betting on Vinnie in a boxing match. However, the show didn't last long and was ultimately cancelled. It had its own spin-off/sequel called "Vinnie & Bobby" a year later, which was also cancelled.

Also, an attempt was made to make a spin-off out of David Garrison's Steve Rhoades character which took place on Bud's Trumaine University called "Radio Free Trumaine" where Garrison played the Dean. "Enemies" was another spin-off, but played to be a spoof on the TV series, "Friends".

On September 11, 2014, it was announced that a spin-off was in the works, centered on the character of Bud Bundy.

Distributed by Columbia Pictures Television Distribution (now Sony Pictures Television Distribution), "Married... with Children" debuted in off-network syndication in the fall of 1991. The series later began airing on cable on FX from September 1998 until 2007. In June 2002, FX became the first television network to air the controversial, previously banned episode "I'll See You in Court", albeit in an edited format. The fully uncensored version of "I'll See You in Court" can only be seen on the DVD release "Married... with Children: The Most Outrageous Episodes Volume 1" and the Mill Creek Entertainment complete series collection. The version found on the Third Season DVD set under Sony is the edited-for-TV version. In 2008, the Spike network reportedly paid US$12 million for broadcast rights to every episode including the unedited version of the infamous episode, "I'll See You in Court".

The series started airing on Spike TV on September 29, 2008 with a weeklong marathon. TBS also began airing the show shortly after, acquiring the show in fall 2008 to run in the early morning hours. Through late September 2018 it ran for two to three hours (on rare occasions four or five) on TBS during the early morning hours (depending on the length of overnight programming) before TBS dropped it from their lineup. TV Land picked up the rights to broadcast the show from its MTV Networks sister Spike in August 2009. Comedy Central began airing the show on February 8, 2010; Comedy Central acquired rights to air the series from TV Land, who in turn, had earlier acquired the rights to the series from Spike, though Comedy Central dropped the rights to the series in April 2010. Spike picked up the rights to the series again, and began airing the series for the second time on July 10, 2010, airing on weekend mornings only. All three cable channels are owned by Viacom. The comedy began airing on Nick at Nite on July 6, 2011. MTV2 added the series on March 21, 2012 and VH1 Classic began airing the series on April 9, 2012. The series has aired on a total of seven MTV Networks owned cable networks since 2008. It previously aired on Antenna TV, Ion Television, and TBS. The series currently airs on GetTV, Logo TV and regularly on WGN America. WGN America gained rights to the show when TBS removed it from their early morning slots in September 2018. In November 2018, the entire 11-season run became available to watch through Hulu.

"Married...with Children" has also been a ratings success in other countries around the world.
The opening footage comprises views of Chicago, opening with a shot of Buckingham Fountain in Grant Park. The aerial downtown shot was taken from the Lake Shore Drive section north of the Loop. The expressway entrance shot was taken from the 1983 movie "National Lampoon's Vacation" featuring the Griswolds' green family truckster with a northeastward view of the Dan Ryan/Stevenson junction southwest of the Loop. The exterior shot used for the Bundy's house was taken in a subdivision in Batavia, Illinois. Both the downtown view and the highway entrance shot were omitted from Season 4 onwards, but the remaining fountain shot included an "In Stereo Where Available" note. Non-English versions might differ, e.g. the dubbed German version always includes the expressway shot.




</doc>
<doc id="18947" url="https://en.wikipedia.org/wiki?curid=18947" title="Metre">
Metre

The metre (Commonwealth spelling and BIPM spelling) or meter (American spelling) (from the French unit "mètre", from the Greek noun μετρούν, "measure") is the base unit of length in the International System of Units (SI). The SI unit symbol is m. The metre is defined as the length of the path travelled by light in a vacuum in of a second.

The metre was originally defined in 1793 as one ten-millionth of the distance from the equator to the North Pole along a great circle, so the Earth's circumference is approximately 40,000 km. In 1799, the metre was redefined in terms of a prototype metre bar (the actual bar used was changed in 1889). In 1960, the metre was redefined in terms of a certain number of wavelengths of a certain emission line of krypton-86. In 1983, the current definition was adopted.

"Metre" is the standard spelling of the metric unit for length in nearly all English-speaking nations except the United States and the Philippines, which use "meter." Other Germanic languages, such as German, Dutch, and the Scandinavian languages likewise spell the word "meter."

Measuring devices (such as ammeter, speedometer) are spelled "-meter" in all variants of English. The suffix "-meter" has the same Greek origin as the unit of length.

The etymological roots of "metre" can be traced to the Greek verb ' (metreo) (to measure, count or compare) and noun ' (metron) (a measure), which were used for physical measurement, for poetic metre and by extension for moderation or avoiding extremism (as in "be measured in your response"). This range of uses is also found in Latin ("metior", "mensura"), French ("mètre", "mesure"), English and other languages. The motto "ΜΕΤΡΩ ΧΡΩ" (metro chro) in the seal of the International Bureau of Weights and Measures (BIPM), which was a saying of the Greek statesman and philosopher Pittacus of Mytilene and may be translated as "Use measure!", thus calls for both measurement and moderation. The use of the word metre (for the French unit "mètre") in English began at least as early as 1797.

In 1671 Jean Picard measured the length of a "seconds pendulum" (a pendulum with a period of two seconds) at the Paris observatory. He found the value of 440.5 lines of the Toise of Châtelet which had been recently renewed. He proposed a universal toise (French: "Toise universelle") which was twice the length of the seconds pendulum. However, it was soon discovered that the length of a seconds pendulum varies from place to place: French astronomer Jean Richer had measured the 0.3% difference in length between Cayenne (in French Guiana) and Paris.

Jean Richer and Giovanni Domenico Cassini measured the parallax of Mars between Paris and Cayenne in French Guiana when Mars was at its closest to Earth in 1672. They arrived at a figure for the solar parallax of 9.5 arcseconds, equivalent to an Earth–Sun distance of about 22000 Earth radii. They were also the first astronomers to have access to an accurate and reliable value for the radius of Earth, which had been measured by their colleague Jean Picard in 1669 as 3269 thousand toises. Picard's geodetic observations had been confined to the determination of the magnitude of the Earth considered as a sphere, but the discovery made by Jean Richer turned the attention of mathematicians to its deviation from a spherical form. In addition to its significance for cartography, the determination of the Figure of the Earth became a problem of the highest importance in astronomy, inasmuch as the diameter of the Earth was the unit to which all celestial distances had to be referred.

As a result of the French Revolution, the French Academy of Sciences charged a commission with determining a single scale for all measures. On 7 October 1790 that commission advised the adoption of a decimal system, and on 19 March 1791 advised the adoption of the term "mètre" ("measure"), a basic unit of length, which they defined as equal to one ten-millionth of the distance between the North Pole and the Equator along the meridian through Paris. In 1793, the French National Convention adopted the proposal.

The French Academy of Sciences commissioned an expedition led by Jean Baptiste Joseph Delambre and Pierre Méchain, lasting from 1792 to 1799, which attempted to accurately measure the distance between a belfry in Dunkerque and Montjuïc castle in Barcelona at the longitude of Paris Panthéon. The expedition was fictionalised in Denis Guedj, "Le Mètre du Monde". Ken Alder wrote factually about the expedition in "The Measure of All Things: the seven year odyssey and hidden error that transformed the world". This portion of the Paris meridian, was to serve as the basis for the length of the half meridian connecting the North Pole with the Equator. From 1801 to 1812 France adopted this definition of the metre as its official unit of length based on results from this expedition combined with those of the Geodesic Mission to Peru. The latter was related by Larrie D. Ferreiro in "Measure of the Earth: The Enlightenment Expedition that Reshaped Our World".

A more accurate determination of the Figure of the Earth would soon result from the measurement of the Struve Geodetic Arc (1816–1855) and would have given another value for the definition of this standard of length. This did not invalidate the metre but highlighted that progresses in science would allow better measurement of Earth's size and shape. After the July Revolution of 1830 the metre became the definitive French standard from 1840. At that time it had already been adopted by Ferdinand Rudolph Hassler for the U.S Survey of the Coast.

"The unit of length to which all distances measured in the Coast Survey are referred is the French metre, an authentic copy of which is preserved in the archives of the Coast Survey Office. It is the property of the American Philosophical Society, to whom it was presented by Mr. Hassler, who had received it from Tralles, a member of the French Committee charged with the construction of the standard metre by comparison with the toise, which had served as unit of length in the measurement of the meridional arcs in France and Peru. It possesses all the authenticity of any original metre extant, bearing not only the stamp of the Committee but also the original mark by which it was distiguished from the other bars during the operation of standarding. It is always designated as the Committee metre" (French : "Mètre des Archives").

In 1830 President Andrew Jackson mandated Ferdinand Rudolf Hassler to work out new standards for all U.S. states. According to the decision of the Congress of the United States, the British Parlementary Standard from 1758 was introduced as the unit of length. Another geodesist with metrology skills was to play a pivotal role in the process of internationalization of weights and measures, Carlos Ibáñez e Ibáñez de Ibero who would become the first president of both the International Geodetic Association and the International Committee for Weights and Measures.

In 1867 at the second general conference of the International Association of Geodesy held in Berlin, the question of an international standard unit of length was discussed in order to combine the measurements made in different countries to determine the size and shape of the Earth. The conference recommended the adoption of the metre in replacement of the toise and the creation of an international metre commission, according to the proposal of Johann Jacob Baeyer, Adolphe Hirsch and Carlos Ibáñez e Ibáñez de Ibero who had devised two geodetic standards calibrated on the metre for the map of Spain. Measurement traceability between the toise and the metre was ensured by comparison of the Spanish standard with the standard devised by Borda and Lavoisier for the survey of the meridian arc connecting Dunkirk with Barcelona.

A member of the Preparatory Committee since 1870 and Spanish representative at the Paris Conference in 1875, Carlos Ibáñez e Ibáñez de Ibero intervened with the French Academy of Sciences to rally France to the project to create an International Bureau of Weights and Measures equipped with the scientific means necessary to redefine the units of the metric system according to the progress of sciences.

In the 1870s and in light of modern precision, a series of international conferences was held to devise new metric standards. The Metre Convention ("Convention du Mètre") of 1875 mandated the establishment of a permanent International Bureau of Weights and Measures (BIPM: ') to be located in Sèvres, France. This new organisation was to construct and preserve a prototype metre bar, distribute national metric prototypes, and maintain comparisons between them and non-metric measurement standards. The organisation distributed such bars in 1889 at the first General Conference on Weights and Measures (CGPM: '), establishing the "International Prototype Metre" as the distance between two lines on a standard bar composed of an alloy of 90% platinum and 10% iridium, measured at the melting point of ice.

The comparison of the new prototypes of the metre with each other and with the Committee metre (French: "Mètre des Archives") involved the development of special measuring equipment and the definition of a reproducible temperature scale. The BIPM's thermometry work led to the discovery of special alloys of iron-nickel, in particular invar, for which its director, the Swiss physicist Charles-Edouard Guillaume, was granted the Nobel Prize for physics in 1920.

As Carlos Ibáñez e Ibáñez de Ibero stated, the progress of metrology combined with those of gravimetry through improvement of Kater's pendulum led to a new era of geodesy. If precision metrology had needed the help of geodesy, the latter could not continue to prosper without the help of metrology. Indeed, how to express all the measurements of terrestrial arcs as a function of a single unit, and all the determinations of the force of gravity with the pendulum, if metrology had not created a common unit, adopted and respected by all civilized nations, and if in addition one had not compared, with great precision, to the same unit all the standards for measuring geodesic bases, and all the pendulum rods that had hitherto been used or would be used in the future? Only when this series of metrological comparisons would be finished with a probable error of a thousandth of a millimetre would geodesy be able to link the works of the different nations with one another, and then proclaim the result of the last measurement of the Globe. As the figure of the Earth could be inferred from variations of the seconds pendulum length with latitude, the United States Coast Survey instructed Charles Sanders Peirce in the spring of 1875 to proceed to Europe for the purpose of making pendulum experiments to chief initial stations for operations of this sort, in order to bring the determinations of the forces of gravity in America into communication with those of other parts of the world; and also for the purpose of making a careful study of the methods of pursuing these researches in the different countries of Europe. In 1886 the association of geodesy changed name for the International Geodetic Association, which Carlos Ibáñez e Ibáñez de Ibero presided up to his death in 1891. During this period the International Geodetic Association (German: "Internationale Erdmessung") gained worldwide importance with the joining of United States, Mexico, Chile, Argentina and Japan.

Efforts to supplement the various national surveying systems, which begun in the 19th century with the foundation of the "Mitteleuropäische Gradmessung", resulted in a series of global ellipsoids of the Earth (e.g., Helmert 1906, Hayford 1910/1924) which would later lead to develop the World Geodetic System. Nowadays the practical realisation of the metre is possible everywhere thanks to the atomic clocks embedded in GPS satellites.

In 1893, the standard metre was first measured with an interferometer by Albert A. Michelson, the inventor of the device and an advocate of using some particular wavelength of light as a standard of length. By 1925, interferometry was in regular use at the BIPM. However, the International Prototype Metre remained the standard until 1960, when the eleventh CGPM defined the metre in the new International System of Units (SI) as equal to wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum.

To further reduce uncertainty, the 17th CGPM in 1983 replaced the definition of the metre with its current definition, thus fixing the length of the metre in terms of the second and the speed of light:

This definition fixed the speed of light in vacuum at exactly metres per second (≈). An intended by-product of the 17th CGPM's definition was that it enabled scientists to compare lasers accurately using frequency, resulting in wavelengths with one-fifth the uncertainty involved in the direct comparison of wavelengths, because interferometer errors were eliminated. To further facilitate reproducibility from lab to lab, the 17th CGPM also made the iodine-stabilised helium–neon laser "a recommended radiation" for realising the metre. For the purpose of delineating the metre, the BIPM currently considers the HeNe laser wavelength, , to be with an estimated relative standard uncertainty ("U") of . This uncertainty is currently one limiting factor in laboratory realisations of the metre, and it is several orders of magnitude poorer than that of the second, based upon the caesium fountain atomic clock (). Consequently, a realisation of the metre is usually delineated (not defined) today in labs as wavelengths of helium-neon laser light in a vacuum, the error stated being only that of frequency determination. This bracket notation expressing the error is explained in the article on measurement uncertainty.

Practical realisation of the metre is subject to uncertainties in characterising the medium, to various uncertainties of interferometry, and to uncertainties in measuring the frequency of the source. A commonly used medium is air, and the National Institute of Standards and Technology (NIST) has set up an online calculator to convert wavelengths in vacuum to wavelengths in air. As described by NIST, in air, the uncertainties in characterising the medium are dominated by errors in measuring temperature and pressure. Errors in the theoretical formulas used are secondary. By implementing a refractive index correction such as this, an approximate realisation of the metre can be implemented in air, for example, using the formulation of the metre as wavelengths of helium–neon laser light in vacuum, and converting the wavelengths in a vacuum to wavelengths in air. Air is only one possible medium to use in a realisation of the metre, and any partial vacuum can be used, or some inert atmosphere like helium gas, provided the appropriate corrections for refractive index are implemented.

The metre is "defined" as the path length travelled by light in a given time and practical laboratory length measurements in metres are determined by counting the number of wavelengths of laser light of one of the standard types that fit into the length, and converting the selected unit of wavelength to metres. Three major factors limit the accuracy attainable with laser interferometers for a length measurement:
Of these, the last is peculiar to the interferometer itself. The conversion of a length in wavelengths to a length in metres is based upon the relation

which converts the unit of wavelength "λ" to metres using "c", the speed of light in vacuum in m/s. Here "n" is the refractive index of the medium in which the measurement is made, and "f" is the measured frequency of the source. Although conversion from wavelengths to metres introduces an additional error in the overall length due to measurement error in determining the refractive index and the frequency, the measurement of frequency is one of the most accurate measurements available.


SI prefixes are often employed to denote decimal multiples and submultiples of the metre, as shown in the table below. As indicated in the table, some are commonly used, while others are not. Long distances are usually expressed in km, astronomical units (149.6 Gm), light-years (10 Pm), or parsecs (31 Pm), rather than in Mm, Gm, Tm, Pm, Em, Zm or Ym; "30 cm", "30 m", and "300 m" are more common than "3 dm", "3 dam", and "3 hm", respectively.

The terms "micron" and (occasionally) "millimicron" are often used instead of "micrometre" (μm) and "nanometre" (nm), but this practice is officially discouraged.

Within this table, "inch" and "yard" mean "international inch" and "international yard" respectively, though approximate conversions in the left column hold for both international and survey units.

One metre is exactly equivalent to inches and to yards.
A simple mnemonic aid exists to assist with conversion, as three "3"s:

The ancient Egyptian cubit was about 0.5m (surviving rods are 523–529mm). Scottish and English definitions of the ell (two cubits) were 941mm (0.941m) and 1143mm (1.143m) respectively. The ancient Parisian "toise" (fathom) was slightly shorter than 2m and was standardised at exactly 2m in the mesures usuelles system, such that 1m was exactly toise. The Russian verst was 1.0668km. The Swedish mil was 10.688km, but was changed to 10km when Sweden converted to metric units.




</doc>
<doc id="18948" url="https://en.wikipedia.org/wiki?curid=18948" title="Mole">
Mole

Mole or Molé may refer to:












</doc>
<doc id="18955" url="https://en.wikipedia.org/wiki?curid=18955" title="Mentha">
Mentha

Mentha (also known as mint, from Greek , Linear B "mi-ta") is a genus of plants in the family Lamiaceae (mint family). The exact distinction between species is unclear; it is estimated that 13 to 24 species exist. Hybridization occurs naturally where some species range overlap. Many hybrids and cultivars are known.

The genus has a subcosmopolitan distribution across Europe, Africa, Asia, Australia, and North America.

The species that makes up the genus "Mentha" are widely distributed and can be found in many environments. Most grow best in wet environments and moist soils. Mints will grow 10–120 cm tall and can spread over an indeterminate area. Due to their tendency to spread unchecked, some mints are considered invasive.

Mints are aromatic, almost exclusively perennial herbs. They have wide-spreading underground and overground stolons and erect, square, branched stems. The leaves are arranged in opposite pairs, from oblong to lanceolate, often downy, and with a serrated margin. Leaf colors range from dark green and gray-green to purple, blue, and sometimes pale yellow. The flowers are white to purple and produced in false whorls called verticillasters. The corolla is two-lipped with four subequal lobes, the upper lobe usually the largest. The fruit is a nutlet, containing one to four seeds.

"Mentha" is a member of the tribe Mentheae in the subfamily Nepetoideae. The tribe contains about 65 genera, and relationships within it remain obscure. Authors have disagreed on the circumscription of "Mentha". For example, "M. cervina" has been placed in "Pulegium" and "Preslia", and "M. cunninghamii" has been placed in "Micromeria". In 2004, a molecular phylogenetic study indicated that both "M. cervina" and "M. cunninghamii" should be included in "Mentha". However, "M. cunninghamii" was excluded in a 2007 treatment of the genus.

More than 3,000 names have been published in the genus "Mentha", at ranks from species to forms, the majority of which are regarded as synonyms or illegitimate names. The taxonomy of the genus is made difficult because many species hybridize readily, or are themselves derived from possibly ancient hybridization events. Seeds from hybrids give rise to variable offspring, which may spread through vegetative propagation. The variability has led to what has been described as "paroxysms of species and subspecific taxa"; for example, one taxonomist published 434 new mint taxa for central Europe alone between 1911 and 1916. Recent sources recognize between 18 and 24 species.

, Plants of the World Online recognized the following species:

The mint genus has a large grouping of recognized hybrids. Those accepted by Plants of the World Online are listed below. Parent species are taken from Tucker & Naczi (2007). Synonyms, along with cultivars and varieties where available, are included within the specific nothospecies.


All mints thrive near pools of water, lakes, rivers, and cool moist spots in partial shade. In general, mints tolerate a wide range of conditions, and can also be grown in full sun. Mint grows all year round.

They are fast-growing, extending their reach along surfaces through a network of runners. Due to their speedy growth, one plant of each desired mint, along with a little care, will provide more than enough mint for home use. Some mint species are more invasive than others. Even with the less invasive mints, care should be taken when mixing any mint with any other plants, lest the mint take over. To control mints in an open environment, they should be planted in deep, bottomless containers sunk in the ground, or planted above ground in tubs and barrels.

Some mints can be propagated by seed, but growth from seed can be an unreliable method for raising mint for two reasons: mint seeds are highly variable — one might not end up with what one supposed was planted — and some mint varieties are sterile. It is more effective to take and plant cuttings from the runners of healthy mints.

The most common and popular mints for commercial cultivation are peppermint ("Mentha × piperita"), native spearmint ("Mentha spicata"), Scotch spearmint ("Mentha x gracilis"), and cornmint ("Mentha arvensis"); also (more recently) apple mint ("Mentha suaveolens").

Mints are supposed to make good companion plants, repelling pesty insects and attracting beneficial ones. They are susceptible to whitefly and aphids.

Harvesting of mint leaves can be done at any time. Fresh leaves should be used immediately or stored up to a few days in plastic bags in a refrigerator. Optionally, leaves can be frozen in ice cube trays. Dried mint leaves should be stored in an airtight container placed in a cool, dark, dry area.

The leaf, fresh or dried, is the culinary source of mint. Fresh mint is usually preferred over dried mint when storage of the mint is not a problem. The leaves have a warm, fresh, aromatic, sweet flavor with a cool aftertaste, and are used in teas, beverages, jellies, syrups, candies, and ice creams. In Middle Eastern cuisine, mint is used in lamb dishes, while in British cuisine and American cuisine, mint sauce and mint jelly are used, respectively.

Mint is a necessary ingredient in Touareg tea, a popular tea in northern African and Arab countries. Tea in Arab countries is popularly drunk this way. Alcoholic drinks sometimes feature mint for flavor or garnish, such as the mint julep and the mojito. "Crème de menthe" is a mint-flavored liqueur used in drinks such as the grasshopper.

Mint essential oil and menthol are extensively used as flavorings in breath fresheners, drinks, antiseptic mouth rinses, toothpaste, chewing gum, desserts, and candies, such as mint (candy) and mint chocolate. The substances that give the mints their characteristic aromas and flavors are menthol (the main aroma of peppermint and Japanese peppermint) and pulegone (in pennyroyal and Corsican mint). The compound primarily responsible for the aroma and flavor of spearmint is -carvone.

Mints are used as food plants by the larvae of some Lepidoptera species, including buff ermine moths.

Mint was originally used as a medicinal herb to treat stomach ache and chest pains. There are several uses in traditional medicine and preliminary research for possible use in treating irritable bowel syndrome.

Menthol from mint essential oil (40–90%) is an ingredient of many cosmetics and some perfumes. Menthol and mint essential oil are also used in aromatherapy which may have clinical use to alleviate post-surgery nausea.

Although it is used in many consumer products, mint may cause allergic reactions in some people, inducing symptoms such as abdominal cramps, diarrhea, headaches, heartburn, tingling or numbing around the mouth, anaphylaxis or contact dermatitis.

Mint oil is also used as an environmentally friendly insecticide for its ability to kill some common pests such as wasps, hornets, ants, and cockroaches.

Known in Greek mythology as the herb of hospitality, one of mint's first known uses in Europe was as a room deodorizer. The herb was strewn across floors to cover the smell of the hard-packed soil. Stepping on the mint helped to spread its scent through the room. Today, it is more commonly used for aromatherapy through the use of essential oils.

Mint descends from the Latin word "mentha", which is rooted in the Greek word "minthe", personified in Greek mythology as Minthe, a nymph who was transformed into a mint plant, and reflex of a proto-Indo-European root whence also Sanskrit "-mantha, mathana" ("premna serratifolia").

Mint leaves, without a qualifier like 'peppermint' or 'apple mint', generally refers to spearmint leaves.

In Spain and Central and South America, mint is known as "menta". In Lusophone countries, especially in Portugal, mint species are popularly known as "". In many Indo-Aryan languages, it is called "pudīna", (), Hindi: पुदीना , ().

The taxonomic family Lamiaceae is known as the mint family. It includes many other aromatic herbs, including most of the more common cooking herbs, such as basil, rosemary, sage, oregano, and catnip.

As an English colloquial term, any small mint-flavored confectionery item can be called a mint.

In common usage, other plants with fragrant leaves may be called "mint", although they are not in the mint family.

†Mentha pliocenica fossil seeds have been excavated in Pliocene deposits of Dvorets on the right bank of the Dnieper river between the cities of Rechitsa and Loyew, in south-eastern Belarus. The fossil seeds are similar to the seeds of "Mentha aquatica" and "Mentha arvensis".



</doc>
<doc id="18956" url="https://en.wikipedia.org/wiki?curid=18956" title="Marjoram">
Marjoram

Marjoram (; Origanum majorana) is a somewhat cold-sensitive perennial herb or undershrub with sweet pine and citrus flavors. In some Middle Eastern countries, marjoram is synonymous with oregano, and there the names sweet marjoram and knotted marjoram are used to distinguish it from other plants of the genus "Origanum". It is also called pot marjoram, although this name is also used for other cultivated species of "Origanum".

Marjoram is indigenous to Cyprus and southern Turkey, and was known to the Greeks and Romans as a symbol of happiness. The name marjoram (Old French "majorane", Medieval Latin "majorana") does not directly derive from the Latin word "maior" (major).

Leaves are smooth, simple, petiolated, ovate to oblong-ovate, long, wide, with obtuse apex, entire margin, symmetrical but tapering base, and reticulate venation. The texture is extremely smooth due to the presence of numerous hairs.

Considered a tender perennial (USDA Zones 7–9), marjoram can sometimes prove hardy even in zone 5.

Marjoram is cultivated for its aromatic leaves, either green or dry, for culinary purposes; the tops are cut as the plants begin to flower and are dried slowly in the shade. It is often used in herb combinations such as "herbes de Provence" and "za'atar". The flowering leaves and tops of marjoram are steam-distilled to produce an essential oil that is yellowish in color (darkening to brown as it ages). It has many chemical components, some of which are borneol, camphor, and pinene.
Oregano ("Origanum vulgare"), sometimes listed with marjoram as "O. majorana", is also called wild marjoram. It is a perennial common in southern Europe and north to Sweden in dry copses and on hedge-banks, with many stout stems high, bearing short-stalked, somewhat ovate leaves and clusters of purple flowers. It has a stronger flavor than marjoram.

Pot marjoram or Cretan oregano ("O. onites") has similar uses to marjoram.

Hardy marjoram or French marjoram, a cross of marjoram with oregano, is much more resistant to cold, but is slightly less sweet. "O. pulchellum" is known as showy marjoram or showy oregano.
Marjoram is used for seasoning soups, stews, dressings, sauces, and for herbal teas.



</doc>
<doc id="18957" url="https://en.wikipedia.org/wiki?curid=18957" title="Medicine">
Medicine

Medicine is the science and practice of establishing the diagnosis, prognosis, treatment, and prevention of disease. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.

Medicine has been around for thousands of years, during most of which it was an art (an area of skill and knowledge) frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). While stitching technique for sutures is an art learned through practice, the knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.

Prescientific forms of medicine are now known as traditional medicine and folk medicine, though they do not fall within the modern definition of “medicine” which is based in medical science. Traditional medicine and folk medicine remain commonly used with, or instead of, scientific medicine and are thus called alternative medicine (meaning “[something] other than medicine”, from Latin "alter", “other”). For example, evidence on the effectiveness of acupuncture is "variable and inconsistent" for any condition, but is generally safe when done by an appropriately trained practitioner. In contrast, alternative treatments outside the bounds not just of scientific medicine, but also outside the bounds of safety and efficacy are termed quackery. Quackery can encompass an array of practices and practitioners, irrespective of whether they are prescientific (traditional medicine and folk medicine) or modern pseudo-scientific, including chiropractic which rejects modern scientific germ theory of disease (instead believing without evidence that human diseases are caused by invisible subluxation of the bones, predominantly of the spine and less so of other bones), with just over half of chiropractors also rejecting the science of immunization.

Medicine (, ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word "medicine" is derived from Latin "medicus", meaning "a physician".

Medical availability and clinical practice varies across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.

In modern clinical practice, physicians personally assess patients in order to diagnose, prognose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g. stethoscope, tongue depressor) are typically used. After examination for signs and interviewing for symptoms, the doctor may order medical tests (e.g. blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks depending upon the complexity of the issue.

The components of the medical interview and encounter are:

The physical examination is the examination of the patient for medical signs of disease, which are objective and observable, in contrast to symptoms which are volunteered by the patient and not necessarily objectively observable. The healthcare provider uses the senses of sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order although auscultation occurs prior to percussion and palpation for abdominal assessments.

The clinical examination involves the study of:

It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.

The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. Follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services.

The medical decision-making (MDM) process involves analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.

On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, and lab or imaging results or specialist consultations.

Contemporary medicine is in general conducted within health care systems. Legal, credentialing and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have significant impact on the way medical care is provided.

From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system, or compulsory private or co-operative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices or by state-owned hospitals and clinics, or by charities, most commonly by a combination of all three.

Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those that can afford to pay for it or have self-insured it (either directly or as part of an employment contract) or who may be covered by care financed by the government or tribe directly.

Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice by patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.

Provision of medical care is classified into primary, secondary, and tertiary care categories.
Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.

Secondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient. Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, Emergency departments, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.

Tertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.

Modern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.

In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.

Separation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is independent from the pharmacist who provides the prescription drug. In the Western world there are centuries of tradition for separating pharmacists from physicians. In Asian countries, it is traditional for physicians to also provide drugs.

Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, surgeons, surgeon's assistant, surgical technologist.

The scope and sciences underpinning human medicine overlap many other fields. Dentistry, while considered by some a separate discipline from medicine, is a medical field.

A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.

Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.

The main branches of medicine are:



In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which have its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.

Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery." "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).

Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. Surgery has many sub-specialties, including "general surgery, ophthalmic surgery, cardiovascular surgery, colorectal surgery, neurosurgery, oral and maxillofacial surgery, oncologic surgery, orthopedic surgery, otolaryngology, plastic surgery, podiatric surgery, transplant surgery, trauma surgery, urology, vascular surgery, and pediatric surgery." In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.

Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.

Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called "internists." Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, "internist" or "physician" (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.

Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such "general physicians" would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.

In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as "specialist physicians" (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.

There are many subspecialities (or subdisciplines) of internal medicine:

Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on "medical education" and "physician" for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.


The following are some major medical specialties that do not directly fit into any of the above-mentioned groups:


Some interdisciplinary sub-specialties of medicine include:

Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.

Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/.
In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.

In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification.
The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.

Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.

Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:

Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.

Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.

Early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.

In Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the "Kahun Gynaecological Papyrus" from around 2000 BCE, which describes gynaecological diseases. The "Edwin Smith Papyrus" dating back to 1600 BCE is an early work on surgery, while the "Ebers Papyrus" dating back to 1500 BCE is akin to a textbook on medicine.

In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The "Huangdi Neijing", the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.

In India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery. Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.

In Greece, the Greek physician Hippocrates, the "father of modern medicine", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.

Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.

The concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire.

Although the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe.

After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine". He wrote "The Canon of Medicine" which became a standard medical text at many medieval European universities, considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Persian physician Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Some volumes of Rhazes's work "Al-Mansuri", namely "On Surgery" and "A General Book on Therapy", became part of the medical curriculum in European universities. Additionally, he has been described as a doctor's doctor, the father of pediatrics, and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light. "Al-Risalah al-Dhahabiah" by Ali al-Ridha, the eighth Imam of Shia Muslims, is revered as the most precious Islamic literature in the Science of Medicine. The Persian Bimaristan hospitals were an early example of public hospitals.

In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.

However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.

The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the 'traditional authority' approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.

Andreas Vesalius was the author of "De humani corporis fabrica", an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry".

Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.

Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of inoculation earlier practiced in Asia), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.

The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.

From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.

Others that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States).
As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, "vinca" alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur.

The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.

Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making.

Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.

Evidence-based medicine, prevention of medical error (and other "iatrogenesis"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations.

Globally, many developing countries lack access to care and access to medicines. As of 2015, most wealthy developed countries provide health care to all citizens, with a few exceptions such as the United States where lack of health insurance coverage may limit access.

The World Health Organization (WHO) defines traditional medicine as "the sum total of the knowledge, skills, and practices based on the theories, beliefs, and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness." Practices known as traditional medicines include Ayurveda, Siddha medicine, Unani, ancient Iranian medicine, Irani, Islamic medicine, traditional Chinese medicine, traditional Korean medicine, acupuncture, Muti, Ifá, and traditional African medicine. 

The WHO stated that "inappropriate use of traditional medicines or practices can have negative or dangerous effects" and that "further research is needed to ascertain the efficacy and safety" of several of the practices and medicinal plants used by traditional medicine systems. As examples, the Supreme Court of India and Indian Medical Association regard traditional medicine practices, such as Ayurveda and Siddha medicine, as quackery. Practitioners of traditional medicine are not authorized to practice medicine in India unless trained at a qualified medical institution, registered with the government, and listed as registered physicians annually in The Gazette of India. Identifying practitioners of traditional medicine, the Supreme Court of India stated in 2018 that "unqualified, untrained quacks are posing a great risk to the entire society and playing with the lives of people without having the requisite training and education in the science from approved institutions".



</doc>
<doc id="18959" url="https://en.wikipedia.org/wiki?curid=18959" title="2001 Mars Odyssey">
2001 Mars Odyssey

2001 Mars Odyssey is a robotic spacecraft orbiting the planet Mars. The project was developed by NASA, and contracted out to Lockheed Martin, with an expected cost for the entire mission of US$297 million. Its mission is to use spectrometers and a thermal imager to detect evidence of past or present water and ice, as well as study the planet's geology and radiation environment. It is hoped that the data "Odyssey" obtains will help answer the question of whether life existed on Mars and create a risk-assessment of the radiation that future astronauts on Mars might experience. It also acts as a relay for communications between the Mars Science Laboratory, and previously the Mars Exploration Rovers and "Phoenix" lander, to Earth. The mission was named as a tribute to Arthur C. Clarke, evoking the name of "".

"Odyssey" was launched April 7, 2001, on a Delta II rocket from Cape Canaveral Air Force Station, and reached Mars orbit on October 24, 2001, at 02:30 UTC (October 23, 19:30 PDT, 22:30 EDT).

By December 15, 2010, it broke the record for longest serving spacecraft at Mars, with 3,340 days of operation. As of 2019 October it is in a polar orbit around Mars with a semi-major axis of about 3,800 km or 2,400 miles. It has enough propellant to function until 2025.

On May 28, 2002 (sol ), NASA reported that "Odyssey"s GRS instrument had detected large amounts of hydrogen, a sign that there must be ice lying within a meter of the planet's surface, and proceeded to map the distribution of water below the shallow surface. The orbiter also discovered vast deposits of bulk water ice near the surface of equatorial regions.

"Odyssey" has also served as the primary means of communications for NASA's Mars surface explorers in the past decade, up to the "Curiosity" rover. By December 15, 2010, it broke the record for longest serving spacecraft at Mars, with 3,340 days of operation, claiming the title from NASA's "Mars Global Surveyor". It currently holds the record for the longest-surviving continually active spacecraft in orbit around a planet other than Earth, ahead of the Pioneer Venus Orbiter (served 14 years) and the Mars Express (serving over 14 years), at .

In August 2000, NASA solicited candidate names for the mission. Out of 200 names submitted, the committee chose Astrobiological Reconnaissance and Elemental Surveyor, abbreviated ARES (a tribute to Ares, the Greek god of war). Faced with criticism that this name was not very compelling, and too aggressive, the naming committee reconvened. The candidate name "2001 Mars Odyssey" had earlier been rejected because of copyright and trademark concerns. However, NASA e-mailed Arthur C. Clarke in Sri Lanka, who responded that he would be delighted to have the mission named after his books, and he had no objections. On September 20, NASA associate administrator Ed Weiler wrote to the associate administrator for public affairs recommending a name change from ARES to "2001 Mars Odyssey". Peggy Wilhide then approved the name change.

The three primary instruments "Odyssey" uses are the:


"Mars Odyssey" launched from Cape Canaveral on April 7, 2001, and arrived at Mars about 200 days later on October 24. The spacecraft's main engine fired in order to decelerate, which allowed it to be captured into orbit around Mars. "Odyssey" then spent about three months aerobraking, using friction from the upper reaches of the Martian atmosphere to gradually slow down and reduce and circularize its orbit. By using the atmosphere of Mars to slow the spacecraft in its orbit rather than firing its engine or thrusters, "Odyssey" was able to save more than 200 kilograms (440 lb) of propellant. This reduction in spacecraft weight allowed the mission to be launched on a Delta II 7925 launch vehicle, rather than a larger, more expensive launcher.

Aerobraking ended in January 2002, and Odyssey began its science mapping mission on February 19, 2002. "Odyssey"s original, nominal mission lasted until August 2004, but repeated mission extensions have kept the mission active.

About 85% of images and other data from NASA's twin Mars Exploration Rovers, "Spirit" and "Opportunity", have reached Earth via communications relay by "Odyssey". The orbiter helped analyze potential landing sites for the rovers and performed the same task for NASA's Phoenix mission, which landed on Mars in May 2008. "Odyssey" aided NASA's "Mars Reconnaissance Orbiter", which reached Mars in March 2006, by monitoring atmospheric conditions during months when the newly arrived orbiter used aerobraking to alter its orbit into the desired shape.

"Odyssey" is in a Sun-synchronous orbit, which provides consistent lighting for its photographs. On September 30, 2008 (sol ) the spacecraft altered its orbit to gain better sensitivity for its infrared mapping of Martian minerals. The new orbit eliminated the use of the gamma ray detector, due to the potential for overheating the instrument at the new orbit.

The payload's MARIE radiation experiment stopped taking measurements after a large solar event bombarded the "Odyssey" spacecraft on October 28, 2003. Engineers believe the most likely cause is that a computer chip was damaged by a solar particle smashing into the MARIE computer board.

The orbiter's orientation is controlled by a set of three reaction wheels and a spare. When one failed in June 2012, the fourth was spun up and successfully brought into service. Since July 2012, "Odyssey" has been back in full, nominal operation mode following three weeks of 'safe' mode on remote maintenance.

On February 11, 2014, mission control accelerated "Odyssey"s drift toward a morning-daylight orbit to "enable observation of changing ground temperatures after sunrise and after sunset in thousands of places on Mars". The orbital change occurred gradually until November 2015. Those observations could yield insight about the composition of the ground and about temperature-driven processes, such as warm seasonal flows observed on some slopes, and geysers fed by spring thawing of carbon dioxide (CO ice near Mars' poles.

On October 19, 2014, NASA reported that the "Mars Odyssey" Orbiter, as well as the "Mars Reconnaissance Orbiter" and "MAVEN", were healthy after the Comet Siding Spring flyby.

In 2010, a spokesman for NASA's Jet Propulsion Laboratory stated that "Odyssey" could continue operating until at least 2016. This estimate has since been extended until 2025.

By 2008, "Mars Odyssey" had mapped the basic distribution of water below the shallow surface. The ground truth for its measurements came on July 31, 2008, when NASA announced that the Phoenix lander confirmed the presence of water on Mars, as predicted in 2002 based on data from the "Odyssey" orbiter. The science team is trying to determine whether the water ice ever thaws enough to be available for microscopic life, and if carbon-containing chemicals and other raw materials for life are present.

The orbiter also discovered vast deposits of bulk water ice near the surface of equatorial regions. Evidence for equatorial hydration is both morphological and compositional and is seen at both the Medusae Fossae formation and the Tharsis Montes.

"Mars Odyssey"s THEMIS instrument was used to help select a landing site for the Mars Science Laboratory (MSL). Several days before MSL's landing in August 2012, "Odyssey"s orbit was altered to ensure that it would be able to capture signals from the rover during its first few minutes on the Martian surface. "Odyssey" also acts as a relay for UHF radio signals from the (MSL) rover "Curiosity". Because "Odyssey" is in a Sun-synchronous orbit, it consistently passes over "Curiosity"s location at the same two times every day, allowing for convenient scheduling of contact with Earth.



</doc>
<doc id="18964" url="https://en.wikipedia.org/wiki?curid=18964" title="Madagascar">
Madagascar

Madagascar (; ), officially the Republic of Madagascar ( ; ), and previously known as the Malagasy Republic, is an island country in the Indian Ocean, approximately off the coast of East Africa. At Madagascar is the world's 2nd largest island country. The nation comprises the island of Madagascar (the fourth-largest island in the world) and numerous smaller peripheral islands. Following the prehistoric breakup of the supercontinent Gondwana, Madagascar split from the Indian subcontinent around 88 million years ago, allowing native plants and animals to evolve in relative isolation. Consequently, Madagascar is a biodiversity hotspot; over 90% of its wildlife is found nowhere else on Earth. The island's diverse ecosystems and unique wildlife are threatened by the encroachment of the rapidly growing human population and other environmental threats.

The archaeological evidence of the earliest human foraging on Madagascar may date up to 10,000 years ago. Human settlement of Madagascar occurred between 350 BC and 550 AD by Indianized Austronesian peoples, arriving on outrigger canoes from Indonesia. The social and religious situation of Indonesia during those times were that of Hinduism and Buddhism, along with native Indonesian culture. These were joined around the 9th century AD by Bantu migrants crossing the Mozambique Channel from East Africa. Other groups continued to settle on Madagascar over time, each one making lasting contributions to Malagasy cultural life. The Malagasy ethnic group is often divided into 18 or more subgroups, of which the largest are the Merina of the central highlands.

Until the late 18th century, the island of Madagascar was ruled by a fragmented assortment of shifting sociopolitical alliances. Beginning in the early 19th century, most of the island was united and ruled as the Kingdom of Madagascar by a series of Merina nobles. The monarchy ended in 1897 when the island was absorbed into the French colonial empire, from which the island gained independence in 1960. The autonomous state of Madagascar has since undergone four major constitutional periods, termed republics. Since 1992, the nation has officially been governed as a constitutional democracy from its capital at Antananarivo. However, in a popular uprising in 2009, president Marc Ravalomanana was made to resign and presidential power was transferred in March 2009 to Andry Rajoelina. Constitutional governance was restored in January 2014, when Hery Rajaonarimampianina was named president following a 2013 election deemed fair and transparent by the international community. Madagascar is a member of the United Nations, the African Union (AU), the Southern African Development Community (SADC), and the Organisation Internationale de la Francophonie.

Madagascar belongs to the group of least developed countries, according to the United Nations. Malagasy and French are both official languages of the state. The majority of the population adheres to traditional beliefs, Christianity, or an amalgamation of both. Ecotourism and agriculture, paired with greater investments in education, health, and private enterprise, are key elements of Madagascar's development strategy. Under Ravalomanana, these investments produced substantial economic growth, but the benefits were not evenly spread throughout the population, producing tensions over the increasing cost of living and declining living standards among the poor and some segments of the middle class. , the economy has been weakened by the 2009–2013 political crisis, and quality of life remains low for the majority of the Malagasy population.

In the Malagasy language, the island of Madagascar is called "Madagasikara" () and its people are referred to as "Malagasy". The island's appellation "Madagascar" is not of local origin but rather was popularized in the Middle Ages by Europeans. The name "Madageiscar" was first recorded in the memoirs of 13th-century Venetian explorer Marco Polo as a corrupted transliteration of the name Mogadishu, the Somali port with which Polo had confused the island.

On St. Laurence's Day in 1500, Portuguese explorer Diogo Dias landed on the island and named it "São Lourenço". Polo's name was preferred and popularized on Renaissance maps. No single Malagasy-language name predating "Madagasikara" appears to have been used by the local population to refer to the island, although some communities had their own name for part or all of the land they inhabited.

At , Madagascar is the world's 47th largest country, the 2nd largest island country and the fourth-largest island. The country lies mostly between latitudes 12°S and 26°S, and longitudes 43°E and 51°E. Neighboring islands include the French territory of Réunion and the country of Mauritius to the east, as well as the state of Comoros and the French territory of Mayotte to the north west. The nearest mainland state is Mozambique, located to the west.

The prehistoric breakup of the supercontinent Gondwana separated the Madagascar–Antarctica–India landmass from the Africa–South America landmass around 135 million years ago. Madagascar later split from India about 88 million years ago during the late Cretaceous period allowing plants and animals on the island to evolve in relative isolation. Along the length of the eastern coast runs a narrow and steep escarpment containing much of the island's remaining tropical lowland forest.

To the west of this ridge lies a plateau in the center of the island ranging in altitude from above sea level. These central highlands, traditionally the homeland of the Merina people and the location of their historic capital at Antananarivo, are the most densely populated part of the island and are characterized by terraced, rice-growing valleys lying between grassy hills and patches of the subhumid forests that formerly covered the highland region. To the west of the highlands, the increasingly arid terrain gradually slopes down to the Mozambique Channel and mangrove swamps along the coast.

Madagascar's highest peaks rise from three prominent highland massifs: Maromokotro in the Tsaratanana Massif is the island's highest point, followed by Boby Peak in the Andringitra Massif, and Tsiafajavona in the Ankaratra Massif. To the east, the "Canal des Pangalanes" is a chain of man-made and natural lakes connected by canals built by the French just inland from the east coast and running parallel to it for some .

The western and southern sides, which lie in the rain shadow of the central highlands, are home to dry deciduous forests, spiny forests, and deserts and xeric shrublands. Due to their lower population densities, Madagascar's dry deciduous forests have been better preserved than the eastern rain forests or the original woodlands of the central plateau. The western coast features many protected harbors, but silting is a major problem caused by sediment from the high levels of inland erosion carried by rivers crossing the broad western plains.

The combination of southeastern trade winds and northwestern monsoons produces a hot rainy season (November–April) with frequently destructive cyclones, and a relatively cooler dry season (May–October). Rain clouds originating over the Indian Ocean discharge much of their moisture over the island's eastern coast; the heavy precipitation supports the area's rainforest ecosystem. The central highlands are both drier and cooler while the west is drier still, and a semi-arid climate prevails in the southwest and southern interior of the island.
Tropical cyclones cause damage to infrastructure and local economies as well as loss of life. In 2004, Cyclone Gafilo became the strongest cyclone ever recorded to hit Madagascar. The storm killed 172 people, left 214,260 homeless and caused more than US$250 million in damage.

As a result of the island's long isolation from neighboring continents, Madagascar is home to various plants and animals found nowhere else on Earth. Approximately 90% of all plant and animal species found in Madagascar are endemic. This distinctive ecology has led some ecologists to refer to Madagascar as the "eighth continent", and the island has been classified by Conservation International as a biodiversity hotspot.

More than 80 percent of Madagascar's 14,883 plant species are found nowhere else in the world, including five plant families. The family "Didiereaceae", composed of four genera and 11 species, is limited to the spiny forests of southwestern Madagascar. Four-fifths of the world's "Pachypodium" species are endemic to the island. Three-fourths of Madagascar's 860 orchid species are found here alone, as are six of the world's nine baobab species. The island is home to around 170 palm species, three times as many as on all of mainland Africa; 165 of them are endemic. Many native plant species are used as herbal remedies for a variety of afflictions. The drugs vinblastine and vincristine are "vinca" alkaloids, used to treat Hodgkin's disease, leukemia, and other cancers, were derived from the Madagascar periwinkle. The traveler's palm, known locally as "ravinala" and endemic to the eastern rain forests, is highly iconic of Madagascar and is featured in the national emblem as well as the Air Madagascar logo.

Like its flora, Madagascar's fauna is diverse and exhibits a high rate of endemism. Lemurs have been characterized as "Madagascar's flagship mammal species" by Conservation International. In the absence of monkeys and other competitors, these primates have adapted to a wide range of habitats and diversified into numerous species. , there were officially 103 species and subspecies of lemur, 39 of which were described by zoologists between 2000 and 2008. They are almost all classified as rare, vulnerable, or endangered. At least 17 species of lemur have become extinct since humans arrived on Madagascar, all of which were larger than the surviving lemur species.

A number of other mammals, including the cat-like fossa, are endemic to Madagascar. Over 300 species of birds have been recorded on the island, of which over 60 percent (including four families and 42 genera) are endemic. The few families and genera of reptile that have reached Madagascar have diversified into more than 260 species, with over 90 percent of these being endemic (including one endemic family). The island is home to two-thirds of the world's chameleon species, including the smallest known, and researchers have proposed that Madagascar may be the origin of all chameleons.

Endemic fish of Madagascar include two families, 15 genera and over 100 species, primarily inhabiting the island's freshwater lakes and rivers. Although invertebrates remain poorly studied on Madagascar, researchers have found high rates of endemism among the known species. All 651 species of terrestrial snail are endemic, as are a majority of the island's butterflies, scarab beetles, lacewings, spiders and dragonflies.

Madagascar's varied fauna and flora are endangered by human activity. Since the arrival of humans around 2,350 years ago, Madagascar has lost more than 90 percent of its original forest. This forest loss is largely fueled by "tavy" ("fat"), a traditional slash-and-burn agricultural practice imported to Madagascar by the earliest settlers. Malagasy farmers embrace and perpetuate the practice not only for its practical benefits as an agricultural technique, but for its cultural associations with prosperity, health and venerated ancestral custom ("fomba malagasy"). As human population density rose on the island, deforestation accelerated beginning around 1,400 years ago. By the 16th century, the central highlands had been largely cleared of their original forests. More recent contributors to the loss of forest cover include the growth in cattle herd size since their introduction around 1,000 years ago, a continued reliance on charcoal as a fuel for cooking, and the increased prominence of coffee as a cash crop over the past century. According to a conservative estimate, about 40 percent of the island's original forest cover was lost from the 1950s to 2000, with a thinning of remaining forest areas by 80 percent. In addition to traditional agricultural practice, wildlife conservation is challenged by the illicit harvesting of protected forests, as well as the state-sanctioned harvesting of precious woods within national parks. Although banned by then-President Marc Ravalomanana from 2000 to 2009, the collection of small quantities of precious timber from national parks was re-authorized in January 2009 and dramatically intensified under the administration of Andry Rajoelina as a key source of state revenues to offset cuts in donor support following Ravalomanana's ousting.

Invasive species have likewise been introduced by human populations. Following the 2014 discovery in Madagascar of the Asian common toad, a relative of a toad species that has severely harmed wildlife in Australia since the 1930s, researchers warned the toad could "wreak havoc on the country's unique fauna." Habitat destruction and hunting have threatened many of Madagascar's endemic species or driven them to extinction. The island's elephant birds, a family of endemic giant ratites, became extinct in the 17th century or earlier, most probably because of human hunting of adult birds and poaching of their large eggs for food. Numerous giant lemur species vanished with the arrival of human settlers to the island, while others became extinct over the course of the centuries as a growing human population put greater pressures on lemur habitats and, among some populations, increased the rate of lemur hunting for food. A July 2012 assessment found that the exploitation of natural resources since 2009 has had dire consequences for the island's wildlife: 90 percent of lemur species were found to be threatened with extinction, the highest proportion of any mammalian group. Of these, 23 species were classified as critically endangered. By contrast, a previous study in 2008 had found only 38 percent of lemur species were at risk of extinction.

In 2003, Ravalomanana announced the Durban Vision, an initiative to more than triple the island's protected natural areas to over or 10 percent of Madagascar's land surface. , areas protected by the state included five Strict Nature Reserves ("Réserves Naturelles Intégrales"), 21 Wildlife Reserves ("Réserves Spéciales") and 21 National Parks ("Parcs Nationaux"). In 2007 six of the national parks were declared a joint World Heritage Site under the name Rainforests of the Atsinanana. These parks are Marojejy, Masoala, Ranomafana, Zahamena, Andohahela and Andringitra. Local timber merchants are harvesting scarce species of rosewood trees from protected rainforests within Marojejy National Park and exporting the wood to China for the production of luxury furniture and musical instruments. To raise public awareness of Madagascar's environmental challenges, the Wildlife Conservation Society opened an exhibit entitled ""Madagascar!"" in June 2008 at the Bronx Zoo in New York.

Archaeological finds such as cut marks on bones found in the northwest and stone tools in the northeast indicate that Madagascar was visited by foragers around 2000 BC. Early Holocene humans might have existed on the island 10,500 years ago, based on grooves found on elephant bird bones left by humans. However, a counterstudy concluded that human-made marks date to 1,200 years ago at the earliest, in which the previously mentioned bone damage may have been made by scavengers, ground movements or cuts from the excavation process.

Traditionally, archaeologists have estimated that the earliest settlers arrived in successive waves in outrigger canoes from the Sunda islands (Malay Archipelago) throughout the period between 350 BC and 550 AD, while others are cautious about dates earlier than 250 AD. In either case, these dates make Madagascar one of the last major landmasses on Earth to be settled by humans.

Upon arrival, early settlers practiced slash-and-burn agriculture to clear the coastal rainforests for cultivation. The first settlers encountered Madagascar's abundance of megafauna, including giant lemurs, elephant birds, giant fossa and the Malagasy hippopotamus, which have since become extinct because of hunting and habitat destruction. By 600 AD, groups of these early settlers had begun clearing the forests of the central highlands. Arab traders first reached the island between the 7th and 9th centuries. A wave of Bantu-speaking migrants from southeastern Africa arrived around 1000 AD. South Indian Tamil merchants arrived around 11th century. They introduced the zebu, a type of long-horned humped cattle, which they kept in large herds. Irrigated paddy fields were developed in the central highland Betsileo Kingdom and were extended with terraced paddies throughout the neighboring Kingdom of Imerina a century later. The rising intensity of land cultivation and the ever-increasing demand for zebu pasturage had largely transformed the central highlands from a forest ecosystem to grassland by the 17th century. The oral histories of the Merina people, who may have arrived in the central highlands between 600 and 1,000 years ago, describe encountering an established population they called the Vazimba. Probably the descendants of an earlier and less technologically advanced Austronesian settlement wave, the Vazimba were assimilated or expelled from the highlands by the Merina kings Andriamanelo, Ralambo and Andrianjaka in the 16th and early 17th centuries. Today, the spirits of the Vazimba are revered as "tompontany" (ancestral masters of the land) by many traditional Malagasy communities.

Madagascar was an important transoceanic trading hub connecting ports of the Indian Ocean in the early centuries following human settlement. The written history of Madagascar began with the Arabs, who established trading posts along the northwest coast by at least the 10th century and introduced Islam, the Arabic script (used to transcribe the Malagasy language in a form of writing known as "sorabe"), Arab astrology, and other cultural elements. European contact began in 1500, when the Portuguese sea captain Diogo Dias sighted the island. The French established trading posts along the east coast in the late 17th century.

From about 1774 to 1824, Madagascar gained prominence among pirates and European traders, particularly those involved in the trans-Atlantic slave trade. The small island of Nosy Boroha off the northeastern coast of Madagascar has been proposed by some historians as the site of the legendary pirate utopia of Libertalia. Many European sailors were shipwrecked on the coasts of the island, among them Robert Drury, whose journal is one of the few written depictions of life in southern Madagascar during the 18th century. The wealth generated by maritime trade spurred the rise of organized kingdoms on the island, some of which had grown quite powerful by the 17th century. Among these were the Betsimisaraka alliance of the eastern coast and the Sakalava chiefdoms of Menabe and Boina on the west coast. The Kingdom of Imerina, located in the central highlands with its capital at the royal palace of Antananarivo, emerged at around the same time under the leadership of King Andriamanelo.

Upon its emergence in the early 17th century, the highland kingdom of Imerina was initially a minor power relative to the larger coastal kingdoms and grew even weaker in the early 18th century when King Andriamasinavalona divided it among his four sons. Following almost a century of warring and famine, Imerina was reunited in 1793 by King Andrianampoinimerina (1787–1810). From his initial capital Ambohimanga, and later from the Rova of Antananarivo, this Merina king rapidly expanded his rule over neighboring principalities. His ambition to bring the entire island under his control was largely achieved by his son and successor, King Radama I (1810–28), who was recognized by the British government as King of Madagascar. Radama concluded a treaty in 1817 with the British governor of Mauritius to abolish the lucrative slave trade in return for British military and financial assistance. Artisan missionary envoys from the London Missionary Society began arriving in 1818 and included such key figures as James Cameron, David Jones and David Griffiths, who established schools, transcribed the Malagasy language using the Roman alphabet, translated the Bible, and introduced a variety of new technologies to the island.

Radama's successor, Queen Ranavalona I (1828–61), responded to increasing political and cultural encroachment on the part of Britain and France by issuing a royal edict prohibiting the practice of Christianity in Madagascar and pressuring most foreigners to leave the territory. William Ellis (missionary) described his visits made during her reign in his book "Three Visits to Madagascar during the years 1853, 1854 and 1856". She made heavy use of the traditional practice of "fanompoana" (forced labor as tax payment) to complete public works projects and develop a standing army of between 20,000 and 30,000 Merina soldiers, whom she deployed to pacify outlying regions of the island and further expand the Kingdom of Merina to encompass most of Madagascar. Residents of Madagascar could accuse one another of various crimes, including theft, Christianity and especially witchcraft, for which the ordeal of "tangena" was routinely obligatory. Between 1828 and 1861, the "tangena" ordeal caused about 3,000 deaths annually. In 1838, it was estimated that as many as 100,000 people in Imerina died as a result of the tangena ordeal, constituting roughly 20 percent of the population. The combination of regular warfare, disease, difficult forced labor and harsh measures of justice resulted in a high mortality rate among soldiers and civilians alike during her 33-year reign, the population of Madagascar is estimated to have declined from around 5 million to 2.5 million between 1833 and 1839.

Among those who continued to reside in Imerina were Jean Laborde, an entrepreneur who developed munitions and other industries on behalf of the monarchy, and Joseph-François Lambert, a French adventurer and slave trader, with whom then-Prince Radama II signed a controversial trade agreement termed the Lambert Charter. Succeeding his mother, Radama II (1861–63) attempted to relax the queen's stringent policies, but was overthrown two years later by Prime Minister Rainivoninahitriniony (1852–1865) and an alliance of "Andriana" (noble) and "Hova" (commoner) courtiers, who sought to end the absolute power of the monarch.

Following the coup, the courtiers offered Radama's queen, Rasoherina (1863–68), the opportunity to rule, if she would accept a power sharing arrangement with the Prime Minister: a new social contract that would be sealed by a political marriage between them. Queen Rasoherina accepted, first marrying Rainivoninahitriniony, then later deposing him and marrying his brother, Prime Minister Rainilaiarivony (1864–95), who would go on to marry Queen Ranavalona II (1868–83) and Queen Ranavalona III (1883–97) in succession. Over the course of Rainilaiarivony's 31-year tenure as prime minister, numerous policies were adopted to modernize and consolidate the power of the central government. Schools were constructed throughout the island and attendance was made mandatory. Army organization was improved and British consultants were employed to train and professionalize soldiers. Polygamy was outlawed and Christianity, declared the official religion of the court in 1869, was adopted alongside traditional beliefs among a growing portion of the populace. Legal codes were reformed on the basis of British common law and three European-style courts were established in the capital city. In his joint role as Commander-in-Chief, Rainilaiarivony also successfully ensured the defense of Madagascar against several French colonial incursions.

Primarily on the basis that the Lambert Charter had not been respected, France invaded Madagascar in 1883 in what became known as the first Franco-Hova War. At the end of the war, Madagascar ceded the northern port town of Antsiranana (Diego Suarez) to France and paid 560,000 francs to Lambert's heirs. In 1890, the British accepted the full formal imposition of a French protectorate on the island, but French authority was not acknowledged by the government of Madagascar. To force capitulation, the French bombarded and occupied the harbor of Toamasina on the east coast, and Mahajanga on the west coast, in December 1894 and January 1895 respectively.

A French military flying column then marched toward Antananarivo, losing many men to malaria and other diseases. Reinforcements came from Algeria and Sub-Saharan Africa. Upon reaching the city in September 1895, the column bombarded the royal palace with heavy artillery, causing heavy casualties and leading Queen Ranavalona III to surrender. France annexed Madagascar in 1896 and declared the island a colony the following year, dissolving the Merina monarchy and sending the royal family into exile on Réunion Island and to Algeria. A two-year resistance movement organized in response to the French capture of the royal palace was effectively put down at the end of 1897.

Under colonial rule, plantations were established for the production of a variety of export crops. Slavery was abolished in 1896 and approximately 500,000 slaves were freed; many remained in their former masters' homes as servants or as sharecroppers; in many parts of the island strong discriminatory views against slave descendants are still held today. Wide paved boulevards and gathering places were constructed in the capital city of Antananarivo and the Rova palace compound was turned into a museum. Additional schools were built, particularly in rural and coastal areas where the schools of the Merina had not reached. Education became mandatory between the ages of 6 to 13 and focused primarily on French language and practical skills.

The Merina royal tradition of taxes paid in the form of labor was continued under the French and used to construct a railway and roads linking key coastal cities to Antananarivo. Malagasy troops fought for France in World War I. In the 1930s, Nazi political thinkers developed the Madagascar Plan that had identified the island as a potential site for the deportation of Europe's Jews. During the Second World War, the island was the site of the Battle of Madagascar between the Vichy government and the British.

The occupation of France during the Second World War tarnished the prestige of the colonial administration in Madagascar and galvanized the growing independence movement, leading to the Malagasy Uprising of 1947. This movement led the French to establish reformed institutions in 1956 under the "Loi Cadre" (Overseas Reform Act), and Madagascar moved peacefully towards independence. The Malagasy Republic was proclaimed on 14 October 1958, as an autonomous state within the French Community. A period of provisional government ended with the adoption of a constitution in 1959 and full independence on 26 June 1960.

Since regaining independence, Madagascar has transitioned through four republics with corresponding revisions to its constitution. The First Republic (1960–72), under the leadership of French-appointed President Philibert Tsiranana, was characterized by a continuation of strong economic and political ties to France. Many high-level technical positions were filled by French expatriates, and French teachers, textbooks and curricula continued to be used in schools around the country. Popular resentment over Tsiranana's tolerance for this "neo-colonial" arrangement inspired a series of farmer and student protests that overturned his administration in 1972.

Gabriel Ramanantsoa, a major general in the army, was appointed interim president and prime minister that same year, but low public approval forced him to step down in 1975. Colonel Richard Ratsimandrava, appointed to succeed him, was assassinated six days into his tenure. General Gilles Andriamahazo ruled after Ratsimandrava for four months before being replaced by another military appointee: Vice Admiral Didier Ratsiraka, who ushered in the socialist-Marxist Second Republic that ran under his tenure from 1975 to 1993.

This period saw a political alignment with the Eastern Bloc countries and a shift toward economic insularity. These policies, coupled with economic pressures stemming from the 1973 oil crisis, resulted in the rapid collapse of Madagascar's economy and a sharp decline in living standards, and the country had become completely bankrupt by 1979. The Ratsiraka administration accepted the conditions of transparency, anti-corruption measures and free market policies imposed by the International Monetary Fund, World Bank and various bilateral donors in exchange for their bailout of the nation's broken economy.

Ratsiraka's dwindling popularity in the late 1980s reached a critical point in 1991 when presidential guards opened fire on unarmed protesters during a rally. Within two months, a transitional government had been established under the leadership of Albert Zafy (1993–96), who went on to win the 1992 presidential elections and inaugurate the Third Republic (1992–2010). The new Madagascar constitution established a multi-party democracy and a separation of powers that placed significant control in the hands of the National Assembly. The new constitution also emphasized human rights, social and political freedoms, and free trade. Zafy's term, however, was marred by economic decline, allegations of corruption, and his introduction of legislation to give himself greater powers. He was consequently impeached in 1996, and an interim president, Norbert Ratsirahonana, was appointed for the three months prior to the next presidential election. Ratsiraka was then voted back into power on a platform of decentralization and economic reforms for a second term which lasted from 1996 to 2001.

The contested 2001 presidential elections in which then-mayor of Antananarivo, Marc Ravalomanana, eventually emerged victorious, caused a seven-month standoff in 2002 between supporters of Ravalomanana and Ratsiraka. The negative economic impact of the political crisis was gradually overcome by Ravalomanana's progressive economic and political policies, which encouraged investments in education and ecotourism, facilitated foreign direct investment, and cultivated trading partnerships both regionally and internationally. National GDP grew at an average rate of 7 percent per year under his administration. In the later half of his second term, Ravalomanana was criticised by domestic and international observers who accused him of increasing authoritarianism and corruption.

Opposition leader and then-mayor of Antananarivo, Andry Rajoelina, led a movement in early 2009 in which Ravalomanana was pushed from power in an unconstitutional process widely condemned as a "coup d'état". In March 2009, Rajoelina was declared by the Supreme Court as the President of the High Transitional Authority, an interim governing body responsible for moving the country toward presidential elections. In 2010, a new constitution was adopted by referendum, establishing a Fourth Republic, which sustained the democratic, multi-party structure established in the previous constitution. Hery Rajaonarimampianina was declared the winner of the 2013 presidential election, which the international community deemed fair and transparent.

Madagascar is a semi-presidential representative democratic multi-party republic, wherein the popularly elected president is the head of state and selects a prime minister, who recommends candidates to the president to form his cabinet of ministers. According to the constitution, executive power is exercised by the government while legislative power is vested in the ministerial cabinet, the Senate and the National Assembly, although in reality these two latter bodies have very little power or legislative role. The constitution establishes independent executive, legislative and judicial branches and mandates a popularly elected president limited to three five-year terms.

The public directly elects the president and the 127 members of the National Assembly to five-year terms. All 33 members of the Senate serve six-year terms, with 22 senators elected by local officials and 11 appointed by the president. The last National Assembly election was held on 20 December 2013 and the last Senate election was held on 30 December 2015.

At the local level, the island's 22 provinces are administered by a governor and provincial council. Provinces are further subdivided into regions and communes. The judiciary is modeled on the French system, with a High Constitutional Court, High Court of Justice, Supreme Court, Court of Appeals, criminal tribunals, and tribunals of first instance. The courts, which adhere to civil law, lack the capacity to quickly and transparently try the cases in the judicial system, often forcing defendants to pass lengthy pretrial detentions in unsanitary and overcrowded prisons.

Antananarivo is the administrative capital and largest city of Madagascar. It is located in the highlands region, near the geographic center of the island. King Andrianjaka founded Antananarivo as the capital of his Imerina Kingdom around 1610 or 1625 upon the site of a captured Vazimba capital on the hilltop of Analamanga. As Merina dominance expanded over neighboring Malagasy peoples in the early 19th century to establish the Kingdom of Madagascar, Antananarivo became the center of administration for virtually the entire island. In 1896 the French colonizers of Madagascar adopted the Merina capital as their center of colonial administration. The city remained the capital of Madagascar after regaining independence in 1960. In 2017, the capital's population was estimated at 1,391,433 inhabitants. The next largest cities are Antsirabe (500,000), Toamasina (450,000) and Mahajanga (400,000).

Since Madagascar gained independence from France in 1960, the island's political transitions have been marked by numerous popular protests, several disputed elections, an impeachment, two military coups and one assassination. The island's recurrent political crises are often prolonged, with detrimental effects on the local economy, international relations and Malagasy living standards. The eight-month standoff between incumbent Ratsiraka and challenger Marc Ravalomanana following the 2001 presidential elections cost Madagascar millions of dollars in lost tourism and trade revenue as well as damage to infrastructure, such as bombed bridges and buildings damaged by arson. A series of protests led by Andry Rajoelina against Ravalomanana in early 2009 became violent, with more than 170 people killed. Modern politics in Madagascar are colored by the history of Merina subjugation of coastal communities under their rule in the 19th century. The consequent tension between the highland and coastal populations has periodically flared up into isolated events of violence.

Madagascar has historically been perceived as being on the margin of mainstream African affairs despite being a founding member of the Organisation of African Unity, which was established in 1963 and dissolved in 2002 to be replaced by the African Union. Madagascar was not permitted to attend the first African Union summit because of a dispute over the results of the 2001 presidential election, but rejoined the African Union in July 2003 after a 14-month hiatus. Madagascar was again suspended by the African Union in March 2009 following the unconstitutional transfer of executive power to Rajoelina. Madagascar is a member of the International Criminal Court with a Bilateral Immunity Agreement of protection for the United States military. Eleven countries have established embassies in Madagascar, including France, the United Kingdom, the United States, China and India, while Madagascar has embassies in sixteen other countries.

Human rights in Madagascar are protected under the constitution and the state is a signatory to numerous international agreements including the Universal Declaration of Human Rights and the Convention on the Rights of the Child. Religious, ethnic and sexual minorities are protected under the law. Freedom of association and assembly are also guaranteed under the law, although in practice the denial of permits for public assembly has occasionally been used to impede political demonstrations. Torture by security forces is rare and state repression is low relative to other countries with comparably few legal safeguards, although arbitrary arrests and the corruption of military and police officers remain problems. Ravalomanana's 2004 creation of BIANCO, an anti-corruption bureau, resulted in reduced corruption among Antananarivo's lower-level bureaucrats in particular, although high-level officials have not been prosecuted by the bureau.

The rise of centralized kingdoms among the Sakalava, Merina and other ethnic groups produced the island's first standing armies by the 16th century, initially equipped with spears but later with muskets, cannons and other firearms. By the early 19th century, the Merina sovereigns of the Kingdom of Madagascar had brought much of the island under their control by mobilizing an army of trained and armed soldiers numbering as high as 30,000. French attacks on coastal towns in the later part of the century prompted then-Prime Minister Rainilaiarivony to solicit British assistance to provide training to the Merina monarchy's army. Despite the training and leadership provided by British military advisers, the Malagasy army was unable to withstand French weaponry and was forced to surrender following an attack on the royal palace at Antananarivo. Madagascar was declared a colony of France in 1897.

The political independence and sovereignty of the Malagasy armed forces, which comprises an army, navy and air force, was restored with independence from France in 1960. Since this time the Malagasy military has never engaged in armed conflict with another state or within its own borders, but has occasionally intervened to restore order during periods of political unrest. Under the socialist Second Republic, Admiral Didier Ratsiraka instated mandatory national armed or civil service for all young citizens regardless of gender, a policy that remained in effect from 1976 to 1991. The armed forces are under the direction of the Minister of the Interior and have remained largely neutral during times of political crisis, as during the protracted standoff between incumbent Ratsiraka and challenger Marc Ravalomanana in the disputed 2001 presidential elections, when the military refused to intervene in favor of either candidate. This tradition was broken in 2009, when a segment of the army defected to the side of Andry Rajoelina, then-mayor of Antananarivo, in support of his attempt to force President Ravalomanana from power.

The Minister of the Interior is responsible for the national police force, paramilitary force ("gendarmerie") and the secret police. The police and gendarmerie are stationed and administered at the local level. However, in 2009 fewer than a third of all communes had access to the services of these security forces, with most lacking local-level headquarters for either corps. Traditional community tribunals, called "dina", are presided over by elders and other respected figures and remain a key means by which justice is served in rural areas where state presence is weak. Historically, security has been relatively high across the island. Violent crime rates are low, and criminal activities are predominantly crimes of opportunity such as pickpocketing and petty theft, although child prostitution, human trafficking and the production and sale of marijuana and other illegal drugs are increasing. Budget cuts since 2009 have severely impacted the national police force, producing a steep increase in criminal activity in recent years.

Madagascar is subdivided into 22 regions ("faritra"). The regions are further subdivided into 119 districts, 1,579 communes, and 17,485 "fokontany".

Agriculture has long influenced settlement on the island. Only 15% of the nation's 24,894,551 population live in the 10 largest cities.

Madagascar became a Member State of the United Nations on 20 September 1960, shortly after gaining its independence on 26 June 1960. As of January 2017, 34 police officers from Madagascar are deployed in Haiti as part of the United Nations Stabilisation Mission in Haiti. Starting in 2015, under the direction of and with assistance from the UN, the World Food Programme started the Madagascar Country Programme with the two main goals of long-term development/ reconstruction efforts and addressing the food insecurity issues in the southern regions of Madagascar. These goals plan to be accomplished by providing meals for specific schools in rural and urban priority areas and by developing national school feeding policies to increase consistency of nourishment throughout the country. Small and local farmers have also been assisted in increasing both the quantity and quality of their production, as well as improving their crop yield in unfavorable weather conditions. In 2017, Madagascar signed the UN treaty on the Prohibition of Nuclear Weapons.

During the era of Madagascar's First Republic, France heavily influenced Madagascar's economic planning and policy and served as its key trading partner. Key products were cultivated and distributed nationally through producers' and consumers' cooperatives. Government initiatives such as a rural development program and state farms were established to boost production of commodities such as rice, coffee, cattle, silk and palm oil. Popular dissatisfaction over these policies was a key factor in launching the socialist-Marxist Second Republic, in which the formerly private bank and insurance industries were nationalized; state monopolies were established for such industries as textiles, cotton and power; and import–export trade and shipping were brought under state control. Madagascar's economy quickly deteriorated as exports fell, industrial production dropped by 75 percent, inflation spiked and government debt increased; the rural population was soon reduced to living at subsistence levels. Over 50 percent of the nation's export revenue was spent on debt servicing.

The IMF forced Madagascar's government to accept structural adjustment policies and liberalization of the economy when the state became bankrupt in 1982 and state-controlled industries were gradually privatized over the course of the 1980s. The political crisis of 1991 led to the suspension of IMF and World Bank assistance. Conditions for the resumption of aid were not met under Zafy, who tried unsuccessfully to attract other forms of revenue for the State before aid was once again resumed under the interim government established upon Zafy's impeachment. The IMF agreed to write off half Madagascar's debt in 2004 under the Ravalomanana administration. Having met a set of stringent economic, governance and human rights criteria, Madagascar became the first country to benefit from the Millennium Challenge Account in 2005.

Madagascar's GDP in 2015 was estimated at US$9.98 billion, with a per capita GDP of $411.82. Approximately 69 percent of the population lives below the national poverty line threshold of one dollar per day. During 2011-15, the average growth rate was 2.6% but was expected to have reached 4.1% in 2016, due to public works programs and a growth of the service sector. The agriculture sector constituted 29 percent of Malagasy GDP in 2011, while manufacturing formed 15 percent of GDP. Madagascar's other sources of growth are tourism, agriculture and the extractive industries. Tourism focuses on the niche eco-tourism market, capitalizing on Madagascar's unique biodiversity, unspoiled natural habitats, national parks and lemur species. An estimated 365,000 tourists visited Madagascar in 2008, but the sector declined during the political crisis with 180,000 tourists visiting in 2010. However, the sector has been growing steadily for a few years; In 2016, 293,000 tourists landed in the African island with an increase of 20% compared to 2015; For 2017 the country has the goal of reaching 366,000 visitors, while for 2018 government estimates are expected to reach 500,000 annual tourists.

The island is still a very poor country in 2018; structural brakes remain in the development of the economy: corruption and the shackles of the public administration, lack of legal certainty, and backwardness of land legislation. The economy, however, has been growing since 2011, with GDP growth exceeding 4% per year; almost all economic indicators are growing, the GDP per capita was around $1600 (PPP) for 2017, one of the lowest in the world, although growing since 2012; unemployment was also cut, which in 2016 was equal to 2.1% with a work force of 13.4 million as of 2017. The main economic resources of Madagascar are tourism, textiles, agriculture, and mining.

Madagascar's natural resources include a variety of unprocessed agricultural and mineral resources. Agriculture (including the growing of raffia), fishing and forestry are mainstays of the economy. Madagascar is the world's principal supplier of vanilla, cloves and ylang-ylang. Madagascar supplies 80% of the world's natural vanilla. Other key agricultural resources include coffee, lychees and shrimp. Key mineral resources include various types of precious and semi-precious stones, and Madagascar currently provides half of the world's supply of sapphires, which were discovered near Ilakaka in the late 1990s.

Madagascar has one of the world's largest reserves of ilmenite (titanium ore), as well as important reserves of chromite, coal, iron, cobalt, copper and nickel. Several major projects are underway in the mining, oil and gas sectors that are anticipated to give a significant boost to the Malagasy economy. These include such projects as ilmenite and zircon mining from heavy mineral sands near Tôlanaro by Rio Tinto, extraction of nickel near Moramanga and its processing near Toamasina by Sherritt International, and the development of the giant onshore heavy oil deposits at Tsimiroro and Bemolanga by Madagascar Oil.

Exports formed 28 percent of GDP in 2009. Most of the country's export revenue is derived from the textiles industry, fish and shellfish, vanilla, cloves and other foodstuffs. France is Madagascar's main trading partner, although the United States, Japan and Germany also have strong economic ties to the country. The Madagascar-U.S. Business Council was formed in May 2003, as a collaboration between USAID and Malagasy artisan producers to support the export of local handicrafts to foreign markets. Imports of such items as foodstuffs, fuel, capital goods, vehicles, consumer goods and electronics consume an estimated 52 percent of GDP. The main sources of Madagascar's imports include China, France, Iran, Mauritius and Hong Kong.

In 2010, Madagascar had approximately of paved roads, of railways and of navigable waterways. The majority of roads in Madagascar are unpaved, with many becoming impassable in the rainy season. Largely paved national routes connect the six largest regional towns to Antananarivo, with minor paved and unpaved routes providing access to other population centers in each district.

There are several rail lines. Antananarivo is connected to Toamasina, Ambatondrazaka and Antsirabe by rail, and another rail line connects Fianarantsoa to Manakara. The most important seaport in Madagascar is located on the east coast at Toamasina. Ports at Mahajanga and Antsiranana are significantly less used because of their remoteness. The island's newest port at Ehoala, constructed in 2008 and privately managed by Rio Tinto, will come under state control upon completion of the company's mining project near Tôlanaro around 2038. Air Madagascar services the island's many small regional airports, which offer the only practical means of access to many of the more remote regions during rainy season road washouts.

Running water and electricity are supplied at the national level by a government service provider, Jirama, which is unable to service the entire population. , only 6.8 percent of Madagascar's "fokontany" had access to water provided by Jirama, while 9.5 percent had access to its electricity services. Fifty-six percent of Madagascar's power is provided by hydroelectric power plants, with the remaining 44% provided by diesel engine generators. Mobile telephone and internet access are widespread in urban areas but remain limited in rural parts of the island. Approximately 30% of the districts are able to access the nations' several private telecommunications networks via mobile telephones or land lines.

Radio broadcasts remain the principal means by which the Malagasy population access international, national, and local news. Only state radio broadcasts are transmitted across the entire island. Hundreds of public and private stations with local or regional range provide alternatives to state broadcasting. In addition to the state television channel, a variety of privately owned television stations broadcast local and international programming throughout Madagascar. Several media outlets are owned by political partisans or politicians themselves, including the media groups MBS (owned by Ravalomanana) and Viva (owned by Rajoelina), contributing to political polarization in reporting.

The media have historically come under varying degrees of pressure to censor their criticism of the government. Reporters are occasionally threatened or harassed, and media outlets are periodically forced to close. Accusations of media censorship have increased since 2009 because of the alleged intensification of restrictions on political criticism. Access to the internet has grown dramatically over the past decade, with an estimated 352,000 residents of Madagascar accessing the internet from home or in one of the nation's many internet cafés in December 2011.

Medical centers, dispensaries and hospitals are found throughout the island, although they are concentrated in urban areas and particularly in Antananarivo. Access to medical care remains beyond the reach of many Malagasy, especially in the rural areas, and many recourse to traditional healers. In addition to the high expense of medical care relative to the average Malagasy income, the prevalence of trained medical professionals remains extremely low. In 2010, Madagascar had an average of three hospital beds per 10,000 people and a total of 3,150 doctors, 5,661 nurses, 385 community health workers, 175 pharmacists, and 57 dentists for a population of 22 million. Fifteen percent of government spending in 2008 was directed toward the health sector. Approximately 70 percent of spending on health was contributed by the government, while 30 percent originated with international donors and other private sources. The government provides at least one basic health center per commune. Private health centers are concentrated within urban areas and particularly those of the central highlands.

Despite these barriers to access, health services have shown a trend toward improvement over the past twenty years. Child immunizations against such diseases as hepatitis B, diphtheria, and measles increased an average of 60 percent in this period, indicating low but increasing availability of basic medical services and treatments. The Malagasy fertility rate in 2009 was 4.6 children per woman, declining from 6.3 in 1990. Teen pregnancy rates of 14.8 percent in 2011, much higher than the African average, are a contributing factor to rapid population growth. In 2010, the maternal mortality rate was 440 per 100,000 births, compared to 373.1 in 2008 and 484.4 in 1990, indicating a decline in perinatal care following the 2009 coup. The infant mortality rate in 2011 was 41 per 1,000 births, with an under-five mortality rate at 61 per 1,000 births. Schistosomiasis, malaria, and sexually transmitted diseases are common in Madagascar, although infection rates of AIDS remain low relative to many countries in mainland Africa, at 0.2 percent of the adult population. The malaria mortality rate is also among the lowest in Africa at 8.5 deaths per 100,000 people, in part because of the highest frequency use of insecticide treated nets in Africa. Adult life expectancy in 2009 was 63 years for men and 67 years for women.

In 2017, Madagascar had an outbreak of the bubonic plague that affected urban areas.

Prior to the 19th century, all education in Madagascar was informal and typically served to teach practical skills as well as social and cultural values, including respect for ancestors and elders. The first formal European-style school was established in 1818 at Toamasina by members of the London Missionary Society (LMS). The LMS was invited by King Radama I to expand its schools throughout Imerina to teach basic literacy and numeracy to aristocratic children. The schools were closed by Ranavalona I in 1835 but reopened and expanded in the decades after her death.

By the end of the 19th century, Madagascar had the most developed and modern school system in pre-colonial Sub-Saharan Africa. Access to schooling was expanded in coastal areas during the colonial period, with French language and basic work skills becoming the focus of the curriculum. During the post-colonial First Republic, a continued reliance on French nationals as teachers, and French as the language of instruction, displeased those desiring a complete separation from the former colonial power. Consequently, under the socialist Second Republic, French instructors and other nationals were expelled, Malagasy was declared the language of instruction, and a large cadre of young Malagasy were rapidly trained to teach at remote rural schools under the mandatory two-year national service policy.

This policy, known as "malgachization", coincided with a severe economic downturn and a dramatic decline in the quality of education. Those schooled during this period generally failed to master the French language or many other subjects and struggled to find employment, forcing many to take low-paying jobs in the informal or black market that mired them in deepening poverty. Excepting the brief presidency of Albert Zafy, from 1992 to 1996, Ratsiraka remained in power from 1975 to 2001 and failed to achieve significant improvements in education throughout his tenure.

Education was prioritized under the Ravalomanana administration (2002–09), and is currently free and compulsory from ages 6 to 13. The primary schooling cycle is five years, followed by four years at the lower secondary level and three years at the upper secondary level. During Ravalomanana's first term, thousands of new primary schools and additional classrooms were constructed, older buildings were renovated, and tens of thousands of new primary teachers were recruited and trained. Primary school fees were eliminated, and kits containing basic school supplies were distributed to primary students.

Government school construction initiatives have ensured at least one primary school per "fokontany" and one lower secondary school within each commune. At least one upper secondary school is located in each of the larger urban centers. The three branches of the national public university are located at Antananarivo, Mahajanga, and Fianarantsoa. These are complemented by public teacher-training colleges and several private universities and technical colleges.

As a result of increased educational access, enrollment rates more than doubled between 1996 and 2006. However, education quality is weak, producing high rates of grade repetition and dropout. Education policy in Ravalomanana's second term focused on quality issues, including an increase in minimum education standards for the recruitment of primary teachers from a middle school leaving certificate (BEPC) to a high school leaving certificate (BAC), and a reformed teacher training program to support the transition from traditional didactic instruction to student-centered teaching methods to boost student learning and participation in the classroom. Public expenditure on education was 2.8 percent of GDP in 2014. The literacy rate is estimated at 64.7%.

In , the population of Madagascar was estimated at /1e6 round 0 million, up from 2.2 million in 1900. The annual population growth rate in Madagascar was approximately 2.9 percent in 2009.

Approximately 42.5 percent of the population is younger than 15 years of age, while 54.5 percent are between the ages of 15 and 64. Those aged 65 and older form 3 percent of the total population. Only two general censuses, in 1975 and 1993, have been carried out after independence. The most densely populated regions of the island are the eastern highlands and the eastern coast, contrasting most dramatically with the sparsely populated western plains.

The Malagasy ethnic group forms over 90 percent of Madagascar's population and is typically divided into 18 ethnic subgroups. Recent DNA research revealed that the genetic makeup of the average Malagasy person constitutes an approximately equal blend of Southeast Asian and East African genes, although the genetics of some communities show a predominance of Southeast Asian or East African origins or some Arab, Indian, or European ancestry.

Southeast Asian features – specifically from the southern part of Borneo – are most predominant among the Merina of the central highlands, who form the largest Malagasy ethnic subgroup at approximately 26 percent of the population, while certain communities among the coastal peoples (collectively called "côtiers") have relatively stronger East African features. The largest coastal ethnic subgroups are the Betsimisaraka (14.9 percent) and the Tsimihety and Sakalava (6 percent each).

Chinese, Indian and Comoran minorities are present in Madagascar, as well as a small European (primarily French) populace. Emigration in the late 20th century has reduced these minority populations, occasionally in abrupt waves, such as the exodus of Comorans in 1976, following anti-Comoran riots in Mahajanga. By comparison, there has been no significant emigration of Malagasy peoples. The number of Europeans has declined since independence, reduced from 68,430 in 1958 to 17,000 three decades later. There were an estimated 25,000 Comorans, 18,000 Indians, and 9,000 Chinese living in Madagascar in the mid-1980s.

The Malagasy language is of Malayo-Polynesian origin and is generally spoken throughout the island. The numerous dialects of Malagasy, which are generally mutually intelligible, can be clustered under one of two subgroups: eastern Malagasy, spoken along the eastern forests and highlands including the Merina dialect of Antananarivo, and western Malagasy, spoken across the western coastal plains. French became the official language during the colonial period, when Madagascar came under the authority of France. In the first national Constitution of 1958, Malagasy and French were named the official languages of the Malagasy Republic. Madagascar is a francophone country, and French is mostly spoken as a second language among the educated population and used for international communication.

No official languages were mentioned in the Constitution of 1992, although Malagasy was identified as the national language. Nonetheless, many sources still claimed that Malagasy and French were official languages, eventually leading a citizen to initiate a legal case against the state in April 2000, on the grounds that the publication of official documents only in the French language was unconstitutional. The High Constitutional Court observed in its decision that, in the absence of a language law, French still had the character of an official language.

In the Constitution of 2007, Malagasy remained the national language while official languages were reintroduced: Malagasy, French, and English. English was removed as an official language from the constitution approved by voters in the November 2010 referendum. The outcome of the referendum, and its consequences for official and national language policy, are not recognized by the political opposition, who cite lack of transparency and inclusiveness in the way the election was organized by the High Transitional Authority.

According to the U.S. Department of State in 2011, 41% of Madagascans practiced Christianity, and 52% adhered to traditional religions, which tends to emphasize links between the living and the "razana" (ancestors); these numbers were drawn from the 1993 census. According to the Pew Research Center in 2010, 85% of the population now practiced Christianity, while just 4.5% of Madagascans practiced folk religions; among Christians, practitioners of Protestantism outnumbered adherents of Roman Catholicism.

The veneration of ancestors has led to the widespread tradition of tomb building, as well as the highlands practice of the "famadihana", whereby a deceased family member's remains are exhumed and re-wrapped in fresh silk shrouds, before being replaced in the tomb. The "famadihana" is an occasion to celebrate the beloved ancestor's memory, reunite with family and community, and enjoy a festive atmosphere. Residents of surrounding villages are often invited to attend the party, where food and rum are typically served, and a "hiragasy" troupe or other musical entertainment is commonly present. Consideration for ancestors is also demonstrated through adherence to "fady", taboos that are respected during and after the lifetime of the person who establishes them. It is widely believed that by showing respect for ancestors in these ways, they may intervene on behalf of the living. Conversely, misfortunes are often attributed to ancestors whose memory or wishes have been neglected. The sacrifice of zebu is a traditional method used to appease or honor the ancestors. In addition, the Malagasy traditionally believe in a creator god, called Zanahary or Andriamanitra.

Today, many Christians integrate their religious beliefs with traditional ones related to honoring the ancestors. For instance, they may bless their dead at church before proceeding with traditional burial rites or invite a Christian minister to consecrate a "famadihana" reburial. The Malagasy Council of Churches comprises the four oldest and most prominent Christian denominations of Madagascar (Roman Catholic, Church of Jesus Christ in Madagascar, Lutheran, and Anglican) and has been an influential force in Malagasy politics.

Islam is also practiced on the island. Islam was first brought to Madagascar in the Middle Ages by Arab and Somali Muslim traders, who established several Islamic schools along the eastern coast. While the use of Arabic script and loan words and the adoption of Islamic astrology would spread across the island, the Islamic religion took hold in only a handful of southeastern coastal communities. Today, Muslims constitute 3–7 percent of the population of Madagascar and are largely concentrated in the northwestern provinces of Mahajanga and Antsiranana. The vast majority of Muslims are Sunni. Muslims are divided between those of Malagasy ethnicity, Indians, Pakistanis and Comorans.

More recently, Hinduism was introduced to Madagascar through Gujarati people immigrating from the Saurashtra region of India in the late 19th century. Most Hindus in Madagascar speak Gujarati or Hindi at home.

Each of the many ethnic subgroups in Madagascar adhere to their own set of beliefs, practices and ways of life that have historically contributed to their unique identities. However, there are a number of core cultural features that are common throughout the island, creating a strongly unified Malagasy cultural identity. In addition to a common language and shared traditional religious beliefs around a creator god and veneration of the ancestors, the traditional Malagasy worldview is shaped by values that emphasize "fihavanana" (solidarity), "vintana" (destiny), "tody" (karma), and "hasina", a sacred life force that traditional communities believe imbues and thereby legitimates authority figures within the community or family. Other cultural elements commonly found throughout the island include the practice of male circumcision; strong kinship ties; a widespread belief in the power of magic, diviners, astrology and witch doctors; and a traditional division of social classes into nobles, commoners, and slaves.

Although social castes are no longer legally recognized, ancestral caste affiliation often continues to affect social status, economic opportunity, and roles within the community. Malagasy people traditionally consult "Mpanandro" ("Makers of the Days") to identify the most auspicious days for important events such as weddings or "famadihana", according to a traditional astrological system introduced by Arabs. Similarly, the nobles of many Malagasy communities in the pre-colonial period would commonly employ advisers known as the "ombiasy" (from "olona-be-hasina", "man of much virtue") of the southeastern Antemoro ethnic group, who trace their ancestry back to early Arab settlers.

The diverse origins of Malagasy culture are evident in its tangible expressions. The most emblematic instrument of Madagascar, the "valiha", is a bamboo tube zither carried to Madagascar by early settlers from southern Borneo, and is very similar in form to those found in Indonesia and the Philippines today. Traditional houses in Madagascar are likewise similar to those of southern Borneo in terms of symbolism and construction, featuring a rectangular layout with a peaked roof and central support pillar. Reflecting a widespread veneration of the ancestors, tombs are culturally significant in many regions and tend to be built of more durable material, typically stone, and display more elaborate decoration than the houses of the living. The production and weaving of silk can be traced back to the island's earliest settlers, and Madagascar's national dress, the woven "lamba", has evolved into a varied and refined art.

The Southeast Asian cultural influence is also evident in Malagasy cuisine, in which rice is consumed at every meal, typically accompanied by one of a variety of flavorful vegetable or meat dishes. African influence is reflected in the sacred importance of zebu cattle and their embodiment of their owner's wealth, traditions originating on the African mainland. Cattle rustling, originally a rite of passage for young men in the plains areas of Madagascar where the largest herds of cattle are kept, has become a dangerous and sometimes deadly criminal enterprise as herdsmen in the southwest attempt to defend their cattle with traditional spears against increasingly armed professional rustlers.

A wide variety of oral and written literature has developed in Madagascar. One of the island's foremost artistic traditions is its oratory, as expressed in the forms of "hainteny" (poetry), "kabary" (public discourse) and "ohabolana" (proverbs). An epic poem exemplifying these traditions, the "Ibonia", has been handed down over the centuries in several different forms across the island, and offers insight into the diverse mythologies and beliefs of traditional Malagasy communities. This tradition was continued in the 20th century by such artists as Jean-Joseph Rabearivelo, who is considered Africa's first modern poet, and Elie Rajaonarison, an exemplar of the new wave of Malagasy poetry. Madagascar has also developed a rich musical heritage, embodied in dozens of regional musical genres such as the coastal "salegy" or highland "hiragasy" that enliven village gatherings, local dance floors and national airwaves. Madagascar also has a growing culture of classical music fostered through youth academies, organizations and orchestras that promote youth involvement in classical music.

The plastic arts are also widespread throughout the island. In addition to the tradition of silk weaving and lamba production, the weaving of raffia and other local plant materials has been used to create a wide array of practical items such as floor mats, baskets, purses and hats. Wood carving is a highly developed art form, with distinct regional styles evident in the decoration of balcony railings and other architectural elements. Sculptors create a variety of furniture and household goods, "aloalo" funerary posts, and wooden sculptures, many of which are produced for the tourist market. The decorative and functional woodworking traditions of the Zafimaniry people of the central highlands was inscribed on UNESCO's list of Intangible Cultural Heritage in 2008.

Among the Antaimoro people, the production of paper embedded with flowers and other decorative natural materials is a long-established tradition that the community has begun to market to eco-tourists. Embroidery and drawn thread work are done by hand to produce clothing, as well as tablecloths and other home textiles for sale in local crafts markets. A small but growing number of fine art galleries in Antananarivo, and several other urban areas, offer paintings by local artists, and annual art events, such as the Hosotra open-air exhibition in the capital, contribute to the continuing development of fine arts in Madagascar.

A number of traditional pastimes have emerged in Madagascar. "Moraingy", a type of hand-to-hand combat, is a popular spectator sport in coastal regions. It is traditionally practiced by men, but women have recently begun to participate. The wrestling of zebu cattle, which is named savika or "tolon-omby", is also practiced in many regions. In addition to sports, a wide variety of games are played. Among the most emblematic is "fanorona", a board game widespread throughout the Highland regions. According to folk legend, the succession of King Andrianjaka after his father Ralambo was partially the result of the obsession that Andrianjaka's older brother may have had with playing "fanorona" to the detriment of his other responsibilities.

Western recreational activities were introduced to Madagascar over the past two centuries. Rugby union is considered the national sport of Madagascar. Soccer is also popular. Madagascar has produced a world champion in pétanque, a French game similar to lawn bowling, which is widely played in urban areas and throughout the Highlands. School athletics programs typically include soccer, track and field, judo, boxing, women's basketball and women's tennis. Madagascar sent its first competitors to the Olympic Games in 1964, and has also competed in the African Games. Scouting is represented in Madagascar by its own local federation of three scouting clubs. Membership in 2011 was estimated at 14,905.

Because of its advanced sports facilities, Antananarivo gained the hosting rights for several of Africa's top international basketball events, including the 2011 FIBA Africa Championship, the 2009 FIBA Africa Championship for Women, the 2014 FIBA Africa Under-18 Championship, the 2013 FIBA Africa Under-16 Championship, and the 2015 FIBA Africa Under-16 Championship for Women. Madagascar's national 3x3 basketball team won the gold medal at the 2019 African Games.

Malagasy cuisine reflects the diverse influences of Southeast Asian, African, Indian, Chinese and European culinary traditions. The complexity of Malagasy meals can range from the simple, traditional preparations introduced by the earliest settlers, to the refined festival dishes prepared for the island's 19th-century monarchs. Throughout almost the entire island, the contemporary cuisine of Madagascar typically consists of a base of rice ("vary") served with an accompaniment ("laoka"). The many varieties of "laoka" may be vegetarian or include animal proteins, and typically feature a sauce flavored with such ingredients as ginger, onion, garlic, tomato, vanilla, coconut milk, salt, curry powder, green peppercorns or, less commonly, other spices or herbs. In parts of the arid south and west, pastoral families may replace rice with maize, cassava, or curds made from fermented zebu milk. A wide variety of sweet and savory fritters as well as other street foods are available across the island, as are diverse tropical and temperate-climate fruits. Locally produced beverages include fruit juices, coffee, herbal teas and teas, and alcoholic drinks such as rum, wine, and beer. Three Horses Beer is the most popular beer on the island and is considered emblematic of Madagascar. The island also produces some of the world's finest chocolate; Chocolaterie Robert, established in 1940, is the most famous chocolate company on the island.





</doc>
