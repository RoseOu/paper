<doc id="13635" url="https://en.wikipedia.org/wiki?curid=13635" title="Hugo Gernsback">
Hugo Gernsback

Hugo Gernsback (; born Hugo Gernsbacher, August 16, 1884 – August 19, 1967) was a Luxembourgish-American inventor, writer, editor, and magazine publisher, best known for publications including the first science fiction magazine. His contributions to the genre as publisher—although not as a writer—were so significant that, along with the novelists H. G. Wells and Jules Verne, he is sometimes called "The Father of Science Fiction". In his honour, annual awards presented at the World Science Fiction Convention are named the "Hugos".

Gernsback was born in 1884 in Luxembourg City, to Berta (Dürlacher), a housewife, and Moritz Gernsbacher, a winemaker. His family was Jewish. Gernsback emigrated to the United States in 1904 and later became a naturalized citizen. He married three times: to Rose Harvey in 1906, Dorothy Kantrowitz in 1921, and Mary Hancher in 1951. In 1925, he founded radio station WRNY, which was broadcast from the 18th floor of the Roosevelt Hotel in New York City. In 1928, WRNY aired some of the first television broadcasts. During the show, audio stopped and each artist waved or bowed onscreen. When audio resumed, they performed. Gernsback is also considered a pioneer in amateur radio.

Before helping to create science fiction, Gernsback was an entrepreneur in the electronics industry, importing radio parts from Europe to the United States and helping to popularize amateur "wireless". In April 1908 he founded "Modern Electrics", the world's first magazine about both electronics and radio, called "wireless" at the time. While the cover of the magazine itself states it was a catalog, most historians note that it contained articles, features, and plotlines, qualifying it as a magazine.

Under its auspices, in January 1909, he founded the Wireless Association of America, which had 10,000 members within a year. In 1912, Gernsback said that he estimated 400,000 people in the U.S. were involved in amateur radio. In 1913, he founded a similar magazine, "The Electrical Experimenter", which became "Science and Invention" in 1920. It was in these magazines that he began including scientific fiction stories alongside science journalism—including his own novel "Ralph 124C 41+" which he ran for 12 months from April 1911 in "Modern Electrics".

He died at Roosevelt Hospital in New York City on August 19, 1967.

Gernsback provided a forum for the modern genre of science fiction in 1926 by founding the first magazine dedicated to it, "Amazing Stories". The inaugural April issue comprised a one-page editorial and reissues of six stories, three less than ten years old and three by Poe, Verne, and Wells. He said he became interested in the concept after reading a translation of the work of Percival Lowell as a child. His idea of a perfect science fiction story was "75 percent literature interwoven with 25 percent science". He also played an important role in starting science fiction fandom, by organizing the Science Fiction League and by publishing the addresses of people who wrote letters to his magazines. Fans began to organize, and became aware of themselves as a movement, a social force; this was probably decisive for the subsequent history of the genre. He also created the term "science fiction", though he preferred the term "scientifiction".

In 1929, he lost ownership of his first magazines after a bankruptcy lawsuit. There is some debate about whether this process was genuine, manipulated by publisher Bernarr Macfadden, or was a Gernsback scheme to begin another company. After losing control of "Amazing Stories", Gernsback founded two new science fiction magazines, "Science Wonder Stories" and "Air Wonder Stories". A year later, due to Depression-era financial troubles, the two were merged into "Wonder Stories", which Gernsback continued to publish until 1936, when it was sold to Thrilling Publications and renamed "Thrilling Wonder Stories". Gernsback returned in 1952–53 with "Science-Fiction Plus".

Gernsback was noted for sharp (and sometimes shady) business practices, and for paying his writers extremely low fees or not paying them at all. H. P. Lovecraft and Clark Ashton Smith referred to him as "Hugo the Rat".

As Barry Malzberg has said:

Gernsback's venality and corruption, his sleaziness and his utter disregard for the financial rights of authors, have been well documented and discussed in critical and fan literature. That the founder of genre science fiction who gave his name to the field's most prestigious award and who was the Guest of Honor at the 1952 Worldcon was pretty much a crook (and a contemptuous crook who stiffed his writers but paid himself $100K a year as President of Gernsback Publications) has been clearly established. 

Jack Williamson, who had to hire an attorney associated with the American Fiction Guild to force Gernsback to pay him, summed up his importance for the genre:

At any rate, his main influence in the field was simply to start Amazing and Wonder Stories and get SF out to the public newsstands—and to name the genre he had earlier called "scientifiction."

Frederik Pohl said in 1965 that Gernsback's "Amazing Stories" published "the kind of stories Gernsback himself used to write: a sort of animated catalogue of gadgets". Gernsback's fiction includes the novel "Ralph 124C 41+"; the title is a pun on the phrase "one to foresee for many" ("one plus"). Even though "Ralph 124C 41+" has been described as pioneering many ideas and themes found in later SF work, it has often been neglected due to what most critics deem poor artistic quality. Author Brian Aldiss called the story a "tawdry illiterate tale" and a "sorry concoction", while author and editor Lester del Rey called it "simply dreadful." While most other modern critics have little positive to say about the story's writing, "Ralph 124C 41+" is considered by science fiction critic Gary Westfahl as "essential text for all studies of science fiction."

Gernsback's second novel, "Baron Münchausen's Scientific Adventures", was serialized in "Amazing Stories" in 1928.

Gernsback's third (and final) novel, "Ultimate World", written c. 1958, was not published until 1971. Lester del Rey described it simply as "a bad book", marked more by routine social commentary than by scientific insight or extrapolation. James Blish, in a caustic review, described the novel as "incompetent, pedantic, graceless, incredible, unpopulated and boring" and concluded that its publication "accomplishes nothing but the placing of a blot on the memory of a justly honored man."

Gernsback combined his fiction and science into "Everyday Science and Mechanics" magazine, serving as the editor in the 1930s.

The Hugo Awards or "Hugos" are the annual achievement awards presented at the World Science Fiction Convention, selected in a process that ends with vote by current Convention members. They originated and acquired the "Hugo" nickname during the 1950s and were formally defined as a convention responsibility under the name "Science Fiction Achievement Awards" early in the 1960s. The nickname soon became almost universal and its use legally protected; "Hugo Award(s)" replaced the longer name in all official uses after the 1991 cycle.

In 1960 Gernsback received a special Hugo Award as "The Father of Magazine Science Fiction".

The Science Fiction and Fantasy Hall of Fame inducted him in 1996, its inaugural class of two deceased and two living persons.

Science fiction author Brian W. Aldiss held a contrary view about Gernsback's contributions: "It is easy to argue that Hugo Gernsback ... was one of the worst disasters to hit the science fiction field ... Gernsback himself was utterly without any literary understanding. He created dangerous precedents which many later editors in the field followed."

Gernsback made significant contributions to the growth of early broadcasting, mostly through his efforts as a publisher. He originated the industry of specialized publications for radio with "Modern Electrics" and "Electrical Experimenter". Later on, and more influentially, he published "Radio News", which would have the largest readership among radio magazines in radio broadcasting's formative years. He edited "Radio News" until 1929. For a short time he hired John F. Rider to be editor. Rider was a former engineer working with the US Army Signal Corps and a radio engineer for Alfred H. Grebe, a radio manufacturer. However, Rider would soon leave Gernsback and form his own publishing company, John F. Rider Publisher, New York around 1931.

Gernsback made use of the magazine to promote his own interests, including having his radio station's call letters on the cover starting in 1925. WRNY and "Radio News" were used to cross-promote each other, with programs on his station often used to discuss articles he had published, and articles in the magazine often covering program activities at WRNY. He also advocated for future directions in innovation and regulation of radio. The magazine contained many drawings and diagrams, encouraging radio listeners of the 1920s to experiment themselves to improve the technology. WRNY was often used as a laboratory to see if various radio inventions were worthwhile.

Articles that were published about television were also tested in this manner when the radio station was used to send pictures to experimental television receivers in August 1928. The technology, however, required sending sight and sound one after the other rather than sending both at the same time, as WRNY only broadcast on one channel. Such experiments were expensive, eventually contributing to Gernsback's Experimenter Publishing Company going into bankruptcy in 1929. WRNY was sold to Aviation Radio, who maintained the channel part-time to broadcast aviation weather reports and related feature programs. Along with other stations sharing the same frequency, it was acquired by Metro-Goldwyn-Mayer and consolidated into that company's WHN in 1934.

Gernsback held 80 patents by the time of his death in New York City on August 19, 1967.

His first patent was a new method for manufacturing dry-cell batteries, a patent applied for on June 28, 1906 and granted February 5, 1907.

Among his inventions are a combined electric hair brush and comb (US Patent 1,016,138), 1912; an ear cushion (US Patent 1,514,152), 1927; and a hydraulic fishery (US Patent 2,718,083), 1955.

Other patents held by Gernsback are related to : Incandescent Lamp, Electrorheostat Regulator, Electro Adjustable Condenser, Detectorium, Relay, Potentiometer, Electrolytic Interrupter, Rotary Variable Condenser, Luminous Electric Mirror, Transmitter, Postal Card, Telephone Headband, Electromagnetic Sounding Device, Submersible Amusement Device, Apparatus for Landing Flying Machines, Tuned Telephone Receiver, Electric Valve, Detector, Acoustic Apparatus, Electrically Operated Fountain, Cord Terminal, Coil Mounting, Radio Horn, Variable Condenser, Switch, Telephone Receiver, Crystal Detector, Process for Mounting Inductances, Depilator, Switch, Code Learner's Instrument.

Novels:


Short stories:








</doc>
<doc id="13636" url="https://en.wikipedia.org/wiki?curid=13636" title="History of computing hardware">
History of computing hardware

The history of computing hardware covers the developments from early simple devices to aid calculation to modern day computers. Before the 20th century, most calculations were done by humans. Early mechanical tools to help humans with digital calculations, like the abacus, were called "calculating machines", called by proprietary names, or referred to as calculators. The machine operator was called the computer.

The first aids to computation were purely mechanical devices which required the operator to set up the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form, for instance distance along a scale, rotation of a shaft, or a voltage. Numbers could also be represented in the form of digits, automatically manipulated by a mechanical mechanism. Although this approach generally required more complex mechanisms, it greatly increased the precision of results. The development of transistor technology and then the integrated circuit chip led to a series of breakthroughs, starting with transistor computers and then integrated circuit computers, causing digital computers to largely replace analog computers. Metal-oxide-semiconductor (MOS) large-scale integration (LSI) then enabled semiconductor memory and the microprocessor, leading to another key breakthrough, the miniaturized personal computer (PC), in the 1970s. The cost of computers gradually became so low that personal computers by the 1990s, and then mobile computers (smartphones and tablets) in the 2000s, became ubiquitous.

Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example. The abacus was early used for arithmetic tasks. What we now call the Roman abacus was used in Babylonia as early as c. 2700–2300 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.

Several analog computers were constructed in ancient and medieval times to perform astronomical calculations. These included the astrolabe and Antikythera mechanism from the Hellenistic world (c. 150–100 BC). In Roman Egypt, Hero of Alexandria (c. 10–70 AD) made mechanical devices including automata and a programmable cart. Other early mechanical devices used to perform one or another type of calculations include the planisphere and other mechanical computing devices invented by Abu Rayhan al-Biruni (c. AD 1000); the equatorium and universal latitude-independent astrolabe by Abū Ishāq Ibrāhīm al-Zarqālī (c. AD 1015); the astronomical analog computers of other medieval Muslim astronomers and engineers; and the astronomical clock tower of Su Song (1094) during the Song dynasty. The castle clock, a hydropowered mechanical astronomical clock invented by Ismail al-Jazari in 1206, was the first programmable analog computer. Ramon Llull invented the Lullian Circle: a notional machine for calculating answers to philosophical questions (in this case, to do with Christianity) via logical combinatorics. This idea was taken up by Leibniz centuries later, and is thus one of the founding elements in computing and information science.

Scottish mathematician and physicist John Napier discovered that the multiplication and division of numbers could be performed by the addition and subtraction, respectively, of the logarithms of those numbers. While producing the first logarithmic tables, Napier needed to perform many tedious multiplications. It was at this point that he designed his 'Napier's bones', an abacus-like device that greatly simplified calculations that involved multiplication and division.
Since real numbers can be represented as distances or intervals on a line, the slide rule was invented in the 1620s, shortly after Napier's work, to allow multiplication and division operations to be carried out significantly faster than was previously possible. Edmund Gunter built a calculating device with a single logarithmic scale at the University of Oxford. His device greatly simplified arithmetic calculations, including multiplication and division. William Oughtred greatly improved this in 1630 with his circular slide rule. He followed this up with the modern slide rule in 1632, essentially a combination of two Gunter rules, held together with the hands. Slide rules were used by generations of engineers and other mathematically involved professional workers, until the invention of the pocket calculator.

Wilhelm Schickard, a German polymath, designed a calculating machine in 1623 which combined a mechanised form of Napier's rods with the world's first mechanical adding machine built into the base. Because it made use of a single-tooth gear there were circumstances in which its carry mechanism would jam. A fire destroyed at least one of the machines in 1624 and it is believed Schickard was too disheartened to build another.

In 1642, while still a teenager, Blaise Pascal started some pioneering work on calculating machines and after three years of effort and 50 prototypes he invented a mechanical calculator. He built twenty of these machines (called Pascal's calculator or Pascaline) in the following ten years. Nine Pascalines have survived, most of which are on display in European museums. A continuing debate exists over whether Schickard or Pascal should be regarded as the "inventor of the mechanical calculator" and the range of issues to be considered is discussed elsewhere.

Gottfried Wilhelm von Leibniz invented the stepped reckoner and his famous stepped drum mechanism around 1672. He attempted to create a machine that could be used not only for addition and subtraction but would utilise a moveable carriage to enable long multiplication and division. Leibniz once said "It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used." However, Leibniz did not incorporate a fully successful carry mechanism. Leibniz also described the binary numeral system, a central ingredient of all modern computers. However, up to the 1940s, many subsequent designs (including Charles Babbage's machines of the 1822 and even ENIAC of 1945) were based on the decimal system.

Around 1820, Charles Xavier Thomas de Colmar created what would over the rest of the century become the first successful, mass-produced mechanical calculator, the Thomas Arithmometer. It could be used to add and subtract, and with a moveable carriage the operator could also multiply, and divide by a process of long multiplication and long division. It utilised a stepped drum similar in conception to that invented by Leibniz. Mechanical calculators remained in use until the 1970s.

In 1804, Joseph-Marie Jacquard developed a loom in which the pattern being woven was controlled by a paper tape constructed from punched cards. The paper tape could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punched cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for automatic pianos and more recently numerical control machine tools.
In the late 1880s, the American Herman Hollerith invented data storage on punched cards that could then be read by a machine. To process these punched cards, he invented the tabulator and the keypunch machine. His machines used electromechanical relays and counters. Hollerith's method was used in the 1890 United States Census. That census was processed two years faster than the prior census had been. Hollerith's company eventually became the core of IBM.

By 1920, electromechanical tabulating machines could add, subtract, and print accumulated totals. Machine functions were directed by inserting dozens of wire jumpers into removable control panels. When the United States instituted Social Security in 1935, IBM punched-card systems were used to process records of 26 million workers. Punched cards became ubiquitous in industry and government for accounting and administration.

Leslie Comrie's articles on punched-card methods and W. J. Eckert's publication of "Punched Card Methods in Scientific Computation" in 1940, described punched-card techniques sufficiently advanced to solve some differential equations or perform multiplication and division using floating point representations, all on punched cards and unit record machines. Such machines were used during World War II for cryptographic statistical processing, as well as a vast number of administrative uses. The Astronomical Computing Bureau, Columbia University, performed astronomical calculations representing the state of the art in computing.

The book "IBM and the Holocaust" by Edwin Black outlines the ways in which IBM's technology helped facilitate Nazi genocide through generation and tabulation of punch cards based on national census data. "See also: Dehomag"

By the 20th century, earlier mechanical calculators, cash registers, accounting machines, and so on were redesigned to use electric motors, with gear position as the representation for the state of a variable. The word "computer" was a job title assigned to primarily women who used these calculators to perform mathematical calculations. By the 1920s, British scientist Lewis Fry Richardson's interest in weather prediction led him to propose human computers and numerical analysis to model the weather; to this day, the most powerful computers on Earth are needed to adequately model its weather using the Navier–Stokes equations.

Companies like Friden, Marchant Calculator and Monroe made desktop mechanical calculators from the 1930s that could add, subtract, multiply and divide. In 1948, the Curta was introduced by Austrian inventor Curt Herzstark. It was a small, hand-cranked mechanical calculator and as such, a descendant of Gottfried Leibniz's Stepped Reckoner and Thomas's Arithmometer.

The world's first "all-electronic desktop" calculator was the British Bell Punch ANITA, released in 1961. It used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick. The tube technology was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a CRT, and introduced reverse Polish notation (RPN).

Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.

The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.

There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. (Later drawings depict a regularized grid layout.) Like the central processing unit (CPU) in a modern computer, the mill would rely on its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards.

The machine was about a century ahead of its time. However, the project was slowed by various problems including disputes with the chief machinist building parts for it. All the parts for his machine had to be made by hand—this was a major problem for a machine with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Ada Lovelace translated and added notes to the ""Sketch of the Analytical Engine"" by Luigi Federico Menabrea. This appears to be the first published description of programming, so Ada Lovelace is widely regarded as the first computer programmer.

Following Babbage, although unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.

The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock). It was used by the Imperial Russian Navy in World War I. The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.

Mechanical devices were also used to aid the accuracy of aerial bombing. Drift Sight was the first such aid, developed by Harry Wimperis in 1916 for the Royal Naval Air Service; it measured the wind speed from the air, and used that measurement to calculate the wind's effects on the trajectory of the bombs. The system was later improved with the Course Setting Bomb Sight, and reached a climax with World War II bomb sights, Mark XIV bomb sight (RAF Bomber Command) and the Norden (United States Army Air Forces).

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927, which built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious; the most powerful was constructed at the University of Pennsylvania's Moore School of Electrical Engineering, where the ENIAC was built.

A fully electronic analog computer was built by Helmut Hölzer in 1942 at Peenemünde Army Research Center

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but hybrid analog computers, controlled by digital electronics, remained in substantial use into the 1950s and 1960s, and later in some specialized applications.

The principle of the modern computer was first described by computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, "On Computable Numbers". Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the "Entscheidungsproblem" by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a "universal machine" (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

The era of modern computing began with a flurry of development before and during World War II. Most digital computers built in this period were electromechanical – electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes.

The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1940. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.
In the same year, electro-mechanical devices called bombes were built by British cryptologists to help decipher German Enigma-machine-encrypted secret messages during World War II. The bombes' initial design was created in 1939 at the UK Government Code and Cypher School (GC&CS) at Bletchley Park by Alan Turing, with an important refinement devised in 1940 by Gordon Welchman. The engineering design and construction was the work of Harold Keen of the British Tabulating Machine Company. It was a substantial development from a device that had been designed in 1938 by Polish Cipher Bureau cryptologist Marian Rejewski, and known as the "cryptologic bomb" (Polish: ""bomba kryptologiczna"").

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22-bit word length that operated at a clock frequency of about 5–10 Hz. Program code and data were stored on punched film. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was probably a Turing-complete machine. In two 1936 patent applications, Zuse also anticipated that machine instructions could be stored in the same storage used for data—the key insight of what became known as the von Neumann architecture, first implemented in 1948 in America in the electromechanical IBM SSEC and in Britain in the fully electronic Manchester Baby.

Zuse suffered setbacks during World War II when some of his machines were destroyed in the course of Allied bombing campaigns. Apparently his work remained largely unknown to engineers in the UK and US until much later, although at least IBM was aware of it as it financed his post-war startup company in 1946 in return for an option on Zuse's patents.

In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories; it was a similar general purpose electro-mechanical computer to the Z3, but was not quite Turing-complete.

The term digital was first suggested by George Robert Stibitz and refers to where a signal, such as a voltage, is not used to directly represent a value (as it would be in an analog computer), but to encode it. In November 1937, George Stibitz, then working at Bell Labs (1930–1941), completed a relay-based calculator he later dubbed the "Model K" (for "kitchen table", on which he had assembled it), which became the first binary adder. Typically signals have two states – low (usually representing 0) and high (usually representing 1), but sometimes three-valued logic is used, especially in high-density memory. Modern computers generally use binary logic, but many early machines were decimal computers. In these machines, the basic unit of data was the decimal digit, encoded in one of several schemes, including binary-coded decimal or BCD, bi-quinary, excess-3, and two-out-of-five code.

The mathematical basis of digital computing is Boolean algebra, developed by the British mathematician George Boole in his work "The Laws of Thought", published in 1854. His Boolean algebra was further refined in the 1860s by William Jevons and Charles Sanders Peirce, and was first presented systematically by Ernst Schröder and A. N. Whitehead. In 1879 Gottlob Frege develops the formal approach to logic and proposes the first logic language for logical equations.

In the 1930s and working independently, American electronic engineer Claude Shannon and Soviet logician Victor Shestakov both showed a one-to-one correspondence between the concepts of Boolean logic and certain electrical circuits, now called logic gates, which are now ubiquitous in digital computers. They showed that electronic relays and switches can realize the expressions of Boolean algebra. This thesis essentially founded practical digital circuit design.

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. Machines such as the Z3, the Atanasoff–Berry Computer, the Colossus computers, and the ENIAC were built by hand, using circuits containing relays or valves (vacuum tubes), and often used punched cards or punched paper tape for input and as the main (non-volatile) storage medium.

The engineer Tommy Flowers joined the telecommunications branch of the General Post Office in 1926. While working at the research station in Dollis Hill in the 1930s, he began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.

In the US, in 1940 Arthur Dickinson (IBM) invented the first digital electronic computer. This calculating device was fully electronic – control, calculations and output (the first electronic display). John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed the Atanasoff–Berry Computer (ABC) in 1942, the first binary electronic digital calculating device. This design was semi-electronic (electro-mechanical control and electronic calculations), and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory. However, its paper card writer/reader was unreliable and the regenerative drum contact system was mechanical. The machine's special-purpose nature and lack of changeable, stored program distinguish it from modern computers.

Computers whose logic was primarily built using vacuum tubes are now known as first generation computers.

During World War II, British codebreakers at Bletchley Park (40 miles north of London) achieved a number of successes at breaking encrypted enemy military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. Women often operated these bombe machines. They ruled out possible Enigma settings by performing chains of logical deductions implemented electrically. Most possibilities led to a contradiction, and the few remaining could be tested by hand.

The Germans also developed a series of teleprinter encryption systems, quite different from Enigma. The Lorenz SZ 40/42 machine was used for high-level Army communications, code-named "Tunny" by the British. The first intercepts of Lorenz messages began in 1941. As part of an attack on Tunny, Max Newman and his colleagues developed the Heath Robinson, a fixed-function machine to aid in code breaking. Tommy Flowers, a senior engineer at the Post Office Research Station was recommended to Max Newman by Alan Turing and spent eleven months from early February 1943 designing and building the more flexible Colossus computer (which superseded the Heath Robinson). After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.
Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Data input to Colossus was by photoelectric reading of a paper tape transcription of the enciphered intercepted message. This was arranged in a continuous loop so that it could be read and re-read multiple times – there being no internal store for the data. The reading mechanism ran at 5,000 characters per second with the paper tape moving at . Colossus Mark 1 contained 1500 thermionic valves (tubes), but Mark 2 with 2400 valves and five processors in parallel, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process. Mark 2 was designed while Mark 1 was being constructed. Allen Coombs took over leadership of the Colossus Mark 2 project when Tommy Flowers moved on to other projects. The first Mark 2 Colossus became operational on 1 June 1944, just in time for the Allied Invasion of Normandy on D-Day.

Most of the use of Colossus was in determining the start positions of the Tunny rotors for a message, which was called "wheel setting". Colossus included the first-ever use of shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean calculations. This enabled five different possible start positions to be examined for one transit of the paper tape. As well as wheel setting some later Colossi included mechanisms intended to help determine pin patterns known as "wheel breaking". Both models were programmable using switches and plug panels in a way their predecessors had not been. Ten Mk 2 Colossi were operational by the end of the war.
Without the use of these machines, the Allies would have been deprived of the very valuable intelligence that was obtained from reading the vast quantity of enciphered high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Details of their existence, design, and use were kept secret well into the 1970s. Winston Churchill personally issued an order for their destruction into pieces no larger than a man's hand, to keep secret that the British were capable of cracking Lorenz SZ cyphers (from German rotor stream cipher machines) during the oncoming Cold War. Two of the machines were transferred to the newly formed GCHQ and the others were destroyed. As a result, the machines were not included in many histories of computing. A reconstructed working copy of one of the Colossus machines is now on display at Bletchley Park.

The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were women who had been trained as mathematicians.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High-speed memory was limited to 20 words (equivalent to about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors. One of its major engineering feats was to minimize the effects of tube burnout, which was a common problem in machine reliability at that time. The machine was in almost constant use for the next ten years.

Early computing machines were programmable in the sense that they could follow the sequence of steps they had been set up to execute, but the "program", or steps that the machine was to execute, were set up usually by changing how the wires were plugged into a patch panel or plugboard. "Reprogramming", when it was possible at all, was a laborious process, starting with engineers working out flowcharts, designing the new set up, and then the often-exacting process of physically re-wiring patch panels. Stored-program computers, by contrast, were designed to store a set of instructions (a program), in memory – typically the same memory as stored data.

The theoretical basis for the stored-program computer had been proposed by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began his work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device.

Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his "First Draft of a Report on the EDVAC" in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.

Turing thought that the speed and the size of computer memory were crucial elements, so he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used "Abbreviated Computer Instructions," an early form of programming language.

The Manchester Baby was the world's first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.

The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device. Invented by Freddie Williams and Tom Kilburn at the University of Manchester in 1946 and 1947, it was a cathode ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.

Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.

The Baby had a 32-bit word length and a memory of 32 words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 2 (262,144), a calculation that was known would take a long time to run—and so prove the computer's reliability—by testing every integer from 2 − 1 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17 instructions and ran for 52 minutes before reaching the correct answer of 131,072, after the Baby had performed 3.5 million operations (for an effective CPU speed of 1.1 kIPS).

The Experimental machine led on to the development of the Manchester Mark 1 at the University of Manchester. Work began in August 1948, and the first version was operational by April 1949; a program written to search for Mersenne primes ran error-free for nine hours on the night of 16/17 June 1949.
The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers.

The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark 1 that computers would be used more in scientific roles than in pure mathematics. In 1951 they started development work on Meg, the Mark 1's successor, which would include a floating point unit.

The other contender for being the first recognizably modern digital stored-program computer was the EDSAC, designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal "First Draft of a Report on the EDVAC" and was one of the first usefully operational electronic digital stored-program computer.

EDSAC ran its first programs on 6 May 1949, when it calculated a table of squares and a list of prime numbers.The EDSAC also served as the basis for the first commercially applied computer, the LEO I, used by food manufacturing company J. Lyons & Co. Ltd. EDSAC 1 and was finally shut down on 11 July 1958, having been superseded by EDSAC 2 which stayed in use until 1965.

ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944, and design work for the EDVAC commenced at the University of Pennsylvania's Moore School of Electrical Engineering, before the ENIAC was fully operational. The design implemented a number of important architectural and logical improvements conceived during the ENIAC's construction, and a high-speed serial-access memory. However, Eckert and Mauchly left the project and its construction floundered.

It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.

The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves). A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.

In October 1947, the directors of J. Lyons & Company, a British catering company famous for its teashops but with strong interests in new office management techniques, decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job. On 17 November 1951, the J. Lyons company began weekly operation of a bakery valuations job on the LEO (Lyons Electronic Office). This was the first business to go live on a stored program computer.
In June 1951, the UNIVAC I (Universal Automatic Computer) was delivered to the U.S. Census Bureau. Remington Rand eventually sold 46 machines at more than US$1 million each ($ as of 2020). UNIVAC was the first "mass produced" computer. It used 5,200 vacuum tubes and consumed 125 kW of power. Its primary storage was serial-access mercury delay lines capable of storing 1,000 words of 11 decimal digits plus sign (72-bit words).

IBM introduced a smaller, more affordable computer in 1954 that proved very popular. The IBM 650 weighed over 900 kg, the attached power supply weighed around 1350 kg and both were held in separate cabinets of roughly 1.5 meters by 0.9 meters by 1.8 meters. It cost US$500,000 ($ as of 2020) or could be leased for US$3,500 a month ($ as of 2020). Its drum memory was originally 2,000 ten-digit words, later expanded to 4,000 words. Memory limitations such as this were to dominate programming for decades afterward. The program instructions were fetched from the spinning drum as the code ran. Efficient execution using drum memory was provided by a combination of hardware architecture: the instruction format included the address of the next instruction; and software: the Symbolic Optimal Assembly Program, SOAP, assigned instructions to the optimal addresses (to the extent possible by static analysis of the source program). Thus many instructions were, when needed, located in the next row of the drum to be read and additional wait time for drum rotation was not required.

In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the central processing unit of a computer could be controlled by a miniature, highly specialised computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode). This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in "IEEE Spectrum" in 1955.

It was widely used in the CPUs and floating-point units of mainframe and other computers; it was implemented for the first time in EDSAC 2, which also used multiple identical "bit slices" to simplify design. Interchangeable, replaceable tube assemblies were used for each bit of the processor.

Magnetic drum memories were developed for the US Navy during WW II with the work continuing at Engineering Research Associates (ERA) in 1946 and 1947. ERA, then a part of Univac included a drum memory in its 1103, announced in February 1953. The first mass-produced computer, the IBM 650, also announced in 1953 had about 8.5 kilobytes of drum memory.

Magnetic core memory patented in 1949 with its first usage demonstrated for the Whirlwind computer in August 1953. Commercialization followed quickly. Magnetic core was used in peripherals of the IBM 702 delivered in July 1955, and later in the 702 itself. The IBM 704 (1955) and the Ferranti Mercury (1957) used magnetic-core memory. It went on to dominate the field into the 1970s, when it was replaced with semiconductor memory. Magnetic core peaked in volume about 1975 and declined in usage and market share thereafter.

As late as 1980, PDP-11/45 machines using magnetic-core main memory and drums for swapping were still in use at many of the original UNIX sites.

The bipolar transistor was invented in 1947. From 1955 onward transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. Transistors greatly reduced computers' size, initial cost, and operating cost. Typically, second-generation computers were composed of large numbers of printed circuit boards such as the IBM Standard Modular System, each carrying one to four logic gates or flip-flops.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Initially the only devices available were germanium point-contact transistors, less reliable than the valves they replaced but which consumed far less power. Their first transistorised computer, and the first in the world, was operational by 1953, and a second version was completed there in April 1955. The 1955 version used 200 transistors, 1,300 solid-state diodes, and had a power consumption of 150 watts. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer.

That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell. The design featured a 64-kilobyte magnetic drum memory store with multiple moving heads that had been designed at the National Physical Laboratory, UK. By 1953 this team had transistor circuits operating to read and write on a smaller magnetic drum from the Royal Radar Establishment. The machine used a low clock speed of only 58 kHz to avoid having to use any valves to generate the clock waveforms.

CADET used 324 point-contact transistors provided by the UK company Standard Telephones and Cables; 76 junction transistors were used for the first stage amplifiers for data read from the drum, since point-contact transistors were too noisy. From August 1956 CADET was offering a regular computing service, during which it often executed continuous computing runs of 80 hours or more. Problems with the reliability of early batches of point contact and alloyed junction transistors meant that the machine's mean time between failures was about 90 minutes, but this improved once the more reliable bipolar junction transistors became available.

The Manchester University Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere. Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years. A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.

Transistorized electronics improved not only the CPU (Central Processing Unit), but also the peripheral devices. The second generation disk data storage units were able to store tens of millions of letters and digits. Next to the fixed disk storage units, connected to the CPU via high-speed data transmission, were removable disk data storage units. A removable disk pack can be easily exchanged with another pack in a few seconds. Even if the removable disks' capacity is smaller than fixed disks, their interchangeability guarantees a nearly unlimited quantity of data close at hand. Magnetic tape provided archival capability for this data, at a lower cost than disk.

Many second-generation CPUs delegated peripheral device communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took at least two memory cycles; one for the instruction, one for the operand data fetch.

During the second generation remote terminal units (often in the form of Teleprinters like a Friden Flexowriter) saw greatly increased use. Telephone connections provided sufficient speed for early remote terminals and allowed hundreds of kilometers separation between remote-terminals and the computing center. Eventually these stand-alone computer networks would be generalized into an interconnected "network of networks"—the Internet.

The early 1960s saw the advent of supercomputing. The Atlas was a joint development between the University of Manchester, Ferranti, and Plessey, and was first installed at Manchester University and officially commissioned in 1962 as one of the world's first supercomputers – considered to be the most powerful computer in the world at that time. It was said that whenever Atlas went offline half of the United Kingdom's computer capacity was lost. It was a second-generation machine, using discrete germanium transistors. Atlas also pioneered the Atlas Supervisor, "considered by many to be the first recognisable modern operating system".

In the US, a series of computers at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance. The CDC 6600, released in 1964, is generally considered the first supercomputer. The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of 3. With performance of about 1 megaFLOPS, the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.

The "third-generation" of digital electronic computers used integrated circuit (IC) chips as the basis of their logic.

The idea of an integrated circuit was conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer.

The first working integrated circuits were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. Kilby's invention was a hybrid integrated circuit (hybrid IC). It had external wire connections, which made it difficult to mass-produce.

Noyce came up with his own idea of an integrated circuit half a year after Kilby. Noyce's invention was a monolithic integrated circuit (IC) chip. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. The basis for Noyce's monolithic IC was Fairchild's planar process, which allowed integrated circuits to be laid out using the same principles as those of printed circuits. The planar process was developed by Noyce's colleague Jean Hoerni in early 1959, based on the silicon surface passivation and thermal oxidation processes developed by Mohamed M. Atalla at Bell Labs in the late 1950s.

Third generation (integrated circuit) computers first appeared in the early 1960s in computers developed for government purposes, and then in commercial computers beginning in the mid-1960s.

The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. In addition to data processing, the MOSFET enabled the practical use of MOS transistors as memory cell storage elements, a function previously served by magnetic cores. Semiconductor memory, also known as MOS memory, was cheaper and consumed less power than magnetic-core memory. MOS random-access memory (RAM), in the form of static RAM (SRAM), was developed by John Schmidt at Fairchild Semiconductor in 1964. In 1966, Robert Dennard at the IBM Thomas J. Watson Research Center developed MOS dynamic RAM (DRAM). In 1967, Dawon Kahng and Simon Sze at Bell Labs developed the floating-gate MOSFET, the basis for MOS non-volatile memory such as EPROM, EEPROM and flash memory.

The "fourth-generation" of digital electronic computers used microprocessors as the basis of their logic. The microprocessor has origins in the MOS integrated circuit (MOS IC) chip. The MOS IC was first proposed by Mohamed M. Atalla at Bell Labs in 1960, and then fabricated by Fred Heiman and Steven Hofstein at RCA in 1962. Due to rapid MOSFET scaling, MOS IC chips rapidly increased in complexity at a rate predicted by Moore's law, leading to large-scale integration (LSI) with hundreds of transistors on a single MOS chip by the late 1960s. The application of MOS LSI chips to computing was the basis for the first microprocessors, as engineers began recognizing that a complete computer processor could be contained on a single MOS LSI chip.

The subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor". The earliest multi-chip microprocessors were the Four-Phase Systems AL-1 in 1969 and Garrett AiResearch MP944 in 1970, developed with multiple MOS LSI chips. The first single-chip microprocessor was the Intel 4004, developed on a single PMOS LSI chip. It was designed and realized by Ted Hoff, Federico Faggin, Masatoshi Shima and Stanley Mazor at Intel, and released in 1971. Tadashi Sasaki and Masatoshi Shima at Busicom, a calculator manufacturer, had the initial insight that the CPU could be a single MOS LSI chip, supplied by Intel.
While the earliest microprocessor ICs literally contained only the processor, i.e. the central processing unit, of a computer, their progressive development naturally led to chips containing most or all of the internal electronic parts of a computer. The integrated circuit in the image on the right, for example, an Intel 8742, is an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip.

During the 1960s there was considerable overlap between second and third generation technologies. IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities. It became possible to simulate analog circuits with the "simulation program with integrated circuit emphasis", or SPICE (1971) on minicomputers, one of the programs for electronic design automation ().
The microprocessor led to the development of the microcomputer, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.
While which specific system is considered the first microcomputer is a matter of debate, as there were several unique hobbyist systems developed based on the Intel 4004 and its successor, the Intel 8008, the first commercially available microcomputer kit was the Intel 8080-based Altair 8800, which was announced in the January 1975 cover article of "Popular Electronics". However, this was an extremely limited system in its initial stages, having only 256 bytes of DRAM in its initial package and no input-output except its toggle switches and LED register display. Despite this, it was initially surprisingly popular, with several hundred sales in the first year, and demand rapidly outstripped supply. Several early third-party vendors such as Cromemco and Processor Technology soon began supplying additional S-100 bus hardware for the Altair 8800.

In April 1975 at the Hannover Fair, Olivetti presented the P6060, the world's first complete, pre-assembled personal computer system. The central processing unit consisted of two cards, code named PUCE1 and PUCE2, and unlike most other personal computers was built with TTL components rather than a microprocessor. It had one or two 8" floppy disk drives, a 32-character plasma display, 80-column graphical thermal printer, 48 Kbytes of RAM, and BASIC language. It weighed . As a complete system, this was a significant step from the Altair, though it never achieved the same success. It was in competition with a similar product by IBM that had an external floppy disk drive.

From 1975 to 1977, most microcomputers, such as the MOS Technology KIM-1, the Altair 8800, and some versions of the Apple I, were sold as kits for do-it-yourselfers. Pre-assembled systems did not gain much ground until 1977, with the introduction of the Apple II, the Tandy TRS-80, the first SWTPC computers, and the Commodore PET. Computing has evolved with microcomputer architectures, with features added from their larger brethren, now dominant in most market segments.

A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb.

Systems as complicated as computers require very high reliability. ENIAC remained on, in continuous operation from 1947 to 1955, for eight years before being shut down. Although a vacuum tube might fail, it would be replaced without bringing down the system. By the simple strategy of never shutting down ENIAC, the failures were dramatically reduced. The vacuum-tube SAGE air-defense computers became remarkably reliable – installed in pairs, one off-line, tubes likely to fail did so when the computer was intentionally run at reduced power to find them. Hot-pluggable hard disks, like the hot-pluggable vacuum tubes of yesteryear, continue the tradition of repair during continuous operation. Semiconductor memories routinely have no errors when they operate, although operating systems like Unix have employed memory tests on start-up to detect failing hardware. Today, the requirement of reliable performance is made even more stringent when server farms are the delivery platform. Google has managed this by using fault-tolerant software to recover from hardware failures, and is even working on the concept of replacing entire server farms on-the-fly, during a service event.

In the 21st century, multi-core CPUs became commercially available. Content-addressable memory (CAM) has become inexpensive enough to be used in networking, and is frequently used for on-chip cache memory in modern microprocessors, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.

This has allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S. The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices.

MIT Technology Review reported 10 November 2017 that IBM has created a 50-qubit computer; currently its quantum state lasts 50 microseconds. Physical Review X reported a technique for 'single-gate sensing as a viable readout method for spin qubits' (a singlet-triplet spin state in silicon) on 26 November 2018. A Google team has succeeded in operating their RF pulse modulator chip at 3 Kelvin, simplifying the cryogenics of their 72-qubit computer, which is setup to operate at 0.3 Kelvin; but the readout circuitry and another driver remain to be brought into the cryogenics. "See: Quantum supremacy" Silicon qubit systems have demonstrated entanglement at non-local distances.

Computing hardware and its software have even become a metaphor for the operation of the universe.

An indication of the rapidity of development of this field can be inferred from the history of the seminal 1947 article by Burks, Goldstine and von Neumann. By the time that anyone had time to write anything down, it was obsolete. After 1945, others read John von Neumann's "First Draft of a Report on the EDVAC", and immediately started implementing their own systems. To this day, the rapid pace of development has continued, worldwide.

A 1966 article in "Time" predicted that: "By 2000, the machines will be producing so much that everyone in the U.S. will, in effect, be independently wealthy. How to use leisure time will be a major problem."






</doc>
<doc id="13637" url="https://en.wikipedia.org/wiki?curid=13637" title="Hausdorff space">
Hausdorff space

In topology and related branches of mathematics, a Hausdorff space, separated space or T space is a topological space where for any two distinct points there exist neighbourhoods of each which are disjoint from each other. Of the many separation axioms that can be imposed on a topological space, the "Hausdorff condition" (T) is the most frequently used and discussed. It implies the uniqueness of limits of sequences, nets, and filters.

Hausdorff spaces are named after Felix Hausdorff, one of the founders of topology. Hausdorff's original definition of a topological space (in 1914) included the Hausdorff condition as an axiom.

Points formula_1 and formula_2 in a topological space formula_3 can be "separated by neighbourhoods" if there exists a neighbourhood formula_4 of formula_1 and a neighbourhood formula_6 of formula_2 such that formula_4 and formula_6 are disjoint (formula_10).
formula_3 is a Hausdorff space if all distinct points in formula_3 are pairwise neighbourhood-separable. This condition is the third separation axiom (after formula_13), which is why Hausdorff spaces are also called formula_14 spaces. The name "separated space" is also used.

A related, but weaker, notion is that of a preregular space. formula_3 is a preregular space if any two topologically distinguishable points can be separated by disjoint neighbourhoods. Preregular spaces are also called "formula_16 spaces".

The relationship between these two conditions is as follows. A topological space is Hausdorff if and only if it is both preregular (i.e. topologically distinguishable points are separated by neighbourhoods) and Kolmogorov (i.e. distinct points are topologically distinguishable). A topological space is preregular if and only if its Kolmogorov quotient is Hausdorff.

For a topological space "X", the following are equivalent:


Almost all spaces encountered in analysis are Hausdorff; most importantly, the real numbers (under the standard metric topology on real numbers) are a Hausdorff space. More generally, all metric spaces are Hausdorff. In fact, many spaces of use in analysis, such as topological groups and topological manifolds, have the Hausdorff condition explicitly stated in their definitions.

A simple example of a topology that is T but is not Hausdorff is the cofinite topology defined on an infinite set.

Pseudometric spaces typically are not Hausdorff, but they are preregular, and their use in analysis is usually only in the construction of Hausdorff gauge spaces. Indeed, when analysts run across a non-Hausdorff space, it is still probably at least preregular, and then they simply replace it with its Kolmogorov quotient, which is Hausdorff.

In contrast, non-preregular spaces are encountered much more frequently in abstract algebra and algebraic geometry, in particular as the Zariski topology on an algebraic variety or the spectrum of a ring. They also arise in the model theory of intuitionistic logic: every complete Heyting algebra is the algebra of open sets of some topological space, but this space need not be preregular, much less Hausdorff, and in fact usually is neither. The related concept of Scott domain also consists of non-preregular spaces.

While the existence of unique limits for convergent nets and filters implies that a space is Hausdorff, there are non-Hausdorff T spaces in which every convergent sequence has a unique limit.

Subspaces and products of Hausdorff spaces are Hausdorff, but quotient spaces of Hausdorff spaces need not be Hausdorff. In fact, "every" topological space can be realized as the quotient of some Hausdorff space.

Hausdorff spaces are T, meaning that all singletons are closed. Similarly, preregular spaces are R.

Another nice property of Hausdorff spaces is that compact sets are always closed. This may fail in non-Hausdorff spaces such as the Sierpiński space. 

The definition of a Hausdorff space says that points can be separated by neighborhoods. It turns out that this implies something which is seemingly stronger: in a Hausdorff space every pair of disjoint compact sets can also be separated by neighborhoods, in other words there is a neighborhood of one set and a neighborhood of the other, such that the two neighborhoods are disjoint. This is an example of the general rule that compact sets often behave like points.

Compactness conditions together with preregularity often imply stronger separation axioms. For example, any locally compact preregular space is completely regular. Compact preregular spaces are normal, meaning that they satisfy Urysohn's lemma and the Tietze extension theorem and have partitions of unity subordinate to locally finite open covers. The Hausdorff versions of these statements are: every locally compact Hausdorff space is Tychonoff, and every compact Hausdorff space is normal Hausdorff.

The following results are some technical properties regarding maps (continuous and otherwise) to and from Hausdorff spaces.

Let "f" : "X" → "Y" be a continuous function and suppose "Y" is Hausdorff. Then the graph of "f", formula_17, is a closed subset of "X" × "Y".

Let "f" : "X" → "Y" be a function and let formula_18 be its kernel regarded as a subspace of "X" × "X".

If "f,g" : "X" → "Y" are continuous maps and "Y" is Hausdorff then the equalizer formula_19 is closed in "X". It follows that if "Y" is Hausdorff and "f" and "g" agree on a dense subset of "X" then "f" = "g". In other words, continuous functions into Hausdorff spaces are determined by their values on dense subsets.

Let "f" : "X" → "Y" be a closed surjection such that "f"("y") is compact for all "y" ∈ "Y". Then if "X" is Hausdorff so is "Y".

Let "f" : "X" → "Y" be a quotient map with "X" a compact Hausdorff space. Then the following are equivalent:

All regular spaces are preregular, as are all Hausdorff spaces. There are many results for topological spaces that hold for both regular and Hausdorff spaces.
Most of the time, these results hold for all preregular spaces; they were listed for regular and Hausdorff spaces separately because the idea of preregular spaces came later.
On the other hand, those results that are truly about regularity generally do not also apply to nonregular Hausdorff spaces.

There are many situations where another condition of topological spaces (such as paracompactness or local compactness) will imply regularity if preregularity is satisfied.
Such conditions often come in two versions: a regular version and a Hausdorff version.
Although Hausdorff spaces are not, in general, regular, a Hausdorff space that is also (say) locally compact will be regular, because any Hausdorff space is preregular.
Thus from a certain point of view, it is really preregularity, rather than regularity, that matters in these situations.
However, definitions are usually still phrased in terms of regularity, since this condition is better known than preregularity.

See History of the separation axioms for more on this issue.

The terms "Hausdorff", "separated", and "preregular" can also be applied to such variants on topological spaces as uniform spaces, Cauchy spaces, and convergence spaces.
The characteristic that unites the concept in all of these examples is that limits of nets and filters (when they exist) are unique (for separated spaces) or unique up to topological indistinguishability (for preregular spaces).

As it turns out, uniform spaces, and more generally Cauchy spaces, are always preregular, so the Hausdorff condition in these cases reduces to the T condition.
These are also the spaces in which completeness makes sense, and Hausdorffness is a natural companion to completeness in these cases.
Specifically, a space is complete if and only if every Cauchy net has at "least" one limit, while a space is Hausdorff if and only if every Cauchy net has at "most" one limit (since only Cauchy nets can have limits in the first place).

The algebra of continuous (real or complex) functions on a compact Hausdorff space is a commutative C*-algebra, and conversely by the Banach–Stone theorem one can recover the topology of the space from the algebraic properties of its algebra of continuous functions. This leads to noncommutative geometry, where one considers noncommutative C*-algebras as representing algebras of functions on a noncommutative space.





</doc>
<doc id="13644" url="https://en.wikipedia.org/wiki?curid=13644" title="Hawkwind">
Hawkwind

Hawkwind are an English rock band known as one of the earliest space rock groups. Since their formation in November 1969, Hawkwind have gone through many incarnations and have incorporated many different styles into their music, including hard rock, progressive rock and psychedelic rock. They are also regarded as an influential proto-punk band. Their lyrics favour urban and science fiction themes.

Many musicians, dancers and writers have worked with the band since their inception. Notable musicians who have performed in Hawkwind include Lemmy, Ginger Baker, Robert Calvert, Nik Turner and Huw Lloyd-Langton. However, the band are most closely associated with their founder, singer, songwriter and guitarist Dave Brock, who is the only remaining original member. 

Hawkwind are best known for the song "Silver Machine", which became a number three UK hit single in 1972, but they scored further hit singles with "Urban Guerrilla" (another Top 40 hit) and "Shot Down in the Night". The band had a run of twenty-two of their albums charting in the UK from 1971 to 1993. 

Dave Brock and Mick Slattery had been in the London-based psychedelic band Famous Cure, and a meeting with bassist John Harrison revealed a mutual interest in electronic music which led the trio to embark upon a new musical venture together. Seventeen-year-old drummer Terry Ollis replied to an advert in a music weekly, while Nik Turner and Michael "Dik Mik" Davies, old acquaintances of Brock, offered help with transport and gear, but were soon pulled into the band.

Gatecrashing a local talent night at the All Saints Hall, Notting Hill, they were so disorganised as to not even have a name, opting for "Group X" at the last minute, nor any songs, choosing to play an extended 20-minute jam on the Byrds' "Eight Miles High". BBC Radio 1 DJ John Peel was in the audience and was impressed enough to tell event organiser, Douglas Smith, to keep an eye on them. Smith signed them up and got them a deal with Liberty Records on the back of a deal he was setting up for Cochise.

The band settled on the name "Hawkwind" after briefly being billed as "Group X" and "Hawkwind Zoo".

An Abbey Road session took place recording demos of "Hurry on Sundown" and others (included on the remasters version of "Hawkwind"), after which Slattery left to be replaced by Huw Lloyd-Langton, who had known Brock from his days working in a music shop selling guitar strings to Brock, then a busker.

Pretty Things guitarist Dick Taylor was brought in to produce the 1970 debut album "Hawkwind". Although it was not a commercial success, it did bring them to the attention of the UK underground scene, which found them playing free concerts, benefit gigs, and festivals. Playing free outside the Bath Festival, they encountered another Ladbroke Grove based band, the Pink Fairies, who shared similar interests in music and recreational activities; a friendship developed which led to the two bands becoming running partners and performing as "Pinkwind". Their use of drugs, however, led to the departure of Harrison, who did not partake, to be replaced briefly by Thomas Crimble (about July 1970 – March 1971). Crimble played on a few BBC sessions before leaving to help organise the Glastonbury Free Festival 1971; he sat in during the band's performance there. Lloyd-Langton also quit, after a bad LSD trip at the Isle of Wight Festival led to a nervous breakdown.

Their follow-up album, 1971's "In Search of Space", brought greater commercial success, reaching number 18 on the UK album charts. This album offered a refinement of the band's image and philosophy courtesy of graphic artist Barney Bubbles and underground press writer Robert Calvert, as depicted in the accompanying "Hawklog" booklet, which would be further developed into the "Space Ritual" stage show. Science fiction author Michael Moorcock and dancer Stacia also started contributing to the band. Dik Mik had left the band, replaced by sound engineer Del Dettmar, but chose to return for this album giving the band two electronics players. Bass player Dave Anderson, who had been in the German band Amon Düül II, had also joined and played on the album but departed before its release because of personal tensions with some other members of the band. Anderson and Lloyd-Langton then formed the short-lived band Amon Din. Meanwhile, Ollis quit, unhappy with the commercial direction the band were heading in.
The addition of bassist Ian "Lemmy" Kilmister and drummer Simon King propelled the band to greater heights. One of the early gigs the band played was a benefit for the Greasy Truckers at The Roundhouse on 13 February 1972. A live album of the concert, "Greasy Truckers Party", was released, and after re-recording the vocal, a single, "Silver Machine", was also released, reaching number three in the UK charts. This generated sufficient funds for the subsequent album "Doremi Fasol Latido" Space Ritual tour. The show featured dancers Stacia and Miss Renee typically performing either topless or wearing only body paint, mime artist Tony Carrera and a light show by Liquid Len and was recorded on the elaborate package "Space Ritual". At the height of their success, in 1973, the band released the single "Urban Guerrilla", which coincided with an IRA bombing campaign in London, so the BBC refused to play it and the band's management reluctantly decided to withdraw it fearing accusations of opportunism, despite the disc having already climbed to number 39 in the UK chart.

Dik Mik departed during 1973 and Calvert ended his association with the band to concentrate on solo projects. Dettmar also indicated that he was to leave the band, so Simon House was recruited as keyboardist and violinist playing live shows, a North America tour and recording the 1974 album "Hall of the Mountain Grill". Dettmar left after a European tour and emigrated to Canada, whilst Alan Powell deputised for an incapacitated King on that European tour, but remained giving the band two drummers.

At the beginning of 1975, the band recorded the album "Warrior on the Edge of Time" in collaboration with Michael Moorcock, loosely based on his Eternal Champion figure. However, during a North American tour in May, Lemmy was caught in possession of amphetamine crossing the border from the US into Canada. The border police mistook the powder for cocaine and he was jailed, forcing the band to cancel some shows. Fed up with his erratic behaviour, the band dismissed the bass player replacing him with their long-standing friend and former Pink Fairies guitarist Paul Rudolph. Lemmy then teamed up with another Pink Fairies guitarist, Larry Wallis, to form Motörhead, named after the last song he had written for Hawkwind.

Calvert made a guest appearance with the band for their headline set at the Reading Festival in August 1975, after which he chose to rejoin the band as a full-time lead vocalist. Stacia chose to relinquish her dancing duties and settle down to family life. The band changed record company to Tony Stratton-Smith's Charisma Records and, on Stratton-Smith's suggestion, band management from Douglas Smith to Tony Howard.

"Astounding Sounds, Amazing Music" is the first album of this era. On the eve of recording the follow-up "Back on the Streets" single, Turner was dismissed for his erratic live playing and Powell was deemed surplus to requirements. After a tour to promote the single and during the recording of the next album, Rudolph was also dismissed, for allegedly trying to steer the band into a musical direction at odds with Calvert and Brock's vision.

Adrian "Ade" Shaw, who, as bass player for Magic Muscle, had supported Hawkwind on the "Space Ritual" tour, came in for the 1977 album "Quark, Strangeness and Charm". The band continued to enjoy moderate commercial success, but Calvert's mental illness often caused problems. A manic phase saw the band abandon a European tour in France, while a depression phase during a 1978 North American tour convinced Brock to disband the group. In between these two tours, the band had recorded the album "PXR5" in January 1978, but its release was delayed until 1979.

On 23 December 1977 in Barnstaple, Brock and Calvert had performed a one-off gig with Devon band Ark as the Sonic Assassins, and looking for a new project in 1978, bassist Harvey Bainbridge and drummer Martin Griffin were recruited from this event. Steve Swindells was recruited as keyboard player. The band was named Hawklords, (probably for legal reasons, the band having recently split from their management), and recording took place on a farm in Devon using a mobile studio, resulting in the album "25 Years On". King had originally been the drummer for the project but quit during recording sessions to return to London, while House, who had temporarily left the band to join a David Bowie tour, elected to remain with Bowie full-time, but nevertheless contributed violin to these sessions. At the end of the band's UK tour, Calvert, wanting King back in the band, dismissed Griffin, then promptly resigned himself, choosing to pursue a career in literature. Swindells left to record a solo album after an offer had been made to him by the record company ATCO.

In late 1979, Hawkwind reformed with Brock, Bainbridge and King being joined by Huw Lloyd-Langton (who had played on the debut album) and Tim Blake (formerly of Gong), embarking upon a UK tour despite not having a record deal or any product to promote. Some shows were recorded and a deal was made with Bronze Records, resulting in the "Live Seventy Nine" album, quickly followed by the studio album "Levitation". However, during the recording of "Levitation" King quit and Ginger Baker was drafted in for the sessions, but he chose to stay with the band for the tour, during which Blake left to be replaced by Keith Hale.

In 1981 Baker and Hale left after their insistence that Bainbridge should be dismissed was ignored, and Brock and Bainbridge elected to handle synthesisers and sequencers themselves, with drummer Griffin from the Hawklords rejoining. Three albums, which again saw Moorcock contributing lyrics and vocals, were recorded for RCA/Active: "Sonic Attack", the electronic "Church of Hawkwind" and "Choose Your Masques". This band headlined the 1981 Glastonbury Festival and made an appearance at the 1982 Donington Monsters of Rock Festival, as well as continuing to play the summer solstice at Stonehenge Free Festival.

In the early 1980s, Brock had started using drum machines for his home demos and became increasingly frustrated at the inability of drummers to keep perfect time, leading to a succession of drummers coming and going. First, Griffin was ousted and the band tried King again, but, unhappy with his playing at that time, he was rejected. Andy Anderson briefly joined while he was also playing for The Cure, and Robert Heaton also filled the spot briefly prior to the rise of New Model Army. Lloyd Langton Group drummer John Clark did some recording sessions, and in late 1983 Rick Martinez joined the band to play drums on the "Earth Ritual" tour in February and March 1984, later replaced by Clive Deamer.

Turner had returned as a guest for the 1982 "Choose Your Masques" tour and was invited back permanently. Further tours ensued with Phil "Dead Fred" Reeves augmenting the line-up on keyboards and violin, but neither Turner nor Reeves would appear on the only recording of 1983–84, "The Earth Ritual Preview", however there was a guest spot for Lemmy. The "Earth Ritual" tour was filmed for Hawkwind's first video release, "Night of the Hawk".

Alan Davey was a young fan of the band who had sent a tape of his playing to Brock, and Brock chose to oust Reeves moving Bainbridge from bass to keyboards to accommodate Davey. This experimental line-up played at the Stonehenge Free Festival in 1984, which was filmed and release as "Stonehenge 84". Subsequent personal and professional tensions between Brock and Turner led to the latter's expulsion at the beginning of 1985. Clive Deamer, who was deemed "too professional" for the band, was eventually replaced in 1985 by Danny Thompson Jr (son of folk-rock bassist Danny Thompson), a friend of Alan Davey, and remained almost to the end of the decade.

Hawkwind's association with Moorcock climaxed in their most ambitious project, "The Chronicle of the Black Sword", based loosely around the Elric series of books and theatrically staged with Tony Crerar as the central character. Moorcock contributed lyrics, but only performed some spoken pieces on some live dates. The tour was recorded and issued as an album "Live Chronicles" and video "The Chronicle of the Black Sword". The band also performed the at the Worldcon (World Science Fiction Convention) in Brighton.

A headline appearance at the 1986 Reading Festival was followed by a UK tour to promote the "Live Chronicles" album which was filmed and released as "Chaos". In 1988 the band recorded the album "The Xenon Codex" with Guy Bidmead, but all was not well in the band and soon after, both Lloyd-Langton and Thompson departed.

Drummer Richard Chadwick, who joined in the summer of 1988, had been playing in small alternative free festival bands, most notably Bath's Smart Pils, for a decade and had frequently crossed paths with Hawkwind and Brock. He was initially invited simply to play with the band, but eventually replaced stand in drummer Mick Kirton to become the band's drummer to the present day.

To fill in the gap of lead sound, lost when Lloyd-Langton left, violinist House was re-instated into the line-up in 1989 (having previously been a member from 1974 until 1978), and, notably, Hawkwind embarked on their first North American visit in eleven years (since the somewhat disastrous 1978 tour), in which House did not partake. The successfully received tour was the first of several over the coming years, in an effort by the band to re-introduce themselves to the American market.

Bridget Wishart, an associate of Chadwick's from the festival circuit, also joined to become the band's one and only singing front-woman, the band had been fronted in earlier days by Stacia but only as a dancer. This band produced two albums, 1990's "Space Bandits" and 1991's "Palace Springs" and also filmed a 1-hour appearance for the "Bedrock TV" series with dancer Julie Murray-Anderson, who performed with Hawkwind between 1988 and 1991.

1990 saw Hawkwind tour North America again, the second instalment in a series of American visits made at around this time in an effort to re-establish the Hawkwind brand in America. The original business plan was to hold three consecutive US tours, annually, from 1989–1991, with the first losing money, the second breaking even, and the third turning a profit, ultimately bringing Hawkwind back into recognition across the Atlantic. Progress, however, was somewhat stunted, due to ex-member Nik Turner touring the United States with his own band at the time, in which the shows were often marketed as Hawkwind.

Still supporting Space Bandits, 1991 commenced with perhaps the most surprising Hawkwind tour in the band's history, without Dave Brock. Brock's temporary replacement was former Smart Pils guitarist Steve Bemand (who had played with Chadwick and Wishart in the Demented Stoats). The tour began in Amsterdam on 12 March and took in Germany, Greece, Italy and France before wrapping up in Belgium on 10 April after 24 dates.

In 1991 Bainbridge, House and Wishart departed and the band continued as a three piece relying heavily on synthesisers and sequencers to create a wall-of-sound. The 1992 album "Electric Tepee" combined hard rock and light ambient pieces, while "It is the Business of the Future to be Dangerous" is almost devoid of the rock leanings. "The Business Trip" is a record of the previous album's tour, but rockier as would be expected from a live outing. The "White Zone" album was released under the alias Psychedelic Warriors to distance itself entirely from the rock expectancy of Hawkwind.

A general criticism of techno music at that time was its facelessness and lack of personality, which the band were coming to feel also plagued them. Ron Tree had known the band on the festival circuit and offered his services as a front-man, and the band duly employed him for the album "Alien 4" and its accompanying tour which resulted in the album "Love in Space" and "video".

In 1996, unhappy with the musical direction of the band, bassist Davey left, forming his own Middle-Eastern flavoured hard-rock group Bedouin and a Motörhead tribute act named Ace of Spades. His bass playing role was reluctantly picked up by singer Tree and the band were joined full-time by lead guitarist Jerry Richards (another stalwart of the festival scene, playing for Tubilah Dog who had merged with Brock's Agents of Chaos during 1988) for the albums "Distant Horizons" and "In Your Area". Rasta chanter Captain Rizz also joined the band for guest spots during live shows.

Hawkestra — a re-union event featuring appearances from past and present members — had originally been intended to coincide with the band's 30th anniversary and the release of the career spanning "Epocheclipse – 30 Year Anthology" set, but logistical problems delayed it until 21 October 2000. It took place at the Brixton Academy with about 20 members taking part in a more than 3-hour set, which was filmed and recorded. Guests included Samantha Fox who sang "Master of the Universe". However, arguments and disputes over financial recompense and musical input resulted in the prospect of the event being re-staged unlikely, and any album or DVD release being indefinitely shelved.

The Hawkestra had set a template for Brock to assemble a core band of Tree, Brock, Richards, Davey, Chadwick and for the use of former members as guests on live shows and studio recordings. The 2000 Christmas Astoria show was recorded with contributions from House, Blake, Rizz, Moorcock, Jez Huggett and Keith Kniveton and released as "Yule Ritual" the following year. In 2001, Davey agreed to rejoin the band permanently, but only after the departure of Tree and Richards.

Meanwhile, having rekindled relationships with old friends at the Hawkestra, Turner organised further Hawkestra gigs resulting in the formation of xhawkwind.com, a band consisting mainly of ex-Hawkwind members and playing old Hawkwind songs. An appearance at Guilfest in 2002 led to confusion as to whether this actually was Hawkwind, sufficiently irking Brock into taking legal action to prohibit Turner from trading under the name Hawkwind. Turner lost the case and the band began performing as Space Ritual.

An appearance at the Canterbury Sound Festival in August 2001, resulting in another live album "Canterbury Fayre 2001", saw guest appearances from Lloyd-Langton, House, Kniveton with Arthur Brown on "Silver Machine". The band organised the first of their own weekend festivals, named Hawkfest, in Devon in the summer of 2002. Brown joined the band in 2002 for a Winter tour which featured some Kingdom Come songs and saw appearances from Blake and Lloyd-Langton, the Newcastle show being released on DVD as "Out of the Shadows" and the London show on CD as "Spaced Out in London".

In 2005 a new album "Take Me to Your Leader" was released. Recorded by the core band of Brock/Davey/Chadwick, contributors included new keyboardist Jason Stuart, Arthur Brown, tabloid writer and TV personality Matthew Wright, 1970s New Wave singer Lene Lovich, Simon House and Jez Huggett. This was followed in 2006 by the CD/DVD "Take Me to Your Future".

The band were the subject of an hour-long television documentary entitled "Hawkwind: Do Not Panic" that aired on BBC Four as part of the "Originals" series. It was broadcast on 30 March 2007 and repeated on 10 August 2007. Although Brock participated in its making he did not appear in the programme, it is alleged that he requested all footage of himself be removed after he was denied any artistic control over the documentary. In one of the documentary's opening narratives regarding Brock, it is stated that he declined to be interviewed for the programme because of Nik Turner's involvement, indicating that the two men have still not reconciled over the xhawkwind.com incident.

December 2006 saw the official departure of Alan Davey, who left to perform and record with two new bands: Gunslinger and Thunor. He was replaced by Mr Dibs, a long-standing member of the road crew. The band performed at their annual Hawkfest festival and headlined the US festival Nearfest and played gigs in PA and NY. At the end of 2007, Tim Blake once again joined the band filling the lead role playing keyboards and theremin. The band played 5 Christmas dates, the London show being released as an audio CD and video DVD under the title "Knights of Space".

In January 2008 the band reversed its anti-taping policy, long a sore-point with many fans, announcing that it would allow audio recording and non-commercial distribution of such recordings, provided there was no competing official release. At the end of 2008, Atomhenge Records (a subsidiary of Cherry Red Records) commenced the re-issuing of Hawkwind's back catalogue from the years 1976 through to 1997 with the release of two triple CD anthologies "Spirit of the Age (anthology 1976–84)" and "The Dream Goes On (anthology 1985–97)".

On 8 September 2008 keyboard player Jason Stuart died due to a brain haemorrhage. In October 2008, Niall Hone (former Tribe of Cro) joined Hawkwind for their Winter 2008 tour playing guitar, along with returning synth/theremin player Tim Blake. In this period, Hone also occasionally played bass guitar alongside Mr Dibs and used laptops for live electronic improvisation.

In 2009, the band began occasionally featuring Jon Sevink from The Levellers as guest violinist at some shows. Later that year, Hawkwind embarked on a winter tour to celebrate the band's 40th anniversary, including two gigs on 28 and 29 August marking the anniversary of their first live performances. In 2010, Hawkwind held their annual Hawkfest at the site of the original Isle of Wight Festival, marking the 40th anniversary of their appearance there.

On 21 June 2010, Hawkwind released a studio album entitled "Blood of the Earth" on Eastworld Records. During and since the "Blood of the Earth" support tours, Hone's primary on-stage responsibility shifted to bass, while Mr. Dibs moved to a more traditional lead singer/front man role.

In 2011, Hawkwind toured Australia for the second time.

April 2012 saw the release of a new album, "Onward", again on Eastworld. Keyboardist Dead Fred rejoined Hawkwind for the 2012 tour in support of "Onward" and has since remained with the band. In November 2012, Brock, Chadwick and Hone — credited as "Hawkwind Light Orchestra" — released "Stellar Variations" on Esoteric Recordings.

2013 marked the first Hawkeaster, a two-day festival held in Seaton, Devon during the Easter weekend. A US tour was booked for October 2013, but due to health issues, was postponed and later cancelled.

In February 2014, as part of a one-off Space Ritual performance, Hawkwind performed at the O2 Shepherd's Bush Empire featuring an appearance by Brian Blessed for the spoken word element of Sonic Attack; a studio recording of this performance was released as a single in September 2014. Later in the year, former Soft Machine guitarist John Etheridge joined the live line-up of the band, though he had departed again prior to early 2015 dates.

Following Hawkeaster 2015, Hawkwind made their debut visit to Japan, playing two sold-out shows in Tokyo. Hawkwind performed two Solstice Ritual shows in December 2015, with Steve Hillage guesting, and Haz Wheaton joining Hawkwind on bass guitar. Wheaton is a former member of the band's road crew who had previously appeared with Technicians of Spaceship Hawkwind, a "skeleton crew" spin off live band. Additionally, he had guested on bass for Dave Brock's solo album "Brockworld" released earlier in the year.

The band released "The Machine Stops" on 15 April 2016. The album marked Wheaton's first appearance on a Hawkwind studio album, and the first album without Tim Blake's involvement since he had rejoined the band in 2010 and appeared on "Blood of the Earth". His departure was offset by increased synthesiser work by Hone and Brock.

Dead Fred's last live appearance with Hawkwind was at The Eastbourne Winter Gardens April 1, 2016. Hone took over keyboards and synth duties live until though Blake returned for shows in summer 2016.

It was announced in November 2016 that Hawkwind were recording a new studio album, entitled "Into The Woods". Keyboardist-guitarist Magnus Martin replaced both Hone and Blake in the lineup for the new album, leaving the 2017 core band composed of Brock, Chadwick, Mr Dibs, Wheaton and Martin.

In 2018, Hawkwind recorded an acoustic album "The Road to Utopia" consisting primarily of cover versions of their 1970s songs with production, arrangement and additional orchestrations by Mike Batt and a guest appearance from Eric Clapton. Batt was scheduled to conduct a series concerts of Hawkwind songs featuring the band and orchestra in October and November.

In May 2018 Haz Wheaton left and later joined Electric Wizard. Niall Hone returned on bass. Mr Dibs left on August 22 stating ‘irreconcilable differences’ in a statement on the Hawkwind fans Facebook page.

In October 2019, Hawkwind released "All Aboard the Skylark," marketed as a return to their space rock roots. This was the first album with the line-up of Brock, Chadwick, Hone, and Martin. Accompanying the CD version, and sold as a separate vinyl LP, was "Acoustic Daze." This recording included tracks from the 2018 album "The Road to Utopia", minus the additions by Batt and Clapton.

Hawkwind have been cited as an influence by artists such as Al Jourgensen of Ministry,Monster Magnet, the Sex Pistols (who covered "Silver Machine"), Henry Rollins and Dez Cadena of Black Flag, Ty Segall, The Mekano Set, and Ozric Tentacles.

Hard rock musician Lemmy of the band Motörhead gained a lot from his tenure in Hawkwind. He has remarked, "I really found myself as an instrumentalist in Hawkwind. Before that I was just a guitar player who was pretending to be good, when actually I was no good at all. In Hawkwind I became a good bass player. It was where I learned I was good at something."

Current members




</doc>
<doc id="13645" url="https://en.wikipedia.org/wiki?curid=13645" title="Horse">
Horse

The horse ("Equus ferus caballus") is one of two extant subspecies of "Equus ferus". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, "Eohippus", into the large, single-toed animal of today. Humans began domesticating horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies "caballus" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.

Horses are adapted to run, allowing them to quickly escape predators, possessing an excellent sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horses tending to sleep significantly more than adults. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under a saddle or in a harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.

Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods", such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.

Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water, and shelter, as well as attention from specialists such as veterinarians and farriers.

Specific terms and specialized language are used to describe equine anatomy, different life stages, and colors and breeds.

Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was "Old Billy", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in "Guinness World Records" as the world's oldest living pony, died in 2007 at age 56.

Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.

The following terminology is used to describe horses of various ages:

In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.

The height of horses is measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.

In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to . The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation "h" or "hh" (for "hands high"). Thus, a horse described as "15.2 h" is 15 hands plus 2 inches, for a total of in height.
The size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from and can weigh from . Larger riding horses usually start at about and often are as tall as , weighing from . Heavy or draft horses are usually at least high and can be as tall as high. They can weigh from about .

The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood high and his peak weight was estimated at . The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is tall and weighs .

Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.

The traditional standard for height of a horse or a pony at maturity is . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under . For competition in the Western division of the United States Equestrian Federation, the cutoff is . The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than at the withers without shoes, which is just over 14.2 h, and , or just over 14.2 h, with shoes.

Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.

Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than , are classified by their registries as very small horses, not ponies.

Horses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.

Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.

Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the "extension gene" or "red factor," as its recessive form is "red" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.

Horses that have a white coat color are often mislabeled; a horse that looks "white" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no "albino" horses, defined as having both pink skin and red eyes.

Gestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an "anestrus" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.

Horses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.

Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.

The horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's four legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's "knee" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the "ankle") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the "knuckles" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.

The critical importance of the feet and legs is summed up by the traditional adage, "no foot, no horse". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average , travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.

Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called "tushes". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as "wolf" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or "bars" of the horse's mouth when the horse is bridled.

An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.

Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A horse will eat of food per day and, under normal use, drink of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can utilize cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or "water gut", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.

The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.

Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.

A horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.

Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well-developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.

Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.

All horses move naturally with four basic gaits: the four-beat walk, which averages ; the two-beat trot or jog at (faster for harness racing horses); the canter or lope, a three-beat gait that is ; and the gallop. The gallop averages , but the world record for a horse galloping over a short, sprint distance is . Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat "ambling" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.

Horses are prey animals with a strong fight-or-flight response. Their first reaction to a threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.

Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, "weaving" (rocking back and forth), and other problems.

Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. They are naturally curious and apt to investigate things they have not seen before. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between "more or less" if the quantity involved is less than four.

Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that "intelligent" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.

Horses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the "hot-bloods", such as many race horses, exhibit more sensitivity and energy, while the "cold-bloods", such as most draft breeds, are quieter and calmer. Sometimes "hot-bloods" are classified as "light horses" or "riding horses", with the "cold-bloods" classified as "draft horses" or "work horses".
"Hot blooded" breeds include "oriental horses" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.

Muscular, heavy draft horses are known as "cold bloods", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed "gentle giants". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.

"Warmblood" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a "light horse" or "riding horse".

Today, the term "Warmblood" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term "warm blood" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.

Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a "stay apparatus" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.

Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.

Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.

The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.

The earliest known member of the family Equidae was the "Hyracotherium", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the "Mesohippus", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern "Equus" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.

By about 15,000 years ago, "Equus ferus" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.

A truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most "wild" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the tarpan and the Przewalski's horse, survived into recorded history and only the latter survives today.

The Przewalski's horse ("Equus ferus przewalskii"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the "taki", and the Kyrgyz people call it a "kirtag". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.

The tarpan or European wild horse ("Equus ferus ferus") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.

Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.

Besides the horse, there are six other species of genus "Equus" in the Equidae family. These are the ass or donkey, "Equus asinus"; the mountain zebra, "Equus zebra"; plains zebra, "Equus quagga"; Grévy's zebra, "Equus grevyi"; the kiang, "Equus kiang"; and the onager, "Equus hemionus".

Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.

Domestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.

The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.

Domestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.

Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.

Feral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.

There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in "wild" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.

The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called "thoroughbreds". Thoroughbred is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.

Breeds developed due to a need for "form to function", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses were developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds had been developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.

Worldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 "poll" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.

Communication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.

Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.

Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as "In-hand" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.
Sports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.

Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: "flat" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.

There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.
Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.

Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.

Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.

Horses are frequently used in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.

People of all ages with physical and mental disabilities obtain beneficial results from an association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.

Horses also provide psychological benefits to people whether they actually ride or not. "Equine-assisted" or "equine-facilitated" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.

Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.

Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.

Horse meat has been used as food for humans and carnivorous animals throughout the ages. Approximately 5 million horses are slaughtered each year for meat wordwide. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a "spinto", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.

Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a adult horse could eat up to of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.

Horses require a plentiful supply of clean water, a minimum of to per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.

Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.




</doc>
<doc id="13647" url="https://en.wikipedia.org/wiki?curid=13647" title="Hermann Ebbinghaus">
Hermann Ebbinghaus

Hermann Ebbinghaus (January 24, 1850 – February 26, 1909) was a German psychologist who pioneered the experimental study of memory, and is known for his discovery of the forgetting curve and the spacing effect. He was also the first person to describe the learning curve. He was the father of the neo-Kantian philosopher Julius Ebbinghaus.

Ebbinghaus was born in Barmen, in the Rhine Province of the Kingdom of Prussia, as the son of a wealthy merchant, Carl Ebbinghaus. Little is known about his infancy except that he was brought up in the Lutheran faith and was a pupil at the town Gymnasium. At the age of 17 (1867), he began attending the University of Bonn, where he had planned to study history and philology. However, during his time there he developed an interest in philosophy. In 1870, his studies were interrupted when he served with the Prussian Army in the Franco-Prussian War. Following this short stint in the military, Ebbinghaus finished his dissertation on Eduard von Hartmann's "" (philosophy of the unconscious) and received his doctorate on August 16, 1873, when he was 23 years old. During the next three years, he spent time at Halle and Berlin.

After acquiring his PhD, Ebbinghaus moved around England and France, tutoring students to support himself. In England, he may have taught in two small schools in the south of the country (Gorfein, 1885). In London, in a used bookstore, he came across Gustav Fechner's book "Elemente der Psychophysik" ("Elements of Psychophysics"), which spurred him to conduct his famous memory experiments. After beginning his studies at the University of Berlin, he founded the third psychological testing lab in Germany (third to Wilhelm Wundt and Georg Elias Müller). He began his memory studies here in 1879. In 1885 — the same year that he published his monumental work, "Über das Gedächtnis. Untersuchungen zur experimentellen Psychologie", later published in English under the title "Memory: A Contribution to Experimental Psychology" — he was made a professor at the University of Berlin, most likely in recognition of this publication. In 1890, along with Arthur König, he founded the psychological journal "Zeitschrift für Physiologie und Psychologie der Sinnesorgane" ("The Psychology and Physiology of the Sense Organs'").

In 1894, he was passed over for promotion to head of the philosophy department at Berlin, most likely due to his lack of publications. Instead, Carl Stumpf received the promotion. As a result of this, Ebbinghaus left to join the University of Breslau (now Wrocław, Poland), in a chair left open by Theodor Lipps (who took over Stumpf's position when he moved to Berlin). While in Breslau, he worked on a commission that studied how children's mental ability declined during the school day. While the specifics on how these mental abilities were measured have been lost, the successes achieved by the commission laid the groundwork for future intelligence testing. At Breslau, he again founded a psychological testing laboratory.

In 1902, Ebbinghaus published his next piece of writing entitled "Die Grundzüge der Psychologie" ("Fundamentals of Psychology"). It was an instant success and continued to be long after his death. In 1904, he moved to Halle where he spent the last few years of his life. His last published work, "Abriss der Psychologie" ("Outline of Psychology") was published six years later, in 1908. This, too, continued to be a success, being re-released in eight different editions. Shortly after this publication, on February 26, 1909, Ebbinghaus died from pneumonia at the age of 59.

Ebbinghaus was determined to show that higher mental processes could actually be studied using experimentation, which was in opposition to the popularly held thought of the time. To control for most potentially confounding variables, Ebbinghaus wanted to use simple acoustic encoding and maintenance rehearsal for which a list of words could have been used. As learning would be affected by prior knowledge and understanding, he needed something that could be easily memorized but which had no prior cognitive associations. Easily formable associations with regular words would interfere with his results, so he used items that would later be called "nonsense syllables" (also known as the CVC trigram). A nonsense syllable is a consonant-vowel-consonant combination, where the consonant does not repeat and the syllable does not have prior meaning. BOL (sounds like "Ball") and DOT (already a word) would then not be allowed. However, syllables such as DAX, BOK, and YAT would all be acceptable (though Ebbinghaus left no examples). After eliminating the meaning-laden syllables, Ebbinghaus ended up with 2,300 resultant syllables. Once he had created his collection of syllables, he would pull out a number of random syllables from a box and then write them down in a notebook. Then, to the regular sound of a metronome, and with the same voice inflection, he would read out the syllables, and attempt to recall them at the end of the procedure. One investigation alone required 15,000 recitations.

It was later determined that humans impose meaning even on nonsense syllables to make them more meaningful. The nonsense syllable PED (which is the first three letters of the word "pedal") turns out to be less nonsensical than a syllable such as KOJ; the syllables are said to differ in association value. It appears that Ebbinghaus recognized this, and only referred to the strings of syllables as "nonsense" in that the syllables might be less likely to have a specific meaning and he should make no attempt to make associations with them for easier retrieval.

There are several limitations to his work on memory. The most important one was that Ebbinghaus was the only subject in his study. This limited the study's generalizability to the population. Although he attempted to regulate his daily routine to maintain more control over his results, his decision to avoid the use of participants sacrificed the external validity of the study despite sound internal validity. In addition, although he tried to account for his personal influences, there is an inherent bias when someone serves as researcher as well as participant. Also, Ebbinghaus's memory research halted research in other, more complex matters of memory such as semantic and procedural memory and mnemonics.

In 1885, he published his groundbreaking "Über das Gedächtnis" ("On Memory", later translated to English as "Memory. A Contribution to Experimental Psychology") in which he described experiments he conducted on himself to describe the processes of learning and forgetting.

Ebbinghaus made several findings that are still relevant and supported to this day. First, Ebbinghaus made a 2,300 three letter syllables to measure mental associations that helped him find that memory is orderly. Second, and arguably his most famous finding, was the forgetting curve. The forgetting curve describes the exponential loss of information that one has learned. The sharpest decline occurs in the first twenty minutes and the decay is significant through the first hour. The curve levels off after about one day.

The learning curve described by Ebbinghaus refers to how fast one learns information. The sharpest increase occurs after the first try and then gradually evens out, meaning that less and less new information is retained after each repetition. Like the forgetting curve, the learning curve is exponential. Ebbinghaus had also documented the serial position effect, which describes how the position of an item affects recall. The two main concepts in the serial position effect are recency and primacy. The recency effect describes the increased recall of the most recent information because it is still in the short-term memory. The primacy effect causes better memory of the first items in a list due to increased rehearsal and commitment to long-term memory.

Another important discovery is that of savings. This refers to the amount of information retained in the subconscious even after this information cannot be consciously accessed. Ebbinghaus would memorize a list of items until perfect recall and then would not access the list until he could no longer recall any of its items. He then would relearn the list, and compare the new learning curve to the learning curve of his previous memorization of the list. The second list was generally memorized faster, and this difference between the two learning curves is what Ebbinghaus called "savings". Ebbinghaus also described the difference between involuntary and voluntary memory, the former occurring "with apparent spontaneity and without any act of the will" and the latter being brought "into consciousness by an exertion of the will".

Prior to Ebbinghaus, most contributions to the study of memory were undertaken by philosophers and centered on observational description and speculation. For example, Immanuel Kant used pure description to discuss recognition and its components and Sir Francis Bacon claimed that the simple observation of the rote recollection of a previously learned list was "no use to the art" of memory. This dichotomy between descriptive and experimental study of memory would resonate later in Ebbinghaus's life, particularly in his public argument with former colleague Wilhelm Dilthey. However, more than a century before Ebbinghaus, Johann Andreas Segner invented the "Segner-wheel" to see the length of after-images by seeing how fast a wheel with a hot coal attached had to move for the red ember circle from the coal to appear complete. (see iconic memory)

Ebbinghaus's effect on memory research was almost immediate. With very few works published on memory in the previous two millennia, Ebbinghaus's works spurred memory research in the United States in the 1890s, with 32 papers published in 1894 alone. This research was coupled with the growing development of mechanized mnemometers, or devices that aided in the recording and study of memory.

The reaction to his work in his day was mostly positive. Noted psychologist William James called the studies "heroic" and said that they were "the single most brilliant investigation in the history of psychology". Edward B. Titchener also mentioned that the studies were the greatest undertaking in the topic of memory since Aristotle.

Ebbinghaus can also be credited with pioneering sentence completion exercises, which he developed in studying the abilities of schoolchildren. It was these same exercises that Alfred Binet had borrowed and incorporated into the Binet-Simon intelligence scale. Sentence completion had since then also been used extensively in memory research, especially in tapping into measures of implicit memory, and also has been used in psychotherapy as a tool to help tap into the motivations and drives of the patient. He had also influenced Charlotte Bühler, who along with Lev Vygotsky and others went on to study language meaning and society.

Ebbinghaus is also largely credited with drafting the first standard research report. In his paper on memory, Ebbinghaus arranged his research into four sections: the introduction, the methods, the results, and a discussion section. The clarity and organization of this format was so impressive to contemporaries that it has now become standard in the discipline, and all research reports follow the same standards laid out by Ebbinghaus.

After Ebbinghaus worked on memory, he also had a contribution with color vision. In 1890, Ebbinghaus came up with the double pyramid design where corners were rounded off.

Unlike notable contemporaries like Titchener and James, Ebbinghaus did not promote any specific school of psychology nor was he known for extensive lifetime research, having done only three works. He never attempted to bestow upon himself the title of the pioneer of experimental psychology, did not seek to have any "disciples", and left the exploitation of the new field to others.

In addition to pioneering experimental psychology, Ebbinghaus was also a strong defender of this direction of the new science, as is illustrated by his public dispute with University of Berlin colleague, Wilhelm Dilthey. Shortly after Ebbinghaus left Berlin in 1893, Dilthey published a paper extolling the virtues of descriptive psychology, and condemning experimental psychology as boring, claiming that the mind was too complex, and that introspection was the desired method of studying the mind. The debate at the time had been primarily whether psychology should aim to explain or understand the mind and whether it belonged to the natural or human sciences. Many had seen Dilthey's work as an outright attack on experimental psychology, Ebbinghaus included, and he responded to Dilthey with a personal letter and also a long scathing public article. Amongst his counterarguments against Dilthey he mentioned that it is inevitable for psychology to do hypothetical work and that the kind of psychology that Dilthey was attacking was the one that existed before Ebbinghaus's "experimental revolution". Charlotte Bühler echoed his words some forty years later, stating that people like Ebbinghaus "buried the old psychology in the 1890s". Ebbinghaus explained his scathing review by saying that he could not believe that Dilthey was advocating the status quo of structuralists like Wilhelm Wundt and Titchener and attempting to stifle psychology's progress.

Some contemporary texts still describe Ebbinghaus as a philosopher rather than a psychologist and he had also spent his life as a professor of philosophy. However, Ebbinghaus himself would probably describe himself as a psychologist considering that he fought to have psychology viewed as a separate discipline from philosophy.

There has been some speculation as to what influenced Ebbinghaus in his undertakings. None of his professors seem to have influenced him, nor are there suggestions that his colleagues affected him. Von Hartmann's work, on which Ebbinghaus based his doctorate, did suggest that higher mental processes were hidden from view, which may have spurred Ebbinghaus to attempt to prove otherwise. The one influence that has always been cited as having inspired Ebbinghaus was Gustav Fechner's two-volume "Elemente der Psychophysik." ("Elements of Psychophysics", 1860), a book which he purchased second-hand in England. It is said that the meticulous mathematical procedures impressed Ebbinghaus so much that he wanted to do for psychology what Fechner had done for psychophysics. This inspiration is also evident in that Ebbinghaus dedicated his second work "Principles of Psychology" to Fechner, signing it "I owe everything to you."




</doc>
<doc id="13648" url="https://en.wikipedia.org/wiki?curid=13648" title="Hilbert (disambiguation)">
Hilbert (disambiguation)

David Hilbert (1862–1943) was a German mathematician.

Hilbert may also refer to:





</doc>
<doc id="13652" url="https://en.wikipedia.org/wiki?curid=13652" title="Hindi">
Hindi

Hindi (Devanagari: हिन्दी, "Hindī") or Modern Standard Hindi (Devanagari: मानक हिन्दी, "Mānak Hindī"), is an Indo-Aryan language spoken in India and across the Indian subcontinent. Modern Hindi is the standardised and Sanskritised register of the Hindustani language, which itself is based primarily on the Khariboli dialect of Delhi and other nearby areas of Northern India. Hindi, written in the Devanagari script, is one of the two official languages of the Government of India, along with the English language. It is one of the 22 scheduled languages of the Republic of India. 

Hindi is the "lingua franca" of the Hindi belt and to a lesser extent other parts of India (usually in a simplified or pidginised variety such as Bazaar Hindustani or Haflong Hindi). Outside India, several other languages are recognised officially as "Hindi" but do not refer to the Standard Hindi language described here and instead descend from other dialects, such as Awadhi and Bhojpuri. Such languages include Fiji Hindi, which is official in Fiji, and Caribbean Hindustani, which is spoken in Trinidad and Tobago, Guyana, and Suriname. Apart from the script and formal vocabulary, spoken Hindi is mutually intelligible with standard Urdu, another recognised register of Hindustani as both share a common colloquial base.

As a linguistic variety, Hindi is the fourth most-spoken first language in the world, after Mandarin, Spanish and English. Hindi alongside Urdu as Hindustani is the third most-spoken language in the world, after Mandarin and English.

The term "Hindī" originally was used to refer to inhabitants of the Indo-Gangetic Plain. It was borrowed from Classical Persian "Hindī" (Iranian Persian pronunciation: "Hendi"), meaning "of or belonging to "Hind" (India)" (hence, "Indian").

Another name "Hindavī" ( "of or belonging to the Hindu/Indian people") was used by Amir Khusrow in his poetry.

Like other Indo-Aryan languages, Hindi is a direct descendant of an early form of Vedic Sanskrit, through Sauraseni Prakrit and Śauraseni Apabhraṃśa (from Sanskrit "apabhraṃśa" "corrupt"), which emerged in the 7th century CE. Afer the arrival of Islamic administrative rule in northern India, Hindi acquired many loanwords from Persian, as well as Arabic.

Before the standardisation of Hindi on the Khariboli dialect, various dialects and languages of the Hindi belt attained prominence through literary standardisation, such as Avadhi and Braj Bhasha. Early Hindi literature came about in the 12th and 13th centuries CE. This body of work included the early epics such as renditions of the "Dhola Maru" in the Marwari of Marwar, the "Prithviraj Raso" in the Braj Bhasha of Braj, and the works of Amir Khusrow in the Khariboli of Delhi.

Modern Standard Hindi is based on the Khariboli dialect, the vernacular of Delhi and the surrounding region, which came to replace earlier prestige dialects such as Awadhi, Maithili (sometimes regarded as separate from the Hindi dialect continuum) and Braj. "Urdu" – another form of Hindustani – acquired linguistic prestige in the latter part of the Mughal period (1800s), and underwent significant Persian influence. Modern Hindi and its literary tradition evolved towards the end of the 18th century. 
John Gilchrist was principally known for his study of the Hindustani language, which was adopted as the lingua franca of northern India (including what is now present-day Pakistan) by British colonists and indigenous people. He compiled and authored "An English-Hindustani Dictionary", "A Grammar of the Hindoostanee Language", "The Oriental Linguist", and many more. His lexicon of Hindustani was published in the Perso-Arabic script, Nāgarī script, and in Roman transliteration. He is also known for his role in the foundation of University College London and for endowing the Gilchrist Educational Trust.
In the late 19th century, a movement to further develop Hindi as a standardised form of Hindustani separate from Urdu took form. In 1881, Bihar accepted Hindi as its sole official language, replacing Urdu, and thus became the first state of India to adopt Hindi. 

After independence, the government of India instituted the following conventions:

On 14 September 1949, the Constituent Assembly of India adopted Hindi written in the Devanagari script as the official language of the Republic of India replacing Urdu's previous usage in British India. To this end, several stalwarts rallied and lobbied pan-India in favour of Hindi, most notably along with Hazari Prasad Dwivedi, Kaka Kalelkar, Maithili Sharan Gupt and Seth Govind Das who even debated in Parliament on this issue. As such, on the 50th birthday of Beohar Rajendra Simha on 14 September 1949, the efforts came to fruition following the adoption of Hindi as the official language. Now, it is celebrated as Hindi Day.

Part XVII of the Indian Constitution deals with the official language of the Indian Commonwealth. Under Article 343, the official languages of the Union has been prescribed, which includes Hindi in Devanagari script and English:

(1) The official language of the Union shall be Hindi in Devanagari script. The form of numerals to be used for the official purposes of the Union shall be the international form of Indian numerals.
(2) Notwithstanding anything in clause (1), for a period of fifteen years from the commencement of this Constitution, the English language shall continue to be used for all the official purposes of the Union for which it was being used immediately before such commencement: Provided that the President may, during the said period, by order authorise the use of the Hindi language in addition to the English language and of the Devanagari form of numerals in addition to the international form of Indian numerals for any of the official purposes of the Union.

It shall be the duty of the Union to promote the spread of the Hindi language, to develop it so that it may serve as a medium of expression for all the elements of the composite culture of India and to secure its enrichment by assimilating without interfering with its genius, the forms, style and expressions used in Hindustani and in the other languages of India specified in the Eighth Schedule, and by drawing, wherever necessary or desirable, for its vocabulary, primarily on Sanskrit and secondarily on other languages.

It was envisioned that Hindi would become the sole working language of the Union Government by 1965 (per directives in Article 344 (2) and Article 351), with state governments being free to function in the language of their own choice. However, widespread resistance to the imposition of Hindi on non-native speakers, especially in South India (such as the those in Tamil Nadu) led to the passage of the Official Languages Act of 1963, which provided for the continued use of English indefinitely for all official purposes, although the constitutional directive for the Union Government to encourage the spread of Hindi was retained and has strongly influenced its policies.

At the state level, Hindi is the official language of the following Indian states: Bihar, Chhattisgarh, Haryana, Himachal Pradesh, Jharkhand, Madhya Pradesh, Mizoram, Rajasthan, Uttar Pradesh and Uttarakhand. It is one of the additional official languages of West Bengal. Each may also designate a "co-official language"; in Uttar Pradesh, for instance, depending on the political formation in power, this language is generally Urdu. Similarly, Hindi is accorded the status of official language in the following Union Territories: National Capital Territory, Andaman and Nicobar Islands and Dadra and Nagar Haveli.

National language status for Hindi is a long-debated theme. In 2010, the Gujarat High Court clarified that Hindi is not the national language of India because the constitution does not mention it as such.

Outside Asia, the Awadhi language (A Hindi dialect) with influence from Bhojpuri, Bihari languages, Fijian and English is spoken in Fiji. It is an official language in Fiji as per the 1997 Constitution of Fiji, where it referred to it as "Hindustani", however in the 2013 Constitution of Fiji, it is simply called "Fiji Hindi". It is spoken by 380,000 people in Fiji.

Hindi is the lingua franca of northern India (which contains the Hindi Belt), as well as an official language of the Government of India, along with English. 

In Northeast India a pidgin known as Haflong Hindi has developed as a "lingua franca" for the people living in Haflong, Assam who speak other languages natively. In Arunachal Pradesh, Hindi emerged as a lingua franca among locals who speak over 50 dialects natively.

Hindi is quite easy to understand for many Pakistanis, who speak Urdu, which, like Hindi, is a standard register of the Hindustani language; additionally, the Indian media is widely viewed in Pakistan.

A sizeable population in Afghanistan, especially in Kabul, can also speak and understand Hindi-Urdu due to the popularity and influence of Bollywood films and songs in the region.

Hindi is also spoken by a large population of Madheshis (people having roots in north-India but have migrated to Nepal over hundreds of years) of Nepal. Apart from this, Hindi is spoken by the large Indian diaspora which hails from, or has its origin from the "Hindi Belt" of India. A substantially large North Indian diaspora lives in countries like the United States of America, the United Kingdom, the United Arab Emirates, Trinidad and Tobago, Guyana, Suriname, South Africa, Fiji and Mauritius, where it is natively spoken at home and among their own Hindustani-speaking communities. Outside India, Hindi speakers are 8 million in Nepal; 863,077 in United States of America; 450,170 in Mauritius; 380,000 in Fiji; 250,292 in South Africa; 150,000 in Suriname; 100,000 in Uganda; 45,800 in United Kingdom; 20,000 in New Zealand; 20,000 in Germany; 26,000 in Trinidad and Tobago; 3,000 in Singapore.

Linguistically, Hindi and Urdu are two registers of the same language and are mutually intelligible. Hindi is written in the Devanagari script and contains more Sanskrit-derived words than Urdu, whereas Urdu is written in the Perso-Arabic script and uses more Arabic and Persian loanwords than does Hindi. However, both share a core vocabulary of native Prakrit and Sanskrit-derived words, with large numbers of Arabic and Persian loanwords. Because of this, as well as the fact that the two registers share an identical grammar, a consensus of linguists consider them to be two standardised forms of the same language, Hindustani or Hindi-Urdu. Hindi is the most commonly used official language in India. Urdu is the and "lingua franca" of Pakistan and is one of 22 official languages of India, also having official status in Uttar Pradesh, Jammu and Kashmir, and Delhi.

The comparison of Hindi and Urdu as separate languages is largely motivated by politics, namely the Indo-Pakistani rivalry.

Hindi is written in the Devanagari script, an abugida. Devanagari consists of 11 vowels and 33 consonants and is written from left to right. Unlike for Sanskrit, Devanagari is not entirely phonetic for Hindi, especially failing to mark schwa dropping in spoken Standard Hindi.

The Government of India uses Hunterian transliteration as its official system of writing Hindi in the Latin script. Various other systems also exist, such as IAST, ITRANS and ISO 15919.

Traditionally, Hindi words are divided into five principal categories according to their etymology:


Hindi also makes extensive use of loan translation (calqueing) and occasionally phono-semantic matching of English.

Hindi has naturally inherited a large portion of its vocabulary from Śaurasenī Prākṛt, in the form of "tadbhava" words. This process usually involves compensatory lengthening of vowels preceding consonant clusters in Prakrit, e.g. Sanskrit "tīkṣṇa" > Prakrit "tikkha" > Hindi "tīkhā".

Much of Modern Standard Hindi's vocabulary is borrowed from Sanskrit as "tatsam" borrowings, especially in technical and academic fields. The formal Hindi standard, from which much of the Persian, Arabic and English vocabulary has been replaced by neologisms compounding "tatsam" words, is called "Śuddh Hindi" (pure Hindi), and is viewed as a more prestigious dialect over other more colloquial forms of Hindi.

Excessive use of "tatsam" words sometimes creates problems for native speakers. They may have Sanskrit consonant clusters which do not exist in native Hindi, causing difficulties in pronunciation.

As a part of the process of Sanskritization, new words are coined using Sanskrit components to be used as replacements for supposedly foreign vocabulary. Usually these neologisms are calques of English words already adopted into spoken Hindi. Some terms such as "dūrbhāṣ" "telephone", literally "far-speech" and "dūrdarśan" "television", literally "far-sight" have even gained some currency in formal Hindi in the place of the English borrowings "(ṭeli)fon" and "ṭīvī".

Hindi also features significant Persian influence, standardised from spoken Hindustani. Early borrowings, beginning in the mid-12th century, were specific to Islam (e.g. "Muhammad", "islām") and so Persian was simply an intermediary for Arabic. Later, under the Delhi Sultanate and Mughal Empire, Persian became the primary administrative language in the Hindi heartland. Persian borrowings reached a heyday in the 17th century, pervading all aspects of life. Even grammatical constructs, namely the izafat, were assimilated into Hindi.

Post-Partition the Indian government advocated for a policy of Sanskritization leading to a marginalisation of the Persian element in Hindi. However, many Persian words (e.g. "muśkil" "difficult", "bas" "enough", "havā" "air", "x(a)yāl" "thought") have remained entrenched in Modern Standard Hindi, and a larger amount are still used in Urdu poetry written in the Devanagari script.

Arabic also shows influence in Hindi, often via Persian but sometimes directly.

Hindi literature is broadly divided into four prominent forms or styles, being "Bhakti" (devotional – Kabir, Raskhan); "Śṛṇgār" (beauty – Keshav, Bihari); "Vīgāthā" (epic); and "Ādhunik" (modern).

Medieval Hindi literature is marked by the influence of Bhakti movement and the composition of long, epic poems. It was primarily written in other varieties of Hindi, particularly Avadhi and Braj Bhasha, but to a degree also in Khariboli, the basis for Modern Standard Hindi. During the British Raj, Hindustani became the prestige dialect.

"Chandrakanta", written by Devaki Nandan Khatri in 1888, is considered the first authentic work of prose in modern Hindi. The person who brought realism in the Hindi prose literature was Munshi Premchand, who is considered as the most revered figure in the world of Hindi fiction and progressive movement. Literary, or "Sāhityik", Hindi was popularised by the writings of Swami Dayananda Saraswati, Bhartendu Harishchandra and others. The rising numbers of newspapers and magazines made Hindustani popular with the educated people.

The "Dvivedī Yug" ("Age of Dwivedi") in Hindi literature lasted from 1900 to 1918. It is named after Mahavir Prasad Dwivedi, who played a major role in establishing Modern Standard Hindi in poetry and broadening the acceptable subjects of Hindi poetry from the traditional ones of religion and romantic love.

In the 20th century, Hindi literature saw a romantic upsurge. This is known as "Chāyāvād" ("shadow-ism") and the literary figures belonging to this school are known as "Chāyāvādī". Jaishankar Prasad, Suryakant Tripathi 'Nirala', Mahadevi Varma and Sumitranandan Pant, are the four major "Chāyāvādī" poets.

"Uttar Ādhunik" is the post-modernist period of Hindi literature, marked by a questioning of early trends that copied the West as well as the excessive ornamentation of the "Chāyāvādī" movement, and by a return to simple language and natural themes.

The Hindi Wikipedia was the first Indian-language wiki to reach 100,000 articles. Hindi literature, music, and film have all been disseminated via the internet. In 2015, Google reported a 94% increase in Hindi-content consumption year-on-year, adding that 21% of users in India prefer content in Hindi. Many Hindi newspapers also offer digital editions.

The following is a sample text in High Hindi, of the Article 1 of the Universal Declaration of Human Rights (by the United Nations):





</doc>
<doc id="13653" url="https://en.wikipedia.org/wiki?curid=13653" title="Huginn and Muninn">
Huginn and Muninn

In Norse mythology, Huginn (from Old Norse "thought") and Muninn (Old Norse "memory" or "mind") are a pair of ravens that fly all over the world, Midgard, and bring information to the god Odin. Huginn and Muninn are attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources: the "Prose Edda" and "Heimskringla", written in the 13th century by Snorri Sturluson; in the "Third Grammatical Treatise", compiled in the 13th century by Óláfr Þórðarson; and in the poetry of skalds. The names of the ravens are sometimes modernly anglicized as Hugin and Munin.

In the "Poetic Edda", a disguised Odin expresses that he fears that they may not return from their daily flights. The "Prose Edda" explains that Odin is referred to as "raven-god" due to his association with Huginn and Muninn. In the "Prose Edda" and the "Third Grammatical Treatise", the two ravens are described as perching on Odin's shoulders. "Heimskringla" details that Odin gave Huginn and Muninn the ability to speak.

Examples of artifacts that may depict Odin with one of the ravens include Migration Period golden bracteates, Vendel era helmet plates, a pair of identical Germanic Iron Age bird-shaped brooches, Viking Age objects depicting a moustached man wearing a helmet, and a portion of the 10th or 11th century . Huginn and Muninn's role as Odin's messengers has been linked to shamanic practices, the Norse raven banner, general raven symbolism among the Germanic peoples, and the Norse concepts of the fylgja and the hamingja.

In the "Poetic Edda" poem "Grímnismál", the god Odin (disguised as "Grímnir") provides the young Agnarr with information about Odin's companions. He tells the prince about Odin's wolves Geri and Freki, and, in the next stanza of the poem, states that Huginn and Muninn fly daily across the entire world, Midgard. Grímnir says that he worries Huginn may not come back, yet more does he fear for Muninn:

In the "Prose Edda" book "Gylfaginning" (chapter 38), the enthroned figure of High tells Gangleri (king Gylfi in disguise) that two ravens named Huginn and Muninn sit on Odin's shoulders. The ravens tell Odin everything they see and hear. Odin sends Huginn and Muninn out at dawn, and the birds fly all over the world before returning at dinner-time. As a result, Odin is kept informed of many events. High adds that it is from this association that Odin is referred to as "raven-god". The above-mentioned stanza from "Grímnismál" is then quoted.

In the "Prose Edda" book "Skáldskaparmál" (chapter 60), Huginn and Muninn appear in a list of poetic names for ravens. In the same chapter, excerpts from a work by the skald Einarr Skúlason are provided. In these excerpts Muninn is referenced in a common noun for 'raven' and Huginn is referenced in a kenning for 'carrion'.

In the "Heimskringla" book "Ynglinga saga", a euhemerized account of the life of Odin is provided. Chapter 7 describes that Odin had two ravens, and upon these ravens he bestowed the gift of speech. These ravens flew all over the land and brought him information, causing Odin to become "very wise in his lore."

In the "Third Grammatical Treatise" an anonymous verse is recorded that mentions the ravens flying from Odin's shoulders; Huginn seeking hanged men, and Muninn slain bodies. The verse reads:

Migration Period (5th and 6th centuries CE) gold bracteates (types A, B, and C) feature a depiction of a human figure above a horse, holding a spear and flanked by one or more often two birds. The presence of the birds has led to the iconographic identification of the human figure as the god Odin, flanked by Huginn and Muninn. Like Snorri's "Prose Edda" description of the ravens, a bird is sometimes depicted at the ear of the human, or at the ear of the horse. Bracteates have been found in Denmark, Sweden, Norway and, in smaller numbers, England and areas south of Denmark. Austrian Germanist Rudolf Simek states that these bracteates may depict Odin and his ravens healing a horse and may indicate that the birds were originally not simply his battlefield companions but also "Odin's helpers in his veterinary function."

Vendel era helmet plates (from the 6th or 7th century) found in a grave in Sweden depict a helmeted figure holding a spear and a shield while riding a horse, flanked by two birds. The plate has been interpreted as Odin accompanied by two birds: his ravens.

A pair of identical Germanic Iron Age bird-shaped brooches from Bejsebakke in northern Denmark may be depictions of Huginn and Muninn. The back of each bird features a mask motif, and the feet of the birds are shaped like the heads of animals. The feathers of the birds are also composed of animal heads. Together, the animal heads on the feathers form a mask on the back of the bird. The birds have powerful beaks and fan-shaped tails, indicating that they are ravens. The brooches were intended to be worn on each shoulder, after Germanic Iron Age fashion. Archaeologist Peter Vang Petersen comments that while the symbolism of the brooches is open to debate, the shape of the beaks and tail feathers confirm that the brooch depictions are ravens. Petersen notes that "raven-shaped ornaments worn as a pair, after the fashion of the day, one on each shoulder, makes one's thoughts turn towards Odin's ravens and the cult of Odin in the Germanic Iron Age." Petersen says that Odin is associated with disguise and that the masks on the ravens may be portraits of Odin.

The Oseberg tapestry fragments, discovered within the Viking Age Oseberg ship burial in Norway, feature a scene containing two black birds hovering over a horse, possibly originally leading a wagon (as a part of a procession of horse-led wagons on the tapestry). In her examination of the tapestry, scholar Anne Stine Ingstad interprets these birds as Huginn and Muninn flying over a covered cart containing an image of Odin, drawing comparison with the images of Nerthus attested by Tacitus in 1 CE.

Excavations in Ribe in Denmark have recovered a Viking Age lead metal-caster's mould and 11 identical casting-moulds. These objects depict a moustached man wearing a helmet that features two head-ornaments. Archaeologist Stig Jensen proposes that these ornaments should be interpreted as Huginn and Muninn, and the wearer as Odin. He notes that "similar depictions occur everywhere the Vikings went—from eastern England to Russia and naturally also in the rest of Scandinavia."

A portion of (a partly surviving runestone erected at Kirk Andreas on the Isle of Man) depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, and a large bird on his shoulder. Andy Orchard comments that this bird may be either Huginn or Muninn. Rundata dates the cross to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by the monstrous wolf Fenrir during the events of Ragnarök.

In November 2009, the Roskilde Museum announced the discovery and subsequent display of a niello-inlaid silver figurine found in Lejre, Denmark, which they dubbed "Odin from Lejre". The silver object depicts a person sitting on a throne. The throne features the heads of animals and is flanked by two birds. The Roskilde Museum identifies the figure as Odin sitting on his throne Hliðskjálf, flanked by the ravens Huginn and Muninn.

Scholars have linked Odin's relation to Huginn and Muninn to shamanic practice. John Lindow relates Odin's ability to send his "thought" (Huginn) and "mind" (Muninn) to the trance-state journey of shamans. Lindow says the "Grímnismál" stanza where Odin worries about the return of Huginn and Muninn "would be consistent with the danger that the shaman faces on the trance-state journey."

Rudolf Simek is critical of the approach, stating that "attempts have been made to interpret Odin's ravens as a personification of the god's intellectual powers, but this can only be assumed from the names Huginn and Muninn themselves which were unlikely to have been invented much before the 9th or 10th centuries" yet that the two ravens, as Odin's companions, appear to derive from much earlier times. Instead, Simek connects Huginn and Muninn with wider raven symbolism in the Germanic world, including the raven banner (described in English chronicles and Scandinavian sagas), a banner which was woven in a method that allowed it, when fluttering in the wind, to appear as if the raven depicted upon it was beating its wings.

Anthony Winterbourne connects Huginn and Muninn to the Norse concepts of the fylgja—a concept with three characteristics; shape-shifting abilities, good fortune, and the guardian spirit—and the hamingja—the ghostly double of a person that may appear in the form of an animal. Winterbourne states that "The shaman's journey through the different parts of the cosmos is symbolized by the "hamingja" concept of the shape-shifting soul, and gains another symbolic dimension for the Norse soul in the account of Oðin's ravens, Huginn and Muninn." In response to Simek's criticism of attempts to interpret the ravens "philosophically", Winterbourne says that "such speculations [...] simply strengthen the conceptual significance made plausible by other features of the mythology" and that the names "Huginn" and "Muninn" "demand more explanation than is usually provided."

The "Heliand", an Old Saxon adaptation of the New Testament from the 9th century, differs from the New Testament in that an explicit reference is made to a dove sitting on the shoulder of Christ. Regarding this, G. Ronald Murphy says "In placing the powerful white dove not just above Christ, but right on his shoulder, the "Heliand" author has portrayed Christ, not only as the Son of the All-Ruler, but also as a new Woden. This deliberate image of Christ triumphantly astride the land with the magnificent bird on his shoulders (the author is perhaps a bit embarrassed that the bird is an unwarlike dove!) is an image intended to calm the fears and longings of those who mourn the loss of Woden and who want to return to the old religion's symbols and ways. With this image, Christ becomes a Germanic god, one into whose ears the Spirit of the Almighty whispers".

Bernd Heinrich theorizes that Huginn and Muninn, along with Odin and his wolves Geri and Freki, reflect a symbiosis observed in the natural world among ravens, wolves, and humans on the hunt:




</doc>
<doc id="13654" url="https://en.wikipedia.org/wiki?curid=13654" title="Heat engine">
Heat engine

In thermodynamics and engineering, a heat engine is a system that converts heat or thermal energy—and chemical energy—to mechanical energy, which can then be used to do mechanical work. It does this by bringing a working substance from a higher state temperature to a lower state temperature. A heat source generates thermal energy that brings the working substance to the high temperature state. The working substance generates work in the working body of the engine while transferring heat to the colder sink until it reaches a low temperature state. During this process some of the thermal energy is converted into work by exploiting the properties of the working substance. The working substance can be any system with a non-zero heat capacity, but it usually is a gas or liquid. During this process, some heat is normally lost to the surroundings and is not converted to work. Also, some energy is unusable because of friction and drag.

In general an engine converts energy to mechanical work. Heat engines distinguish themselves from other types of engines by the fact that their efficiency is fundamentally limited by Carnot's theorem. Although this efficiency limitation can be a drawback, an advantage of heat engines is that most forms of energy can be easily converted to heat by processes like exothermic reactions (such as combustion), absorption of light or energetic particles, friction, dissipation and resistance. Since the heat source that supplies thermal energy to the engine can thus be powered by virtually any kind of energy, heat engines cover a wide range of applications.

Heat engines are often confused with the cycles they attempt to implement. Typically, the term "engine" is used for a physical device and "cycle" for the models.

In thermodynamics, heat engines are often modeled using a standard engineering model such as the Otto cycle. The theoretical model can be refined and augmented with actual data from an operating engine, using tools such as an indicator diagram. Since very few actual implementations of heat engines exactly match their underlying thermodynamic cycles, one could say that a thermodynamic cycle is an ideal case of a mechanical engine. In any case, fully understanding an engine and its efficiency requires a good understanding of the (possibly simplified or idealised) theoretical model, the practical nuances of an actual mechanical engine and the discrepancies between the two.

In general terms, the larger the difference in temperature between the hot source and the cold sink, the larger is the potential thermal efficiency of the cycle. On Earth, the cold side of any heat engine is limited to being close to the ambient temperature of the environment, or not much lower than 300 Kelvin, so most efforts to improve the thermodynamic efficiencies of various heat engines focus on increasing the temperature of the source, within material limits. The maximum theoretical efficiency of a heat engine (which no engine ever attains) is equal to the temperature difference between the hot and cold ends divided by the temperature at the hot end, each expressed in absolute temperature (Kelvin).

The efficiency of various heat engines proposed or used today has a large range:

The efficiency of these processes is roughly proportional to the temperature drop across them. Significant energy may be consumed by auxiliary equipment, such as pumps, which effectively reduces efficiency.

It is important to note that although some cycles have a typical combustion location (internal or external), they often can be implemented with the other. For example, John Ericsson developed an external heated engine running on a cycle very much like the earlier Diesel cycle. In addition, externally heated engines can often be implemented in open or closed cycles.

Everyday examples of heat engines include the thermal power station, internal combustion engine and steam locomotive. All of these heat engines are powered by the expansion of heated gases.

Earth's atmosphere and hydrosphere—Earth’s heat engine—are coupled processes that constantly even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds and ocean circulation, when distributing heat around the globe.

A Hadley cell is an example of a heat engine. It involves the rising of warm and moist air in the earth's equatorial region and the descent of colder air in the subtropics creating a thermally driven direct circulation, with consequent net production of kinetic energy.

In these cycles and engines, the working fluids are gases and liquids. The engine converts the working fluid from a gas to a liquid, from liquid to gas, or both, generating work from the fluid expansion or compression.

In these cycles and engines the working fluid is always a gas (i.e., there is no phase change):

In these cycles and engines the working fluid are always like liquid:



A domestic refrigerator is an example of a heat pump: a heat engine in reverse. Work is used to create a heat differential. Many cycles can run in reverse to move heat from the cold side to the hot side, making the cold side cooler and the hot side hotter. Internal combustion engine versions of these cycles are, by their nature, not reversible.

Refrigeration cycles include:

The Barton evaporation engine is a heat engine based on a cycle producing power and cooled moist air from the evaporation of water into hot dry air.

Mesoscopic heat engines are nanoscale devices that may serve the goal of processing heat fluxes and perform useful work at small scales. Potential applications include e.g. electric cooling devices.
In such mesoscopic heat engines, work per cycle of operation fluctuates due to thermal noise.
There is exact equality that relates average of exponents of work performed by any heat engine and the heat transfer from the hotter heat bath. This relation transforms the Carnot's inequality into exact equality. This relation is also a Carnot cycle equality

The efficiency of a heat engine relates how much useful work is output for a given amount of heat energy input.

From the laws of thermodynamics, after a completed cycle:

In other words, a heat engine absorbs heat energy from the high temperature heat source, converting part of it to useful work and delivering the rest to the cold temperature heat sink.

In general, the efficiency of a given heat transfer process (whether it be a refrigerator, a heat pump or an engine) is defined informally by the ratio of "what is taken out" to "what is put in".

In the case of an engine, one desires to extract work and puts in a heat transfer.

The "theoretical" maximum efficiency of any heat engine depends only on the temperatures it operates between. This efficiency is usually derived using an ideal imaginary heat engine such as the Carnot heat engine, although other engines using different cycles can also attain maximum efficiency. Mathematically, this is because in reversible processes, the change in entropy of the cold reservoir is the negative of that of the hot reservoir (i.e., formula_7), keeping the overall change of entropy zero. Thus:

where formula_9 is the absolute temperature of the hot source and formula_10 that of the cold sink, usually measured in kelvins. Note that formula_11 is positive while formula_12 is negative; in any reversible work-extracting process, entropy is overall not increased, but rather is moved from a hot (high-entropy) system to a cold (low-entropy one), decreasing the entropy of the heat source and increasing that of the heat sink.

The reasoning behind this being the maximal efficiency goes as follows. It is first assumed that if a more efficient heat engine than a Carnot engine is possible, then it could be driven in reverse as a heat pump. Mathematical analysis can be used to show that this assumed combination would result in a net decrease in entropy. Since, by the second law of thermodynamics, this is statistically improbable to the point of exclusion, the Carnot efficiency is a theoretical upper bound on the reliable efficiency of "any" thermodynamic cycle.

Empirically, no heat engine has ever been shown to run at a greater efficiency than a Carnot cycle heat engine.

Figure 2 and Figure 3 show variations on Carnot cycle efficiency. Figure 2 indicates how efficiency changes with an increase in the heat addition temperature for a constant compressor inlet temperature. Figure 3 indicates how the efficiency changes with an increase in the heat rejection temperature for a constant turbine inlet temperature.

By its nature, any maximally efficient Carnot cycle must operate at an infinitesimal temperature gradient; this is because any transfer of heat between two bodies of differing temperatures is irreversible, therefore the Carnot efficiency expression applies only to the infinitesimal limit. The major problem is that the objective of most heat-engines is to output power, and infinitesimal power is seldom desired.

A different measure of ideal heat-engine efficiency is given by considerations of endoreversible thermodynamics, where the cycle is identical to the Carnot cycle except that the two processes of heat transfer are "not" reversible (Callen 1985):

This model does a better job of predicting how well real-world heat-engines can do (Callen 1985, see also endoreversible thermodynamics):

As shown, the endo-reversible efficiency much more closely models that observed.

Heat engines have been known since antiquity but were only made into useful devices at the time of the industrial revolution in the 18th century. They continue to be developed today.

Engineers have studied the various heat-engine cycles to improve the amount of usable work they could extract from a given power source. The Carnot cycle limit cannot be reached with any gas-based cycle, but engineers have found at least two ways to bypass that limit and one way to get better efficiency without bending any rules:

Each process is one of the following:




</doc>
<doc id="13655" url="https://en.wikipedia.org/wiki?curid=13655" title="Heimdallr">
Heimdallr

In Norse mythology, Heimdallr is a god who possesses the resounding horn Gjallarhorn, owns the golden-maned horse Gulltoppr, is called the shining god and the whitest of the gods, has gold teeth, and is the son of Nine Mothers (who may represent personified waves). Heimdallr is attested as possessing foreknowledge, keen eyesight and hearing, and keeps watch for invaders and the onset of Ragnarök while drinking fine mead in his dwelling Himinbjörg, located where the burning rainbow bridge Bifröst meets the sky. Heimdallr is said to be the originator of social classes among humanity and once regained Freyja's treasured possession Brísingamen while doing battle in the shape of a seal with Loki. Heimdallr and Loki are foretold to kill one another during the events of Ragnarök. Heimdallr is additionally referred to as Rig, Hallinskiði, Gullintanni, and Vindlér or Vindhlér.

Heimdallr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional material; in the "Prose Edda" and "Heimskringla", both written in the 13th century by Snorri Sturluson; in the poetry of skalds; and on an Old Norse runic inscription found in England. Two lines of an otherwise lost poem about the god, "Heimdalargaldr", survive. Due to the problematic and enigmatic nature of these attestations, scholars have produced various theories about the nature of the god, including his apparent relation to rams, that he may be a personification of or connected to the world tree Yggdrasil, and potential Indo-European cognates.

The etymology of the name is obscure, but 'the one who illuminates the world' has been proposed. "Heimdallr" may be connected to "Mardöll", one of Freyja's names. "Heimdallr" and its variants are sometimes modernly anglicized as Heimdall (; with the nominative "-r" dropped).

Heimdallr is attested as having three other names; "Hallinskiði", "Gullintanni", and "Vindlér" or "Vindhlér". The name "Hallinskiði" is obscure, but has resulted in a series of attempts at deciphering it. "Gullintanni" literally means 'the one with the golden teeth'. "Vindlér" (or "Vindhlér") translates as either 'the one protecting against the wind' or 'wind-sea'. All three have resulted in numerous theories about the god.

A lead spindle whorl bearing an Old Norse Younger Futhark inscription that mentions Heimdallr was discovered in Saltfleetby, England on September 1, 2010. The spindle whorl itself is dated from the year 1000 to 1100 AD. On the inscription, the god Heimdallr is mentioned alongside the god Odin and Þjálfi, a name of one of the god Thor's servants. Regarding the inscription reading, John Hines of Cardiff University comments that there is "quite an essay to be written over the uncertainties of translation and identification here; what are clear, and very important, are the names of two of the Norse gods on the side, Odin and Heimdallr, while Þjalfi (masculine, not the feminine in -a) is the recorded name of a servant of the god Thor."

In the "Poetic Edda", Heimdallr is attested in six poems; "Völuspá", "Grímnismál", "Lokasenna", "Þrymskviða", "Rígsþula", and "Hrafnagaldr Óðins".

Heimdallr is mentioned thrice in "Völuspá". In the first stanza of the poem, the undead völva reciting the poem calls out for listeners to be silent and refers to Heimdallr:

This stanza has led to various scholarly interpretations. The "holy races" have been considered variously as either humanity or the gods. The notion of humanity as "Heimdallr's sons" is otherwise unattested and has also resulted in various interpretations. Some scholars have pointed to the prose introduction to the poem "Rígsþula", where Heimdallr is said to have once gone about people, slept between couples, and so doled out classes among them (see "Rígsthula" section below).

Later in "Völuspá", the völva foresees the events of Ragnarök and the role in which Heimdallr and Gjallarhorn will play at its onset; Heimdallr will raise his horn and blow loudly. Due to manuscript differences, translations of the stanza vary:

Regarding this stanza, scholar Andy Orchard comments that the name "Gjallarhorn" may here mean "horn of the river Gjöll" as "Gjöll is the name of one of the rivers of the Underworld, whence much wisdom is held to derive", but notes that in the poem "Grímnismál" Heimdallr is said to drink fine mead in his heavenly home Himinbjörg.

Earlier in the same poem, the völva mentions a scenario involving the hearing or horn (depending on translation of the Old Norse noun "hljóð"—translations bolded below for the purpose of illustration) of the god Heimdallr:

Scholar Paul Schach comments that the stanzas in this section of " Völuspá" are "all very mysterious and obscure, as it was perhaps meant to be". Schach details that ""Heimdallar hljóð" has aroused much speculation. Snorri [in the "Prose Edda"] seems to have confused this word with "gjallarhorn", but there is otherwise no attestation of the use of "hljóð" in the sense of 'horn' in Icelandic. Various scholars have read this as "hearing" rather than "horn".

Scholar Carolyne Larrington comments that if "hearing" rather than "horn" is understood to appear in this stanza, the stanza indicates that Heimdallr, like Odin, has left a body part in the well; his ear. Larrington says that "Odin exchanged one of his eyes for wisdom from Mimir, guardian of the well, while Heimdall seems to have forfeited his ear."

In the poem "Grímnismál", Odin (disguised as "Grímnir"), tortured, starved and thirsty, tells the young Agnar of a number of mythological locations. The eighth location he mentions is Himinbjörg, where he says that Heimdallr drinks fine mead:

Regarding the above stanza, Henry Adams Bellows comments that "in this stanza the two functions of Heimdall—as father of humanity [ . . . ] and as warder of the gods—seem both to be mentioned, but the second line in the manuscripts is apparently in bad shape, and in the editions it is more or less conjecture".

In the poem "Lokasenna", Loki flyts with various gods who have met together to feast. At one point during the exchanges, the god Heimdallr says that Loki is drunk and witless, and asks Loki why he won't stop speaking. Loki tells Heimdallr to be silent, that he was fated a "hateful life", that Heimdallr must always have a muddy back, and that he must serve as watchman of the gods. The goddess Skaði interjects and the flyting continues in turn.

The poem "Þrymskviða" tells of Thor's loss of his hammer, Mjöllnir, to the jötnar and quest to get it back. At one point in the tale, the gods gather at the thing and debate how to get Thor's hammer back from the jötnar, who demand the beautiful goddess Freyja in return for it. Heimdallr advises that they simply dress Thor up as Freyja, during which he is described as "hvítastr ása" (translations of the phrase vary below) and is said to have foresight like the Vanir, a group of gods:

Regarding Heimdallr's status as "hvítastr ása" (variously translated above as "brightest" (Thorpe), "whitest" (Bellows), and "most glittering" (Dodds)) and the comparison to the Vanir, scholar John Lindow comments that there are no other indications of Heimdallr being considered among the Vanir, and that Heimdallr's status as ""hvítastr ása "" has not been explained.

The introductory prose to the poem "Rígsþula" says that "people say in the old stories" that Heimdallr, described as a god among the Æsir, once fared on a journey. Heimdallr wandered along a seashore, and referred to himself as "Rígr". In the poem, Rígr, who is described as a wise and powerful god, walks in the middle of roads on his way to steads, where he meets a variety of couples and dines with them, giving them advice and spending three nights at a time between them in their bed. The wives of the couples become pregnant, and from them come the various classes of humanity. Eventually a warrior home produces a promising boy, and as the boy grows older, Rígr comes out of a thicket, teaches the boy runes, gives him a name, and proclaims him to be his son. Rígr tells him to strike out and get land for himself. The boy does so, and so becomes a great war leader with many estates. He marries a beautiful woman and the two have many children and are happy. One of the children eventually becomes so skilled that he is able to share in runic knowledge with Heimdallr, and so earns the title of "Rígr" himself. The poem continues without further mention of the god.

In the "Prose Edda", Heimdallr is mentioned in the books "Gylfaginning", "Skáldskaparmál", and "Háttatal". In "Gylfaginning", the enthroned figure of High tells the disguised mythical king Gangleri of various gods, and, in chapter 25, mentions Heimdallr. High says that Heimdallr is known as "the white As", is "great and holy", and that nine maidens, all sisters, gave birth to him. Heimdallr is called "Hallinskiði" and "Gullintanni", and he has gold teeth. High continues that Heimdallr lives in "a place" called Himinbjörg and that it is near Bifröst. Heimdallr is the watchman of the gods, and he sits on the edge of heaven to guard the Bifröst bridge from the berg jötnar. Heimdallr requires less sleep than a bird, can see at night just as well as if it were day, and for over a hundred leagues. Heimdallr's hearing is also quite keen; he can hear grass as it grows on the earth, wool as it grows on sheep, and anything louder. Heimdallr possesses a trumpet, Gjallarhorn, that, when blown, can be heard in all worlds, and "the head is referred to as Heimdall's sword". High then quotes the above-mentioned "Grímnismál" stanza about Himinbjörg and provides two lines from the otherwise lost poem about Heimdallr, "Heimdalargaldr", in which Heimdallr proclaims himself to be the son of Nine Mothers.

In chapter 49, High tells of the god Baldr's funeral procession. Various deities are mentioned as having attended, including Heimdallr, who there rode his horse Gulltopr.

In chapter 51, High foretells the events of Ragnarök. After the enemies of the gods will gather at the plain Vígríðr, Heimdallr will stand and mightily blow into Gjallarhorn. The gods will awake and assemble together at the thing. At the end of the battle between various gods and their enemies, Heimdallr will face Loki and they will kill one another. After, the world will be engulfed in flames. High then quotes the above-mentioned stanza regarding Heimdallr raising his horn in "Völuspá".

At the beginning of "Skáldskaparmál", Heimdallr is mentioned as having attended a banquet in Asgard with various other deities. Later in the book, "Húsdrápa", a poem by 10th century skald Úlfr Uggason, is cited, during which Heimdallr is described as having ridden to Baldr's funeral pyre.

In chapter 8, means of referring to Heimdallr are provided; "son of nine mothers", "guardian of the gods", "the white As" (see "Poetic Edda" discussion regarding "hvítastr ása" above), "Loki's enemy", and "recoverer of Freyja's necklace". The section adds that the poem "Heimdalargaldr" is about him, and that, since the poem, "the head has been called Heimdall's doom: man's doom is an expression for sword". Hiemdallr is the owner of Gulltoppr, is also known as Vindhlér, and is a son of Odin. Heimdallr visits Vágasker and Singasteinn and there vied with Loki for Brísingamen. According to the chapter, the skald Úlfr Uggason composed a large section of his "Húsdrápa" about these events and that "Húsdrápa" says that the two were in the shape of seals. A few chapters later, ways of referring to Loki are provided, including "wrangler with Heimdall and Skadi", and section of Úlfr Uggason's "Húsdrápa" is then provided in reference:

The chapter points out that in the above "Húsdrápa" section Heimdallr is said to be the son of nine mothers.

Heimdallr is mentioned once in "Háttatal". There, in a composition by Snorri Sturluson, a sword is referred to as "Vindhlér's helmet-filler", meaning "Heimdallr's head".

In "Ynglinga saga" compiled in "Heimskringla", Snorri presents a euhemerized origin of the Norse gods and rulers descending from them. In chapter 5, Snorri asserts that the Æsir settled in what is now Sweden and built various temples. Snorri writes that Odin settled in Lake Logrin "at a place which formerly was called Sigtúnir. There he erected a large temple and made sacrifices according to the custom of the Æsir. He took possession of the land as far as he had called it Sigtúnir. He gave dwelling places to the temple priests." Snorri adds that, after this, Njörðr dwelt in Nóatún, Freyr dwelt in Uppsala, Heimdall at Himinbjörg, Thor at Þrúðvangr, Baldr at Breiðablik and that to everyone Odin gave fine estates.

A figure holding a large horn to his lips and clasping a sword on his hip appears on a stone cross from the Isle of Man. Some scholars have theorized that this figure is a depiction of Heimdallr with Gjallarhorn.

A 9th or 10th century Gosforth Cross in Cumbria, England depicts a figure holding a horn and a sword standing defiantly before two open-mouthed beasts. This figure has been often theorized as depicting Heimdallr with Gjallarhorn.

Heimdallr's attestations have proven troublesome and enigmatic to interpret for scholars. Scholar Georges Dumézil summarizes the difficulties as follows:




</doc>
<doc id="13658" url="https://en.wikipedia.org/wiki?curid=13658" title="House of Lords">
House of Lords

The House of Lords, also known as the House of Peers and domestically usually referred to simply as the Lords, is the upper house of the Parliament of the United Kingdom. Membership is granted by appointment or else by heredity or official function. Like the House of Commons, it meets in the Palace of Westminster. Officially, the full name of the house is the Right Honourable the Lords Spiritual and Temporal of the United Kingdom of Great Britain and Northern Ireland in Parliament assembled.

Unlike the elected House of Commons, members of the House of Lords (excluding 90 hereditary peers elected among themselves and two peers who are "ex officio" members) are appointed. The membership of the House of Lords is drawn from the peerage and is made up of Lords Spiritual and Lords Temporal. The Lords Spiritual are 26 bishops in the established Church of England. Of the Lords Temporal, the majority are life peers who are appointed by the monarch on the advice of the Prime Minister, or on the advice of the House of Lords Appointments Commission. However, they also include some hereditary peers including four dukes.

Membership was once an entitlement of all hereditary peers, other than those in the peerage of Ireland, but under the House of Lords Act 1999, the right to membership was restricted to 92 hereditary peers. Since 2008, only one of them is female (Countess of Mar); most hereditary peerages can be inherited only by men.

While the House of Commons has a defined number of seats membership, the number of members in the House of Lords is not fixed. The House of Lords is the only upper house of any bicameral parliament in the world to be larger than its lower house.

The House of Lords scrutinises bills that have been approved by the House of Commons. It regularly reviews and amends Bills from the Commons. While it is unable to prevent Bills passing into law, except in certain limited circumstances, it can delay Bills and force the Commons to reconsider their decisions. In this capacity, the House of Lords acts as a check on the House of Commons that is independent from the electoral process. Bills can be introduced into either the House of Lords or the House of Commons. While members of the Lords may also take on roles as government ministers, high-ranking officials such as cabinet ministers are usually drawn from the Commons. The House of Lords has its own support services, separate from the Commons, including the House of Lords Library.

The Queen's Speech is delivered in the House of Lords during the State Opening of Parliament. In addition to its role as the upper house, until the establishment of the Supreme Court in 2009, the House of Lords, through the Law Lords, acted as the final court of appeal in the United Kingdom judicial system. The House also has a Church of England role, in that Church Measures must be tabled within the House by the Lords Spiritual.

Today's Parliament of the United Kingdom largely descends, in practice, from the Parliament of England, through the Treaty of Union of 1706 and the Acts of Union that ratified the Treaty in 1707 and created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland. This new parliament was, in effect, the continuation of the Parliament of England with the addition of 45 MPs and 16 Peers to represent Scotland.

The House of Lords developed from the "Great Council" ("Magnum Concilium") that advised the King during medieval times. This royal council came to be composed of ecclesiastics, noblemen, and representatives of the counties of England and Wales (afterwards, representatives of the boroughs as well). The first English Parliament is often considered to be the "Model Parliament" (held in 1295), which included archbishops, bishops, abbots, earls, barons, and representatives of the shires and boroughs.

The power of Parliament grew slowly, fluctuating as the strength of the monarchy grew or declined. For example, during much of the reign of Edward II (1307–1327), the nobility was supreme, the Crown weak, and the shire and borough representatives entirely powerless. In 1569, the authority of Parliament was for the first time recognised not simply by custom or royal charter, but by an authoritative statute, passed by Parliament itself.

During the reign of Edward II's successor, Edward III, Parliament clearly separated into two distinct chambers: the House of Commons (consisting of the shire and borough representatives) and the House of Lords (consisting of the bishops, abbots and peers). The authority of Parliament continued to grow, and during the early 15th century both Houses exercised powers to an extent not seen before. The Lords were far more powerful than the Commons because of the great influence of the great landowners and the prelates of the realm.

The power of the nobility declined during the civil wars of the late 15th century, known as the Wars of the Roses. Much of the nobility was killed on the battlefield or executed for participation in the war, and many aristocratic estates were lost to the Crown. Moreover, feudalism was dying, and the feudal armies controlled by the barons became obsolete. Henry VII (1485–1509) clearly established the supremacy of the monarch, symbolised by the "Crown Imperial". The domination of the Sovereign continued to grow during the reigns of the Tudor monarchs in the 16th century. The Crown was at the height of its power during the reign of Henry VIII (1509–1547).

The House of Lords remained more powerful than the House of Commons, but the Lower House continued to grow in influence, reaching a zenith in relation to the House of Lords during the middle 17th century. Conflicts between the King and the Parliament (for the most part, the House of Commons) ultimately led to the English Civil War during the 1640s. In 1649, after the defeat and execution of King Charles I, the Commonwealth of England was declared, but the nation was effectively under the overall control of Oliver Cromwell, Lord Protector of England, Scotland and Ireland.

The House of Lords was reduced to a largely powerless body, with Cromwell and his supporters in the Commons dominating the Government. On 19 March 1649, the House of Lords was abolished by an Act of Parliament, which declared that "The Commons of England [find] by too long experience that the House of Lords is useless and dangerous to the people of England." The House of Lords did not assemble again until the Convention Parliament met in 1660 and the monarchy was restored. It returned to its former position as the more powerful chamber of Parliament—a position it would occupy until the 19th century.

The 19th century was marked by several changes to the House of Lords. The House, once a body of only about 50 members, had been greatly enlarged by the liberality of George III and his successors in creating peerages. The individual influence of a Lord of Parliament was thus diminished.

Moreover, the power of the House as a whole decreased, whilst that of the House of Commons grew. Particularly notable in the development of the Lower House's superiority was the Reform Bill of 1832. The electoral system of the House of Commons was far from democratic: property qualifications greatly restricted the size of the electorate, and the boundaries of many constituencies had not been changed for centuries.

Entire cities such as Manchester had not even one representative in the House of Commons, while the 11 voters living in Old Sarum retained their ancient right to elect two MPs. A small borough was susceptible to bribery, and was often under the control of a patron, whose nominee was guaranteed to win an election. Some aristocrats were patrons of numerous "pocket boroughs", and therefore controlled a considerable part of the membership of the House of Commons.

When the House of Commons passed a Reform Bill to correct some of these anomalies in 1831, the House of Lords rejected the proposal. The popular cause of reform, however, was not abandoned by the ministry, despite a second rejection of the bill in 1832. Prime Minister Charles Grey, 2nd Earl Grey advised the King to overwhelm opposition to the bill in the House of Lords by creating about 80 new pro-Reform peers. William IV originally balked at the proposal, which effectively threatened the opposition of the House of Lords, but at length relented.

Before the new peers were created, however, the Lords who opposed the bill admitted defeat and abstained from the vote, allowing the passage of the bill. The crisis damaged the political influence of the House of Lords but did not altogether end it. A vital reform was effected by the Lords themselves in 1868, when they changed their standing orders to abolish proxy voting, preventing Lords from voting without taking the trouble to attend. Over the course of the century the powers of the Upper House were further reduced stepwise, culminating in the 20th century with the Parliament Act 1911; the Commons gradually became the stronger House of Parliament.

The status of the House of Lords returned to the forefront of debate after the election of a Liberal Government in 1906. In 1909 the Chancellor of the Exchequer, David Lloyd George, introduced into the House of Commons the "People's Budget", which proposed a land tax targeting wealthy landowners. The popular measure, however, was defeated in the heavily Conservative House of Lords.

Having made the powers of the House of Lords a primary campaign issue, the Liberals were narrowly re-elected in January 1910. Prime Minister H. H. Asquith then proposed that the powers of the House of Lords be severely curtailed. After a further general election in December 1910, and with an undertaking by King George V to create sufficient new Liberal peers to overcome Lords' opposition to the measure if necessary, the Asquith Government secured the passage of a bill to curtail the powers of the House of Lords.

The Parliament Act 1911 effectively abolished the power of the House of Lords to reject legislation, or to amend it in a way unacceptable to the House of Commons: most bills could be delayed for no more than three parliamentary sessions or two calendar years. It was not meant to be a permanent solution; more comprehensive reforms were planned. Neither party, however, pursued the matter with much enthusiasm, and the House of Lords remained primarily hereditary. The Parliament Act 1949 reduced the delaying power of the House of Lords further to two sessions or one year.

In 1958 the predominantly hereditary nature of the House of Lords was changed by the Life Peerages Act 1958, which authorised the creation of life baronies, with no numerical limits. The number of Life Peers then gradually increased, though not at a constant rate.

The Labour Party had, for most of the 20th century, a commitment, based on the party's historic opposition to class privilege, to abolish the House of Lords, or at least expel the hereditary element. In 1968 the Labour Government of Harold Wilson attempted to reform the House of Lords by introducing a system under which hereditary peers would be allowed to remain in the House and take part in debate, but would be unable to vote. This plan, however, was defeated in the House of Commons by a coalition of traditionalist Conservatives (such as Enoch Powell), and Labour members who continued to advocate the outright abolition of the Upper House (such as Michael Foot).

When Michael Foot became leader of the Labour Party in 1980, abolition of the House of Lords became a part of the party's agenda; under his successor, Neil Kinnock, however, a reformed Upper House was proposed instead. In the meantime, the creation of hereditary peerages (except for members of the Royal Family) has been arrested, with the exception of three creations during the administration of the Conservative Margaret Thatcher in the 1980s.

Whilst some hereditary peers were at best apathetic, the Labour Party's clear commitments were not lost on Merlin Hanbury-Tracy, 7th Baron Sudeley, who for decades was considered an expert on the House of Lords. In December 1979 the Conservative Monday Club published his extensive paper entitled "Lords Reform – Why tamper with the House of Lords?" and in July 1980 "The Monarchist" carried another article by Sudeley entitled "Why Reform or Abolish the House of Lords?". In 1990 he wrote a further booklet for the Monday Club entitled "The Preservation of the House of Lords".

There were no women sitting in the House of Lords until 1958, when a small number came into the chamber as a result of the Life Peerages Act 1958. One of these was Irene Curzon, 2nd Baroness Ravensdale, who had inherited her father's peerage in 1925 and was made a life peer to enable her to sit. After a campaign stretching back in some cases to the 1920s, another twelve women who held hereditary peerages in their own right were admitted by the Peerage Act 1963.

The Labour Party included in its 1997 general election manifesto a commitment to remove the hereditary peerage from the House of Lords. Their subsequent election victory in 1997 under Tony Blair led to the denouement of the traditional House of Lords. The Labour Government introduced legislation to expel all hereditary peers from the Upper House as a first step in Lords reform. As a part of a compromise, however, it agreed to permit 92 hereditary peers to remain until the reforms were complete. Thus all but 92 hereditary peers were expelled under the House of Lords Act 1999 (see below for its provisions), making the House of Lords predominantly an appointed house.

Since 1999, however, no further reform has taken place. The Wakeham Commission proposed introducing a 20% elected element to the Lords, but this plan was widely criticised. A parliamentary Joint Committee was established in 2001 to resolve the issue, but it reached no conclusion and instead gave Parliament seven options to choose from (fully appointed, 20% elected, 40% elected, 50% elected, 60% elected, 80%, and fully elected). In a confusing series of votes in February 2003, all of these options were defeated, although the 80% elected option fell by just three votes in the Commons. Socialist MPs favouring outright abolition voted against all the options.

In 2005, a cross-party group of senior MPs (Kenneth Clarke, Paul Tyler, Tony Wright, George Young and Robin Cook) published a report proposing that 70% of members of the House of Lords should be elected—each member for a single long term—by the single transferable vote system. Most of the remainder were to be appointed by a Commission to ensure a mix of "skills, knowledge and experience". This proposal was also not implemented. A cross-party campaign initiative called "Elect the Lords" was set up to make the case for a predominantly elected Second Chamber in the run up to the 2005 general election.

At the 2005 election, the Labour Party proposed further reform of the Lords, but without specific details. The Conservative Party, which had, prior to 1997, opposed any tampering with the House of Lords, favoured an 80% elected Second Chamber, while the Liberal Democrats called for a fully elected Senate. During 2006, a cross-party committee discussed Lords reform, with the aim of reaching a consensus: its findings were published in early 2007.

On 7 March 2007, members of the House of Commons voted ten times on a variety of alternative compositions for the upper chamber. Outright abolition, a wholly appointed house, a 20% elected house, a 40% elected house, a 50% elected house and a 60% elected house were all defeated in turn. Finally the vote for an 80% elected chamber was won by 305 votes to 267, and the vote for a wholly elected chamber was won by an even greater margin: 337 to 224. Significantly this last vote represented an overall majority of MPs.

Furthermore, examination of the names of MPs voting at each division shows that, of the 305 who voted for the 80% elected option, 211 went on to vote for the 100% elected option. Given that this vote took place after the vote on 80% – whose result was already known when the vote on 100% took place – this showed a clear preference for a fully elected upper house among those who voted for the only other option that passed. But this was nevertheless only an indicative vote and many political and legislative hurdles remained to be overcome for supporters of an elected second chamber. The House of Lords, soon after, rejected this proposal and voted for an entirely appointed House of Lords.

In July 2008, Jack Straw, the Secretary of State for Justice and Lord Chancellor, introduced a white paper to the House of Commons proposing to replace the House of Lords with an 80–100% elected chamber, with one third being elected at each general election, for a term of approximately 12–15 years. The white paper stated that as the peerage would be totally separated from membership of the upper house, the name "House of Lords" would no longer be appropriate: it went on to explain that there is cross-party consensus for the new chamber to be titled the "Senate of the United Kingdom"; however, to ensure the debate remains on the role of the upper house rather than its title, the white paper was neutral on the title of the new house.

On 30 November 2009, a "Code of Conduct for Members of the House of Lords" was agreed by them; certain amendments were agreed by them on 30 March 2010 and on 12 June 2014. The scandal over expenses in the Commons was at its highest pitch only six months before, and the Labourite leadership under Baroness Royall of Blaisdon determined that something sympathetic should be done.

In Meg Russell's article "Is the House of Lords already reformed?", she states three essential features of a legitimate House of Lords. The first is that it must have adequate powers over legislation to make the government think twice before making a decision. The House of Lords, she argues, currently has enough power to make it relevant. During Tony Blair's first year, he was defeated 38 times in the Lords. Secondly, as to the composition of the Lords, Meg Russell suggests that the composition must be distinct from the Commons, otherwise it would render the Lords useless. The third feature is the perceived legitimacy of the Lords. She writes, "In general legitimacy comes with election."

The Conservative–Liberal Democrat coalition agreed, after the 2010 general election, to outline clearly a provision for a wholly or mainly elected second chamber, elected by proportional representation. These proposals sparked a debate on 29 June 2010. As an interim measure, appointment of new peers would reflect the shares of the vote secured by the political parties in the last general election.

Detailed proposals for Lords reform, including a draft House of Lords Reform Bill, were published on 17 May 2011. These included a 300-member hybrid house, of whom 80% would be elected. A further 20% would be appointed, and reserve space would be included for some Church of England bishops. Under the proposals, members would also serve single non-renewable terms of 15 years. Former MPs would be allowed to stand for election to the Upper House, but members of the Upper House would not be immediately allowed to become MPs.

The details of the proposal were:

The proposals were considered by a Joint Committee on House of Lords Reform made up of both MPs and Peers, which issued its final report on 23 April 2012, making the following suggestions:

Deputy Prime Minister Nick Clegg introduced the House of Lords Reform Bill 2012 on 27 June 2012 which built on proposals published on 17 May 2011. However, this Bill was abandoned by the Government on 6 August 2012 following opposition from within the Conservative Party.

A private members bill to introduce some reforms was introduced by Dan Byles in 2013. The House of Lords Reform Act 2014 received the Royal Assent in 2014. Under the new law:


The House of Lords (Expulsion and Suspension) Act 2015 authorised the House to expel or suspend members.

This act makes provision to preferentially admit bishops of the Church of England who are women to the Lords Spiritual in the 10 years following its commencement.

In 2015, Rachel Treweek, Bishop of Gloucester, became the first woman to sit as a Lord Spiritual in the House of Lords. As of 2019, five women bishops sit as Lords Spiritual, four of them due to this act.

In 2019, a seven-month enquiry by Naomi Ellenbogen QC found that one in five staff of the house had experienced bullying or harassment which they did not report for fear of reprisals. This was proceeded by several cases, including Liberal Democrat Lord Lester, of lords who used their position to sexually harass or abuse women.

On 19 January 2020, it was announced that House of Lords may be moved from London to a city in Northern England, likely York, or Birmingham, in the Midlands, in an attempt to "reconnect" the area. It is unclear how the Queen's Speech will be conducted in the event of a move. The idea was received negatively by many peers.

The size of the House of Lords has varied greatly throughout its history. The English House of Lords—then comprising 168 members—was joined at Westminster by 16 Scottish peers to represent the peerage of Scotland—a total of 184 nobles—in 1707's first Parliament of Great Britain. A further 28 Irish members to represent the peerage of Ireland were added in 1801 to the first Parliament of the United Kingdom. From about 220 peers in the eighteenth century, the house saw continued expansion; with the increasing numbers of life peers after the Life Peerages Act 1958 and the inclusion of all Scottish peers and the first female peers in the Peerage Act 1963, it increased to a record size of 1,330 in October 1999, before Lords reform reduced it to 669, mostly life peers, by March 2000. The chamber's membership again expanded in the following decades, increasing to above eight hundred active members in 2014 and prompting further reforms in the House of Lords Reform Act that year. A cap of 600 members was subsequently proposed by the Lords, though the current figure is more than this.

In April 2011, a cross-party group of former leading politicians, including many senior members of the House of Lords, called on the Prime Minister David Cameron to stop creating new peers. He had created 117 new peers since becoming prime minister in May 2010, a faster rate of elevation than any PM in British history. The expansion occurred while his government had tried (in vain) to reduce the size of the House of Commons by 50 members, from 650 to 600.

In August 2014, despite there being a seating capacity of only around 230 to 400 on the benches in the Lords chamber, the House had 774 active members (plus 54 who were not entitled to attend or vote, having been suspended or granted leave of absence). This made the House of Lords the largest parliamentary chamber in any democracy. In August 2014, former Speaker of the House of Commons Baroness Betty Boothroyd requested that "older peers should retire gracefully" to ease the overcrowding in the House of Lords. She also criticised successive prime ministers for filling the second chamber with "lobby fodder" in an attempt to help their policies become law. She made her remarks days before a new batch of peers were due to be created and several months after the passage of the House of Lords Reform Act 2014 which enabled peers to retire or resign their seats in the House, which had previously been impossible.

In August 2015, following the creation of a further 45 peers in the Dissolution Honours, the total number of eligible members of the Lords increased to 826. In a report entitled "Does size matter?" the BBC said: "Increasingly, yes. Critics argue the House of Lords is the second largest legislature after the Chinese National People's Congress and dwarfs Upper Houses in other bicameral democracies such as the United States (100 senators), France (348 senators), Australia (76 senators), Canada (105 appointed senators) and India (250 members). The Lords is also larger than the Supreme People's Assembly of North Korea (687 members). [...] Peers grumble that there is not enough room to accommodate all of their colleagues in the Chamber, where there are only about 400 seats, and say they are constantly jostling for space – particularly during high-profile sittings", but added, "On the other hand, defenders of the Lords say that it does a vital job scrutinising legislation, a lot of which has come its way from the Commons in recent years". In late 2016, a Lord Speaker's committee formed to examine the issue of overcrowding, with fears membership could swell to above 1,000, and in October 2017 the committee presented its findings. In December 2017, the Lords debated and broadly approved its report, which proposed a cap on membership at 600 peers, with a fifteen-year term limit for new peers and a "two-out, one-in" limit on new appointments. By October 2018, the Lord Speaker's committee commended the reduction in peers' numbers, noting that the rate of departures had been greater than expected, with the House of Commons's Public Administration and Constitutional Affairs Select Committee approving the progress achieved without legislation. By April 2019, with the retirement of nearly one hundred peers since the passage of the House of Lords Reform Act 2014, the number of active peers had been reduced to a total of 782, of whom 665 were life peers. This total however, remains greater than the membership of 669 peers in March 2000, after implementation of the House of Lords Act 1999 removed the bulk of the hereditary peers from their seats, remains well above the 600-member cap, and is still larger than the House of Commons's 650 members.

Legislation, with the exception of money bills, may be introduced in either House.

The House of Lords debates legislation, and has power to amend or reject bills. However, the power of the Lords to reject a bill passed by the House of Commons is severely restricted by the Parliament Acts. Under those Acts, certain types of bills may be presented for the Royal Assent without the consent of the House of Lords (i.e. the Commons can override the Lords' veto). The House of Lords cannot delay a money bill (a bill that, in the view of the Speaker of the House of Commons, solely concerns national taxation or public funds) for more than one month.

Other public bills cannot be delayed by the House of Lords for more than two parliamentary sessions, or one calendar year. These provisions, however, only apply to public bills that originate in the House of Commons, and cannot have the effect of extending a parliamentary term beyond five years. A further restriction is a constitutional convention known as the Salisbury Convention, which means that the House of Lords does not oppose legislation promised in the Government's election manifesto.

By a custom that prevailed even before the Parliament Acts, the House of Lords is further restrained insofar as financial bills are concerned. The House of Lords may neither originate a bill concerning taxation or Supply (supply of treasury or exchequer funds), nor amend a bill so as to insert a taxation or Supply-related provision. (The House of Commons, however, often waives its privileges and allows the Upper House to make amendments with financial implications.) Moreover, the Upper House may not amend any Supply Bill. The House of Lords formerly maintained the absolute power to reject a bill relating to revenue or Supply, but this power was curtailed by the Parliament Acts.

The House of Lords does not control the term of the Prime Minister or of the Government. Only the Lower House may force the Prime Minister to resign or call elections by passing a motion of no-confidence or by withdrawing supply. Thus, the House of Lords' oversight of the government is limited.

Most Cabinet ministers are from the House of Commons rather than the House of Lords. In particular, all Prime Ministers since 1902 have been members of the Lower House. (Alec Douglas-Home, who became Prime Minister in 1963 whilst still an Earl, disclaimed his peerage and was elected to the Commons soon after his term began.) In recent history, it has been very rare for major cabinet positions (except Lord Chancellor and Leader of the House of Lords) to have been filled by peers.

Exceptions include Lord Carrington, who was the Foreign Secretary between 1979 and 1982, Lord Young of Graffham (Minister without Portfolio, then Secretary of State for Employment and then Secretary of State for Trade and Industry from 1984 to 1989), Baroness Amos, who served as Secretary of State for International Development and Lord Mandelson, who served as First Secretary of State, Secretary of State for Business, Innovation and Skills and President of the Board of Trade. Lord Robertson of Port Ellen was briefly a peer whilst serving as Secretary of State for Defence before resigning to take up the post of Secretary General of NATO. From 1999 to 2010 the Attorney General for England and Wales was a Member of the House of Lords; the most recent was Patricia Scotland.

The House of Lords remains a source for junior ministers and members of government. Like the House of Commons, the Lords also has a Government Chief Whip as well as several Junior Whips. Where a government department is not represented by a minister in the Lords or one is not available, government whips will act as spokesmen for them.

Historically, the House of Lords held several judicial functions. Most notably, until 2009 the House of Lords served as the court of last resort for most instances of UK law. Since 1 October 2009 this role is now held by the Supreme Court of the United Kingdom.

The Lords' judicial functions originated from the ancient role of the Curia Regis as a body that addressed the petitions of the King's subjects. The functions were exercised not by the whole House, but by a committee of "Law Lords". The bulk of the House's judicial business was conducted by the twelve Lords of Appeal in Ordinary, who were specifically appointed for this purpose under the Appellate Jurisdiction Act 1876.

The judicial functions could also be exercised by Lords of Appeal (other members of the House who happened to have held high judicial office). No Lord of Appeal in Ordinary or Lord of Appeal could sit judicially beyond the age of seventy-five. The judicial business of the Lords was supervised by the Senior Lord of Appeal in Ordinary and their deputy, the Second Senior Lord of Appeal in Ordinary.

The jurisdiction of the House of Lords extended, in civil and in criminal cases, to appeals from the courts of England and Wales, and of Northern Ireland. From Scotland, appeals were possible only in civil cases; Scotland's High Court of Justiciary is the highest court in criminal matters. The House of Lords was not the United Kingdom's only court of last resort; in some cases, the Judicial Committee of the Privy Council performs such a function. The jurisdiction of the Privy Council in the United Kingdom, however, is relatively restricted; it encompasses appeals from ecclesiastical courts, disputes under the House of Commons Disqualification Act 1975, and a few other minor matters. Issues related to devolution were transferred from the Privy Council to the Supreme Court in 2009.

The twelve Law Lords did not all hear every case; rather, after World War II cases were heard by panels known as Appellate Committees, each of which normally consisted of five members (selected by the Senior Lord). An Appellate Committee hearing an important case could consist of more than five members. Though Appellate Committees met in separate committee rooms, judgement was given in the Lords Chamber itself. No further appeal lay from the House of Lords, although the House of Lords could refer a "preliminary question" to the European Court of Justice in cases involving an element of European Union law, and a case could be brought at the European Court of Human Rights if the House of Lords did not provide a satisfactory remedy in cases where the European Convention on Human Rights was relevant.

A distinct judicial function—one in which the whole House used to participate—is that of trying impeachments. Impeachments were brought by the House of Commons, and tried in the House of Lords; a conviction required only a majority of the Lords voting. Impeachments, however, are to all intents and purposes obsolete; the last impeachment was that of Henry Dundas, 1st Viscount Melville, in 1806.

Similarly, the House of Lords was once the court that tried peers charged with high treason or felony. The House would be presided over not by the Lord Chancellor, but by the Lord High Steward, an official especially appointed for the occasion of the trial. If Parliament was not in session, then peers could be tried in a separate court, known as the Lord High Steward's Court. Only peers, their wives, and their widows (unless remarried) were entitled to such trials; the Lords Spiritual were tried in ecclesiastical courts. In 1948, the right of peers to be tried in such special courts was abolished; now, they are tried in the regular courts. The last such trial in the House was of Edward Russell, 26th Baron de Clifford, in 1935. An illustrative dramatisation circa 1928 of a trial of a peer (the fictional Duke of Denver) on a charge of murder (a felony) is portrayed in the 1972 BBC Television adaption of Dorothy L. Sayers' Lord Peter Wimsey mystery "Clouds of Witness".

The Constitutional Reform Act 2005 resulted in the creation of a separate Supreme Court of the United Kingdom, to which the judicial function of the House of Lords, and some of the judicial functions of the Judicial Committee of the Privy Council, were transferred. In addition, the office of Lord Chancellor was reformed by the act, removing his ability to act as both a government minister and a judge. This was motivated in part by concerns about the historical admixture of legislative, judicial, and executive power. The new Supreme Court is located at Middlesex Guildhall.

Members of the House of Lords who sit by virtue of their ecclesiastical offices are known as Lords Spiritual. Formerly, the Lords Spiritual were the majority in the English House of Lords, comprising the church's archbishops, (diocesan) bishops, abbots, and those priors who were entitled to wear a mitre. After the English Reformation's highpoint in 1539, only the archbishops and bishops continued to attend, as the Dissolution of the Monasteries had just disproved of and suppressed the positions of abbot and prior. In 1642, during the few Lords' gatherings convened during English Interregnum which saw periodic war, the Lords Spiritual were excluded altogether, but they returned under the Clergy Act 1661.

The number of Lords Spiritual was further restricted by the Bishopric of Manchester Act 1847, and by later Acts. The Lords Spiritual can now number no more than 26; these are the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham, the Bishop of Winchester (who sit by right regardless of seniority) and the 21 longest-serving bishops from other dioceses in the Church of England (excluding the dioceses of Sodor and Man and Gibraltar in Europe, as these lie entirely outside the United Kingdom). Following a change to the law in 2014 to allow women to be ordained bishops, the Lords Spiritual (Women) Act 2015 was passed, which provides that whenever a vacancy arises among the Lords Spiritual during the ten years following the Act coming into force, the vacancy has to be filled by a woman, if one is eligible. This does not apply to the five bishops who sit by right.

The current Lords Spiritual represent only the Church of England. Bishops of the Church of Scotland historically sat in the Parliament of Scotland but were finally excluded in 1689 (after a number of previous exclusions) when the Church of Scotland became permanently Presbyterian. There are no longer bishops in the Church of Scotland in the traditional sense of the word, and that Church has never sent members to sit in the Westminster House of Lords. The Church of Ireland did obtain representation in the House of Lords after the union of Ireland and Great Britain in 1801.

Of the Church of Ireland's ecclesiastics, four (one archbishop and three bishops) were to sit at any one time, with the members rotating at the end of every parliamentary session (which normally lasted about one year). The Church of Ireland, however, was disestablished in 1871, and thereafter ceased to be represented by Lords Spiritual. Bishops of Welsh sees in the Church of England originally sat in the House of Lords (after 1847, only if their seniority within the church entitled them to), but the Church in Wales ceased to be a part of the Church of England in 1920 and was simultaneously disestablished in Wales. Accordingly, bishops of the Church in Wales were no longer eligible to be appointed to the House as bishops of the Church of England, but those already appointed remained.

Other ecclesiastics have sat in the House of Lords as Lords Temporal in recent times: Chief Rabbi Immanuel Jakobovits was appointed to the House of Lords (with the consent of the Queen, who acted on the advice of Prime Minister Margaret Thatcher), as was his successor Chief Rabbi Jonathan Sacks. Julia Neuberger is the senior rabbi to the West London Synagogue. In recognition of his work at reconciliation and in the peace process in Northern Ireland, the Archbishop of Armagh (the senior Anglican bishop in Northern Ireland), Robin Eames, was appointed to the Lords by John Major. Other clergy appointed include Donald Soper, Timothy Beaumont, and some Scottish clerics.

There have been no Roman Catholic clergy appointed, though it was rumoured that Cardinal Basil Hume and his successor Cormac Murphy O'Connor were offered peerages by James Callaghan, Margaret Thatcher and Tony Blair respectively, but declined. Hume later accepted the Order of Merit, a personal appointment of the Queen, shortly before his death. O'Connor said he had his maiden speech ready, but Roman Catholics who have received holy orders are prohibited by canon law from holding major offices connected with any government other than the Holy See.

Former Archbishops of Canterbury, having reverted to the status of a regular bishop but no longer diocesans, are invariably given life peerages and sit as Lords Temporal.

By custom at least one of the bishops reads prayers in each legislative day (a role taken by the chaplain in the Commons). They often speak in debates; in 2004 Rowan Williams, the Archbishop of Canterbury, opened a debate into sentencing legislation. Measures (proposed laws of the Church of England) must be put before the Lords, and the Lords Spiritual have a role in ensuring that this takes place.

Since the Dissolution of the Monasteries, the Lords Temporal have been the most numerous group in the House of Lords. Unlike the Lords Spiritual, they may be publicly partisan, aligning themselves with one or another of the political parties that dominate the House of Commons. Publicly non-partisan Lords are called crossbenchers. Originally, the Lords Temporal included several hundred hereditary peers (that is, those whose peerages may be inherited), who ranked variously as dukes, marquesses, earls, viscounts, and barons (as well as Scottish Lords of Parliament). Such hereditary dignities can be created by the Crown; in modern times this is done on the advice of the Prime Minister of the day (except in the case of members of the Royal Family).

Holders of Scottish and Irish peerages were not always permitted to sit in the Lords. When Scotland united with England to form Great Britain in 1707, it was provided that the Scottish hereditary peers would only be able to elect 16 representative peers to sit in the House of Lords; the term of a representative was to extend until the next general election. A similar provision was enacted when Ireland merged with Great Britain in 1801 to form the United Kingdom; the Irish peers were allowed to elect 28 representatives, who were to retain office for life. Elections for Irish representatives ended in 1922, when most of Ireland became an independent state; elections for Scottish representatives ended with the passage of the Peerage Act 1963, under which all Scottish peers obtained seats in the Upper House.

In 1999, the Labour government brought forward the House of Lords Act removing the right of several hundred hereditary peers to sit in the House. The Act provided, as a measure intended to be temporary, that 92 people would continue to sit in the Lords by virtue of hereditary peerages, and this is still in effect.

Of the 92, two remain in the House of Lords because they hold royal offices connected with Parliament: the Earl Marshal and the Lord Great Chamberlain. Of the remaining ninety peers sitting in the Lords by virtue of a hereditary peerage, 15 are elected by the whole House and 75 are chosen by fellow hereditary peers in the House of Lords, grouped by party. (If a hereditary peerage holder is given a life peerage, he or she becomes a member of the House of Lords without a need for a by-election.) The exclusion of other hereditary peers removed Charles, Prince of Wales (who is also Earl of Chester) and all other Royal Peers, including Prince Philip, Duke of Edinburgh; Prince Andrew, Duke of York; Prince Edward, Earl of Wessex; Prince Richard, Duke of Gloucester; and Prince Edward, Duke of Kent.

The number of hereditary peers to be chosen by a political group reflects the proportion of hereditary peers that belonged to that group (see current composition below) in 1999. When an elected hereditary peer dies, a by-election is held, with a variant of the Alternative Vote system being used. If the recently deceased hereditary peer had been elected by the whole House, then so is his or her replacement; a hereditary peer elected by a specific political group (including the non-aligned crossbenchers) is replaced by a vote of the hereditary peers already elected to the Lords belonging to that political group (whether elected by that group or by the whole house).

Until 2009, the Lords Temporal also included the Lords of Appeal in Ordinary, more commonly known as Law Lords, a group of individuals appointed to the House of Lords so that they could exercise its judicial functions. Lords of Appeal in Ordinary were first appointed under the Appellate Jurisdiction Act 1876. They were selected by the Prime Minister of the day, but were formally appointed by the Sovereign. A Lord of Appeal in Ordinary had to retire at the age of 70, or, if his or her term was extended by the government, at the age of 75; after reaching such an age, the Law Lord could not hear any further cases in the House of Lords.

The number of Lords of Appeal in Ordinary (excluding those who were no longer able to hear cases because of age restrictions) was limited to twelve, but could be changed by statutory instrument. By a convention of the House, Lords of Appeal in Ordinary did not take part in debates on new legislation, so as to maintain judicial independence. Lords of Appeal in Ordinary held their seats in the House of Lords for life, remaining as members even after reaching the judicial retirement age of 70 or 75. Former Lord Chancellors and holders of other high judicial office could also sit as Law Lords under the Appellate Jurisdiction Act, although in practice this right was only rarely exercised.

Under the Constitutional Reform Act 2005, the Lords of Appeal in Ordinary when the Act came into effect in 2009 became judges of the new Supreme Court of the United Kingdom and were then barred from sitting or voting in the House of Lords until they had retired as judges. One of the main justifications for the new Supreme Court was to establish a separation of powers between the judiciary and the legislature. It is therefore unlikely that future appointees to the Supreme Court of the United Kingdom will be made Lords of Appeal in Ordinary.

The largest group of Lords Temporal, and indeed of the whole House, are life peers. As of June 2019 there are 661 life peers. Life peerages rank only as barons or baronesses, and are created under the Life Peerages Act 1958. Like all other peers, life peers are created by the Sovereign, who acts on the advice of the Prime Minister or the House of Lords Appointments Commission. By convention, however, the Prime Minister allows leaders of other parties to nominate some life peers, so as to maintain a political balance in the House of Lords. Moreover, some non-party life peers (the number being determined by the Prime Minister) are nominated by the independent House of Lords Appointments Commission.

In 2000, the government announced it would set up an Independent Appointments Commission, under Lord Stevenson of Coddenham, to select fifteen so-called "people's peers" for life peerages. However, when the choices were announced in April 2001, from a list of 3,000 applicants, the choices were treated with criticism in the media, as all were distinguished in their field, and none were "ordinary people" as some had originally hoped.

Several different qualifications apply for membership of the House of Lords. No person may sit in the House of Lords if under the age of 21. Furthermore, only United Kingdom, Irish and Commonwealth citizens may sit in the House of Lords. The nationality restrictions were previously more stringent: under the Act of Settlement 1701, and prior to the British Nationality Act 1948, only natural-born subjects qualified.

Additionally, some bankruptcy-related restrictions apply to members of the Upper House. A person may not sit in the House of Lords if he or she is the subject of a Bankruptcy Restrictions Order (applicable in England and Wales only), or if he or she is adjudged bankrupt (in Northern Ireland), or if his or her estate is sequestered (in Scotland). A final restriction bars an individual convicted of high treason from sitting in the House of Lords until completing his or her full term of imprisonment. An exception applies, however, if the individual convicted of high treason receives a full pardon. Note that an individual serving a prison sentence for an offence other than high treason is "not" automatically disqualified.

Women were excluded from the House of Lords until the Life Peerages Act 1958, passed to address the declining number of active members, made possible the creation of peerages for life. Women were immediately eligible and four were among the first life peers appointed. However, hereditary peeresses continued to be excluded until the passage of the Peerage Act 1963. Since the passage of the House of Lords Act 1999, hereditary peeresses remain eligible for election to the Upper House; there is one (Margaret of Mar, 31st Countess of Mar) among the 90 hereditary peers who continue to sit.

The Honours (Prevention of Abuses) Act 1925 made it illegal for a peerage, or other honour, to be bought or sold. Nonetheless, there have been repeated allegations that life peerages (and thus membership of the House of Lords) have been made available to major political donors in exchange for donations. The most prominent case, the 2006 Cash for Honours scandal, saw a police investigation, with no charges being brought. A 2015 study found that of 303 people nominated for peerages in the period 2005–14, a total of 211 were former senior figures within politics (including former MPs), or were non-political appointments. Of the remaining 92 political appointments from outside public life, 27 had made significant donations to political parties. The authors concluded firstly that nominees from outside public life were much more likely to have made large gifts than peers nominated after prior political or public service. They also found that significant donors to parties were far more likely to be nominated for peerages than other party members.

Traditionally there was no mechanism by which members could resign or be removed from the House of Lords (compare the situation as regards resignation from the House of Commons). The Peerage Act 1963 permitted a person to disclaim their newly inherited peerage (within certain time limits); this meant that such a person could effectively renounce their membership of the Lords. This might be done in order to remain or become qualified to sit in the House of Commons, as in the case of Tony Benn (formerly the second Viscount Stansgate), who had campaigned for such a change.

The House of Lords Reform Act 2014 made provision for members' resignation from the House, removal for non-attendance, and automatic expulsion upon conviction for a serious criminal offence (if resulting in a jail sentence of at least one year). In June 2015, under the House of Lords (Expulsion and Suspension) Act 2015, the House's Standing Orders may provide for the expulsion or suspension of a member upon a resolution of the House.

Traditionally the House of Lords did not elect its own speaker, unlike the House of Commons; rather, the "ex officio" presiding officer was the Lord Chancellor. With the passage of the Constitutional Reform Act 2005, the post of Lord Speaker was created, a position to which a peer is elected by the House and subsequently appointed by the Crown. The first Lord Speaker, elected on 4 May 2006, was Baroness Hayman, a former Labour peer. As the Speaker is expected to be an impartial presiding officer, Hayman resigned from the Labour Party. In 2011, Baroness D'Souza was elected as the second Lord Speaker, replacing Hayman in September 2011. D'Souza was in turn succeeded by Lord Fowler in September 2016, the incumbent Lord Speaker.

This reform of the post of Lord Chancellor was made due to the perceived constitutional anomalies inherent in the role. The Lord Chancellor was not only the Speaker of the House of Lords, but also a member of the Cabinet; his or her department, formerly the Lord Chancellor's Department, is now called the Ministry of Justice. The Lord Chancellor is no longer the head of the judiciary of England and Wales. Hitherto, the Lord Chancellor was part of all three branches of government: the legislative, the executive, and the judicial.

The overlap of the legislative and executive roles is a characteristic of the Westminster system, as the entire cabinet consists of members of the House of Commons or the House of Lords; however, in June 2003, the Blair Government announced its intention to abolish the post of Lord Chancellor because of the office's mixed executive and judicial responsibilities. The abolition of the office was rejected by the House of Lords, and the Constitutional Reform Act 2005 was thus amended to preserve the office of Lord Chancellor. The Act no longer guarantees that the office holder of Lord Chancellor is the presiding officer of the House of Lords, and therefore allows the House of Lords to elect a speaker of their own.
The Lord Speaker may be replaced as presiding officer by one of his or her deputies. The Chairman of Committees, the Principal Deputy Chairman of Committees, and several Chairmen are all deputies to the Lord Speaker, and are all appointed by the House of Lords itself at the beginning of each session. By custom, the Crown appoints each Chairman, Principal Deputy Chairman and Deputy Chairman to the additional office of Deputy Speaker of the House of Lords. There was previously no legal requirement that the Lord Chancellor or a Deputy Speaker be a member of the House of Lords (though the same has long been customary).

Whilst presiding over the House of Lords, the Lord Chancellor traditionally wore ceremonial black and gold robes. Robes of black and gold are now worn by the Lord Chancellor and Secretary of State for Justice in the House of Commons, on ceremonial occasions. This is no longer a requirement for the Lord Speaker except for State occasions outside of the chamber. The Speaker or Deputy Speaker sits on the Woolsack, a large red seat stuffed with wool, at the front of the Lords Chamber.

When the House of Lords resolves itself into committee (see below), the Chairman of Committees or a Deputy Chairman of Committees presides, not from the Woolsack, but from a chair at the Table of the House. The presiding officer has little power compared to the Speaker of the House of Commons. He or she only acts as the mouthpiece of the House, performing duties such as announcing the results of votes. This is because, unlike in the House of Commons where all statements are directed to "Mr/Madam Speaker", in the House of Lords they are directed to "My Lords"; i.e., the entire body of the House.

The Lord Speaker or Deputy Speaker cannot determine which members may speak, or discipline members for violating the rules of the House; these measures may be taken only by the House itself. Unlike the politically neutral Speaker of the House of Commons, the Lord Chancellor and Deputy Speakers originally remained members of their respective parties, and were permitted to participate in debate; however, this is no longer true of the new role of Lord Speaker.

Another officer of the body is the Leader of the House of Lords, a peer selected by the Prime Minister. The Leader of the House is responsible for steering Government bills through the House of Lords, and is a member of the Cabinet. The Leader also advises the House on proper procedure when necessary, but such advice is merely informal, rather than official and binding. A Deputy Leader is also appointed by the Prime Minister, and takes the place of an absent or unavailable leader.

The Clerk of the Parliaments is the chief clerk and officer of the House of Lords (but is not a member of the House itself). The Clerk, who is appointed by the Crown, advises the presiding officer on the rules of the House, signs orders and official communications, endorses bills, and is the keeper of the official records of both Houses of Parliament. Moreover, the Clerk of the Parliaments is responsible for arranging by-elections of hereditary peers when necessary. The deputies of the Clerk of the Parliaments (the Clerk Assistant and the Reading Clerk) are appointed by the Lord Speaker, subject to the House's approval.

The Gentleman Usher of the Black Rod is also an officer of the House; he takes his title from the symbol of his office, a black rod. Black Rod (as the Gentleman Usher is normally known) is responsible for ceremonial arrangements, is in charge of the House's doorkeepers, and may (upon the order of the House) take action to end disorder or disturbance in the Chamber. Black Rod also holds the office of Serjeant-at-Arms of the House of Lords, and in this capacity attends upon the Lord Speaker. The Gentleman Usher of the Black Rod's duties may be delegated to the Yeoman Usher of the Black Rod or to the Assistant Serjeant-at-Arms.

The House of Lords and the House of Commons assemble in the Palace of Westminster. The Lords Chamber is lavishly decorated, in contrast with the more modestly furnished Commons Chamber. Benches in the Lords Chamber are coloured red. The Woolsack is at the front of the Chamber; the Government sit on benches on the right of the Woolsack, while members of the Opposition sit on the left. Crossbenchers, sit on the benches immediately opposite the Woolsack.

The Lords Chamber is the site of many formal ceremonies, the most famous of which is the State Opening of Parliament, held at the beginning of each new parliamentary session. During the State Opening, the Sovereign, seated on the Throne in the Lords Chamber and in the presence of both Houses of Parliament, delivers a speech outlining the Government's agenda for the upcoming parliamentary session.

In the House of Lords, members need not seek the recognition of the presiding officer before speaking, as is done in the House of Commons. If two or more Lords simultaneously rise to speak, the House decides which one is to be heard by acclamation, or, if necessary, by voting on a motion. Often, however, the Leader of the House will suggest an order, which is thereafter generally followed. Speeches in the House of Lords are addressed to the House as a whole ("My Lords") rather than to the presiding officer alone (as is the custom in the Lower House). Members may not refer to each other in the second person (as "you"), but rather use third person forms such as "the noble Duke", "the noble Earl", "the noble Lord", "my noble friend", "The most Reverend Primate", etc.

Each member may make no more than one speech on a motion, except that the mover of the motion may make one speech at the beginning of the debate and another at the end. Speeches are not subject to any time limits in the House; however, the House may put an end to a speech by approving a motion "that the noble Lord be no longer heard". It is also possible for the House to end the debate entirely, by approving a motion "that the Question be now put". This procedure is known as Closure, and is extremely rare. Six closure motions were passed on 4 April 2019 to significant media attention as part of consideration of a private member's bill concerning the United Kingdom's withdrawal from the European Union.

Once all speeches on a motion have concluded, or Closure invoked, the motion may be put to a vote. The House first votes by voice vote; the Lord Speaker or Deputy Speaker puts the question, and the Lords respond either "content" (in favour of the motion) or "not content" (against the motion). The presiding officer then announces the result of the voice vote, but if his assessment is challenged by any Lord, a recorded vote known as a division follows.

Members of the House enter one of two lobbies (the "content" lobby or the "not-content" lobby) on either side of the Chamber, where their names are recorded by clerks. At each lobby are two Tellers (themselves members of the House) who count the votes of the Lords. The Lord Speaker may not take part in the vote. Once the division concludes, the Tellers provide the results thereof to the presiding officer, who then announces them to the House.

If there is an equality of votes, the motion is decided according to the following principles: legislation may proceed in its present form, unless there is a majority in favour of amending or rejecting it; any other motions are rejected, unless there is a majority in favour of approving it. The quorum of the House of Lords is just three members for a general or procedural vote, and 30 members for a vote on legislation. If fewer than three or 30 members (as appropriate) are present, the division is invalid.

By contrast with the House of Commons, the House of Lords has not until recently had an established procedure for imposing sanctions on its members. When a cash for influence scandal was referred to the Committee of Privileges in January 2009, the Leader of the House of Lords also asked the Privileges Committee to report on what sanctions the House had against its members. After seeking advice from the Attorney General for England and Wales and the former Lord Chancellor Lord Mackay of Clashfern, the committee decided that the House "possessed an inherent power" to suspend errant members, although not to withhold a writ of summons nor to expel a member permanently. When the House subsequently suspended Lord Truscott and Lord Taylor of Blackburn for their role in the scandal, they were the first to meet this fate since 1642.

Recent changes have expanded the disciplinary powers of the House. Section 3 of the House of Lords Reform Act 2014 now provides that any member of the House of Lords convicted of a crime and sentenced to imprisonment for more than one year loses their seat. The House of Lords (Expulsion and Suspension) Act 2015 allows the House to set up procedures to suspend, and to expel, its members.

There are two motions which have grown up through custom and practice and which govern questionable conduct within the House. They are brought into play by a member standing up, possibly intervening on another member, and moving the motion without notice. When the debate is getting excessively heated, it is open to a member to move "that the Standing Order on Asperity of Speech be read by the Clerk". The motion can be debated, but if agreed by the House, the Clerk of the Parliaments will read Standing Order 33 which provides "That all personal, sharp, or taxing speeches be forborn". The Journals of the House of Lords record only four instances on which the House has ordered the Standing Order to be read since the procedure was invented in 1871.

For more serious problems with an individual Lord, the option is available to move "That the noble Lord be no longer heard". This motion also is debatable, and the debate which ensues has sometimes offered a chance for the member whose conduct has brought it about to come to order so that the motion can be withdrawn. If the motion is passed, its effect is to prevent the member from continuing their speech on the motion then under debate. The Journals identify eleven occasions on which this motion has been moved since 1884; four were eventually withdrawn, one was voted down, and six were passed.

In 1958, to counter criticism that some peers only appeared at major decisions in the House and thereby particular votes were swayed, the Standing Orders of the House of Lords were enhanced. Peers who did not wish to attend meetings regularly or were prevented by ill health, age or further reasons, were now able to request Leave of Absence. During the granted time a peer is expected not to visit the House's meetings until either its expiration or termination, announced at least a month prior to their return.

Members of the House of Lords can, since 2010, opt to receive a £300 per day attendance allowance (increased in 2017 to £310), plus limited travel expenses. Peers can elect to receive a reduced attendance allowance of £150 per day instead. Prior to 2010 peers from outside London could claim an overnight allowance of £174.

Unlike in the House of Commons, when the term committee is used to describe a stage of a bill, this committee does not take the form of a public bill committee, but what is described as Committee of the Whole House. It is made up of all Members of the House of Lords allowing any Member to contribute to debates if he or she chooses to do so and allows for more flexible rules of procedure. It is presided over by the Chairman of Committees.

The term committee is also used to describe Grand Committee, where the same rules of procedure apply as in the main chamber, except that no divisions may take place. For this reason, business that is discussed in Grand Committee is usually uncontroversial and likely to be agreed unanimously.

Public bills may also be committed to pre-legislative committees. A pre-legislative Committee is specifically constituted for a particular bill. These committees are established in advance of the bill being laid before either the House of Lords or the House of Commons and can take evidence from the public. Such committees are rare and do not replace any of the usual stages of a bill, including committee stage.

The House of Lords also has 15 Select committees. Typically, these are "sessional committees", meaning that their members are appointed by the House at the beginning of each session, and continue to serve until the next parliamentary session begins. In practice, these are often permanent committees, which are re-established during every session. These committees are typically empowered to make reports to the House "from time to time", that is, whenever they wish. Other committees are "ad-hoc committees", which are set up to investigate a specific issue. When they are set up by a motion in the House, the motion will set a deadline by which the Committee must report. After this date, the Committee will cease to exist unless it is granted an extension. One example of this is the Committee on Public Service and Demographic Change. The House of Lords may appoint a chairman for a committee; if it does not do so, the Chairman of Committees or a Deputy Chairman of Committees may preside instead. Most of the Select Committees are also granted the power to co-opt members, such as the European Union Committee. The primary function of Select Committees is to scrutinise and investigate Government activities; to fulfil these aims, they are permitted to hold hearings and collect evidence. Bills may be referred to Select Committees, but are more often sent to the Committee of the Whole House and Grand Committees.

The committee system of the House of Lords also includes several Domestic Committees, which supervise or consider the House's procedures and administration. One of the Domestic Committees is the Committee of Selection, which is responsible for assigning members to many of the House's other committees.

There are currently sitting members of the House of Lords. As of June 2019, 661 are life peers. An additional Lords are ineligible from participation, including eight peers who are constitutionally disqualified as members of the Judiciary.

The House of Lords Act 1999 allocated 75 of the 92 hereditary peers to the parties based on the proportion of hereditary peers that belonged to that party in 1999:

Of the initial 42 hereditary peers elected as Conservatives, one, Lord Willoughby de Broke, defected to UKIP, though he left the party in 2018.

Fifteen hereditary peers are elected by the whole House, and the remaining hereditary peers are the two royal office-holders, the Earl Marshal and the Lord Great Chamberlain, both of whom are currently on leave of absence.

A report in 2007 stated that many members of the Lords (particularly the life peers) do not attend regularly; the average daily attendance was around 408.

While the number of hereditary peers is limited to 92, and that of Lords spiritual to 26, there is no maximum limit to the number of life peers who may be members of the House of Lords at any time.


Baroness Blackwood of North Oxford – Parliamentary Under Secretary of State for Health
Baroness Vere of Norbiton – Parliamentary Under Secretary of State at the Department for Transport









</doc>
<doc id="13660" url="https://en.wikipedia.org/wiki?curid=13660" title="Homeomorphism">
Homeomorphism

In the mathematical field of topology, a homeomorphism, topological isomorphism, or bicontinuous function is a continuous function between topological spaces that has a continuous inverse function. Homeomorphisms are the isomorphisms in the category of topological spaces—that is, they are the mappings that preserve all the topological properties of a given space. Two spaces with a homeomorphism between them are called homeomorphic, and from a topological viewpoint they are the same. The word "homeomorphism" comes from the Greek words "ὅμοιος" ("homoios") = similar or same and "μορφή" ("morphē") = shape, form, introduced to mathematics by Henri Poincaré in 1895. 

Very roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. However, this description can be misleading. Some continuous deformations are not homeomorphisms, such as the deformation of a line into a point. Some homeomorphisms are not continuous deformations, such as the homeomorphism between a trefoil knot and a circle.

An often-repeated mathematical joke is that topologists can't tell the difference between a coffee cup and a donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in the cup's handle.

A function formula_1 between two topological spaces is a homeomorphism if it has the following properties:


A homeomorphism is sometimes called a bicontinuous function. If such a function exists, formula_6 and formula_7 are homeomorphic. A self-homeomorphism is a homeomorphism from a topological space onto itself. "Being homeomorphic" is an equivalence relation on topological spaces. Its equivalence classes are called homeomorphism classes.



The third requirement, that formula_22 be continuous, is essential. Consider for instance the function formula_23 (the unit circle in formula_24) defined byformula_25. This function is bijective and continuous, but not a homeomorphism (formula_26 is compact but formula_27 is not). The function formula_22 is not continuous at the point formula_29, because although formula_22 maps formula_29 to formula_32, any neighbourhood of this point also includes points that the function maps close to formula_33, but the points it maps to numbers in between lie outside the neighbourhood.

Homeomorphisms are the isomorphisms in the category of topological spaces. As such, the composition of two homeomorphisms is again a homeomorphism, and the set of all self-homeomorphisms formula_34 forms a group, called the homeomorphism group of "X", often denoted formula_35. This group can be given a topology, such as the compact-open topology, which under certain assumptions makes it a topological group.

For some purposes, the homeomorphism group happens to be too big, but by means of the isotopy relation, one can reduce this group to the mapping class group.

Similarly, as usual in category theory, given two spaces that are homeomorphic, the space of homeomorphisms between them, formula_36 is a torsor for the homeomorphism groups formula_35 and formula_38, and, given a specific homeomorphism between formula_6 and formula_7, all three sets are identified.


The intuitive criterion of stretching, bending, cutting and gluing back together takes a certain amount of practice to apply correctly—it may not be obvious from the description above that deforming a line segment to a point is impermissible, for instance. It is thus important to realize that it is the formal definition given above that counts. In this case, for example, the line segment possesses infinitely many points, and therefore cannot be put into a bijection with a set containing only a finite number of points, including a single point.

This characterization of a homeomorphism often leads to a confusion with the concept of homotopy, which is actually "defined" as a continuous deformation, but from one "function" to another, rather than one space to another. In the case of a homeomorphism, envisioning a continuous deformation is a mental tool for keeping track of which points on space "X" correspond to which points on "Y"—one just follows them as "X" deforms. In the case of homotopy, the continuous deformation from one map to the other is of the essence, and it is also less restrictive, since none of the maps involved need to be one-to-one or onto. Homotopy does lead to a relation on spaces: homotopy equivalence.

There is a name for the kind of deformation involved in visualizing a homeomorphism. It is (except when cutting and regluing are required) an isotopy between the identity map on "X" and the homeomorphism from "X" to "Y".




</doc>
<doc id="13661" url="https://en.wikipedia.org/wiki?curid=13661" title="Hvergelmir">
Hvergelmir

In Norse mythology, Hvergelmir (Old Norse "bubbling boiling spring") is a major spring. Hvergelmir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", Hvergelmir is mentioned in a single stanza, which details that it is the location where liquid from the antlers of the stag Eikþyrnir flow, and that the spring, "whence all waters rise", is the source of numerous rivers. The "Prose Edda" repeats this information and adds that the spring is located in Niflheim, that it is one of the three major springs at the primary roots of the cosmic tree Yggdrasil (the other two are Urðarbrunnr and Mímisbrunnr), and that within the spring are a vast amount of snakes and the dragon Níðhöggr.

Hvergelmir is attested in the following works:

Hvergelmir receives a single mention in the "Poetic Edda", found in the poem "Grímnismál":

This stanza is followed by three stanzas consisting mainly of the names of 42 rivers. Some of these rivers lead to the dwelling of the gods (such as Gömul and Geirvimul), while at least two (Gjöll and Leipt), reach to Hel.

Hvergelmir is mentioned several times in the "Prose Edda". In "Gylfaginning", Just-as-High explains that the spring Hvergelmir is located in the foggy realm of Niflheim: "It was many ages before the earth was created that Niflheim was made, and in its midst lies a spring called Hvergelmir, and from it flows the rivers called Svol, Gunnthra, Fiorm, Fimbulthul, Slidr and Hrid, Sylg and Ylg, Vid, Leiptr; Gioll is next to Hell-gates."

Later in "Gylfaginning", Just-as-High describes the central tree Yggdrasil. Just-as-High says that three roots of the tree support it and "extend very, very far" and that the third of these three roots extends over Niflheim. Beneath this root, says Just-as-High, is the spring Hvergelmir, and that the base of the root is gnawed on by the dragon Níðhöggr. Additionally, High says that Hvergelmir contains not only Níðhöggr but also so many snakes that "no tongue can enumerate them".

The spring is mentioned a third time in "Gylfaginning" where High recounts its source: the stag Eikþyrnir stands on top of the afterlife hall Valhalla feeding branches of Yggdrasil, and from the stag's antlers drips great amounts of liquid down into Hvergelmir. High tallies 26 rivers here.

Hvergelmir is mentioned a final time in the "Prose Edda" where Third discusses the unpleasantries of Náströnd. Third notes that Hvergelmir yet worse than the venom-filled Náströnd because—by way of quoting a portion of a stanza from the "Poetic Edda" poem "Völuspá"—"There Nidhogg torments the bodies of the dead".



</doc>
<doc id="13665" url="https://en.wikipedia.org/wiki?curid=13665" title="Hausdorff maximal principle">
Hausdorff maximal principle

In mathematics, the Hausdorff maximal principle is an alternate and earlier formulation of Zorn's lemma proved by Felix Hausdorff in 1914 (Moore 1982:168). It states that in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset.

The Hausdorff maximal principle is one of many statements equivalent to the axiom of choice over ZF (Zermelo–Fraenkel set theory without the axiom of choice). The principle is also called the Hausdorff maximality theorem or the Kuratowski lemma (Kelley 1955:33).

The Hausdorff maximal principle states that, in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset. Here a maximal totally ordered subset is one that, if enlarged in any way, does not remain totally ordered. The maximal set produced by the principle is not unique, in general; there may be many maximal totally ordered subsets containing a given totally ordered subset.

An equivalent form of the principle is that in every partially ordered set there exists a maximal totally ordered subset.

To prove that it follows from the original form, let "A" be a poset. Then formula_1 is a totally ordered subset of "A", hence there exists a maximal totally ordered subset containing formula_1, in particular "A" contains a maximal totally ordered subset.

For the converse direction, let "A" be a partially ordered set and "T" a totally ordered subset of "A". Then
is partially ordered by set inclusion formula_4, therefore it contains a maximal totally ordered subset "P". Then the set formula_5 satisfies the desired properties.

The proof that the Hausdorff maximal principle is equivalent to Zorn's lemma is very similar to this proof.

EXAMPLE 1. If "A" is any collection of sets, the relation "is a proper subset of" is a strict partial order on "A". Suppose that "A" is the collection of all circular regions (interiors of circles) in the plane. One maximal totally ordered sub-collection of "A" consists of all circular regions with centers at the origin. Another maximal totally ordered sub-collection consists of all circular regions bounded by circles tangent from the right to the y-axis at the origin.

EXAMPLE 2. If (x, y) and (x, y) are two points of the plane ℝ, define (x, y) < (x, y)

if y = y and x < x. This is a partial ordering of ℝ under which two points are comparable only if they lie on the same horizontal line. The maximal totally ordered sets are horizontal lines in ℝ.




</doc>
<doc id="13666" url="https://en.wikipedia.org/wiki?curid=13666" title="Hel (being)">
Hel (being)

In Norse mythology, Hel is a being who presides over a realm of the same name, where she receives a portion of the dead. Hel is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In addition, she is mentioned in poems recorded in "Heimskringla" and "Egils saga" that date from the 9th and 10th centuries, respectively. An episode in the Latin work "Gesta Danorum", written in the 12th century by Saxo Grammaticus, is generally considered to refer to Hel, and Hel may appear on various Migration Period bracteates.

In the "Poetic Edda", "Prose Edda", and "Heimskringla", Hel is referred to as a daughter of Loki. In the "Prose Edda" book "Gylfaginning", Hel is described as having been appointed by the god Odin as ruler of a realm of the same name, located in Niflheim. In the same source, her appearance is described as half blue and half flesh-coloured and further as having a gloomy, downcast appearance. The "Prose Edda" details that Hel rules over vast mansions with many servants in her underworld realm and plays a key role in the attempted resurrection of the god Baldr.

Scholarly theories have been proposed about Hel's potential connections to figures appearing in the 11th-century "Old English Gospel of Nicodemus" and Old Norse "Bartholomeus saga postola", that she may have been considered a goddess with potential Indo-European parallels in Bhavani, Kali, and Mahakali or that Hel may have become a being only as a late personification of the location of the same name.

The Old Norse feminine proper noun "Hel" is identical to the name of the location over which she rules, Old Norse "Hel". The word has cognates in all branches of the Germanic languages, including Old English "hell" (and thus Modern English "hell"), Old Frisian "helle", Old Saxon "hellia", Old High German "hella", and Gothic "halja". All forms ultimately derive from the reconstructed Proto-Germanic feminine noun *"xaljō" or *"haljō" ('concealed place, the underworld'). In turn, the Proto-Germanic form derives from the o-grade form of the Proto-Indo-European root *"kel-", *"kol"-: 'to cover, conceal, save'.

The term is etymologically related to Modern English "hall" and therefore also "Valhalla", an afterlife 'hall of the slain' in Norse Mythology. "Hall" and its numerous Germanic cognates derive from Proto-Germanic *"hallō" 'covered place, hall', from Proto-Indo-European *"kol-".

Related early Germanic terms and concepts include Proto-Germanic *"xalja-rūnō(n)", a feminine compound noun, and *"xalja-wītjan", a neutral compound noun. This form is reconstructed from the Latinized Gothic plural noun *"haliurunnae" (attested by Jordanes; according to philologist Vladimir Orel, meaning 'witches'), Old English "helle-rúne" ('sorceress, necromancer', according to Orel), and Old High German "helli-rūna" 'magic'. The compound is composed of two elements: *"xaljō" (*"haljō") and *"rūnō", the Proto-Germanic precursor to Modern English "rune". The second element in the Gothic "haliurunnae" may however instead be an agent noun from the verb "rinnan" ("to run, go"), which would make its literal meaning "one who travels to the netherworld".)

Proto-Germanic *"xalja-wītjan" (or *"halja-wītjan") is reconstructed from Old Norse "hel-víti" 'hell', Old English "helle-wíte" 'hell-torment, hell', Old Saxon "helli-wīti" 'hell', and the Middle High German feminine noun "helle-wīze". The compound is a compound of *"xaljō" (discussed above) and *"wītjan" (reconstructed from forms such as Old English "witt" 'right mind, wits', Old Saxon "gewit" 'understanding', and Gothic "un-witi" 'foolishness, understanding').

The "Poetic Edda", compiled in the 13th century from earlier traditional sources, features various poems that mention Hel. In the "Poetic Edda" poem "Völuspá", Hel's realm is referred to as the "Halls of Hel." In stanza 31 of "Grímnismál", Hel is listed as living beneath one of three roots growing from the world tree Yggdrasil. In "Fáfnismál", the hero Sigurd stands before the mortally wounded body of the dragon Fáfnir, and states that Fáfnir lies in pieces, where "Hel can take" him. In "Atlamál", the phrases "Hel has half of us" and "sent off to Hel" are used in reference to death, though it could be a reference to the location and not the being, if not both. In stanza 4 of "Baldrs draumar", Odin rides towards the "high hall of Hel."

Hel may also be alluded to in "Hamðismál". Death is periphrased as "joy of the troll-woman" (or "ogress") and ostensibly it is Hel being referred to as the troll-woman or the ogre ("flagð"), although it may otherwise be some unspecified "dís".

Hel is referred to in the "Prose Edda", written in the 13th century by Snorri Sturluson. In chapter 34 of the book "Gylfaginning", Hel is listed by High as one of the three children of Loki and Angrboða; the wolf Fenrir, the serpent Jörmungandr, and Hel. High continues that, once the gods found that these three children are being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" then the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.

High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands," Odin threw Hel into Niflheim, and bestowed upon her authority over nine worlds, in that she must "administer board and lodging to those sent to her, and that is those who die of sickness or old age." High details that in this realm Hel has "great Mansions" with extremely high walls and immense gates, a hall called Éljúðnir, a dish called "Hunger," a knife called "Famine," the servant Ganglati (Old Norse "lazy walker"), the serving-maid Ganglöt (also "lazy walker"), the entrance threshold "Stumbling-block," the bed "Sick-bed," and the curtains "Gleaming-bale." High describes Hel as "half black and half flesh-coloured," adding that this makes her easily recognizable, and furthermore that Hel is "rather downcast and fierce-looking."

In chapter 49, High describes the events surrounding the death of the god Baldr. The goddess Frigg asks who among the Æsir will earn "all her love and favour" by riding to Hel, the location, to try to find Baldr, and offer Hel herself a ransom. The god Hermóðr volunteers and sets off upon the eight-legged horse Sleipnir to Hel. Hermóðr arrives in Hel's hall, finds his brother Baldr there, and stays the night. The next morning, Hermóðr begs Hel to allow Baldr to ride home with him, and tells her about the great weeping the Æsir have done upon Baldr's death. Hel says the love people have for Baldr that Hermóðr has claimed must be tested, stating:

If all things in the world, alive or dead, weep for him, then he will be allowed to return to the Æsir. If anyone speaks against him or refuses to cry, then he will remain with Hel.
Later in the chapter, after the female jötunn Þökk refuses to weep for the dead Baldr, she responds in verse, ending with "let Hel hold what she has." In chapter 51, High describes the events of Ragnarök, and details that when Loki arrives at the field Vígríðr "all of Hel's people" will arrive with him.

In chapter 5 of the "Prose Edda" book "Skáldskaparmál", Hel is mentioned in a kenning for Baldr ("Hel's companion"). In chapter 16, "Hel's [...] relative or father" is given as a kenning for Loki. In chapter 50, Hel is referenced ("to join the company of the quite monstrous wolf's sister") in the skaldic poem "Ragnarsdrápa".

In the "Heimskringla" book "Ynglinga saga", written in the 13th century by Snorri Sturluson, Hel is referred to, though never by name. In chapter 17, the king Dyggvi dies of sickness. A poem from the 9th-century "Ynglingatal" that forms the basis of "Ynglinga saga" is then quoted that describes Hel's taking of Dyggvi:

In chapter 45, a section from "Ynglingatal" is given which refers to Hel as "howes'-warder" (meaning "guardian of the graves") and as taking King Halfdan Hvitbeinn from life. In chapter 46, King Eystein Halfdansson dies by being knocked overboard by a sail yard. A section from "Ynglingatal" follows, describing that Eystein "fared to" Hel (referred to as "Býleistr's-brother's-daughter"). In chapter 47, the deceased Eystein's son King Halfdan dies of an illness, and the excerpt provided in the chapter describes his fate thereafter, a portion of which references Hel:

In a stanza from "Ynglingatal" recorded in chapter 72 of the "Heimskringla" book "Saga of Harald Sigurdsson", "given to Hel" is again used as a phrase to referring to death.

The Icelanders' saga "Egils saga" contains the poem "Sonatorrek". The saga attributes the poem to 10th century skald Egill Skallagrímsson, and writes that it was composed by Egill after the death of his son Gunnar. The final stanza of the poem contains a mention of Hel, though not by name:

In the account of Baldr's death in Saxo Grammaticus' early 13th century work "Gesta Danorum", the dying Baldr has a dream visitation from Proserpina (here translated as "the goddess of death"):

The following night the goddess of death appeared to him in a dream standing at his side, and declared that in three days time she would clasp him in her arms. It was no idle vision, for after three days the acute pain of his injury brought his end.

Scholars have assumed that Saxo used Proserpina as a goddess equivalent to the Norse Hel.

It has been suggested that several imitation medallions and bracteates of the Migration Period (ca. first centuries AD) feature depictions of Hel. In particular the bracteates IK 14 and IK 124 depict a rider traveling down a slope and coming upon a female being holding a scepter or a staff. The downward slope may indicate that the rider is traveling towards the realm of the dead and the woman with the scepter may be a female ruler of that realm, corresponding to Hel.

Some B-class bracteates showing three godly figures have been interpreted as depicting Baldr's death, the best known of these is the Fakse bracteate. Two of the figures are understood to be Baldr and Odin while both Loki and Hel have been proposed as candidates for the third figure. If it is Hel she is presumably greeting the dying Baldr as he comes to her realm.

The "Old English Gospel of Nicodemus", preserved in two manuscripts from the 11th century, contains a female figure referred to as "Seo hell" who engages in flyting with Satan and tells him to leave her dwelling (Old English "ut of mynre onwununge"). Regarding Seo Hell in the "Old English Gospel of Nicodemus", Michael Bell states that "her vivid personification in a dramatically excellent scene suggests that her gender is more than grammatical, and invites comparison with the Old Norse underworld goddess Hel and the Frau Holle of German folklore, to say nothing of underworld goddesses in other cultures" yet adds that "the possibility that these genders "are" merely grammatical is strengthened by the fact that an Old Norse version of Nicodemus, possibly translated under English influence, personifies Hell in the neutral (Old Norse "þat helvíti")."

The Old Norse "Bartholomeus saga postola", an account of the life of Saint Bartholomew dating from the 13th century, mentions a "Queen Hel." In the story, a devil is hiding within a pagan idol, and bound by Bartholomew's spiritual powers to acknowledge himself and confess, the devil refers to Jesus as the one which "made war on Hel our queen" (Old Norse "heriaði a Hel drottning vara"). "Queen Hel" is not mentioned elsewhere in the saga.

Michael Bell says that while Hel "might at first appear to be identical with the well-known pagan goddess of the Norse underworld" as described in chapter 34 of "Gylfaginning", "in the combined light of the Old English and Old Norse versions of "Nicodemus" she casts quite a different a shadow," and that in "Bartholomeus saga postola" "she is clearly the queen of the Christian, not pagan, underworld."

Jacob Grimm theorized that Hel (whom he refers to here as "Halja", the theorized Proto-Germanic form of the term) is essentially an "image of a greedy, unrestoring, female deity" and that "the higher we are allowed to penetrate into our antiquities, the less hellish and more godlike may "Halja" appear. Of this we have a particularly strong guarantee in her affinity to the Indian Bhavani, who travels about and bathes like Nerthus and Holda, but is likewise called "Kali" or "Mahakali", the great "black" goddess. In the underworld she is supposed to sit in judgment on souls. This office, the similar name and the black hue [...] make her exceedingly like "Halja". And "Halja" is one of the oldest and commonest conceptions of our heathenism."

Grimm theorizes that the Helhest, a three legged-horse that roams the countryside "as a harbinger of plague and pestilence" in Danish folklore, was originally the steed of the goddess Hel, and that on this steed Hel roamed the land "picking up the dead that were her due." In addition, Grimm says that a wagon was once ascribed to Hel, with which Hel made journeys. Grimm says that Hel is an example of a "half-goddess;" "one who cannot be shown to be either wife or daughter of a god, and who stands in a dependent relation to higher divinities" and that "half-goddesses" stand higher than "half-gods" in Germanic mythology.

Hilda Ellis Davidson (1948) states that Hel "as a goddess" in surviving sources seems to belong to a genre of literary personification, that the word "hel" is generally "used simply to signify death or the grave," and that the word often appears as the equivalent to the English 'death,' which Davidson states "naturally lends itself to personification by poets." Davidson explains that "whether this personification has originally been based on a belief in a goddess of death called Hel is another question," but that she does not believe that the surviving sources give any reason to believe so. Davidson adds that, on the other hand, various other examples of "certain supernatural women" connected with death are to be found in sources for Norse mythology, that they "seem to have been closely connected with the world of death, and were pictured as welcoming dead warriors," and that the depiction of Hel "as a goddess" in "Gylfaginning" "might well owe something to these."

In a later work (1998), Davidson states that the description of Hel found in chapter 33 of "Gylfaginning" "hardly suggests a goddess." Davidson adds that "yet this is not the impression given in the account of Hermod's ride to Hel later in "Gylfaginning" (49)" and points out that here Hel "[speaks] with authority as ruler of the underworld" and that from her realm "gifts are sent back to Frigg and Fulla by Balder's wife Nanna as from a friendly kingdom." Davidson posits that Snorri may have "earlier turned the goddess of death into an allegorical figure, just as he made Hel, the underworld of shades, a place 'where wicked men go,' like the Christian Hell ("Gylfaginning" 3)." Davidson continues that:

On the other hand, a goddess of death who represents the horrors of slaughter and decay is something well known elsewhere; the figure of Kali in India is an outstanding example. Like Snorri's Hel, she is terrifying to in appearance, black or dark in colour, usually naked, adorned with severed heads or arms or the corpses of children, her lips smeared with blood. She haunts the battlefield or cremation ground and squats on corpses. Yet for all this she is "the recipient of ardent devotion from countless devotees who approach her as their mother" [...].

Davidson further compares to early attestations of the Irish goddesses Badb (Davidson points to the description of Badb from "The Destruction of Da Choca's Hostel" where Badb is wearing a dusky mantle, has a large mouth, is dark in color, and has gray hair falling over her shoulders, or, alternatively, "as a red figure on the edge of the ford, washing the chariot of a king doomed to die") and The Morrígan. Davidson concludes that, in these examples, "here we have the fierce destructive side of death, with a strong emphasis on its physical horrors, so perhaps we should not assume that the gruesome figure of Hel is wholly Snorri's literary invention."

John Lindow states that most details about Hel, as a figure, are not found outside of Snorri's writing in "Gylfaginning", and says that when older skaldic poetry "says that people are 'in' rather than 'with' Hel, we are clearly dealing with a place rather than a person, and this is assumed to be the older conception," that the noun and place "Hel" likely originally simply meant "grave," and that "the personification came later." He also draws a parallel between the personified Hel's banishment to the underworld and the binding of Fenrir as part of a recurring theme of the bound monster, where an enemy of the gods is bound but destined to break free at Ragnarok. Rudolf Simek theorizes that the figure of Hel is "probably a very late personification of the underworld Hel," and says that "the first scriptures using the goddess Hel are found at the end of the 10th and in the 11th centuries." Simek states that the allegorical description of Hel's house in "Gylfaginning" "clearly stands in the Christian tradition," and that "on the whole nothing speaks in favour of there being a belief in Hel in pre-Christian times." However, Simek also cites Hel as possibly appearing as one of three figures appearing together on Migration Period B-bracteates.

In January 2017, the Icelandic Naming Committee ruled that parents could not name their child "Hel" "on the grounds that the name would cause the child significant distress and trouble as it grows up". 





</doc>
<doc id="13667" url="https://en.wikipedia.org/wiki?curid=13667" title="Hawar Islands">
Hawar Islands

The Hawar Islands (; transliterated: Juzur Ḩawār) are an archipelago of desert islands owned by Bahrain, situated off the west coast of Qatar in the Gulf of Bahrain of the Persian Gulf.

The islands used to be one of the settlements of the Bahraini branch of the Dawasir who settled there in the early 19th century. The islands were first surveyed in 1820, when they were called the Warden’s Islands, and two villages were recorded. They are now uninhabited, other than a police garrison and a hotel on the main island; access to all but Hawar island itself is severely restricted. Local fishermen are allowed to fish in adjacent waters and there is some recreational fishing and tourism on and around the islands. Fresh water has always been scarce; historically it was obtained by surface collection and even today, with the desalinisation plant, additional supplies have to be brought in.

Despite their proximity to Qatar (they are only about from the Qatari mainland whilst being about from the main islands of Bahrain), most of the islands belong to Bahrain, having been a part of a dispute between Bahrain and Qatar which was resolved in 2001. The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain. The land area of the islands is approximately 52 km.

Although there are 36 islands in the group, many of the smaller islands are little more than sand or shingle accumulations on areas of exposed bedrock molded by the ongoing processes of sedimentation and accretion.
The application named 8 major islands (see table hereafter), which conforms to the description of the islands when first surveyed as consisting of 8 or 9 islands. It has often been described as an archipelago of 16 islands. Janan Island, to the south of Hawar island, is not legally considered to be a part of the group and is owned by Qatar.

The islands are home to many bird species, notably Socotra cormorants. There are small herds of Arabian oryx and sand gazelle on Hawar island, and the seas around support a large population of dugong.

The islands were listed as a Ramsar site in 1997. In 2002, the Bahraini government applied to have the islands recognised as a World Heritage Site due to their unique environment and habitat for endangered species; the application was ultimately unsuccessful.

The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain.

In 2014, a Best Western hotel with 140 rooms replaced a much smaller Hawar Islands Resort. However, the resort closed in mid-2016.

By far the largest island is Hawar, which accounts for more than 41 km of the 54.5 km land area. Following in size are Suwād al Janūbīyah, Suwād ash Shamālīyah, Rubud Al Sharqiyah, Rubud Al Gharbiyah, and Muhazwarah (Umm Hazwarah).

The following were not considered as part of the Hawar islands in the International Court of Justice (ICJ) judgment, being located between Hawar and the Bahrain Islands and not disputed by Qatar, but have been included in the Hawar archipelago by the Bahrain government as part of the 2002 World Heritage Site application.

Janan Island, a small island south of Hawar island, was also considered in the 2001 judgment. Based on a previous agreement when both Qatar and Bahrain were under British protection, it was judged to be separate from the Hawar islands and so considered by the court separately. It was awarded to Qatar.





</doc>
<doc id="13669" url="https://en.wikipedia.org/wiki?curid=13669" title="Hans-Dietrich Genscher">
Hans-Dietrich Genscher

Hans-Dietrich Genscher (21 March 1927 – 31 March 2016) was a German statesman and a member of the liberal Free Democratic Party (FDP), who served as the Federal Minister of the Interior from 1969 to 1974, and as the Federal Minister of Foreign Affairs and Vice Chancellor of Germany from 1974 to 1992 (except for a two-week break in 1982, after the FDP had left the Third Schmidt cabinet), making him the longest-serving occupant of either post and the only person, holding one of these posts under two different Chancellors of the Federal Republic of Germany. In 1991 he was chairman of the Organization for Security and Co-operation in Europe (OSCE).

A proponent of Realpolitik, Genscher has been called "a master of diplomacy." He is widely regarded as having been a principal "architect of German reunification." In 1991, he played a pivotal role in the breakup of Yugoslavia by successfully pushing for international recognition of Croatia, Slovenia and other republics declaring independence, in an effort to halt "a trend towards a Greater Serbia." After leaving office, he worked as a lawyer and international consultant. He was President of the German Council on Foreign Relations and was involved with several international organisations, and with former Czech President Václav Havel, he called for a Cold War museum to be built in Berlin.

Genscher was born on 21 March 1927 in Reideburg (Province of Saxony), now a part of Halle, in what later became East Germany. He was the son of Hilda Kreime and Kurt Genscher. His father, a lawyer, died when Genscher was nine years old. In 1943, he was drafted to serve as a member of the Air Force Support Personnel ("Luftwaffenhelfer") at the age of 16. At age 17, close to the end of the war, he and his fellow soldiers became members of the Nazi Party due to a collective application ("Sammelantrag") by his Wehrmacht unit. He later said he was unaware of it at the time.
Late in the war, Genscher was deployed as a soldier in General Walther Wenck's 12th Army, which ostensibly was directed to relieve the siege of Berlin. After the German surrender he was an American and British prisoner of war, but was released after two months. Following World War II, he studied law and economics at the universities of Halle and Leipzig (1946–1949) and joined the East German Liberal Democratic Party (LDPD) in 1946.

In 1952, Genscher fled to West Germany, where he joined the Free Democratic Party (FDP). He passed his second state examination in law in Hamburg in 1954 and became a solicitor in Bremen. During these early years after the war, Genscher continuously struggled with illness. From 1956 to 1959 he was a research assistant of the FDP parliamentary group in Bonn. From 1959 to 1965 he was the FDP group managing director, while from 1962 to 1964 he was National Secretary of the FDP.

In 1965 Genscher was elected on the North Rhine-Westphalian FDP list to the West German parliament and remained a member of parliament until his retirement in 1998. He was elected deputy national chairman in 1968. From 1969 he served as minister of the interior in the SPD-FDP coalition government led by Chancellor Willy Brandt.

In 1974 he became foreign minister and vice chancellor, both posts he would hold for 18 years. From 1 October 1974 to 23 February 1985 he was Chairman of the FDP. It was during his tenure as party chairman that the FDP switched from being the junior member of social-liberal coalition to being the junior member of the 1982 coalition with the CDU/CSU. In 1985 he gave up the post of national chairman. After his resignation as Foreign Minister, Genscher was appointed honorary chairman of the FDP in 1992.

After the federal election of 1969 Genscher was instrumental in the formation of the social-liberal coalition of chancellor Willy Brandt and was on 22 October 1969 appointed as federal minister of the interior. 
In 1972, while minister for the interior, Genscher rejected Israel's offer to send an Israeli special forces unit to Germany to deal with the Munich Olympics hostage crisis. A flawed rescue attempt by German police forces at Fürstenfeldbruck air base resulted in a bloody shootout, which left all eleven hostages, five terrorists, and one German policeman dead. Genscher's popularity with Israel declined further when he endorsed the release of the three captured attackers following the hijacking of a Lufthansa aircraft on 29 October 1972.

In the SPD–FDP coalition, Genscher helped shape Brandt's policy of deescalation with the communist East, commonly known as "Ostpolitik", which was continued under chancellor Helmut Schmidt after Brandt's resignation in 1974. He would later be a driving factor in continuing this policy in the new conservative-liberal coalition under Helmut Kohl.

In the negotiations on a coalition government of SPD and FDP following the 1976 elections, it took Genscher 73 days to reach agreement with Chancellor Helmut Schmidt.

As Foreign Minister, Genscher stood for a policy of compromise between East and West, and developed strategies for an active policy of détente and the continuation of the East-West dialogue with the USSR. He was widely regarded a strong advocate of negotiated settlements to international problems. As a popular story on Genscher's preferred method of shuttle diplomacy has it, "two Lufthansa jets crossed over the Atlantic, and Genscher was on both."

Genscher was a major player in the negotiations on the text of the Helsinki Accords. In December 1976, the General Assembly of the United Nations in New York City accepted Genscher's proposal of an anti-terrorism convention in New York, which was set among other things, to respond to demands from hostage-takers under any circumstances.

Genscher was one of the FDP's driving forces when, in 1982, the party switched sides from its coalition with the SPD to support the CDU/CSU in their Constructive vote of no confidence to have incumbent Helmut Schmidt replaced with opposition leader Helmut Kohl as Chancellor. The reason for this was the increase in the differences between the coalition partners, particularly in economic and social policy. The switch was controversial, not least in his own party.

At several points in his tenure, he irritated the governments of the United States and other allies of Germany by appearing not to support Western initiatives fully. "During the Cold War, his penchant to seek the middle ground at times exasperated United States policy-makers who wanted a more decisive, less equivocal Germany," according to Tyler Marshall. Genscher's perceived quasi-neutralism was dubbed "Genscherism". "Fundamental to "Genscherism" was said to be the belief that Germany could play a role as a bridge between East and West without losing its status as a reliable NATO ally." In the 1980s, Genscher opposed the deployment of new short-range NATO missiles in Germany. At the time, the Reagan Administration questioned whether Germany was straying from the Western alliance and following a program of its own.

In 1984, Genscher became the first Western foreign minister to visit Tehran since the Iranian Revolution of 1979. In 1988, he appointed Jürgen Hellner as West Germany's new ambassador to Libya, a post that had been vacant since the 1986 Berlin discotheque bombing, a tragedy which U.S. officials blamed on the government of Muammar Gaddafi.

Genscher's proposals frequently set the tone and direction of foreign affairs among Western Europe's democracies. He was also an active participant in the further development of the European Union, taking an active part in the Single European Act Treaty negotiations in the mid-1980s, as well as the joint publication of the Genscher-Colombo plan with Italian Minister of Foreign Affairs Emilio Colombo which advocated further integration and deepening of relations in the European Union towards a more federal Europe. He later was among the politicians who pushed hard for monetary union alongside Edouard Balladur, France's finance minister, and Giuliano Amato, circulating a memorandum to that effect.

Genscher retained his posts as foreign minister and vice chancellor through German reunification and until 1992 when he stepped down for health reasons.

Genscher is most respected for his efforts that helped spell the end of the Cold War, in the late 1980s when Communist eastern European governments toppled, and which led to German reunification. During his time in office, he focused on maintaining stability and balance between the West and the Soviet bloc. From the beginning, he argued that the West should seek cooperation with Communist governments rather than treat them as implacably hostile; this policy was embraced by many Germans and other Europeans.

Genscher had great interest in European integration and the success of German reunification. He soon pushed for effective support of political reform processes in Poland and Hungary. For this purpose, he visited Poland to meet the chairman of Solidarity Lech Wałęsa as early as January 1980. Especially from 1987 he campaigned for an "active relaxation" policy response by the West to the Soviet efforts. In the years before German reunification, he made a point of maintaining strong ties with his birthplace Halle, which was regarded as significant by admirers and critics alike.

When thousands of East Germans sought refuge in West German embassies in Czechoslovakia and Poland, Genscher held discussions on the refugee crisis at the United Nations in New York with the foreign ministers of Czechoslovakia, Poland, East Germany and the Soviet Union in September 1989. Genscher's 30 September 1989 speech from the balcony of the German embassy in Prague was an important milestone on the road to the end of the GDR. In the embassy courtyard thousands of East German citizens had assembled. They were trying to travel to West Germany, but were being denied permission to travel by the Czechoslovak government at the request of East Germany. He announced that he had reached an agreement with the Communist Czechoslovak government that the refugees could leave: "We have come to you to tell you that today, your departure ..." (German: "Wir sind zu Ihnen gekommen, um Ihnen mitzuteilen, dass heute Ihre Ausreise ..."). After these words, the speech was drowned in cheers.

With his fellow foreign ministers James Baker of the United States and Eduard Shevardnadze of the Soviet Union, Genscher is widely credited with securing Germany's subsequent peaceful unification and the withdrawal of Soviet forces. He negotiated the German reunification in 1990 with his counterpart from the GDR, Markus Meckel. On 12 September 1990 he signed the Treaty on the Final Settlement with Respect to Germany on behalf of West Germany. In November 1990, Genscher and his Polish counterpart Krzysztof Skubiszewski signed the German-Polish Border Treaty on the establishment of the Oder–Neisse line as Poland's western border. Meanwhile, he strongly endorsed the plans of the Bush Administration to assure continued U.S. influence in a post-Cold War Europe.

In 1991, Genscher successfully pushed for Germany's recognition of the Republic of Croatia in the Croatian War of Independence shortly after JNA entered Vukovar. After Croatia and Slovenia had declared independence, Genscher concluded that Yugoslavia could not be held together, and that republics that wanted to break from the Serbian-dominated federation deserved quick diplomatic recognition. He hoped that such recognition would stop the fighting. The rest of the European Union was subsequently pressured to follow suit soon afterward. The UN Secretary-General Javier Pérez de Cuéllar had warned the German Government, that a recognition of Slovenia and Croatia would lead to an increase in aggression in the former Yugoslavia.

At a meeting of the European Community's foreign ministers in 1991, Genscher proposed to press for a war crimes trial for President Saddam Hussein of Iraq, accusing him of aggression against Kuwait, using chemical weapons against civilians and condoning genocide against the Kurds.

During the Gulf War, Genscher sought to deal with Iraq after other Western leaders had decided to go to war to force it out of Kuwait. Germany made a substantial financial contribution to the allied cause but, citing constitutional restrictions on the use of its armed forces, provided almost no military assistance. In January 1991, Germany sent Genscher on a state visit to Israel and followed up with an agreement to provide the Jewish state with $670 million in military aid, including financing for two submarines long coveted by Israel, a battery of Patriot missiles to defend against Iraqi missiles, 58 armored vehicles specially fitted to detect chemical and biological attacks, and a shipment of gas masks. When, in the aftermath of the war, a far-reaching political debate broke out over how Germany should fulfill its global responsibilities, Genscher responded that if foreign powers expect Germany to assume greater responsibility in the world, they should give it a chance to express its views "more strongly" in the United Nations Security Council. He also famously held that "whatever floats is fine, whatever rolls is not" to sum up Germany's military export policy for restless countries – based on a navy's unsuitability for use against a country's own people.

In 1992, Genscher, together with his Danish colleague Uffe Ellemann-Jensen, took the initiative to create the Council of the Baltic Sea States (CBSS) and the EuroFaculty.

More than half a century after Nazi leaders assembled their infamous exhibition "Degenerate Art," a sweeping condemnation of the work of the avant-garde, Genscher opened a re-creation of the show at the Altes Museum in March 1992, describing Nazi attempts to restrict artistic expression as "a step toward the catastrophe that produced the mass murder of European Jews and the war of extermination against Germany's neighbors." "The paintings in this exhibition have survived oppression and censorship," he asserted in his opening remarks. "They are not only a monument but also a sign of hope. They stand for the triumph of creative freedom over barbarism."

On 18 May 1992 Genscher retired at his own request from the federal government, which he had been member of for a total of 23 years. At the time, he was the world's longest-serving foreign minister and Germany's most popular politician. He had announced his decision three weeks earlier, on 27 April 1992. At that time he was Europe's longest-serving foreign minister. Genscher did not specify his reasons for quitting; however, he had suffered two heart attacks by that time. His resignation took effect in May, but he remained a member of parliament and continued to be influential in the Free Democratic Party.

Following Genscher's resignation, Chancellor Helmut Kohl and FDP chairman Otto Graf Lambsdorff named Irmgard Schwaetzer, a former aide to Genscher, to be the new Foreign Minister. In a surprise decision, however, a majority of the FDP parliamentary group rejected her nomination and voted instead to name Justice Minister Klaus Kinkel to head the Foreign Ministry.

Ahead of the German presidential election in 1994, Genscher proclaimed his lack of interest in the position, but was nonetheless widely considered a leading contender. After a poll taken for "Stern" magazine showed him to be the favored candidate of 48 percent of German voters, he reiterated in 1993 that he would "in no case" accept the presidency.

Having finished his political career, Genscher remained active as a lawyer and in international organizations. In late 1992, Genscher was appointed chairman of a newly established donors' board of the Berlin State Opera. Between 1997 and 2010, Genscher was affiliated with the law firm Büsing, Müffelmann & Theye. He founded his own consulting firm, Hans-Dietrich Genscher Consult GmbH, in 2000. Between 2001 and 2003, he served as president of the German Council on Foreign Relations. In 2001, Genscher headed an arbitration that ended a monthlong battle between German airline Lufthansa and its pilots' union and resulted in an agreement on increasing wages by more than 15 percent by the end of the following year.

In 2008, Genscher joined former Czech President Václav Havel, former United States Ambassador to Germany John Kornblum and several other well-known political figures in calling for a Cold War museum to be built at Checkpoint Charlie in Berlin. In 2009 Genscher expressed public concern at Pope Benedict XVI's lifting of excommunication of the bishops of the Society of Saint Pius X. Genscher wrote in the "Mitteldeutsche Zeitung": "Poles can be proud of Pope John Paul II. At the last papal election, we said We are the pope! But please—not like this." He argued that Pope Benedict XVI was making a habit of offending non-Catholics. "This is a deep moral and political question. It is about respect for the victims of crimes against humanity", Genscher said.

On 20 December 2013, it was revealed that Genscher played a key role in coordinating the release and flight to Germany of Mikhail Khodorkovsky, the former head of Yukos. Genscher had first met Khodorkovsky in 2002 and had chaired a conference at which Khodorkovsky blasted Russian President Vladimir Putin's pursuit of his oil company. Khodorkovsky asked his lawyers during a 2011 prison visit to let Genscher help mediate early release. Once Putin was re-elected in 2012, German Chancellor Angela Merkel instructed her officials to lobby for the president to meet Genscher. The subsequent negotiations involved two meetings between Genscher and Putin — one at Berlin Tegel Airport at the end of Putin's first visit to Germany after he was re-elected in 2012, the other in Moscow. While keeping the chancellor informed, Khodorkovsky's attorneys and Genscher spent the ensuing months developing a variety of legal avenues that could allow Putin to release his former rival early, ranging from amendments to existing laws to clemency. When Khodorkovsky's mother was in a Berlin hospital with cancer in November 2013, Genscher passed a message to Khodorkovsky suggesting the prisoner should write a pardon letter to Putin emphasizing his mother's ill health. Following Putin's pardoning of Khodorkovsky "for humanitarian reasons" in December 2013, a private plane provided by Genscher brought Khodorkovsky to Berlin for a family reunion at the Hotel Adlon.

Genscher signed on in 2014 to be a member of the Southern Corridor Advisory Panel, a BP-led consortium which includes former British Prime Minister Tony Blair and Peter Sutherland, chairman of Goldman Sachs International. The panel's purpose is to facilitate the expansion of a vast natural-gas field in the Caspian Sea and the building of two pipelines across Europe. The $45 billion enterprise, championed by the Azerbaijani president, Ilham Aliyev, has been called by critics "the Blair Rich Project."

Genscher died at his home outside Bonn in Wachtberg on 31 March 2016 from heart failure, one week and three days after his 89th birthday.


Genscher has been awarded honorary citizenship by his birthplace Halle (Saale) (in 1991) and the city of Berlin (in 1993).







 


</doc>
<doc id="13675" url="https://en.wikipedia.org/wiki?curid=13675" title="Henry Ainsworth">
Henry Ainsworth

Henry Ainsworth (1571–1622) was an English Nonconformist clergyman and scholar. He led the Ancient Church, a Brownist or English Separatist congregation in Amsterdam alongside Francis Johnson from 1597, and after their split led his own congregation. His translations of and commentaries on the Hebrew scriptures were influential for centuries.

Ainsworth was born of a farming family of Swanton Morley, Norfolk. He was educated at St John's College, Cambridge, later moving to Caius College, but left without a degree. After associating with the Puritan party in the Church, he joined the Brownists, but submitted to the Church of England after being arrested in London, and again when he was arrested in Ireland.

By 1597, Ainsworth moved to Amsterdam and found a home in "a blind lane at Amsterdam", working as porter to a bookseller, and lived in severe poverty. According to Roger Williams, Ainsworth ‘lived on 9d a week with roots boiled’. When the pastor Francis Johnson came to the church from London, wher he had been in prison, Ainsworth was elected as teacher (or doctor), thanks to his knowledge of Hebrew.

Ainsworth attempted to arbitrate the quarrel between Francis and Thomasine Johnson on the one side and his brother George Johnson on the other, where George accused Thomasine of dressing immodestly and Francis of ruling the church tyrannically. Though he may initially have sympathised with George, on 15 January 1598, Ainsworth chaired a church meeting which censured him. Francis and Ainsworth also ex-communicated their elder Matthew Slade for refusing to stop going to services in the Dutch Reformed Church. Ainsworth himself caused some scandal when it emerged that he had twice submitted to the Church of England, but he was not disciplined.

Though often involved in controversy, Ainsworth was not arrogant, but was a steadfast and cultured champion of the principles represented by the early Congregationalists. Amid all the controversy, he steadily pursued his studies. The combination was so unique that some have mistaken him for two different individuals. Confusion has also been occasioned through his friendly controversy with one John Ainsworth, who left the Anglican for the Roman Catholic church.

In 1604, Johnson and Ainsworth wrote a petition for toleration of their church and took it to England in the hope of delivering it to James I. In their attempts to get it to the king, they rewrote it twice, and on their return to Amsterdam published all three versions under the title "An Apologie or Defence of svch trve Christians as are commonly (vnjustly) called Brovvnists." 

In 1610, Johnson changed his mind about the democratic Congregational structure of the Ancient Church, arguing that authority lay with the ministers, not the people. After nearly a year of debate, on 15 December, Ainsworth and his followers split from Johnson, and successfully sued them for possession of the church building. John Robinson tried to mediate between the two factions, but ended up taking Ainsworth's side.

In 1620, after Johnson's church had departed for North America, but before Robinson's had left on the "Mayflower", Ainsworth's church considered joining the latter in their journey and put some money into the project. Robert Cushman criticised the proposal, saying 'Our liberty is to them as ratsbane, and their rigour as bad to us as the Spanish Inquisition.' Though nothing came of the plan, the Ainsworth church still waved the pilgrims off from Leiden.

On 29 April 1607, Ainsworth married Marjory Appelbey, a widow from Ipswich with one daughter. In 1612, the elder in the Ancient Church, Daniel Studley, was accused of ‘many lascivious attempts’ the girl, and confessed to having 'clapped' her. 

Henry Ainsworth died in 1622, leaving unfinished work on works on Hosea, Matthew and Hebrews. 

Ainsworth was one of the most able apologists of the so-called Brownist movement. His first solo work "The communion of saincts" (1607) is summarised by the historian of Separatism Stephen Tomkins as arguing 'that the true church is a holy community while a church that incorporates the entire population is neither holy nor a community'. Tomkins describes his second book "Covnterpoyson" (1608) as 'the most compelling apologia that the Separatist movement ever produced'. It was written in reply to the puritan minister John Sprint and to Richard Bernard's "The Separatist Schisme".

Ainsworth also wrote reply to John Smyth, who has been called "the first Baptist", entitled "Defence of Holy Scripture, Worship and Ministry used in the Christian Churches separated from Antichrist, against the Challenges, Cavils and Contradictions of Mr Smyth" (1609). Of Smyth's progression to becoming a Baptist, Ainsworth said he 'had gone ‘from error to error, and now at last to the abomination of Anabaptism’, which ‘in him was the worship … of the devil’.

His scholarly works include his "Annotations"—on "Genesis" (1616); "Exodus" (1617); "Leviticus" (1618); "Numbers" (1619); "Deuteronomy" (1619); "Psalms" (including a metrical version, 1612); and the "Song of Solomon" (1623). These were collected in folio in 1627. From the outset the "Annotations" took a commanding place, especially among continental scholars, establishing a scholarly tradition for English nonconformity. Tomkins notes that 'as late as 1866, W.S. Plumer’s commentary on Psalms cited Ainsworth as an authority more than a hundred times and the 1885 (English) Revised Version of the Bible drew on his work.' 

His publication of Psalms, "The Book of Psalmes: Englished both in Prose and Metre with Annotations" (Amsterdam, 1612), which includes thirty-nine separate monophonic psalm tunes, constituted the Ainsworth Psalter, the only book of music brought to New England in 1620 by the Pilgrim settlers. Although its content was later reworked into the Bay Psalm Book, it had an important influence on the early development of American psalmody. An early critic of the Brownists said that ‘by the uncouth and strange translation and metre used in them, the congregation was made a laughing stock’, while the 1885 "Dictionary of National Biography" said that Ainsworth ‘had not the faintest breath of poetical inspiration’.

Ainsworth died in 1622, or early in 1623, for in that year was published his "Seasonable Discourse, or a Censure upon a Dialogue of the Anabaptists", in which the editor speaks of him as a departed worthy.



</doc>
<doc id="13677" url="https://en.wikipedia.org/wiki?curid=13677" title="Hindus">
Hindus

Hindus () are persons who regard themselves as culturally, ethnically, or religiously adhering to aspects of Hinduism. Historically, the term has also been used as a geographical, cultural, and later religious identifier for people living in the Indian subcontinent. 

The historical meaning of the term "Hindu" has evolved with time. Starting with the Persian and Greek references to the land of the Indus in the 1st millennium BCE through the texts of the medieval era, the term Hindu implied a geographic, ethnic or cultural identifier for people living in the Indian subcontinent around or beyond the Sindhu (Indus) river. By the 16th century, the term began to refer to residents of the subcontinent who were not Turkic or Muslims.

The historical development of Hindu self-identity within the local South Asian population, in a religious or cultural sense, is unclear. Competing theories state that Hindu identity developed in the British colonial era, or that it may have developed post-8th century CE after the Islamic invasion and medieval Hindu-Muslim wars. A sense of Hindu identity and the term "Hindu" appears in some texts dated between the 13th and 18th century in Sanskrit and Bengali. The 14th- and 18th-century Indian poets such as Vidyapati, Kabir and Eknath used the phrase "Hindu dharma" (Hinduism) and contrasted it with "Turaka dharma" (Islam). The Christian friar Sebastiao Manrique used the term 'Hindu' in religious context in 1649. In the 18th century, the European merchants and colonists began to refer to the followers of Indian religions collectively as "Hindus", in contrast to "Mohamedans" for Mughals and Arabs following Islam. By the mid-19th century, colonial orientalist texts further distinguished Hindus from Buddhists, Sikhs and Jains, but the colonial laws continued to consider all of them to be within the scope of the term "Hindu" until about mid-20th century. Scholars state that the custom of distinguishing between Hindus, Buddhists, Jains and Sikhs is a modern phenomenon. Hindoo is an archaic spelling variant, whose use today may be considered derogatory.
At more than 1.03 billion, Hindus are the world's third largest group after Christians and Muslims. The vast majority of Hindus, approximately 966 million, live in India, according to India's 2011 census. After India, the next 9 countries with the largest Hindu populations are, in decreasing order: Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom and Myanmar. These together accounted for 99% of the world's Hindu population, and the remaining nations of the world together had about 6 million Hindus in 2010.

The word "Hindu" is derived from the Indo-Aryan and Sanskrit word "Sindhu", which means "a large body of water", covering "river, ocean". It was used as the name of the Indus River and also referred to its tributaries. The actual term " first occurs, states Gavin Flood, as "a Persian geographical term for the people who lived beyond the river Indus (Sanskrit: "Sindhu")", more specifically in the 6th-century BCE inscription of Darius I. The Punjab region, called Sapta Sindhu in the Vedas, is called "Hapta Hindu" in Zend Avesta. The 6th-century BCE inscription of Darius I mentions the province of "Hi[n]dush", referring to northwestern India. The people of India were referred to as "Hinduvān" (Hindus) and "hindavī" was used as the adjective for Indian in the 8th century text "Chachnama". The term 'Hindu' in these ancient records is an ethno-geographical term and did not refer to a religion. The Arabic equivalent "Al-Hind" likewise referred to the country of India.

Among the earliest known records of 'Hindu' with connotations of religion may be in the 7th-century CE Chinese text "Record of the Western Regions" by the Buddhist scholar Xuanzang. Xuanzang uses the transliterated term "In-tu" whose "connotation overflows in the religious" according to Arvind Sharma. While Xuanzang suggested that the term refers to the country named after the moon, another Buddhist scholar I-tsing contradicted the conclusion saying that "In-tu" was not a common name for the country.

Al-Biruni's 11th-century text "Tarikh Al-Hind", and the texts of the Delhi Sultanate period use the term 'Hindu', where it includes all non-Islamic people such as Buddhists, and retains the ambiguity of being "a region or a religion". The 'Hindu' community occurs as the amorphous 'Other' of the Muslim community in the court chronicles, according to Romila Thapar. Wilfred Cantwell Smith notes that 'Hindu' retained its geographical reference initially: 'Indian', 'indigenous, local', virtually 'native'. Slowly, the Indian groups themselves started using the term, differentiating themselves and their "traditional ways" from those of the invaders.
The text "Prithviraj Raso", by Chanda Baradai, about the 1192 CE defeat of Prithviraj Chauhan at the hands of Muhammad Ghori, is full of references to "Hindus" and "Turks", and at one stage, says "both the religions have drawn their curved swords;" however, the date of this text is unclear and considered by most scholars to be more recent. In Islamic literature, 'Abd al-Malik Isami's Persian work, "Futuhu's-salatin", composed in the Deccan in 1350, uses the word " to mean Indian in the ethno-geographical sense and the word " to mean 'Hindu' in the sense of a follower of the Hindu religion". The poet Vidyapati's poem "Kirtilata" contrasts the cultures of Hindus and Turks (Muslims) in a city and concludes "The Hindus and the Turks live close together; Each makes fun of the other's religion ("dhamme")." One of the earliest uses of word 'Hindu' in religious context in a European language (Spanish), was the publication in 1649 by Sebastiao Manrique.

Other prominent mentions of 'Hindu' include the epigraphical inscriptions from Andhra Pradesh kingdoms who battled military expansion of Muslim dynasties in the 14th century, where the word 'Hindu' partly implies a religious identity in contrast to 'Turks' or Islamic religious identity. The term "Hindu" was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts, including "Chaitanya Charitamrita" and "Chaitanya Bhagavata". These texts used it to contrast Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century "Chaitanya Charitamrita" text and the 17th-century "Bhakta Mala" text using the phrase "Hindu dharma".

One of the earliest but ambiguous uses of the word Hindu is, states Arvind Sharma, in the 'Brahmanabad settlement' which Muhammad ibn Qasim made with non-Muslims after the Arab invasion of northwestern Sindh region of India, in 712 CE. The term 'Hindu' meant people who were non-Muslims, and it included Buddhists of the region. In the 11th-century text of Al Biruni, Hindus are referred to as "religious antagonists" to Islam, as those who believe in rebirth, presents them to hold a diversity of beliefs, and seems to oscillate between Hindus holding a centralist and pluralist religious views. In the texts of Delhi Sultanate era, states Sharma, the term Hindu remains ambiguous on whether it means people of a region or religion, giving the example of Ibn Battuta's explanation of the name "Hindu Kush" for a mountain range in Afghanistan. It was so called, wrote Ibn Battuta, because many Indian slaves died there of snow cold, as they were marched across that mountain range. The term "Hindu" there is ambivalent and could mean geographical region or religion.

The term Hindu appears in the texts from the Mughal Empire era. It broadly refers to non-Muslims. Pashaura Singh states, "in Persian writings, Sikhs were regarded as Hindu in the sense of non-Muslim Indians". Jahangir, for example, called the Sikh Guru Arjan a Hindu:

During the colonial era, the term Hindu had connotations of native religions of India, that is religions other than Christianity and Islam. In early colonial era Anglo-Hindu laws and British India court system, the term Hindu referred to people of all Indian religions as well as two non-Indian religions: Judaism and Zoroastrianism. In the 20th-century, personal laws were formulated for Hindus, and the term 'Hindu' in these colonial 'Hindu laws' applied to Buddhists, Jains and Sikhs in addition to denominational Hindus.

Beyond the stipulations of British law, colonial orientalists and particularly the influential Asiatick Researches founded in the 18th century, later called The Asiatic Society, initially identified just two religions in India – Islam, and Hinduism. These orientalists included all Indian religions such as Buddhism as a subgroup of Hinduism in the 18th century. These texts called followers of Islam as "Mohamedans", and all others as "Hindus". The text, by the early 19th century, began dividing Hindus into separate groups, for chronology studies of the various beliefs. Among the earliest terms to emerge were "Seeks and their College" (later spelled Sikhs by Charles Wilkins), "Boudhism" (later spelled Buddhism), and in the 9th volume of Asiatick Researches report on religions in India, the term "Jainism" received notice.

According to Pennington, the terms Hindu and Hinduism were thus constructed for colonial studies of India. The various sub-divisions and separation of subgroup terms were assumed to be result of "communal conflict", and Hindu was constructed by these orientalists to imply people who adhered to "ancient default oppressive religious substratum of India", states Pennington. Followers of other Indian religions so identified were later referred Buddhists, Sikhs or Jains and distinguished from Hindus, in an antagonistic two-dimensional manner, with Hindus and Hinduism stereotyped as irrational traditional and others as rational reform religions. However, these mid-19th-century reports offered no indication of doctrinal or ritual differences between Hindu and Buddhist, or other newly constructed religious identities. These colonial studies, states Pennigton, "puzzled endlessly about the Hindus and intensely scrutinized them, but did not interrogate and avoided reporting the practices and religion of Mughal and Arabs in South Asia", and often relied on Muslim scholars to characterise Hindus.

In contemporary era, the term Hindus are individuals who identify with one or more aspects of Hinduism, whether they are practising or non-practicing or "Laissez-faire". The term does not include those who identify with other Indian religions such as Buddhism, Jainism, Sikhism or various animist tribal religions found in India such as "Sarnaism". The term Hindu, in contemporary parlance, includes people who accept themselves as culturally or ethnically Hindu rather than with a fixed set of religious beliefs within Hinduism. One need not be religious in the minimal sense, states Julius Lipner, to be accepted as Hindu by Hindus, or to describe oneself as Hindu.

Hindus subscribe to a diversity of ideas on spirituality and traditions, but have no ecclesiastical order, no unquestionable religious authorities, no governing body, nor a single founding prophet; Hindus can choose to be polytheistic, pantheistic, monotheistic, monistic, agnostic, atheistic or humanist. Because of the wide range of traditions and ideas covered by the term Hinduism, arriving at a comprehensive definition is difficult. The religion "defies our desire to define and categorize it". A Hindu may, by his or her choice, draw upon ideas of other Indian or non-Indian religious thought as a resource, follow or evolve his or her personal beliefs, and still identify as a Hindu.

In 1995, Chief Justice P. B. Gajendragadkar was quoted in an Indian Supreme Court ruling:

Although Hinduism contains a broad range of philosophies, Hindus share philosophical concepts, such as but not limiting to dharma, karma, kama, artha, moksha and samsara, even if each subscribes to a diversity of views. Hindus also have shared texts such as the Vedas with embedded Upanishads, and common ritual grammar (Sanskara (rite of passage)) such as rituals during a wedding or when a baby is born or cremation rituals. Some Hindus go on pilgrimage to shared sites they consider spiritually significant, practice one or more forms of bhakti or puja, celebrate mythology and epics, major festivals, love and respect for guru and family, and other cultural traditions. A Hindu could:

In the Constitution of India, the word "Hindu" has been used in some places to denote persons professing any of these religions: Hinduism, Jainism, Buddhism or Sikhism. This however has been challenged by the Sikhs and by neo-Buddhists who were formerly Hindus. According to Sheen and Boyle, Jains have not objected to being covered by personal laws termed under 'Hindu', but Indian courts have acknowledged that Jainism is a distinct religion.

The Republic of India is in the peculiar situation that the Supreme Court of India has repeatedly been called upon to define "Hinduism" because the Constitution of India, while it prohibits "discrimination of any citizen" on grounds of religion in article 15, article 30 foresees special rights for "All minorities, whether based on religion or language". As a consequence, religious groups have an interest in being recognised as distinct from the Hindu majority in order to qualify as a "religious minority". Thus, the Supreme Court was forced to consider the question whether Jainism is part of Hinduism in 2005 and 2006.

Starting after the 10th century and particularly after the 12th century Islamic invasion, states Sheldon Pollock, the political response fused with the Indic religious culture and doctrines. Temples dedicated to deity Rama were built from north to south India, and textual records as well as hagiographic inscriptions began comparing the Hindu epic of Ramayana to regional kings and their response to Islamic attacks. The Yadava king of Devagiri named "Ramacandra", for example states Pollock, is described in a 13th-century record as, "How is this Rama to be described.. who freed Varanasi from the "mleccha" (barbarian, Turk Muslim) horde, and built there a golden temple of Sarngadhara". Pollock notes that the Yadava king "Ramacandra" is described as a devotee of deity Shiva (Shaivism), yet his political achievements and temple construction sponsorship in Varanasi, far from his kingdom's location in the Deccan region, is described in the historical records in Vaishnavism terms of Rama, a deity Vishnu avatar. Pollock presents many such examples and suggests an emerging Hindu political identity that was grounded in the Hindu religious text of Ramayana, one that has continued into the modern times, and suggests that this historic process began with the arrival of Islam in India.

Brajadulal Chattopadhyaya has questioned the Pollock theory and presented textual and inscriptional evidence. According to Chattopadhyaya, the Hindu identity and religious response to Islamic invasion and wars developed in different kingdoms, such as wars between Islamic Sultanates and the Vijayanagara kingdom (Karnataka), and Islamic raids on the kingdoms in Tamil Nadu. These wars were described not just using the mythical story of Rama from Ramayana, states Chattopadhyaya, the medieval records used a wide range of religious symbolism and myths that are now considered as part of Hindu literature. This emergence of religious with political terminology began with the first Muslim invasion of Sindh in the 8th century CE, and intensified 13th century onwards. The 14th-century Sanskrit text, "Madhuravijayam", a memoir written by "Gangadevi", the wife of Vijayanagara prince, for example describes the consequences of war using religious terms,

The historiographic writings in Telugu language from the 13th- and 14th-century Kakatiya dynasty period presents a similar "alien other (Turk)" and "self-identity (Hindu)" contrast. Chattopadhyaya, and other scholars, state that the military and political campaign during the medieval era wars in Deccan peninsula of India, and in the north India, were no longer a quest for sovereignty, they embodied a political and religious animosity against the "otherness of Islam", and this began the historical process of Hindu identity formation.

Andrew Nicholson, in his review of scholarship on Hindu identity history, states that the vernacular literature of Bhakti movement sants from 15th to 17th century, such as Kabir, Anantadas, Eknath, Vidyapati, suggests that distinct religious identities, between Hindus and Turks (Muslims), had formed during these centuries. The poetry of this period contrasts Hindu and Islamic identities, states Nicholson, and the literature vilifies the Muslims coupled with a "distinct sense of a Hindu religious identity".

Scholars state that Hindu, Buddhist and Jain identities are retrospectively-introduced modern constructions. Inscriptional evidence from the 8th century onwards, in regions such as South India, suggests that medieval era India, at both elite and folk religious practices level, likely had a "shared religious culture", and their collective identities were "multiple, layered and fuzzy". Even among Hinduism denominations such as Shaivism and Vaishnavism, the Hindu identities, states Leslie Orr, lacked "firm definitions and clear boundaries".

Overlaps in Jain-Hindu identities have included Jains worshipping Hindu deities, intermarriages between Jains and Hindus, and medieval era Jain temples featuring Hindu religious icons and sculpture. Beyond India, on Java island of Indonesia, historical records attest to marriages between Hindus and Buddhists, medieval era temple architecture and sculptures that simultaneously incorporate Hindu and Buddhist themes, where Hinduism and Buddhism merged and functioned as "two separate paths within one overall system", according to Ann Kenney and other scholars. Similarly, there is an organic relation of Sikhs to Hindus, states Zaehner, both in religious thought and their communities, and virtually all Sikhs' ancestors were Hindus. Marriages between Sikhs and Hindus, particularly among "Khatris", were frequent. Some Hindu families brought up a son as a Sikh, and some Hindus view Sikhism as a tradition within Hinduism, even though the Sikh faith is a distinct religion.

Julius Lipner states that the custom of distinguishing between Hindus, Buddhists, Jains, and Sikhs is a modern phenomena, but one that is a convenient abstraction. Distinguishing Indian traditions is a fairly recent practice, states Lipner, and is the result of "not only Western preconceptions about the nature of religion in general and of religion in India in particular, but also with the political awareness that has arisen in India" in its people and a result of Western influence during its colonial history.

Scholars such as Fleming and Eck state that the post-Epic era literature from the 1st millennium CE amply demonstrate that there was a historic concept of the Indian subcontinent as a sacred geography, where the sacredness was a shared set of religious ideas. For example, the twelve "Jyotirlingas" of Shaivism and fifty-one "Shaktipithas" of Shaktism are described in the early medieval era Puranas as pilgrimage sites around a theme. This sacred geography and Shaiva temples with same iconography, shared themes, motifs and embedded legends are found across India, from the Himalayas to hills of South India, from Ellora Caves to Varanasi by about the middle of 1st millennium. Shakti temples, dated to a few centuries later, are verifiable across the subcontinent. Varanasi as a sacred pilgrimage site is documented in the "Varanasimahatmya" text embedded inside the "Skanda Purana", and the oldest versions of this text are dated to 6th to 8th-century CE.

The idea of twelve sacred sites in Shiva Hindu tradition spread across the Indian subcontinent appears not only in the medieval era temples but also in copper plate inscriptions and temple seals discovered in different sites. According to Bhardwaj, non-Hindu texts such as the memoirs of Chinese Buddhist and Persian Muslim travellers attest to the existence and significance of the pilgrimage to sacred geography among Hindus by later 1st millennium CE.

According to Fleming, those who question whether the term Hindu and Hinduism are a modern construction in a religious context present their arguments based on some texts that have survived into the modern era, either of Islamic courts or of literature published by Western missionaries or colonial-era Indologists aiming for a reasonable construction of history. However, the existence of non-textual evidence such as cave temples separated by thousands of kilometers, as well as lists of medieval era pilgrimage sites, is evidence of a shared sacred geography and existence of a community that was self-aware of shared religious premises and landscape. Further, it is a norm in evolving cultures that there is a gap between the "lived and historical realities" of a religious tradition and the emergence of related "textual authorities". The tradition and temples likely existed well before the medieval era Hindu manuscripts appeared that describe them and the sacred geography. This, states Fleming, is apparent given the sophistication of the architecture and the sacred sites along with the variance in the versions of the Puranic literature. According to Diana L. Eck and other Indologists such as André Wink, Muslim invaders were aware of Hindu sacred geography such as Mathura, Ujjain, and Varanasi by the 11th-century. These sites became a target of their serial attacks in the centuries that followed.

The Hindus have been persecuted during the medieval and modern era. The medieval persecution included waves of plunder, killing, destruction of temples and enslavement by Turk-Mongol Muslim armies from central Asia. This is documented in Islamic literature such as those relating to 8th century Muhammad bin-Qasim, 11th century Mahmud of Ghazni, the Persian traveler Al Biruni, the 14th century Islamic army invasion led by Timur, and various Sunni Islamic rulers of the Delhi Sultanate and Mughal Empire. There were occasional exceptions such as Akbar who stopped the persecution of Hindus, and occasional severe persecution such as under Aurangzeb, who destroyed temples, forcibly converted non-Muslims to Islam and banned the celebration of Hindu festivals such as Holi and Diwali.

Other recorded persecution of Hindus include those under the reign of 18th century Tipu Sultan in south India, and during the colonial era. In the modern era, religious persecution of Hindus have been reported outside India.

Christophe Jaffrelot states that modern Hindu nationalism was born in Maharashtra, in the 1920s, as a reaction to the Islamic Khilafat Movement wherein Indian Muslims championed and took the cause of the Turkish Ottoman sultan as the Caliph of all Muslims, at the end of the World War I. Hindus viewed this development as one of divided loyalties of Indian Muslim population, of pan-Islamic hegemony, and questioned whether Indian Muslims were a part of an inclusive anti-colonial Indian nationalism. The Hindu nationalism ideology that emerged, states Jeffrelot, was codified by Savarkar while he was a political prisoner of the British colonial empire.

Chris Bayly traces the roots of Hindu nationalism to the Hindu identity and political independence achieved by the Maratha confederacy, that overthrew the Islamic Mughal empire in large parts of India, allowing Hindus the freedom to pursue any of their diverse religious beliefs and restored Hindu holy places such as Varanasi. A few scholars view Hindu mobilisation and consequent nationalism to have emerged in the 19th century as a response to British colonialism by Indian nationalists and neo-Hinduism gurus. Jaffrelot states that the efforts of Christian missionaries and Islamic proselytizers, during the British colonial era, each of whom tried to gain new converts to their own religion, by stereotyping and stigmatising Hindus to an identity of being inferior and superstitious, contributed to Hindus re-asserting their spiritual heritage and counter cross examining Islam and Christianity, forming organisations such as the "Hindu Sabhas" (Hindu associations), and ultimately a Hindu-identity driven nationalism in the 1920s.

The colonial era Hindu revivalism and mobilisation, along with Hindu nationalism, states Peter van der Veer, was primarily a reaction to and competition with Muslim separatism and Muslim nationalism. The successes of each side fed the fears of the other, leading to the growth of Hindu nationalism and Muslim nationalism in the Indian subcontinent. In the 20th century, the sense of religious nationalism grew in India, states van der Veer, but only Muslim nationalism succeeded with the formation of the West and East Pakistan (later split into Pakistan and Bangladesh), as "an Islamic state" upon independence. Religious riots and social trauma followed as millions of Hindus, Jains, Buddhists and Sikhs moved out of the newly created Islamic states and resettled into the Hindu-majority post-British India. After the separation of India and Pakistan in 1947, the Hindu nationalism movement developed the concept of Hindutva in second half of the 20th century.

The Hindu nationalism movement has sought to reform Indian laws, that critics say attempts to impose Hindu values on India's Islamic minority. Gerald Larson states, for example, that Hindu nationalists have sought a uniform civil code, where all citizens are subject to the same laws, everyone has equal civil rights, and individual rights do not depend on the individual's religion. In contrast, opponents of Hindu nationalists remark that eliminating religious law from India poses a threat to the cultural identity and religious rights of Muslims, and people of Islamic faith have a constitutional right to Islamic shariah-based personal laws. A specific law, contentious between Hindu nationalists and their opponents in India, relates to the legal age of marriage for girls. Hindu nationalists seek that the legal age for marriage be eighteen that is universally applied to all girls regardless of their religion and that marriages be registered with local government to verify the age of marriage. Muslim clerics consider this proposal as unacceptable because under the shariah-derived personal law, a Muslim girl can be married at any age after she reaches puberty.

Hindu nationalism in India, states Katharine Adeney, is a controversial political subject, with no consensus about what it means or implies in terms of the form of government and religious rights of the minorities.

According to Pew Research, there are over 1 billion Hindus worldwide (15% of world's population). Along with Christians (31.5%), Muslims (23.2%) and Buddhists (7.1%), Hindus are one of the four major religious groups of the world.

Most Hindus are found in Asian countries. The countries with most Hindu residents and citizens include (in decreasing order) are India, Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom, Myanmar, Canada, Mauritius, Guyana, South Africa, Trinidad and Tobago, Fiji, Suriname.

The fertility rate, that is children per woman, for Hindus is 2.4, which is less than the world average of 2.5. Pew Research projects that there will be 1.161 billion Hindus by 2020.

In more ancient times, Hindu kingdoms arose and spread the religion and traditions across Southeast Asia, particularly Thailand, Nepal, Burma, Malaysia, Indonesia, Cambodia, Laos, Philippines, and what is now central Vietnam.

Over 3 million Hindus are found in Bali Indonesia, a culture whose origins trace back to ideas brought by Tamil Hindu traders to Indonesian islands in the 1st millennium CE. Their sacred texts are also the Vedas and the Upanishads. The Puranas and the Itihasa (mainly "Ramayana" and the "Mahabharata") are enduring traditions among Indonesian Hindus, expressed in community dances and shadow puppet ("wayang") performances. As in India, Indonesian Hindus recognises four paths of spirituality, calling it "Catur Marga". Similarly, like Hindus in India, Balinese Hindu believe that there are four proper goals of human life, calling it "Catur Purusartha" – dharma (pursuit of moral and ethical living), artha (pursuit of wealth and creative activity), kama (pursuit of joy and love) and moksha (pursuit of self-knowledge and liberation).




</doc>
<doc id="13678" url="https://en.wikipedia.org/wiki?curid=13678" title="Hernando de Alarcón">
Hernando de Alarcón

Hernando de Alarcón (born 1500) was a Spanish explorer and navigator of the 16th century, noted for having led an early expedition to the Baja California Peninsula, during which he became one of the first Europeans to ascend the Colorado River from its mouth and perhaps the first to reach Alta California.

Little is known about Alarcón's life outside of his exploits in New Spain. He was probably born in the town of Trujillo, in present-day Extremadura, Spain, in the first years of the 16th century and traveled to the Spanish colonies in the Americas as a young man.

By 1540, Mexico had been conquered and state-sponsored expeditions were being sent north in search of new wealth and the existence of a water passage between the Atlantic and Pacific oceans. Viceroy of New Spain Antonio de Mendoza commissioned Francisco Vázquez de Coronado to undertake a massive overland expedition with the purpose of finding the Seven Cities of Cibola, which were rumored to exist in the unexplored northern interior. The expedition was to be resupplied with stores and provisions delivered by ships traveling up the Sea of Cortés, the commander of which would be Alarcón.

Alarcón set sail from Acapulco with two ships, the "San Pedro" and the "Santa Catalina", on May 9, 1540, and was later joined by the "San Gabriel" at St. Jago de Buena Esperanza, in Colima. His orders from Mendoza were to await the arrival of Coronado's land expedition at a certain latitude along the coast. The meeting with Coronado was never effected, though Alarcón reached the appointed place and left letters, which were soon afterwards discovered by Melchior Diaz, another explorer.

Alarcón eventually sailed to the northern terminus of the Gulf of California and completed the explorations begun by Francisco de Ulloa the preceding year. During this voyage Alarcón proved to his satisfaction that no open-water passage existed between the Gulf of California and the South Sea. Subsequently, on September 26, he entered the mouth of the Colorado River, which he named the "Buena Guia". He was the first European to ascend the river for a distance considerable enough to make important observations. On a second voyage, he probably proceeded past the present-day site of Yuma, Arizona. A map drawn by one of Alarcón's pilots is the earliest accurately detailed representation of the Gulf of California and the lower course of the Colorado River.

Alarcón is almost unique among 16th-century "conquistadores" in that he reportedly treated the Indians he met humanely, as opposed to the often reckless and cruel behavior known from accounts of his contemporaries. Bernard de Voto, in his 1953 "Westward the Course of Empire", observed: "The Indians had an experience they were never to repeat: they were sorry to see these white men leave." Alarcón wrote of his contact with the Yuma-speaking Indians along the Colorado. The information he compiled consisted of their practices in warfare, religion, curing and even sexual customs.

California Historical Landmark No. 568, on the west bank of the Colorado River near Andrade in Imperial County, California, commemorates Alarcón's expedition having been the first non-Indians to sight land within the present-day state of California.




</doc>
<doc id="13679" url="https://en.wikipedia.org/wiki?curid=13679" title="Hakka cuisine">
Hakka cuisine

Hakka cuisine, is the cooking style of the Hakka people, who may also be found in other parts of Taiwan and in countries with significant overseas Hakka communities. There are numerous restaurants in Taiwan, Hong Kong, Indonesia, Malaysia, Singapore and Thailand serving Hakka cuisine. Hakka cuisine was listed in 2014 on the first Hong Kong Inventory of Intangible Cultural Heritage.

The Hakka people have a marked cuisine and style of Chinese cooking which is little known outside the Hakka home. It concentrates on the texture of food – the hallmark of Hakka cuisine. Whereas preserved meats feature in Hakka delicacy, stewed, braised, roast meats – 'texturised' contributions to the Hakka palate – have a central place in their repertoire. Preserved vegetables (梅菜) are commonly used for steamed and braised dishes such as steamed minced pork with preserved vegetables and braised pork with salted vegetables. In fact, the raw materials for Hakka food are no different from raw materials for any other type of regional Chinese cuisine where what is cooked depends on what is available in the market. Hakka cuisine may be described as outwardly simple but tasty. The skill in Hakka cuisine lies in the ability to cook meat thoroughly without hardening it, and to naturally bring out the proteinous flavour (umami taste) of meat.

The Hakka who settled in the harbour and port areas of Hong Kong placed great emphasis on seafood cuisine. Hakka cuisine in Hong Kong is less dominated by expensive meats; instead, emphasis is placed on an abundance of vegetables. Pragmatic and simple, Hakka cuisine is garnished lightly with sparse or little flavouring. Modern Hakka cooking in Hong Kong favours offal, an example being deep-fried intestines (). Others include tofu with preservatives, along with their signature dish, salt baked chicken (). Another specialty is the poon choi (). While it may be difficult to prove these were the actual diets of the old Hakka community, it is at present a commonly accepted view. The above dishes and their variations are in fact found and consumed throughout China, including Guangdong Province, and are not particularly unique or confined to the Hakka population.

Besides meat as source of protein, there is a unique vegan dish called lei cha (). It comprises combinations of vegetables and beans. Although not specifically unique for all Hakka people but are definitely famous among the Hakka-Hopo families. This vegetable-based rice tea dish is gaining momentum in some multicultural countries like Malaysia. Cooking of this dish requires the help from other family members to complete all eight combinations. It helps foster the relationship between family members in return. 

Steamed bun (茶果) is a popular snack for Hakka people. It is mainly made from glutinous rice and is available in sweet or salty options. Sweet version consists of sweetened black-eyed pea pastes or peanuts. Salty version consists of preserved radish.

Hakka food also includes other traditional Taiwanese dishes, just as other Taiwanese ethnic groups do. Some of the more notable dishes in Hakka cuisine are listed as follow:

In India, Pakistan and other regions with significant South Asian populations, the locally known "Hakka cuisine" is actually a Desi adaptation of original Hakka dishes. This variation of Hakka cuisine is in reality, mostly Indian Chinese cuisine and Pakistani Chinese cuisine. It is called "Hakka cuisine" because in India and areas of Pakistan, many owners of restaurants who serve this cuisine are of Hakka origin. Typical dishes include 'chilli chicken' and 'Dongbei (northeastern) chow mein/hakka noodles' (an Indian version of real Northeastern Chinese cuisine), and these restaurants also serve traditional South Asian dishes such as pakora. Being very popular in these areas, this style of cuisine is often mistakenly credited as being representative of Hakka cuisine in general, whereas the authentic style of Hakka cuisine is rarely known in these regions.

Outside of South Asia, the premiere place to enjoy Indo-Pak-Chinese cuisine is in Toronto, Canada, due to the large number of Chinese from South Asia who have emigrated to the region and have chosen to open restaurants and most of it being halal.

In Thailand, Bangkok's Chinatown is Yaowarat and including neighboring areas such as Sampheng, Charoen Chai, Charoen Krung, Suan Mali, Phlapphla Chai or Wong Wian Yi Sip Song Karakadakhom (July 22nd Circle). In the past, many Hakka restaurants are located in the Suan Mali near Bangkok Metropolitan Administration General Hospital. But now they had moved into many places, such as Talad Phlu, which is also one of the Chinatown as well.




</doc>
<doc id="13680" url="https://en.wikipedia.org/wiki?curid=13680" title="Hunan cuisine">
Hunan cuisine

Hunan cuisine, also known as Xiang cuisine, consists of the cuisines of the Xiang River region, Dongting Lake and western Hunan Province in China. It is one of the Eight Great Traditions of Chinese cuisine and is well known for its hot and spicy flavours, fresh aroma and deep colours. Common cooking techniques include stewing, frying, pot-roasting, braising and smoking. Due to the high agricultural output of the region, ingredients for Hunan dishes are many and varied.

The history of the cooking skills employed in Hunan cuisine dates back to the 17th century. The first mention of chili peppers in local gazettes in the province date to 1684, 21st year of the Kangxi Emperor. During the course of its history, Hunan cuisine assimilated a variety of local forms, eventually evolving into its own style. Some well-known dishes include fried chicken with Sichuan spicy sauce () and smoked pork with dried long green beans ().

Hunan cuisine consists of three primary styles:

Known for its liberal use of chili peppers, shallots and garlic, Hunan cuisine is known for being "gan la" () or purely hot, as opposed to Sichuan cuisine, to which it is often compared. Sichuan cuisine is known for its distinctive "ma la" () seasoning and other complex flavour combinations, frequently employs Sichuan pepper along with chilies which are often dried. It also utilises more dried or preserved ingredients and condiments. Hunan cuisine, on the other hand, is often spicier by pure chili content and contains a larger variety of fresh ingredients. Both Hunan and Sichuan cuisine are perhaps significantly oilier than the other cuisines in China, but Sichuan dishes are generally oilier than Hunan dishes. Another characteristic distinguishing Hunan cuisine from Sichuan cuisine is that, in general, Hunan cuisine uses smoked and cured goods in its dishes much more frequently.
Another feature of Hunan cuisine is that the menu changes with the seasons. In a hot and humid summer, a meal will usually start with cold dishes or a platter holding a selection of cold meats with chilies for opening the pores and keeping cool in the summer. In winter, a popular choice is the hot pot, thought to heat the blood in the cold months. A special hot pot called "yuanyang huoguo" () is notable for splitting the pot into two sides – a spicy one and a mild one. One of the classic dishes in Hunan cuisine served in restaurants and at home is farmer pepper fried pork. It is made with several common ingredients: pork belly, green pepper, fermented black beans and other spices.



</doc>
<doc id="13681" url="https://en.wikipedia.org/wiki?curid=13681" title="Hyperinflation">
Hyperinflation

In economics, hyperinflation is very high and typically accelerating inflation. It quickly erodes the real value of the local currency, as the prices of all goods increase. This causes people to minimize their holdings in that currency as they usually switch to more stable foreign currencies, often the US Dollar. Prices typically remain stable in terms of other relatively stable currencies.

Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices, the nominal cost of goods, and in the supply of money. Typically, however, the general price level rises even more rapidly than the money supply as people try ridding themselves of the devaluing currency as quickly as possible. As this happens, the real stock of money (i.e., the amount of circulating money divided by the price level) decreases considerably.

Hyperinflation is often associated with some stress to the government budget, such as wars or their aftermath, sociopolitical upheavals, a collapse in aggregate supply or one in export prices, or other crises that make it difficult for the government to collect tax revenue. A sharp decrease in real tax revenue coupled with a strong need to maintain government spending, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.

In 1956, Phillip Cagan wrote "The Monetary Dynamics of Hyperinflation", the book often regarded as the first serious study of hyperinflation and its effects (though "The Economics of Inflation" by C. Bresciani-Turroni on the German hyperinflation was published in Italian in 1931). In his book, Cagan defined a hyperinflationary episode as starting in the month that the monthly inflation rate exceeds 50%, and as ending when the monthly inflation rate drops below 50% and stays that way for at least a year. Economists usually follow Cagan’s description that hyperinflation occurs when the monthly inflation rate exceeds 50% (this is equivalent to a yearly rate of 12,874.63%).

The International Accounting Standards Board has issued guidance on accounting rules in a hyperinflationary environment. It does not establish an absolute rule on when hyperinflation arises. Instead, it lists factors that indicate the existence of hyperinflation:

While there can be a number of causes of high inflation, most hyperinflations have been caused by government budget deficits financed by money creation. Peter Bernholz analysed 29 hyperinflations (following Cagan's definition) and concludes that at least 25 of them have been caused in this way. A necessary condition for hyperinflation is the use of paper money, instead of gold or silver coins. Most hyperinflations in history, with some exceptions, such as the French hyperinflation of 1789–1796, occurred after the use of fiat currency became widespread in the late 19th century. The French hyperinflation took place after the introduction of a non-convertible paper currency, the assignats.

Hyperinflation occurs when there is a continuing (and often accelerating) rapid increase in the amount of money that is not supported by a corresponding growth in the output of goods and services.

The increases in price that result from the rapid money creation creates a vicious circle, requiring ever growing amounts of new money creation to fund government deficits. Hence both monetary inflation and price inflation proceed at a rapid pace. Such rapidly increasing prices cause widespread unwillingness of the local population to hold the local currency as it rapidly loses its buying power. Instead they quickly spend any money they receive, which increases the velocity of money flow; this in turn causes further acceleration in prices. This means that the increase in the price level is greater than that of the money supply. The real stock of money, M/P, decreases. Here M refers to the money stock and P to the price level.

This results in an imbalance between the supply and demand for the money (including currency and bank deposits), causing rapid inflation. Very high inflation rates can result in a loss of confidence in the currency, similar to a bank run. Usually, the excessive money supply growth results from the government being either unable or unwilling to fully finance the government budget through taxation or borrowing, and instead it finances the government budget deficit through the printing of money.

Governments have sometimes resorted to excessively loose monetary policy, as it allows a government to devalue its debts and reduce (or avoid) a tax increase. Monetary inflation is effectively a flat tax on creditors that also redistributes proportionally to private debtors. Distributional effects of monetary inflation are complex and vary based on the situation, with some models finding regressive effects but other empirical studies progressive effects. As a form of tax, it is less overt than levied taxes and is therefore harder to understand by ordinary citizens. Inflation can obscure quantitative assessments of the true cost of living, as published price indices only look at data in retrospect, so may increase only months later. Monetary inflation can become hyperinflation if monetary authorities fail to fund increasing government expenses from taxes, government debt, cost cutting, or by other means, because either

Theories of hyperinflation generally look for a relationship between seigniorage and the inflation tax. In both Cagan's model and the neo-classical models, a tipping point occurs when the increase in money supply or the drop in the monetary base makes it impossible for a government to improve its financial position. Thus when fiat money is printed, government obligations that are not denominated in money increase in cost by more than the value of the money created.

From this, it might be wondered why any rational government would engage in actions that cause or continue hyperinflation. One reason for such actions is that often the alternative to hyperinflation is either depression or military defeat. The root cause is a matter of more dispute. In both classical economics and monetarism, it is always the result of the monetary authority irresponsibly borrowing money to pay all its expenses. These models focus on the unrestrained seigniorage of the monetary authority, and the gains from the inflation tax.

In neo-classical economic theory, hyperinflation is rooted in a deterioration of the monetary base, that is the confidence that there is a store of value that the currency will be able to command later. In this model, the perceived risk of holding currency rises dramatically, and sellers demand increasingly high premiums to accept the currency. This in turn leads to a greater fear that the currency will collapse, causing even higher premiums. One example of this is during periods of warfare, civil war, or intense internal conflict of other kinds: governments need to do whatever is necessary to continue fighting, since the alternative is defeat. Expenses cannot be cut significantly since the main outlay is armaments. Further, a civil war may make it difficult to raise taxes or to collect existing taxes. While in peacetime the deficit is financed by selling bonds, during a war it is typically difficult and expensive to borrow, especially if the war is going poorly for the government in question. The banking authorities, whether central or not, "monetize" the deficit, printing money to pay for the government's efforts to survive. The hyperinflation under the Chinese Nationalists from 1939 to 1945 is a classic example of a government printing money to pay civil war costs. By the end, currency was flown in over the Himalayas, and then old currency was flown out to be destroyed.

Hyperinflation is a complex phenomenon and one explanation may not be applicable to all cases. In both of these models, however, whether loss of confidence comes first, or central bank seigniorage, the other phase is ignited. In the case of rapid expansion of the money supply, prices rise rapidly in response to the increased supply of money relative to the supply of goods and services, and in the case of loss of confidence, the monetary authority responds to the risk premiums it has to pay by "running the printing presses."

Nevertheless, the immense acceleration process that occurs during hyperinflation (such as during the German hyperinflation of 1922/23) still remains unclear and unpredictable. The transformation of an inflationary development into the hyperinflation has to be identified as a very complex phenomenon, which could be a further advanced research avenue of the complexity economics in conjunction with research areas like mass hysteria, bandwagon effect, social brain, and mirror neurons.

A number of hyperinflations were caused by some sort of extreme negative supply shock, often but not always associated with wars, the breakdown of the communist system or natural disasters.

Since hyperinflation is visible as a monetary effect, models of hyperinflation center on the demand for money. Economists see both a rapid increase in the money supply and an increase in the velocity of money if the (monetary) inflating is not stopped. Either one, or both of these together are the root causes of inflation and hyperinflation. A dramatic increase in the velocity of money as the cause of hyperinflation is central to the "crisis of confidence" model of hyperinflation, where the risk premium that sellers demand for the paper currency over the nominal value grows rapidly. The second theory is that there is first a radical increase in the amount of circulating medium, which can be called the "monetary model" of hyperinflation. In either model, the second effect then follows from the first—either too little confidence forcing an increase in the money supply, or too much money destroying confidence.

In the "confidence model", some event, or series of events, such as defeats in battle, or a run on stocks of the specie that back a currency, removes the belief that the authority issuing the money will remain solvent—whether a bank or a government. Because people do not want to hold notes that may become valueless, they want to spend them. Sellers, realizing that there is a higher risk for the currency, demand a greater and greater premium over the original value. Under this model, the method of ending hyperinflation is to change the backing of the currency, often by issuing a completely new one. War is one commonly cited cause of crisis of confidence, particularly losing in a war, as occurred during Napoleonic Vienna, and capital flight, sometimes because of "contagion" is another. In this view, the increase in the circulating medium is the result of the government attempting to buy time without coming to terms with the root cause of the lack of confidence itself.

In the "monetary model", hyperinflation is a positive feedback cycle of rapid monetary expansion. It has the same cause as all other inflation: money-issuing bodies, central or otherwise, produce currency to pay spiraling costs, often from lax fiscal policy, or the mounting costs of warfare. When business people perceive that the issuer is committed to a policy of rapid currency expansion, they mark up prices to cover the expected decay in the currency's value. The issuer must then accelerate its expansion to cover these prices, which pushes the currency value down even faster than before. According to this model the issuer cannot "win" and the only solution is to abruptly stop expanding the currency. Unfortunately, the end of expansion can cause a severe financial shock to those using the currency as expectations are suddenly adjusted. This policy, combined with reductions of pensions, wages, and government outlays, formed part of the Washington consensus of the 1990s.

Whatever the cause, hyperinflation involves both the supply and velocity of money. Which comes first is a matter of debate, and there may be no universal story that applies to all cases. But once the hyperinflation is established, the pattern of increasing the money stock, by whichever agencies are allowed to do so, is universal. Because this practice increases the supply of currency without any matching increase in demand for it, the price of the currency, that is the exchange rate, naturally falls relative to other currencies. Inflation becomes hyperinflation when the increase in money supply turns specific areas of pricing power into a general frenzy of spending quickly before money becomes worthless. The purchasing power of the currency drops so rapidly that holding cash for even a day is an unacceptable loss of purchasing power. As a result, no one holds currency, which increases the velocity of money, and worsens the crisis.

Because rapidly rising prices undermine the role of money as a store of value, people try to spend it on real goods or services as quickly as possible. Thus, the monetary model predicts that the velocity of money will increase as a result of an excessive increase in the money supply. At the point when money velocity and prices rapidly accelerate in a vicious circle, hyperinflation is out of control, because ordinary policy mechanisms, such as increasing reserve requirements, raising interest rates, or cutting government spending will be ineffective and be responded to by shifting away from the rapidly devalued money and towards other means of exchange.

During a period of hyperinflation, bank runs, loans for 24-hour periods, switching to alternate currencies, the return to use of gold or silver or even barter become common. Many of the people who hoard gold today expect hyperinflation, and are hedging against it by holding specie. There may also be extensive capital flight or flight to a "hard" currency such as the US dollar. This is sometimes met with capital controls, an idea that has swung from standard, to anathema, and back into semi-respectability. All of this constitutes an economy that is operating in an "abnormal" way, which may lead to decreases in real production. If so, that intensifies the hyperinflation, since it means that the amount of goods in "too much money chasing too few goods" formulation is also reduced. This is also part of the vicious circle of hyperinflation.

Once the vicious circle of hyperinflation has been ignited, dramatic policy means are almost always required. Simply raising interest rates is insufficient. Bolivia, for example, underwent a period of hyperinflation in 1985, where prices increased 12,000% in the space of less than a year. The government raised the price of gasoline, which it had been selling at a huge loss to quiet popular discontent, and the hyperinflation came to a halt almost immediately, since it was able to bring in hard currency by selling its oil abroad. The crisis of confidence ended, and people returned deposits to banks. The German hyperinflation (1919–November 1923) was ended by producing a currency based on assets loaned against by banks, called the Rentenmark. Hyperinflation often ends when a civil conflict ends with one side winning.

Although wage and price controls are sometimes used to control or prevent inflation, no episode of hyperinflation has been ended by the use of price controls alone, because price controls that force merchants to sell at prices far below their restocking costs result in shortages that cause prices to rise still further.

Nobel prize winner Milton Friedman said "We economists don't know much, but we do know how to create a shortage. If you want to create a shortage of tomatoes, for example, just pass a law that retailers can't sell tomatoes for more than two cents per pound. Instantly you'll have a tomato shortage. It's the same with oil or gas."

Hyperinflation effectively wipes out the purchasing power of private and public savings; distorts the economy in favor of the hoarding of real assets; causes the monetary base, whether specie or hard currency, to flee the country; and makes the afflicted area anathema to investment.

One of the most important characteristics of hyperinflation is the accelerating substitution of the inflating money by stable money—gold and silver in former times, then relatively stable foreign currencies after the breakdown of the gold or silver standards (Thiers' Law). If inflation is high enough, government regulations like heavy penalties and fines, often combined with exchange controls, cannot prevent this currency substitution. As a consequence, the inflating currency is usually heavily undervalued compared to stable foreign money in terms of purchasing power parity. So foreigners can live cheaply and buy at low prices in the countries hit by high inflation. It follows that governments that do not succeed in engineering a successful currency reform in time must finally legalize the stable foreign currencies (or, formerly, gold and silver) that threaten to fully substitute the inflating money. Otherwise, their tax revenues, including the inflation tax, will approach zero. The last episode of hyperinflation in which this process could be observed was in Zimbabwe in the first decade of the 21st century. In this case, the local money was mainly driven out by the US dollar and the South African rand.

Enactment of price controls to prevent discounting the value of paper money relative to gold, silver, hard currency, or other commodities fail to force acceptance of a paper money that lacks intrinsic value. If the entity responsible for printing a currency promotes excessive money printing, with other factors contributing a reinforcing effect, hyperinflation usually continues. Hyperinflation is generally associated with paper money, which can easily be used to increase the money supply: add more zeros to the plates and print, or even stamp old notes with new numbers. Historically, there have been numerous episodes of hyperinflation in various countries followed by a return to "hard money". Older economies would revert to hard currency and barter when the circulating medium became excessively devalued, generally following a "run" on the store of value.

Much attention on hyperinflation centers on the effect on savers whose investments become worthless. Interest rate changes often cannot keep up with hyperinflation or even high inflation, certainly with contractually fixed interest rates. For example, in the 1970s in the United Kingdom inflation reached 25% per annum, yet interest rates did not rise above 15%—and then only briefly—and many fixed interest rate loans existed. Contractually, there is often no bar to a debtor clearing his long term debt with "hyperinflated cash", nor could a lender simply somehow suspend the loan. Contractual "early redemption penalties" were (and still are) often based on a penalty of "n" months of interest/payment; again no real bar to paying off what had been a large loan. In interwar Germany, for example, much private and corporate debt was effectively wiped out—certainly for those holding fixed interest rate loans.

Ludwig von Mises used the term "crack-up boom" (German: Katastrophenhausse) to describe the economic consequences of an unmitigated increasing in the base-money supply. As more and more money is provided, interest rates decline towards zero. Realizing that fiat money is losing value, investors will try to place money in assets such as real estate, stocks, even art; as these appear to represent "real" value. Asset prices are thus becoming inflated. This potentially spiraling process will ultimately lead to the collapse of the monetary system. The Cantillon effect says that those institutions that receive the new money first are the beneficiaries of the policy.

Hyperinflation is ended by drastic remedies, such as imposing the shock therapy of slashing government expenditures or altering the currency basis. One form this may take is dollarization, the use of a foreign currency (not necessarily the U.S. dollar) as a national unit of currency. An example was dollarization in Ecuador, initiated in September 2000 in response to a 75% loss of value of the Ecuadorian sucre in early 2000. But usually the "dollarization" takes place in spite of all efforts of the government to prevent it by exchange controls, heavy fines and penalties. The government has thus to try to engineer a successful currency reform stabilizing the value of the money. If it does not succeed with this reform the substitution of the inflating by stable money goes on. Thus it is not surprising that there have been at least seven historical cases in which the good (foreign) money did fully drive out the use of the inflating currency. In the end the government had to legalize the former, for otherwise its revenues would have fallen to zero.

Hyperinflation has always been a traumatic experience for the people who suffer it, and the next political regime almost always enacts policies to try to prevent its recurrence. Often this means making the central bank very aggressive about maintaining price stability, as was the case with the German Bundesbank, or moving to some hard basis of currency, such as a currency board. Many governments have enacted extremely stiff wage and price controls in the wake of hyperinflation, but this does not prevent further inflation of the money supply by the central bank, and always leads to widespread shortages of consumer goods if the controls are rigidly enforced.

In countries experiencing hyperinflation, the central bank often prints money in larger and larger denominations as the smaller denomination notes become worthless. This can result in the production of unusually large denominations of banknotes, including those denominated in amounts of 1,000,000,000 or more.

One way to avoid the use of large numbers is by declaring a new unit of currency. (As an example, instead of 10,000,000,000 dollars, a central bank might set 1 new dollar = 1,000,000,000 old dollars, so the new note would read "10 new dollars".) A recent example of this is Turkey's revaluation of the Lira on 1 January 2005, when the old Turkish lira (TRL) was converted to the New Turkish lira (TRY) at a rate of 1,000,000 old to 1 new Turkish Lira. While this does not lessen the actual value of a currency, it is called redenomination or revaluation and also occasionally happens in countries with lower inflation rates. During hyperinflation, currency inflation happens so quickly that bills reach large numbers before revaluation.

Some banknotes were stamped to indicate changes of denomination, as it would have taken too long to print new notes. By the time new notes were printed, they would be obsolete (that is, they would be of too low a denomination to be useful).

Metallic coins were rapid casualties of hyperinflation, as the scrap value of metal enormously exceeded its face value. Massive amounts of coinage were melted down, usually illicitly, and exported for hard currency.

Governments will often try to disguise the true rate of inflation through a variety of techniques. None of these actions addresses the root causes of inflation; and if discovered, they tend to further undermine trust in the currency, causing further increases in inflation. Price controls will generally result in shortages and hoarding and extremely high demand for the controlled goods, causing disruptions of supply chains. Products available to consumers may diminish or disappear as businesses no longer find it economic to continue producing and/or distributing such goods at the legal prices, further exacerbating the shortages.

There are also issues with computerized money-handling systems. In Zimbabwe, during the hyperinflation of the Zimbabwe dollar, many automated teller machines and payment card machines struggled with arithmetic overflow errors as customers required many billions and trillions of dollars at one time.

During the Crisis of the Third Century, Rome underwent hyperinflation caused by years of coinage devaluation.

In 1922, inflation in Austria reached 1,426%, and from 1914 to January 1923, the consumer price index rose by a factor of 11,836, with the highest banknote in denominations of 500,000 Austrian krones. After World War I, essentially all State enterprises ran at a loss, and the number of state employees in the capital, Vienna, was greater than in the earlier monarchy, even though the new republic was nearly one-eighth of the size.

Observing the Austrian response to developing hyperinflation, which included the hoarding of food and the speculation in foreign currencies, Owen S. Phillpotts, the Commercial Secretary at the British Legation in Vienna wrote: "The Austrians are like men on a ship who cannot manage it, and are continually signalling for help. While waiting, however, most of them begin to cut rafts, each for himself, out of the sides and decks. The ship has not yet sunk despite the leaks so caused, and those who have acquired stores of wood in this way may use them to cook their food, while the more seamanlike look on cold and hungry. The population lack courage and energy as well as patriotism."

Increasing hyperinflation in Bolivia has plagued, and at times crippled, its economy and currency since the 1970s. At one time in 1985, the country experienced an annual inflation rate of more than 20,000%. Fiscal and monetary reform reduced the inflation rate to single digits by the 1990s, and in 2004 Bolivia experienced a manageable 4.9% rate of inflation.

In 1987, the Bolivian peso was replaced by a new boliviano at a rate of one million to one (when 1 US dollar was worth 1.8–1.9 million pesos). At that time, 1 new boliviano was roughly equivalent to 1 U.S. dollar.

Brazilian hyperinflation lasted from 1985 (the year when the military dictatorship ended) to 1994, with prices rising by 184,901,570,954.39% in that time due to the uncontrolled printing of money. There were many economic plans that tried to contain hyperinflation including zeroes cuts, price freezes and even confiscation of bank accounts.

The highest value was in March 1990, when the government inflation index reached 82.39%. Hyperinflation ended in July 1994 with the Real Plan during the government of Itamar Franco. During the period of inflation Brazil adopted a total of six different currencies, as the government constantly changed due to rapid devaluation and increase in the number of zeros.

As the first user of fiat currency, China was also the first country to experience hyperinflation. Paper currency was introduced during the Tang Dynasty, and was generally welcomed. It maintained its value, as successive Chinese governments put in place strict controls on issuance. The convenience of paper currency for trade purposes led to strong demand for paper currency. It was only when discipline on quantity supplied broke down that hyperinflation emerged. The Yuan Dynasty (1271–1368) was the first to print large amounts of fiat paper money to fund its wars, resulting in hyperinflation. Much later, the Republic of China went through hyperinflation from 1948 to 1949. In 1947, the highest denomination bill was 50,000 yuan. By mid-1948, the highest denomination was 180,000,000 yuan. The 1948 currency reform replaced the yuan by the gold yuan at an exchange rate of 1 gold yuan = 3,000,000 yuan. In less than a year, the highest denomination was 10,000,000 gold yuan. In the final days of the civil war, the silver yuan was briefly introduced at the rate of 500,000,000 gold yuan. Meanwhile, the highest denomination issued by a regional bank was 6,000,000,000 yuan (issued by Xinjiang Provincial Bank in 1949). After renminbi was instituted by the new communist government, hyperinflation ceased, with a revaluation of 1:10,000 old yuan in 1955.

During the French Revolution and first Republic, the National Assembly issued bonds, some backed by seized church property, called assignats. Napoleon replaced them with the franc in 1803, at which time the assignats were basically worthless. Stephen D. Dillaye pointed out that one of the reasons for the failure was massive counterfeiting of the paper currency, largely through London. According to Dillaye: "Seventeen manufacturing establishments were in full operation in London, with a force of four hundred men devoted to the production of false and forged Assignats."

By November 1922, the value in gold of money in circulation had fallen from £300 million before World War I to £20 million. The Reichsbank responded by the unlimited printing of notes, thereby accelerating the devaluation of the mark. In his report to London, Lord D'Abernon wrote: "In the whole course of history, no dog has ever run after its own tail with the speed of the Reichsbank." Germany went through its worst inflation in 1923. In 1922, the highest denomination was 50,000 marks. By 1923, the highest denomination was 100,000,000,000,000 (10) Marks. In December 1923 the exchange rate was 4,200,000,000,000 (4.2 × 10) Marks to 1 US dollar. In 1923, the rate of inflation hit 3.25 × 10 percent per month (prices double every two days). Beginning on 20 November 1923, 1,000,000,000,000 old Marks were exchanged for 1 Rentenmark, so that 4.2 Rentenmarks were worth 1 US dollar, exactly the same rate the Mark had in 1914.

With the German invasion in April 1941, there was an abrupt increase in prices. This was due to psychological factors related to the fear of shortages and to the hoarding of goods. During the German and Italian Axis occupation of Greece (1941–1944), the agricultural, mineral, industrial etc. production of Greece were used to sustain the occupation forces, but also to secure provisions for the Afrika Korps. One part of these "sales" of provisions was settled with bilateral clearing through the German DEGRIGES and the Italian Sagic companies at very low prices. As the value of Greek exports in drachmas fell, the demand for drachmas followed suit and so did its forex rate. While shortages started due to naval blockades and hoarding, the prices of commodities soared. The other part of the "purchases" was settled with drachmas secured from the Bank of Greece and printed for this purpose by private printing presses. As prices soared, the Germans and Italians started requesting more and more drachmas from the Bank of Greece to offset price increases; each time prices increased, the note circulation followed suit soon afterwards. For the year starting November 1943, the inflation rate was 2.5 × 10%, the circulation was 6.28 × 10 drachmae and one gold sovereign cost 43,167 billion drachmas. The hyperinflation started subsiding immediately after the departure of the German occupation forces, but inflation rates took several years before they fell below 50%.

The Treaty of Trianon and political instability between 1919 and 1924 led to a major inflation of Hungary's currency. In 1921, in an attempt to stop this inflation, the national assembly of Hungary passed the Hegedűs reforms, including a 20% levy on bank deposits, but this precipitated a mistrust of banks by the public, especially the peasants, and resulted in a reduction in savings, and thus in the amount of currency in circulation. Due to the reduced tax base, the government resorted to printing money, and in 1923 inflation in Hungary reached 98% per month.

Between the end of 1945 and July 1946, Hungary went through the highest inflation ever recorded. In 1944, the highest banknote value was 1,000 pengő. By the end of 1945, it was 10,000,000 pengő, and the highest value in mid-1946 was 100,000,000,000,000,000,000 (10) pengő. A special currency, the adópengő (or tax pengő) was created for tax and postal payments. The inflation was such that the value of the adópengő was adjusted each day by radio announcement. On 1 January 1946, one adópengő equaled one pengő, but by late July, one adópengő equaled 2,000,000,000,000,000,000,000 or 2×10 (2 sextillion) pengő.

When the pengő was replaced in August 1946 by the forint, the total value of all Hungarian banknotes in circulation amounted to of one US cent. Inflation had peaked at 1.3 × 10% per month (i.e. prices doubled every 15.6 hours). On 18 August 1946, 400,000,000,000,000,000,000,000,000,000 or 4 pengő (four hundred quadrilliard on the long scale used in Hungary, or four hundred octillion on short scale) became 1 forint.

North Korea has most likely experienced hyperinflation from December 2009 to mid-January 2011. Based on the price of rice, North Korea's hyperinflation peaked in mid-January 2010, but according to black market exchange-rate data, and calculations based on purchasing power parity, North Korea experienced its peak month of inflation in early March 2010. These data are unofficial, however, and therefore must be treated with a degree of caution.

In modern history, Peru underwent a period of hyperinflation period in the 1980s to the early 1990s starting with President Fernando Belaúnde's second administration, heightened during Alan García's first administration, to the beginning of Alberto Fujimori's term. Over 3,210,000,000 old soles would be worth one USD. Garcia's term introduced the inti, which worsened inflation into hyperinflation. Peru's currency and economy were pacified under Fujimori's Nuevo Sol program, which has remained Peru's currency since 1991.

Poland has gone through two episodes of hyperinflation since the country regained independence following the end of World War I, the first in 1923, the second in 1989–1990. Both events resulted in the introduction of new currencies. In 1924, the "złoty" replaced the original currency of post-war Poland, the mark. This currency was subsequently replaced by another of the same name in 1950, which was assigned the ISO code of PLZ. As a result of the second hyperinflation crisis, the current "new złoty" was introduced in 1990 (ISO code: PLN). See the article on Polish złoty for more information about the currency's history.

The newly independent Poland had been struggling with a large budget deficit since its inception in 1918 but it was in 1923 when inflation reached its peak. The exchange rate to the American dollar went from 9 Polish marks per dollar in 1918 to 6,375,000 marks per dollar at the end of 1923. A new personal 'inflation tax' was introduced. The resolution of the crisis is attributed to Władysław Grabski, who became prime minister of Poland in December 1923. Having nominated an all-new government and being granted extraordinary lawmaking powers by the Sejm for a period of six months, he introduced a new currency, established a new national bank and scrapped the inflation tax, which took place throughout 1924.

The economic crisis in Poland in the 1980s was accompanied by rising inflation when new money was printed to cover a budget deficit. Although inflation was not as acute as in 1920s, it is estimated that its annual rate reached around 600% in a period of over a year spanning parts of 1989 and 1990. The economy was stabilised by the adoption of the Balcerowicz Plan in 1989, named after the main author of the reforms, minister of finance Leszek Balcerowicz. The plan was largely inspired by the previous Grabski's reforms.

The Japanese government occupying the Philippines during World War II issued fiat currencies for general circulation. The Japanese-sponsored Second Philippine Republic government led by Jose P. Laurel at the same time outlawed possession of other currencies, most especially "guerrilla money". The fiat money's lack of value earned it the derisive nickname "Mickey Mouse money". Survivors of the war often tell tales of bringing suitcases or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with Japanese-issued bills. Early on, 75 Mickey Mouse pesos could buy one duck egg. In 1944, a box of matches cost more than 100 Mickey Mouse pesos.

In 1942, the highest denomination available was 10 pesos. Before the end of the war, because of inflation, the Japanese government was forced to issue 100-, 500-, and 1000-peso notes.

Malaya and Singapore were under Japanese occupation from 1942 until 1945. The Japanese issued banana money as the official currency to replace the Straits currency issued by the British. During that time, the cost of basic necessities increased drastically. As the occupation proceeded, the Japanese authorities printed more money to fund their wartime activities, which resulted in hyperinflation and a severe depreciation in value of the banana note.

From February to December 1942, $100 of Straits currency was worth $100 in Japanese scrip, after which the value of Japanese scrip began to erode, reaching $385 on December 1943 and $1,850 one year later. By 1 August 1945, this had inflated to $10,500, and 11 days later it had reached $95,000. After 13 August 1945, Japanese scrip had become valueless.

A seven-year period of uncontrollable spiralling inflation occurred in the early Soviet Union, running from the earliest days of the Bolshevik Revolution in November 1917 to the reestablishment of the gold standard with the introduction of the chervonets as part of the New Economic Policy. The inflationary crisis effectively ended in March 1924 with the introduction of the so-called "gold ruble" as the country's standard currency.

The early Soviet hyperinflationary period was marked by three successive redenominations of its currency, in which "new rubles" replaced old at the rates of 10,000:1 (1 January 1922), 100:1 (1 January 1923), and 50,000:1 (7 March 1924), respectively.

Between 1921 and 1922, inflation in the Soviet Union reached 213%.

Venezuela's hyperinflation began in November 2016. Inflation of Venezuela's bolivar fuerte (VEF) in 2014 reached 69% and was the highest in the world. In 2015, inflation was 181%, the highest in the world and the highest in the country's history at that time, 800% in 2016, over 4,000% in 2017, and 1,698,488% in 2018, with Venezuela spiraling into hyperinflation. While the Venezuelan government "has essentially stopped" producing official inflation estimates as of early 2018, one estimate of the rate at that time was 5,220%, according to inflation economist Steve Hanke of Johns Hopkins University.

Inflation has affected Venezuelans so much that in 2017, some people became video game gold farmers and could be seen playing games such as "RuneScape" to sell in-game currency or characters for real currency. In many cases, these gamers made more money than salaried workers in Venezuela even though they were earning just a few dollars per day. During the Christmas season of 2017, some shops would no longer use price tags since prices would inflate so quickly, so customers were required to ask staff at stores how much each item was.

The International Monetary Fund estimated in 2018 that Venezuela's inflation rate would reach 1,000,000% by the end of the year. This forecast was criticized by Steve H. Hanke, professor of applied economics at The Johns Hopkins University and senior fellow at the Cato Institute. According to Hanke, the IMF had released a "bogus forecast" because "no one has ever been able to accurately forecast the course or the duration of an episode of hyperinflation. But that has not stopped the IMF from offering inflation forecasts for Venezuela that have proven to be wildly inaccurate".

In July 2018, hyperinflation in Venezuela was sitting at 33,151%, "the 23rd most severe episode of hyperinflation in history".

In April 2019, the International Monetary Fund has estimated that inflation would reach 10,000,000% by the end of 2019. However, the Central Bank of Venezuela officially estimates that inflation is at 53,798,500% between 2016 and April 2019.

In May 2019, the Central Bank of Venezuela released economic data for the first time since 2015. According to this release, the inflation of Venezuela was 274% in 2016, 863% in 2017 and 130,060% in 2018. The new reports imply a contraction of more than half of the economy in five years, according to the "Financial Times" "one of the biggest contractions in Latin American history". According two undisclosed sources from Reuters, the release of this numbers was due to pressure from China, a Maduro ally. One of this sources claims that the disclosure of economic numbers may bring Venezuela into compliance with the IMF, making it harder to support Juan Guaidó during the presidential crisis. At the time, the IMF was not able to support the validity of the data as they had not been able to contact the authorities.

Yugoslavia went through a period of hyperinflation and subsequent currency reforms from 1989 to 1994. One of several regional conflicts accompanying the dissolution of Yugoslavia was the Bosnian War (1992–1995). The Belgrade government of Slobodan Milošević backed ethnic Serbian forces in the conflict, resulting in a United Nations boycott of Yugoslavia. The UN boycott collapsed an economy already weakened by regional war, with the projected monthly inflation rate accelerating to one million percent by December 1993 (prices double every 2.3 days).

The highest denomination in 1988 was 50,000 dinars. By 1989 it was 2,000,000 dinars. In the 1990 currency reform, 1 new dinar was exchanged for 10,000 old dinars. In the 1992 currency reform, 1 new dinar was exchanged for 10 old dinars. The highest denomination in 1992 was 50,000 dinars. By 1993, it was 10,000,000,000 dinars. In the 1993 currency reform, 1 new dinar was exchanged for 1,000,000 old dinars. Before the year was over, however, the highest denomination was 500,000,000,000 dinars. In the 1994 currency reform, 1 new dinar was exchanged for 1,000,000,000 old dinars. In another currency reform a month later, 1 novi dinar was exchanged for 13 million dinars (1 novi dinar = 1 German mark at the time of exchange). The overall impact of hyperinflation was that 1 novi dinar was equal to 1 × 10 – 1.3 × 10 pre-1990 dinars. Yugoslavia's rate of inflation hit 5 × 10% cumulative inflation over the time period 1 October 1993 and 24 January 1994. 


Hyperinflation in Zimbabwe was one of the few instances that resulted in the abandonment of the local currency. At independence in 1980, the Zimbabwe dollar (ZWD) was worth about US$1.25. Afterwards, however, rampant inflation and the collapse of the economy severely devalued the currency. Inflation was steady until British Prime Minister Tony Blair reneged on land reform agreements arrived at between Margaret Thatcher and Robert Mugabe that continued land redistribution from the white farming community in 1998, resulting in reductions in food production and the decline of foreign investment. Several multinational companies began hoarding retail goods in warehouses in Zimbabwe and just south of the border, preventing commodities from becoming available on the market. The result was that to pay its expenditures Mugabe's government and Gideon Gono's Reserve Bank printed more and more notes with higher face values.

Hyperinflation began early in the 21st century, reaching 624% in 2004. It fell back to low triple digits before surging to a new high of 1,730% in 2006. The Reserve Bank of Zimbabwe revalued on 1 August 2006 at a ratio of 1,000 ZWD to each second dollar (ZWN), but year-to-year inflation rose by June 2007 to 11,000% (versus an earlier estimate of 9,000%). Larger denominations were progressively issued in 2008:

Inflation by 16 July officially surged to 2,200,000% with some analysts estimating figures surpassing 9,000,000%. As of 22 July 2008 the value of the ZWN fell to approximately 688 billion per US$1, or 688 trillion pre-August 2006 Zimbabwean dollars.

On 1 August 2008, the Zimbabwe dollar was redenominated at the ratio of ZWN to each third dollar (ZWR). On 19 August 2008, official figures announced for June estimated the inflation over 11,250,000%. Zimbabwe's annual inflation was 231,000,000% in July (prices doubling every 17.3 days). By October 2008 Zimbabwe was mired in hyperinflation with wages falling far behind inflation. In this dysfunctional economy hospitals and schools had chronic staffing problems, because many nurses and teachers could not afford bus fare to work. Most of the capital of Harare was without water because the authorities had stopped paying the bills to buy and transport the treatment chemicals. Desperate for foreign currency to keep the government functioning, Zimbabwe's central bank governor, Gideon Gono, sent runners into the streets with suitcases of Zimbabwean dollars to buy up American dollars and South African rand.

For periods after July 2008, no official inflation statistics were released. Prof. Steve H. Hanke overcame the problem by estimating inflation rates after July 2008 and publishing the Hanke Hyperinflation Index for Zimbabwe. Prof. Hanke's HHIZ measure indicated that the inflation peaked at an annual rate of 89.7 sextillion percent (89,700,000,000,000,000,000,000%) in mid-November 2008. The peak monthly rate was 79.6 billion percent, which is equivalent to a 98% daily rate, or around % yearly rate. At that rate, prices were doubling every 24.7 hours. Note that many of these figures should be considered mostly theoretical since hyperinflation did not proceed at this rate over a whole year.

At its November 2008 peak, Zimbabwe's rate of inflation approached, but failed to surpass, Hungary's July 1946 world record. On 2 February 2009, the dollar was redenominated for the third time at the ratio of ZWR to 1 ZWL, only three weeks after the $100 trillion banknote was issued on 16 January, but hyperinflation waned by then as official inflation rates in USD were announced and foreign transactions were legalised, and on 12 April the Zimbabwe dollar was abandoned in favour of using only foreign currencies. The overall impact of hyperinflation was US$1 = ZWD.

Some countries experienced very high inflation, but did not reach hyperinflation, as defined as a "monthly" inflation rate of 50%.

Between 1620 and 1622 the Kreuzer fell from 1 Reichsthaler to 124 Kreuzer in end of 1619 to 1 Reichstaler to over 600 (regionally over 1000) Kreuzer in end of 1622, during the Thirty Years' War. This is a monthly inflation rate of over 20.6% (regionally over 34.4%).

Between 1987 and 1995 the Iraqi Dinar went from an official value of 0.306 Dinars/USD (or US$3.26 per dinar, though the black market rate is thought to have been substantially lower) to 3,000 dinars/USD due to government printing of 10s of trillions of dinars starting with a base of only tens of billions. That equates to approximately 315% inflation per year averaged over that eight-year period.

In spite of increased oil prices in the late 1970s (Mexico is a producer and exporter), Mexico defaulted on its external debt in 1982. As a result, the country suffered a severe case of capital flight and several years of acute inflation and peso devaluation, leading to an accumulated inflation rate of almost 27,000% between December 1975 and late 1988. On 1 January 1993, Mexico created a new currency, the "nuevo peso" ("new peso", or MXN), which chopped three zeros off the old peso (One new peso was equal to 1,000 old MXP pesos).

Between 1998 and 1999, Ecuador faced a period of economic instability that resulted from a combined banking crisis, currency crisis, and sovereign debt crisis. Severe inflation and devaluation of the Ecuadorean Sucre lead to President Jamil Mahuad announcing on January 9, 2000 that the US dollar would be adopted as the national currency.

Despite the government's efforts to curb inflation, the Sucre depreciated rapidly at the end of 1999, resulting in widespread informal use of U.S. dollars in the financial system. As a last resort to prevent hyperinflation, the government formally adopted the U.S. dollar in January 2000. The stability of the new currency was a necessary first step towards economic recovery, but the exchange rate was fixed at 25,000:1, which resulted in great losses of wealth.

In Roman Egypt, where the best documentation on pricing has survived, the price of a measure of wheat was 200 drachmae in 276 AD, and increased to more than 2,000,000 drachmae in 334 AD, roughly 1,000,000% inflation in a span of 58 years.

Although the price increased by a factor of 10,000 over 58 years, the annual rate of inflation was only 17.2% (1.4% monthly) compounded.

Romania experienced high inflation in the 1990s. The highest denomination in 1990 was 100 lei and in 1998 was 100,000 lei. By 2000 it was 500,000 lei. In early 2005 it was 1,000,000 lei. In July 2005 the lei was replaced by the new leu at 10,000 old lei = 1 new leu. Inflation in 2005 was 9%. In July 2005 the highest denomination became 500 lei (= 5,000,000 old lei).

The Second Transnistrian ruble consisted solely of banknotes and suffered from high inflation, necessitating the issue of notes overstamped with higher denominations. 1 and sometimes 10 ruble become 10,000 ruble, 5 ruble become 50,000 and 10 ruble become 100,000 ruble. In 2000, a new ruble was introduced at a rate of 1 new ruble = 1,000,000 old rubles.

Since the end of 2017 Turkey has high inflation rates. It is speculated that the new elections took place frustrated because of the impending crisis to forestall. In October 2017, inflation was at 11.9%, the highest rate since July 2008. The Turkish lira fall from 1.503 TRY = 1 US dollar in 2010 to 5.5695 TRY = 1 US dollar in August 2018.

During the Revolutionary War, when the Continental Congress authorized the printing of paper called continental currency, the monthly inflation rate reached a peak of 47% in November 1779 (Bernholz 2003: 48). These notes depreciated rapidly, giving rise to the expression "not worth a continental". One cause of the inflation was counterfeiting by the British, who ran a press on HMS "Phoenix", moored in New York Harbor. The counterfeits were advertised and sold almost for the price of the paper they were printed on.
During the U.S. Civil War between January 1861 and April 1865, the Confederate States decided to finance the war by printing money. The Lerner Commodity Price Index of leading cities in the eastern Confederacy states subsuqenetly increased from 100 to 9,200 in that time. In the final months of the Civil War, the Confederate dollar was almost worthless. Similarly, the Union government inflated its greenbacks, with the monthly rate peaking at 40% in March 1864 (Bernholz 2003: 107).

Vietnam went through a period of chaos and high inflation in the late 1980s, with inflation peaking at 774% in 1988, after the country's "price-wage-currency" reform package, led by then-Deputy Prime Minister , had failed. High inflation also occurred in the early stages of the socialist-oriented market economic reforms commonly referred to as the Đổi Mới.

Inflation rate is usually measured in percent per year. It can also be measured in percent per month or in price doubling time.

formula_1

formula_2

formula_3

formula_4

Often, at redenominations, three zeroes are cut from the bills. It can be read from the table that if the (annual) inflation is for example 100%, it takes 3.32 years to produce one more zero on the price tags, or 3 × 3.32 = 9.96 years to produce three zeroes. Thus can one expect a redenomination to take place about 9.96 years after the currency was introduced.





</doc>
<doc id="13682" url="https://en.wikipedia.org/wiki?curid=13682" title="Herbert Hoover">
Herbert Hoover

Herbert Clark Hoover (August 10, 1874 – October 20, 1964) was an American engineer, businessman, and politician who served as the 31st president of the United States from 1929 to 1933. A member of the Republican Party, he held office during the onset of the Great Depression. Prior to serving as president, Hoover led the Commission for Relief in Belgium, served as the director of the U.S. Food Administration, and served as the 3rd U.S. Secretary of Commerce.

Born to a Quaker family in West Branch, Iowa, he grew up in Oregon. Hoover took a position with a London-based mining company after graduating from Stanford University in 1895. After the outbreak of World War I, he became the head of the Commission for Relief in Belgium, an international relief organization that provided food to occupied Belgium. When the U.S. entered the war, President Woodrow Wilson appointed Hoover to lead the Food Administration, and Hoover became known as the country's "food czar". After the war, Hoover led the American Relief Administration, which provided food to the inhabitants of Central Europe and Eastern Europe. Hoover's war-time service made him a favorite of many progressives, and he unsuccessfully sought the Republican nomination in the 1920 presidential election.

After the 1920 election, newly-elected Republican President Warren G. Harding appointed Hoover as Secretary of Commerce; Hoover continued to serve under President Calvin Coolidge after Harding died in 1923. Hoover was an unusually active and visible cabinet member, becoming known as "Secretary of Commerce and Under-Secretary of all other departments". He was influential in the development of radio and air travel and led the federal response to the Great Mississippi Flood of 1927. Hoover won the Republican nomination in the 1928 presidential election, and decisively defeated the Democratic candidate, Al Smith. The stock market crashed shortly after Hoover took office, and the Great Depression became the central issue of his presidency. Hoover pursued a variety of policies in an attempt to lift the economy, but opposed directly involving the federal government in relief efforts.

In the midst of an ongoing economic crisis, Hoover was decisively defeated by Democratic nominee Franklin D. Roosevelt in the 1932 presidential election. Hoover enjoyed one of the longest retirements of any former president, and he authored numerous works. After leaving office, Hoover became increasingly conservative, and he strongly criticized Roosevelt's foreign policy and New Deal domestic agenda. In the 1940s and 1950s, Hoover's public reputation was rehabilitated as he served for Presidents Harry S. Truman and Dwight D. Eisenhower in various assignments, including as chairman of the Hoover Commission. Nevertheless, Hoover is generally not ranked highly in historical rankings of presidents of the United States.

Herbert Hoover was born on August 10, 1874 in West Branch, Iowa. His father, Jesse Hoover, was a blacksmith and farm implement store owner of German, Swiss, and English ancestry. Hoover's mother, Hulda Randall Minthorn, was raised in Norwich, Ontario, Canada, before moving to Iowa in 1859. Like most other citizens of West Branch, Jesse and Hulda were Quakers. As a child, Hoover consistently attended schools, but he did little reading on his own aside from the Bible. Hoover's father, noted by the local paper for his "pleasant, sunshiny disposition", died in 1880 at the age of 34. Hoover's mother died in 1884, leaving Hoover, his older brother, Theodore, and his younger sister, May, as orphans.
In 1882, Hoover spent a year living at the Osage Agency in Pawhuska, Oklahoma with his aunt Agnes (Minthorn) Miles and uncle Laban J. Miles who was a U.S. Indian Agent. And in 1885, Hoover was sent to Newberg, Oregon to live with his uncle John Minthorn, a Quaker physician and businessman whose own son had died the year before. The Minthorn household was considered cultured and educational, and imparted a strong work ethic. Much like West Branch, Newberg was a frontier town settled largely by Midwestern Quakers. Minthorn ensured that Hoover received an education, but Hoover disliked the many chores assigned to him and often resented Minthorn. One observer described Hoover as "an orphan [who] seemed to be neglected in many ways." Hoover attended Friends Pacific Academy (now George Fox University), but dropped out at the age of thirteen to become an office assistant for his uncle's real estate office (Oregon Land Company) in Salem, Oregon. Though he did not attend high school, Hoover learned bookkeeping, typing, and mathematics at a night school.

Hoover entered Stanford University in 1891, its inaugural year, despite failing all the entrance exams except mathematics. During his freshman year, he switched his major from mechanical engineering to geology after working for John Casper Branner, the chair of Stanford's geology department. Hoover was a mediocre student, and he spent much of his time working in various part-time jobs or participating in campus activities. Though he was initially shy among fellow students, Hoover won election as student treasurer and became known for his distaste for fraternities and sororities. He served as student manager of both the baseball and football teams, and helped organize the inaugural Big Game versus the University of California. During the summers before and after his senior year, Hoover interned under economic geologist Waldemar Lindgren of the United States Geological Survey; these experiences convinced Hoover to pursue a career as a mining geologist.

When Hoover graduated from Stanford in 1895, the country was in the midst of the Panic of 1893, and he initially struggled to find a job. He worked in various low-level mining jobs in the Sierra Nevada mountain range until he convinced prominent mining engineer Louis Janin to hire him. After working as a mine scout for a year, Hoover was hired by Bewick, Moreing & Co., a London-based company that operated gold mines in Western Australia. Hoover first went to Coolgardie, then the center of the Eastern Goldfields. Though Hoover received a $5,000 salary (), conditions were harsh in the goldfields. Hoover described the Coolgardie and Murchison rangelands on the edge of the Great Victoria Desert as a land of "black flies, red dust and white heat."

Hoover traveled constantly across the Outback to evaluate and manage the company's mines. He convinced Bewick, Moreing to purchase the Sons of Gwalia gold mine, which proved to be one of the most successful mines in the region. Partly due to Hoover's efforts, the company eventually controlled approximately 50 percent of gold production in Western Australia. Hoover brought in many Italian immigrants to cut costs and counter the labour movement of the Australian miners. During his time with the mining company, Hoover became opposed to measures such as a minimum wage and workers' compensation, feeling that they were unfair to owners. Hoover's work impressed his employers, and in 1898 he was promoted to junior partner. An open feud developed between Hoover and his boss, Ernest Williams, but company leaders defused the situation by offering Hoover a compelling position in China.

Upon arriving in China, Hoover developed gold mines near Tianjin on behalf of Bewick, Moreing and the Chinese-owned Chinese Engineering and Mining Company. He became deeply interested in Chinese history, but quickly gave up on learning the language and, like many of his contemporaries, viewed the Chinese people as racially inferior. He made recommendations to improve the lot of the Chinese worker, seeking to end the practice of imposing long-term servitude contracts and to institute reforms for workers based on merit. The Boxer Rebellion broke out shortly after Hoover arrived in China, trapping the Hoovers and numerous other foreign nationals until a multi-national force defeated Boxer forces in the Battle of Tientsin. Fearing the imminent collapse of the Chinese government, the director of the Chinese Engineering and Mining Company agreed to establish a new Sino-British venture with Bewick, Moreing. After Hoover and Bewick, Moreing established effective control over the new Chinese mining company, Hoover became the operating partner of Bewick, Moreing in late 1901.

As operating partner, Hoover continually traveled the world on behalf of Bewick, Moreing, visiting mines operated by the company on different continents. Beginning in December 1902, the company faced mounting legal and financial issues after one of the partners admitted to having fraudulently sold stock in a mine. More issues arose in 1904, after the British government formed two separate royal commission to investigate Bewick, Moreing's labor practices and financial dealings in Western Australia. After the company lost a suit filed by the former director of the Chinese Engineering and Mining Company, Hoover began looking for a way to get out of the partnership, and he sold his shares in mid-1908.

After leaving Bewick, Moreing, Hoover worked as a London-based independent mining consultant and financier. Though he had risen to prominence as a geologist and mine operator, Hoover focused much of his attention on raising money, restructuring corporate organizations, and financing new ventures. He specialized in rejuvenating troubled mining operations, taking a share of the profits in exchange for his technical and financial expertise. Hoover thought of himself and his associates as "engineering doctors to sick concerns", and he earned a reputation as a "doctor of sick mines". He made investments on every continent and had offices in San Francisco, London, New York City, Paris, St. Petersburg (Florida), Florida, and Mandalay, Myanmar. By 1914, Hoover was a very wealthy man, with an estimated personal fortune of $4 million (equivalent to $ million in ).

He co-founded the Zinc Corporation to extract zinc near the Australian city of Broken Hill. The Zinc Corporation developed the froth flotation process to extract zinc from lead-silver ore, and operated the world's first selective or differential flotation plant. Hoover worked with the Burma Corporation, a British firm that produced silver, lead, and zinc in large quantities at the Namtu Bawdwin Mine. He also helped increase copper production in Kyshtym, Russia, through the use of pyritic smelting. He also agreed to manage a separate mine in the Altai Mountains that, according to Hoover, "developed probably the greatest and richest single body of ore known in the world."

In his spare time, Hoover wrote. His lectures at Columbia and Stanford universities were published in 1909 as "Principles of Mining", which became a standard textbook. The book reflects his move towards progressive ideals, as Hoover came to endorse eight-hour workdays and organized labor. Hoover became deeply interested in the history of science, and he was especially drawn to the "De re metallica", an influential 16th century work on mining and metallurgy. In 1912, Hoover and his wife published the first English translation of "De re metallica". Hoover also joined the board of trustees at Stanford, and led a successful campaign to appoint John Branner as the university's president.

During his senior year at Stanford, Hoover became smitten with a classmate named Lou Henry, though his financial situation precluded marriage at that time. The daughter of a banker from Monterey, California, Lou Henry decided to study geology at Stanford after attending a lecture delivered by John Branner. Immediately after earning a promotion in 1898, Hoover cabled Lou Henry, asking her to marry him. After she cabled back her acceptance of the proposal, Hoover briefly returned to the United States for their wedding. They would remain married until Lou Henry's death in 1944. Though his Quaker upbringing strongly influenced his career, Hoover rarely attended Quaker meetings during his adult life. Hoover and his wife had two children: Herbert Hoover Jr. (born in 1903) and Allan Henry Hoover (born in 1907). The Hoover family began living in London in 1902, though they frequently traveled as part of Hoover's career. After 1916, the Hoovers began living in the United States, maintaining homes in Palo Alto, California and Washington, D.C.

World War I broke out in June 1914, pitting the Allied Powers (France, Russia, Britain, and other countries) against the Central Powers (Germany, Austria-Hungary, and other countries). Hoover and other London-based American businessmen established a committee to organize the return of the roughly 100,000 Americans stranded in Europe. Hoover was appointed as the committee's chair and, with the assent of Congress and the executive branch, took charge of the distribution of relief to Americans in Europe. Hoover later stated, "I did not realize it at the moment, but on August 3, 1914, my career was over forever. I was on the slippery road of public life." By early October 1914, Hoover's organization had distributed relief to at least 40,000 Americans.

The German invasion of Belgium in August 1914 set off a food crisis in Belgium, which relied heavily on food imports. The Germans refused to take responsibility for feeding Belgian citizens in captured territory, and the British refused to lift their blockade of German-occupied Belgium unless the U.S. government supervised Belgian food imports as a neutral party in the war. With the cooperation of the Wilson administration and the CNSA, a Belgian relief organization, Hoover established the Commission for Relief in Belgium (CRB). The CRB obtained and imported millions of tons of foodstuffs for the CNSA to distribute, and helped ensure that the German army did not appropriate the food. Private donations and government grants supplied the majority of its $11-million-a-month budget, and the CRB became a veritable independent republic of relief, with its own flag, navy, factories, mills, and railroads. A British official described the CRB as a "piratical state organized for benevolence."

Hoover worked 14-hour days from London, administering the distribution of over two million tons of food to nine million war victims. In an early form of shuttle diplomacy, he crossed the North Sea forty times to meet with German authorities and persuade them to allow food shipments. He also convinced British Chancellor of the Exchequer David Lloyd George to allow individuals to send money to the people of Belgium, thereby lessening workload of the CRB. At the request of the French government, the CRB began delivering supplies to the people of Northern France in 1915. American diplomat Walter Page described Hoover as "probably the only man living who has privately (i.e., without holding office) negotiated understandings with the British, French, German, Dutch, and Belgian governments."

The United States declared war upon Germany in April 1917 after Germany engaged in unrestricted submarine warfare against American vessels in British waters. With the U.S. mobilizing for war, President Woodrow Wilson appointed Hoover to head the U.S. Food Administration, which was charged with ensuring the nation's food needs during the war. Hoover had hoped to join the administration in some capacity since at least 1916, and he obtained the position after lobbying several members of Congress and Wilson's confidant, Edward M. House. Earning the appellation of "food czar", Hoover recruited a volunteer force of hundreds of thousands of women and deployed propaganda in movie theaters, schools, and churches. He carefully selected men to assist in the agency leadership—Alonzo Taylor (technical abilities), Robert Taft (political associations), Gifford Pinchot (agricultural influence), and Julius Barnes (business acumen).

World War I had created a global food crisis that dramatically increased food prices and caused food riots and starvation in the countries at war. Hoover's chief goal as food czar was to provide supplies to the Allied Powers, but he also sought to stabilize domestic prices and to prevent domestic shortages. Under the broad powers granted by the Food and Fuel Control Act, the Food Administration supervised food production throughout the United States, and the administration made use of its authority to buy, import, store, and sell food. Determined to avoid rationing, Hoover established set days for people to avoid eating specified foods and save them for soldiers' rations: meatless Mondays, wheatless Wednesdays, and "when in doubt, eat potatoes". These policies were dubbed "Hooverizing" by government publicists, in spite of Hoover's continual orders that publicity should not mention him by name. The Food Administration shipped 23 million metric tons of food to the Allied Powers, preventing their collapse and earning Hoover great acclaim. As head of the Food Administration, Hoover gained a following in the United States, especially among progressives who saw in Hoover an expert administrator and symbol of efficiency.

World War I came to an end in November 1918, but Europe continued to face a critical food situation; Hoover estimated that as many as 400 million people faced the possibility of starvation. The United States Food Administration became the American Relief Administration (ARA), and Hoover was charged with providing food to Central and Eastern Europe. In addition to providing relief, the ARA rebuilt infrastructure in an effort to rejuvenate the economy of Europe. Throughout the Paris Peace Conference, Hoover served as a close adviser to President Wilson, and he largely shared Wilson's goals of establishing the League of Nations, settling borders on the basis of self-determination, and refraining from inflicting a harsh punishment on the defeated Central Powers. The following year, famed British economist John Maynard Keynes wrote in The Economic Consequences of the Peace that if Hoover's realism, "knowledge, magnanimity and disinterestedness" had found wider play in the councils of Paris, the world would have had "the Good Peace." After U.S. government funding for the ARA expired in mid-1919, Hoover transformed the ARA into a private organization, raising millions of dollars from private donors. He also established the European Children's Fund, which provided relief to fifteen million children across fourteen countries.

Despite the opposition of Senator Henry Cabot Lodge and other Republicans, Hoover provided aid to the defeated German nation after the war, as well as relief to famine-stricken Bolshevik-controlled areas of Russia. Hoover condemned Bolshevism, but warned President Wilson against an intervention in Russia, as he viewed the White Russian forces as little better than the Bolsheviks and feared the possibility of a protracted U.S. involvement. The Russian famine of 1921–22 claimed six million people, but the intervention of the ARA likely saved millions of lives. When asked if he was not helping Bolshevism by providing relief, Hoover stated, "twenty million people are starving. Whatever their politics, they shall be fed!" Reflecting the gratitude of many Europeans, in July 1922, Soviet author Maxim Gorky told Hoover that "your help will enter history as a unique, gigantic achievement, worthy of the greatest glory, which will long remain in the memory of millions of Russians whom you have saved from death."

In 1919, Hoover established the Hoover War Collection at Stanford University. He donated all the files of the Commission for Relief in Belgium, the U.S. Food Administration, and the American Relief Administration, and pledged $50,000 as an endowment (). Scholars were sent to Europe to collect pamphlets, society publications, government documents, newspapers, posters, proclamations, and other ephemeral materials related to the war and the revolutions that followed it. The collection was renamed the Hoover War Library in 1922 and is now known as the Hoover Institution. During the post-war period, Hoover also served as the president of the Federated American Engineering Societies.

Hoover had been little known among the American public before 1914, but his service in the Wilson administration established him as a contender in the 1920 presidential election. Hoover's wartime push for higher taxes, criticism of Attorney General A. Mitchell Palmer's actions during the First Red Scare, and his advocacy for measures such as the minimum wage, forty-eight-hour workweek, and elimination of child labor made him appealing to progressives of both parties. Despite his service in the Democratic administration of Woodrow Wilson, Hoover had never been closely affiliated with either the Democrats or the Republicans. He initially sought to avoid committing to any party in the 1920 election, hoping that either of the two major parties would draft him for president at their respective national convention. In March 1920, he changed his strategy and declared himself to be a Republican; he was motivated in large part by the belief that the Democratic candidate would have little chance of winning the 1920 presidential election. Despite his national renown, Hoover's service in the Wilson administration had alienated farmers and the conservative Old Guard of the GOP, and his presidential candidacy fizzled out after his defeat in the California primary by favorite son Hiram Johnson. At the 1920 Republican National Convention, Warren G. Harding emerged as a compromise candidate after the convention became deadlocked between supporters of Johnson, Leonard Wood, and Frank Orren Lowden. Hoover backed Harding's successful campaign in the general election, and he began laying the groundwork for a future presidential run by building up a base of strong supporters in the Republican Party.

After his election as president in 1920, Harding rewarded Hoover for his support, offering to appoint him as either Secretary of the Interior or Secretary of Commerce. Secretary of Commerce was considered a minor Cabinet post, with limited and vaguely defined responsibilities, but Hoover decided to accept the position. Hoover's progressive stances, continuing support for the League of Nations, and recent conversion to the Republican Party aroused opposition to his appointment from many Senate Republicans. To overcome this opposition, Harding paired Hoover's nomination with that of conservative favorite Andrew Mellon as Secretary of the Treasury, and the nominations of both Hoover and Mellon were confirmed by the Senate. Hoover would serve as Secretary of Commerce from 1921 to 1929, serving under Harding and, after Harding's death in 1923, President Calvin Coolidge. While some of the most prominent members of the Harding administration, including Attorney General Harry M. Daugherty and Secretary of Interior Albert B. Fall, were implicated in major scandals, Hoover emerged largely unscathed from investigations into the Harding administration.

Hoover envisioned the Commerce Department as the hub of the nation's growth and stability. His experience mobilizing the war-time economy convinced him that the federal government could promote efficiency by eliminating waste, increasing production, encouraging the adoption of data-based practices, investing in infrastructure, and conserving natural resources. Contemporaries described Hoover's approach as a "third alternative" between "unrestrained capitalism" and socialism, which was becoming increasingly popular in Europe. Hoover sought to foster a balance among labor, capital, and the government, and for this he has been variously labeled a corporatist or an associationalist.

Hoover demanded, and received, authority to coordinate economic affairs throughout the government. He created many sub-departments and committees, overseeing and regulating everything from manufacturing statistics to air travel. In some instances he "seized" control of responsibilities from other Cabinet departments when he deemed that they were not carrying out their responsibilities well; some began referring to him as the "Secretary of Commerce and Under-Secretary of all other departments." In response to the Depression of 1920–21, he convinced Harding to assemble a presidential commission on unemployment, which encouraged local governments to engage in countercyclical infrastructure spending. He endorsed much of Mellon's tax reduction program, but favored a more progressive tax system and opposed the treasury secretary's efforts to eliminate the estate tax.

Between 1923 and 1929, the number of families with radios grew from 300,000 to 10 million, and Hoover's tenure as Secretary of Commerce heavily influenced radio use in the United States. In the early and mid-1920s, Hoover's radio conferences played a key role in the organization, development, and regulation of radio broadcasting. Hoover also helped pass the Radio Act of 1927, which allowed the government to intervene and abolish radio stations that were deemed "non-useful" to the public. Hoover's attempts at regulating radio were not supported by all congressmen, and he received much opposition from the Senate and from radio station owners.

Hoover was also influential in the early development of air travel, and he sought to create a thriving private industry boosted by indirect government subsidies. He encouraged the development of emergency landing fields, required all runways to be equipped with lights and radio beams, and encouraged farmers to make use of planes for crop dusting. He also established the federal government's power to inspect planes and license pilots, setting a precedent for the later Federal Aviation Administration.

As Commerce Secretary, Hoover hosted national conferences on street traffic collectively known as the National Conference on Street and Highway Safety. Hoover's chief objective was to address the growing casualty toll of traffic accidents, but the scope of the conferences grew and soon embraced motor vehicle standards, rules of the road, and urban traffic control. He left the invited interest groups to negotiate agreements among themselves, which were then presented for adoption by states and localities. Because automotive trade associations were the best organized, many of the positions taken by the conferences reflected their interests. The conferences issued a model Uniform Vehicle Code for adoption by the states, and a Model Municipal Traffic Ordinance for adoption by cities. Both were widely influential, promoting greater uniformity between jurisdictions and tending to promote the automobile's priority in city streets.

With the goal of encouraging wise business investments, Hoover made the Commerce Department a clearinghouse of information. He recruited numerous academics from various fields and tasked them with publishing reports on different aspects of the economy, including steel production and films. To eliminate waste, he encouraged standardization of products like automobile tires and baby bottle nipples. Other efforts at eliminating waste included reducing labor losses from trade disputes and seasonal fluctuations, reducing industrial losses from accident and injury, and reducing the amount of crude oil spilled during extraction and shipping. He promoted international trade by opening overseas offices to advise businessmen. Hoover was especially eager to promote Hollywood films overseas. His "Own Your Own Home" campaign was a collaboration to promote ownership of single-family dwellings, with groups such as the Better Houses in America movement, the Architects' Small House Service Bureau, and the Home Modernizing Bureau. He worked with bankers and the savings and loan industry to promote the new long-term home mortgage, which dramatically stimulated home construction. Other accomplishments included winning the agreement of U.S. Steel to adopt an eight-hour workday, and the fostering of the Colorado River Compact, a water rights compact among Southwestern states.

The Great Mississippi Flood of 1927 broke the banks and levees of the lower Mississippi River in early 1927, resulting in the flooding of millions of acres and leaving 1.5 million people displaced from their homes. Although disaster response did not fall under the duties of the Commerce Department, the governors of six states along the Mississippi River specifically asked President Coolidge to appoint Hoover to coordinate the response to the flood. Believing that disaster response was not the domain of the federal government, Coolidge initially refused to become involved, but he eventually acceded to political pressure and appointed Hoover to chair a special committee to help the region. Hoover established over one hundred tent cities and a fleet of more than six hundred vessels, and raised $17 million (equivalent to $ million in ). In large part due to his leadership during the flood crisis, by 1928, Hoover had begun to overshadow President Coolidge himself. Though Hoover received wide acclaim for his role in the crisis, he ordered the suppression of reports of mistreatment of African Americans in refugee camps. He did so with the cooperation of African-American leader Robert Russa Moton, who was promised unprecedented influence once Hoover became president.

Hoover quietly built up support for a future presidential bid throughout the 1920s, but he carefully avoided alienating Coolidge, who was eligible to run for another term in the 1928 presidential election. Along with the rest of the nation, he was surprised when Coolidge announced in August 1927 that he would not seek another term. With the impending retirement of Coolidge, Hoover immediately emerged as the front-runner for the 1928 Republican nomination, and he quickly put together a strong campaign team led by Hubert Work, Will H. Hays, and Reed Smoot. Coolidge was unwilling to anoint Hoover as his successor; on one occasion he remarked that, "for six years that man has given me unsolicited advice—all of it bad." Despite his lukewarm feelings towards Hoover, Coolidge had no desire to split the party by publicly opposing the popular Commerce Secretary's candidacy.

Many wary Republican leaders cast about for an alternative candidate, such as Treasury Secretary Andrew Mellon or former Secretary of State Charles Evans Hughes. However, Hughes and Mellon declined to run, and other potential contenders like Frank Orren Lowden and Vice President Charles G. Dawes failed to garner widespread support. Hoover won the presidential nomination on the first ballot of the 1928 Republican National Convention. Convention delegates considered re-nominating Vice President Charles Dawes to be Hoover's running mate, but Coolidge, who hated Dawes, remarked that this would be "a personal affront" to him. The convention instead selected Senator Charles Curtis of Kansas. Hoover accepted the nomination at Stanford Stadium, telling a huge crowd that he would continue the policies of the Harding and Coolidge administrations. The Democrats nominated New York governor Al Smith, who became the first Catholic major party nominee for president.

Hoover centered his campaign around the Republican record of peace and prosperity, as well as his own reputation as a successful engineer and public official. Averse to giving political speeches, Hoover largely stayed out of the fray and left the campaigning to Curtis and other Republicans. Smith was more charismatic and gregarious than Hoover, but his campaign was damaged by anti-Catholicism and his overt opposition to Prohibition. Hoover had never been a strong proponent of Prohibition, but he accepted the Republican Party's plank in favor of it and issued an ambivalent statement calling Prohibition "a great social and economic experiment, noble in motive and far-reaching in purpose." In the South, Hoover and the national party pursued a "lily-white" strategy, removing black Republicans from leadership positions in an attempt to curry favor with white Southerners.

Hoover maintained polling leads throughout the 1928 campaign, and he decisively defeated Smith on election day, taking 58 percent of the popular vote and 444 of the 531 electoral votes. Historians agree that Hoover's national reputation and the booming economy, combined with deep splits in the Democratic Party over religion and Prohibition, guaranteed his landslide victory. Hoover's appeal to Southern white voters succeeded in cracking the "Solid South", and he won five Southern states. Hoover's victory was positively received by newspapers; one wrote that Hoover would "drive so forcefully at the tasks now before the nation that the end of his eight years as president will find us looking back on an era of prodigious achievement."

Hoover's detractors wondered why he did not do anything to reapportion congress after the 1920 United States Census which saw an increase in urban and immigrant populations. The 1920 Census was the first and only Decennial Census where the results were not used to reapportion Congress, which ultimately influenced the 1928 Electoral College and impacted the Presidential Election.

Hoover saw the presidency as a vehicle for improving the conditions of all Americans by encouraging public-private cooperation—what he termed "volunteerism". He tended to oppose governmental coercion or intervention, as he thought they infringed on American ideals of individualism and self-reliance. The first major bill that he signed, the Agricultural Marketing Act of 1929, established the Federal Farm Board in order to stabilize farm prices. Hoover made extensive use of commissions to study issues and propose solutions, and many of those commissions were sponsored by private donors rather than by the government. One of the commissions started by Hoover, the Research Committee on Social Trends, was tasked with surveying the entirety of American society. He appointed a Cabinet consisting largely of wealthy, business-oriented conservatives, including Secretary of the Treasury Andrew Mellon. Lou Henry Hoover was an activist First Lady. She typified the new woman of the post-World War I era: intelligent, robust, and aware of multiple female possibilities.

On taking office, Hoover said that "given the chance to go forward with the policies of the last eight years, we shall soon with the help of God, be in sight of the day when poverty will be banished from this nation." Having seen the fruits of prosperity brought by technological progress, many shared Hoover's optimism, and the already bullish stock market climbed even higher on Hoover's accession. This optimism concealed several threats to sustained U.S. economic growth, including a persistent farm crisis, a saturation of consumer goods like automobiles, and growing income inequality. Most dangerous of all to the economy was excessive speculation that had raised stock prices far beyond their value. Some regulators and bankers had warned Coolidge and Hoover that a failure to curb speculation would lead to "one of the greatest financial catastrophes that this country has ever seen," but both presidents were reluctant to become involved with the workings of the Federal Reserve System, which regulated banks.

In late October 1929, the Stock Market Crash of 1929 occurred, and the worldwide economy began to spiral downward into the Great Depression. The causes of the Great Depression remain a matter of debate, but Hoover viewed a lack of confidence in the financial system as the fundamental economic problem facing the nation. He sought to avoid direct federal intervention, believing that the best way to bolster the economy was through the strengthening of businesses such as banks and railroads. He also feared that allowing individuals on the "dole" would permanently weaken the country. Instead, Hoover strongly believed that local governments and private giving should address the needs of individuals.

Though he attempted to put a positive spin on Black Tuesday, Hoover moved quickly to address the stock market collapse. In the days following Black Tuesday, Hoover gathered business and labor leaders, asking them to avoid wage cuts and work stoppages while the country faced what he believed would be a short recession similar to the Depression of 1920–21. Hoover also convinced railroads and public utilities to increase spending on construction and maintenance, and the Federal Reserve announced that it would cut interest rates. In early 1930, Hoover acquired from Congress an additional $100 million to continue the Federal Farm Board lending and purchasing policies. These actions were collectively designed to prevent a cycle of deflation and provide a fiscal stimulus. At the same time, Hoover opposed congressional proposals to provide federal relief to the unemployed, as he believed that such programs were the responsibility of state and local governments and philanthropic organizations.

Hoover had taken office hoping to raise agricultural tariffs in order to help farmers reeling from the farm crisis of the 1920s, but his attempt to raise agricultural tariffs became connected with a bill that broadly raised tariffs. Hoover refused to become closely involved in the congressional debate over the tariff, and Congress produced a tariff bill that raised rates for many goods. Despite the widespread unpopularity of the bill, Hoover felt that he could not reject the main legislative accomplishment of the Republican-controlled 71st Congress. Over the objection of many economists, Hoover signed the Smoot–Hawley Tariff Act into law in June 1930. Canada, France, and other nations retaliated by raising tariffs, resulting in a contraction of international trade and a worsening of the economy. Progressive Republicans such as Senator William E. Borah of Idaho were outraged when Hoover signed the tariff act, and Hoover's relations with that wing of the party never recovered.

By the end of 1930, the national unemployment rate had reached 11.9 percent, but it was not yet clear to most Americans that the economic downturn would be worse than the Depression of 1920–21. A series of bank failures in late 1930 heralded a larger collapse of the economy in 1931. While other countries left the gold standard, Hoover refused to abandon it; he derided any other monetary system as "collectivism." Hoover viewed the weak European economy as a major cause of economic troubles in the United States. In response to the collapse of the German economy, Hoover marshaled congressional support behind a one-year moratorium on European war debts. The Hoover Moratorium was warmly received in Europe and the United States, but Germany remained on the brink of defaulting on its loans. As the worldwide economy worsened, democratic governments fell; in Germany, Nazi Party leader Adolf Hitler assumed power.

By mid-1931, the unemployment rate had reached 15 percent, giving rise to growing fears that the country was experiencing a depression far worse than recent economic downturns. A reserved man with a fear of public speaking, Hoover allowed his opponents in the Democratic Party to define him as cold, incompetent, reactionary, and out-of-touch. Hoover's opponents developed defamatory epithets to discredit him, such as "Hooverville" (the shanty towns and homeless encampments), "Hoover leather" (cardboard used to cover holes in the soles of shoes), and "Hoover blanket" (old newspaper used to cover oneself from the cold). While Hoover continued to resist direct federal relief efforts, Governor Franklin D. Roosevelt of New York launched the Temporary Emergency Relief Administration to provide aid to the unemployed. Democrats positioned the program as a kinder alternative to Hoover's alleged apathy towards the unemployed.

The economy continued to worsen, with unemployment rates nearing 23 percent in early 1932, and Hoover finally heeded calls for more direct federal intervention. In January 1932, he convinced Congress to authorize the establishment of the Reconstruction Finance Corporation (RFC), which would provide government-secured loans to financial institutions, railroads, and local governments. The RFC saved numerous businesses from failure, but it failed to stimulate commercial lending as much as Hoover had hoped, partly because it was run by conservative bankers unwilling to make riskier loans. The same month the RFC was established, Hoover signed the Federal Home Loan Bank Act, establishing 12 district banks overseen by a Federal Home Loan Bank Board in a manner similar to the Federal Reserve System. He also helped arrange passage of the Glass–Steagall Act of 1932, emergency banking legislation designed to expand banking credit by expanding the collateral on which Federal Reserve banks were authorized to lend. As these measures failed to stem the economic crisis, Hoover signed the Emergency Relief and Construction Act, a $2 billion public works bill, in July 1932.

After a decade of budget surpluses, the federal government experienced a budget deficit in 1931. Though some economists, like William Trufant Foster, favored deficit spending to address the Great Depression, most politicians and economists believed in the necessity of keeping a balanced budget. In late 1931, Hoover proposed a tax plan to increase tax revenue by 30 percent, resulting in the passage of the Revenue Act of 1932. The act increased taxes across the board, rolling back much of the tax cut reduction program Mellon had presided over during the 1920s. Top earners were taxed at 63 percent on their net income, the highest rate since the early 1920s. The act also doubled the top estate tax rate, cut personal income tax exemptions, eliminated the corporate income tax exemption, and raised corporate tax rates. Despite the passage of the Revenue Act, the federal government continued to run a budget deficit.

Hoover seldom mentioned civil rights while he was president. He believed that African Americans and other races could improve themselves with education and individual initiative. Hoover appointed more African Americans to federal positions than Harding and Coolidge had combined, but many African-American leaders condemned various aspects of the Hoover administration, including Hoover's unwillingness to push for a federal anti-lynching law. Hoover also continued to pursue the lily-white strategy, removing African Americans from positions of leadership in the Republican Party in an attempt to end the Democratic Party's dominance in the South. Though Robert Moton and some other black leaders accepted the lily-white strategy as a temporary measure, most African-American leaders were outraged. Hoover further alienated black leaders by nominating conservative Southern judge John J. Parker to the Supreme Court; Parker's nomination ultimately failed in the Senate due to opposition from the NAACP and organized labor. Many black voters switched to the Democratic Party in the 1932 election, and African Americans would later become an important part of Franklin Roosevelt's New Deal coalition.

As part of his efforts to limit unemployment, Hoover sought to cut immigration to the United States, and in 1930 he promulgated an executive order requiring individuals to have employment before migrating to the United States. With the goal of opening up more jobs for U.S. citizens, Secretary of Labor William N. Doak began a campaign to prosecute illegal immigrants in the United States. Though Doak did not seek to deport one specific group of immigrants, his campaign most strongly affected Mexican Americans, especially Mexican Americans living in Southern California. Many of the deportations were overseen by state and local authorities who acted on the encouragement of Doak and the Department of Labor. During the 1930s, approximately one million Mexican Americans were forcibly "repatriated" to Mexico; approximately sixty percent of those deported were birthright citizens. According to legal professor Kevin R. Johnson, the repatriation campaign meets the modern legal standards of ethnic cleansing, as it involved the forced removal of a racial minority by government actors.

On taking office, Hoover urged Americans to obey the Eighteenth Amendment and the Volstead Act, which had established Prohibition across the United States. To make public policy recommendations regarding Prohibition, he created the Wickersham Commission. Hoover had hoped that the commission's public report would buttress his stance in favor of Prohibition, but the report criticized the enforcement of the Volstead Act and noted the growing public opposition to Prohibition. After the Wickersham Report was published in 1931, Hoover rejected the advice of some of his closest allies and refused to endorse any revision of the Volstead Act or the Eighteenth Amendment, as he feared doing so would undermine his support among Prohibition advocates. As public opinion increasingly turned against Prohibition, more and more people flouted the law, and a grassroots movement began working in earnest for Prohibition's repeal. In January 1933, a constitutional amendment repealing the Eighteenth Amendment was approved by Congress and submitted to the states for ratification. By December 1933, it had been ratified by the requisite number of states to become the Twenty-first Amendment.

According to Leuchtenburg, Hoover was "the last American president to take office with no conspicuous need to pay attention to the rest of the world". Nevertheless, during Hoover's term, the world order established in the immediate aftermath of World War I began to crumble. As president, Hoover largely made good on his pledge made prior to assuming office not to interfere in Latin America's internal affairs. In 1930, he released the Clark Memorandum, a rejection of the Roosevelt Corollary and a move towards non-interventionism in Latin America. Hoover did not completely refrain from the use of the military in Latin American affairs; he thrice threatened intervention in the Dominican Republic, and he sent warships to El Salvador to support the government against a left-wing revolution. Notwithstanding those actions, he wound down the Banana Wars, ending the occupation of Nicaragua and nearly bringing an end to the occupation of Haiti.

Hoover placed a priority on disarmament, which he hoped would allow the United States to shift money from the military to domestic needs. Hoover and Secretary of State Henry L. Stimson focused on extending the 1922 Washington Naval Treaty, which sought to prevent a naval arms race. As a result of Hoover's efforts, the United States and other major naval powers signed the 1930 London Naval Treaty. The treaty represented the first time that the naval powers had agreed to cap their tonnage of auxiliary vessels, as previous agreements had only affected capital ships. 

At the 1932 World Disarmament Conference, Hoover urged further cutbacks in armaments and the outlawing of tanks and bombers, but his proposals were not adopted.

In 1931, Japan invaded Manchuria, defeating the Republic of China's military forces and establishing Manchukuo, a puppet state. The Hoover administration deplored the invasion, but also sought to avoid antagonizing the Japanese, fearing that taking too strong a stand would weaken the moderate forces in the Japanese government and alienate a potential ally against the Soviet Union, which he saw as a much greater threat. In response to the Japanese invasion, Hoover and Secretary of State Stimson outlined the Stimson Doctrine, which held that the United States would not recognize territories gained by force. The 1931 photo illustration to the right highlights a goodwill visit by Japanese Royal family representatives who sought to maintain goodwill between the U.S. and Japan. The brother of Emperor Hirohito, Prince Takamatsu and his recent bride are being escorted down Pennsylvania Avenue in the U.S. Capital by President Hoover.

Thousands of World War I veterans and their families demonstrated and camped out in Washington, DC, during June 1932, calling for immediate payment of bonuses that had been promised by the World War Adjusted Compensation Act in 1924; the terms of the act called for payment of the bonuses in 1945. Although offered money by Congress to return home, some members of the "Bonus Army" remained. Washington police attempted to disperse the demonstrators, but they were outnumbered and unsuccessful. Shots were fired by the police in a futile attempt to attain order, and two protesters were killed while many officers were injured. Hoover sent U.S. Army forces led by General Douglas MacArthur to the protests. MacArthur, believing he was fighting a Communist revolution, chose to clear out the camp with military force. Though Hoover had not ordered MacArthur's clearing out of the protesters, he endorsed it after the fact. The incident proved embarrassing for the Hoover administration, and destroyed any remaining chance he had of winning re-election.

By mid-1931 few observers thought that Hoover had much hope of winning a second term in the midst of the ongoing economic crisis. Nonetheless, Hoover faced little opposition for re-nomination at the 1932 Republican National Convention, as Coolidge and other prominent Republicans all passed on the opportunity to challenge Hoover. Franklin D. Roosevelt won the presidential nomination on the fourth ballot of the 1932 Democratic National Convention, defeating the 1928 Democratic nominee, Al Smith. The Democrats attacked Hoover as the cause of the Great Depression, and for being indifferent to the suffering of millions. As Governor of New York, Roosevelt had called on the New York legislature to provide aid for the needy, establishing Roosevelt's reputation for being more favorable toward government interventionism during the economic crisis. The Democratic Party, including Al Smith and other national leaders, coalesced behind Roosevelt, while progressive Republicans like George Norris and Robert La Follette Jr. deserted Hoover.

Hoover's detractors wondered why he did not do anything to reapportion congress after the 1920 United States Census which saw an increase in urban and immigrant populations. The 1920 Census was the first and only Decennial Census where the results were not used to reapportion Congress; which ultimately influenced the 1928 Electoral College and impacted the Presidential Election.

Hoover originally planned to make only one or two major speeches, and to leave the rest of the campaigning to proxies, as sitting presidents had traditionally done. However, encouraged by Republican pleas and outraged by Democratic claims, Hoover entered the public fray. In his nine major radio addresses Hoover primarily defended his administration and his philosophy of government, urging voters to hold to the "foundations of experience" and reject the notion that government interventionism could save the country from the Depression. In his campaign trips around the country, Hoover was faced with perhaps the most hostile crowds ever seen by a sitting president. Besides having his train and motorcades pelted with eggs and rotten fruit, he was often heckled while speaking, and on several occasions, the Secret Service halted attempts to kill Hoover by disgruntled citizens, including capturing one man nearing Hoover carrying sticks of dynamite, and another already having removed several spikes from the rails in front of the president's train.

Hoover's attempts to vindicate his administration fell on deaf ears, as much of the public blamed his administration for the depression. In the electoral vote, Hoover lost 59–472, carrying six states. Hoover won just 39.7 percent of the popular vote, a reduction of 26 percentage points from his result in the 1928 election. Roosevelt's performance in the popular vote made him the first Democratic presidential nominee to win the presidency with a majority of the popular vote since the Civil War.

Hoover departed from Washington in March 1933, bitter at his election loss and continuing unpopularity. As Coolidge, Harding, Wilson, and Taft had all died during the 1920s or early 1930s, Hoover was the sole living ex-president from 1933 to 1953. Hoover and his wife lived in Palo Alto until her death in 1944, at which point Hoover began to live permanently at the Waldorf Astoria hotel in New York City. During the 1930s, Hoover increasingly self-identified as a conservative. He closely followed national events after leaving public office, becoming a constant critic of Franklin Roosevelt. In response to continued attacks on his character and presidency, Hoover wrote more than two dozen books, including "The Challenge to Liberty" (1934), which harshly criticized Roosevelt's New Deal. Hoover described the New Deal's National Recovery Administration and Agricultural Adjustment Administration as "fascistic", and he called the 1933 Banking Act a "move to gigantic socialism."

Only 58 when he left office, Hoover held out hope for another term as president throughout the 1930s. At the 1936 Republican National Convention, Hoover's speech attacking the New Deal was well received, but the nomination went to Kansas Governor Alf Landon. In the general election, Hoover delivered numerous well-publicized speeches on behalf of Landon, but Landon was defeated by Roosevelt. Though Hoover was eager to oppose Roosevelt at every turn, Senator Arthur Vandenberg and other Republicans urged the still-unpopular Hoover to remain out of the fray during the debate over Roosevelt's proposed Judiciary Reorganization Bill of 1937. At the 1940 Republican National Convention, Hoover again hoped for the presidential nomination, but it went to the internationalist Wendell Willkie, who lost to Roosevelt in the general election.

During a 1938 trip to Europe, Hoover met with Adolf Hitler and stayed at Hermann Göring's hunting lodge. He expressed dismay at the persecution of Jews in Germany and believed that Hitler was mad, but did not present a threat to the U.S. Instead, Hoover believed that Roosevelt posed the biggest threat to peace, holding that Roosevelt's policies provoked Japan and discouraged France and the United Kingdom from reaching an "accommodation" with Germany. After the September 1939 invasion of Poland by Germany, Hoover opposed U.S. involvement in World War II, including the Lend-Lease policy. He rejected Roosevelt's offers to help coordinate relief in Europe, but, with the help of old friends from the CRB, helped establish the Commission for Polish Relief.

During a radio broadcast on June 29, 1941, one week after the Nazi invasion of the Soviet Union, Hoover disparaged any "tacit alliance" between the U.S. and the USSR, stating, "if we join the war and Stalin wins, we have aided him to impose more communism on Europe and the world... War alongside Stalin to impose freedom is more than a travesty. It is a tragedy." Much to his own frustration, Hoover was not called upon to serve after the United States entered World War II due to his differences with Roosevelt and his continuing unpopularity. He did not pursue the presidential nomination at the 1944 Republican National Convention, and, at the request of Republican nominee Thomas E. Dewey, refrained from campaigning during the general election.

Following World War II, Hoover befriended President Harry S. Truman despite their ideological differences. Because of Hoover's experience with Germany at the end of World War I, in 1946 Truman selected the former president to tour Germany to ascertain the food needs of the occupied nation. After touring Germany, Hoover produced a number of reports critical of U.S. occupation policy. He stated in one report that "there is the illusion that the New Germany left after the annexations can be reduced to a 'pastoral state.' It cannot be done unless we exterminate or move 25,000,000 people out of it." On Hoover's initiative, a school meals program in the American and British occupation zones of Germany was begun on April 14, 1947; the program served 3,500,000 children.

In 1947, Truman appointed Hoover to a commission to reorganize the executive departments; the commission elected Hoover as chairman and became known as the Hoover Commission. The commission recommended changes designed to strengthen the president's ability to manage the federal government. Though Hoover had opposed Roosevelt's concentration of power in the 1930s, he believed that a stronger presidency was required with the advent of the Atomic Age. During the 1948 presidential election, Hoover supported Republican nominee Thomas Dewey's unsuccessful campaign against Truman, but he remained on good terms with Truman. Hoover favored the United Nations in principle, but he opposed granting membership to the Soviet Union and other Communist states. He viewed the Soviet Union to be as morally repugnant as Nazi Germany and supported the efforts of Richard Nixon and others to expose Communists in the United States.

Hoover backed conservative leader Robert A. Taft at the 1952 Republican National Convention, but the party's presidential nomination instead went to Dwight D. Eisenhower, who went on to win the 1952 election. Though Eisenhower appointed Hoover to another presidential commission, Hoover disliked Eisenhower, faulting the latter's failure to roll back the New Deal. Hoover's public work helped to rehabilitate his reputation, as did his use of self-deprecating humor; he occasionally remarked that "I am the only person of distinction who's ever had a depression named after him." In 1958, Congress passed the Former Presidents Act, offering a $25,000 yearly pension () to each former president. Hoover took the pension even though he did not need the money, possibly to avoid embarrassing Truman, whose precarious financial status played a role in the law's enactment. In the early 1960s, President John F. Kennedy offered Hoover various positions; Hoover declined the offers but defended Kennedy after the Bay of Pigs invasion and was personally distraught by Kennedy's assassination in 1963.

Hoover wrote several books during his retirement, including "The Ordeal of Woodrow Wilson", in which he strongly defended Wilson's actions at the Paris Peace Conference. In 1944, he began working on "Freedom Betrayed", which he often referred to as his "magnum opus." In "Freedom Betrayed", Hoover strongly critiques Roosevelt's foreign policy, especially Roosevelt's decision to recognize the Soviet Union in order to provide aid to that country during World War II. The book was published in 2012 after being edited by historian George H. Nash.

Hoover faced three major illnesses during the last two years of his life, including an August 1962 operation in which a growth on his large intestine was removed. He died on October 20, 1964 in New York City following massive internal bleeding.
Though Hoover's last spoken words are unknown, his last known written words were a get well message to his friend Harry Truman, six days before his death, after he heard that Truman sustained injuries from slipping in a bathroom: "Bathtubs are a menace to ex-presidents for as you may recall a bathtub rose up and fractured my vertebrae when I was in Venezuela on your world famine mission in 1946. My warmest sympathy and best wishes for your recovery." Two months earlier, on August 10, Hoover reached the age of 90, only the second U.S. president (after John Adams) to do so. When asked how he felt on reaching the milestone, Hoover replied, "Too old." At the time of his death Hoover had been out of office for over 31 years ( days altogether). This was the longest retirement in presidential history until Jimmy Carter broke that record in September 2012.

Hoover was honored with a state funeral in which he lay in state in the United States Capitol rotunda. Then, on October 25, he was buried in West Branch, Iowa, near his presidential library and birthplace on the grounds of the Herbert Hoover National Historic Site. Afterwards, Hoover's wife, Lou Henry, who had been buried in Palo Alto, California, following her death in 1944, was re-interred beside him.

Hoover was extremely unpopular when he left office after the 1932 election, and his historical reputation would not begin to recover until the 1970s. According to Professor David E. Hamilton, historians have credited Hoover for his genuine belief in voluntarism and cooperation, as well as the innovation of some of his programs. However, Hamilton also notes that Hoover was politically inept and failed to recognize the severity of the Great Depression. Nicholas Lemann writes that Hoover has been remembered "as the man who was too rigidly conservative to react adeptly to the Depression, as the hapless foil to the great Franklin Roosevelt, and as the politician who managed to turn a Republican country into a Democratic one." Polls of historians and political scientists have generally ranked Hoover in the bottom third of presidents. A 2018 poll of the American Political Science Association's Presidents and Executive Politics section ranked Hoover as the 36th best president. A 2017 C-Span poll of historians also ranked Hoover as the 36th best president.

Although Hoover is generally regarded as having had a failed presidency, he has also received praise for his actions as a humanitarian and public official. Biographer Glen Jeansonne writes that Hoover was "one of the most extraordinary Americans of modern times," adding that Hoover "led a life that was a prototypical Horatio Alger story, except that Horatio Alger stories stop at the pinnacle of success." Biographer Kenneth Whyte writes that, "the question of where Hoover belongs in the American political tradition remains a loaded one to this day. While he clearly played important roles in the development of both the progressive and conservative traditions, neither side will embrace him for fear of contamination with the other."

The Herbert Hoover Presidential Library and Museum is located in West Branch, Iowa next to the Herbert Hoover National Historic Site. The library is one of thirteen presidential libraries run by the National Archives and Records Administration. The Hoover–Minthorn House, where Hoover lived from 1885 to 1891, is located in Newberg, Oregon. His Rapidan fishing camp in Virginia, which he donated to the government in 1933, is now a National Historic Landmark within the Shenandoah National Park. The Lou Henry and Herbert Hoover House, built in 1919 in Stanford, California, is now the official residence of the president of Stanford University, and a National Historic Landmark. Also located at Stanford is the Hoover Institution, a think tank and research institution started by Hoover.

Hoover has been memorialized in the names of several things, including the Hoover Dam on the Colorado River and numerous elementary, middle, and high schools across the United States. Two minor planets, 932 Hooveria and 1363 Herberta, are named in his honor. The Polish capital of Warsaw has a square named after Hoover, and the historic townsite of Gwalia, Western Australia contains the Hoover House Bed and Breakfast, where Hoover resided while managing and visiting the mine during the first decade of the twentieth century. A medicine ball game known as Hooverball is named for Hoover; it was invented by White House physician Admiral Joel T. Boone to help Hoover keep fit while serving as president.





</doc>
<doc id="13684" url="https://en.wikipedia.org/wiki?curid=13684" title="Hildegard of Bingen">
Hildegard of Bingen

Hildegard of Bingen (; ; 1098 – 17 September 1179), also known as Saint Hildegard and the Sibyl of the Rhine, was a German Benedictine abbess, writer, composer, philosopher, Christian mystic, visionary, and polymath. She is one of the best-known composers of sacred monophony, as well as the most-recorded in modern history. She has been considered by many in Europe to be the founder of scientific natural history in Germany.

Hildegard's fellow nuns elected her as "magistra" in 1136; she founded the monasteries of Rupertsberg in 1150 and Eibingen in 1165. She wrote theological, botanical, and medicinal texts, as well as letters, liturgical songs, and poems, while supervising miniature illuminations in the Rupertsberg manuscript of her first work, "Scivias". There are more surviving chants by Hildegard than by any other composer from the entire Middle Ages, and she is one of the few known composers to have written both the music and the words. One of her works, the "Ordo Virtutum", is an early example of liturgical drama and arguably the oldest surviving morality play. She is also noted for the invention of a constructed language known as "Lingua Ignota".

Although the history of her formal canonization is complicated, branches of the Roman Catholic Church have recognized her as a saint for centuries. On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization". On 7 October 2012, he named her a Doctor of the Church, in recognition of "her holiness of life and the originality of her teaching."

Hildegard was born around the year 1098, although the exact date is uncertain. Her parents were Mechtild of Merxheim-Nahet and Hildebert of Bermersheim, a family of the free lower nobility in the service of the Count Meginhard of Sponheim. Sickly from birth, Hildegard is traditionally considered their youngest and tenth child, although there are records of only seven older siblings. In her "Vita", Hildegard states that from a very young age she had experienced visions.

From early childhood, long before she undertook her public mission or even her monastic vows, Hildegard's spiritual awareness was grounded in what she called the umbra viventis lucis, the reflection of the living Light. Her letter to Guibert of Gembloux, which she wrote at the age of seventy- seven, describes her experience of this light with admirable precision:

“From my early childhood, before my bones, nerves and veins were fully strengthened, I have always seen this vision in my soul, even to the present time when I am more than seventy years old. In this vision my soul, as God would have it, rises up high into the vault of heaven and into the changing sky and spreads itself out among different peoples, although they are far away from me in distant lands and places. And because I see them this way in my soul, I observe them in accord with the shifting of clouds and other created things. I do not hear them with my outward ears, nor do I perceive them by the thoughts of my own heart or by any combination of my five senses, but in my soul alone, while my outward eyes are open. So I have never fallen prey to ecstasy in the visions, but I see them wide awake, day and night. And I am constantly fettered by sickness, and often in the grip of pain so intense that it threatens to kill me, but God has sustained me until now. The light which I see thus is not spatial, but it is far, far brighter than a cloud which carries the sun. I can measure neither height, nor length, nor breadth in it; and I call it "the reflection of the living Light." And as the sun, the moon, and the stars appear in water, so writings, sermons, virtues, and certain human actions take form for me and gleam."

Perhaps because of Hildegard's visions, or as a method of political positioning (or both), Hildegard's parents offered her as an oblate to the Benedictine monastery at the Disibodenberg, which had been recently reformed in the Palatinate Forest. The date of Hildegard's enclosure at the monastery is the subject of debate. Her "Vita" says she was professed with an older woman, Jutta, the daughter of Count Stephan II of Sponheim, at the age of eight. However, Jutta's date of enclosure is known to have been in 1112, when Hildegard would have been fourteen. Their vows were received by Bishop Otto Bamberg on All Saints' Day, 1112. Some scholars speculate that Hildegard was placed in the care of Jutta at the age of eight, and the two women were then enclosed together six years later.

In any case, Hildegard and Jutta were enclosed together at the Disibodenberg, and formed the core of a growing community of women attached to the male monastery. Jutta was also a visionary and thus attracted many followers who came to visit her at the cloister. Hildegard tells us that Jutta taught her to read and write, but that she was unlearned and therefore incapable of teaching Hildegard sound biblical interpretation. The written record of the "Life of Jutta" indicates that Hildegard probably assisted her in reciting the psalms, working in the garden and other handiwork, and tending to the sick. This might have been a time when Hildegard learned how to play the ten-stringed psaltery. Volmar, a frequent visitor, may have taught Hildegard simple psalm notation. The time she studied music could have been the beginning of the compositions she would later create.

Upon Jutta's death in 1136, Hildegard was unanimously elected as "magistra" of the community by her fellow nuns. Abbot Kuno of Disibodenberg asked Hildegard to be Prioress, which would be under his authority. Hildegard, however, wanted more independence for herself and her nuns, and asked Abbot Kuno to allow them to move to Rupertsberg. This was to be a move towards poverty, from a stone complex that was well established to a temporary dwelling place. When the abbot declined Hildegard's proposition, Hildegard went over his head and received the approval of Archbishop Henry I of Mainz. Abbot Kuno did not relent until Hildegard was stricken by an illness that kept her paralyzed and unable to move from her bed, an event that she attributed to God's unhappiness at her not following his orders to move her nuns to a new location in Rupertsberg. It was only when the Abbot himself could not move Hildegard that he decided to grant the nuns their own monastery. Hildegard and about twenty nuns thus moved to the St. Rupertsberg monastery in 1150, where Volmar served as provost, as well as Hildegard's confessor and scribe. In 1165 Hildegard founded a second monastery for her nuns at Eibingen.

Before Hildegard's death, a problem arose with the clergy of Mainz. A man buried in Rupertsburg had died after excommunication from the Church. Therefore, the clergy wanted to remove his body from the sacred ground. Hildegard did not accept this idea, replying that it was a sin and that the man had been reconciled to the church at the time of his death.

Hildegard said that she first saw "The Shade of the Living Light" at the age of three, and by the age of five she began to understand that she was experiencing visions. She used the term 'visio' (the Latin for "vision") to describe this feature of her experience and recognized that it was a gift that she could not explain to others. Hildegard explained that she saw all things in the light of God through the five senses: sight, hearing, taste, smell, and touch. Hildegard was hesitant to share her visions, confiding only to Jutta, who in turn told Volmar, Hildegard's tutor and, later, secretary. Throughout her life, she continued to have many visions, and in 1141, at the age of 42, Hildegard received a vision she believed to be an instruction from God, to "write down that which you see and hear." Still hesitant to record her visions, Hildegard became physically ill. The illustrations recorded in the book of Scivias were visions that Hildegard experienced, causing her great suffering and tribulations. In her first theological text, "Scivias" ("Know the Ways"), Hildegard describes her struggle within:

But I, though I saw and heard these things, refused to write for a long time through doubt and bad opinion and the diversity of human words, not with stubbornness but in the exercise of humility, until, laid low by the scourge of God, I fell upon a bed of sickness; then, compelled at last by many illnesses, and by the witness of a certain noble maiden of good conduct [the nun Richardis von Stade] and of that man whom I had secretly sought and found, as mentioned above, I set my hand to the writing. While I was doing it, I sensed, as I mentioned before, the deep profundity of scriptural exposition; and, raising myself from illness by the strength I received, I brought this work to a close – though just barely – in ten years. (...) And I spoke and wrote these things not by the invention of my heart or that of any other person, but as by the secret mysteries of God I heard and received them in the heavenly places. And again I heard a voice from Heaven saying to me, 'Cry out, therefore, and write thus!'

It was between November 1147 and February 1148 at the synod in Trier that Pope Eugenius heard about Hildegard's writings. It was from this that she received Papal approval to document her visions as revelations from the Holy Spirit giving her instant credence.

On 17 September 1179, when Hildegard died, her sisters claimed they saw two streams of light appear in the skies and cross over the room where she was dying.

Hildegard's hagiography, "Vita Sanctae Hildegardis", was compiled by the monk Theoderic of Echternach after Hildegard's death. He included the hagiographical work "Libellus" or "Little Book" begun by Godfrey of Disibodenberg. Godfrey had died before he was able to complete his work. Guibert of Gembloux was invited to finish the work; however, he had to return to his monastery with the project unfinished. Theoderic utilized sources Guibert had left behind to complete the "Vita".

Hildegard's works include three great volumes of visionary theology; a variety of musical compositions for use in liturgy, as well as the musical morality play "Ordo Virtutum"; one of the largest bodies of letters (nearly 400) to survive from the Middle Ages, addressed to correspondents ranging from popes to emperors to abbots and abbesses, and including records of many of the sermons she preached in the 1160s and 1170s; two volumes of material on natural medicine and cures; an invented language called the "Lingua ignota" ("unknown language"); and various minor works, including a gospel commentary and two works of hagiography.

Several manuscripts of her works were produced during her lifetime, including the illustrated Rupertsberg manuscript of her first major work, "Scivias" (lost since 1945); the Dendermonde Codex, which contains one version of her musical works; and the Ghent manuscript, which was the first fair-copy made for editing of her final theological work, the "Liber Divinorum Operum". At the end of her life, and probably under her initial guidance, all of her works were edited and gathered into the single Riesenkodex manuscript.

Hildegard's most significant works were her three volumes of visionary theology: "Scivias" ("Know the Ways", composed 1142–1151), "Liber Vitae Meritorum" ("Book of Life's Merits" or "Book of the Rewards of Life", composed 1158–1163); and "Liber Divinorum Operum" ("Book of Divine Works", also known as "De operatione Dei", "On God's Activity", composed 1163/4–1172 or 1174). In these volumes, the last of which was completed when she was well into her seventies, Hildegard first describes each vision, whose details are often strange and enigmatic, and then interprets their theological contents in the words of the "voice of the Living Light."

With permission from Abbot Kuno of Disibodenberg, she began journaling visions she had (which is the basis for "Scivias"). "Scivias" is a contraction of "Sci vias Domini" ("Know the Ways of the Lord"), and it was Hildegard's first major visionary work, and one of the biggest milestones in her life. Perceiving a divine command to "write down what you see and hear," Hildegard began to record and interpret her visionary experiences. In total, 26 visionary experiences were captured in this compilation.

"Scivias" is structured into three parts of unequal length. The first part (six visions) chronicles the order of God's creation: the Creation and Fall of Adam and Eve, the structure of the universe (famously described as the shape of an "egg"), the relationship between body and soul, God's relationship to his people through the Synagogue, and the choirs of angels. The second part (seven visions) describes the order of redemption: the coming of Christ the Redeemer, the Trinity, the Church as the Bride of Christ and the Mother of the Faithful in baptism and confirmation, the orders of the Church, Christ's sacrifice on the Cross and the Eucharist, and the fight against the devil. Finally, the third part (thirteen visions) recapitulates the history of salvation told in the first two parts, symbolized as a building adorned with various allegorical figures and virtues. It concludes with the Symphony of Heaven, an early version of Hildegard's musical compositions.

In early 1148, a commission was sent by the Pope to Disibodenberg to find out more about Hildegard and her writings. The commission found that the visions were authentic and returned to the Pope, with a portion of the "Scivias". Portions of the uncompleted work were read aloud to Pope Eugenius III at the Synod of Trier in 1148, after which he sent Hildegard a letter with his blessing. This blessing was later construed as papal approval for all of Hildegard's wide-ranging theological activities. Towards the end of her life, Hildegard commissioned a richly decorated manuscript of "Scivias" (the Rupertsberg Codex); although the original has been lost since its evacuation to Dresden for safekeeping in 1945, its images are preserved in a hand-painted facsimile from the 1920s.

In her second volume of visionary theology, composed between 1158 and 1163, after she had moved her community of nuns into independence at the Rupertsberg in Bingen, Hildegard tackled the moral life in the form of dramatic confrontations between the virtues and the vices. She had already explored this area in her musical morality play, "Ordo Virtutum", and the "Book of the Rewards of Life" takes up that play's characteristic themes. Each vice, although ultimately depicted as ugly and grotesque, nevertheless offers alluring, seductive speeches that attempt to entice the unwary soul into their clutches. Standing in our defence, however, are the sober voices of the Virtues, powerfully confronting every vicious deception.

Amongst the work's innovations is one of the earliest descriptions of purgatory as the place where each soul would have to work off its debts after death before entering heaven. Hildegard's descriptions of the possible punishments there are often gruesome and grotesque, which emphasize the work's moral and pastoral purpose as a practical guide to the life of true penance and proper virtue.

Hildegard's last and grandest visionary work had its genesis in one of the few times she experienced something like an ecstatic loss of consciousness. As she described it in an autobiographical passage included in her Vita, sometime in about 1163, she received "an extraordinary mystical vision" in which was revealed the "sprinkling drops of sweet rain" that John the Evangelist experienced when he wrote, "In the beginning was the Word..." (John 1:1). Hildegard perceived that this Word was the key to the "Work of God", of which humankind is the pinnacle. The "Book of Divine Works", therefore, became in many ways an extended explication of the Prologue to John's Gospel.

The ten visions of this work's three parts are cosmic in scale, to illustrate various ways of understanding the relationship between God and his creation. Often, that relationship is established by grand allegorical female figures representing Divine Love ("Caritas") or Wisdom ("Sapientia"). The first vision opens the work with a salvo of poetic and visionary images, swirling about to characterize God's dynamic activity within the scope of his work within the history of salvation. The remaining three visions of the first part introduce the famous image of a human being standing astride the spheres that make up the universe and detail the intricate relationships between the human as microcosm and the universe as macrocosm. This culminates in the final chapter of Part One, Vision Four with Hildegard's commentary on the Prologue to John's Gospel (John 1:1–14), a direct rumination on the meaning of "In the beginning was the Word..." The single vision that constitutes the whole of Part Two stretches that rumination back to the opening of Genesis, and forms an extended commentary on the seven days of the creation of the world told in Genesis 1–2:3. This commentary interprets each day of creation in three ways: literal or cosmological; allegorical or ecclesiological (i.e. related to the Church's history); and moral or tropological (i.e. related to the soul's growth in virtue). Finally, the five visions of the third part take up again the building imagery of "Scivias" to describe the course of salvation history. The final vision (3.5) contains Hildegard's longest and most detailed prophetic program of the life of the Church from her own days of "womanish weakness" through to the coming and ultimate downfall of the Antichrist.

Attention in recent decades to women of the medieval Church has led to a great deal of popular interest in Hildegard's music. In addition to the "Ordo Virtutum," sixty-nine musical compositions, each with its own original poetic text, survive, and at least four other texts are known, though their musical notation has been lost. This is one of the largest repertoires among medieval composers.
One of her better-known works, "Ordo Virtutum" ("Play of the Virtues"), is a morality play. It is uncertain when some of Hildegard's compositions were composed, though the "Ordo Virtutum" is thought to have been composed as early as 1151. It is an independent Latin morality play with music (82 songs); it does not supplement or pay homage to the Mass or the Office of a certain feast. The most significant part of this entire composition is, however, that the "Ordo virtutum" is the earliest known surviving musical drama that is not attached to a liturgy.

This entertainment was both performed and bemused by a select community of noblewomen and nuns. Even more fascinating about this piece, the devil has no music whatsoever in the plot of the play, he instead shouts and bellows all his lines. All other characters sing in monophonic plainchant. This includes Patriarchs, Prophets, A Happy Soul, A Unhappy Soul and A Penitent Soul along with 16 female Virtues (including Mercy, Innocence, Chasity, Obedience, Hope, and Faith).

The "Ordo virtutum" was probably performed as a manifestation of the theology Hildegard delineated in the "Scivias". The play serves as a group enchantment of the Christian story of sin, confession, repentance, and forgiveness. Notably, it is the female Virtues who restore the fallen to the community of the faithful, not the male Patriarchs or Prophets. This would have been a significant message to the nuns in Hildegard's convent. Scholars assert that the role of the Devil would have been played by Volmar, while Hildegard's nuns would have played the parts of Anima (the human souls) and the Virtues.

In addition to the "Ordo Virtutum", Hildegard composed many liturgical songs that were collected into a cycle called the "Symphonia armoniae celestium revelationum". The songs from the Symphonia are set to Hildegard's own text and range from antiphons, hymns, and sequences, to responsories. Her music is described as monophonic, that is, consisting of exactly one melodic line. Its style is characterized by soaring melodies that can push the boundaries of the more staid ranges of traditional Gregorian chant. Though Hildegard's music is often thought to stand outside the normal practices of monophonic monastic chant, current researchers are also exploring ways in which it may be viewed in comparison with her contemporaries, such as Hermannus Contractus. Another feature of Hildegard's music that both reflects twelfth-century evolutions of chant and pushes those evolutions further is that it is highly melismatic, often with recurrent melodic units. Scholars such as Margot Fassler, Marianne Richert Pfau, and Beverly Lomer also note the intimate relationship between music and text in Hildegard's compositions, whose rhetorical features are often more distinct than is common in twelfth-century chant. As with all medieval chant notation, Hildegard's music lacks any indication of tempo or rhythm; the surviving manuscripts employ late German style notation, which uses very ornamental neumes. The reverence for the Virgin Mary reflected in music shows how deeply influenced and inspired Hildegard of Bingen and her community were by the Virgin Mary and the saints.

The definition of viriditas or "greenness" is an earthly expression of the heavenly in an integrity that overcomes dualisms. This greenness or power of life appears frequently in Hildegard's works.

Despite Hildegard's self-professed view that her compositions have as their object the praise of God, one scholar has asserted that Hildegard made a close association between music and the female body in her musical compositions. According to him, the poetry and music of Hildegard's Symphonia would, therefore, be concerned with the anatomy of female desire thus described as Sapphonic, or pertaining to Sappho, connecting her to a history of female rhetoricians.

Hildegard's medicinal and scientific writings, though thematically complementary to her ideas about nature expressed in her visionary works, are different in focus and scope. Neither claim to be rooted in her visionary experience and its divine authority. Rather, they spring from her experience helping in and then leading the monastery's herbal garden and infirmary, as well as the theoretical information she likely gained through her wide-ranging reading in the monastery's library. As she gained practical skills in diagnosis, prognosis, and treatment, she combined physical treatment of physical diseases with holistic methods centered on "spiritual healing." She became well known for her healing powers involving the practical application of tinctures, herbs, and precious stones. She combined these elements with a theological notion ultimately derived from Genesis: all things put on earth are for the use of humans. In addition to her hands-on experience, she also gained medical knowledge, including elements of her humoral theory, from traditional Latin texts.

Hildegard catalogued both her theory and practice in two works. The first, "Physica," contains nine books that describe the scientific and medicinal properties of various plants, stones, fish, reptiles, and animals. This document is also thought to contain the first recorded reference of the usage of hops in beer as a preservative. The second, "Causae et Curae", is an exploration of the human body, its connections to the rest of the natural world, and the causes and cures of various diseases. Hildegard documented various medical practices in these books, including the use of bleeding and home remedies for many common ailments. She also explains remedies for common agricultural injuries such as burns, fractures, dislocations, and cuts. Hildegard may have used the books to teach assistants at the monastery. These books are historically significant because they show areas of medieval medicine that were not well documented because their practitioners (mainly women) rarely wrote in Latin. Her writings were commentated on by Mélanie Lipinska, a Polish scientist.

In addition to its wealth of practical evidence, "Causae et Curae" is also noteworthy for its organizational scheme. Its first part sets the work within the context of the creation of the cosmos and then humanity as its summit, and the constant interplay of the human person as microcosm both physically and spiritually with the macrocosm of the universe informs all of Hildegard's approach. Her hallmark is to emphasize the vital connection between the "green" health of the natural world and the holistic health of the human person. "Viriditas", or greening power, was thought to sustain human beings and could be manipulated by adjusting the balance of elements within a person. Thus, when she approached medicine as a type of gardening, it was not just as an analogy. Rather, Hildegard understood the plants and elements of the garden as direct counterparts to the humors and elements within the human body, whose imbalance led to illness and disease.

Thus, the nearly three hundred chapters of the second book of "Causae et Curae" "explore the etiology, or causes, of disease as well as human sexuality, psychology, and physiology." In this section, she give specific instructions for bleeding based on various factors, including gender, the phase of the moon (bleeding is best done when the moon is waning), the place of disease (use veins near diseased organ or body part) or prevention (big veins in arms), and how much blood to take (described in imprecise measurements, like "the amount that a thirsty person can swallow in one gulp"). She even includes bleeding instructions for animals to keep them healthy. In the third and fourth sections, Hildegard describes treatments for malignant and minor problems and diseases according to the humoral theory, again including information on animal health. The fifth section is about diagnosis and prognosis, which includes instructions to check the patient's blood, pulse, urine and stool. Finally, the sixth section documents a lunar horoscope to provide an additional means of prognosis for both disease and other medical conditions, such as conception and the outcome of pregnancy. For example, she indicates that a waxing moon is good for human conception and is also good for sowing seeds for plants (sowing seeds is the plant equivalent of conception). Elsewhere, Hildegard is even said to have stressed the value of boiling drinking water in an attempt to prevent infection.

As Hildegard elaborates the medical and scientific relationship between the human microcosm and the macrocosm of the universe, she often focuses on interrelated patterns of four: "the four elements (fire, air, water, and earth), the four seasons, the four humors, the four zones of the earth, and the four major winds." Although she inherited the basic framework of humoral theory from ancient medicine, Hildegard's conception of the hierarchical inter-balance of the four humors (blood, phlegm, black bile, and yellow bile) was unique, based on their correspondence to "superior" and "inferior" elements—blood and phlegm corresponding to the "celestial" elements of fire and air, and the two biles corresponding to the "terrestrial" elements of water and earth. Hildegard understood the disease-causing imbalance of these humors to result from the improper dominance of the subordinate humors. This disharmony reflects that introduced by Adam and Eve in the Fall, which for Hildegard marked the indelible entrance of disease and humoral imbalance into humankind. As she writes in "Causae et Curae" c. 42:

It happens that certain men suffer diverse illnesses. This comes from the phlegm which is superabundant within them. For if man had remained in paradise, he would not have had the "flegmata" within his body, from which many evils proceed, but his flesh would have been whole and without dark humor ["livor"]. However, because he consented to evil and relinquished good, he was made into a likeness of the earth, which produces good and useful herbs, as well as bad and useless ones, and which has in itself both good and evil moistures. From tasting evil, the blood of the sons of Adam was turned into the poison of semen, out of which the sons of man are begotten. And therefore their flesh is ulcerated and permeable [to disease]. These sores and openings create a certain storm and smoky moisture in men, from which the "flegmata" arise and coagulate, which then introduce diverse infirmities to the human body. All this arose from the first evil, which man began at the start, because if Adam had remained in paradise, he would have had the sweetest health, and the best dwelling-place, just as the strongest balsam emits the best odor; but on the contrary, man now has within himself poison and phlegm and diverse illnesses.

Hildegard also invented an alternative alphabet. "Litterae ignotae" ("Alternate Alphabet") was another work and was more or less a secret code, or even an intellectual code – much like a modern crossword puzzle today.

The text of her writing and compositions reveals Hildegard's use of this form of modified medieval Latin, encompassing many invented, conflated and abridged words. Because of her inventions of words for her lyrics and use of a constructed script, many conlangers look upon her as a medieval precursor.

Hildegard's "Lingua ignota" ("Unknown Language") was a composition that comprised a series of invented words that corresponded to an eclectic list of nouns. Scholars believe that Hildegard used her "Lingua Ignota" to increase solidarity among her nuns.

Maddocks claims that it is likely Hildegard learned simple Latin and the tenets of the Christian faith but was not instructed in the Seven Liberal Arts, which formed the basis of all education for the learned classes in the Middle Ages: the "Trivium" of grammar, dialectic, and rhetoric plus the "Quadrivium" of arithmetic, geometry, astronomy, and music. The correspondence she kept with the outside world, both spiritual and social, transcended the cloister as a space of spiritual confinement and served to document Hildegard's grand style and strict formatting of medieval letter writing.

Contributing to Christian European rhetorical traditions, Hildegard "authorized herself as a theologian" through alternative rhetorical arts. Hildegard was creative in her interpretation of theology. She believed that her monastery should exclude novices who were not from the nobility because she did not want her community to be divided on the basis of social status. She also stated that "woman may be made from man, but no man can be made without a woman."
Because of church limitation on public, discursive rhetoric, the medieval rhetorical arts included preaching, letter writing, poetry, and the encyclopedic tradition. Hildegard's participation in these arts speaks to her significance as a female rhetorician, transcending bans on women's social participation and interpretation of scripture. The acceptance of public preaching by a woman, even a well-connected abbess and acknowledged prophet, does not fit the stereotype of this time. Her preaching was not limited to the monasteries; she preached publicly in 1160 in Germany. (New York: Routledge, 2001, 9). She conducted four preaching tours throughout Germany, speaking to both clergy and laity in chapter houses and in public, mainly denouncing clerical corruption and calling for reform.

Many abbots and abbesses asked her for prayers and opinions on various matters. She traveled widely during her four preaching tours. She had several devoted followers, including Guibert of Gembloux, who wrote to her frequently and became her secretary after Volmar's death in 1173. Hildegard also influenced several monastic women, exchanging letters with Elisabeth of Schönau, a nearby visionary.

Hildegard corresponded with popes such as Eugene III and Anastasius IV, statesmen such as Abbot Suger, German emperors such as Frederick I Barbarossa, and other notable figures such as Saint Bernard of Clairvaux, who advanced her work, at the behest of her abbot, Kuno, at the Synod of Trier in 1147 and 1148. Hildegard of Bingen's correspondence is an important component of her literary output.

Hildegard was one of the first persons for whom the Roman canonization process was officially applied, but the process took so long that four attempts at canonization were not completed and she remained at the level of her beatification. Her name was nonetheless taken up in the Roman Martyrology at the end of the 16th century. Her feast day is 17 September. Numerous popes have referred to Hildegard as a saint, including Pope John Paul II and Pope Benedict XVI.

On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization," thus laying the groundwork for naming her a Doctor of the Church. On 7 October 2012, the feast of the Holy Rosary, the pope named her a Doctor of the Church, the fourth woman among 35 saints given that title by the Roman Catholic Church. He called her "perennially relevant" and "an authentic teacher of theology and a profound scholar of natural science and music."

Hildegard of Bingen also appears in the calendar of saints of various Anglican churches, such as that of the Church of England, in which she is commemorated on 17 September.

Hildegard's parish and pilgrimage church in Eibingen near Rüdesheim houses her relics.

In recent years, Hildegard has become of particular interest to feminist scholars. They note her reference to herself as a member of the weaker sex and her rather constant belittling of women. Hildegard frequently referred to herself as an unlearned woman, completely incapable of Biblical exegesis. Such a statement on her part, however, worked to her advantage because it made her statements that all of her writings and music came from visions of the Divine more believable, therefore giving Hildegard the authority to speak in a time and place where few women were permitted a voice. Hildegard used her voice to amplify the Church's condemnation of institutional corruption, in particular simony.

Hildegard has also become a figure of reverence within the contemporary New Age movement, mostly because of her holistic and natural view of healing, as well as her status as a mystic. Though her medical writings were long neglected, and then studied without reference to their context, she was the inspiration for Dr. Gottfried Hertzka's "Hildegard-Medicine", and is the namesake for June Boyce-Tillman's Hildegard Network, a healing center that focuses on a holistic approach to wellness and brings together people interested in exploring the links between spirituality, the arts, and healing. Her reputation as a medicinal writer and healer was also used by early feminists to argue for women's rights to attend medical schools. Hildegard's reincarnation has been debated since 1924 when Austrian mystic Rudolf Steiner lectured that a nun of her description was the past life of Russian poet-philosopher Vladimir Soloviev, whose Sophianic visions are often compared to Hildegard's. Sophiologist Robert Powell writes that hermetic astrology proves the match, while mystical communities in Hildegard's lineage include that of artist Carl Schroeder as studied by Columbia sociologist Courtney Bender and supported by reincarnation researchers Walter Semkiw and Kevin Ryerson.

Recordings and performances of Hildegard's music have gained critical praise and popularity since 1979. See Discography listed below.

The following modern musical works are directly linked to Hildegard and her music or texts:

The artwork "The Dinner Party" features a place setting for Hildegard.

In space, the minor planet 898 Hildegard is named for her.

In film, Hildegard has been portrayed by Patricia Routledge in a BBC documentary called "Hildegard of Bingen" (1994), by Ángela Molina in "Barbarossa" (2009) and by Barbara Sukowa in the film "Vision", directed by Margarethe von Trotta.

Hildegard was the subject of a 2012 fictionalized biographic novel "Illuminations" by Mary Sharatt.

The plant genus "Hildegardia" is named after her because of her contributions to herbal medicine.

Hildegard makes an appearance in "The Baby-Sitters Club #101: Claudia Kishi, Middle School Drop-Out" by Ann M. Martin, when Anna Stevenson dresses as Hildegard for Halloween.

A feature documentary film, "," was released by American director Michael M. Conti in 2014.

Grace McLean's "In the Green", a musical that examines Hildegard's early life, premiered off-Broadway in June 2019.





Primary Sources (in translation):
Secondary Sources:



</doc>
<doc id="13686" url="https://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum () is a city and municipality in the province of North Holland, Netherlands. Located in the heart of the Gooi, it is the largest urban centre in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.

Hilversum lies south-east of Amsterdam and north of Utrecht. The town is known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.

Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centres (such as Hilvertshof, Winkelcentrum Kerkelanden, De Riebeeckgalerij and Winkelcentrum Seinhorst). Locally, the town centre is known as "het dorp", which means "the village".

Hilversum is often called "media city", since it is the principal centre for radio and television broadcasting in the Netherlands, and is home to an extensive complex of radio and television studios and to the administrative headquarters of the multiple broadcasting organizations which make up the Netherlands Public Broadcasting system. Hilversum is also home to many newer commercial TV production companies. Radio Netherlands, which has been broadcasting worldwide via shortwave radio since the 1920s, is also based here.

The following is a list of organizations that have, or are continuing to, broadcast from studios in Hilversum:
One result of the town's history as an important radio transmission centre is that many older radio sets throughout Europe featured "Hilversum" as a pre-marked dial position on their tuning scales.

Dutch national voting in the Eurovision Song Contest is normally co-ordinated from Hilversum.

Hilversum has a variety of international schools, such as the "Violenschool" and "International School Hilversum "Alberdingk Thijm"". Also, Nike's, Hunkemöller's and Converse's European headquarters are located in Hilversum.

Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BC material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424 Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development.

The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. 

In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair.

In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."

For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. 

The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands.

Following the defeat of Allied forces in the Netherlands in 1940, and its occupation by Nazi Germany, Hilversum became the headquarters of the German Army ("Heer") in the Netherlands..

In 1948, NSF was taken over by Philips. However, Dutch radio broadcasting organizations (followed by television broadcasters during the 1950s) centralised their operations in Hilversum, providing a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.

In 1964, the population reached a record high – over 103,000 people called Hilversum home. However, the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. 

After the 1960s, the population gradually declined, until stabilising at around 85,000. Several factors other than the slump in manufacturing have featured in this decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat (""). The third reason for this decline of the population was because the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.

Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.

Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from Leefbaar Hilversum teamed up with Leefbaar Utrecht leaders to found a national Leefbaar Nederland party. By strange coincidence, in 2002 the most vocal Leefbaar Rotterdam politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.

The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo (""), the largest man-made wildlife crossing in the world.

The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn; in 2015, a gunman carrying a false pistol stormed into Nederlandse Omroep Stichting's headquarters, demanding airtime on the evening news.

The population declined from 103,000 in 1964 to 84,000 in 2006, but rose again to 90.000 in 2018. The decline is mostly due to the fact that families are smaller these days.

The large Catholic neo-gothic St. Vitus church (P.J.H. Cuypers, 1892, bell tower 96 metres).

The city played host to many landscape artists during the 19th century, including Barend Cornelis Koekkoek.

The 1958 Eurovision Song Contest took place in Hilversum.

Once held a major European Tennis tournament in the 50s and 60s.

Hilversum is well connected to the Dutch railway network, and has three stations.
Most local and regional buses are operated by Connexxion, but two of the bus routes are operated by Syntus Utrecht and two others by U-OV and Pouw Vervoer. Regional bus route 320 is operated by both Connexxion and Pouw Vervoer.
In 2018, major road works started to make room for a new BRT bus lane from Hilversum to Huizen, set to open in early 2021.

The municipal council of Hilversum consists of 37 seats, which are divided as followed since the last local election of 2018:


Government

After the 2018 elections, the municipal government was made up of aldermen from the political parties Hart voor Hilversum, D66 and VVD.

The mayor of Hilversum is Pieter Broertjes, former lead editor of the Volkskrant, a nationwide distributed newspaper.

It was the first city with a "Leefbaar" party (which was intended as just a local party). Today, Leefbaar Hilversum has been reduced to only 1 seat, but some other parties have their origins in Leefbaar Hilversum:

Notable people born in Hilversum:







</doc>
<doc id="13688" url="https://en.wikipedia.org/wiki?curid=13688" title="The Hound of Heaven">
The Hound of Heaven

"The Hound of Heaven" is a 182-line poem written by English poet Francis Thompson (1859–1907). The poem became famous and was the source of much of Thompson's posthumous reputation. The poem was first published in Thompson's first volume of poems in 1893. It was included in the "Oxford Book of English Mystical Verse" (1917). Thompson's work was praised by G. K. Chesterton, and it was also an influence on J. R. R. Tolkien, who presented a paper on Thompson in 1914.

This Christian poem has been described as follows:
"The name is strange. It startles one at first. It is so bold, so new, so fearless. It does not attract, rather the reverse. But when one reads the poem this strangeness disappears. The meaning is understood. As the hound follows the hare, never ceasing in its running, ever drawing nearer in the chase, with unhurrying and imperturbed pace, so does God follow the fleeing soul by His Divine grace. And though in sin or in human love, away from God it seeks to hide itself, Divine grace follows after, unwearyingly follows ever after, till the soul feels its pressure forcing it to turn to Him alone in that never ending pursuit." J.F.X. O'Conor, S.J.






</doc>
<doc id="13692" url="https://en.wikipedia.org/wiki?curid=13692" title="History of the Internet">
History of the Internet

The history of the Internet has its origin in the efforts to interconnect computer networks that arose from research and development in the United States and involved international collaboration, particularly with researchers in the United Kingdom and France.

Computer science was an emerging discipline in the late 1950s that began to consider time-sharing between users and, later, the possibility of achieving this over wide area networks. Independently, Paul Baran proposed a distributed network based on data in message blocks in the early 1960s and Donald Davies first demonstrated packet switching in 1967 at the National Physics Laboratory (NPL) in the UK, which became a testbed for research for almost two decades. The U.S. Department of Defense awarded contracts in 1969 for the development of the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts. ARPANET adopted the packet switching technology proposed by Davies and Baran, underpinned by mathematical work by Leonard Kleinrock. The network was built by Bolt, Beranek, and Newman.

Early packet switching networks such as the NPL network, ARPANET, Merit Network, CYCLADES, and Telenet in the early 1970s researched and provided data networking. The ARPANET project and international working groups led to the development of protocols for internetworking, in which multiple separate networks could be joined into a network of networks, which produced various standards. Vint Cerf, at Stanford University, and Bob Kahn, at ARPA, developed the Internet Protocol (IP) and Transmission Control Protocol (TCP) in 1973, the two original protocols of the Internet protocol suite. The design included concepts from the French CYCLADES project directed by Louis Pouzin. ARPANET converted to TCP/IP and other networks attached to it, and each other, via those protocols over time, forming the Internet of today.

In the early 1980s the NSF funded national supercomputing centers at several universities in the United States and provided interconnectivity in 1986 with the NSFNET project, which created network access to these supercomputer sites for research and academic organizations in the United States and internationally. Commercial Internet service providers (ISPs) began to emerge in the very late 1980s. The ARPANET was decommissioned in 1990. Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990. The NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic.

Research at CERN in Switzerland by British computer scientist Tim Berners-Lee in 1989-90 resulted in the World Wide Web, linking hypertext documents into an information system, accessible from any node on the network. Since the mid-1990s, the Internet has had a revolutionary impact on culture, commerce, and technology, including the rise of near-instant communication by electronic mail, instant messaging, voice over Internet Protocol (VoIP) telephone calls, two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1 Gbit/s, 10 Gbit/s, or more. The Internet's takeover of the global communication landscape was rapid in historical terms: it only communicated 1% of the information flowing through two-way telecommunications networks in the year 1993, 51% by 2000, and more than 97% of the telecommunicated information by 2007. Today, the Internet continues to grow, driven by ever greater amounts of online information, commerce, entertainment, and social networking. However, the future of the global network may be shaped by regional differences.

The concept of data communication – transmitting data between two different places through an electromagnetic medium such as radio or an electric wire – pre-dates the introduction of the first computers. Such communication systems were typically limited to point to point communication between two end devices. Semaphore lines, telegraph systems and telex machines can be considered early precursors of this kind of communication. The telegraph in the late 19th century was the first fully digital communication system.

Early computers had a central processing unit and remote terminals. As the technology evolved, new systems were devised to allow communication over longer distances (for terminals) or with higher speed (for interconnection of local devices) that were necessary for the mainframe computer model. These technologies made it possible to exchange data (such as files) between remote computers. However, the point-to-point communication model was limited, as it did not allow for direct communication between any two arbitrary systems; a physical link was necessary. The technology was also considered unsafe for strategic and military use because there were no alternative paths for the communication in case of an enemy attack.

Fundamental theoretical work in data transmission and information theory was developed by Claude Shannon, Harry Nyquist, and Ralph Hartley in the early 20th century. Information theory, as enunciated by Shannon in 1948, provided a firm theoretical underpinning to understand the trade-offs between signal-to-noise ratio, bandwidth, and error-free transmission in the presence of noise, in telecommunications technology.

The development of transistor technology was fundamental to a new generation of electronic devices that later effected almost every aspect of the human experience. The long-sought realization of the field-effect transistor, in form of the MOS transistor (MOSFET), by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, brought new opportunities for miniaturization and mass-production for a wide range of uses. It became the basic building block of the information revolution and the information age, and laid the foundation for power electronic technology that later enabled the development of wireless Internet technology. Network bandwidth has been doubling every 18 months since the 1970s, which found expression in Edholm's law, similar to the scaling expressed by Moore's law for semiconductors.

With limited exceptions, the earliest computers were connected directly to terminals used by individual users, typically in the same building or site.

Wide area networks (WANs) emerged during the 1950s and became established during the 1960s.

Christopher Strachey, who became Oxford University's first professor of computation, filed a patent application for time-sharing in February 1959. He passed the concept on to J. C. R. Licklider at a UNESCO-sponsored conference on Information Processing in Paris that year. Licklider, Vice President at Bolt Beranek and Newman, Inc., discussed a computer network in his January 1960 paper "Man-Computer Symbiosis":

In August 1962, Licklider and Welden Clark published the paper "On-Line Man-Computer Communication" which was one of the first descriptions of a networked future.

In October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within DARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos in 1963 describing a distributed network to the IPTO staff, whom he called "Members and Affiliates of the Intergalactic Computer Network". 

Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for one of his successors, Robert Taylor, to initiate the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.

The issue of connecting separate physical networks to form one logical network was the first of many problems. Early networks used message switched systems that required rigid routing structures prone to single point of failure. In the 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information transmitted across Baran's network would be divided into what he called "message blocks". Independently, Donald Davies (National Physical Laboratory, UK), proposed and was the first to put into practice a local area network based on what he called packet switching, the term that would ultimately be adopted. Larry Roberts applied Davies' concepts of packet switching for the ARPANET wide area network, and sought input from Paul Baran and Leonard Kleinrock. Kleinrock subsequently developed the mathematical theory behind the performance of this technology building on his earlier work on queueing theory.

Packet switching is a rapid store and forward networking design that divides messages up into arbitrary packets, with routing decisions made per-packet. It provides better bandwidth utilization and response times than the traditional circuit-switching technology used for telephony, particularly on resource-limited interconnection links.

The software for establishing links between network sites in the ARPANET was the Network Control Program (NCP), completed in c. 1970. Further development in the early 1970s by Robert E. Kahn and Vint Cerf let to the formulation of the "Transmission Control Program", and its specification in December 1974 in . This work also coined the terms "catenet" (concatenated network) and "internet" as a contraction of "internetworking", which describe the interconnection of multiple networks. This software was monolithic in design using two simplex communication channels for each user session. The software was redesigned as a modular protocol stack, using full-duplex channels. Originally named IP/TCP it was installed in the ARPANET for production use in January 1983.

Following discussions with J. C. R. Licklider in 1965, Donald Davies became interested in data communications for computer networks. Later that year, at the National Physical Laboratory (United Kingdom), Davies designed and proposed a national data network based on packet switching. The following year, he described the use of an "Interface computer" to act as a router. The proposal was not taken up nationally but by 1967, a pilot experiment had demonstrated the feasibility of packet switched networks.

By 1969 he had begun building the Mark I packet-switched network to meet the needs of the multidisciplinary laboratory and prove the technology under operational conditions. In 1976, 12 computers and 75 terminal devices were attached, and more were added until the network was replaced in 1986. NPL, followed by ARPANET, were the first two networks in the world to use packet switching, and were interconnected in the early 1970s.

Robert Taylor was promoted to the head of the information processing office at Defense Advanced Research Projects Agency (DARPA) in June 1966. He intended to realize Licklider's ideas of an interconnected networking system. As part of the information processing office's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at University of California, Berkeley, and one for the Compatible Time-Sharing System project at Massachusetts Institute of Technology (MIT). Taylor's identified need for networking became obvious from the waste of resources apparent to him. 

Bringing in Larry Roberts from MIT, he initiated a project to build such a network. Roberts and Thomas Merrill had been researching wide area networking for computer time-sharing. At the first ACM Symposium on Operating Systems Principles in October 1967, Roberts presented a proposal for the "ARPA net", a distributed network using Interface Message Processors to create a message switching network. At the conference, Roger Scantlebury presented Donald Davies' work on packet switching and mentioned the work of Paul Baran at RAND. Based on theoretical work by Leonard Kleinrock that supported the viability of packet switching, Roberts incorporated their concepts into the ARPANET design and upgraded the proposed communications speed to be used from 2.4 kbps to 50 kbps.

ARPA awarded the contract to build the network to Bolt Beranek & Newman and the first ARPANET link was established between the University of California, Los Angeles (UCLA) and the Stanford Research Institute at 22:30 hours on October 29, 1969.

By December 5, 1969, a 4-node network was connected by adding the University of Utah and the University of California, Santa Barbara. Building on ideas developed in ALOHAnet, the ARPANET grew rapidly. By 1981, the number of hosts had grown to 213, with a new host being added approximately every twenty days.

ARPANET development was centered around the Request for Comments (RFC) process, still used today for proposing and distributing Internet Protocols and Systems. RFC 1, entitled "Host Software", was written by Steve Crocker from the University of California, Los Angeles, and published on April 7, 1969. These early years were documented in the 1972 film .

ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used. The early ARPANET used the Network Control Program (NCP, sometimes Network Control Protocol) rather than TCP/IP. On January 1, 1983, known as flag day, NCP on the ARPANET was replaced by the more flexible and powerful family of TCP/IP protocols, marking the start of the modern Internet.

International collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks. Notable exceptions were the "Norwegian Seismic Array" (NORSAR) in 1972, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter Kirstein's research group in the UK, initially at the Institute of Computer Science, London University and later at University College London.

The Merit Network was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.

The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture.

Based on ARPA's research, packet switching network standards were developed by the International Telecommunication Union (ITU) in the form of X.25 and related standards. While using packet switching, X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET. The initial ITU Standard on X.25 was approved in March 1976.

The British Post Office, Western Union International and Tymnet collaborated to create the first international packet switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.

Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.

The first public dial-in networks used asynchronous TTY terminal protocols to reach a concentrator operated in the public network. Some networks, such as CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.

In 1979, two students at Duke University, Tom Truscott and Jim Ellis, originated the idea of using Bourne shell scripts to transfer news and messages on a serial line UUCP connection with nearby University of North Carolina at Chapel Hill. Following public release of the software in 1980, the mesh of UUCP hosts forwarding on the Usenet news rapidly expanded. UUCPnet, as it would later be named, also created gateways and links between FidoNet and dial-up BBS hosts. UUCP networks spread quickly due to the lower costs involved, ability to use existing leased lines, X.25 links or even ARPANET connections, and the lack of strict use policies compared to later networks like CSNET and Bitnet. All connects were local. By 1981 the number of UUCP hosts had grown to 550, nearly doubling to 940 in 1984. – Sublink Network, operating since 1987 and officially founded in Italy in 1989, based its interconnectivity upon UUCP to redistribute mail and news groups messages throughout its Italian nodes (about 100 at the time) owned both by private individuals and small companies. Sublink Network represented possibly one of the first examples of the Internet technology becoming progress through popular diffusion.

With so many different network methods, something was needed to unify them. Robert E. Kahn of DARPA and ARPANET recruited Vinton Cerf of Stanford University to work with him on the problem. By 1973, they had worked out a fundamental reformulation, where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann, , Louis Pouzin (designer of the CYCLADES network), and his graduate students Judy Estrin, Richard Karp, Yogen Dalal and Carl Sunshine with important work on this design. This Stanford research team became known as the International Network Working Group, formed in 1973 and led by Cerf.

The specification of the resulting protocol, the Transmission Control Protocol (TCP), was published as by the Network Working Group in December 1974. It contains the first attested use of the term "internet", as a shorthand for "internetworking".

Between 1976 and 1977, Yogen Dalal proposed separating TCP's routing and transmission control functions into two discrete layers, which led to the splitting of TCP into the TCP and IP protocols, and the development of TCP/IP.

With the role of the network reduced to a core of functionality, it became possible to exchange traffic with other network independently from their detailed characteristics, thereby solving Kahn's initial problem. DARPA agreed to fund development of prototype software, and after several years of work, the first demonstration of a gateway between the Packet Radio network in the SF Bay area and the ARPANET was conducted by the Stanford Research Institute. On November 22, 1977 a three network demonstration was conducted including the ARPANET, the SRI's Packet Radio Van on the Packet Radio Network and the Atlantic Packet Satellite network.

Stemming from the first specifications of TCP in 1974, TCP/IP emerged in 1978 in nearly its final form, as used for the first decades of the Internet. which is described in IETF publication RFC 791 (September 1981).
IPv4 uses 32-bit addresses which limits the address space to 2 addresses, i.e. addresses. The last available IPv4 address was assigned in January 2011. IPv4 is being replaced by its successor, called "IPv6", which uses 128 bit addresses, providing 2 addresses, i.e. . This is a vastly increased address space. The shift to IPv6 is expected to take many years, decades, or perhaps longer, to complete, since there were four billion machines with IPv4 when the shift began.

The associated standards for IPv4 were published by 1981 as RFCs 791, 792 and 793, and adopted for use. DARPA sponsored or encouraged the development of TCP/IP implementations for many operating systems and then scheduled a migration of all hosts on all of its packet networks to TCP/IP. On January 1, 1983, known as flag day, TCP/IP protocols became the standard for the ARPANET, replacing the earlier NCP protocol.

After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting edge research and development, not running a communications utility. Eventually, in July 1975, the network had been turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.

The networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, and even to a growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were.

Several other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.
NASA developed the TCP/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.

In 1981 NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP/IP, and ran TCP/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange.

In 1986, the NSF created NSFNET, a 56 kbit/s backbone to support the NSF-sponsored supercomputing centers. The NSFNET also provided support for the creation of regional research and education networks in the United States, and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990. NSFNET was expanded and upgraded to 45 Mbit/s in 1991, and was decommissioned in 1995 when it was replaced by backbones operated by several commercial Internet service providers.

The research and academic community continues to develop and use advanced networks such as Internet2 in the United States and JANET in the United Kingdom.

The term "internet" was reflected in the first RFC published on the TCP protocol (RFC 675: Internet Transmission Control Program, December 1974) as a short form of "internetworking", when the two terms were used interchangeably. In general, an internet was a collection of networks linked by a common protocol. In the time period when the ARPANET was connected to the newly formed NSFNET project in the late 1980s, the term was used as the name of the network, Internet, being the large and global TCP/IP network.

As interest in networking grew by needs of collaboration, exchange of data, and access of remote computing resources, the TCP/IP technologies spread throughout the rest of the world. The hardware-agnostic approach in TCP/IP supported the use of existing network infrastructure, such as the IPSS X.25 network, to carry Internet traffic.

Many sites unable to link directly to the Internet created simple gateways for the transfer of electronic mail, the most important application of the time. Sites with only intermittent connections used UUCP or FidoNet and relied on the gateways between these networks and the Internet. Some gateway services went beyond simple mail peering, such as allowing access to File Transfer Protocol (FTP) sites via UUCP or mail.

Finally, routing technologies were developed for the Internet to remove the remaining centralized routing aspects. The Exterior Gateway Protocol (EGP) was replaced by a new protocol, the Border Gateway Protocol (BGP). This provided a meshed topology for the Internet and reduced the centric architecture which ARPANET had emphasized. In 1994, Classless Inter-Domain Routing (CIDR) was introduced to support better conservation of address space which allowed use of route aggregation to decrease the size of routing tables.

In 1982, one year earlier than the ARPANET, Peter Kirstein replaced the University College London transatlantic satellite links with TCP/IP over IPSS.

Between 1984 and 1988 CERN began installation and operation of TCP/IP to interconnect its major internal computer systems, workstations, PCs and an accelerator control system. CERN continued to operate a limited self-developed system (CERNET) internally and several incompatible (typically proprietary) network protocols externally. There was considerable resistance in Europe towards more widespread use of TCP/IP, and the CERN TCP/IP intranets remained isolated from the Internet until 1989.

In 1988, Daniel Karrenberg, from Centrum Wiskunde & Informatica (CWI) in Amsterdam, visited Ben Segal, CERN's TCP/IP Coordinator, looking for advice about the transition of the European side of the UUCP Usenet network (much of which ran over X.25 links) over to TCP/IP. In 1987, Ben Segal had met with Len Bosack from the then still small company Cisco about purchasing some TCP/IP routers for CERN, and was able to give Karrenberg advice and forward him on to Cisco for the appropriate hardware. This expanded the European portion of the Internet across the existing UUCP networks, and in 1989 CERN opened its first external TCP/IP connections. This coincided with the creation of Réseaux IP Européens (RIPE), initially a group of IP network administrators who met regularly to carry out coordination work together. Later, in 1992, RIPE was formally registered as a cooperative in Amsterdam.

At the same time as the rise of internetworking in Europe, ad hoc networking to ARPA and in-between Australian universities formed, based on various technologies such as X.25 and UUCPNet. These were limited in their connection to the global networks, due to the cost of making individual international UUCP dial-up or X.25 connections. In 1989, Australian universities joined the push towards using IP protocols to unify their networking infrastructures. AARNet was formed in 1989 by the Australian Vice-Chancellors' Committee and provided a dedicated IP based network for Australia.

The Internet began to penetrate Asia in the 1980s. In May 1982 South Korea became the second country to successfully set up TCP/IP IPv4 network. Japan, which had built the UUCP-based network JUNET in 1984, connected to NSFNET in 1989. It hosted the annual meeting of the Internet Society, INET'92, in Kobe. Singapore developed TECHNET in 1990, and Thailand gained a global Internet connection between Chulalongkorn University and UUNET in 1992.

While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they are building organizations for Internet resource administration and sharing operational experience, as more and more transmission facilities go into place.

At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.

In August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit/s, serving a Sun host computer and twelve US Robotics dial-up modems.

In 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Ivory Coast and Benin in 1998.

Africa is building an Internet infrastructure. AFRINIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.

There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.

The Asia Pacific Network Information Centre (APNIC), headquartered in Australia, manages IP address allocation for the continent. APNIC sponsors an operational forum, the Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT).

South Korea's first Internet system, the System Development Network (SDN) began operation on 15 May 1982. SDN was connected to the rest of the world in August 1983 using UUCP (Unixto-Unix-Copy); connected to CSNET in December 1984; and formally connected to the U.S. Internet in 1990.

In 1991, the People's Republic of China saw its first TCP/IP college network, Tsinghua University's TUNET. The PRC went on to make its first global Internet connection in 1994, between the Beijing Electro-Spectrometer Collaboration and Stanford University's Linear Accelerator Center. However, China went on to implement its own digital divide by implementing a country-wide content filter.

As with the other regions, the Latin American and Caribbean Internet Addresses Registry (LACNIC) manages the IP address space and other resources for its area. LACNIC, headquartered in Uruguay, operates DNS root, reverse DNS, and other key services.

Initially, as with its predecessor networks, the system that would evolve into the Internet was primarily for government and government body use.

However, interest in commercial use of the Internet quickly became a commonly debated topic. Although commercial use was forbidden, the exact definition of commercial use was unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. (Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.)
As a result, during the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. The first commercial dialup ISP in the United States was The World, which opened in 1989.

In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, , which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.

By 1990, ARPANET's goals had been fulfilled and new networking technologies exceeded the original scope and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point of the Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995 when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service and the service ended. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.

The World Wide Web (sometimes abbreviated "www" or "W3") is an information space where documents and other web resources are identified by URIs, interlinked by hypertext links, and can be accessed via the Internet using a web browser and (more recently) web-based applications. It has become known simply as "the Web". As of the 2010s, the World Wide Web is the primary tool billions use to interact on the Internet, and it has changed people's lives immeasurably.

Precursors to the web browser emerged in the form of hyperlinked applications during the mid and late 1980s (the bare concept of hyperlinking had by then existed for some decades). Following these, Tim Berners-Lee is credited with inventing the World Wide Web in 1989 and developing in 1990 both the first web server, and the first web browser, called WorldWideWeb (no spaces) and later renamed Nexus. Many others were soon developed, with Marc Andreessen's 1993 Mosaic (later Netscape), being particularly easy to use and install, and often credited with sparking the Internet boom of the 1990s. Other major web browsers have been Internet Explorer, Firefox, Google Chrome, Microsoft Edge, Opera and Safari.

NCSA Mosaic was a graphical browser which ran on several popular office and home computers. It is credited with first bringing multimedia content to non-technical users by including images and text on the same page, unlike previous browser designs; Marc Andreessen, its creator, also established the company that in 1994, released Netscape Navigator, which resulted in one of the early browser wars, when it ended up in a competition for dominance (which it lost) with Microsoft Windows' Internet Explorer. Commercial use restrictions were lifted in 1995. The online service America Online (AOL) offered their users a connection to the Internet via their own internal browser.

During the first decade or so of the public Internet, the immense changes it would eventually enable in the 2000s were still nascent. In terms of providing context for this period, mobile cellular devices ("smartphones" and other cellular devices) which today provide near-universal access, were used for business and not a routine household item owned by parents and children worldwide. Social media in the modern sense had yet to come into existence, laptops were bulky and most households did not have computers. Data rates were slow and most people lacked means to video or digitize video; media storage was transitioning slowly from analog tape to digital optical discs (DVD and to an extent still, floppy disc to CD). Enabling technologies used from the early 2000s such as PHP, modern JavaScript and Java, technologies such as AJAX, HTML 4 (and its emphasis on CSS), and various software frameworks, which enabled and simplified speed of web development, largely awaited invention and their eventual widespread adoption.

The Internet was widely used for mailing lists, emails, e-commerce and early popular online shopping (Amazon and eBay for example), online forums and bulletin boards, and personal websites and blogs, and use was growing rapidly, but by more modern standards the systems used were static and lacked widespread social engagement. It awaited a number of events in the early 2000s to change from a communications technology to gradually develop into a key part of global society's infrastructure.

Typical design elements of these "Web 1.0" era websites included: Static pages instead of dynamic HTML; content served from filesystems instead of relational databases; pages built using Server Side Includes or CGI instead of a web application written in a dynamic programming language; HTML 3.2-era structures such as frames and tables to create page layouts; online guestbooks; overuse of GIF buttons and similar small graphics promoting particular items; and HTML forms sent via email. (Support for server side scripting was rare on shared servers so the usual feedback mechanism was via email, using mailto forms and their email program.

During the period 1997 to 2001, the first speculative investment bubble related to the Internet took place, in which "dot-com" companies (referring to the ".com" top level domain used by businesses) were propelled to exceedingly high valuations as investors rapidly stoked stock values, followed by a market crash; the first dot-com bubble. However this only temporarily slowed enthusiasm and growth, which quickly recovered and continued to grow.

The changes that would propel the Internet into its place as a social system took place during a relatively short period of no more than five years, starting from around 2004. They included:

and shortly after (approximately 2007–2008 onward):

With the call to Web 2.0, the period up to around 2004–2005 was retrospectively named and described by some as Web 1.0.

The term "Web 2.0" describes websites that emphasize user-generated content (including user-to-user interaction), usability, and interoperability. It first appeared in a January 1999 article called "Fragmented Future" written by Darcy DiNucci, a consultant on electronic information design, where she wrote:

The term resurfaced during 2002 – 2004, and gained prominence in late 2004 following presentations by Tim O'Reilly and Dale Dougherty at the first Web 2.0 Conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the "Web as Platform", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that "customers are building your business for you". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be "harnessed" to create value.

Web 2.0 does not refer to an update to any technical specification, but rather to cumulative changes in the way Web pages are made and used. Web 2.0 describes an approach, in which sites focus substantially upon allowing users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking sites, blogs, wikis, folksonomies, video sharing sites, hosted services, Web applications, and mashups. Terry Flew, in his 3rd Edition of "New Media" described what he believed to characterize the differences between Web 1.0 and Web 2.0:
This era saw several household names gain prominence through their community-oriented operation – YouTube, Twitter, Facebook, Reddit and Wikipedia being some examples.

The process of change that generally coincided with "Web 2.0" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices. This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information "on the move" – and used socially, as opposed to items on a desk at home or just used for work.

Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as "m.website.com") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term "App" emerged (short for "Application program" or "Program") as did the "App store".

The first Internet link into low earth orbit was established on January 22, 2010 when astronaut T. J. Creamer posted the first unassisted update to his Twitter account from the International Space Station, marking the extension of the Internet into space. (Astronauts at the ISS had used email and Twitter before, but these messages had been relayed to the ground through a NASA data link before being posted by a human proxy.) This personal Web access, which NASA calls the Crew Support LAN, uses the space station's high-speed Ku band microwave link. To surf the Web, astronauts can use a station laptop computer to control a desktop computer on Earth, and they can talk to their families and friends on Earth using Voice over IP equipment.

Communication with spacecraft beyond earth orbit has traditionally been over point-to-point links through the Deep Space Network. Each such data link must be manually scheduled and configured. In the late 1990s NASA and Google began working on a new network protocol, Delay-tolerant networking (DTN) which automates this process, allows networking of spaceborne transmission nodes, and takes the fact into account that spacecraft can temporarily lose contact because they move behind the Moon or planets, or because space weather disrupts the connection. Under such conditions, DTN retransmits data packages instead of dropping them, as the standard TCP/IP Internet Protocol does. NASA conducted the first field test of what it calls the "deep space internet" in November 2008. Testing of DTN-based communications between the International Space Station and Earth (now termed Disruption-Tolerant Networking) has been ongoing since March 2009, and is scheduled to continue until March 2014.

This network technology is supposed to ultimately enable missions that involve multiple spacecraft where reliable inter-vessel communication might take precedence over vessel-to-earth downlinks. According to a February 2011 statement by Google's Vint Cerf, the so-called "Bundle protocols" have been uploaded to NASA's EPOXI mission spacecraft (which is in orbit around the Sun) and communication with Earth has been tested at a distance of approximately 80 light seconds.

As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. Each constituent network chooses the technologies and protocols it deploys from the technical standards that are developed by the Internet Engineering Task Force (IETF). However, successful interoperation of many networks requires certain parameters that must be common throughout the network. For managing such parameters, the Internet Assigned Numbers Authority (IANA) oversees the allocation and assignment of various technical identifiers. In addition, the Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for the two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.

The IANA function was originally performed by USC Information Sciences Institute (ISI), and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. ISI's Jonathan Postel managed the IANA, served as RFC Editor and performed other key roles until his premature death in 1998.

As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by ISI's Paul Mockapetris in 1983. The Defense Data Network—Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.

The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the "growth of the Internet and its increasing globalization" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of "continental dimensions"). Registries would be "unbiased and widely recognized by network providers and subscribers" within their region.
The RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.

Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.

Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.

In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry / registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.

The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).

The IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the work of the IETF is organized into "Working Groups". Standardization efforts of the Working Groups are often adopted by the Internet community, but the IETF does not control or patrol the Internet.

The IETF grew out of quarterly meeting of U.S. government-funded researchers, starting in January 1986. Non-government representatives were invited by the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth meeting in February 1987. The seventh meeting in July 1987 was the first meeting with more than one hundred attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, The Netherlands, in July 1993. Today, the IETF meets three times per year and attendance has been as high as ca. 2,000 participants. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is typically ca. 50%, even at meetings held in the United States.

The IETF is not a legal entity, has no governing board, no members, and no dues. The closest status resembling membership is being on an IETF or Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer term research issues.

Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, "Host Software", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.

RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that "obsoletes" the original.

The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 "to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". With offices near Washington, DC, USA, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form "chapters" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.

ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision-making.

Since the 1990s, the Internet's governance and organization has been of global importance to governments, commerce, civil society, and individuals. The organizations which held control of certain technical aspects of the Internet were the successors of the old ARPANET oversight and the current decision-makers in the day-to-day technical aspects of the network. While recognized as the administrators of certain aspects of the Internet, their roles and their decision-making authority are limited and subject to increasing international scrutiny and increasing objections. These objections have led to the ICANN removing themselves from relationships with first the University of Southern California in 2000, and in September 2009, gaining autonomy from the US government by the ending of its longstanding agreements, although some contractual obligations with the U.S. Department of Commerce continued. Finally, on October 1, 2016 ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA), allowing oversight to pass to the global Internet community.

The IETF, with financial and organizational support from the Internet Society, continues to serve as the Internet's ad-hoc standards body and issues Request for Comments.

In November 2005, the World Summit on the Information Society, held in Tunis, called for an Internet Governance Forum (IGF) to be convened by United Nations Secretary General. The IGF opened an ongoing, non-binding conversation among stakeholders representing governments, the private sector, civil society, and the technical and academic communities about the future of Internet governance. The first IGF meeting was held in October/November 2006 with follow up meetings annually thereafter. Since WSIS, the term "Internet governance" has been broadened beyond narrow technical concerns to include a wider range of Internet-related policy issues.

Tim Berners-Lee, inventor of the Internet, was becoming concerned about threats to the web's future and in November 2009 at the IGF in Washington DC launched the World Wide Web Foundation (WWWF) to campaign to make the web a safe and empowering tool for the good of humanity with access to all. In November 2019 at the IGF in Berlin, Berners-Lee and the WWWF went on to launch the "Contract for the Web", a campaign initiative to persuade governments, companies and citizens to commit to nine principles to stop "misuse" with the warning "If we don't act now - and act together - to prevent the web being misused by those who want to exploit, divide and undermine, we are at risk of squandering" (its potential for good).

Due to its prominence and immediacy as an effective means of mass communication, the Internet has also become more politicized as it has grown. This has led in turn, to discourses and activities that would once have taken place in other ways, migrating to being mediated by internet.

Examples include political activities such as public protest and canvassing of support and votes, but also:


On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress HR discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to "The New York Times".

On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."

On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.

On December 14, 2017, the F.C.C Repealed their March 12, 2015 decision by a 3–2 vote regarding net neutrality rules.

E-mail has often been called the killer application of the Internet. It predates the Internet, and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.

The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.

A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.

In addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the sflovers mailing list).

During the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.

As the Internet grew through the 1980s and early 1990s, many people realized the increasing need to be able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the FTP Archive list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.

One of the most promising user interface paradigms during this period was hypertext. The technology had been inspired by Vannevar Bush's "Memex" and developed through Ted Nelson's research on Project Xanadu, Douglas Engelbart's research on NLS and Augment, and Andries van Dam's research from HES in 1968, through FRESS, Intermedia, and others. Many small self-contained hypertext systems had been created as well, such as Apple Computer's HyperCard (1987). Gopher became the first commonly used hypertext interface to the Internet. While Gopher menu items were examples of hypertext, they were not commonly perceived in that way.
In 1989, while working at CERN, Tim Berners-Lee invented a network-based implementation of the hypertext concept. By releasing his invention to public use, he encouraged widespread use. For his work in developing the World Wide Web, Berners-Lee received the Millennium technology prize in 2004. One early popular web browser, modeled after HyperCard, was ViolaWWW.

A turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana–Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the High-Performance Computing and Communications Initiative, a funding program initiated by the High Performance Computing and Communication Act of 1991, also known as the "Gore Bill". Mosaic's graphical interface soon became more popular than Gopher, which at the time was primarily text-based, and the WWW became the preferred interface for accessing the Internet. (Gore's reference to his role in "creating the Internet", however, was ridiculed in his presidential election campaign. See the full article Al Gore and information technology).

Mosaic was superseded in 1994 by Andreessen's Netscape Navigator, which replaced Mosaic as the world's most popular browser. While it held this title for some time, eventually competition from Internet Explorer and a variety of other browsers almost completely displaced it. Another important event held on January 11, 1994, was "The Superhighway Summit" at UCLA's Royce Hall. This was the "first public conference bringing together all of the major industry, government and academic leaders in the field [and] also began the national dialogue about the "Information Superhighway" and its implications."

"24 Hours in Cyberspace", "the largest one-day online event" (February 8, 1996) up to that date, took place on the then-active website, "cyber24.com." It was headed by photographer Rick Smolan. A photographic exhibition was unveiled at the Smithsonian Institution's National Museum of American History on January 23, 1997, featuring 70 photos from the project.

Even before the World Wide Web, there were search engines that attempted to organize the Internet. The first of these was the Archie search engine from McGill University in 1990, followed in 1991 by WAIS and Gopher. All three of those systems predated the invention of the World Wide Web but all continued to index the Web and the rest of the Internet for several years after the Web appeared. There are still Gopher servers as of 2006, although there are a great many more web servers.

As the Web grew, search engines and Web directories were created to track pages on the Web and allow people to find things. The first full-text Web search engine was WebCrawler in 1994. Before WebCrawler, only Web page titles were searched. Another early search engine, Lycos, was created in 1993 as a university project, and was the first to achieve commercial success. During the late 1990s, both Web directories and Web search engines were popular—Yahoo! (founded 1994) and Altavista (founded 1995) were the respective industry leaders. By August 2001, the directory model had begun to give way to search engines, tracking the rise of Google (founded 1998), which had developed new approaches to relevancy ranking. Directory features, while still commonly available, became after-thoughts to search engines.

Database size, which had been a significant marketing feature through the early 2000s, was similarly displaced by emphasis on relevancy ranking, the methods by which search engines attempt to sort the best results first. Relevancy ranking first became a major issue circa 1996, when it became apparent that it was impractical to review full lists of results. Consequently, algorithms for relevancy ranking have continuously improved. Google's PageRank method for ordering the results has received the most press, but all major search engines continually refine their ranking methodologies with a view toward improving the ordering of results. As of 2006, search engine rankings are more important than ever, so much so that an industry has developed ("search engine optimizers", or "SEO") to help web-developers improve their search ranking, and an entire body of case law has developed around matters that affect search engine rankings, such as use of trademarks in metatags. The sale of search rankings by some search engines has also created controversy among librarians and consumer advocates.

On June 3, 2009, Microsoft launched its new search engine, Bing. The following month Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search.

Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.

In 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.

All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazaa in 2006, and Limewire in 2010 to shut down or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.

Suddenly the low price of reaching millions worldwide, and the possibility of selling to or hearing from those people at the same moment when they were reached, promised to overturn established business dogma in advertising, mail-order sales, customer relationship management, and many more areas. The web was a new killer app—it could bring together unrelated buyers and sellers in seamless and low-cost ways. Entrepreneurs around the world developed new business models, and ran to their nearest venture capitalist. While some of the new entrepreneurs had experience in business and economics, the majority were simply people with ideas, and did not manage the capital influx prudently. Additionally, many dot-com business plans were predicated on the assumption that by using the Internet, they would bypass the distribution channels of existing businesses and therefore not have to compete with them; when the established businesses with strong existing brands developed their own Internet presence, these hopes were shattered, and the newcomers were left attempting to break into markets dominated by larger, more established businesses. Many did not have the ability to do so.

The dot-com bubble burst in March 2000, with the technology heavy NASDAQ Composite index peaking at 5,048.62 on March 10 (5,132.52 intraday), more than double its value just a year before. By 2001, the bubble's deflation was running full speed. A majority of the dot-coms had ceased trading, after having burnt through their venture capital and IPO capital, often without ever making a profit. But despite this, the Internet continues to grow, driven by commerce, ever greater amounts of online information and knowledge and social networking.

The first mobile phone with Internet connectivity was the Nokia 9000 Communicator, launched in Finland in 1996. The viability of Internet services access on mobile phones was limited until prices came down from that model, and network providers started to develop systems and services conveniently accessible on phones. NTT DoCoMo in Japan launched the first mobile Internet service, i-mode, in 1999 and this is considered the birth of the mobile phone Internet services. In 2001, the mobile phone email system by Research in Motion (now BlackBerry Limited) for their BlackBerry product was launched in America. To make efficient use of the small screen and tiny keypad and one-handed operation typical of mobile phones, a specific document and networking model was created for mobile devices, the Wireless Application Protocol (WAP). Most mobile device Internet services operate using WAP. The growth of mobile phone services was initially a primarily Asian phenomenon with Japan, South Korea and Taiwan all soon finding the majority of their Internet users accessing resources by phone rather than by PC. Developing countries followed, with India, South Africa, Kenya, the Philippines, and Pakistan all reporting that the majority of their domestic users accessed the Internet from a mobile phone rather than a PC. The European and North American use of the Internet was influenced by a large installed base of personal computers, and the growth of mobile phone Internet access was more gradual, but had reached national penetration levels of 20–30% in most Western countries. The cross-over occurred in 2008, when more Internet access devices were mobile phones than personal computers. In many parts of the developing world, the ratio is as much as 10 mobile phone users to one PC user.

Web pages were initially conceived as structured documents based upon Hypertext Markup Language (HTML) which can allow access to images, video, and other content. Hyperlinks in the page permit users to navigate to other pages. In the earliest browsers, images opened in a separate "helper" application. Marc Andreessen's 1993 Mosaic and 1994 Netscape introduced mixed text and images for non-technical users. HTML evolved during the 1990s, leading to HTML 4 which introduced large elements of CSS styling and, later, extensions to allow browser code to make calls and ask for content from servers in a structured way (AJAX).

There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:




</doc>
<doc id="13693" url="https://en.wikipedia.org/wiki?curid=13693" title="Horace">
Horace

Quintus Horatius Flaccus (8 December 65 BC – 27 November 8 BC), known in the English-speaking world as Horace (), was the leading Roman lyric poet during the time of Augustus (also known as Octavian). The rhetorician Quintilian regarded his "Odes" as just about the only Latin lyrics worth reading: "He can be lofty sometimes, yet he is also full of charm and grace, versatile in his figures, and felicitously daring in his choice of words."

Horace also crafted elegant hexameter verses ("Satires" and "Epistles") and caustic iambic poetry ("Epodes"). The hexameters are amusing yet serious works, friendly in tone, leading the ancient satirist Persius to comment: "as his friend laughs, Horace slyly puts his finger on his every fault; once let in, he plays about the heartstrings".

His career coincided with Rome's momentous change from a republic to an empire. An officer in the republican army defeated at the Battle of Philippi in 42 BC, he was befriended by Octavian's right-hand man in civil affairs, Maecenas, and became a spokesman for the new regime. For some commentators, his association with the regime was a delicate balance in which he maintained a strong measure of independence (he was "a master of the graceful sidestep") but for others he was, in John Dryden's phrase, "a well-mannered court slave".

Horace can be regarded as the world's first autobiographer – In his writings, he tells us far more about himself, his character, his development, and his way of life than any other great poet in antiquity. Some of the biographical writings contained in his writings can be supplemented from the short but valuable "Life of Horace" by Suetonius (in his "Lives of the Poets").

He was born on 8 December 65 BC in the Samnite south of Italy. His home town, Venusia, lay on a trade route in the border region between Apulia and Lucania (Basilicata). Various Italic dialects were spoken in the area and this perhaps enriched his feeling for language. He could have been familiar with Greek words even as a young boy and later he poked fun at the jargon of mixed Greek and Oscan spoken in neighbouring Canusium. One of the works he probably studied in school was the "Odyssia" of Livius Andronicus, taught by teachers like the 'Orbilius' mentioned in one of his poems. Army veterans could have been settled there at the expense of local families uprooted by Rome as punishment for their part in the Social War (91–88 BC). Such state-sponsored migration must have added still more linguistic variety to the area. According to a local tradition reported by Horace, a colony of Romans or Latins had been installed in Venusia after the Samnites had been driven out early in the third century. In that case, young Horace could have felt himself to be a Roman though there are also indications that he regarded himself as a Samnite or Sabellus by birth. Italians in modern and ancient times have always been devoted to their home towns, even after success in the wider world, and Horace was no different. Images of his childhood setting and references to it are found throughout his poems.

Horace's father was probably a Venutian taken captive by Romans in the Social War, or possibly he was descended from a Sabine captured in the Samnite Wars. Either way, he was a slave for at least part of his life. He was evidently a man of strong abilities however and managed to gain his freedom and improve his social position. Thus Horace claimed to be the free-born son of a prosperous 'coactor'. The term 'coactor' could denote various roles, such as tax collector, but its use by Horace was explained by scholia as a reference to 'coactor argentareus' i.e. an auctioneer with some of the functions of a banker, paying the seller out of his own funds and later recovering the sum with interest from the buyer.

The father spent a small fortune on his son's education, eventually accompanying him to Rome to oversee his schooling and moral development. The poet later paid tribute to him in a poem that one modern scholar considers the best memorial by any son to his father. The poem includes this passage:
If my character is flawed by a few minor faults, but is otherwise decent and moral, if you can point out only a few scattered blemishes on an otherwise immaculate surface, if no one can accuse me of greed, or of prurience, or of profligacy, if I live a virtuous life, free of defilement (pardon, for a moment, my self-praise), and if I am to my friends a good friend, my father deserves all the credit... As it is now, he deserves from me unstinting gratitude and praise. I could never be ashamed of such a father, nor do I feel any need, as many people do, to apologize for being a freedman's son. "Satires 1.6.65–92"
He never mentioned his mother in his verses and he might not have known much about her. Perhaps she also had been a slave.

Horace left Rome, possibly after his father's death, and continued his formal education in Athens, a great centre of learning in the ancient world, where he arrived at nineteen years of age, enrolling in The Academy. Founded by Plato, The Academy was now dominated by Epicureans and Stoics, whose theories and practises made a deep impression on the young man from Venusia. Meanwhile, he mixed and lounged about with the elite of Roman youth, such as Marcus, the idle son of Cicero, and the Pompeius to whom he later addressed a poem. It was in Athens too that he probably acquired deep familiarity with the ancient tradition of Greek lyric poetry, at that time largely the preserve of grammarians and academic specialists (access to such material was easier in Athens than in Rome, where the public libraries had yet to be built by Asinius Pollio and Augustus).

Rome's troubles following the assassination of Julius Caesar were soon to catch up with him. Marcus Junius Brutus came to Athens seeking support for the republican cause. Brutus was fêted around town in grand receptions and he made a point of attending academic lectures, all the while recruiting supporters among the young men studying there, including Horace. An educated young Roman could begin military service high in the ranks and Horace was made tribunus militum (one of six senior officers of a typical legion), a post usually reserved for men of senatorial or equestrian rank and which seems to have inspired jealousy among his well-born confederates. He learned the basics of military life while on the march, particularly in the wilds of northern Greece, whose rugged scenery became a backdrop to some of his later poems. It was there in 42 BC that Octavian (later Augustus) and his associate Mark Antony crushed the republican forces at the Battle of Philippi. Horace later recorded it as a day of embarrassment for himself, when he fled without his shield, but allowance should be made for his self-deprecating humour. Moreover, the incident allowed him to identify himself with some famous poets who had long ago abandoned their shields in battle, notably his heroes Alcaeus and Archilochus. The comparison with the latter poet is uncanny: Archilochus lost his shield in a part of Thrace near Philippi, and he was deeply involved in the Greek colonization of Thasos, where Horace's die-hard comrades finally surrendered.

Octavian offered an early amnesty to his opponents and Horace quickly accepted it. On returning to Italy, he was confronted with yet another loss: his father's estate in Venusia was one of many throughout Italy to be confiscated for the settlement of veterans (Virgil lost his estate in the north about the same time). Horace later claimed that he was reduced to poverty and this led him to try his hand at poetry. In reality, there was no money to be had from versifying. At best, it offered future prospects through contacts with other poets and their patrons among the rich. Meanwhile, he obtained the sinecure of "scriba quaestorius", a civil service position at the "aerarium" or Treasury, profitable enough to be purchased even by members of the "ordo equester" and not very demanding in its work-load, since tasks could be delegated to "scribae" or permanent clerks. It was about this time that he began writing his "Satires" and "Epodes".

The "Epodes" belong to iambic poetry. Iambic poetry features insulting and obscene language; sometimes, it is referred to as "blame poetry". "Blame poetry", or "shame poetry", is poetry written to blame and shame fellow citizens into a sense of their social obligations. Horace modelled these poems on the poetry of Archilochus. Social bonds in Rome had been decaying since the destruction of Carthage a little more than a hundred years earlier, due to the vast wealth that could be gained by plunder and corruption. These social ills were magnified by rivalry between Julius Caesar, Mark Antony and confederates like Sextus Pompey, all jockeying for a bigger share of the spoils. One modern scholar has counted a dozen civil wars in the hundred years leading up to 31 BC, including the Spartacus rebellion, eight years before Horace's birth. As the heirs to Hellenistic culture, Horace and his fellow Romans were not well prepared to deal with these problems:

Horace's Hellenistic background is clear in his Satires, even though the genre was unique to Latin literature. He brought to it a style and outlook suited to the social and ethical issues confronting Rome but he changed its role from public, social engagement to private meditation. Meanwhile, he was beginning to interest Octavian's supporters, a gradual process described by him in one of his satires. The way was opened for him by his friend, the poet Virgil, who had gained admission into the privileged circle around Maecenas, Octavian's lieutenant, following the success of his "Eclogues". An introduction soon followed and, after a discreet interval, Horace too was accepted. He depicted the process as an honourable one, based on merit and mutual respect, eventually leading to true friendship, and there is reason to believe that his relationship was genuinely friendly, not just with Maecenas but afterwards with Augustus as well. On the other hand, the poet has been unsympathetically described by one scholar as "a sharp and rising young man, with an eye to the main chance." There were advantages on both sides: Horace gained encouragement and material support, the politicians gained a hold on a potential dissident. His republican sympathies, and his role at Philippi, may have caused him some pangs of remorse over his new status. However most Romans considered the civil wars to be the result of "contentio dignitatis", or rivalry between the foremost families of the city, and he too seems to have accepted the principate as Rome's last hope for much needed peace.

In 37 BC, Horace accompanied Maecenas on a journey to Brundisium, described in one of his poems as a series of amusing incidents and charming encounters with other friends along the way, such as Virgil. In fact the journey was political in its motivation, with Maecenas en route to negotiatie the Treaty of Tarentum with Antony, a fact Horace artfully keeps from the reader (political issues are largely avoided in the first book of satires). Horace was probably also with Maecenas on one of Octavian's naval expeditions against the piratical Sextus Pompeius, which ended in a disastrous storm off Palinurus in 36 BC, briefly alluded to by Horace in terms of near-drowning. There are also some indications in his verses that he was with Maecenas at the Battle of Actium in 31 BC, where Octavian defeated his great rival, Antony. By then Horace had already received from Maecenas the famous gift of his Sabine farm, probably not long after the publication of the first book of "Satires". The gift, which included income from five tenants, may have ended his career at the Treasury, or at least allowed him to give it less time and energy. It signalled his identification with the Octavian regime yet, in the second book of "Satires" that soon followed, he continued the apolitical stance of the first book. By this time, he had attained the status of "eques Romanus", perhaps as a result of his work at the Treasury.

"Odes" 1–3 were the next focus for his artistic creativity. He adapted their forms and themes from Greek lyric poetry of the seventh and sixth centuries BC. The fragmented nature of the Greek world had enabled his literary heroes to express themselves freely and his semi-retirement from the Treasury in Rome to his own estate in the Sabine hills perhaps empowered him to some extent also yet even when his lyrics touched on public affairs they reinforced the importance of private life. Nevertheless, his work in the period 30–27 BC began to show his closeness to the regime and his sensitivity to its developing ideology. In "Odes" 1.2, for example, he eulogized Octavian in hyperboles that echo Hellenistic court poetry. The name "Augustus", which Octavian assumed in January 27 BC, is first attested in "Odes" 3.3 and 3.5. In the period 27–24 BC, political allusions in the "Odes" concentrated on foreign wars in Britain (1.35), Arabia (1.29) Spain (3.8) and Parthia (2.2). He greeted Augustus on his return to Rome in 24 BC as a beloved ruler upon whose good health he depended for his own happiness (3.14).

The public reception of "Odes" 1–3 disappointed him, however. He attributed the lack of success to jealousy among imperial courtiers and to his isolation from literary cliques. Perhaps it was disappointment that led him to put aside the genre in favour of verse letters. He addressed his first book of "Epistles" to a variety of friends and acquaintances in an urbane style reflecting his new social status as a knight. In the opening poem, he professed a deeper interest in moral philosophy than poetry but, though the collection demonstrates a leaning towards stoic theory, it reveals no sustained thinking about ethics. Maecenas was still the dominant confidante but Horace had now begun to assert his own independence, suavely declining constant invitations to attend his patron. In the final poem of the first book of "Epistles", he revealed himself to be forty-four years old in the consulship of Lollius and Lepidus i.e. 21 BC, and "of small stature, fond of the sun, prematurely grey, quick-tempered but easily placated".

According to Suetonius, the second book of "Epistles" was prompted by Augustus, who desired a verse epistle to be addressed to himself. Augustus was in fact a prolific letter-writer and he once asked Horace to be his personal secretary. Horace refused the secretarial role but complied with the emperor's request for a verse letter. The letter to Augustus may have been slow in coming, being published possibly as late as 11 BC. It celebrated, among other things, the 15 BC military victories of his stepsons, Drusus and Tiberius, yet it and the following letter were largely devoted to literary theory and criticism. The literary theme was explored still further in "Ars Poetica", published separately but written in the form of an epistle and sometimes referred to as "Epistles" 2.3 (possibly the last poem he ever wrote). He was also commissioned to write odes commemorating the victories of Drusus and Tiberius and one to be sung in a temple of Apollo for the Secular Games, a long-abandoned festival that Augustus revived in accordance with his policy of recreating ancient customs ("Carmen Saeculare").

Suetonius recorded some gossip about Horace's sexual activities late in life, claiming that the walls of his bedchamber were covered with obscene pictures and mirrors, so that he saw erotica wherever he looked. The poet died at 56 years of age, not long after his friend Maecenas, near whose tomb he was laid to rest. Both men bequeathed their property to Augustus, an honour that the emperor expected of his friends.

The dating of Horace's works isn't known precisely and scholars often debate the exact order in which they were first 'published'. There are persuasive arguments for the following chronology:

Horace composed in traditional metres borrowed from Archaic Greece, employing hexameters in his "Satires" and "Epistles", and iambs in his "Epodes", all of which were relatively easy to adapt into Latin forms. His "Odes" featured more complex measures, including alcaics and sapphics, which were sometimes a difficult fit for Latin structure and syntax. Despite these traditional metres, he presented himself as a partisan in the development of a new and sophisticated style. He was influenced in particular by Hellenistic aesthetics of brevity, elegance and polish, as modelled in the work of Callimachus.
In modern literary theory, a distinction is often made between immediate personal experience ("Urerlebnis") and experience mediated by cultural vectors such as literature, philosophy and the visual arts ("Bildungserlebnis"). The distinction has little relevance for Horace however since his personal and literary experiences are implicated in each other. "Satires" 1.5, for example, recounts in detail a real trip Horace made with Virgil and some of his other literary friends, and which parallels a Satire by Lucilius, his predecessor. Unlike much Hellenistic-inspired literature, however, his poetry was not composed for a small coterie of admirers and fellow poets, nor does it rely on abstruse allusions for many of its effects. Though elitist in its literary standards, it was written for a wide audience, as a public form of art. Ambivalence also characterizes his literary persona, since his presentation of himself as part of a small community of philosophically aware people, seeking true peace of mind while shunning vices like greed, was well adapted to Augustus's plans to reform public morality, corrupted by greedhis personal plea for moderation was part of the emperor's grand message to the nation.

Horace generally followed the examples of poets established as classics in different genres, such as Archilochus in the "Epodes", Lucilius in the "Satires" and Alcaeus in the "Odes", later broadening his scope for the sake of variation and because his models weren't actually suited to the realities confronting him. Archilochus and Alcaeus were aristocratic Greeks whose poetry had a social and religious function that was immediately intelligible to their audiences but which became a mere artifice or literary motif when transposed to Rome. However, the artifice of the "Odes" is also integral to their success, since they could now accommodate a wide range of emotional effects, and the blend of Greek and Roman elements adds a sense of detachment and universality. Horace proudly claimed to introduce into Latin the spirit and iambic poetry of Archilochus but (unlike Archilochus) without persecuting anyone ("Epistles" 1.19.23–5). It was no idle boast. His "Epodes" were modelled on the verses of the Greek poet, as 'blame poetry', yet he avoided targeting real scapegoats. Whereas Archilochus presented himself as a serious and vigorous opponent of wrong-doers, Horace aimed for comic effects and adopted the persona of a weak and ineffectual critic of his times (as symbolized for example in his surrender to the witch Canidia in the final epode). He also claimed to be the first to introduce into Latin the lyrical methods of Alcaeus ("Epistles" 1.19.32–3) and he actually was the first Latin poet to make consistent use of Alcaic meters and themes: love, politics and the symposium. He imitated other Greek lyric poets as well, employing a 'motto' technique, beginning each ode with some reference to a Greek original and then diverging from it.

The satirical poet Lucilius was a senator's son who could castigate his peers with impunity. Horace was a mere freedman's son who had to tread carefully. Lucilius was a rugged patriot and a significant voice in Roman self-awareness, endearing himself to his countrymen by his blunt frankness and explicit politics. His work expressed genuine freedom or libertas. His style included 'metrical vandalism' and looseness of structure. Horace instead adopted an oblique and ironic style of satire, ridiculing stock characters and anonymous targets. His libertas was the private freedom of a philosophical outlook, not a political or social privilege. His "Satires" are relatively easy-going in their use of meter (relative to the tight lyric meters of the "Odes") but formal and highly controlled relative to the poems of Lucilius, whom Horace mocked for his sloppy standards ("Satires" 1.10.56–61)

The "Epistles" may be considered among Horace's most innovative works. There was nothing like it in Greek or Roman literature. Occasionally poems had had some resemblance to letters, including an elegiac poem from Solon to Mimnermus and some lyrical poems from Pindar to Hieron of Syracuse. Lucilius had composed a satire in the form of a letter, and some epistolary poems were composed by Catullus and Propertius. But nobody before Horace had ever composed an entire collection of verse letters, let alone letters with a focus on philosophical problems. The sophisticated and flexible style that he had developed in his "Satires" was adapted to the more serious needs of this new genre. Such refinement of style was not unusual for Horace. His craftsmanship as a wordsmith is apparent even in his earliest attempts at this or that kind of poetry, but his handling of each genre tended to improve over time as he adapted it to his own needs. Thus for example it is generally agreed that his second book of "Satires", where human folly is revealed through dialogue between characters, is superior to the first, where he propounds his ethics in monologues. Nevertheless, the first book includes some of his most popular poems.

Horace developed a number of inter-related themes throughout his poetic career, including politics, love, philosophy and ethics, his own social role, as well as poetry itself. His "Epodes" and "Satires" are forms of 'blame poetry' and both have a natural affinity with the moralising and diatribes of Cynicism. This often takes the form of allusions to the work and philosophy of Bion of Borysthenes but it is as much a literary game as a philosophical alignment. By the time he composed his "Epistles", he was a critic of Cynicism along with all impractical and "high-falutin" philosophy in general. The "Satires" also include a strong element of Epicureanism, with frequent allusions to the Epicurean poet Lucretius. So for example the Epicurean sentiment "carpe diem" is the inspiration behind Horace's repeated punning on his own name ("Horatius ~ hora") in "Satires" 2.6. The "Satires" also feature some Stoic, Peripatetic and Platonic ("Dialogues") elements. In short, the "Satires" present a medley of philosophical programs, dished up in no particular ordera style of argument typical of the genre. The "Odes" display a wide range of topics. Over time, he becomes more confident about his political voice. Although he is often thought of as an overly intellectual lover, he is ingenious in representing passion. The "Odes" weave various philosophical strands together, with allusions and statements of doctrine present in about a third of the "Odes" Books 1–3, ranging from the flippant (1.22, 3.28) to the solemn (2.10, 3.2, 3.3). Epicureanism is the dominant influence, characterizing about twice as many of these odes as Stoicism. A group of odes combines these two influences in tense relationships, such as "Odes" 1.7, praising Stoic virility and devotion to public duty while also advocating private pleasures among friends. While generally favouring the Epicurean lifestyle, the lyric poet is as eclectic as the satiric poet, and in "Odes" 2.10 even proposes Aristotle's golden mean as a remedy for Rome's political troubles. Many of Horace's poems also contain much reflection on genre, the lyric tradition, and the function of poetry. "Odes" 4, thought to be composed at the emperor's request, takes the themes of the first three books of "Odes" to a new level. This book shows greater poetic confidence after the public performance of his "Carmen saeculare" or "Century hymn" at a public festival orchestrated by Augustus. In it, Horace addresses the emperor Augustus directly with more confidence and proclaims his power to grant poetic immortality to those he praises. It is the least philosophical collection of his verses, excepting the twelfth ode, addressed to the dead Virgil as if he were living. In that ode, the epic poet and the lyric poet are aligned with Stoicism and Epicureanism respectively, in a mood of bitter-sweet pathos. The first poem of the "Epistles" sets the philosophical tone for the rest of the collection: "So now I put aside both verses and all those other games: What is true and what befits is my care, this my question, this my whole concern." His poetic renunciation of poetry in favour of philosophy is intended to be ambiguous. Ambiguity is the hallmark of the "Epistles". It is uncertain if those being addressed by the self-mocking poet-philosopher are being honoured or criticized. Though he emerges as an Epicurean, it is on the understanding that philosophical preferences, like political and social choices, are a matter of personal taste. Thus he depicts the ups and downs of the philosophical life more realistically than do most philosophers.

The reception of Horace's work has varied from one epoch to another and varied markedly even in his own lifetime. "Odes" 1–3 were not well received when first 'published' in Rome, yet Augustus later commissioned a ceremonial ode for the Centennial Games in 17 BC and also encouraged the publication of "Odes" 4, after which Horace's reputation as Rome's premier lyricist was assured. His Odes were to become the best received of all his poems in ancient times, acquiring a classic status that discouraged imitation: no other poet produced a comparable body of lyrics in the four centuries that followed (though that might also be attributed to social causes, particularly the parasitism that Italy was sinking into). In the seventeenth and eighteenth centuries, ode-writing became highly fashionable in England and a large number of aspiring poets imitated Horace both in English and in Latin.

In a verse epistle to Augustus (Epistle 2.1), in 12 BC, Horace argued for classic status to be awarded to contemporary poets, including Virgil and apparently himself. In the final poem of his third book of Odes he claimed to have created for himself a monument more durable than bronze ("Exegi monumentum aere perennius", "Carmina" 3.30.1). For one modern scholar, however, Horace's personal qualities are more notable than the monumental quality of his achievement:
Yet for men like Wilfred Owen, scarred by experiences of World War I, his poetry stood for discredited values:

The same motto, "Dulce et decorum est pro patria mori", had been adapted to the ethos of martyrdom in the lyrics of early Christian poets like Prudentius.

These preliminary comments touch on a small sample of developments in the reception of Horace's work. More developments are covered epoch by epoch in the following sections.

Horace's influence can be observed in the work of his near contemporaries, Ovid and Propertius. Ovid followed his example in creating a completely natural style of expression in hexameter verse, and Propertius cheekily mimicked him in his third book of elegies. His "Epistles" provided them both with a model for their own verse letters and it also shaped Ovid's exile poetry.

His influence had a perverse aspect. As mentioned before, the brilliance of his "Odes" may have discouraged imitation. Conversely, they may have created a vogue for the lyrics of the archaic Greek poet Pindar, due to the fact that Horace had neglected that style of lyric (see Pindar#Influence and legacy). The iambic genre seems almost to have disappeared after publication of Horace's "Epodes". Ovid's "Ibis" was a rare attempt at the form but it was inspired mainly by Callimachus, and there are some iambic elements in Martial but the main influence there was Catullus. A revival of popular interest in the satires of Lucilius may have been inspired by Horace's criticism of his unpolished style. Both Horace and Lucilius were considered good role-models by Persius, who critiqued his own satires as lacking both the acerbity of Lucillius and the gentler touch of Horace. Juvenal's caustic satire was influenced mainly by Lucilius but Horace by then was a school classic and Juvenal could refer to him respectfully and in a round-about way as ""the Venusine lamp"".

Statius paid homage to Horace by composing one poem in Sapphic and one in Alcaic meter (the verse forms most often associated with "Odes"), which he included in his collection of occasional poems, "Silvae". Ancient scholars wrote commentaries on the lyric meters of the "Odes", including the scholarly poet Caesius Bassus. By a process called "derivatio", he varied established meters through the addition or omission of syllables, a technique borrowed by Seneca the Younger when adapting Horatian meters to the stage.

Horace's poems continued to be school texts into late antiquity. Works attributed to Helenius Acro and Pomponius Porphyrio are the remnants of a much larger body of Horatian scholarship. Porphyrio arranged the poems in non-chronological order, beginning with the "Odes", because of their general popularity and their appeal to scholars (the "Odes" were to retain this privileged position in the medieval manuscript tradition and thus in modern editions also). Horace was often evoked by poets of the fourth century, such as Ausonius and Claudian. Prudentius presented himself as a Christian Horace, adapting Horatian meters to his own poetry and giving Horatian motifs a Christian tone. On the other hand, St Jerome, modelled an uncompromising response to the pagan Horace, observing: ""What harmony can there be between Christ and the Devil? What has Horace to do with the Psalter?"" By the early sixth century, Horace and Prudentius were both part of a classical heritage that was struggling to survive the disorder of the times. Boethius, the last major author of classical Latin literature, could still take inspiration from Horace, sometimes mediated by Senecan tragedy. It can be argued that Horace's influence extended beyond poetry to dignify core themes and values of the early Christian era, such as self-sufficiency, inner contentment and courage.

Classical texts almost ceased being copied in the period between the mid sixth century and the Carolingian revival. Horace's work probably survived in just two or three books imported into northern Europe from Italy. These became the ancestors of six extant manuscripts dated to the ninth century. Two of those six manuscripts are French in origin, one was produced in Alsace, and the other three show Irish influence but were probably written in continental monasteries (Lombardy for example). By the last half of the ninth century, it was not uncommon for literate people to have direct experience of Horace's poetry. His influence on the Carolingian Renaissance can be found in the poems of Heiric of Auxerre and in some manuscripts marked with neumes, mysterious notations that may have been an aid to the memorization and discussion of his lyric meters. "Ode" is neumed with the melody of a hymn to John the Baptist, "Ut queant laxis", composed in Sapphic stanzas. This hymn later became the basis of the solfege system ("Do, re, mi...")an association with western music quite appropriate for a lyric poet like Horace, though the language of the hymn is mainly Prudentian. Lyons argues that the melody in question was linked with Horace's Ode well before Guido d'Arezzo fitted Ut queant laxis to it. However, the melody is unlikely to be a survivor from classical times, although Ovid testifies to Horace's use of the lyre while performing his Odes.

The German scholar, Ludwig Traube, once dubbed the tenth and eleventh centuries "The age of Horace" ("aetas Horatiana"), and placed it between the "aetas Vergiliana" of the eighth and ninth centuries, and the "aetas Ovidiana" of the twelfth and thirteenth centuries, a distinction supposed to reflect the dominant classical Latin influences of those times. Such a distinction is over-schematized since Horace was a substantial influence in the ninth century as well. Traube had focused too much on Horace's "Satires". Almost all of Horace's work found favour in the Medieval period. In fact medieval scholars were also guilty of over-schematism, associating Horace's different genres with the different ages of man. A twelfth-century scholar encapsulated the theory: "...Horace wrote four different kinds of poems on account of the four ages, the "Odes" for boys, the "Ars Poetica" for young men, the "Satires" for mature men, the "Epistles" for old and complete men." It was even thought that Horace had composed his works in the order in which they had been placed by ancient scholars. Despite its naivety, the schematism involved an appreciation of Horace's works as a collection, the "Ars Poetica", "Satires" and "Epistles" appearing to find favour as well as the "Odes". The later Middle Ages however gave special significance to "Satires" and "Epistles", being considered Horace's mature works. Dante referred to Horace as "Orazio satiro", and he awarded him a privileged position in the first circle of Hell, with Homer, Ovid and Lucan.

Horace's popularity is revealed in the large number of quotes from all his works found in almost every genre of medieval literature, and also in the number of poets imitating him in quantitative Latin meter . The most prolific imitator of his "Odes" was the Bavarian monk, Metellus of Tegernsee, who dedicated his work to the patron saint of Tegernsee Abbey, St Quirinus, around the year 1170. He imitated all Horace's lyrical meters then followed these up with imitations of other meters used by Prudentius and Boethius, indicating that variety, as first modelled by Horace, was considered a fundamental aspect of the lyric genre. The content of his poems however was restricted to simple piety. Among the most successful imitators of "Satires" and "Epistles" was another Germanic author, calling himself Sextus Amarcius, around 1100, who composed four books, the first two exemplifying vices, the second pair mainly virtues.

Petrarch is a key figure in the imitation of Horace in accentual meters. His verse letters in Latin were modelled on the "Epistles" and he wrote a letter to Horace in the form of an ode. However he also borrowed from Horace when composing his Italian sonnets. One modern scholar has speculated that authors who imitated Horace in accentual rhythms (including stressed Latin and vernacular languages) may have considered their work a natural sequel to Horace's metrical variety. In France, Horace and Pindar were the poetic models for a group of vernacular authors called the Pléiade, including for example Pierre de Ronsard and Joachim du Bellay. Montaigne made constant and inventive use of Horatian quotes. The vernacular languages were dominant in Spain and Portugal in the sixteenth century, where Horace's influence is notable in the works of such authors as Garcilaso de la Vega, Juan Boscán Sá de Miranda, Antonio Ferreira and Fray Luis de León, the latter for example writing odes on the Horatian theme "beatus ille" ("happy the man"). The sixteenth century in western Europe was also an age of translations (except in Germany, where Horace wasn't translated until well into the seventeenth century). The first English translator was Thomas Drant, who placed translations of Jeremiah and Horace side by side in "Medicinable Morall", 1566. That was also the year that the Scot George Buchanan paraphrased the Psalms in a Horatian setting. Ben Jonson put Horace on the stage in 1601 in "Poetaster", along with other classical Latin authors, giving them all their own verses to speak in translation. Horace's part evinces the independent spirit, moral earnestness and critical insight that many readers look for in his poems.

During the seventeenth and eighteenth centuries, or the Age of Enlightenment, neo-classical culture was pervasive. English literature in the middle of that period has been dubbed Augustan. It is not always easy to distinguish Horace's influence during those centuries (the mixing of influences is shown for example in one poet's pseudonym, "Horace Juvenal"). However a measure of his influence can be found in the diversity of the people interested in his works, both among readers and authors.

New editions of his works were published almost yearly. There were three new editions in 1612 (two in Leiden, one in Frankfurt) and again in 1699 (Utrecht, Barcelona, Cambridge). Cheap editions were plentiful and fine editions were also produced, including one whose entire text was engraved by John Pine in copperplate. The poet James Thomson owned five editions of Horace's work and the physician James Douglas had five hundred books with Horace-related titles. Horace was often commended in periodicals such as The Spectator, as a hallmark of good judgement, moderation and manliness, a focus for moralising. His verses offered a fund of mottoes, such as "simplex munditiis", (elegance in simplicity) "splendide mendax" (nobly untruthful.), "sapere aude", "nunc est bibendum", "carpe diem" (the latter perhaps being the only one still in common use today), quoted even in works as prosaic as Edmund Quincy's "A treatise of hemp-husbandry" (1765). The fictional hero Tom Jones recited his verses with feeling. His works were also used to justify commonplace themes, such as patriotic obedience, as in James Parry's English lines from an Oxford University collection in 1736:

Horatian-style lyrics were increasingly typical of Oxford and Cambridge verse collections for this period, most of them in Latin but some like the previous ode in English. John Milton's Lycidas first appeared in such a collection. It has few Horatian echoes yet Milton's associations with Horace were lifelong. He composed a controversial version of "Odes" 1.5, and Paradise Lost includes references to Horace's 'Roman' "Odes" 3.1–6 (Book 7 for example begins with echoes of "Odes" 3.4). Yet Horace's lyrics could offer inspiration to libertines as well as moralists, and neo-Latin sometimes served as a kind of discrete veil for the risqué. Thus for example Benjamin Loveling authored a catalogue of Drury Lane and Covent Garden prostitutes, in Sapphic stanzas, and an encomium for a dying lady "of salacious memory". Some Latin imitations of Horace were politically subversive, such as a marriage ode by Anthony Alsop that included a rallying cry for the Jacobite cause. On the other hand, Andrew Marvell took inspiration from Horace's "Odes" 1.37 to compose his English masterpiece Horatian Ode upon Cromwell's Return from Ireland, in which subtly nuanced reflections on the execution of Charles I echo Horace's ambiguous response to the death of Cleopatra (Marvell's ode was suppressed in spite of its subtlety and only began to be widely published in 1776). Samuel Johnson took particular pleasure in reading "The Odes". Alexander Pope wrote direct "Imitations" of Horace (published with the original Latin alongside) and also echoed him in "Essays" and The Rape of the Lock. He even emerged as "a quite Horatian Homer" in his translation of the "Iliad". Horace appealed also to female poets, such as Anna Seward ("Original sonnets on various subjects, and odes paraphrased from Horace", 1799) and Elizabeth Tollet, who composed a Latin ode in Sapphic meter to celebrate her brother's return from overseas, with tea and coffee substituted for the wine of Horace's sympotic settings:
Horace's "Ars Poetica" is second only to Aristotle's "Poetics" in its influence on literary theory and criticism. Milton recommended both works in his treatise "of Education". Horace's "Satires" and "Epistles" however also had a huge impact, influencing theorists and critics such as John Dryden. There was considerable debate over the value of different lyrical forms for contemporary poets, as represented on one hand by the kind of four-line stanzas made familiar by Horace's Sapphic and Alcaic "Odes" and, on the other, the loosely structured Pindarics associated with the odes of Pindar. Translations occasionally involved scholars in the dilemmas of censorship. Thus Christopher Smart entirely omitted "Odes" and re-numbered the remaining odes. He also removed the ending of "Odes" . Thomas Creech printed "Epodes" and in the original Latin but left out their English translations. Philip Francis left out both the English and Latin for those same two epodes, a gap in the numbering the only indication that something was amiss. French editions of Horace were influential in England and these too were regularly bowdlerized.

Most European nations had their own 'Horaces': thus for example Friedrich von Hagedorn was called "The German Horace" and Maciej Kazimierz Sarbiewski "The Polish Horace" (the latter was much imitated by English poets such as Henry Vaughan and Abraham Cowley). Pope Urban VIII wrote voluminously in Horatian meters, including an ode on gout.

Horace maintained a central role in the education of English-speaking elites right up until the 1960s. A pedantic emphasis on the formal aspects of language-learning at the expense of literary appreciation may have made him unpopular in some quarters yet it also confirmed his influencea tension in his reception that underlies Byron's famous lines from "Childe Harold" (Canto iv, 77):

William Wordsworth's mature poetry, including the preface to Lyrical Ballads, reveals Horace's influence in its rejection of false ornament and he once expressed "a wish / to meet the shade of Horace...". John Keats echoed the opening of Horace's "Epodes" 14 in the opening lines of "Ode to a Nightingale".

The Roman poet was presented in the nineteenth century as an honorary English gentleman. William Thackeray produced a version of "Odes" in which Horace's 'boy' became 'Lucy', and Gerard Manley Hopkins translated the boy innocently as 'child'. Horace was translated by Sir Theodore Martin (biographer of Prince Albert) but minus some ungentlemanly verses, such as the erotic "Odes" and "Epodes" 8 and 12. Edward Bulwer-Lytton produced a popular translation and William Gladstone also wrote translations during his last days as Prime Minister.

Edward FitzGerald's "Rubaiyat of Omar Khayyam", though formally derived from the Persian "ruba'i", nevertheless shows a strong Horatian influence, since, as one modern scholar has observed,""...the quatrains inevitably recall the stanzas of the 'Odes', as does the narrating first person of the world-weary, ageing Epicurean Omar himself, mixing sympotic exhortation and 'carpe diem' with splendid moralising and 'memento mori' nihilism."" Matthew Arnold advised a friend in verse not to worry about politics, an echo of "Odes" , yet later became a critic of Horace's inadequacies relative to Greek poets, as role models of Victorian virtues, observing: ""If human life were complete without faith, without enthusiasm, without energy, Horace...would be the perfect interpreter of human life."" Christina Rossetti composed a sonnet depicting a woman willing her own death steadily, drawing on Horace's depiction of 'Glycera' in "Odes" and Cleopatra in "Odes" . A. E. Housman considered "Odes" , in Archilochian couplets, the most beautiful poem of antiquity and yet he generally shared Horace's penchant for quatrains, being readily adapted to his own elegiac and melancholy strain. The most famous poem of Ernest Dowson took its title and its heroine's name from a line of "Odes" , "Non sum qualis eram bonae sub regno Cynarae", as well as its motif of nostalgia for a former flame. Kipling wrote a famous parody of the "Odes", satirising their stylistic idiosyncrasies and especially the extraordinary syntax, but he also used Horace's Roman patriotism as a focus for British imperialism, as in the story "Regulus" in the school collection "Stalky & Co.", which he based on "Odes" . Wilfred Owen's famous poem, quoted above, incorporated Horatian text to question patriotism while ignoring the rules of Latin scansion. However, there were few other echoes of Horace in the war period, possibly because war is not actually a major theme of Horace's work.
Both W.H.Auden and Louis MacNeice began their careers as teachers of classics and both responded as poets to Horace's influence. Auden for example evoked the fragile world of the 1930s in terms echoing "Odes" , where Horace advises a friend not to let worries about frontier wars interfere with current pleasures.

The American poet, Robert Frost, echoed Horace's "Satires" in the conversational and sententious idiom of some of his longer poems, such as "The Lesson for Today" (1941), and also in his gentle advocacy of life on the farm, as in "Hyla Brook" (1916), evoking Horace's "fons Bandusiae" in "Ode" . Now at the start of the third millennium, poets are still absorbing and re-configuring the Horatian influence, sometimes in translation (such as a 2002 English/American edition of the "Odes" by thirty-six poets) and sometimes as inspiration for their own work (such as a 2003 collection of odes by a New Zealand poet).

Horace's "Epodes" have largely been ignored in the modern era, excepting those with political associations of historical significance. The obscene qualities of some of the poems have repulsed even scholars yet more recently a better understanding of the nature of Iambic poetry has led to a re-evaluation of the "whole" collection. A re-appraisal of the "Epodes" also appears in creative adaptations by recent poets (such as a 2004 collection of poems that relocates the ancient context to a 1950s industrial town).


The Oxford Latin Course textbooks use the life of Horace to illustrate an average Roman's life in the late Republic to Early Empire.






</doc>
<doc id="13694" url="https://en.wikipedia.org/wiki?curid=13694" title="Microsoft Windows version history">
Microsoft Windows version history

Microsoft Windows was announced by Bill Gates on November 10, 1983. Microsoft introduced Windows as a graphical user interface for MS-DOS, which had been introduced a couple of years earlier. In the 1990s, the product line evolved from an operating environment into a fully complete, modern operating system over two lines of development, each with their own separate codebase. 

The first versions of Windows (1.0 through to 3.11) were graphical shells that ran from MS-DOS. Later on, Windows 95, though still being based on MS-DOS, was its own operating system, using a 16-bit DOS-based kernel and a 32-bit user space. Windows 95 introduced many features that have been part of the product ever since, including the Start menu, the taskbar, and Windows Explorer (renamed File Explorer in Windows 8). In 1997, Microsoft released Internet Explorer 4 which included the (at the time) controversial Windows Desktop Update. It aimed to integrate Internet Explorer and the web into the user interface and also brought many new features into Windows, such as the ability to display JPEG images as the desktop wallpaper and single window navigation in Windows Explorer. In 1998, Microsoft released Windows 98, which also included the Windows Desktop Update and Internet Explorer 4 by default. The inclusion of Internet Explorer 4 and the Desktop Update led to an anti-trust case in the United States. Windows 98 also includes plug and play, which allows devices to work when plugged in without requiring a system reboot or manual configuration, and USB support out of the box. Windows Me, the last DOS-based version of Windows, was aimed at consumers and released in 2000. It introduced System Restore, Help and Support Center, updated versions of the Disk Defragmenter and other system tools.

In 1993, Microsoft released Windows NT 3.1, the first version of the newly-developed Windows NT operating system. Unlike the Windows 9x series of operating systems, it is a fully 32-bit operating system. NT 3.1 introduced NTFS, a file system designed to replace the older File Allocation Table (FAT) which was used by DOS and the DOS-based Windows operating systems. In 1996, Windows NT 4.0 was released, which includes a fully 32-bit version of Windows Explorer written specifically for it, making the operating system work just like Windows 95. Windows NT was originally designed to be used on high-end systems and servers, however with the release of Windows 2000, many consumer-oriented features from Windows 95 and Windows 98 were included, such as the Windows Desktop Update, Internet Explorer 5, USB support and Windows Media Player. These consumer-oriented features were continued and further extended in Windows XP, which introduced a new theme called Luna, a more user-friendly interface, updated versions of Windows Media Player and Internet Explorer, and extended features from Windows Me, such as the Help and Support Center and System Restore. Windows Vista focused on securing the Windows operating system against computer viruses and other malicious software by introducing features such as User Account Control. New features include Windows Aero, updated versions of the standard games (e.g. Solitaire), Windows Movie Maker, and Windows Mail to replace Outlook Express. Despite this, Windows Vista was critically panned for its poor performance on older hardware and its at-the-time high system requirements. Windows 7 followed two and a half years later, and despite technically having higher system requirements, reviewers noted that it ran better than Windows Vista. Windows 7 also removed many extra features, such as Windows Movie Maker, Windows Photo Gallery and Windows Mail, instead requiring users download a separate Windows Live Essentials to gain those features and other online services. Windows 8 and Windows 8.1, a free upgrade for Windows 8, introduced many controversial changes, such as the replacement of the Start menu with the Start Screen, the removal of the Aero glass interface in favor of a flat, colored interface as well as the introduction of "Metro" apps (later renamed to Universal Windows Platform apps) and the Charms Bar user interface element, all of which received considerable criticism from reviewers.

The current version of Windows, Windows 10, reintroduced the Start menu and added the ability to run Universal Windows Platform apps in a window instead of always in full screen. Windows 10 was well-received, with many reviewers stating that Windows 10 is what Windows 8 should have been. Windows 10 also marks the last version of Windows to be traditionally released. Instead, "feature updates" are released twice a year with names such as "Creators Update" and "Fall Creators Update" that introduce new capabilities.

The first independent version of Microsoft Windows, version 1.0, released on November 20, 1985, achieved little popularity. The project was briefly codenamed "Interface Manager" before the windowing system was developed - contrary to popular belief that it was the original name for Windows and Rowland Hanson, the head of marketing at Microsoft, convinced the company that the name "Windows" would be more appealing to customers.

Windows 1.0 was not a complete operating system, but rather an "operating environment" that extended MS-DOS, and shared the latter's inherent flaws and errors.

The first version of Microsoft Windows included a simple graphics painting program called Windows Paint; Windows Write, a simple word processor; an appointment calendar; a card-filer; a notepad; a clock; a control panel; a computer terminal; Clipboard; and RAM driver. It also included the MS-DOS Executive and a game called Reversi.

Microsoft had worked with Apple Computer to develop applications for Apple's new Macintosh computer, which featured a graphical user interface. As part of the related business negotiations, Microsoft had licensed certain aspects of the Macintosh user interface from Apple; in later litigation, a district court summarized these aspects as "screen displays".
In the development of Windows 1.0, Microsoft intentionally limited its borrowing of certain GUI elements from the Macintosh user interface, to comply with its license. For example, windows were only displayed "tiled" on the screen; that is, they could not overlap or overlie one another.

Microsoft Windows version 2 came out on December 9, 1987, and proved slightly more popular than its predecessor.
Much of the popularity for Windows 2.0 came by way of its inclusion as a "run-time version" with Microsoft's new graphical applications, Excel and Word for Windows. They could be run from MS-DOS, executing Windows for the duration of their activity, and closing down Windows upon exit.

Microsoft Windows received a major boost around this time when Aldus PageMaker appeared in a Windows version, having previously run only on Macintosh. Some computer historians date this, the first appearance of a significant "and" non-Microsoft application for Windows, as the start of the success of Windows.

Versions 2.0x used the real-mode memory model, which confined it to a maximum of 1 megabyte of memory.
In such a configuration, it could run under another multitasker like DESQview, which used the 286 protected mode.

Later, two new versions were released: Windows/286 2.1 and Windows/386 2.1. Like prior versions of Windows, Windows/286 2.1 used the real-mode memory model, but was the first version to support the High Memory Area. Windows/386 2.1 had a protected mode kernel with LIM-standard EMS emulation. All Windows and DOS-based applications at the time were real mode, running over the protected mode kernel by using the virtual 8086 mode, which was new with the 80386 processor.

Version 2.03, and later 3.0, faced challenges from Apple over its overlapping windows and other features Apple charged mimicked the ostensibly copyrighted "look and feel" of its operating system and "embodie[d] and generated a copy of the Macintosh" in its OS. Judge William Schwarzer dropped all but 10 of Apple's 189 claims of copyright infringement, and ruled that most of the remaining 10 were over uncopyrightable ideas.

Windows 3.0, released in May 1990, improved capabilities given to native applications. It also allowed users to better multitask older MS-DOS based software compared to Windows/386, thanks to the introduction of virtual memory.

Windows 3.0's user interface finally resembled a serious competitor to the user interface of the Macintosh computer. PCs had improved graphics by this time, due to VGA video cards, and the protected/enhanced mode allowed Windows applications to use more memory in a more painless manner than their DOS counterparts could. Windows 3.0 could run in real, standard, or 386 enhanced modes, and was compatible with any Intel processor from the 8086/8088 up to the 80286 and 80386. This was the first version to run Windows programs in protected mode, although the 386 enhanced mode kernel was an enhanced version of the protected mode kernel in Windows/386.

Windows 3.0 received two updates. A few months after introduction, Windows 3.0a was released as a maintenance release, resolving bugs and improving stability. A "multimedia" version, Windows 3.0 with Multimedia Extensions 1.0, was released in October 1991. This was bundled with "multimedia upgrade kits", comprising a CD-ROM drive and a sound card, such as the Creative Labs Sound Blaster Pro. This version was the precursor to the multimedia features available in Windows 3.1 (first released in April 1992) and later, and was part of Microsoft's specification for the Multimedia PC.

The features listed above and growing market support from application software developers made Windows 3.0 wildly successful, selling around 10 million copies in the two years before the release of version 3.1. Windows 3.0 became a major source of income for Microsoft, and led the company to revise some of its earlier plans. Support was discontinued on December 31, 2001.

During the mid to late 1980s, Microsoft and IBM had cooperatively been developing OS/2 as a successor to DOS. OS/2 would take full advantage of the aforementioned protected mode of the Intel 80286 processor and up to 16 MB of memory. OS/2 1.0, released in 1987, supported swapping and multitasking and allowed running of DOS executables.

IBM licensed Windows's GUI for OS/2 as Presentation Manager, and the two companies stated that it and Windows 2.0 would be almost identical. Presentation Manager was not available with OS/2 until version 1.1, released in 1988. Its API was incompatible with Windows. Version 1.2, released in 1989, introduced a new file system, HPFS, to replace the FAT file system.

By the early 1990s, conflicts developed in the Microsoft/IBM relationship. They cooperated with each other in developing their PC operating systems, and had access to each other's code. Microsoft wanted to further develop Windows, while IBM desired for future work to be based on OS/2. In an attempt to resolve this tension, IBM and Microsoft agreed that IBM would develop OS/2 2.0, to replace OS/2 1.3 and Windows 3.0, while Microsoft would develop a new operating system, OS/2 3.0, to later succeed OS/2 2.0.

This agreement soon fell apart however, and the Microsoft/IBM relationship was terminated. IBM continued to develop OS/2, while Microsoft changed the name of its (as yet unreleased) OS/2 3.0 to Windows NT. Both retained the rights to use OS/2 and Windows technology developed up to the termination of the agreement; Windows NT, however, was to be written anew, mostly independently (see below).

After an interim 1.3 version to fix up many remaining problems with the 1.x series, IBM released OS/2 version 2.0 in 1992. This was a major improvement: it featured a new, object-oriented GUI, the Workplace Shell (WPS), that included a desktop and was considered by many to be OS/2's best feature. Microsoft would later imitate much of it in Windows 95. Version 2.0 also provided a full 32-bit API, offered smooth multitasking and could take advantage of the 4 gigabytes of address space provided by the Intel 80386. Still, much of the system had 16-bit code internally which required, among other things, device drivers to be 16-bit code as well. This was one of the reasons for the chronic shortage of OS/2 drivers for the latest devices. Version 2.0 could also run DOS and Windows 3.0 programs, since IBM had retained the right to use the DOS and Windows code as a result of the breakup.

In response to the impending release of OS/2 2.0, Microsoft developed Windows 3.1 (first released in April 1992), which included several improvements to Windows 3.0, such as display of TrueType scalable fonts (developed jointly with Apple), improved disk performance in 386 Enhanced Mode, multimedia support, and bugfixes. It also removed Real Mode, and only ran on an 80286 or better processor. Later Microsoft also released Windows 3.11, a touch-up to Windows 3.1 which included all of the patches and updates that followed the release of Windows 3.1 in 1992.

In 1992 and 1993, Microsoft released Windows for Workgroups (WfW), which was available both as an add-on for existing Windows 3.1 installations and in a version that included the base Windows environment and the networking extensions all in one package. Windows for Workgroups included improved network drivers and protocol stacks, and support for peer-to-peer networking. There were two versions of Windows for Workgroups, WfW 3.1 and WfW 3.11. Unlike prior versions, Windows for Workgroups 3.11 ran in 386 Enhanced Mode only, and needed at least an 80386SX processor. One optional download for WfW was the "Wolverine" TCP/IP protocol stack, which allowed for easy access to the Internet through corporate networks.

All these versions continued version 3.0's impressive sales pace. Even though the 3.1x series still lacked most of the important features of OS/2, such as long file names, a desktop, or protection of the system against misbehaving applications, Microsoft quickly took over the OS and GUI markets for the IBM PC. The Windows API became the de facto standard for consumer software.

Meanwhile, Microsoft continued to develop Windows NT. The main architect of the system was Dave Cutler, one of the chief architects of VMS at Digital Equipment Corporation (later acquired by Compaq, now part of Hewlett-Packard). Microsoft hired him in October 1988 to create a successor to OS/2, but Cutler created a completely new system instead. Cutler had been developing a follow-on to VMS at DEC called Mica, and when DEC dropped the project he brought the expertise and around 20 engineers with him to Microsoft. DEC also believed he brought Mica's code to Microsoft and sued. Microsoft eventually paid US$150 million and agreed to support DEC's Alpha CPU chip in NT.

Windows NT Workstation (Microsoft marketing wanted Windows NT to appear to be a continuation of Windows 3.1) arrived in Beta form to developers at the July 1992 Professional Developers Conference in San Francisco. Microsoft announced at the conference its intentions to develop a successor to both Windows NT and Windows 3.1's replacement (Windows 95, codenamed Chicago), which would unify the two into one operating system. This successor was codenamed Cairo. In hindsight, Cairo was a much more difficult project than Microsoft had anticipated and, as a result, NT and Chicago would not be unified until Windows XP—albeit Windows 2000, oriented to business, had already unified most of the system's bolts and gears, it was XP that was sold to home consumers like Windows 95 and came to be viewed as the final unified OS. Parts of Cairo have still not made it into Windows as of 2017 - most notably, the WinFS file system, which was the much touted Object File System of Cairo. Microsoft announced that they have discontinued the separate release of WinFS for Windows XP and Windows Vista and will gradually incorporate the technologies developed for WinFS in other products and technologies, notably Microsoft SQL Server.

Driver support was lacking due to the increased programming difficulty in dealing with NT's superior hardware abstraction model. This problem plagued the NT line all the way through Windows 2000. Programmers complained that it was too hard to write drivers for NT, and hardware developers were not going to go through the trouble of developing drivers for a small segment of the market. Additionally, although allowing for good performance and fuller exploitation of system resources, it was also resource-intensive on limited hardware, and thus was only suitable for larger, more expensive machines.

However, these same features made Windows NT perfect for the LAN server market (which in 1993 was experiencing a rapid boom, as office networking was becoming common). NT also had advanced network connectivity options and NTFS, an efficient file system. Windows NT version 3.51 was Microsoft's entry into this field, and took away market share from Novell (the dominant player) in the following years.

One of Microsoft's biggest advances initially developed for Windows NT was a new 32-bit API, to replace the legacy 16-bit Windows API. This API was called Win32, and from then on Microsoft referred to the older 16-bit API as Win16. The Win32 API had three levels of implementation: the complete one for Windows NT, a subset for Chicago (originally called Win32c) missing features primarily of interest to enterprise customers (at the time) such as security and Unicode support, and a more limited subset called Win32s which could be used on Windows 3.1 systems. Thus Microsoft sought to ensure some degree of compatibility between the Chicago design and Windows NT, even though the two systems had radically different internal architectures. Windows NT was the first Windows operating system based on a hybrid kernel.

As released, Windows NT 3.x went through three versions (3.1, 3.5, and 3.51), changes were primarily internal and reflected back end changes. The 3.5 release added support for new types of hardware and improved performance and data reliability; the 3.51 release was primarily to update the Win32 APIs to be compatible with software being written for the Win32c APIs in what became Windows 95.

After Windows 3.11, Microsoft began to develop a new consumer oriented version of the operating system codenamed Chicago. Chicago was designed to have support for 32-bit preemptive multitasking like OS/2 and Windows NT, although a 16-bit kernel would remain for the sake of backward compatibility. The Win32 API first introduced with Windows NT was adopted as the standard 32-bit programming interface, with Win16 compatibility being preserved through a technique known as "thunking". A new object oriented GUI was not originally planned as part of the release, although elements of the Cairo user interface were borrowed and added as other aspects of the release (notably Plug and Play) slipped.

Microsoft did not change all of the Windows code to 32-bit, parts of it remained 16-bit (albeit not directly using real mode) for reasons of compatibility, performance, and development time. Additionally it was necessary to carry over design decisions from earlier versions of Windows for reasons of backwards compatibility, even if these design decisions no longer matched a more modern computing environment. These factors eventually began to impact the operating system's efficiency and stability.

Microsoft marketing adopted Windows 95 as the product name for Chicago when it was released on August 24, 1995. Microsoft had a double gain from its release: first, it made it impossible for consumers to run Windows 95 on a cheaper, non-Microsoft DOS, secondly, although traces of DOS were never completely removed from the system and MS DOS 7 would be loaded briefly as a part of the booting process, Windows 95 applications ran solely in 386 enhanced mode, with a flat 32-bit address space and virtual memory. These features make it possible for Win32 applications to address up to 2 gigabytes of virtual RAM (with another 2 GB reserved for the operating system), and in theory prevented them from inadvertently corrupting the memory space of other Win32 applications. In this respect the functionality of Windows 95 moved closer to Windows NT, although Windows 95/98/Me did not support more than 512 megabytes of physical RAM without obscure system tweaks.

IBM continued to market OS/2, producing later versions in OS/2 3.0 and 4.0 (also called Warp). Responding to complaints about OS/2 2.0's high demands on computer hardware, version 3.0 was significantly optimized both for speed and size. Before Windows 95 was released, OS/2 Warp 3.0 was even shipped pre-installed with several large German hardware vendor chains. However, with the release of Windows 95, OS/2 began to lose market share.

It is probably impossible to choose one specific reason why OS/2 failed to gain much market share. While OS/2 continued to run Windows 3.1 applications, it lacked support for anything but the Win32s subset of Win32 API (see above). Unlike with Windows 3.1, IBM did not have access to the source code for Windows 95 and was unwilling to commit the time and resources to emulate the moving target of the Win32 API. IBM later introduced OS/2 into the United States v. Microsoft case, blaming unfair marketing tactics on Microsoft's part.

Microsoft went on to release five different versions of Windows 95:

OSR2, OSR2.1, and OSR2.5 were not released to the general public, rather, they were available only to OEMs that would preload the OS onto computers. Some companies sold new hard drives with OSR2 preinstalled (officially justifying this as needed due to the hard drive's capacity).

The first Microsoft Plus! add-on pack was sold for Windows 95.

Windows NT 4.0 was the successor of 3.5 (1994) and 3.51 (1995). Microsoft released Windows NT 4.0 to manufacturing in July 1996, one year after the release of Windows 95. Major new features included the new Explorer shell from Windows 95, scalability and feature improvements to the core architecture, kernel, USER32, COM and MSRPC.

Windows NT 4.0 came in four versions:

On June 25, 1998, Microsoft released Windows 98 (code-named Memphis). It included new hardware drivers and the FAT32 file system which supports disk partitions that are larger than 2 GB (first introduced in Windows 95 OSR2). USB support in Windows 98 is marketed as a vast improvement over Windows 95. The release continued the controversial inclusion of the Internet Explorer browser with the operating system that started with Windows 95 OEM Service Release 1. The action eventually led to the filing of the United States v. Microsoft case, dealing with the question of whether Microsoft was introducing unfair practices into the market in an effort to eliminate competition from other companies such as Netscape.

In 1999, Microsoft released Windows 98 Second Edition, an interim release. One of the more notable new features was the addition of Internet Connection Sharing, a form of network address translation, allowing several machines on a LAN (Local Area Network) to share a single Internet connection. Hardware support through device drivers was increased and this version shipped with Internet Explorer 5. Many minor problems that existed in the first edition were fixed making it, according to many, the most stable release of the Windows 9x family.

Microsoft released Windows 2000 on February 17, 2000. It has the version number Windows NT 5.0. Windows 2000 has had four official service packs. It was successfully deployed both on the server and the workstation markets. Amongst Windows 2000's most significant new features was Active Directory, a near-complete replacement of the NT 4.0 Windows Server domain model, which built on industry-standard technologies like DNS, LDAP, and Kerberos to connect machines to one another. Terminal Services, previously only available as a separate edition of NT 4, was expanded to all server versions. A number of features from Windows 98 were incorporated also, such as an improved Device Manager, Windows Media Player, and a revised DirectX that made it possible for the first time for many modern games to work on the NT kernel. Windows 2000 is also the last NT-kernel Windows operating system to lack product activation.

While Windows 2000 upgrades were available for Windows 95 and Windows 98, it was not intended for home users.

Windows 2000 was available in four editions:

In September 2000, Microsoft released a successor to Windows 98 called Windows Me, short for "Millennium Edition". It was the last DOS-based operating system from Microsoft. Windows Me introduced a new multimedia-editing application called Windows Movie Maker, came standard with Internet Explorer 5.5 and Windows Media Player 7, and debuted the first version of System Restore – a recovery utility that enables the operating system to revert system files back to a prior date and time. System Restore was a notable feature that would continue to thrive in all later versions of Windows.

Windows Me was conceived as a quick one-year project that served as a stopgap release between Windows 98 and Windows XP. Many of the new features were available from the Windows Update site as updates for older Windows versions ("System Restore" and "Windows Movie Maker" were exceptions). Windows Me was criticized for stability issues, as well as for lacking real mode DOS support, to the point of being referred to as the "Mistake Edition." Windows Me was the last operating system to be based on the Windows 9x (monolithic) kernel and MS-DOS.

On October 25, 2001, Microsoft released Windows XP (codenamed "Whistler"). The merging of the Windows NT/2000 and Windows 95/98/Me lines was finally achieved with Windows XP. Windows XP uses the Windows NT 5.1 kernel, marking the entrance of the Windows NT core to the consumer market, to replace the aging Windows 9x branch. The initial release was met with considerable criticism, particularly in the area of security, leading to the release of three major Service Packs. Windows XP SP1 was released in September 2002, SP2 was released in August 2004 and SP3 was released in April 2008. Service Pack 2 provided significant improvements and encouraged widespread adoption of XP among both home and business users. Windows XP lasted longer as Microsoft's flagship operating system than any other version of Windows, from October 25, 2001 to January 30, 2007 when it was succeeded by Windows Vista.

Windows XP is available in a number of versions:

On April 25, 2003, Microsoft launched Windows Server 2003, a notable update to Windows 2000 Server encompassing many new security features, a new "Manage Your Server" wizard that simplifies configuring a machine for specific roles, and improved performance. It has the version number NT 5.2. A few services not essential for server environments are disabled by default for stability reasons, most noticeable are the "Windows Audio" and "Themes" services; users have to enable them manually to get sound or the "Luna" look as per Windows XP. The hardware acceleration for display is also turned off by default, users have to turn the acceleration level up themselves if they trust the display card driver.

In December 2005, Microsoft released Windows Server 2003 R2, which is actually Windows Server 2003 with SP1 (Service Pack 1), together with an add-on package.
Among the new features are a number of management features for branch offices, file serving, printing and company-wide identity integration.

Windows Server 2003 is available in six editions:

Windows Server 2003 R2, an update of Windows Server 2003, was released to manufacturing on December 6, 2005. It is distributed on two CDs, with one CD being the Windows Server 2003 SP1 CD. The other CD adds many optionally installable features for Windows Server 2003. The R2 update was released for all x86 and x64 versions, except Windows Server 2003 R2 Enterprise Edition, which was not released for Itanium.

On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003, x64 Editions in Standard, Enterprise and Datacenter SKUs. Windows XP Professional x64 Edition is an edition of Windows XP for x86-64 personal computers. It is designed to use the expanded 64-bit memory address space provided by the x86-64 architecture.

Windows XP Professional x64 Edition is based on the Windows Server 2003 codebase, with the server features removed and client features added. Both "Windows Server 2003 x64" and Windows XP Professional x64 Edition use identical kernels.

Windows XP "Professional" "x64 Edition" is not to be confused with Windows XP "64-bit Edition", as the latter was designed for Intel Itanium processors. During the initial development phases, Windows XP Professional x64 Edition was named "Windows XP 64-Bit Edition for 64-Bit Extended Systems".

In July 2005, Microsoft released a thin-client version of Windows XP Service Pack 2, called Windows Fundamentals for Legacy PCs (WinFLP). It is only available to Software Assurance customers. The aim of WinFLP is to give companies a viable upgrade option for older PCs that are running Windows 95, 98, and Me that will be supported with patches and updates for the next several years. Most user applications will typically be run on a remote machine using Terminal Services or Citrix.

While being visually the same as Windows XP, it has some differences. For example, if the screen has been set to 16 bit colors, the Windows 2000 recycle bin icon and some XP 16-bit icons will show. Paint and some games like Solitaire aren't present too.
Windows Home Server (code-named Q, Quattro) is a server product based on Windows Server 2003, designed for consumer use. The system was announced on January 7, 2007 by Bill Gates. Windows Home Server can be configured and monitored using a console program that can be installed on a client PC. Such features as Media Sharing, local and remote drive backup and file duplication are all listed as features. The release of Windows Home Server Power Pack 3 added support for Windows 7 to Windows Home Server.

Windows Vista was released on November 30, 2006 to business customers - consumer versions followed on January 30, 2007. Windows Vista intended to have enhanced security by introducing a new restricted user mode called User Account Control, replacing the "administrator-by-default" philosophy of Windows XP. Vista was the target of much criticism and negative press, and in general was not well regarded, this was seen as leading to the relatively swift release of Windows 7.

One major difference between Vista and earlier versions of Windows, Windows 95 and later, is that the original start button was replaced with the Windows icon in a circle (called the Start Orb). Vista also features new graphics features, the Windows Aero GUI, new applications (such as Windows Calendar, Windows DVD Maker and some new games including Chess, Mahjong, and Purble Place), Internet Explorer 7, Windows Media Player 11, and a large number of underlying architectural changes. Windows Vista has the version number NT 6.0. Since its release, Windows Vista has had two service packs.

Windows Vista ships in six editions:

All editions (except Starter edition) are currently available in both 32-bit and 64-bit versions. The biggest advantage of the 64-bit version is breaking the 4 gigabyte memory barrier, which 32-bit computers cannot fully access.

Windows Server 2008, released on February 27, 2008, was originally known as Windows Server Codename "Longhorn". Windows Server 2008 builds on the technological and security advances first introduced with Windows Vista, and is significantly more modular than its predecessor, Windows Server 2003.

Windows Server 2008 ships in ten editions:

Windows 7 was released to manufacturing on July 22, 2009, and reached general retail availability on October 22, 2009. It was previously known by the codenames Blackcomb and Vienna. Windows 7 has the version number NT 6.1. Since its release, Windows 7 has had one service pack.

Some features of Windows 7 are faster booting, Device Stage, Windows PowerShell, less obtrusive User Account Control, multi-touch, and improved window management. Features included with Windows Vista and not in Windows 7 include the sidebar (although gadgets remain) and several programs that were removed in favor of downloading their Windows Live counterparts.

Windows 7 ships in six editions:
In some countries (Austria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, United Kingdom, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and Switzerland), there are other editions that lack some features such as Windows Media Player, Windows Media Center and Internet Explorer - these editions were called names such as "Windows 7 N."
Microsoft focuses on selling Windows 7 Home Premium and Professional. All editions, except the Starter edition, are available in both 32-bit and 64-bit versions.
Unlike the corresponding Vista editions, the Professional and Enterprise editions are supersets of the Home Premium edition.

At the Professional Developers Conference (PDC) 2008, Microsoft also announced Windows Server 2008 R2, as the server variant of Windows 7. Windows Server 2008 R2 ships in 64-bit versions (x64 and Itanium) only.

In 2010, Microsoft released Windows Thin PC or WinTPC, which is a feature-and size-reduced locked-down version of Windows 7 expressly designed to turn older PCs into thin clients. WinTPC is available for software assurance customers and relies on cloud computing in a business network. Wireless operation is supported since WinTPC has full wireless stack integration, but wireless operation may not be as good as the operation on a wired connection.
Windows Home Server 2011 code named 'Vail' was released on April 6, 2011. Windows Home Server 2011 is built on the Windows Server 2008 R2 code base and removed the Drive Extender drive pooling technology in the original Windows Home Server release. Windows Home Server 2011 is considered a "major release". Its predecessor was built on Windows Server 2003. WHS 2011 only supports x86-64 hardware.

Microsoft decided to discontinue Windows Home Server 2011 on July 5, 2012 while including its features into Windows Server 2012 Essentials. Windows Home Server 2011 was supported until April 12, 2016.

On October 26, 2012, Microsoft released Windows 8 to the public. One edition, Windows RT, runs on some system-on-a-chip devices with mobile 32-bit ARM (ARMv7) processors. Windows 8 features a redesigned user interface, designed to make it easier for touchscreen users to use Windows. The interface introduced an updated Start menu known as the Start screen, and a new full-screen application platform. The desktop interface is also present for running windowed applications, although Windows RT will not run any desktop applications not included in the system. On the Building Windows 8 blog, it was announced that a computer running Windows 8 can boot up much faster than Windows 7. New features also include USB 3.0 support, the Windows Store, the ability to run from USB drives with Windows To Go, and others. Windows 8 was given the kernel number NT 6.2, with its successor 8.1 receiving the kernel number 6.3. So far, neither has had any service packs yet, although many consider Windows 8.1 to be a service pack for Windows 8.

Windows 8 is available in the following editions:

The first public preview of Windows Server 2012 and was also shown by Microsoft at the 2011 Microsoft Worldwide Partner Conference.

Windows 8 Release Preview and Windows Server 2012 Release Candidate were both released on May 31, 2012. Product development on Windows 8 was completed on August 1, 2012, and it was released to manufacturing the same day. Windows Server 2012 went on sale to the public on September 4, 2012. Windows 8 went on sale October 26, 2012.

Windows 8.1 and Windows Server 2012 R2 were released on October 17, 2013. Windows 8.1 is available as an update in the Windows store for Windows 8 users only and also available to download for clean installation. The update adds new options for resizing the live tiles on the Start screen.

Windows 10, codenamed Threshold (Later Redstone), is the current release of the Microsoft Windows operating system. Unveiled on September 30, 2014, it was released on July 29, 2015. It was distributed without charge to Windows 7 and 8.1 users for one year after release. A number of new features like Cortana, the Microsoft Edge web browser, the ability to view Windows Store apps as a window instead of fullscreen, virtual desktops, revamped core apps, Continuum, and a unified Settings app were all features debuted in Windows 10. Microsoft has announced that Windows 10 will be the last major version of its series of operating systems to be released. Instead, Microsoft will release major updates to the operating system via download or in Windows Update, similar to the way updates are delivered in macOS.

So far, nine major versions of Windows 10 have been released, with the version 19H2 being the latest stable release, and 20H2 as the latest preview version.



Windows Server 2016 is a release of the Microsoft Windows Server operating system that was unveiled on September 30, 2014. Windows Server 2016 was officially released at Microsoft's Ignite Conference, September 26–30, 2016.

Windows Server 2019 is a release of the Microsoft Windows Server operating system.

Windows Server 2019 was announced on March 20, 2018, and the first Windows Insider preview version was released on the same day. It was released for general availability on October 2, 2018.

On October 6, 2018, the distribution of Windows version 1809 (build 17763) was paused while Microsoft investigated an issue with user data being deleted during an in-place upgrade. It affected systems where a user profile folder (e.g. Documents, Music or Pictures) had been moved to another location, but data was left in the original location. As Windows Server 2019 is based on the Windows version 1809 codebase, it too was removed from distribution at the time but was re-released on November 13, 2018. The software product life cycle for Server 2019 was reset in accordance with the new release date.
ويندوس 1.0 يس ا فارسن وف ويندوس



</doc>
<doc id="13696" url="https://en.wikipedia.org/wiki?curid=13696" title="Helsinki">
Helsinki

Helsinki ( , ; , ) is the capital and most populous city of Finland. Located on the shore of the Gulf of Finland, it is the seat of the region of Uusimaa in southern Finland, and has a population of . The city's urban area has a population of , making it by far the most populous urban area in Finland as well as the country's most important center for politics, education, finance, culture, and research. Helsinki is located north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. It has close historical ties with these three cities.

Together with the cities of Espoo, Vantaa, and Kauniainen, and surrounding commuter towns, Helsinki forms the Greater Helsinki metropolitan area, which has a population of nearly 1.5 million. Often considered to be Finland's only metropolis, it is the world's northernmost metro area with over one million people as well as the northernmost capital of an EU member state. After Stockholm and Oslo, Helsinki is the third largest municipality in the Nordic countries. Swedish and Finnish are both official langueges. The city is served by the international Helsinki Airport, located in the neighboring city of Vantaa, with frequent service to many destinations in Europe and Asia.

Helsinki was the World Design Capital for 2012, the venue for the 1952 Summer Olympics, and the host of the 52nd Eurovision Song Contest in 2007.

Helsinki has one of the highest urban standards of living in the world. In 2011, the British magazine "Monocle" ranked Helsinki the world's most liveable city in its liveable cities index. In the Economist Intelligence Unit's 2016 liveability survey, Helsinki was ranked ninth among 140 cities.

According to a theory presented in the 1630s, at the time of Swedish colonisation of coastal areas of Finland, colonists from Hälsingland in central Sweden had arrived at what is now known as the Vantaa River and called it "Helsingå" ("Helsinge River"), which gave rise to the names of Helsinge village and church in the 1300s. This theory is questionable, because dialect research suggests that the settlers arrived from Uppland and nearby areas. Others have proposed the name as having been derived from the Swedish word "helsing", an archaic form of the word "hals" (neck), referring to the narrowest part of a river, the rapids. Other Scandinavian cities at similar geographic locations were given similar names at the time, e.g. Helsingør in Denmark and Helsingborg in Sweden.

When a town was founded in Forsby village in 1548, it was named "Helsinge fors", "Helsinge rapids". The name refers to the Vanhankaupunginkoski rapids at the mouth of the river. The town was commonly known as "Helsinge" or "Helsing", from which the contemporary Finnish name arose.

Official Finnish Government documents and Finnish language newspapers have used the name "Helsinki" since 1819, when the Senate of Finland moved itself into the city from Turku, the former capital of Finland. The decrees issued in Helsinki were dated with Helsinki as the place of issue. This is how the form Helsinki came to be used in written Finnish. As part of the Grand Duchy of Finland in the Russian Empire, Helsinki was known as "Gelsingfors" in Russian.

In Helsinki slang, the city is called "Stadi" (from the Swedish word "stad", meaning "city") or "Hesa" (short for Helsinki). "" is the Northern Sami name of Helsinki.

In the Iron Age the area occupied by present-day Helsinki was inhabited by Tavastians. They used the area for fishing and hunting, but due to a lack of archeological finds it is difficult to say how extensive their settlements were. Pollen analysis has shown that there were cultivating settlements in the area in the 10th century and surviving historical records from the 14th century describe Tavastian settlements in the area.

Swedes colonized the coastline of the Helsinki region in the late 13th century after the successful Second Crusade to Finland, which led to the defeat of the Tavastians.

Helsinki was established as a trading town by King Gustav I of Sweden in 1550 as the town of Helsingfors, which he intended to be a rival to the Hanseatic city of Reval (today known as Tallinn). In order to populate his newly founded town, the King issued an order to resettle the bourgeoisie of Porvoo, Ekenäs, Rauma and Ulvila into the town. Little came of the plans as Helsinki remained a tiny town plagued by poverty, wars, and diseases. The plague of 1710 killed the greater part of the inhabitants of Helsinki. The construction of the naval fortress Sveaborg (in Finnish "Viapori", today also "Suomenlinna") in the 18th century helped improve Helsinki's status, but it was not until Russia defeated Sweden in the Finnish War and annexed Finland as the autonomous Grand Duchy of Finland in 1809 that the town began to develop into a substantial city. Russians besieged the Sveaborg fortress during the war, and about one quarter of the town was destroyed in an 1808 fire.

Russian Emperor Alexander I of Russia moved the Finnish capital from Turku to Helsinki in 1812 to reduce Swedish influence in Finland, and to bring the capital closer to Saint Petersburg. Following the Great Fire of Turku in 1827, the Royal Academy of Turku, which at the time was the country's only university, was also relocated to Helsinki and eventually became the modern University of Helsinki. The move consolidated the city's new role and helped set it on a path of continuous growth. This transformation is highly apparent in the downtown core, which was rebuilt in the neoclassical style to resemble Saint Petersburg, mostly to a plan by the German-born architect C. L. Engel. As elsewhere, technological advancements such as railroads and industrialization were key factors behind the city's growth.

Despite the tumultuous nature of Finnish history during the first half of the 20th century (including the Finnish Civil War and the Winter War which both left marks on the city), Helsinki continued its steady development. A landmark event was the 1952 Olympic Games, held in Helsinki. Finland's rapid urbanization in the 1970s, occurring late relative to the rest of Europe, tripled the population in the metropolitan area, and the Helsinki Metro subway system was built. The relatively sparse population density of Helsinki and its peculiar structure have often been attributed to the lateness of its growth.

Called the "Daughter of the Baltic", Helsinki is on the tip of a peninsula and on 315 islands. The inner city is located on a southern peninsula, "Helsinginniemi" ("Cape of Helsinki), which is rarely referred to by its actual name, Vironniemi ("Cape of Estonia"). Population density in certain parts of Helsinki's inner city area is comparatively higher, reaching in the district of Kallio, but as a whole Helsinki's population density of ranks the city as rather sparsely populated in comparison to other European capital cities. Outside of the inner city, much of Helsinki consists of postwar suburbs separated by patches of forest. A narrow, long Helsinki Central Park, stretching from the inner city to Helsinki's northern border, is an important recreational area for residents. The City of Helsinki has about 11,000 boat berths and possesses over 14,000 hectares (34,595 acres; 54.1 sq mi) of marine fishing waters adjacent to the Capital Region. Some 60 fish species are found in this area and recreational fishing is popular.

Major islands in Helsinki include Seurasaari, Vallisaari, Lauttasaari, and Korkeasaari – the lattermost being the site of Finland's largest zoo. Other noteworthy islands are the fortress island of Suomenlinna (Sveaborg), the military island of Santahamina, and Isosaari. Pihlajasaari island is a favorite summer spot for gay men and naturists, comparable to Fire Island in New York City.

The Helsinki metropolitan area, also known as the Capital Region (Finnish: "Pääkaupunkiseutu", Swedish: "Huvudstadsregionen") comprises four municipalities: Helsinki, Espoo, Vantaa, and Kauniainen. The Helsinki urban area is considered to be the only metropolis in Finland. It has a population of over 1.1 million, and is the most densely populated area of Finland. The Capital Region spreads over a land area of and has a population density of . With over 20 percent of the country's population in just 0.2 percent of its surface area, the area's housing density is high by Finnish standards.

The Helsinki Metropolitan Area (Greater Helsinki) consists of the cities of Helsinki Capital Region and ten surrounding municipalities. The Metropolitan Area covers and has a population of over 1.4 million, or about a fourth of the total population of Finland. The metropolitan area has a high concentration of employment: approximately 750,000 jobs. Despite the intensity of land use, the region also has large recreational areas and green spaces. The Greater Helsinki area is the world's northernmost urban area with a population of over one million people, and the northernmost EU capital city.

The Helsinki urban area is an officially recognized urban area in Finland, defined by its population density. The area stretches throughout 11 municipalities, and is the largest such area in Finland, with a land area of and approximately 1,2 million inhabitants.

Helsinki has a humid continental climate (Köppen: "Dfb") similar to that of Hokkaido or Nova Scotia coastal. Owing to the mitigating influence of the Baltic Sea and North Atlantic Current (see also Extratropical cyclone), temperatures during the winter are higher than the northern location might suggest, with the average in January and February around .

Winters in Helsinki are notably warmer than in the north of Finland, and the snow season is much shorter in the capital, due to it being in extreme Southern Finland and the urban heat island effect. Temperatures below occur a few times a year at most. However, because of the latitude, days last 5 hours and 48 minutes around the winter solstice with very low sun (at noon, the sun is a little bit over 6 degrees in the sky), and the cloudy weather at this time of year exacerbates darkness. Conversely, Helsinki enjoys long daylight during the summer; during the summer solstice, days last 18 hours and 57 minutes.

The average maximum temperature from June to August is around . Due to the marine effect, especially during hot summer days, daily temperatures are a little cooler and night temperatures higher than further inland. The highest temperature ever recorded in the city was , on 28 July 2019 at Kaisaniemi weather station, breaking the previous record of that was observed in July 1945 at Ilmala weather station. The lowest temperature ever recorded in the city was , on 10 January 1987 although an unofficial low of -35 was recorded in December 1876. Helsinki Airport (in Vantaa, north of the Helsinki city centre) recorded a temperature of , on 29 July 2010, and a low of , on 9 January 1987. Precipitation is received from frontal passages and thunderstorms. Thunderstorms are most common in the summer.

Helsinki is divided into three major areas: Helsinki Downtown (, ), North Helsinki (, ) and East Helsinki (, ).

Carl Ludvig Engel, appointed to plan a new city centre on his own, designed several neoclassical buildings in Helsinki. The focal point of Engel's city plan was the Senate Square. It is surrounded by the Government Palace (to the east), the main building of Helsinki University (to the west), and (to the north) the large Helsinki Cathedral, which was finished in 1852, twelve years after Engel's death. Helsinki's epithet, "The White City of the North", derives from this construction era.

Helsinki is also home to numerous Art Nouveau-influenced (Jugend in Finnish) buildings belonging to the romantic nationalism trend, designed in the early 20th century and strongly influenced by "Kalevala", which was a common theme of the era. Helsinki's Art Nouveau style is also featured in central residential districts, such as Katajanokka and Ullanlinna. An important architect of the Finnish Art Nouveau style was Eliel Saarinen, whose architectural masterpiece was the Helsinki Central Station.

Helsinki also features several buildings by Finnish architect Alvar Aalto, recognized as one of the pioneers of architectural functionalism. However, some of his works, such as the headquarters of the paper company Stora Enso and the concert venue Finlandia Hall, have been subject to divided opinions from the citizens.

Functionalist buildings in Helsinki by other architects include the Olympic Stadium, the Tennis Palace, the Rowing Stadium, the Swimming Stadium, the Velodrome, the Glass Palace, the Töölö Sports Hall, and Helsinki-Malmi Airport. The sports venues were built to serve the 1940 Helsinki Olympic Games; the games were initially cancelled due to the Second World War, but the venues fulfilled their purpose in the 1952 Olympic Games. Many of them are listed by DoCoMoMo as significant examples of modern architecture. The Olympic Stadium and Helsinki-Malmi Airport are also catalogued by the Finnish National Board of Antiquities as cultural-historical environments of national significance.

Helsinki's neoclassical buildings were often used as a backdrop for scenes set to take place in the Soviet Union in many Cold War era Hollywood movies, when filming in the USSR was not possible. Some of them include "The Kremlin Letter" (1970), "Reds" (1981), and "Gorky Park" (1983). Because some streetscapes were reminiscent of Leningrad's and Moscow's old buildings, they too were used in movie productions. At the same time the government secretly instructed Finnish officials not to extend assistance to such film projects.

The start of the 21st century marked the beginning of highrise construction in Helsinki, when the city decided to allow the construction of skyscrapers. As of April 2017 there are no skyscrapers taller than 100 meters in the Helsinki area, but there are several projects under construction or planning, mainly in Pasila and Kalasatama. An international architecture competition for at least 10 high-rises to be built in Pasila is being held. Construction of the towers will start before 2020. In Kalasatama, the first 35-story (130 m, 427 ft) and 32-story (122 m, 400 ft) residential towers are already under construction. Later they will be joined by a 37-story (140 metres, 459 ft), two 32-story (122 metres, 400 feet), 31-story (120 metres, 394 ft), and 27-story (100 metres, 328 ft) residential buildings. In the Kalasatama area, there will be about 15 high-rises within 10 years.
As is the case with all Finnish municipalities, Helsinki's city council is the main decision-making organ in local politics, dealing with issues such as urban planning, schools, health care, and public transport. The council is chosen in the nationally held municipal elections, which are held every four years.

Helsinki's city council consists of eighty-five members. Following the most recent municipal elections in 2017, the three largest parties are the National Coalition Party (25), the Green League (21), and the Social Democratic Party (12).

The Mayor of Helsinki is Jan Vapaavuori.

At 53 percent of the population, women form a greater proportion of Helsinki residents than the national average of 51 percent. Helsinki's population density of 2,739.36 people per square kilometre makes Helsinki the most densely-populated city in Finland. The life expectancy for men and women is slightly below the national averages: 75.1 years for men as compared to 75.7 years, 81.7 years for women as compared to 82.5 years.

Helsinki has experienced strong growth since the 1810s, when it replaced Turku as the capital of the Grand Duchy of Finland, which later became the sovereign Republic of Finland. The city continued its growth from that time on, with an exception during the Finnish Civil War. From the end of World War II up until the 1970s there was a massive exodus of people from the countryside to the cities of Finland, in particular Helsinki. Between 1944 and 1969 the population of the city nearly doubled from 275,000 to 525,600.

In the 1960s, the population growth of Helsinki began to decrease, mainly due to a lack of housing. Some residents began to move to the neighbouring cities of Espoo and Vantaa, resulting in increased population growth in both municipalities. Espoo's population increased ninefold in sixty years, from 22,874 people in 1950 to 244,353 in 2009. Vantaa saw an even more dramatic change in the same time span: from 14,976 in 1950 to 197,663 in 2009, a thirteenfold increase. These population changes prompted the municipalities of Greater Helsinki into more intense cooperation in areas such as public transportation – resulting in the foundation of HSL – and waste management. The increasing scarcity of housing and the higher costs of living in the capital region have pushed many daily commuters to find housing in formerly rural areas, and even further, to cities such as Lohja, Hämeenlinna, Lahti, and Porvoo.

Finnish and Swedish are the official languages of Helsinki. 79.1% of the citizens speak Finnish as their native language. 5.7% speak Swedish. The remaining 15.3% of the population speaks a native language other than Finnish or Swedish.

Helsinki slang is a regional dialect of the city. It combines influences mainly from Finnish and English, and has traditionally had strong Russian and Swedish influences. Finnish today is the common language of communication between Finnish speakers, Swedish speakers, and speakers of other languages (New Finns) in day-to-day affairs in the public sphere between unknown persons. Swedish is commonly spoken in city or national agencies specifically aimed at Finland-Swedish speakers, such as the Social Services Department on Hämeentie or the Luckan Cultural centre in Kamppi. Knowledge of Finnish is also essential in business and is usually a basic requirement in the employment market.

Finnish speakers surpassed Swedish speakers in 1890 to become the majority of the city's population. At the time, the population of Helsinki was 61,530.

As the crossroads of many international ports and Finland's largest airport, Helsinki is the global gateway to and from Finland. The city has Finland's largest immigrant population in both absolute and relative terms. There are over 140 nationalities represented in Helsinki. It is home to the world's largest Estonian community outside of Estonia.

Foreign citizens make up 9.6% of the population, while the total immigrant population makes up 16%. In 2018, 101,825 residents spoke a native language other than Finnish, Swedish, or one of the three Sami languages spoken in Finland, and 103,499 had a foreign background. The largest groups of residents not of Finnish background come from Russia (14,532), Estonia (9,065), and Somalia (6,845). One third of Finland's immigrant population lives in the city of Helsinki.

The number of people with a foreign mother tongue is expected to be 196,500 in 2035, or 26% of the population. 114,000 will speak non-European languages, which will be 15% of the population.

Greater Helsinki generates approximately one third of Finland's GDP. GDP per capita is roughly 1.3 times the national average. Helsinki profits on serviced-related IT and public sectors. Having moved from heavy industrial works, shipping companies also employ a substantial number of people.

The metropolitan area's gross value added per capita is 200% of the mean of 27 European metropolitan areas, equalling those of Stockholm and Paris. The gross value added annual growth has been around 4%.

83 of the 100 largest Finnish companies have their headquarters in Greater Helsinki. Two-thirds of the 200 highest-paid Finnish executives live in Greater Helsinki and 42% in Helsinki. The average income of the top 50 earners was 1.65 million euro.

The tap water is of excellent quality and it is supplied by long Päijänne Water Tunnel, one of the world's longest continuous rock tunnels.

The Temppeliaukio Church is a Lutheran church in the Töölö neighborhood of the city. The church was designed by architects and brothers Timo and Tuomo Suomalainen and opened in 1969. Built directly into solid rock, it is also known as the Church of the Rock and Rock Church. The Cathedral of the Diocese of Helsinki is the Helsinki Cathedral, completed in 1852. It is a major landmark in the city and has 1,300 seats.

The largest Orthodox congregation is the Orthodox Church of Helsinki. It has 20,000 members. Its main church is the Uspenski Cathedral. The two largest Catholic congregations are Saint Henry’s Cathedral Parish, with 4,552 members, established in 1860 and St. Mary Catholic Parish, with 4,107 members, established in 1854. The main Catholic churches are the Cathedral of Saint Henry and St. Mary's Church.

in 2017, 53.6% of the population belongs to Evangelical Lutheran Church of Finland. Helsinki is the least Lutheran municipality in Finland.

There are around 30 mosques in the Helsinki region. Many linguistic and ethnic groups such as Bangladeshis, Kosovars, Kurds and Bosniaks have established their own mosques. The largest congregation in both Helsinki and Finland is the Helsinki Islamic Center, established in 1995. It has over 2,800 members as of 2017, and it received 24,131€ in government assistance.

In 2015, imam Anas Hajar estimated that on big celebrations around 10,000 Muslims visit mosques. In 2004, it was estimated that there were 8,000 Muslims in Helsinki, 1.5% of the population at the time.

The main synagogue of Helsinki is the Helsinki Synagogue, located in Kamppi. It has over 1,200 members, out of the 1,800 Jews in Finland. The congregation includes a synagogue, Jewish kindergarten, school, library, Jewish meat shop, two Jewish cemeteries and an retirement home. Many Jewish organizations and societies are based there, and the synagogue publishes the main Jewish magazine in Finland, HaKehila.

Helsinki has 190 comprehensive schools, 41 upper secondary schools, and 15 vocational institutes. Half of the 41 upper secondary schools are private or state-owned, the other half municipal. Higher level education is given in eight universities (see the section "Universities" below) and four polytechnics.



Helsinki is one of the co-location centres of the Knowledge and Innovation Community (Future information and communication society) of The European Institute of Innovation and Technology (EIT).

The biggest historical museum in Helsinki is the National Museum of Finland, which displays a vast historical collection from prehistoric times to the 21st century. The museum building itself, a national romantic style neomedieval castle, is a tourist attraction. Another major historical museum is the Helsinki City Museum, which introduces visitors to Helsinki's 500-year history. The University of Helsinki also has many significant museums, including the Helsinki University Museum "Arppeanum" and the Finnish Museum of Natural History.

The Finnish National Gallery consists of three museums: Ateneum Art Museum for classical Finnish art, Sinebrychoff Art Museum for classical European art, and Kiasma Art Museum for modern art, in a building by architect Steven Holl. The old Ateneum, a neo-Renaissance palace from the 19th century, is one of the city's major historical buildings. All three museum buildings are state-owned through Senate Properties.

The city of Helsinki hosts its own art collection in the Helsinki Art Museum (HAM), primarily located in its Tennispalatsi gallery. Pieces outside of Tennispalatsi include about 200 public art pieces and all art held in property owned by the city.

The Design Museum is devoted to the exhibition of both Finnish and foreign design, including industrial design, fashion, and graphic design. Other museums in Helsinki include the Military Museum of Finland, Didrichsen Art Museum, Amos Rex Art Museum, and the Tram Museum.

Helsinki has three major theatres: The Finnish National Theatre, the Helsinki City Theatre, and the Swedish Theatre ("Svenska Teatern"). Other notable theatres in the city include the Alexander Theatre, "Q-teatteri", Savoy Theatre, KOM-theatre, and "Teatteri Jurkka".

Helsinki is home to two full-size symphony orchestras, the Helsinki Philharmonic Orchestra and the Finnish Radio Symphony Orchestra, both of which perform at the Helsinki Music Centre concert hall. Acclaimed contemporary composers Kaija Saariaho, Magnus Lindberg, Esa-Pekka Salonen, and Einojuhani Rautavaara, among others, were born and raised in Helsinki, and studied at the Sibelius Academy. The Finnish National Opera, the only full-time, professional opera company in Finland, is located in Helsinki. The opera singer Martti Wallén, one of the company's long-time soloists, was born and raised in Helsinki, as was mezzo-soprano Monica Groop.

Many widely renowned and acclaimed bands have originated in Helsinki, including Hanoi Rocks, HIM, Stratovarius, The 69 Eyes, Finntroll, Ensiferum, Wintersun, The Rasmus, Poets of the Fall, and Apocalyptica.

The city's main musical venues are the Finnish National Opera, the Finlandia concert hall, and the Helsinki Music Centre. The Music Centre also houses a part of the Sibelius Academy. Bigger concerts and events are usually held at one of the city's two big ice hockey arenas: the Hartwall Arena or the Helsinki Ice Hall. Helsinki has Finland's largest fairgrounds, the Messukeskus Helsinki.

Helsinki Arena hosted the Eurovision Song Contest 2007, the first Eurovision Song Contest arranged in Finland, following Lordi's win in 2006.

The Helsinki Festival is an annual arts and culture festival, which takes place every August (including the Night of the Arts).

Vappu is an annual carnival for students and workers.

At the Senate Square in fall 2010, Finland's largest open-air art exhibition to date took place: About 1.4 million people saw the international exhibition of "United Buddy Bears".

Helsinki was the 2012 World Design Capital, in recognition of the use of design as an effective tool for social, cultural, and economic development in the city. In choosing Helsinki, the World Design Capital selection jury highlighted Helsinki's use of 'Embedded Design', which has tied design in the city to innovation, "creating global brands, such as Nokia, Kone, and Marimekko, popular events, like the annual Helsinki Design Week, outstanding education and research institutions, such as the Aalto University School of Arts, Design and Architecture, and exemplary architects and designers such as Eliel Saarinen and Alvar Aalto".

Helsinki hosts many film festivals. Most of them are small venues, while some have generated interest internationally. The most prolific of these is the Love & Anarchy film festival, also known as Helsinki International Film Festival, which features films on a wide spectrum. Night Visions, on the other hand, focuses on genre cinema, screening horror, fantasy, and science fiction films in very popular movie marathons that last the entire night. Another popular film festival is DocPoint, a festival that focuses solely on documentary cinema.

Today, there are around 200 newspapers, 320 popular magazines, 2,100 professional magazines, 67 commercial radio stations, three digital radio channels, and one nationwide and five national public service radio channels.

Sanoma publishes Finland's journal of record, "Helsingin Sanomat", the tabloid "Ilta-Sanomat", the commerce-oriented "Taloussanomat", and the television channel Nelonen. Another Helsinki-based media house, Alma Media, publishes over thirty magazines, including the newspaper "Aamulehti", the tabloid "Iltalehti", and the commerce-oriented "Kauppalehti".

Finland's national public-broadcasting institution Yle operates five television channels and thirteen radio channels in both national languages. Yle is headquartered in the neighbourhood of Pasila. All TV channels are broadcast digitally, both terrestrially and on cable.

The commercial television channel MTV3 and commercial radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).

Helsinki has a long tradition of sports: the city gained much of its initial international recognition during the 1952 Summer Olympics, and the city has arranged sporting events such as the first World Championships in Athletics 1983 and 2005, and the European Championships in Athletics 1971, 1994, and 2012. Helsinki hosts successful local teams in both of the most popular team sports in Finland: football and ice hockey. Helsinki houses HJK Helsinki, Finland's largest and most successful football club, and IFK Helsingfors, their local rivals with 7 championship titles. The fixtures between the two are commonly known as Stadin derby. Helsinki's track and field club Helsingin Kisa-Veikot is also dominant within Finland. Ice hockey is popular among many Helsinki residents, who usually support either of the local clubs IFK Helsingfors (HIFK) or Jokerit. HIFK, with 14 Finnish championships titles, also plays in the highest bandy division, along with Botnia-69. The Olympic stadium hosted the first ever Bandy World Championship in 1957.

Helsinki was elected host-city of the 1940 Summer Olympics, but due to World War II they were canceled. Instead Helsinki was the host of the 1952 Summer Olympics. The Olympics were a landmark event symbolically and economically for Helsinki and Finland as a whole that was recovering from the winter war and the continuation war fought with the Soviet Union. Helsinki was also in 1983 the first ever city to host the World Championships in Athletics. Helsinki also hosted the event in 2005, thus also becoming the first city to ever host the Championships for a second time. The Helsinki City Marathon has been held in the city every year since 1980, usually in August. A Formula 3000 race through the city streets was held on 25 May 1997. In 2009 Helsinki was host of the European Figure Skating Championships, and in 2017 it hosted World Figure Skating Championships.

The backbone of Helsinki's motorway network consists of three semicircular beltways, Ring I, Ring II, and Ring III, which connect expressways heading to other parts of Finland, and the western and eastern arteries of "Länsiväylä" and "Itäväylä" respectively. While variants of a "Keskustatunneli" tunnel under the city centre have been repeatedly proposed, the plan remains on the drawing board.

Helsinki has some 390 cars per 1000 inhabitants. This is less than in cities of similar population and construction density, such as Brussels' 483 per 1000, Stockholm's 401, and Oslo's 413.

The Helsinki Central Railway Station is the main terminus of the rail network in Finland. Two rail corridors lead out of Helsinki, the Main Line to the north (to Tampere, Oulu, Rovaniemi), and the Coastal Line to the west (to Turku). The railway connection to the east branches from the Main Line outside of Helsinki at Kerava, and leads via Lahti to eastern parts of Finland and to Russia.

A majority of intercity passenger services in Finland originate or terminate at the Helsinki Central Railway Station. All major cities in Finland are connected to Helsinki by rail service, with departures several times a day. The most frequent service is to Tampere, with more than 25 intercity departures per day as of 2017. There are international services from Helsinki to Saint Petersburg and to Moscow in Russia. The Saint Petersburg to Helsinki route is operated with the Allegro high-speed trains.

A Helsinki to Tallinn Tunnel has been proposed and agreed upon by representatives of the cities. The rail tunnel would connect Helsinki to the Estonian capital Tallinn, further linking Helsinki to the rest of continental Europe by Rail Baltica.

Air traffic is handled primarily from Helsinki Airport, located approximately north of Helsinki's downtown area, in the neighbouring city of Vantaa. Helsinki's own airport, Helsinki-Malmi Airport, is mainly used for general and private aviation. Charter flights are available from Hernesaari Heliport.

Like many other cities, Helsinki was deliberately founded at a location on the sea in order to take advantage of shipping. The freezing of the sea imposed limitations on sea traffic up to the end of the 19th century. But for the last hundred years, the routes leading to Helsinki have been kept open even in winter with the aid of icebreakers, many of them built in the Helsinki Hietalahti shipyard. The arrival and departure of ships has also been a part of everyday life in Helsinki. Regular route traffic from Helsinki to Stockholm, Tallinn, and Saint Petersburg began as far back as 1837. Over 300 cruise ships and 360,000 cruise passengers visit Helsinki annually. There are international cruise ship docks in South Harbour, Katajanokka, West Harbour, and Hernesaari. Helsinki is the second busiest passenger port in Europe with approximately 11 million passengers in 2013. Ferry connections to Tallinn, Mariehamn, and Stockholm are serviced by various companies. Finnlines passenger-freight ferries to Gdynia, Poland; Travemünde, Germany; and Rostock, Germany are also available. St. Peter Line offers passenger ferry service to Saint Petersburg several times a week.

In the Helsinki metropolitan area, public transportation is managed by the Helsinki Regional Transport Authority, the metropolitan area transportation authority. The diverse public transport system consists of trams, commuter rail, the metro, bus lines, two ferry lines and a public bike system.

Helsinki's tram system has been in operation with electric drive continuously since 1900. 13 routes that cover the inner part of the city are operated. As of 2017, the city is expanding the tram network, with several major tram line construction projects under way. These include the 550 trunk line (Raide-Jokeri), roughly along Ring I around the city center, and a new tramway to the island of Laajasalo.
The Helsinki Metro, opened in 1982, is the only metro system in Finland, albeit the Helsinki commuter rail trains operate at metro-like frequencies. In 2006, the construction of the long debated extension of the metro into Western Helsinki and Espoo was approved. The extension finally opened after delays in November 2017. An eastern extension into the planned new district of Östersundom and neighboring Sipoo has also been seriously debated. Helsinki's metro system currently consists of 25 stations, with 14 of them underground.

The commuter rail system includes purpose-built double track for local services in two rail corridors along intercity railways, and the Ring Rail Line, an urban double-track railway with a station at the Helsinki Airport in Vantaa. Electric operation of commuter trains was first begun in 1969, and the system has been gradually expanded since. 15 different services are operated as of 2017, some extending outside of the Helsinki region. The frequent services run at a 10-minute headway in peak traffic.

Helsinki has no official sister cities, but it has a special partnership relation with:






</doc>
<doc id="13699" url="https://en.wikipedia.org/wiki?curid=13699" title="Hobart">
Hobart

Hobart () is the capital and most populous city of the Australian island state of Tasmania. With a population of approximately 240,342 (over 45% of Tasmania's population), it is the least populated Australian state capital city, and second smallest if territories are taken into account (after Darwin, Northern Territory). Founded in 1804 as a British penal colony, Hobart, formerly known as Hobart Town or Hobarton, is Australia's second oldest capital city after Sydney, New South Wales. Prior to British settlement, the Hobart area had been occupied for possibly as long as 35,000 years, by the semi-nomadic Mouheneener tribe, a sub-group of the Nuennone, or South-East tribe. The descendants of these Aboriginal Tasmanians often refer to themselves as 'Palawa'.

Since its foundation as a colonial outpost, the city has expanded from the mouth of Sullivans Cove in a generally north-south direction along both banks of the River Derwent, from 22 km inland from the estuary at Storm Bay to the point where the river reverts to fresh water at Bridgewater. Penal transportation ended in the 1850s, after which the city experienced periods of growth and decline. The early 20th century saw an economic boom on the back of mining, agriculture and other primary industries, and the loss of men who served in the world wars was counteracted by an influx of immigration. Despite the rise in migration from Asia and other non-English speaking parts of the world, Hobart's population remains predominantly ethnically Anglo-Celtic, and has the highest percentage of Australian-born residents among the Australian capital cities.

The city is located in the state's south-east on the estuary of the River Derwent, making it the most southern of Australia's capital cities. It is also the least populated state capital, and the only one smaller than Canberra (excluding Darwin, as it is not a state capital). Its harbour forms the second-deepest natural port in the world. Its skyline is dominated by the kunanyi/Mount Wellington, and much of the city's waterfront consists of reclaimed land. It is the financial and administrative hub of Tasmania, serving as the home port for both Australian and French Antarctic operations and acting as a major tourist hub, with over 1.192 million visitors in 2011-12. The metropolitan area is often referred to as "Greater Hobart", to differentiate it from the City of Hobart, one of the five local government areas that cover the city.

The first European settlement began in 1803 as a military camp at Risdon Cove on the eastern shores of the River Derwent, amid British concerns over the presence of French explorers. In 1804, along with the military, settlers and convicts from the abandoned Port Phillip settlement, the camp at Risdon Cove was moved by Captain David Collins to a better location at the present site of Hobart at Sullivans Cove. The city, initially known as "Hobart Town" or "Hobarton", was named after Lord Hobart, the British secretary of state for war and the colonies.

The area's indigenous inhabitants were members of the semi-nomadic "Mouheneener" tribe. Violent conflict with the European settlers, and the effects of diseases brought by them, dramatically reduced the aboriginal population, which was rapidly replaced by free settlers and the convict population. Charles Darwin visited Hobart Town in February 1836 as part of the "Beagle" expedition. He writes of Hobart and the Derwent estuary in his "Voyage of the Beagle":
...The lower parts of the hills which skirt the bay are cleared; and the bright yellow fields of corn, and dark green ones of potatoes, appear very luxuriant... I was chiefly struck with the comparative fewness of the large houses, either built or building. Hobart Town, from the census of 1835, contained 13,826 inhabitants, and the whole of Tasmania 36,505.

The River Derwent was one of Australia's finest deepwater ports and was the centre of the Southern Ocean whaling and sealing trades. The settlement rapidly grew into a major port, with allied industries such as shipbuilding.

Hobart Town became a city on 21 August 1842, and was renamed Hobart from the beginning of 1881.

Hobart is located on the estuary of the River Derwent in the state's south-east. Geologically Hobart is built predominantly on Jurassic dolerite around the foothills interspersed with smaller areas of Triassic siltstone and Permian mudstone. Hobart extends along both sides of the River Derwent; on the western shore from the Derwent valley in the north through the flatter areas of Glenorchy which rests on older Triassic sediment and into the hilly areas of New Town, Lenah Valley. Both of these areas rest on the younger Jurassic dolerite deposits, before stretching into the lower areas such as the beaches of Sandy Bay in the south, in the Derwent estuary. South of the Derwent estuary lies Storm Bay and the Tasman Peninsula.

The Eastern Shore also extends from the Derwent valley area in a southerly direction hugging the Meehan Range in the east before sprawling into flatter land in suburbs such as Bellerive. These flatter areas of the eastern shore rest on far younger deposits from the Quaternary. From there the city extends in an easterly direction through the Meehan Range into the hilly areas of Rokeby and Oakdowns, before reaching into the tidal flatland area of Lauderdale.

Hobart has access to a number of beach areas including those in the Derwent estuary itself; Sandy Bay, Cornelian Bay, Nutgrove, Kingston, Bellerive, and Howrah Beaches as well as many more in Frederick Henry Bay such as; Seven Mile, Roaches, Cremorne, Clifton, and Goats Beaches.

Hobart has a mild temperate oceanic climate (). The highest temperature recorded was on 4 January 2013 and the lowest was on 25 June 1972 and 11 July 1981. Annually, Hobart receives 40.8 clear days. Compared to other major Australian cities, Hobart has the fewest daily average hours of sunshine, with 5.9 hours per day. However, during the summer it has the most hours of daylight of any Australian city, with 15.3 hours on the summer solstice.

Although Hobart itself rarely receives snow during the winter (the city's geographic position keeps temperatures from plummeting far below zero Celsius), the adjacent kunanyi/Mount Wellington is frequently seen with a snowcap in winter. Mountain snow covering has also been known to occur during the other seasons. During the 20th century, the city itself has received snowfalls at sea level on average only once every 15 years; however, outer suburbs lying higher on the slopes of Mount Wellington receive snow more often, owing to cold air masses arriving from Antarctica coupled with them resting at higher altitude. These snow-bearing winds often carry on through Tasmania and Victoria to the Snowy Mountains in northern Victoria and southern New South Wales.
The average temperature of the sea ranges from in September to in February.

At the 2016 census there were 222,356 people in the Greater Hobart area making it the second least populated capital city in Australia. The City of Hobart local government area had a population of 50,439.

The most common occupation categories were professionals (22.6%), clerical and administrative workers (14.7%), technicians and trades workers (13.3%), community and personal service workers (12.8%), and managers (11.3%). The median weekly household income was $1,234, compared with $1,438 nationally.

At the 2016 census, the most commonly nominated ancestries were: 
20.2% of the population was born overseas at the 2016 census. The five largest groups of overseas-born were from England (3.6%), Mainland China (1.1%), New Zealand (0.9%), India (0.6%) and Germany (0.5%).

3.8% of the population, or 8,534 people, identified as Indigenous Australians (Aboriginal Australians and Torres Strait Islanders) in 2016.

At the 2016 census, 86.5% of the population spoke only English at home. The other languages most commonly spoken at home were Mandarin (1.3%) Greek (0.5%), Nepali (0.4%), German (0.4%) and Italian (0.3%).

In the 2016 census, 52.1% of Greater Hobart residents who responded to the question specified a Christian religion. Major religious affiliations were Anglican (19.8%), Catholic (17.0%) and Uniting Church (2.5%). In addition, 39.9% specified "No Religion" and 9.3% did not answer.

Hobart has a small Mormon community of around 642 (2011), with meetinghouses in Glenorchy, Rosny, and Glen Huon. There is also a synagogue where the Jewish community, of around 111 (2001), or 0.05% of the Hobart population, worships. Hobart has a Bahá'í community, with a Bahá'í Centre of Learning, located within the city.

In 2013, Hillsong Church established a Hillsong Connect campus in Hobart.

Shipping is significant to the city's economy. Hobart is the home port for the Antarctic activities of Australia and France. The port loads around 2,000 tonnes of Antarctic cargo a year for the Australian research vessel "Aurora Australis." The city is also a popular cruise ship destination during the summer months, with 47 such ships docking during the course of the 2016–17 summer season.

The city also supports many other industries. Major local employers include catamaran builder Incat, zinc refinery Nyrstar, Cascade Brewery and Cadbury's Chocolate Factory, Norske Skog and Wrest Point Casino. The city also supports a host of light industry manufacturers, as well as a range of redevelopment projects, including the $689 million Royal Hobart Hospital Redevelopment – standing as the states largest ever Health Infrastructure project. Tourism is a significant part of the economy, with visitors coming to the city to explore its historic inner suburbs and nationally acclaimed restaurants and cafes, as well as its vibrant music and nightlife culture. The two major draw-cards are the weekly market in Salamanca Place, and the Museum of Old and New Art. The city is also used as a base from which to explore the rest of Tasmania.

The last 15–20 years has seen Hobart's wine industry thrive as many vineyards have developed in countryside areas outside of the city in the Coal River Wine Region and D'Entrecasteaux Channel, including Moorilla Estate at Berriedale one of the most awarded vineyards in Australia.

Hobart is an Antarctic gateway city, with geographical proximity to East Antarctica and the Southern Ocean. Infrastructure is provided by the port of Hobart for scientific research and cruise ships, and Hobart International Airport supports an Antarctic Airlink to Wilkins Runway at Casey Station. Hobart is a logistics point for the French icebreaker "L'Astrolabe".

Hobart is the home port for the Australian and French Antarctic programs, and provides port services for other visiting Antarctic nations and Antarctic cruise ships. Antarctic and Southern Ocean expeditions are supported by a specialist cluster offering cold climate products, services and scientific expertise. The majority of these businesses and organisations are members of the Tasmanian polar network, supported in part by the Tasmanian State Government.

Tasmania has a high concentration of Antarctic and Southern Ocean scientists. Hobart is home to the following Antarctic and Southern Ocean scientific institutions:

Hobart serves as a focal point and mecca for tourism in the state of Tasmania. In 2016, Hobart received 1.8 million visitors, surpassing both Perth and Canberra, tying equally with Brisbane.

The Royal Tasmanian Botanical Gardens is a popular recreation area a short distance from the city centre. It is the second-oldest Botanic Gardens in Australia and holds extensive significant plant collections.

Hadley's Orient Hotel, on Hobart's Murray Street, is the oldest continuously operating hotel in Australia.

kunanyi/Mount Wellington, accessible by passing through Fern Tree, is the dominant feature of Hobart's skyline. Indeed, many descriptions of Hobart have used the phrase "nestled amidst the foothills", so undulating is the landscape. At 1,271 metres, the mountain has its own ecosystems, is rich in biodiversity and plays a large part in determining the local weather.

The Tasman Bridge is also a uniquely important feature of the city, connecting the two shores of Hobart and visible from many locations. The Hobart Synagogue is the oldest synagogue in Australia and a rare surviving example of an Egyptian Revival synagogue.

Hobart is known for its well-preserved Colonial architecture, much of it dating back to the Georgian and Victorian eras, giving the city a distinctly "Old World" feel. For locals, this became a source of discomfiture about the city's convict past, but is now a draw card for tourists. Regions within the city centre, such as Salamanca Place, contain many of the city's heritage-listed buildings. Historic homes and mansions also exist in the suburbs, much of the inner-city neighbourhoods are dotted with weatherboard cottages and two-storey Victorian houses.

Kelly's Steps were built in 1839 by shipwright and adventurer James Kelly to provide a short-cut from Kelly Street and Arthur Circus in Battery Point to the warehouse and dockyards district of Salamanca Place. In 1835, John Lee Archer designed and oversaw the construction of the sandstone Customs House, facing Sullivans Cove. Completed in 1840, it was used as Tasmania's parliament house, and is now commemorated by a pub bearing the same name (built in 1844) which is frequented by yachtsmen after they have completed the Sydney to Hobart yacht race.

Hobart is also home to many historic churches. The Scots Church (formerly known as St Andrew's) was built in Bathurst Street from 1834 to 1836, and a small sandstone building within the churchyard was used as the city's first Presbyterian Church. The Salamanca Place warehouses and the Theatre Royal were also constructed in this period. The Greek revival St George's Anglican Church in Battery Point was completed in 1838, and a classical tower, designed by James Blackburn, was added in 1847. St Joseph's was built in 1840. St David's Cathedral, Hobart's first cathedral, was consecrated in 1874.

Hobart has very few high rise buildings in comparison to other Australian cities. This is partly a result of height limits imposed due to Hobart's proximity to River Derwent and Mount Wellington.

Hobart is home to the Tasmanian Symphony Orchestra, which is resident at the Federation Concert Hall on the city's waterfront. It offers a year-round program of concerts and is thought to be one of the finest small orchestras in the world. Hobart also plays host to the University of Tasmania's acclaimed Australian International Symphony Orchestra Institute (AISOI) which brings pre-professional advanced young musicians to town from all over Australia and internationally. The AISOI plays host to a public concert season during the first two weeks of December every year focusing on large symphonic music. Like the Tasmanian Symphony Orchestra, the AISOI uses the Federation Concert Hall as its performing base.

Hobart is home to Australia's oldest theatre, the Theatre Royal, as well as the Playhouse theatre, the Backspace theatre and many smaller stage theatres. It also has three Village Cinema complexes, one each in Hobart CBD, Glenorchy and Rosny, with the possibility of a fourth being developed in Kingston. The State Cinema in North Hobart specialises in arthouse and foreign films.

Australia's first published novel, " Quintus Servinton", was written and published in Hobart. It was written by a convict, Henry Savery, in a Hobart prison cell in 1830, while serving a sentence for forgery. A generally autobiographical work, it's the story of what happens to a well educated man from a relatively well to do family, who makes poor choices in life.

The city has also long been home to a thriving classical, jazz, folk, punk, hip-hop, electro, metal and rock music scene. Internationally recognised musicians such as metal acts Striborg and Psycroptic, indie-electro bands The Paradise Motel and The Scientists of Modern Music, singer-songwriters Sacha Lucashenko (of The Morning After Girls), Michael Noga (of The Drones), and Monique Brumby, two-thirds of indie rock band Love of Diagrams, post punk band Sea Scouts, theremin player Miles Brown, blues guitarist Phil Manning (of blues-rock band Chain), power-pop group The Innocents are all successful expatriates. In addition, founding member of Violent Femmes, Brian Ritchie, now calls Hobart home, and has formed a local band, The Green Mist. Ritchie also curates the annual international arts festival MONA FOMA, held at Salamanca Place's waterfront venue, Princes Wharf, Shed No. 1. Hobart hosts many significant festivals including summer's Taste of Tasmania celebrating local produce, wine and music, "Dark Mofo" marking the winter solstice, Australia's premier festival celebration of voice the "Festival of Voices", and Tasmania's biennial international arts festival Ten Days On The Island. Other festivals, including the "Hobart Fringe Festival", Hobart Summer Festival, Southern Roots Festival, the Falls Festival in Marion Bay and the Soundscape Festival also capitalise on Hobart's artistic communities.

Hobart is home to the Tasmanian Museum and Art Gallery. The Meadowbank Estate winery and restaurant features a floor mural by Tom Samek, part funded by the Federal Government. The Museum of Old and New Art (MONA) opened in 2011 to coincide with the third annual MONA FOMA festival. The multi-storey MONA gallery was built directly underneath the historic Sir Roy Grounds courtyard house, overlooking the River Derwent. This building serves as the entrance to the MONA Gallery.

Designed by the prolific architect Sir Roy Grounds, the 17-storey Wrest Point Hotel Casino in Sandy Bay, opened as Australia's first legal casino in 1973.

The city's nightlife primarily revolves around Salamanca Place, the waterfront area, Elizabeth St in North Hobart and Sandy Bay, but popular pubs, bars and nightclubs exist around the city as well. Major national and international music events are usually held at the Derwent Entertainment Centre, or the Casino. Popular restaurant strips include Elizabeth Street in North Hobart, and Salamanca Place near the waterfront. These include numerous ethnic restaurants including Chinese, Thai, Greek, Pakistani, Italian, Indian and Mexican. The major shopping street in the CBD is Elizabeth Street, with the pedestrianised Elizabeth Mall and the General Post Office.

Close Shave, one of Australia's longest serving male a cappella quartets, is based in Hobart.
Hobart is internationally famous among the yachting community as the finish of the Sydney to Hobart Yacht Race which starts in Sydney on Boxing Day (the day after Christmas Day). The arrival of the yachts is celebrated as part of the Hobart Summer Festival, a food and wine festival beginning just after Christmas and ending in mid-January. The Taste of Tasmania is a major part of the festival, where locals and visitors can taste fine local and international food and wine.

The city is the finishing point of the Targa Tasmania rally car event, which has been held annually in April since 1991.

The annual Tulip Festival at the Royal Tasmanian Botanical Gardens is a popular Spring celebration in the city.

The Australian Wooden Boat Festival is a biennial event held in Hobart celebrating wooden boats. It is held concurrently with the Royal Hobart Regatta, which began in 1830 and is therefore Tasmania's oldest surviving sporting event.

Most professional Hobart-based sports teams represent Tasmania as a whole rather than exclusively the city.

Cricket is a popular game of the city. The Tasmanian Tigers cricket team plays its home games at the Bellerive Oval on the Eastern Shore. A new team, Hobart Hurricanes represent the city in the Big Bash League. Bellerive Oval has been the breeding ground of some world class cricket players including the former Australia captain Ricky Ponting.

Despite Australian rules football's huge popularity in the state of Tasmania, the state does not have a team in the Australian Football League. However, a bid for an Tasmanian AFL team is a popular topic among football fans. The State government is one of the potential sponsors of such a team. Local domestic club football is still played. Tasmanian State League football features five clubs from Hobart, and other leagues such as Southern Football League and the Old Scholars Football Association are also played each Winter.

The city has two local rugby league football teams (Hobart Tigers and South Hobart Storm) that compete in the Tasmanian Rugby League.

Tasmania is not represented by teams in the NRL, Super Rugby, ANZ Championship, A-League, or NBL. However, the Hobart Chargers do represent Hobart in the second-tier South East Australian Basketball League. Besides the bid for an AFL club which was passed over in favour of a second Queensland team, despite several major local businesses and the Premier pioneering for a club, there is also a Hobart bid for entry into the A-League.

Hockey Tasmania has a men's team (the Tasmanian Tigers) and a women's team (the Van Demons) competing in the Australian Hockey League. Hobart hosted the FIH junior men's world cup in 2001.

The city co-hosted the basketball FIBA Oceania Championship 1975.

Five free-to-air television stations service Hobart:
Each station broadcasts a primary channel and several multichannels.

Hobart is served by twenty-eight digital free-to-air television channels:

The majority of pay television services are provided by Foxtel via satellite, although other smaller pay television providers do service Hobart.

Commercial radio stations licensed to cover the Hobart market include Triple M Hobart, HIT 100.9 and 7HO FM. Local community radio stations include Christian radio station Ultra106five, Edge Radio and 92FM which targets the wider community with specialist programmes. The five ABC radio networks available on analogue radio broadcast to Hobart via 936 ABC Hobart, Radio National, Triple J, NewsRadio and ABC Classic FM. Hobart is also home to the video creation company Biteable.

Hobart's major newspaper is "The Mercury", which was founded by John Davies in 1854 and has been continually published ever since. The paper is owned and operated by Rupert Murdoch's News Limited.

Greater Hobart metropolitan area consists of five local government areas of which three, City of Hobart, City of Glenorchy and City of Clarence are designated as cities. Hobart also includes the urbanised local governments of the Municipality of Kingborough and Municipality of Brighton. Each local government services all the suburbs that are within its geographical boundaries and are responsible for their own urban area, up to a certain scale, and residential planning as well as waste management and mains water storage.

Most citywide events such as the Taste of Tasmania and Hobart Summer Festival are funded by the Tasmanian State Government as a joint venture with the Hobart City Council. Urban planning of the Hobart CBD in particular the Heritage listed areas such as Sullivans Cove are also intensely scrutinised by State Government, which is operated out of Parliament House on the waterfront.

Hobart is home to the main campus of the University of Tasmania, located in Sandy Bay. On-site accommodation colleges include Christ College, Jane Franklin Hall and St John Fisher College. Other campuses are in Launceston and Burnie.

The Greater Hobart area contains 122 primary, secondary and pretertiary (College) schools distributed throughout Clarence, Glenorchy and Hobart City Councils and Kingborough and Brighton Municipalities. These schools are made up of a mix of public, catholic, private and independent run, with the heaviest distribution lying in the more densely populated West around the Hobart city core. TasTAFE operates a total of seven polytechnic campuses within the Greater Hobart area that provide vocational education and training.

Royal Hobart Hospital is a major public hospital in central Hobart with 501 beds, which also serves as a teaching hospital for the University of Tasmania.

A private hospital, Hobart Private Hospital is located adjacent to it and operated by Australian healthcare provider Healthscope. The company also owns another hospital in the city, the St. Helen's Private Hospital, which features a mother-baby unit.

The only public transportation within the city of Hobart is via a network of Metro Tasmania buses funded by
the Tasmanian Government and a small number of private bus services. Like many large Australian cities, Hobart once operated passenger tram services, a trolleybus network consisting of six routes which operated until 1968. However, the tramway closed in the early 1960s. The tracks are still visible in the older streets of Hobart.

Suburban passenger trains, run by the Tasmanian Government Railways, were closed in 1974 and the intrastate passenger service, the Tasman Limited, ceased running in 1978. Recently though there has been a push from the city, and increasingly from government, to establish a light rail network, intended to be fast, efficient, and eco-friendly, along existing tracks in a North South corridor; to help relieve the frequent jamming of traffic in Hobart CBD.

The main arterial routes within the urban area are the Brooker Highway to Glenorchy and the northern suburbs, the Tasman Bridge and Bowen Bridge across the river to Rosny and the Eastern Shore. The East Derwent Highway to Lindisfarne, Geilston Bay, and Northwards to Brighton, the South Arm Highway leading to Howrah, Rokeby, Lauderdale and Opossum Bay and the Southern Outlet south to Kingston and the D'Entrecasteaux Channel. Leaving the city, motorists can travel the Lyell Highway to the west coast, Midland Highway to Launceston and the north, Tasman Highway to the east coast, or the Huon Highway to the far south.

Ferry services from Hobart's Eastern Shore into the city were once a common form of public transportation, but with lack of government funding, as well as a lack of interest from the private sector, there has been the demise of a regular commuter ferry service – leaving Hobart's commuters relying solely on travel by automobiles and buses. There is however a water taxi service operating from the Eastern Shore into Hobart which provides an alternative to the Tasman Bridge.

Hobart is served by Hobart International Airport with flights to/from Melbourne (Qantas, Virgin Australia, Jetstar Airways and Tiger Airways Australia); Sydney (Qantas, Jetstar and Virgin); Brisbane (Virgin); Perth (Virgin); Gold Coast (Tiger Airways); and Adelaide (Jetstar).The smaller Cambridge Aerodrome mainly serves small charter airlines offering local tourist flights. In the past decade, Hobart International Airport received a huge upgrade, with the airport now being a first class airport facility.

In 2009, it was announced that Hobart Airport would receive more upgrades, including a first floor, aerobridges (currently, passengers must walk on the tarmac) and shopping facilities. Possible new international flights to Asia and New Zealand, and possible new domestic flights to Darwin and Cairns have been proposed. A second runway, possibly to be constructed in the next 15 years, would assist with growing passenger numbers to Hobart. Hobart Control Tower may be renovated and fitted with new radar equipment, and the airport's carpark may be extended further. Also, new facilities will be built just outside the airport. A new service station, hotel and day care centre have already been built and the road leading to the airport has been maintained and re-sealed. In 2016, work began on a 500-metre extension of the existing runway in addition to a $100 million upgrade of the airport. The runway extension is expected to allow international flights to land and increase air-traffic with Antarctica. This upgrade was, in part, funded under a promise made during the 2013 federal election by the Abbott government.









</doc>
<doc id="13700" url="https://en.wikipedia.org/wiki?curid=13700" title="Hesiod">
Hesiod

Hesiod (; "Hēsíodos") was a Greek poet generally thought by scholars to have been active between 750 and 650 BC, around the same time as Homer. He is generally regarded as the first written poet in the Western tradition to regard himself as an individual persona with an active role to play in his subject. Ancient authors credited Hesiod and Homer with establishing Greek religious customs. Modern scholars refer to him as a major source on Greek mythology, farming techniques, early economic thought (he is sometimes considered history's first economist), archaic Greek astronomy and ancient time-keeping.

The dating of Hesiod's life is a contested issue in scholarly circles ("see § Dating below"). Epic narrative allowed poets like Homer no opportunity for personal revelations. However, Hesiod's extant work comprises several didactic poems in which he went out of his way to let his audience in on a few details of his life. There are three explicit references in "Works and Days", as well as some passages in his "Theogony" that support inferences made by scholars. The former poem says that his father came from Cyme in Aeolis (on the coast of Asia Minor, a little south of the island Lesbos) and crossed the sea to settle at a hamlet, near Thespiae in Boeotia, named Ascra, "a cursed place, cruel in winter, hard in summer, never pleasant" ("Works" 640). Hesiod's patrimony there, a small piece of ground at the foot of Mount Helicon, occasioned lawsuits with his brother Perses, who seems, at first, to have cheated him of his rightful share thanks to corrupt authorities or "kings" but later became impoverished and ended up scrounging from the thrifty poet ("Works" 35, 396).

Unlike his father, Hesiod was averse to sea travel, but he once crossed the narrow strait between the Greek mainland and Euboea to participate in funeral celebrations for one Athamas of Chalcis, and there won a tripod in a singing competition. He also describes a meeting between himself and the Muses on Mount Helicon, where he had been pasturing sheep when the goddesses presented him with a laurel staff, a symbol of poetic authority ("Theogony" 22–35). Fanciful though the story might seem, the account has led ancient and modern scholars to infer that he was not a professionally trained rhapsode, or he would have been presented with a lyre instead.

Some scholars have seen Perses as a literary creation, a foil for the moralizing that Hesiod develops in "Works and Days", but there are also arguments against that theory. For example, it is quite common for works of moral instruction to have an imaginative setting, as a means of getting the audience's attention, but it could be difficult to see how Hesiod could have travelled around the countryside entertaining people with a narrative about himself if the account was known to be fictitious. Gregory Nagy, on the other hand, sees both "Pérsēs" ("the destroyer" from , "pérthō") and "Hēsíodos" ("he who emits the voice" from , "híēmi" and , "audḗ") as fictitious names for poetical personae. 

It might seem unusual that Hesiod's father migrated from Asia Minor westwards to mainland Greece, the opposite direction to most colonial movements at the time, and Hesiod himself gives no explanation for it. However around 750 BC or a little later, there was a migration of seagoing merchants from his original home in Cyme in Asia Minor to Cumae in Campania (a colony they shared with the Euboeans), and possibly his move west had something to do with that, since Euboea is not far from Boeotia, where he eventually established himself and his family. The family association with Aeolian Cyme might explain his familiarity with eastern myths, evident in his poems, though the Greek world might have already developed its own versions of them.

In spite of Hesiod's complaints about poverty, life on his father's farm could not have been too uncomfortable if "Works and Days" is anything to judge by, since he describes the routines of prosperous yeomanry rather than peasants. His farmer employs a friend ("Works and Days" 370) as well as servants (502, 573, 597, 608, 766), an energetic and responsible ploughman of mature years (469 ff.), a slave boy to cover the seed (441–6), a female servant to keep house (405, 602) and working teams of oxen and mules (405, 607f.). One modern scholar surmises that Hesiod may have learned about world geography, especially the catalogue of rivers in "Theogony" (337–45), listening to his father's accounts of his own sea voyages as a merchant. The father probably spoke in the Aeolian dialect of Cyme but Hesiod probably grew up speaking the local Boeotian, belonging to the same dialect group. However, while his poetry features some Aeolisms there are no words that are certainly Boeotian. His basic language was the main literary dialect of the time, Homer's Ionian.

It is probable that Hesiod wrote his poems down, or dictated them, rather than passed them on orally, as rhapsodes did—otherwise the pronounced personality that now emerges from the poems would surely have been diluted through oral transmission from one rhapsode to another. Pausanias asserted that Boeotians showed him an old tablet made of lead on which the "Works" were engraved. If he did write or dictate, it was perhaps as an aid to memory or because he lacked confidence in his ability to produce poems extempore, as trained rhapsodes could do. It certainly wasn't in a quest for immortal fame since poets in his era had probably no such notions for themselves. However, some scholars suspect the presence of large-scale changes in the text and attribute this to oral transmission. Possibly he composed his verses during idle times on the farm, in the spring before the May harvest or the dead of winter.

The personality behind the poems is unsuited to the kind of "aristocratic withdrawal" typical of a rhapsode but is instead "argumentative, suspicious, ironically humorous, frugal, fond of proverbs, wary of women." He was in fact a misogynist of the same calibre as the later poet Semonides. He resembles Solon in his preoccupation with issues of good versus evil and "how a just and all-powerful god can allow the unjust to flourish in this life". He recalls Aristophanes in his rejection of the idealised hero of epic literature in favour of an idealised view of the farmer. Yet the fact that he could eulogise kings in "Theogony" (80 ff., 430, 434) and denounce them as corrupt in "Works and Days" suggests that he could resemble whichever audience he composed for.

Various legends accumulated about Hesiod and they are recorded in several sources: 

Two different—yet early—traditions record the site of Hesiod's grave. One, as early as Thucydides, reported in Plutarch, the "Suda" and John Tzetzes, states that the Delphic oracle warned Hesiod that he would die in Nemea, and so he fled to Locris, where he was killed at the local temple to Nemean Zeus, and buried there. This tradition follows a familiar ironic convention: the oracle predicts accurately after all. The other tradition, first mentioned in an epigram by Chersias of Orchomenus written in the 7th century BC (within a century or so of Hesiod's death) claims that Hesiod lies buried at Orchomenus, a town in Boeotia. According to Aristotle's "Constitution of Orchomenus," when the Thespians ravaged Ascra, the villagers sought refuge at Orchomenus, where, following the advice of an oracle, they collected the ashes of Hesiod and set them in a place of honour in their "agora", next to the tomb of Minyas, their eponymous founder. Eventually they came to regard Hesiod too as their "hearth-founder" (, "oikistēs"). Later writers attempted to harmonize these two accounts.

Greeks in the late 5th and early 4th centuries BC considered their oldest poets to be Orpheus, Musaeus, Hesiod and Homer—in that order. Thereafter, Greek writers began to consider Homer earlier than Hesiod. Devotees of Orpheus and Musaeus were probably responsible for precedence being given to their two cult heroes and maybe the Homeridae were responsible in later antiquity for promoting Homer at Hesiod's expense.

The first known writers to locate Homer earlier than Hesiod were Xenophanes and Heraclides Ponticus, though Aristarchus of Samothrace was the first actually to argue the case. Ephorus made Homer a younger cousin of Hesiod, the 5th century BC historian Herodotus ("Histories" II, 53) evidently considered them near-contemporaries, and the 4th century BC sophist Alcidamas in his work "Mouseion" even brought them together for an imagined poetic "ágōn" (), which survives today as the "Contest of Homer and Hesiod". Most scholars today agree with Homer's priority but there are good arguments on either side.

Hesiod certainly predates the lyric and elegiac poets whose work has come down to the modern era. Imitations of his work have been observed in Alcaeus, Epimenides, Mimnermus, Semonides, Tyrtaeus and Archilochus, from which it has been inferred that the latest possible date for him is about 650 BC.

An upper limit of 750 BC is indicated by a number of considerations, such as the probability that his work was written down, the fact that he mentions a sanctuary at Delphi that was of little national significance before c. 750 BC ("Theogony" 499), and that he lists rivers that flow into the Euxine, a region explored and developed by Greek colonists beginning in the 8th century BC. ("Theogony" 337–45).

Hesiod mentions a poetry contest at Chalcis in Euboea where the sons of one Amphidamas awarded him a tripod ("Works and Days" 654–662). Plutarch identified this Amphidamas with the hero of the Lelantine War between Chalcis and Eretria and he concluded that the passage must be an interpolation into Hesiod's original work, assuming that the Lelantine War was too late for Hesiod. Modern scholars have accepted his identification of Amphidamas but disagreed with his conclusion. The date of the war is not known precisely but estimates placing it around 730–705 BC, fit the estimated chronology for Hesiod. In that case, the tripod that Hesiod won might have been awarded for his rendition of "Theogony", a poem that seems to presuppose the kind of aristocratic audience he would have met at Chalcis.

Three works have survived which are attributed to Hesiod by ancient commentators: "Works and Days", "Theogony", and "Shield of Heracles". Only fragments exist of other works attributed to him. The surviving works and fragments were all written in the conventional metre and language of epic. However, the "Shield of Heracles" is now known to be spurious and probably was written in the sixth century BC. Many ancient critics also rejected "Theogony" (e.g., Pausanias 9.31.3), even though Hesiod mentions himself by name in that poem. "Theogony" and "Works and Days" might be very different in subject matter, but they share a distinctive language, metre, and prosody that subtly distinguish them from Homer's work and from the "Shield of Heracles" (see Hesiod's Greek below). Moreover, they both refer to the same version of the Prometheus myth. Yet even these authentic poems may include interpolations. For example, the first ten verses of the "Works and Days" may have been borrowed from an Orphic hymn to Zeus (they were recognised as not the work of Hesiod by critics as ancient as Pausanias).
Some scholars have detected a proto-historical perspective in Hesiod, a view rejected by Paul Cartledge, for example, on the grounds that Hesiod advocates a not-forgetting without any attempt at verification. Hesiod has also been considered the father of gnomic verse. He had "a passion for systematizing and explaining things". Ancient Greek poetry in general had strong philosophical tendencies and Hesiod, like Homer, demonstrates a deep interest in a wide range of 'philosophical' issues, from the nature of divine justice to the beginnings of human society. Aristotle ("Metaphysics" 983b–987a) believed that the question of first causes may even have started with Hesiod ("Theogony" 116–53) and Homer ("Iliad" 14.201, 246).

He viewed the world from outside the charmed circle of aristocratic rulers, protesting against their injustices in a tone of voice that has been described as having a "grumpy quality redeemed by a gaunt dignity" but, as stated in the biography section, he could also change to suit the audience. This ambivalence appears to underlie his presentation of human history in "Works and Days", where he depicts a golden period when life was easy and good, followed by a steady decline in behaviour and happiness through the silver, bronze, and Iron Ages – except that he inserts a heroic age between the last two, representing its warlike men as better than their bronze predecessors. He seems in this case to be catering to two different world-views, one epic and aristocratic, the other unsympathetic to the heroic traditions of the aristocracy.

The "Theogony" is commonly considered Hesiod's earliest work. Despite the different subject matter between this poem and the "Works and Days", most scholars, with some notable exceptions, believe that the two works were written by the same man. As M.L. West writes, "Both bear the marks of a distinct personality: a surly, conservative countryman, given to reflection, no lover of women or life, who felt the gods' presence heavy about him."

The "Theogony" concerns the origins of the world (cosmogony) and of the gods (theogony), beginning with Chaos, Gaia, Tartarus and Eros, and shows a special interest in genealogy. Embedded in Greek myth, there remain fragments of quite variant tales, hinting at the rich variety of myth that once existed, city by city; but Hesiod's retelling of the old stories became, according to Herodotus, the accepted version that linked all Hellenes.

The creation myth in Hesiod has long been held to have Eastern influences, such as the Hittite Song of Kumarbi and the Babylonian Enuma Elis. This cultural crossover would have occurred in the eighth and ninth century Greek trading colonies such as Al Mina in North Syria. (For more discussion, read Robin Lane Fox's "Travelling Heroes" and Walcot's "Hesiod and the Near East.")

The "Works and Days" is a poem of over 800 lines which revolves around two general truths: labour is the universal lot of Man, but he who is willing to work will get by. Scholars have interpreted this work against a background of agrarian crisis in mainland Greece, which inspired a wave of documented colonisations in search of new land. This poem is one of the earliest known musings on economic thought.

This work lays out the five Ages of Man, as well as containing advice and wisdom, prescribing a life of honest labour and attacking idleness and unjust judges (like those who decided in favour of Perses) as well as the practice of usury. It describes immortals who roam the earth watching over justice and injustice. The poem regards labor as the source of all good, in that both gods and men hate the idle, who resemble drones in a hive. In the horror of the triumph of violence over hard work and honor, verses describing the "Golden Age" present the social character and practice of nonviolent diet through agriculture and fruit-culture as a higher path of living sufficiently.

In addition to the "Theogony" and "Works and Days", numerous other poems were ascribed to Hesiod during antiquity. Modern scholarship has doubted their authenticity, and these works are generally referred to as forming part of the "Hesiodic Corpus" whether or not their authorship is accepted. The situation is summed up in this formulation by Glenn Most:

Of these works forming the extended Hesiodic corpus, only the "Shield of Heracles" (, "Aspis Hērakleous") is transmitted intact via a medieval manuscript tradition.

Classical authors also attributed to Hesiod a lengthy genealogical poem known as "Catalogue of Women" or "Ehoiai" (because sections began with the Greek words "ē hoiē," "Or like the one who ..."). It was a mythological catalogue of the mortal women who had mated with gods, and of the offspring and descendants of these unions.

Several additional hexameter poems were ascribed to Hesiod:

In addition to these works, the "Suda" lists an otherwise unknown "dirge for Batrachus, [Hesiod's] beloved".



The Roman bronze bust, the so-called "Pseudo-Seneca," of the late first century BC found at Herculaneum is now thought not to be of Seneca the Younger. It has been identified by Gisela Richter as an imagined portrait of Hesiod. In fact, it has been recognized since 1813 that the bust was not of Seneca, when an inscribed herma portrait of Seneca with quite different features was discovered. Most scholars now follow Richter's identification.

Hesiod employed the conventional dialect of epic verse, which was Ionian. Comparisons with Homer, a native Ionian, can be unflattering. Hesiod's handling of the dactylic hexameter was not as masterful or fluent as Homer's and one modern scholar refers to his "hobnailed hexameters". His use of language and meter in "Works and Days" and "Theogony" distinguishes him also from the author of the "Shield of Heracles". All three poets, for example, employed digamma inconsistently, sometimes allowing it to affect syllable length and meter, sometimes not. The ratio of observance/neglect of digamma varies between them. The extent of variation depends on how the evidence is collected and interpreted but there is a clear trend, revealed for example in the following set of statistics.
Hesiod does not observe digamma as often as the others do. That result is a bit counter-intuitive since digamma was still a feature of the Boeotian dialect that Hesiod probably spoke, whereas it had already vanished from the Ionic vernacular of Homer. This anomaly can be explained by the fact that Hesiod made a conscious effort to compose like an Ionian epic poet at a time when digamma was not heard in Ionian speech, while Homer tried to compose like an older generation of Ionian bards, when it was heard in Ionian speech. There is also a significant difference in the results for "Theogony" and "Works and Days", but that is merely due to the fact that the former includes a catalog of divinities and therefore it makes frequent use of the definite article associated with digamma, oἱ.

Though typical of epic, his vocabulary features some significant differences from Homer's. One scholar has counted 278 un-Homeric words in "Works and Days", 151 in "Theogony" and 95 in "Shield of Heracles". The disproportionate number of un-Homeric words in "W & D" is due to its un-Homeric subject matter. Hesiod's vocabulary also includes quite a lot of formulaic phrases that are not found in Homer, which indicates that he may have been writing within a different tradition.






</doc>
<doc id="13702" url="https://en.wikipedia.org/wiki?curid=13702" title="Hebrew numerals">
Hebrew numerals

The system of Hebrew numerals is a quasi-decimal alphabetic numeral system using the letters of the Hebrew alphabet.
The system was adapted from that of the Greek numerals in the late 2nd century BCE.

The current numeral system is also known as the "Hebrew alphabetic numerals" to contrast with earlier systems of writing numerals used in classical antiquity. These systems were inherited from usage in the Aramaic and Phoenician scripts, attested from c. 800 BC in the so-called Samaria ostraca and sometimes known as "Hebrew-Aramaic numerals", ultimately derived from the Egyptian Hieratic numerals.

The Greek system was adopted in Hellenistic Judaism and had been in use in Greece since about the 5th century BC.

In this system, there is no notation for zero, and the numeric values for individual letters are added together. Each unit (1, 2, ..., 9) is assigned a separate letter, each tens (10, 20, ..., 90) a separate letter, and the first four hundreds (100, 200, 300, 400) a separate letter. The later hundreds (500, 600, 700, 800 and 900) are represented by the sum of two or three letters representing the first four hundreds. To represent numbers from 1,000 to 999,999, the same letters are reused to serve as thousands, tens of thousands, and hundreds of thousands. Gematria (Jewish numerology) uses these transformations extensively.

In Israel today, the decimal system of Arabic numerals (ex. 0, 1, 2, 3, etc.) is used in almost all cases (money, age, date on the civil calendar). The Hebrew numerals are used only in special cases, such as when using the Hebrew calendar, or numbering a list (similar to a, b, c, d, etc.), much as Roman numerals are used in the West.

The Hebrew language has names for common numbers that range from zero to one million. Letters of the Hebrew alphabet are used to represent numbers in a few traditional contexts, for example in calendars. In other situations Arabic numerals are used. Cardinal and ordinal numbers must agree in gender with the noun they are describing. If there is no such noun (e.g. telephone numbers), the feminine form is used. For ordinal numbers greater than ten the cardinal is used and numbers above the value 20 have no gender.

Note: For ordinal numbers greater than 10, cardinal numbers are used instead.

Note: For numbers greater than 20, gender does not apply. Officially, numbers greater than million were represented by the long scale; However, since January 21st, 2013, the modified short scale (under which the long scale milliard is substituted for the strict short scale billion), which was already the colloquial standard, became official.

Cardinal and ordinal numbers must agree in gender (masculine or feminine; mixed groups are treated as masculine) with the noun they are describing. If there is no such noun (e.g. a telephone number or a house number in a street address), the feminine form is used. Ordinal numbers must also agree in number and definite status like other adjectives. The cardinal number precedes the noun (e.g., "shlosha yeladim"), except for the number one which succeeds it (e.g., "yeled echad"). The number two is special: "shnayim" (m.) and "shtayim" (f.) become "shney" (m.) and "shtey" (f.) when followed by the noun they count. For ordinal numbers (numbers indicating position) greater than ten the cardinal is used.

The Hebrew numeric system operates on the additive principle in which the numeric values of the letters are added together to form the total. For example, 177 is represented as קעז which (from right to left) corresponds to 100 + 70 + 7 = 177.

Mathematically, this type of system requires 27 letters (1-9, 10-90, 100-900). In practice the last letter, "tav" (which has the value 400) is used in combination with itself and/or other letters from "kof" (100) onwards, to generate numbers from 500 and above. Alternatively, the 22-letter Hebrew numeral set is sometimes extended to 27 by using 5 "sofit" (final) forms of the Hebrew letters.

By convention, the numbers 15 and 16 are represented as ט״ו (9 + 6) and ט״ז (9 + 7), respectively, in order to refrain from using the two-letter combinations י-ה (10 + 5) and י-ו (10 + 6), which are alternate written forms for the Name of God in everyday writing. In the calendar, this manifests every full moon, since all Hebrew months start on a new moon (see for example: Tu BiShvat).

Combinations which would spell out words with negative connotations are sometimes avoided by switching the order of the letters. For instance, 744 which should be written as תשמ״ד (meaning "you/it will be destroyed") might instead be written as תשד״מ or תמש״ד (meaning "end to demon").

The Hebrew numeral system has sometimes been extended to include the five final letter forms—ך (500‎), ם (600‎), ן (700‎), ף (800‎) and ץ (900‎)—which are then used to indicate the numbers from 500 to 900.

The ordinary forms for 500 to 900 are: ת״ק (500‎), ת״ר (600‎), ת״ש (700‎), ת״ת (800‎) and תת״ק (900‎).

Gershayim (U+05F4 in Unicode, and resembling a double quote mark) (sometimes erroneously referred to as "merkha'ot", which is Hebrew for double quote) are inserted before (to the right of) the last (leftmost) letter to indicate that the sequence of letters represents a number rather than a word. This is used in the case where a number is represented by two or more Hebrew numerals ("e.g.," 28 → כ״ח).

Similarly, a single Geresh (U+05F3 in Unicode, and resembling a single quote mark) is appended after (to the left of) a single letter to indicate that the letter represents a number rather than a (one-letter) word. This is used in the case where a number is represented by a single Hebrew numeral ("e.g.," 100 → ק׳).

Note that Geresh and Gershayim merely indicate ""not a (normal) word."" Context usually determines whether they indicate a number or something else (such as ""abbreviation"").

An alternative method found in old manuscripts and still found on modern-day tombstones is to put a dot above each letter of the number.

In print, Arabic numerals are employed in Modern Hebrew for most purposes. Hebrew numerals are used nowadays primarily for writing the days and years of the Hebrew calendar; for references to traditional Jewish texts (particularly for Biblical chapter and verse and for Talmudic folios); for bulleted or numbered lists (similar to "A", "B", "C", "etc.", in English); and in numerology (gematria).

Thousands are counted separately, and the thousands count precedes the rest of the number (to the "right", since Hebrew is read from right to left). There are no special marks to signify that the “count” is starting over with thousands, which can theoretically lead to ambiguity, although a single quote mark is sometimes used after the letter. When specifying years of the Hebrew calendar in the present millennium, writers usually omit the thousands (which is presently 5 []), but if they do not this is accepted to mean 5 * 1000, with no ambiguity. The current Israeli coinage includes the thousands.

“Monday, 15 Adar 5764” (where 5764 = 5(×1000) + 400 + 300 + 60 + 4, and 15 = 9 + 6):

“Thursday, 3 Nisan 5767” (where 5767 = 5(×1000) + 400 + 300 + 60 + 7):

To see how "today's" date in the Hebrew calendar is written, see, for example, Hebcal date converter.

5780 (2019–20) = ה׳תש״פ

5779 (2018–19) = ה׳תשע״ט

5772 (2011–12) = ה׳תשע״ב

5771 (2010–11) = ה׳תשע״א

5770 (2009–10) = ה׳תש״ע

5769 (2008–09) = ה׳תשס״ט

5761 (2000–01) = ה׳תשס״א

5760 (1999–00) = ה׳תש״ס

The Abjad numerals are equivalent to the Hebrew numerals up to 400. The Greek numerals differ from the Hebrew ones from 90 upwards because in the Greek alphabet there is no equivalent for "Tsade" (צ).




</doc>
<doc id="13704" url="https://en.wikipedia.org/wiki?curid=13704" title="Hydroxy">
Hydroxy

Hydroxy can refer to:


</doc>
<doc id="13706" url="https://en.wikipedia.org/wiki?curid=13706" title="Hero">
Hero

A hero is a real person or a main fictional character who, in the face of danger, combats adversity through feats of ingenuity, courage, or strength. Like other formerly solely gender-specific terms (like "actor"), "hero" is often used to refer to both men and women, though "heroine" only refers to women. The original hero type of classical epics did such things for the sake of glory and honor. On the other hand, are post-classical and modern heroes, who perform great deeds or selfless acts for the common good instead of the classical goal of wealth, pride, and fame. The antonym of a hero is a villain. Other terms associated with the concept of a hero, may include "good guy" or "white hat".

In classical literature, the hero is the main or revered character in heroic epic poetry celebrated through ancient legends of a people, often striving for military conquest and living by a continually flawed personal honor code. The definition of a hero has changed throughout time. Merriam Webster dictionary defines a hero as "a person who is admired for great or brave acts or fine qualities." Examples of heroes range from mythological figures, such as Gilgamesh, Achilles and Iphigenia, to historical figures, such as Joan of Arc, Giuseppe Garibaldi or Sophie Scholl, modern heroes like Alvin York, Audie Murphy and Chuck Yeager, and fictional superheroes, including Superman, Spider-Man, Batman, and Captain America.

The word "hero" comes from the Greek ἥρως ("hērōs"), "hero" (literally "protector" or "defender"), particularly one such as Heracles with divine ancestry or later given divine honors. Before the decipherment of Linear B the original form of the word was assumed to be *, "hērōw-", but the Mycenaean compound "ti-ri-se-ro-e" demonstrates the absence of -w-. Hero as a name appears in pre-Homeric Greek mythology, wherein Hero was a priestess of the goddess, Aphrodite, in a myth that has been referred to often in literature.

According to "The American Heritage Dictionary of the English Language", the Proto-Indo-European root is "*ser" meaning "to protect". According to Eric Partridge in "Origins", the Greek word "hērōs" "is akin to" the Latin "seruāre", meaning "to safeguard". Partridge concludes, "The basic sense of both Hera and hero would therefore be 'protector'." R. S. P. Beekes rejects an Indo-European derivation and asserts that the word has a Pre-Greek origin. Hera was a Greek goddess with many attributes, including protection and her worship appears to have similar proto-Indo-European origins.

A classical hero is considered to be a "warrior who lives and dies in the pursuit of honor" and asserts their greatness by "the brilliancy and efficiency with which they kill". Each classical hero's life focuses on fighting, which occurs in war or during an epic quest. Classical heroes are commonly semi-divine and extraordinarily gifted, such as Achilles, evolving into heroic characters through their perilous circumstances. While these heroes are incredibly resourceful and skilled, they are often foolhardy, court disaster, risk their followers' lives for trivial matters, and behave arrogantly in a childlike manner. During classical times, people regarded heroes with the highest esteem and utmost importance, explaining their prominence within epic literature. The appearance of these mortal figures marks a revolution of audiences and writers turning away from immortal gods to mortal mankind, whose heroic moments of glory survive in the memory of their descendants, extending their legacy.

Hector was a Trojan prince and the greatest fighter for Troy in the Trojan War, which is known primarily through Homer's "Iliad". Hector acted as leader of the Trojans and their allies in the defense of Troy, "killing 31,000 Greek fighters," offers Hyginus. Hector was known not only for his courage, but also for his noble and courtly nature. Indeed, Homer places Hector as peace-loving, thoughtful, as well as bold, a good son, husband and father, and without darker motives. However, his familial values conflict greatly with his heroic aspirations in the "Iliad," as he cannot be both the protector of Troy and a father to his child. Hector is ultimately betrayed by the deities when Athena appears disguised as his ally Deiphobus and convinces him challenge Achilles, leading to his death at the hands of a superior warrior.
Achilles was a Greek hero who was considered the most formidable military fighter in the entire Trojan War and the central character of the "Iliad". He was the child of Thetis and Peleus, making him a demi-god. He wielded superhuman strength on the battlefield and was blessed with a close relationship to the deities. Achilles famously refused to fight after his dishonoring at the hands of Agamemnon, and only returned to the war due to unadulterated rage after Hector killed his close friend Patroclus. Achilles was known for uncontrollable rage that defined many of his bloodthirsty actions, such as defiling Hector's corpse by dragging it around the city of Troy. Achilles plays a tragic role in the "Iliad" brought about by constant de-humanization throughout the epic, having his "menis" (wrath) overpower his "philos" (love).

Heroes in myth often had close, but conflicted relationships with the deities. Thus Heracles's name means "the glory of Hera", even though he was tormented all his life by Hera, the Queen of the Greek deities. Perhaps the most striking example is the Athenian king Erechtheus, whom Poseidon killed for choosing Athena rather than him as the city's patron deity. When the Athenians worshiped Erechtheus on the Acropolis, they invoked him as "Poseidon Erechtheus".

Fate, or destiny, plays a massive role in the stories of classical heroes. The classical hero's heroic significance stems from battlefield conquests, an inherently dangerous action. The deities in Greek mythology, when interacting with the heroes, often foreshadow the hero's eventual death on the battlefield. Countless heroes and deities go to great lengths to alter their pre-destined fates, but with no success, as none, neither human or immortal can change their prescribed outcomes by the three powerful Fates. The most characteristic example of this is found in "Oedipus Rex." After learning that his son, Oedipus, will end up killing him, the King of Thebes, Laius, takes huge steps to assure his son's death by removing him from the kingdom. But, Oedipus slays his father without an afterthought when he was unknown to him and he encounters him in a dispute on the road many years later. The lack of recognition enabled Oedipus to slay his father, ironically further binding his father to his fate.

Stories of heroism may serve as moral examples. However, classical heroes often didn't embody the Christian notion of an upstanding, perfectly moral hero. For example, Achilles's character-issues of hateful rage lead to merciless slaughter and his overwhelming pride lead to him only joining the Trojan War because he didn't want his soldiers to win all of the glory. Classical heroes, regardless of their morality, were placed in religion. In classical antiquity, cults that venerated deified heroes such as Heracles, Perseus, and Achilles played an important role in Ancient Greek religion. These ancient Greek hero cults worshipped heroes from oral epic tradition, with these heroes often bestowing blessings, especially healing ones, on individuals.

The concept of the "Mythic Hero Archetype" was first developed by Lord Raglan in his 1936 book, "The Hero, A Study in Tradition, Myth and Drama". It is a set of 22 common traits that he said were shared by many heroes in various cultures, myths, and religions throughout history and around the world. Raglan argued that the higher the score, the more likely the figure is mythical.

The concept of a story archetype of the standard monomythical "hero's quest" that was reputed to be pervasive across all cultures, is somewhat controversial. Expounded mainly by Joseph Campbell in his 1949 work "The Hero with a Thousand Faces", it illustrates several uniting themes of hero stories that hold similar ideas of what a hero represents, despite vastly different cultures and beliefs. The monomyth or Hero's Journey consists of three separate stages including the Departure, Initiation, and Return. Within these stages there are several archetypes that the hero of either gender may follow, including the call to adventure (which they may initially refuse), supernatural aid, proceeding down a road of trials, achieving a realization about themselves (or an apotheosis), and attaining the freedom to live through their quest or journey. Campbell offered examples of stories with similar themes such as Krishna, Buddha, Apollonius of Tyana, and Jesus. One of the themes he explores is the androgynous hero, who combines male and female traits, such as Bodhisattva: "The first wonder to be noted here is the androgynous character of the Bodhisattva: masculine Avalokiteshvara, feminine Kwan Yin." In his 1968 book, "The Masks of God: Occidental Mythology", Campbell writes, "It is clear that, whether accurate or not as to biographical detail, the moving legend of the Crucified and Risen Christ was fit to bring a new warmth, immediacy, and humanity, to the old motifs of the beloved Tammuz, Adonis, and Osiris cycles."

Vladimir Propp, in his analysis of Russian fairy tales, concluded that a fairy tale had only eight "dramatis personæ", of which one was the hero, and his analysis has been widely applied to non-Russian folklore. The actions that fall into such a hero's sphere include:
Propp distinguished between "seekers" and "victim-heroes". A villain could initiate the issue by kidnapping the hero or driving him out; these were victim-heroes. On the other hand, an antagonist could rob the hero, or kidnap someone close to him, or, without the villain's intervention, the hero could realize that he lacked something and set out to find it; these heroes are seekers. Victims may appear in tales with seeker heroes, but the tale does not follow them both.

No history can be written without consideration of the lengthy list of recipients of national medals for bravery, populated by firefighters, policemen and policewomen, ambulance medics, and ordinary have-a-go heroes. These persons risked their lives to try to save or protect the lives of others: for example, the Canadian Cross of Valour (C.V.) "recognizes acts of the most conspicuous courage in circumstances of extreme peril"; examples of recipients are Mary Dohey and David Gordon Cheverie.

The philosopher Hegel gave a central role to the "hero", personalized by Napoleon, as the incarnation of a particular culture's "Volksgeist", and thus of the general "Zeitgeist". Thomas Carlyle's 1841 work, "On Heroes, Hero Worship and the Heroic in History", also accorded a key function to heroes and great men in history. Carlyle centered history on the biography of a few central individuals such as Oliver Cromwell or Frederick the Great. His heroes were political and military figures, the founders or topplers of states. His history of great men included geniuses good and, perhaps for the first time in historical study, evil.

Explicit defenses of Carlyle's position were rare in the second part of the 20th century. Most in the philosophy of history school contend that the motive forces in history may best be described only with a wider lens than the one that Carlyle used for his portraits. For example, Karl Marx argued that history was determined by the massive social forces at play in "class struggles", not by the individuals by whom these forces are played out. After Marx, Herbert Spencer wrote at the end of the 19th century: "You must admit that the genesis of the great man depends on the long series of complex influences which has produced the race in which he appears, and the social state into which that race has slowly grown...[b]efore he can remake his society, his society must make him." Michel Foucault argued in his analysis of societal communication and debate that history was mainly the "science of the sovereign", until its inversion by the "historical and political popular discourse".

Modern examples of the typical hero are, Minnie Vautrin, Norman Bethune, Alan Turing, Raoul Wallenberg, Chiune Sugihara, Martin Luther King, Jr., Mother Teresa, Nelson Mandela, Oswaldo Payá, Óscar Elías Biscet, and Aung San Suu Kyi.

The Annales school, led by Lucien Febvre, Marc Bloch, and Fernand Braudel, would contest the exaggeration of the role of individual subjects in history. Indeed, Braudel distinguished various time scales, one accorded to the life of an individual, another accorded to the life of a few human generations, and the last one to civilizations, in which geography, economics, and demography play a role considerably more decisive than that of individual subjects.

Among noticeable events in the studies of the role of the hero and great man in history one should mention Sidney Hook's book (1943) "The Hero in History". In the second half of the twentieth century such male-focused theory has been contested, among others by feminists writers such as Judith Fetterley in "The Resisting Reader" (1977) and literary theorist Nancy K. Miller, "The Heroine's Text: Readings in the French and English Novel, 1722–1782".

In the epoch of globalization an individual may change the development of the country and of the whole world, so this gives reasons to some scholars to suggest returning to the problem of the role of the hero in history from the viewpoint of modern historical knowledge and using up-to-date methods of historical analysis.

Within the frameworks of developing counterfactual history, attempts are made to examine some hypothetical scenarios of historical development. The hero attracts much attention because most of those scenarios are based on the suppositions: what would have happened if this or that historical individual had or had not been alive.

The word "hero" (or "heroine" in modern times), is sometimes used to describe the protagonist or the romantic interest of a story, a usage which may conflict with the superhuman expectations of heroism. A good example is Anna Karenina, the lead character in the novel of the same title by Leo Tolstoy. In modern literature the hero is more and more a problematic concept. In 1848, for example, William Makepeace Thackeray gave "Vanity Fair" the subtitle, "A Novel without a Hero", and imagined a world in which no sympathetic character was to be found. "Vanity Fair" is a satirical representation of the absence of truly moral heroes in the modern world. The story focuses on the characters, Emmy Sedley and Becky Sharpe (the latter as the clearly defined anti-hero), with the plot focused on the eventual marriage of these two characters to rich men, revealing character flaws as the story progresses. Even the most sympathetic characters, such as Captain Dobbin, are susceptible to weakness, as he is often narcissistic and melancholy.

The larger-than-life hero is a more common feature of fantasy (particularly in comic books and epic fantasy) than more realist works. However, these larger-than life figures remain prevalent in society. The superhero genre is a multibillion-dollar industry that includes comic books, movies, toys, and video games. Superheroes usually possess extraordinary talents and powers that no living human could ever possess. The superhero stories often pit a super villain against the hero, with the hero fighting the crime caused by the super villain. Examples of long-running superheroes include Superman, Wonder Woman, Batman, and Spider-Man.

Social psychology has begun paying attention to heroes and heroism. Zeno Franco and Philip Zimbardo point out differences between heroism and altruism, and they offer evidence that observer perceptions of unjustified risk play a role above and beyond risk type in determining the ascription of heroic status.

Psychologists have also identified the traits of heroes. Elaine Kinsella and her colleagues have identified 12 central traits of heroism, which consist of brave, moral integrity, conviction, courageous, self-sacrifice, protecting, honest, selfless, determined, saves others, inspiring, and helpful. Scott Allison and George Goethals uncovered evidence for "the great eight traits" of heroes consisting of wise, strong, resilient, reliable, charismatic, caring, selfless, and inspiring. These researchers have also identified four primary functions of heroism. Heroes give us wisdom; they enhance us; they provide moral modeling; and they offer protection.

An evolutionary psychology explanation for heroic risk-taking is that it is a costly signal demonstrating the ability of the hero. It may be seen as one form of altruism for which there are several other evolutionary explanations as well.

Roma Chatterji has suggested that the hero or more generally protagonist is first and foremost a symbolic representation of the person who is experiencing the story while reading, listening, or watching; thus the relevance of the hero to the individual relies a great deal on how much similarity there is between them and the character. Chatterji suggested that one reason for the hero-as-self interpretation of stories and myths is the human inability to view the world from any perspective but a personal one.

In the Pulitzer Prize-winning book, "The Denial of Death", Ernest Becker argues that human civilization is ultimately an elaborate, symbolic defense mechanism against the knowledge of our mortality, which in turn acts as the emotional and intellectual response to our basic survival mechanism. Becker explains that a basic duality in human life exists between the physical world of objects and a symbolic world of human meaning. Thus, since humanity has a dualistic nature consisting of a physical self and a symbolic self, he asserts that humans are able to transcend the dilemma of mortality through heroism, by focusing attention mainly on the symbolic selve. This symbolic self-focus takes the form of an individual's "immortality project" (or ""causa sui" project"), which is essentially a symbolic belief-system that ensures that one is believed superior to physical reality. By successfully living under the terms of the immortality project, people feel they can become heroic and, henceforth, part of something eternal; something that will never die as compared to their physical body. This he asserts, in turn, gives people the feeling that their lives have meaning, a purpose, and are significant in the grand scheme of things. Another theme running throughout the book is that humanity's traditional "hero-systems", such as religion, are no longer convincing in the age of reason. Science attempts to serve as an immortality project, something that Becker believes it can never do, because it is unable to provide agreeable, absolute meanings to human life. The book states that we need new convincing "illusions" that enable people to feel heroic in ways that are agreeable. Becker, however, does not provide any definitive answer, mainly because he believes that there is no perfect solution. Instead, he hopes that gradual realization of humanity's innate motivations, namely death, may help to bring about a better world. Terror Management Theory (TMT) has generated evidence supporting this perspective.



</doc>
<doc id="13711" url="https://en.wikipedia.org/wiki?curid=13711" title="Hydroxide">
Hydroxide

Hydroxide is a diatomic anion with chemical formula OH. It consists of an oxygen and hydrogen atom held together by a covalent bond, and carries a negative electric charge. It is an important but usually minor constituent of water. It functions as a base, a ligand, a nucleophile, and a catalyst. The hydroxide ion forms salts, some of which dissociate in aqueous solution, liberating solvated hydroxide ions. Sodium hydroxide is a multi-million-ton per annum commodity chemical. A hydroxide attached to a strongly electropositive center may itself ionize, liberating a hydrogen cation (H), making the parent compound an acid.

The corresponding electrically neutral compound HO is the hydroxyl radical. The corresponding covalently-bound group –OH of atoms is the hydroxy group.
Hydroxide ion and hydroxy group are nucleophiles and can act as a catalysts in organic chemistry.

Many inorganic substances which bear the word "hydroxide" in their names are not ionic compounds of the hydroxide ion, but covalent compounds which contain hydroxy groups.

The hydroxide ion is a natural part of water because of the self-ionization reaction in which its complement, hydronium, is passed hydrogen:
The equilibrium constant for this reaction, defined as
has a value close to 10 at 25 °C, so the concentration of hydroxide ions in pure water is close to 10 mol∙dm, in order to satisfy the equal charge constraint. The pH of a solution is equal to the decimal cologarithm of the hydrogen cation concentration; the pH of pure water is close to 7 at ambient temperatures. The concentration of hydroxide ions can be expressed in terms of pOH, which is close to (14 − pH), so the pOH of pure water is also close to 7. Addition of a base to water will reduce the hydrogen cation concentration and therefore increase the hydroxide ion concentration (increase pH, decrease pOH) even if the base does not itself contain hydroxide. For example, ammonia solutions have a pH greater than 7 due to the reaction NH + H , which decreases the hydrogen cation concentration, which increases the hydroxide ion concentration. pOH can be kept at a nearly constant value with various buffer solutions.

In aqueous solution the hydroxide ion is a base in the Brønsted–Lowry sense as it can accept a proton from a Brønsted–Lowry acid to form a water molecule. It can also act as a Lewis base by donating a pair of electrons to a Lewis acid. In aqueous solution both hydrogen and hydroxide ions are strongly solvated, with hydrogen bonds between oxygen and hydrogen atoms. Indeed, the bihydroxide ion has been characterized in the solid state. This compound is centrosymmetric and has a very short hydrogen bond (114.5 pm) that is similar to the length in the bifluoride ion (114 pm). In aqueous solution the hydroxide ion forms strong hydrogen bonds with water molecules. A consequence of this is that concentrated solutions of sodium hydroxide have high viscosity due to the formation of an extended network of hydrogen bonds as in hydrogen fluoride solutions.

In solution, exposed to air, the hydroxide ion reacts rapidly with atmospheric carbon dioxide, acting as an acid, to form, initially, the bicarbonate ion.
The equilibrium constant for this reaction can be specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see Carbonic acid for values and details). At neutral or acid pH, the reaction is slow, but is catalyzed by the enzyme carbonic anhydrase, which effectively creates hydroxide ions at the active site.

Solutions containing the hydroxide ion attack glass. In this case, the silicates in glass are acting as acids. Basic hydroxides, whether solids or in solution, are stored in airtight plastic containers.

The hydroxide ion can function as a typical electron-pair donor ligand, forming such complexes as tetrahydroxoaluminate/tetrahydroxidoaluminate [Al(OH)]. It is also often found in mixed-ligand complexes of the type [ML(OH)], where L is a ligand. The hydroxide ion often serves as a bridging ligand, donating one pair of electrons to each of the atoms being bridged. As illustrated by [Pb(OH)], metal hydroxides are often written in a simplified format. It can even act as a 3-electron-pair donor, as in the tetramer [PtMe(OH)].

When bound to a strongly electron-withdrawing metal centre, hydroxide ligands tend to ionise into oxide ligands. For example, the bichromate ion [HCrO] dissociates according to
with a p"K" of about 5.9.

The infrared spectra of compounds containing the OH functional group have strong absorption bands in the region centered around 3500 cm. The high frequency of molecular vibration is a consequence of the small mass of the hydrogen atom as compared to the mass of the oxygen atom, and this makes detection of hydroxyl groups by infrared spectroscopy relatively easy. A band due to an OH group tends to be sharp. However, the band width increases when the OH group is involved in hydrogen bonding. A water molecule has an HOH bending mode at about 1600 cm, so the absence of this band can be used to distinguish an OH group from a water molecule.

When the OH group is bound to a metal ion in a coordination complex, an M−OH bending mode can be observed. For example, in [Sn(OH)] it occurs at 1065 cm. The bending mode for a bridging hydroxide tends to be at a lower frequency as in [(bipyridine)Cu(OH)Cu(bipyridine)] (955 cm). M−OH stretching vibrations occur below about 600 cm. For example, the tetrahedral ion [Zn(OH)] has bands at 470 cm (Raman-active, polarized) and 420 cm (infrared). The same ion has a (HO)–Zn–(OH) bending vibration at 300 cm.

Sodium hydroxide solutions, also known as lye and caustic soda, are used in the manufacture of pulp and paper, textiles, drinking water, soaps and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes. The principal method of manufacture is the chloralkali process.

Solutions containing the hydroxide ion are generated when a salt of a weak acid is dissolved in water. Sodium carbonate is used as an alkali, for example, by virtue of the hydrolysis reaction
Although the base strength of sodium carbonate solutions is lower than a concentrated sodium hydroxide solution, it has the advantage of being a solid. It is also manufactured on a vast scale (42 million tonnes in 2005) by the Solvay process. An example of the use of sodium carbonate as an alkali is when washing soda (another name for sodium carbonate) acts on insoluble esters, such as triglycerides, commonly known as fats, to hydrolyze them and make them soluble.

Bauxite, a basic hydroxide of aluminium, is the principal ore from which the metal is manufactured. Similarly, goethite (α-FeO(OH)) and lepidocrocite (γ-FeO(OH)), basic hydroxides of iron, are among the principal ores used for the manufacture of metallic iron. Numerous other uses can be found in the articles on individual hydroxides.

Aside from NaOH and KOH, which enjoy very large scale applications, the hydroxides of the other alkali metals also are useful. Lithium hydroxide is a strong base, with a p"K" of −0.36. Lithium hydroxide is used in breathing gas purification systems for spacecraft, submarines, and rebreathers to remove carbon dioxide from exhaled gas.
The hydroxide of lithium is preferred to that of sodium because of its lower mass. Sodium hydroxide, potassium hydroxide, and the hydroxides of the other alkali metals are also strong bases.

Beryllium hydroxide Be(OH) is amphoteric. The hydroxide itself is insoluble in water, with a solubility product log "K"* of −11.7. Addition of acid gives soluble hydrolysis products, including the trimeric ion [Be(OH)(HO)], which has OH groups bridging between pairs of beryllium ions making a 6-membered ring. At very low pH the aqua ion [Be(HO)] is formed. Addition of hydroxide to Be(OH) gives the soluble tetrahydroxoberyllate/tetrahydroxidoberyllate anion, [Be(OH)].

The solubility in water of the other hydroxides in this group increases with increasing atomic number. Magnesium hydroxide Mg(OH) is a strong base (up to the limit of its solubility, which is very low in pure water), as are the hydroxides of the heavier alkaline earths: calcium hydroxide, strontium hydroxide, and barium hydroxide. A solution or suspension of calcium hydroxide is known as limewater and can be used to test for the weak acid carbon dioxide. The reaction Ca(OH) + CO Ca + + OH illustrates the basicity of calcium hydroxide. Soda lime, which is a mixture of the strong bases NaOH and KOH with Ca(OH), is used as a CO absorbent.

The simplest hydroxide of boron B(OH), known as boric acid, is an acid. Unlike the hydroxides of the alkali and alkaline earth hydroxides, it does not dissociate in aqueous solution. Instead, it reacts with water molecules acting as a Lewis acid, releasing protons.
A variety of oxyanions of boron are known, which, in the protonated form, contain hydroxide groups.

Aluminium hydroxide Al(OH) is amphoteric and dissolves in alkaline solution.
In the Bayer process for the production of pure aluminium oxide from bauxite minerals this equilibrium is manipulated by careful control of temperature and alkali concentration. In the first phase, aluminium dissolves in hot alkaline solution as , but other hydroxides usually present in the mineral, such as iron hydroxides, do not dissolve because they are not amphoteric. After removal of the insolubles, the so-called red mud, pure aluminium hydroxide is made to precipitate by reducing the temperature and adding water to the extract, which, by diluting the alkali, lowers the pH of the solution. Basic aluminium hydroxide AlO(OH), which may be present in bauxite, is also amphoteric.

In mildly acidic solutions, the hydroxo/hydroxido complexes formed by aluminium are somewhat different from those of boron, reflecting the greater size of Al(III) vs. B(III). The concentration of the species [Al(OH)] is very dependent on the total aluminium concentration. Various other hydroxo complexes are found in crystalline compounds. Perhaps the most important is the basic hydroxide AlO(OH), a polymeric material known by the names of the mineral forms boehmite or diaspore, depending on crystal structure. Gallium hydroxide, indium hydroxide, and thallium(III) hydroxide are also amphoteric. Thallium(I) hydroxide is a strong base.

Carbon forms no simple hydroxides. The hypothetical compound C(OH) (orthocarbonic acid or methanetetrol) is unstable in aqueous solution:

Carbon dioxide is also known as carbonic anhydride, meaning that it forms by dehydration of carbonic acid HCO (OC(OH)).

Silicic acid is the name given to a variety of compounds with a generic formula [SiO(OH)]. "Orthosilicic acid" has been identified in very dilute aqueous solution. It is a weak acid with p"K" = 9.84, p"K" = 13.2 at 25 °C. It is usually written as HSiO, but the formula Si(OH) is generally accepted. Other silicic acids such as "metasilicic acid" (HSiO), "disilicic acid" (HSiO), and "pyrosilicic acid" (HSiO) have been characterized. These acids also have hydroxide groups attached to the silicon; the formulas suggest that these acids are protonated forms of polyoxyanions.

Few hydroxo complexes of germanium have been characterized. Tin(II) hydroxide Sn(OH) was prepared in anhydrous media. When tin(II) oxide is treated with alkali the pyramidal hydroxo complex is formed. When solutions containing this ion are acidified, the ion [Sn(OH)] is formed together with some basic hydroxo complexes. The structure of [Sn(OH)] has a triangle of tin atoms connected by bridging hydroxide groups. Tin(IV) hydroxide is unknown but can be regarded as the hypothetical acid from which stannates, with a formula [Sn(OH)], are derived by reaction with the (Lewis) basic hydroxide ion.

Hydrolysis of Pb in aqueous solution is accompanied by the formation of various hydroxo-containing complexes, some of which are insoluble. The basic hydroxo complex [PbO(OH)] is a cluster of six lead centres with metal–metal bonds surrounding a central oxide ion. The six hydroxide groups lie on the faces of the two external Pb tetrahedra. In strongly alkaline solutions soluble plumbate ions are formed, including [Pb(OH)].

In the higher oxidation states of the pnictogens, chalcogens, halogens, and noble gases there are oxoacids in which the central atom is attached to oxide ions and hydroxide ions. Examples include phosphoric acid HPO, and sulfuric acid HSO. In these compounds one or more hydroxide groups can dissociate with the liberation of hydrogen cations as in a standard Brønsted–Lowry acid. Many oxoacids of sulfur are known and all feature OH groups that can dissociate.

Telluric acid is often written with the formula HTeO·2HO but is better described structurally as Te(OH).

"Ortho"-periodic acid can lose all its protons, eventually forming the periodate ion [IO]. It can also be protonated in strongly acidic conditions to give the octahedral ion [I(OH)], completing the isoelectronic series, [E(OH)], E = Sn, Sb, Te, I; "z" = −2, −1, 0, +1. Other acids of iodine(VII) that contain hydroxide groups are known, in particular in salts such as the "meso"periodate ion that occurs in K[IO(OH)]·8HO.

As is common outside of the alkali metals, hydroxides of the elements in lower oxidation states are complicated. For example, phosphorous acid HPO predominantly has the structure OP(H)(OH), in equilibrium with a small amount of P(OH).

The oxoacids of chlorine, bromine, and iodine have the formula OA(OH), where "n" is the oxidation number: +1, +3, +5, or +7, and A = Cl, Br, or I. The only oxoacid of fluorine is F(OH), hypofluorous acid. When these acids are neutralized the hydrogen atom is removed from the hydroxide group.

The hydroxides of the transition metals and post-transition metals usually have the metal in the +2 (M = Mn, Fe, Co, Ni, Cu, Zn) or +3 (M = Fe, Ru, Rh, Ir) oxidation state. None are soluble in water, and many are poorly defined. One complicating feature of the hydroxides is their tendency to undergo further condensation to the oxides, a process called olation. Hydroxides of metals in the +1 oxidation state are also poorly defined or unstable. For example, silver hydroxide Ag(OH) decomposes spontaneously to the oxide (AgO). Copper(I) and gold(I) hydroxides are also unstable, although stable adducts of CuOH and AuOH are known. The polymeric compounds M(OH) and M(OH) are in general prepared by increasing the pH of an aqueous solutions of the corresponding metal cations until the hydroxide precipitates out of solution. On the converse, the hydroxides dissolve in acidic solution. Zinc hydroxide Zn(OH) is amphoteric, forming the tetrahydroxidozincate ion in strongly alkaline solution.

Numerous mixed ligand complexes of these metals with the hydroxide ion exist. In fact these are in general better defined than the simpler derivatives. Many can be made by deprotonation of the corresponding metal aquo complex.

Vanadic acid HVO shows similarities with phosphoric acid HPO though it has a much more complex vanadate oxoanion chemistry. Chromic acid HCrO, has similarities with sulfuric acid HSO; for example, both form acid salts A[HMO]. Some metals, e.g. V, Cr, Nb, Ta, Mo, W, tend to exist in high oxidation states. Rather than forming hydroxides in aqueous solution, they convert to oxo clusters by the process of olation, forming polyoxometalates.

In some cases the products of partial hydrolysis of metal ion, described above, can be found in crystalline compounds. A striking example is found with zirconium(IV). Because of the high oxidation state, salts of Zr are extensively hydrolyzed in water even at low pH. The compound originally formulated as ZrOCl·8HO was found to be the chloride salt of a tetrameric cation [Zr(OH)(HO)] in which there is a square of Zr ions with two hydroxide groups bridging between Zr atoms on each side of the square and with four water molecules attached to each Zr atom.

The mineral malachite is a typical example of a basic carbonate. The formula, CuCO(OH) shows that it is halfway between copper carbonate and copper hydroxide. Indeed, in the past the formula was written as CuCO·Cu(OH). The crystal structure is made up of copper, carbonate and hydroxide ions. The mineral atacamite is an example of a basic chloride. It has the formula, CuCl(OH). In this case the composition is nearer to that of the hydroxide than that of the chloride CuCl·3Cu(OH). Copper forms hydroxyphosphate (libethenite), arsenate (olivenite), sulfate (brochantite), and nitrate compounds. White lead is a basic lead carbonate, (PbCO)·Pb(OH), which has been used as a white pigment because of its opaque quality, though its use is now restricted because it can be a source for lead poisoning.

The hydroxide ion appears to rotate freely in crystals of the heavier alkali metal hydroxides at higher temperatures so as to present itself as a spherical ion, with an effective ionic radius of about 153 pm. Thus, the high-temperature forms of KOH and NaOH have the sodium chloride structure, which gradually freezes in a monoclinically distorted sodium chloride structure at temperatures below about 300 °C. The OH groups still rotate even at room temperature around their symmetry axes and, therefore, cannot be detected by X-ray diffraction. The room-temperature form of NaOH has the thallium iodide structure. LiOH, however, has a layered structure, made up of tetrahedral Li(OH) and (OH)Li units. This is consistent with the weakly basic character of LiOH in solution, indicating that the Li–OH bond has much covalent character.

The hydroxide ion displays cylindrical symmetry in hydroxides of divalent metals Ca, Cd, Mn, Fe, and Co. For example, magnesium hydroxide Mg(OH) (brucite) crystallizes with the cadmium iodide layer structure, with a kind of close-packing of magnesium and hydroxide ions.

The amphoteric hydroxide Al(OH) has four major crystalline forms: gibbsite (most stable), bayerite, nordstrandite, and doyleite.
All these polymorphs are built up of double layers of hydroxide ions – the aluminium atoms on two-thirds of the octahedral holes between the two layers – and differ only in the stacking sequence of the layers. The structures are similar to the brucite structure. However, whereas the brucite structure can be described as a close-packed structure in gibbsite the OH groups on the underside of one layer rest on the groups of the layer below. This arrangement led to the suggestion that there are directional bonds between OH groups in adjacent layers. This is an unusual form of hydrogen bonding since the two hydroxide ion involved would be expected to point away from each other. The hydrogen atoms have been located by neutron diffraction experiments on α-AlO(OH) (diaspore). The O–H–O distance is very short, at 265 pm; the hydrogen is not equidistant between the oxygen atoms and the short OH bond makes an angle of 12° with the O–O line. A similar type of hydrogen bond has been proposed for other amphoteric hydroxides, including Be(OH), Zn(OH), and Fe(OH).

A number of mixed hydroxides are known with stoichiometry AM(OH), AM(OH), and AM(OH). As the formula suggests these substances contain M(OH) octahedral structural units. Layered double hydroxides may be represented by the formula . Most commonly, "z" = 2, and M = Ca, Mg, Mn, Fe, Co, Ni, Cu, or Zn; hence "q" = "x".

Potassium hydroxide and sodium hydroxide are two well-known reagents in organic chemistry.

The hydroxide ion may act as a base catalyst. The base abstracts a proton from a weak acid to give an intermediate that goes on to react with another reagent. Common substrates for proton abstraction are alcohols, phenols, amines, and carbon acids. The p"K" value for dissociation of a C–H bond is extremely high, but the pK alpha hydrogens of a carbonyl compound are about 3 log units lower. Typical p"K" values are 16.7 for acetaldehyde and 19 for acetone. Dissociation can occur in the presence of a suitable base.
The base should have a p"K" value not less than about 4 log units smaller, or the equilibrium will lie almost completely to the left.

The hydroxide ion by itself is not a strong enough base, but it can be converted in one by adding sodium hydroxide to ethanol
to produce the ethoxide ion. The pK for self-dissociation of ethanol is about 16, so the alkoxide ion is a strong enough base. The addition of an alcohol to an aldehyde to form a hemiacetal is an example of a reaction that can be catalyzed by the presence of hydroxide. Hydroxide can also act as a Lewis-base catalyst.

The hydroxide ion is intermediate in nucleophilicity between the fluoride ion F, and the amide ion . The hydrolysis of an ester
also known as saponification is an example of a nucleophilic acyl substitution with the hydroxide ion acting as a nucleophile. In this case the leaving group is an alkoxide ion, which immediately removes a proton from a water molecule to form an alcohol. In the manufacture of soap, sodium chloride is added to salt out the sodium salt of the carboxylic acid; this is an example of the application of the common ion effect.

Other cases where hydroxide can act as a nucleophilic reagent are amide hydrolysis, the Cannizzaro reaction, nucleophilic aliphatic substitution, nucleophilic aromatic substitution, and in elimination reactions. The reaction medium for KOH and NaOH is usually water but with a phase-transfer catalyst the hydroxide anion can be shuttled into an organic solvent as well, for example in the generation of the reactive intermediate dichlorocarbene.



</doc>
<doc id="13713" url="https://en.wikipedia.org/wiki?curid=13713" title="H. R. Giger">
H. R. Giger

Hans Ruedi Giger ( ; ; 5 February 1940 – 12 May 2014) was a Swiss painter, best known for airbrush images of humans and machines linked together in a cold biomechanical relationship. Later he abandoned airbrush work for pastels, markers, and ink. He was part of the special effects team that won an Academy Award for design work on the film "Alien". In Switzerland there are two themed bars that reflect his interior designs, and his work is on permanent display at the H.R. Giger Museum at Gruyères. His style has been adapted to many forms of media, including record album covers, furniture, and tattoos.

Giger was born in 1940 in Chur, the capital city of Graubünden, the largest and easternmost Swiss canton. His father, a pharmacist, viewed art as a "breadless profession" and strongly encouraged him to enter pharmacy. He moved to Zürich in 1962, where he studied architecture and industrial design at the School of Applied Arts until 1970.

Giger's first success was when H. H. Kunz, co-owner of Switzerland's first poster publishing company, printed and distributed Giger's first posters, beginning in 1969.

Giger's style and thematic execution were influential. He was part of the special effects team that won an Academy Award for Best Achievement in Visual Effects for their design work on the film "Alien". His design for the Alien was inspired by his painting "Necronom IV" and earned him an Oscar in 1980. His books of paintings, particularly "Necronomicon" and "Necronomicon II" (1985) and the frequent appearance of his art in "Omni" magazine continued his rise to international prominence. Giger was admitted to the Science Fiction and Fantasy Hall of Fame in 2013. He is also well known for artwork on several music recording albums including "" by Danzig, "Brain Salad Surgery" by Emerson, Lake & Palmer, "Heartwork" by "Carcass", "To Mega Therion" by "Celtic Frost", "Eparistera Daimones" by Triptykon, and Deborah Harry's "KooKoo".

In 1998, Giger acquired the Saint-Germain Castle in Gruyères, Switzerland, and it now houses the H.R. Giger Museum, a permanent repository of his work.

Giger had a relationship with Swiss actress Li Tobler until she committed suicide in 1975. Li's image appears in many of his paintings. He married Mia Bonzanigo in 1979; they divorced a year and a half later.

The artist lived and worked in Zürich with his second wife, Carmen Maria Scheifele Giger, who is the Director of the H.R. Giger Museum.

On 12 May 2014, Giger died in a hospital in Zürich after having suffered injuries in a fall.

In addition to his awards, Giger was recognized by a variety of festivals and institutions. On the one year anniversary of his death, the Museum of Arts and Design in New York City staged the series "The Unseen Cinema of HR Giger" in May 2015.

"", a biographical documentary by Belinda Sallin, debuted 27 September 2014 in Zurich, Switzerland.

On 11 July 2018, the asteroid 109712 Giger was named in his memory.

Giger started with small ink drawings before progressing to oil paintings. For most of his career, Giger had worked predominantly in airbrush, creating monochromatic canvasses depicting surreal, nightmarish dreamscapes. He also worked with pastels, markers and ink.

Giger's most distinctive stylistic innovation was that of a representation of human bodies and machines in a cold, interconnected relationship, he described as "biomechanical". His main influences were painters Dado, Ernst Fuchs and Salvador Dalí. He met Salvador Dalí, to whom he was introduced by painter Robert Venosa. Giger was also influenced by the work of the Polish sculptor Stanislaw Szukalski, and by the painters Austin Osman Spare and Mati Klarwein. He was also a personal friend of Timothy Leary. Giger studied interior and industrial design at the School of Commercial Art in Zurich (from 1962 to 1965) and made his first paintings as a means of art therapy.

Giger directed a number of films, including "Swiss Made" (1968), "Tagtraum" (1973), "Giger's Necronomicon" (1975) and "Giger's Alien" (1979).

Giger created furniture designs, particularly the Harkonnen Capo Chair for a film of the novel "Dune" that was to be directed by Alejandro Jodorowsky. Many years later, David Lynch directed the film, using only rough concepts by Giger. Giger had wished to work with Lynch, as he stated in one of his books that Lynch's film "Eraserhead" was closer than even Giger's own films to realizing his vision.

Giger applied his biomechanical style to interior design. One "Giger Bar" appeared in Tokyo, but the realization of his designs was a great disappointment to him, since the Japanese organization behind the venture did not wait for his final designs, and instead used Giger's rough preliminary sketches. For that reason Giger disowned the Tokyo bar. The two Giger Bars in his native Switzerland, in Gruyères and Chur, were built under Giger's close supervision and they accurately reflect his original concepts. At The Limelight in Manhattan, Giger's artwork was licensed to decorate the VIP room, the uppermost chapel of the landmarked church, but it was never intended to be a permanent installation and bore no similarity to the bars in Switzerland. The arrangement was terminated after two years when the Limelight closed. 

Giger's art has greatly influenced tattooists and fetishists worldwide. Under a licensing deal Ibanez guitars released an H. R. Giger signature series: the Ibanez ICHRG2, an Ibanez Iceman, features "NY City VI", the Ibanez RGTHRG1 has "NY City XI" printed on it, the S Series SHRG1Z has a metal-coated engraving of "Biomechanical Matrix" on it, and a 4-string SRX bass, SRXHRG1, has "N.Y. City X" on it.

Giger is often referred to in popular culture, especially in science fiction and cyberpunk. William Gibson (who wrote an early script for "Alien 3") seems particularly fascinated: A minor character in "Virtual Light", Lowell, is described as having "New York XXIV" tattooed across his back, and in "Idoru" a secondary character, Yamazaki, describes the buildings of nanotech Japan as Giger-esque.







</doc>
<doc id="13714" url="https://en.wikipedia.org/wiki?curid=13714" title="Hispaniola">
Hispaniola

Hispaniola (, ; ; Latin and ; ; ) is an island in the Caribbean archipelago known as the Greater Antilles. It is the most populous island in the West Indies and the region's second largest after Cuba.

The island is divided into two separate, sovereign nations: the Spanish-speaking Dominican Republic (48,445 km, 18,705 sq mi) to the east and French / Haitian Creole-speaking Haiti (27,750 km, 10,710 sq mi) to the west. The only other shared island in the Caribbean is Saint Martin, which is shared between France (Saint Martin) and the Netherlands (Sint Maarten).

Hispaniola is the site of the first permanent European settlement in the Americas, La Navidad founded by Christopher Columbus on his voyages in 1492 and 1493 of what is now a part of the northeastern coast of Haiti, as well as the first permanent city Santo Domingo, the modern-day capital of the Dominican Republic.

The island was called by various names by its native people, the Taíno Amerindians. No known Taíno texts exist, hence, historical evidence for those names comes through three European historians: the Italian Pietro Martyr d‘Anghiera, and the Spaniards Bartolomé de las Casas and Gonzalo Fernández de Oviedo. Fernández de Oviedo and de las Casas both recorded that the island was called "Haití" ("Mountainous Land") by the Taíno. D'Anghiera added another name, "Quizqueia" (supposedly "Mother of all Lands"), but later research shows that the word does not seem to derive from the original Arawak Taíno language. (Quisqueya is today mostly used in the Dominican Republic.) Although the Taínos' use of Haití is verified, and the name was used by all three historians, evidence suggests that it referred only to the northeast region now known as Los Haitises in the Dominican Republic, rather than the whole island.

When Columbus took possession of the island in 1492, he named it "Insula Hispana" in Latin and "La Isla Española" in Spanish, with both meaning "the Spanish island". De las Casas shortened the name to "Española", and when d‘Anghiera detailed his account of the island in Latin, he rendered its name as "Hispaniola". In the oldest documented map of the island, created by Andrés de Morales, Los Haitises is labeled "Montes de Haití" ("Haiti Mountains"), and de las Casas apparently named the whole island "Haiti" on the basis of that particular region, as d'Anghiera states that the name of one part was given to the whole island.

Due to Taíno, Spanish and French influences on the island, historically the whole island was often referred to as "Haiti", "Hayti", "Santo Domingo", "St. Domingue", or "San Domingo". The colonial terms Saint-Domingue and Santo Domingo are sometimes still applied to the whole island, though these names refer, respectively, to the colonies that became Haiti and the Dominican Republic. Since Anghiera's literary work was translated into English and French soon after being written, the name Hispaniola became the most frequently used term in English-speaking countries for the island in scientific and cartographic works. In 1918, the United States occupation government, led by Harry Shepard Knapp, obliged the use of the name Hispaniola on the island, and recommended the use of that name to the National Geographic Society.

The name Haïti was adopted by Haitian revolutionary Jean-Jacques Dessalines in 1804, as the official name of independent Saint-Domingue, as a tribute to the Amerindian predecessors. It was also adopted as the official name of independent Santo Domingo, as the Republic of Spanish Haiti, a state that existed from November 1821 until its annexation by Haiti in February 1822.

The primary indigenous group on the island of Hispaniola was the Arawak/Taíno people. The Arawak tribe originated in the Orinoco Delta, spreading from Venezuela. They traveled to Hispaniola around 1200 CE. Each society on the island was a small independent kingdom with a lead known as a cacique. In 1492, which is considered the peak of the Taíno, there were five different kingdoms on the island, the Xaragua, Higuey (Caizcimu), Magua (Huhabo), Ciguayos (Cayabo or Maguana), and Marien (Bainoa). Many distinct Taíno languages also existed in this time period. There is still heated debate over the population of Taíno people on the island of Hispaniola in 1492, but estimates range upwards of 750,000.

An Arawak/Taíno home consisted of a circular building with woven straw and palm leaves as covering. Most individuals slept in fashioned hammocks, but grass beds were also used. The cacique lived in a different structure with larger rectangular walls and a porch. The Taíno village also had a flat court used for ball games and festivals. Religiously, the Arawak/Taíno people were polytheists, and their gods were called Zemí. Religious worship and dancing were common, and medicine men or priests also consulted the Zemí for advice in public ceremonies. 

For food, the Arawak/Taíno relied on meat and fish as a primary source for protein; some small mammals on the island were hunted such as rats, but ducks, turtles, snakes and bats as a common food source. The Taíno also relied on agriculture as a primary food source. The indigenous people of Hispaniola raised crops in a conuco, which is a large mound packed with leaves and fixed crops to prevent erosion. Some common agricultural goods were cassava, maize, squash, beans, peppers, peanuts, cotton, and tobacco, which was used as an aspect of social life and religious ceremonies.

The Arawak/Taíno people traveled often and used hollowed canoes with paddles when on the water for fishing or for migration purposes, and upwards of 100 people could fit into a single canoe. The Taíno came in contact with the Caribs, another indigenous tribe, often. The Caribs lived mostly in modern day Puerto Rico and northeast Hispaniola and were known to be hostile towards other tribes. The Arawak/Taíno people had to defend themselves using bow and arrows with poisoned tips and some war clubs. When Columbus landed on Hispaniola, many Taíno leaders wanted protection from the Caribs.

Christopher Columbus first landed at Hispaniola on December 6, 1492 at a small bay he named San Nicolas, now called Môle-Saint-Nicolas on the north coast of present-day Haiti. He was welcomed in a friendly fashion by the indigenous people known as the Taino. Trading with the natives yielded more gold than they had come across previously on the other Caribbean islands and Columbus was led to believe that much more gold would be found inland. Before he could explore further, his flagship, the "Santa Maria", ran aground and sank in the bay on December 24. With only two smaller ships remaining for the voyage home, Columbus built a fortified encampment, La Navidad, on the shore and left behind 21 crewman to await his return the following year.

Colonization began in earnest the following year when Columbus brought 1,300 men to Hispaniola in November 1493 with the intention of establishing a permanent settlement. They found the encampment at Navidad had been destroyed and all the crewmen left behind killed by the natives. Columbus decided to sail east in search of a better site to found a new settlement. In January 1494 they established La Isabela in present-day Dominican Republic.

In 1496, the town of Nueva Isabela was founded. After being destroyed by a hurricane, it was rebuilt on the opposite side of the Ozama River and called Santo Domingo. It is the oldest permanent European settlement in the Americas.
Harsh enslavement practiced by Spanish colonists against the Taínos, as well as redirection of food supplies and labor of the indigenous for feeding Spanish settlers, had a devastating impact on both mortality and fertility of the Taíno population over the first quarter century. Colonial administrators and Dominican and Hyeronimite priests observed that the search for gold and agrarian enslavement through the encomienda system were depressing population. Demographic data from two provinces in 1514 shows a low birth rate, consistent with a 3.5% annual population decline. In 1503 the colony began to import African slaves after a charter was passed in 1501 allowing the import of slaves by Ferdinand and Isabel. The Spanish believed Africans would be more capable of performing physical labor. From 1519 to 1533, the indigenous uprising known as Enriquillo's Revolt, after the Taíno cacique who led them, ensued, resulting from escaped African slaves on the island (maroons) possibly working with the Taíno people.

Precious metals played a large role in the history of the island after Columbus's arrival. One of the first inhabitants Columbus came across on this island was "a girl wearing only a gold nose plug". Soon the Taínos were trading pieces of gold for hawk's bells with their cacique declaring the gold came from Cibao. Traveling further east from Navidad, Columbus came across the Yaque del Norte River, which he named Río de Oro (River of Gold) because its "sands abound in gold dust".

On Columbus's return during his second voyage, he learned it was the cacique Caonabo who had massacred his settlement at Navidad. While Columbus established a new settlement the village of La Isabela on Jan. 1494, he sent Alonso de Ojeda and 15 men to search for the mines of Cibao. After a six-day journey, Ojeda came across an area containing gold, in which the gold was extracted from streams by the Taíno people. Columbus himself visited the mines of Cibao on 12 March 1494. He constructed the Fort of Santo Tomás, present day Jánico, leaving Captain Pedro Margarit in command of 56 men. On 24 March 1495, Columbus, with his ally Guacanagarix, embarked on a war of revenge against Caonabo, capturing him and his family while killing and capturing many natives. Afterwards, every person over the age of fourteen had to produce a hawksbill of gold.

Miguel Díaz and Francisco de Garay discovered large gold nuggets on the lower Haina River in 1496. These San Cristobal mines were later known as the Minas Viejas mines. Then, in 1499, the first major discovery of gold was made in the cordillera central, which led to a mining boom. By 1501 Columbus's cousin, Giovanni Colombo, had discovered gold near Buenaventura. The deposits were later known as Minas Nuevas. Two major mining areas resulted, one along San Cristobal-Buenaventura, and another in Cibao within the La Vega-Cotuy-Bonao triangle, while Santiago de los Caballeros, Concepción, and Bonao became mining towns. The gold rush of 1500–1508 ensued, and Ovando expropriated the gold mines of Miguel Díaz and Francisco de Garay in 1504, as pit mines became royal mines for Ferdinand, who reserved the best mines for himself, though placers were open to private prospectors. Furthermore, Ferdinand kept 967 natives in the San Cristobal mining area, supervised by salaried miners.

Under Nicolás de Ovando y Cáceres' governorship, the Indians were made to work in the gold mines. By 1503, the Spanish Crown legalized the distribution of Indians to work the mines through the encomienda system. Once the Indians entered the mines, they were often wiped out by hunger and difficult conditions. By 1508, the Taíno population of about 400,000 was reduced to 60,000, and by 1514, only 26,334 remained. About half resided in the mining towns of Concepción, Santiago, Santo Domingo, and Buenaventura. The repartimiento of 1514 accelerated emigration of the Spanish colonists, coupled with the exhaustion of the mines. The first documented outbreak of smallpox, previously an Eastern hemisphere disease, occurred on Hispaniola in December 1518 among enslaved African miners. Some scholars speculate that European diseases arrived before this date, but there is no compelling evidence for an outbreak. The natives had no immunity to European diseases, including smallpox. By May 1519, as many as one-third of the remaining Taínos had died.

Christopher Columbus brought sugar cane to the island in 1493, on his second voyage. The first sugar mill in the Caribbean was established in Hispaniola in 1516. Molasses was the chief product. Diego Colón's plantation had 40 African slaves in 1522. By 1526, 19 mills were in operation from Azua to Santo Domingo. In 1574, a census taken of the Greater Antilles reported 1,000 Spaniards and 12,000 African slaves on Hispaniola.

As Spain conquered new regions on the mainland of the Americas (Spanish Main), its interest in Hispaniola waned, and the colony’s population grew slowly. By the early 17th century, the island and its smaller neighbors (notably Tortuga) became regular stopping points for Caribbean pirates. In 1606, the government of Philip III ordered all inhabitants of Hispaniola to move close to Santo Domingo, to avoid interaction with pirates. Rather than secure the island, his action meant that French, English, and Dutch pirates established their own bases on the abandoned north and west coasts of the island.
In 1665, French colonization of the island was officially recognized by King Louis XIV. The French colony was given the name Saint-Domingue. In the 1697 Treaty of Ryswick, Spain formally ceded the western third of the island to France. Saint-Domingue quickly came to overshadow the east in both wealth and population. Nicknamed the "Pearl of the Antilles", it became the richest and most prosperous colony in the West Indies, with a system of human slavery used to grow and harvest sugar cane during a time when European demand for sugar was high. Slavery kept prices low and profit was maximized. It was an important port in the Americas for goods and products flowing to and from France and Europe.

European colonists often died young due to tropical fevers, as well as from violent slave resistance in the late eighteenth century. In 1791, during the French Revolution, a major slave revolt broke out on Saint-Domingue. When the French Republic abolished slavery in the colonies on February 4, 1794, it was a European first. The ex-slave army joined forces with France in its war against its European neighbors. In the second 1795 Treaty of Basel (July 22), Spain ceded the eastern two-thirds of the island of Hispaniola, later to become the Dominican Republic. French settlers had begun to colonize some areas in the Spanish side of the territory.

Under Napoleon, France reimposed slavery in most of its Caribbean islands in 1802 and sent an army to bring Saint-Domingue under tighter control. However, thousands of the French troops succumbed to yellow fever during the summer months, and more than half of the French army died because of disease. After the French removed the surviving 7,000 troops in late 1803, the leaders of the revolution declared western Hispaniola the new nation of independent Haiti in early 1804. France continued to rule Spanish Santo Domingo. In 1805, Haitian troops of General Henri Christophe tried to conquer all of Hispaniola. They invaded Santo Domingo and sacked the towns of Santiago de los Caballeros and Moca, killing most of their residents, but news of a French fleet sailing towards Haiti forced General Christophe to withdraw from the east, leaving it in French hands. 

In 1808, following Napoleon's invasion of Spain, the criollos of Santo Domingo revolted against French rule and, with the aid of the United Kingdom, returned Santo Domingo to Spanish control. Fearing the influence of a society that had successfully revolted against their owners, the United States and European powers refused to recognize Haiti, the second republic in the Western Hemisphere. France demanded a high payment for compensation to slaveholders who lost their property, and Haiti was saddled with unmanageable debt for decades. It became one of the poorest countries in the Americas, while the Dominican Republic gradually has developed into the one of the largest economies of Central America and the Caribbean.

Hispaniola is the second-largest island in the Caribbean (after Cuba), with an area of , of which is under the sovereignty of the Dominican Republic occupying the eastern portion and under the sovereignty of Haiti occupying the western portion.

The island of Cuba lies to the northwest across the Windward Passage; 190 km to the southwest lies Jamaica, separated by the Jamaica Channel. Puerto Rico lies 130 km east of Hispaniola across the Mona Passage. The Bahamas and Turks and Caicos Islands lie to the north. Its westernmost point is known as Cap Carcasse. Cuba, Hispaniola, Jamaica, and Puerto Rico are collectively known as the Greater Antilles.

The island has five major ranges of mountains: The Central Range, known in the Dominican Republic as the Cordillera Central, spans the central part of the island, extending from the south coast of the Dominican Republic into northwestern Haiti, where it is known as the Massif du Nord. This mountain range boasts the highest peak in the Antilles, Pico Duarte at above sea level. The Cordillera Septentrional runs parallel to the Central Range across the northern end of the Dominican Republic, extending into the Atlantic Ocean as the Samaná Peninsula. The Cordillera Central and Cordillera Septentrional are separated by the lowlands of the Cibao Valley and the Atlantic coastal plains, which extend westward into Haiti as the Plaine du Nord (Northern Plain). The lowest of the ranges is the Cordillera Oriental, in the eastern part of the country.

The Sierra de Neiba rises in the southwest of the Dominican Republic, and continues northwest into Haiti, parallel to the Cordillera Central, as the Montagnes Noires, Chaîne des Matheux and the Montagnes du Trou d'Eau. The Plateau Central lies between the Massif du Nord and the Montagnes Noires, and the Plaine de l‘Artibonite lies between the Montagnes Noires and the Chaîne des Matheux, opening westward toward the Gulf of Gonâve, the largest gulf of the Antilles.

The southern range begins in the southwestern most Dominican Republic as the Sierra de Bahoruco, and extends west into Haiti as the Massif de la Selle and the Massif de la Hotte, which form the mountainous spine of Haiti’s southern peninsula. Pic de la Selle is the highest peak in the southern range, the third highest peak in the Antilles and consequently the highest point in Haiti, at above sea level. A depression runs parallel to the southern range, between the southern range and the Chaîne des Matheux-Sierra de Neiba. It is known as the Plaine du Cul-de-Sac in Haiti, and Haiti’s capital Port-au-Prince lies at its western end. The depression is home to a chain of salt lakes, including Lake Azuei in Haiti and Lake Enriquillo in the Dominican Republic.

The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.

There are many bird species in Hispaniola, and the island's amphibian species are also diverse. Numerous land species on the island are endangered and could become extinct. There are many species endemic to the island including insects and other invertebrates, reptiles, and mammals. The most famous endemic mammal on the island is the Hispaniola Hutia (Plagiodontia aedium). There are also many avian species on the island. The six endemic genera are Calyptophilus, Dulus, Nesoctites, Phaenicophilus, Xenoligea and Microligea. More than half of the original ecoregion has been lost to habitat destruction impacting the local fauna.

The island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic
In Haiti, deforestation has long been cited by scientists as a source of ecological crisis; the timber industry dates back to French colonial rule. Haiti has seen a dramatic reduction of forests due to the excessive and increasing use of charcoal as fuel for cooking. Various media outlets have suggested that the country has just 2% forest cover, but this has not been substantiated by research.

Recent in-depth studies of satellite imagery and environmental analysis regarding forest classification conclude that Haiti actually has approximately 30% tree cover; this is, nevertheless, a stark decrease from the country's 60% forest cover in 1925. The country has been significantly deforested over the last 50 years, resulting in the desertification of portions of the Haitian territory.

In the Dominican Republic, the forest cover has increased. In 2003, the Dominican forest cover had been reduced to 32% of the territory, but by 2011, forest cover had increased to nearly 40%. The success of the Dominican forest growth is due to several Dominican government policies and private organizations for the purpose, and a strong educational campaign that has resulted in increased awareness on the Dominican people of the importance of forests for their welfare and in other forms of life on the island.

Owing to its mountainous topography, Hispaniola’s climate shows considerable variation over short distances, and is the most varied of all the Antilles.

Except in the Northern Hemisphere summer season, the predominant winds over Hispaniola are the northeast trade winds. As in Jamaica and Cuba, these winds deposit their moisture on the northern mountains, and create a distinct rain shadow on the southern coast, where some areas receive as little as of rainfall, and have semi-arid climates. Annual rainfall under also occurs on the southern coast of Haiti’s northwest peninsula and in the central Azúa region of the Plaine du Cul-de-Sac. In these regions, moreover, there is generally little rainfall outside hurricane season from August to October, and droughts are by no means uncommon when hurricanes do not come.
On the northern coast, in contrast, rainfall may peak between December and February, though some rain falls in all months of the year. Annual amounts typically range from on the northern coastal lowlands; there is probably much more in the Cordillera Septentrional, though no data exist.

The interior of Hispaniola, along with the southeastern coast centered around Santo Domingo, typically receives around per year, with a distinct wet season from May to October. Usually, this wet season has two peaks: one around May, the other around the hurricane season. In the interior highlands, rainfall is much greater, around per year, but with a similar pattern to that observed in the central lowlands.

As is usual for tropical islands, variations of temperature are much less marked than rainfall variations, and depend only on altitude. Lowland Hispaniola is generally oppressively hot and humid, with temperatures averaging . with high humidity during the daytime, and around at night. At higher altitudes, temperatures fall steadily, so that frosts occur during the dry season on the highest peaks, where maxima are no higher than .

"See also: Demographics of the Dominican Republic and Demographics of Haiti"

Hispaniola is the most populous Caribbean island with combined population of almost 22 million inhabitants as of April 2019.

The Dominican Republic is a Hispanophone nation of approximately 10 million people. Spanish is spoken by all Dominicans as a primary language. Roman Catholicism is the official and dominant religion.

Haiti is a Francophone nation of roughly 10 million people. Although French is spoken as a primary language by the educated and wealthy minority, virtually the entire population speaks Haitian Creole, one of several French-derived creole languages. Roman Catholicism is the dominant religion, practiced by more than half the population, although in some cases in combination with Haitian Vodou faith. Another 25% of the populace belong to Protestant churches. Haiti emerged as the first Black republic in the world.

The ethnic composition of the Dominican population is 73% mulatto, 16% white and 11% black. Descendants of early Spanish settlers and of black slaves from West Africa constitute the two main racial strains.

The ethnic composition of Haiti is estimated to be 95% black and 5% white and mulatto.

In recent times, Dominican and Puerto Rican researchers identified in the current Dominican population the presence of genes belonging to the aborigines of the Canary Islands (commonly called Guanches). These types of genes also have been detected in Puerto Rico.

The island has the largest economy in the Greater Antilles, however most of the economic development is found in the Dominican Republic, the Dominican economy being nearly 800% larger than the Haitian economy.

The estimated annual per capita income is US$1,300 in Haiti and US$8,200 in Dominican Republic.

The divergence between the level of economic development between Haiti and Dominican Republic makes its border the higher contrast of all western land borders and is evident that the Dominican Republic has one of the highest migration issues in the Americas.

The island also has an economic history and current day interest and involvement in precious metals. In 1860, it was observed that the island contained a large supply of gold, of which the early Spaniards had hardly developed. By 1919, Condit and Ross noted that much of the island was covered by government granted concessions for mining different types of minerals. Besides gold, these minerals included silver, manganese, copper, magnetite, iron and nickel.

Mining operations in 2016 have taken advantage of the volcanogenic massive sulfide ore deposits (VMS) around Maimón. To the northeast, the Pueblo Viejo Gold Mine was operated by state-owned Rosario Dominicana from 1975 until 1991. In 2009, Pueblo Viejo Dominicana Corporation, formed by Barrick Gold and Goldcorp, started open-pit mining operations of the Monte Negro and Moore oxide deposits. The mined ore is processed with gold cyanidation. Pyrite and sphalerite are the main sulfide minerals found in the 120 m thick volcanic conglomerates and agglomerates, which constitute the world's second largest sulphidation gold deposit.

Between Bonao and Maimón, Falconbridge Dominicana has been mining nickel laterites since 1971. The Cerro de Maimon copper/gold open-pit mine southeast of Maimón has been operated by Perilya since 2006. Copper is extracted from the sulfide ores, while gold and silver are extracted from both the sulfide and the oxide ores. Processing is via froth flotation and cyanidation. The ore is located in the VMS Early Cretaceous Maimón Formation. Goethite enriched with gold and silver is found in the 30 m thick oxide cap. Below that cap is a supergene zone containing pyrite, chalcopyrite, and sphalerite. Below the supergene zone is found the unaltered massive sulphide mineralization.




</doc>
<doc id="13717" url="https://en.wikipedia.org/wiki?curid=13717" title="Halle Berry">
Halle Berry

Halle Maria Berry (born Maria Halle Berry; August 14, 1966) is an American actress. Berry won the Academy Award for Best Actress for her performance in the romantic drama film "Monster's Ball" (2001), becoming the only woman of African American descent to have won the award.

Before becoming an actress, Berry was a model and entered several beauty contests, finishing as the first runner-up in the Miss USA pageant and coming in sixth in the Miss World 1986. Her breakthrough film role was in the romantic comedy "Boomerang" (1992), alongside Eddie Murphy, which led to roles in films, such as the family comedy "The Flintstones" (1994), the political comedy-drama "Bulworth" (1998) and the television film "Introducing Dorothy Dandridge" (1999), for which she won a Primetime Emmy Award and a Golden Globe Award.

In addition to her Academy Award, Berry garnered high-profile roles in the 2000s, such as Storm in "X-Men" (2000), the thrillers "Swordfish" (2001) and "Gothika" (2003), and the spy film "Die Another Day" (2002), where she played Bond girl Jinx. She then appeared in the "X-Men" sequels, "X2" (2003) and "" (2006). In the 2010s, she has featured in the science-fiction film "Cloud Atlas" (2012), the crime thriller "The Call" (2013) and the action films "" (2014), "" (2017) and "" (2019).

Berry was one of the highest-paid actresses in Hollywood during the 2000s, and has been involved in the production of several of the films in which she performed. Berry is also a Revlon spokesmodel. She was formerly married to baseball player David Justice, singer-songwriter Eric Benét, and actor Olivier Martinez. She has a child each with Martinez and model Gabriel Aubry.

Berry was born Maria Halle Berry; her name was legally changed to Halle Maria Berry at age five. Her parents selected her middle name from Halle's Department Store, which was then a local landmark in her birthplace of Cleveland, Ohio. Her mother, Judith Ann (née Hawkins), is white and was born in Liverpool, England. Judith Ann worked as a psychiatric nurse. Her father, Jerome Jesse Berry, was an African-American hospital attendant in the psychiatric ward where her mother worked; he later became a bus driver. Berry's parents divorced when she was four years old; she and her older sister, Heidi Berry-Henderson, were raised exclusively by their mother.

Berry has said in published reports that she has been estranged from her father since her childhood, noting in 1992, "I haven't heard from him since [he left]. Maybe he's not alive." Her father was very abusive to her mother. Berry has recalled witnessing her mother being beaten daily, kicked down stairs and hit in the head with a wine bottle.

Berry grew up in Oakwood, Ohio and graduated from Bedford High School where she was a cheerleader, honor student, editor of the school newspaper and prom queen. She worked in the children's department at Higbee's Department store. She then studied at Cuyahoga Community College. In the 1980s, she entered several beauty contests, winning Miss Teen All American in 1985 and Miss Ohio USA in 1986. She was the 1986 Miss USA first runner-up to Christy Fichtner of Texas. In the Miss USA 1986 pageant interview competition, she said she hoped to become an entertainer or to have something to do with the media. Her interview was awarded the highest score by the judges. She was the first African-American Miss World entrant in 1986, where she finished sixth and Trinidad and Tobago's Giselle Laronde was crowned Miss World. According to the "Current Biography Yearbook", Berry "...pursued a modeling career in New York... Berry's first weeks in New York were less than auspicious: She slept in a homeless shelter and then in a YMCA".

In 1989, Berry moved to New York City to pursue her acting ambitions. During her early time there, she ran out of money and had to live briefly in a homeless shelter. Her situation improved by the end of that year, and she was cast in the role of model Emily Franklin in the short-lived ABC television series "Living Dolls", which was shot in New York and was a spin-off of the hit series "Who's the Boss?". During the taping of "Living Dolls", she lapsed into a coma and was diagnosed with Type 1 diabetes. After the cancellation of "Living Dolls", she moved to Los Angeles. She went on to have a recurring role on the long-running primetime serial "Knots Landing".

Berry's film debut was in a small role for Spike Lee's "Jungle Fever" (1991), in which she played Vivian, a drug addict. That same year, Berry had her first co-starring role in "Strictly Business". In 1992, Berry portrayed a career woman who falls for the lead character played by Eddie Murphy in the romantic comedy "Boomerang". The following year, she caught the public's attention as a headstrong biracial slave in the TV adaptation of "", based on the book by Alex Haley. Berry was in the live-action "Flintstones" movie playing the part of "Sharon Stone", a sultry secretary who seduced Fred Flintstone.

Berry tackled a more serious role, playing a former drug addict struggling to regain custody of her son in "Losing Isaiah" (1995), starring opposite Jessica Lange. She portrayed Sandra Beecher in "Race the Sun" (1996), which was based on a true story, shot in Australia, and co-starred alongside Kurt Russell in "Executive Decision". Beginning in 1996, she was a Revlon spokeswoman for seven years and renewed her contract in 2004.

She starred alongside Natalie Deselle Reid in the 1997 comedy film "B*A*P*S". In 1998, Berry received praise for her role in "Bulworth" as an intelligent woman raised by activists who gives a politician (Warren Beatty) a new lease on life. The same year, she played the singer Zola Taylor, one of the three wives of pop singer Frankie Lymon, in the biopic "Why Do Fools Fall in Love". In the 1999 HBO biopic "Introducing Dorothy Dandridge", she portrayed the first black woman to be nominated for the Academy Award for Best Actress, and it was to Berry a heart-felt project that she introduced, co-produced and fought intensely for it to come through. Berry's performance was recognized with several awards, including a Primetime Emmy Award and Golden Globe Award.

Berry portrayed the mutant superhero Storm in the film adaptation of the comic book series "X-Men" (2000) and its sequels, "X2" (2003), "" (2006) and "" (2014). In 2001, Berry appeared in the film "Swordfish", which featured her first topless scene. At first, she was opposed to a sunbathing scene in the film in which she would appear topless, but Berry eventually agreed. Some people attributed her change of heart to a substantial increase in the amount Warner Bros. offered her; she was reportedly paid an additional $500,000 for the short scene. Berry denied these stories, telling one interviewer that they amused her and "made for great publicity for the movie". After turning down numerous roles that required nudity, she said she decided to make "Swordfish" because her then-husband, Eric Benét, supported her and encouraged her to take risks.

Berry appeared as Leticia Musgrove, the troubled wife of an executed murderer (Sean Combs), in the 2001 feature film "Monster's Ball". Her performance was awarded the National Board of Review and the Screen Actors Guild Award for Best Actress; in an interesting coincidence she became the first woman of color to win the Academy Award for Best Actress (earlier in her career, she portrayed Dorothy Dandridge, the first African American to be nominated for Best Actress, and who was born at the same hospital as Berry, in Cleveland, Ohio). The NAACP issued the statement: "Congratulations to Halle Berry and Denzel Washington for giving us hope and making us proud. If this is a sign that Hollywood is finally ready to give opportunity and judge performance based on skill and not on skin color then it is a good thing." This role generated controversy. Her graphic nude love scene with a racist character played by co-star Billy Bob Thornton was the subject of much media chatter and discussion among African Americans. Many in the African-American community were critical of Berry for taking the part. Berry responded: "I don't really see a reason to ever go that far again. That was a unique movie. That scene was special and pivotal and needed to be there, and it would be a really special script that would require something like that again."

Berry asked for a higher fee for Revlon advertisements after winning the Oscar. Ron Perelman, the cosmetics firm's chief, congratulated her, saying how happy he was that she modeled for his company. She replied, "Of course, you'll have to pay me more." Perelman stalked off in a rage. In accepting her award, she gave an acceptance speech honoring previous black actresses who had never had the opportunity. She said, "This moment is so much bigger than me. This is for every nameless, faceless woman of color who now has a chance tonight because this door has been opened."

As Bond girl Giacinta 'Jinx' Johnson in the 2002 blockbuster "Die Another Day", Berry recreated a scene from "Dr. No", emerging from the surf to be greeted by James Bond as Ursula Andress had 40 years earlier. Lindy Hemming, costume designer on "Die Another Day", had insisted that Berry wear a bikini and knife as a homage. Berry has said of the scene: "It's splashy", "exciting", "sexy", "provocative" and "it will keep me still out there after winning an Oscar". The bikini scene was shot in Cadiz; the location was reportedly cold and windy, and footage has been released of Berry wrapped in thick towels in between takes to try to stay warm. According to an ITV news poll, Jinx was voted the fourth toughest girl on screen of all time. Berry was hurt during filming when debris from a smoke grenade flew into her eye. It was removed in a 30-minute operation. After Berry won the Academy Award, rewrites were commissioned to give her more screentime for "X2".

She starred in the psychological thriller "Gothika" opposite Robert Downey, Jr. in November 2003, during which she broke her arm in a scene with Downey, who twisted her arm too hard. Production was halted for eight weeks. It was a moderate hit at the United States box office, taking in $60 million; it earned another $80 million abroad. Berry appeared in the nu metal band Limp Bizkit's music video for "Behind Blue Eyes" for the motion picture soundtrack for the film. The same year, she was named #1 in "FHM"s 100 Sexiest Women in the World poll.

Berry starred as the title role in the film "Catwoman", for which she received US$12.5 million. An over-US$100 million movie; it grossed only US$17 million on its first weekend, and is widely regarded by critics as one of the worst films ever made. She was awarded the Worst Actress Razzie Award for her performance; she appeared at the ceremony to accept the award in person (while holding her Oscar from "Monster's Ball") with a sense of humor, considering it an experience of the "rock bottom" in order to be "at the top". Holding the Academy Award in one hand and the Razzie in the other she said, "I never in my life thought that I would be up here, winning a Razzie! It's not like I ever aspired to be here, but thank you. When I was a kid, my mother told me that if you could not be a good loser, then there's no way you could be a good winner."
Her next film appearance was in the Oprah Winfrey-produced ABC television movie "Their Eyes Were Watching God" (2005), an adaptation of Zora Neale Hurston's novel, with Berry portraying a free-spirited woman whose unconventional sexual mores upset her 1920s contemporaries in a small community. She received her second Primetime Emmy Award nomination for her role. Also in 2005, she served as an executive producer in "Lackawanna Blues", and landed her voice for the character of Cappy, one of the many mechanical beings in the animated feature "Robots".

In the thriller "Perfect Stranger" (2007), Berry starred with Bruce Willis, playing a reporter who goes undercover to uncover the killer of her childhood friend. The film grossed a modest US$73 million worldwide, and received lukewarm reviews from critics, who felt that despite the presence of Berry and Willis, it is "too convoluted to work, and features a twist ending that's irritating and superfluous". Her next 2007 film release was the drama "Things We Lost in the Fire", co-starring Benicio del Toro, where she took on the role of a recent widow befriending the troubled friend of her late husband. The film was the first time in which she worked with a female director, Danish Susanne Bier, giving her a new feeling of "thinking the same way", which she appreciated. While the film made US$8.6 million in its global theatrical run, it garnered positive reviews from writers; "The Austin Chronicle" found the film to be "an impeccably constructed and perfectly paced drama of domestic and internal volatility" and felt that "Berry is brilliant here, as good as she's ever been".

In April 2007, Berry was awarded a star on the Hollywood Walk of Fame in front of the Kodak Theatre at 6801 Hollywood Boulevard for her contributions to the film industry, and by the end of the decade, she established herself as one of the highest-paid actresses in Hollywood, earning an estimated $10 million per film.

In the independent drama "Frankie and Alice" (2010), Berry played the leading role of a young multiracial American woman with dissociative identity disorder struggling against her alter personality to retain her true self. The film received a limited theatrical release, to a mixed critical response. "The Hollywood Reporter" nevertheless described the film as "a well-wrought psychological drama that delves into the dark side of one woman's psyche" and found Berry to be "spellbinding" in it. She earned the African-American Film Critics Association Award for Best Actress and a Golden Globe Award nomination for Best Actress – Motion Picture Drama. She next made part of a large ensemble cast in Garry Marshall's romantic comedy "New Year's Eve" (2011), with Michelle Pfeiffer, Jessica Biel, Robert De Niro, Josh Duhamel, Zac Efron, Sarah Jessica Parker, and Sofía Vergara, among many others. In the film, she took on the supporting role of a nurse befriending a man in the final stages (De Niro). While the film was panned by critics, it made US$142 million worldwide.

In 2012, Berry starred as an expert diver tutor alongside then-husband Oliver Martinez in the little-seen thriller "Dark Tide", and led an ensemble cast opposite Tom Hanks and Jim Broadbent in The Wachowskis's epic science fiction film "Cloud Atlas" (2012), with each of the actors playing six different characters across a period of five centuries. Budgeted at US$128.8 million, "Cloud Atlas" made US$130.4 million worldwide, and garnered polarized reactions from both critics and audiences.

Berry appeared in a segment of the independent anthology comedy "Movie 43" (2013), which the "Chicago Sun-Times" called "the "Citizen Kane" of awful". Berry found greater success with her next performance, as a 9-1-1 operator receiving a call from a girl kidnapped by a serial killer, in the crime thriller "The Call" (2013). Berry was drawn to "the idea of being a part of a movie that was so empowering for women. We don't often get to play roles like this, where ordinary people become heroic and do something extraordinary." Manohla Dargis of "The New York Times" found the film to be "an effectively creepy thriller", while reviewer Dwight Brown felt that "the script gives Berry a blue-collar character she can make accessible, vulnerable and gutsy[...]". "The Call" was a sleeper hit, grossing US$68.6 million around the globe.

In 2014, Berry signed on to star and serve as a co-executive producer in CBS drama series "Extant", where she took on the role of Molly Woods, an astronaut who struggles to reconnect with her husband and android son after spending 13 months in space. The show ran for two seasons until 2015, receiving largely positive reviews from critics. "USA Today" remarked: "She [Halle Berry] brings a dignity and gravity to Molly, a projected intelligence that allows you to buy her as an astronaut and to see what has happened to her as frightening rather than ridiculous. Berry's all in, and you float along". Also in 2014, Berry launched a new production company, 606 Films, with producing partner Elaine Goldsmith-Thomas. It is named after the Anti-Paparazzi Bill, SB 606, that the actress pushed for and which was signed into law by California Governor Jerry Brown in the fall of 2013. The new company emerged as part of a deal for Berry to work in "Extant".

In the stand-up comedy concert film "" (2016), Berry appeared as herself, opposide Kevin Hart, attending a poker game event that goes horribly wrong. "Kidnap", an abduction thriller Berry filmed in 2014, was released in 2017. In the film, she starred as a diner waitress tailing a vehicle when her son is kidnapped by its occupants. "Kidnap" grossed US$34 million and garnered mixed reviews from writers, who felt that it "strays into poorly scripted exploitation too often to take advantage of its pulpy premise — or the still-impressive talents of [Berry]." She next played an agent employed by a secret American spy organisation in the action comedy sequel "" (2017), as part of an ensemble cast, consisting of Colin Firth, Taron Egerton, Mark Strong, Julianne Moore, and Elton John. While critical response towards the film was mixed, it made US$414 million worldwide.

Alongside Daniel Craig, Berry starred as a working-class mother during the 1992 Los Angeles riots in Deniz Gamze Ergüven's drama "Kings" (2017). The film found a limited theatrical release following its initial screening at the Toronto International Film Festival, and as part of an overall lukewarm reception, "Variety" noted: "It should be said that Berry has given some of the best and worst performances of the past quarter-century, but this is perhaps the only one that swings to both extremes in the same movie". She played Sofia, an assassin, in the film "", which was released on May 17, 2019 by Lionsgate.

Berry competed against James Corden in the first rap battle on the first episode of TBS's "Drop the Mic", originally aired on October 24, 2017.

She currently serves as executive producer of the BET television series "Boomerang", based on the film in which she starred. The series premiered February 12, 2019.

Berry dated Chicago dentist John Ronan from March 1989 to October 1991. In November 1993, Ronan sued Berry for $80,000 in what he claimed were unpaid loans to help launch her career. Berry contended that the money was a gift, and a judge dismissed the case because Ronan did not list Berry as a debtor when he filed for bankruptcy in 1992. According to Berry, a beating from a former abusive boyfriend during the filming of "The Last Boy Scout" in 1991 punctured her eardrum and caused her to lose eighty percent of her hearing in her left ear. Berry has never named the abuser but has said that he is someone well known in Hollywood. In 2004, former boyfriend Christopher Williams accused Wesley Snipes of being responsible for the incident, saying "I'm so tired of people thinking I'm the guy [who did it]. Wesley Snipes busted her eardrum, not me."

Berry first saw baseball player David Justice on TV playing in an MTV celebrity baseball game in February 1992. When a reporter from Justice's hometown of Cincinnati told her that Justice was a fan, Berry gave her phone number to the reporter to give to Justice. Berry married Justice shortly after midnight on January 1, 1993. Following their separation in February 1996, Berry stated publicly that she was so depressed that she considered taking her own life. Berry and Justice were officially divorced on June 24, 1997.

Berry married her second husband, singer-songwriter Eric Benét, on January 24, 2001, following a two-year courtship. Benét underwent treatment for sex addiction in 2002, and by early October 2003 they had separated, with the divorce finalized on January 3, 2005.

In November 2005, Berry began dating French Canadian model Gabriel Aubry, whom she met at a Versace photoshoot. Berry gave birth to their daughter in March 2008. On April 30, 2010, Berry and Aubry announced their relationship had ended some months earlier. In January 2011, Berry and Aubry became involved in a highly publicized custody battle, centered primarily on Berry's desire to move with their daughter from Los Angeles, where Berry and Aubry resided, to France, the home of French actor Olivier Martinez, whom Berry had started dating in 2010 after they met while filming "Dark Tide" in South Africa. Aubry objected to the move on the grounds that it would interfere with their joint custody arrangement. In November 2012, a judge denied Berry's request to move the couple's daughter to France in light of Aubry's objections. Less than two weeks later, on November 22, 2012, Aubry and Martinez were both treated at a hospital for injuries after engaging in a physical altercation at Berry's residence. Martinez performed a citizen's arrest on Aubry, and because it was considered a domestic violence incident, was granted a temporary emergency protective order preventing Aubry from coming within 100 yards of Berry, Martinez, and the child with whom he shares custody with Berry, until November 29, 2012. In turn, Aubry obtained a temporary restraining order against Martinez on November 26, 2012, asserting that the fight began when Martinez threatened to kill Aubry if he did not allow the couple to move to France. Leaked court documents included photos showing significant injuries to Aubry's face, which were widely displayed in the media.

On November 29, 2012, Berry's lawyer announced that Berry and Aubry had reached an amicable custody agreement in court. In June 2014, a Superior Court ruling called for Berry to pay Aubry $16,000 a month in child support (around 200k/year) as well as a retroactive payment of $115,000 and a sum of $300,000 for Aubry's attorney fees.

Berry and Martinez confirmed their engagement in March 2012, and married in France on July 13, 2013. In October 2013, Berry gave birth to their son. In 2015, after two years of marriage, the couple announced they were divorcing. The divorce became final in December 2016.

In February 2000, Berry was involved in a traffic collision and left the scene. She pleaded no contest to misdemeanor leaving the scene of an accident.

Along with Pierce Brosnan, Cindy Crawford, Jane Seymour, Dick Van Dyke, Téa Leoni, and Daryl Hannah, Berry successfully fought in 2006 against the Cabrillo Port Liquefied Natural Gas facility that was proposed off the coast of Malibu. Berry said, "I care about the air we breathe, I care about the marine life and the ecosystem of the ocean." In May 2007, Governor Arnold Schwarzenegger vetoed the facility. Hasty Pudding Theatricals gave her its 2006 "Woman of The Year" award. Berry took part in a nearly 2,000-house cell-phone bank campaign for Barack Obama in February 2008. In April 2013, she appeared in a video clip for Gucci's "Chime for Change" campaign that aims to raise funds and awareness of women's issues in terms of education, health, and justice. In August 2013, Berry testified alongside Jennifer Garner before the California State Assembly's Judiciary Committee in support of a bill that would protect celebrities' children from harassment by photographers. The bill passed in September.

Berry was ranked No. 1 on "People" "50 Most Beautiful People in the World" list in 2003 after making the top ten seven times and appeared No. 1 on "FHM" "100 Sexiest Women in the World" the same year. She was named "Esquire" magazine's "Sexiest Woman Alive" in October 2008, about which she stated: "I don't know exactly what it means, but being 42 and having just had a baby, I think I'll take it." "Men's Health" ranked her at No. 35 on their "100 Hottest Women of All-Time" list. In 2009, she was voted #23 on "Empire"'s 100 Sexiest Film Stars. The same year, rapper Hurricane Chris released a song entitled "Halle Berry (She's Fine)", extolling Berry's beauty and sex appeal. At the age of 42 (in 2008), she was named the "Sexiest Black Woman" by Access Hollywood's "TV One Access" survey. Born to an African-American father and a white mother, Berry has stated that her biracial background was "painful and confusing" when she was a young woman, and she made the decision early on to identify as a black woman because she knew that was how she would be perceived.

Berry will make directorial debut with Bruised in which she plays a disgraced MMA fighter named Jackie Justice, who reconnects with her estranged son. Filming began in 2019 with shooting in Atlantic City and Newark.




!colspan="3" style="background:#C1D8FF;"| Acting roles


</doc>
<doc id="13722" url="https://en.wikipedia.org/wiki?curid=13722" title="Robert Koch">
Robert Koch

Heinrich Hermann Robert Koch (; ; 11 December 1843 – 27 May 1910) was a German physician and microbiologist. As one of the main founders of modern bacteriology, he identified the specific causative agents of tuberculosis, cholera, and anthrax and also gave experimental support for the concept of infectious disease, which included experiments on humans and other animals. Koch created and improved laboratory technologies and techniques in the field of microbiology, and made key discoveries in public health. His research led to the creation of Koch's postulates, a series of four generalized principles linking specific microorganisms to specific diseases that remain today the "gold standard" in medical microbiology. For his research on tuberculosis, Koch received the Nobel Prize in Physiology or Medicine in 1905. The Robert Koch Institute is named in his honour.

Koch was born in Clausthal, Germany, on 11 December 1843, to Hermann Koch (1814–1877) and Mathilde Julie Henriette (née Biewend; 1818–1871). Koch excelled in academics from an early age. Before entering school in 1848, he had taught himself how to read and write. He graduated from high school in 1862, having excelled in science and math. At the age of 19, Koch entered the University of Göttingen, studying natural science. However, after three semesters, Koch decided to change his area of study to medicine, as he aspired to be a physician. During his fifth semester of medical school, Jacob Henle, an anatomist who had published a theory of contagion in 1840, asked him to participate in his research project on uterine nerve structure. In his sixth semester, Koch began to research at the Physiological Institute, where he studied the secretion of succinic acid, which is a signaling molecule that is also involved in the metabolism of the mitochondria. This would eventually form the basis of his dissertation. In January 1866, Koch graduated from medical school, earning honors of the highest distinction.

Several years after his graduation in 1866, he worked as a surgeon in the Franco-Prussian War, and following his service, worked as a physician in in Prussian Posen (now Wolsztyn, Poland). From 1880 to 1885, Koch held a position as government advisor with the Imperial Department of Health. Koch began conducting research on microorganisms in a laboratory connected to his patient examination room. Koch's early research in this laboratory yielded one of his major contributions to the field of microbiology, as he developed the technique of growing bacteria. Furthermore, he managed to isolate and grow selected pathogens in pure laboratory culture.

From 1885 to 1890, he served as an administrator and professor at Berlin University.

In 1891, Koch relinquished his Professorship and became a director of the which consisted of a clinical division and beds for the division of clinical research. For this he accepted harsh conditions. The Prussian Ministry of Health insisted after the 1890 scandal with tuberculin, which Koch had discovered and intended as a remedy for tuberculosis, that any of Koch's inventions would unconditionally belong to the government and he would not be compensated. Koch lost the right to apply for patent protection.

In an attempt to grow bacteria, Koch began to use solid nutrients such as potato slices. Through these initial experiments, Koch observed individual colonies of identical, pure cells. He found that potato slices were not suitable media for all organisms, and later began to use nutrient solutions with gelatin. However, he soon realized that gelatin, like potato slices, was not the optimal medium for bacterial growth, as it did not remain solid at 37 °C, the ideal temperature for growth of most human pathogens. As suggested to him by Walther and Fanny Hesse, Koch began to utilize agar to grow and isolate pure cultures, because this polysaccharide remains solid at 37 °C, is not degraded by most bacteria, and results in a transparent medium.

During his time as government advisor, Koch published a report, in which he stated the importance of pure cultures in isolating disease-causing organisms and explained the necessary steps to obtain these cultures, methods which are summarized in Koch's four postulates. Koch's discovery of the causative agent of anthrax led to the formation of a generic set of postulates which can be used in the determination of the cause of most infectious diseases. These postulates, which not only outlined a method for linking cause and effect of an infectious disease but also established the significance of laboratory culture of infectious agents, are listed here:

Robert Koch is widely known for his work with anthrax, discovering the causative agent of the fatal disease to be "Bacillus anthracis". He discovered the formation of spores in anthrax bacteria, which could remain dormant under specific conditions. However, under optimal conditions, the spores were activated and caused disease. To determine this causative agent, he dry-fixed bacterial cultures onto glass slides, used dyes to stain the cultures, and observed them through a microscope. His work with anthrax is notable in that he was the first to link a specific microorganism with a specific disease, rejecting the idea of spontaneous generation and supporting the germ theory of disease.

During his time as the government advisor with the Imperial Department of Health in Berlin in the 1880s, Robert Koch became interested in tuberculosis research. At the time, it was widely believed that tuberculosis was an inherited disease. However, Koch was convinced that the disease was caused by a bacterium and was infectious, and tested his four postulates using guinea pigs. Through these experiments, he found that his experiments with tuberculosis satisfied all four of his postulates. In 1882, he published his findings on tuberculosis, in which he reported the causative agent of the disease to be the slow-growing "Mycobacterium tuberculosis". Later, Koch's attempt at developing a drug to treat tuberculosis, tuberculin, led to a scandalous failure: he did not divulge the exact composition, and the claimed treatment success did not materialize; the substance is today used for tuberculosis diagnosis. 

Koch and his relationship to Paul Ehrlich, who developed a mechanism to diagnose TB, were portrayed in the 1940 movie "Dr. Ehrlich's Magic Bullet".

Koch next turned his attention to cholera, and began to conduct research in Egypt in the hopes of isolating the causative agent of the disease. However, he was not able to complete the task before the epidemic in Egypt ended, and after a short trip to Persia traveled to India to continue with the study. In 1884 in Bombay state of India (the present day State of Maharastra, India), Koch resided and researched at Grant Medical College, (or by some accounts in Kolkata, formerly Calcutta in undivided British India) where he was able to determine the causative agent of cholera, isolating "Vibrio cholerae". The bacterium had originally been isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.

Koch observed the phenomenon of acquired immunity. On December 26, 1900, he arrived as part of an expedition to German New Guinea, which was then a protectorate of the German Reich. Koch serially examined the Papuan people, the indigenous inhabitants, and their blood samples and noticed they contained Plasmodium parasites, the cause of malaria, but their bouts of malaria were mild or could not even be noticed, i.e. were subclinical. On the contrary, German settlers and Chinese workers, who had been brought to New Guinea, fell sick immediately. The longer they had stayed in the country, however, the more they too seemed to develop a resistance against it.

In 1897, Koch was elected a Foreign Member of the Royal Society (ForMemRS). 
In 1905, Koch won the Nobel Prize in Physiology and Medicine for his work with tuberculosis.
In 1906, research on tuberculosis and tropical diseases won him the Prussian Order Pour le Merite and in 1908, the Robert Koch Medal, established to honour the greatest living physicians.

Koch's name is one of 23, from the fields of hygiene and tropical medicine, featured on the frieze of the London School of Hygiene & Tropical Medicine building in Keppel Street, Bloomsbury.

A large marble statue of Koch stands in a small park known as Robert Koch Platz, just north of the Charity Hospital, in the Mitte section of Berlin. His life was the subject of a 1939 German produced motion picture that featured Oscar winning actor Emil Jannings in the title role. On 10 December 2017, Google showed a Doodle in celebration of Koch's birthday.

In July 1867, Koch married Emma (Emmy) Adolfine Josephine Fraatz, and the two had a daughter, Gertrude, in 1868. Their marriage ended after 26 years in 1893, and later that same year, he married actress Hedwig Freiberg (1872–1945).

On 9 April 1910, Koch suffered a heart attack and never made a complete recovery. On 27 May, three days after giving a lecture on his tuberculosis research at the Prussian Academy of Sciences, Koch died in Baden-Baden at the age of 66. Following his death, the Institute named its establishment after him in his honour. He was irreligious.




</doc>
<doc id="13726" url="https://en.wikipedia.org/wiki?curid=13726" title="Hogshead">
Hogshead

A hogshead (abbreviated "hhd", plural "hhds") is a large cask of liquid (or, less often, of a food commodity). More specifically, it refers to a specified volume, measured in either imperial or US customary measures, primarily applied to alcoholic beverages, such as wine, ale, or cider.

A tobacco hogshead was used in British and American colonial times to transport and store tobacco. It was a very large wooden barrel. A standardized hogshead measured long and in diameter at the head (at least , depending on the width in the middle). Fully packed with tobacco, it weighed about .

A hogshead contains about .

The "Oxford English Dictionary" (OED) notes that the hogshead was first standardized by an act of Parliament in 1423, though the standards continued to vary by locality and content. For example, the OED cites an 1897 edition of "Whitaker's Almanack", which specified the number of gallons of wine in a hogshead varying by type of wine: claret (presumably) , port , sherry ; and Madeira . The "American Heritage Dictionary" claims that a hogshead can consist of anything from (presumably) .

Eventually, a hogshead of wine came to be , while a hogshead of beer or ale is 54 gallons (250 L if old beer/ale gallons, 245 L if imperial).

A hogshead was also used as unit of measurement for sugar in Louisiana for most of the 19th century. Plantations were listed in sugar schedules as having produced "x" number of hogsheads of sugar or molasses. A hogshead was also used for the measurement of herring fished for sardines in Blacks Harbour, New Brunswick.

The etymology of hogshead is uncertain. According to English philologist Walter William Skeat (1835–1912), the origin is to be found in the name for a cask or liquid measure appearing in various forms in several Germanic languages, in Dutch "oxhooft" (modern "okshoofd"), Danish "oxehoved", Old Swedish "oxhuvud", etc. The word should therefore be "oxhead", "hogshead" being a mere corruption. It has been suggested that the name arose from the branding of such a measure with the head of an ox.

A hogshead of Madeira wine was approximately equal to 45–48 gallons (0.205–0.218 m). A hogshead of brandy was approximately equal to 56–61 gallons (0.255–0.277) m.



</doc>
<doc id="13727" url="https://en.wikipedia.org/wiki?curid=13727" title="Huallaga">
Huallaga

Huallaga may refer to:



</doc>
<doc id="13729" url="https://en.wikipedia.org/wiki?curid=13729" title="Honda">
Honda

Honda has been the world's largest motorcycle manufacturer since 1959, reaching a production of 400 million by the end of 2019, as well as the world's largest manufacturer of internal combustion engines measured by volume, producing more than 14 million internal combustion engines each year. Honda became the second-largest Japanese automobile manufacturer in 2001. Honda was the eighth largest automobile manufacturer in the world in 2015.

Honda was the first Japanese automobile manufacturer to release a dedicated luxury brand, Acura, in 1986. Aside from their core automobile and motorcycle businesses, Honda also manufactures garden equipment, marine engines, personal watercraft and power generators, and other products. Since 1986, Honda has been involved with artificial intelligence/robotics research and released their ASIMO robot in 2000. They have also ventured into aerospace with the establishment of GE Honda Aero Engines in 2004 and the Honda HA-420 HondaJet, which began production in 2012. Honda has three joint-ventures in China: Honda China, Dongfeng Honda, and Guangqi Honda.

In 2013, Honda invested about 5.7% (US$6.8 billion) of its revenues in research and development. Also in 2013, Honda became the first Japanese automaker to be a net exporter from the United States, exporting 108,705 Honda and Acura models, while importing only 88,357.

Throughout his life, Honda's founder, Soichiro Honda, had an interest in automobiles. He worked as a mechanic at the Art Shokai garage, where he tuned cars and entered them in races. In 1937, with financing from his acquaintance Kato Shichirō, Honda founded Tōkai Seiki (Eastern Sea Precision Machine Company) to make piston rings working out of the Art Shokai garage. After initial failures, Tōkai Seiki won a contract to supply piston rings to Toyota, but lost the contract due to the poor quality of their products. After attending engineering school without graduating, and visiting factories around Japan to better understand Toyota's quality control processes, by 1941 Honda was able to mass-produce piston rings acceptable to Toyota, using an automated process that could employ even unskilled wartime laborers.

Tōkai Seiki was placed under control of the Ministry of Commerce and Industry (called the Ministry of Munitions after 1943) at the start of World War II, and Soichiro Honda was demoted from president to senior managing director after Toyota took a 40% stake in the company. Honda also aided the war effort by assisting other companies in automating the production of military aircraft propellers. The relationships Honda cultivated with personnel at Toyota, Nakajima Aircraft Company and the Imperial Japanese Navy would be instrumental in the postwar period. A US B-29 bomber attack destroyed Tōkai Seiki's Yamashita plant in 1944, and the Itawa plant collapsed in 13 January 1945 Mikawa earthquake. Soichiro Honda sold the salvageable remains of the company to Toyota after the war for ¥450,000, and used the proceeds to found the Honda Technical Research Institute in October 1946.

With a staff of 12 men working in a shack, they built and sold improvised motorized bicycles, using a supply of 500 two-stroke "50 cc" Tohatsu war surplus radio generator engines. When the engines ran out, Honda began building their own copy of the Tohatsu engine, and supplying these to customers to attach to their bicycles. This was the Honda A-Type, nicknamed the Bata Bata for the sound the engine made. In 1949, the Honda Technical Research Institute was liquidated for 1,000,000, or about 5,000 today; these funds were used to incorporate Honda Motor Co., Ltd. At about the same time Honda hired engineer Kihachiro Kawashima, and Takeo Fujisawa who provided indispensable business and marketing expertise to complement Soichiro Honda's technical bent. The close partnership between Soichiro Honda and Fujisawa lasted until they stepped down together in October 1973.

The first complete motorcycle, with both the frame and engine made by Honda, was the 1949 D-Type, the first Honda to go by the name Dream. Honda Motor Company grew in a short time to become the world's largest manufacturer of motorcycles by 1964.

The first production automobile from Honda was the T360 mini pick-up truck, which went on sale in August 1963. Powered by a small 356-cc straight-4 gasoline engine, it was classified under the cheaper Kei car tax bracket. The first production car from Honda was the S500 sports car, which followed the T360 into production in October 1963. Its chain-driven rear wheels pointed to Honda's motorcycle origins.

Over the next few decades, Honda worked to expand its product line and expanded operations and exports to numerous countries around the world. In 1986, Honda introduced the successful Acura brand to the American market in an attempt to gain ground in the luxury vehicle market. The year 1991 saw the introduction of the Honda NSX supercar, the first all-aluminum monocoque vehicle that incorporated a mid-engine V6 with variable-valve timing.

CEO Tadashi Kume was succeeded by Nobuhiko Kawamoto in 1990. Kawamoto was selected over Shoichiro Irimajiri, who oversaw the successful establishment of Honda of America Manufacturing, Inc. in Marysville, Ohio. Irimajiri and Kawamoto shared a friendly rivalry within Honda; owing to health issues, Irimajiri would resign in 1992.

Following the death of Soichiro Honda and the departure of Irimajiri, Honda found itself quickly being outpaced in product development by other Japanese automakers and was caught off-guard by the truck and sport utility vehicle boom of the 1990s, all which took a toll on the profitability of the company. Japanese media reported in 1992 and 1993 that Honda was at serious risk of an unwanted and hostile takeover by Mitsubishi Motors, which at the time was a larger automaker by volume and was flush with profits from its successful Pajero and Diamante models.

Kawamoto acted quickly to change Honda's corporate culture, rushing through market-driven product development that resulted in recreational vehicles such as the first-generation Odyssey and the CR-V, and a refocusing away from some of the numerous sedans and coupes that were popular with the company's engineers but not with the buying public. The most shocking change to Honda came when Kawamoto ended the company's successful participation in Formula One after the 1992 season, citing costs in light of the takeover threat from Mitsubishi as well as the desire to create a more environmentally friendly company image.

The Honda Aircraft Company was established in 1995, as a wholly owned subsidiary; its goal was to produce jet aircraft under Honda's name.

On 23 February 2015, Honda announced that CEO and President Takanobu Ito would step down and be replaced by Takahiro Hachigo by June; additional retirements by senior managers and directors were expected.

In October 2019, Honda was reported to be in talks with Hitachi to merge the two companies' car parts businesses, creating a components supplier with almost $17 billion in annual sales.

In January 2020, Honda announced that it would be withdrawing employees working in the city of Wuhan, Hubei, China in response to the Wuhan coronavirus outbreak.

Honda is headquartered in Minato, Tokyo, Japan. Their shares trade on the Tokyo Stock Exchange and the New York Stock Exchange, as well as exchanges in Osaka, Nagoya, Sapporo, Kyoto, Fukuoka, London, Paris and Switzerland.

The company has assembly plants around the globe. These plants are located in China, the United States, Pakistan, Canada, England, Japan, Belgium, Brazil, México, New Zealand, Malaysia, Indonesia, India, Philippines, Thailand, Vietnam, Turkey, Taiwan, Perú and Argentina. As of July 2010, 89 percent of Honda and Acura vehicles sold in the United States were built in North American plants, up from 82.2 percent a year earlier. This shields profits from the yen's advance to a 15-year high against the dollar.

American Honda Motor Company is based in Torrance, California. Honda Racing Corporation (HRC) is Honda's motorcycle racing division. Honda Canada Inc. is headquartered in Markham, Ontario, it was originally planned to be located in Richmond Hill, Ontario, but delays led them to look elsewhere. Their manufacturing division, Honda of Canada Manufacturing, is based in Alliston, Ontario. Honda has also created joint ventures around the world, such as Honda Siel Cars and Hero Honda Motorcycles in India, Guangzhou Honda and Dongfeng Honda in China, Boon Siew Honda in Malaysia and Honda Atlas in Pakistan. The company also runs a businesss innovation intitiave called Honda Xcelerator, in order to build relationships with innovators, partner with Silicon Valley startups and entrepreneurs, and help other companies work on prototypes. Xcelerator had worked with reportedly 40 companies as of January 2019. Xcelerator and a developer studio are part of the Honda Innovations group, formed in Spring 2017 and based in Mountain View, California.

Following the Japanese earthquake and tsunami in March 2011 Honda announced plans to halve production at its UK plants. The decision was made to put staff at the Swindon plant on a 2-day week until the end of May as the manufacturer struggled to source supplies from Japan. It's thought around 22,500 cars were produced during this period.

For the fiscal year 2018, Honda reported earnings of US$9.534 billion, with an annual revenue of US$138.250 billion, an increase of 6.2% over the previous fiscal cycle. Honda's shares traded at over $32 per share, and its market capitalization was valued at US$50.4 billion in October 2018.
Honda's Net Sales and Other Operating Revenue by Geographical Regions in 2007

Honda's automotive manufacturing ambitions can be traced back to 1963, with the Honda T360, a kei car truck built for the Japanese market. This was followed by the two-door roadster, the Honda S500 also introduced in 1963. In 1965, Honda built a two-door commercial delivery van, called the Honda L700. Honda's first four-door sedan was not the Accord, but the air-cooled, four-cylinder, gasoline-powered Honda 1300 in 1969. The Civic was a hatchback that gained wide popularity internationally, but it wasn't the first two-door hatchback built. That was the Honda N360, another "Kei car" that was adapted for international sale as the N600. The Civic, which appeared in 1972 and replaced the N600 also had a smaller sibling that replaced the air-cooled N360, called the Honda Life that was water-cooled.

The Honda Life represented Honda's efforts in competing in the "kei" car segment, offering sedan, delivery van and small pick-up platforms on a shared chassis. The Life StepVan had a novel approach that, while not initially a commercial success, appears to be an influence in vehicles with the front passengers sitting behind the engine, a large cargo area with a flat roof and a liftgate installed in back, and utilizing a transversely installed engine with a front-wheel-drive powertrain.

As Honda entered into automobile manufacturing in the late 1960s, where Japanese manufacturers such as Toyota and Nissan had been making cars since before WWII, it appears that Honda instilled a sense of doing things a little differently than its Japanese competitors. Its mainstay products, like the Accord and Civic (with the exception of its USA-market 1993–97 Passport which was part of a vehicle exchange program with Isuzu (part of the Subaru-Isuzu joint venture)), have always employed front-wheel-drive powertrain implementation, which is currently a long-held Honda tradition. Honda also installed new technologies into their products, first as optional equipment, then later standard, like anti lock brakes, speed sensitive power steering, and multi-port fuel injection in the early 1980s. This desire to be the first to try new approaches is evident with the creation of the first Japanese luxury chain Acura, and was also evident with the all aluminum, mid-engined sports car, the Honda NSX, which also introduced variable valve timing technology, Honda calls VTEC.

The Civic is a line of compact cars developed and manufactured by Honda. In North America, the Civic is the second-longest continuously running nameplate from a Japanese manufacturer; only its perennial rival, the Toyota Corolla, introduced in 1968, has been in production longer. The Civic, along with the Accord and Prelude, comprised Honda's vehicles sold in North America until the 1990s, when the model lineup was expanded. Having gone through several generational changes, the Civic has become larger and more upmarket, and it currently slots between the Fit and Accord.

Honda produces Civic hybrid, a hybrid electric vehicle that competes with the Toyota Prius, and also produces the Insight and CR-Z.

In 2008, Honda increased global production to meet demand for small cars and hybrids in the U.S. and emerging markets. The company shuffled U.S. production to keep factories busy and boost car output, while building fewer minivans and sport utility vehicles as light truck sales fell.

Its first entrance into the pickup segment, the light duty Ridgeline, won Truck of the Year from "Motor Trend" magazine in 2006. Also in 2006, the redesigned Civic won Car of the Year from the magazine, giving Honda a rare double win of Motor Trend honors.

It is reported that Honda plans to increase hybrid sales in Japan to more than 20% of its total sales in fiscal year 2011, from 14.8% in previous year.

Five of United States Environmental Protection Agency's top ten most fuel-efficient cars from 1984 to 2010 comes from Honda, more than any other automakers. The five models are: 2000–2006 Honda Insight ( combined), 1986–1987 Honda Civic Coupe HF ( combined), 1994–1995 Honda Civic hatchback VX ( combined), 2006– Honda Civic Hybrid ( combined), and 2010– Honda Insight ( combined). The ACEEE has also rated the Civic GX as the greenest car in America for seven consecutive years.

Honda is the largest motorcycle manufacturer in Japan and has been since it started production in 1955.
At its peak in 1982, Honda manufactured almost three million motorcycles annually. By 2006 this figure had reduced to around 550,000 but was still higher than its three domestic competitors.

In 2017, India became the largest motorcycle market of Honda. In India, Honda is leading in the scooters segment, with 59 percent market share.

During the 1960s, when it was a small manufacturer, Honda broke out of the Japanese motorcycle market and began exporting to the U.S. Working with the advertising agency Grey Advertising, Honda created an innovative marketing campaign, using the slogan "You meet the nicest people on a Honda." In contrast to the prevailing negative stereotypes of motorcyclists in America as tough, antisocial rebels, this campaign suggested that Honda motorcycles were made for the everyman. The campaign was hugely successful; the ads ran for three years, and by the end of 1963 alone, Honda had sold 90,000 motorcycles.

Taking Honda's story as an archetype of the smaller manufacturer entering a new market already occupied by highly dominant competitors, the story of their market entry, and their subsequent huge success in the U.S. and around the world, has been the subject of some academic controversy. Competing explanations have been advanced to explain Honda's strategy and the reasons for their success.

The first of these explanations was put forward when, in 1975, Boston Consulting Group (BCG) was commissioned by the UK government to write a report explaining why and how the British motorcycle industry had been out-competed by its Japanese competitors. The report concluded that the Japanese firms, including Honda, had sought a very high scale of production (they had made a large number of motorbikes) in order to benefit from economies of scale and learning curve effects. It blamed the decline of the British motorcycle industry on the failure of British managers to invest enough in their businesses to profit from economies of scale and scope.
The second explanation was offered in 1984 by Richard Pascale, who had interviewed the Honda executives responsible for the firm's entry into the U.S. market. As opposed to the tightly focused strategy of low cost and high scale that BCG accredited to Honda, Pascale found that their entry into the U.S. market was a story of "miscalculation, serendipity, and organizational learning" – in other words, Honda's success was due to the adaptability and hard work of its staff, rather than any long term strategy. For example, Honda's initial plan on entering the US was to compete in large motorcycles, around 300 cc. Honda's motorcycles in this class suffered performance and reliability problems when ridden the relatively long distances of the US highways. When the team found that the scooters they were using to get themselves around their U.S. base of San Francisco attracted positive interest from consumers that they fell back on selling the Super Cub instead.

The most recent school of thought on Honda's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989. Creating the concept of core competencies with Honda as an example, they argued that Honda's success was due to its focus on leadership in the technology of internal combustion engines. For example, the high power-to-weight ratio engines Honda produced for its racing bikes provided technology and expertise which was transferable into mopeds. Honda's entry into the U.S. motorcycle market during the 1960s is used as a case study for teaching introductory strategy at business schools worldwide.

Production started in 1953 with H-type engine (prior to motorcycle).

Honda power equipment reached record sales in 2007 with 6.4 million units. By 2010 this figure had decreased to 4,7 million units. Cumulative production of power products has exceeded 85 million units (as of September 2008).

Honda power equipment includes:
Honda engines powered the entire 33-car starting field of the 2010 Indianapolis 500 and for the fifth consecutive race, there were no engine-related retirements during the running of the Memorial Day Classic.

In the 1980s Honda developed the GY6 engine for use in motor scooters. Although no longer manufactured by Honda it is still commonly used in many Chinese, Korean and Taiwanese light vehicles.

Honda, despite being known as an engine company, has never built a V8 for passenger vehicles. In the late 1990s, the company resisted considerable pressure from its American dealers for a V8 engine (which would have seen use in top-of-the-line Honda SUVs and Acuras), with American Honda reportedly sending one dealer a shipment of V8 beverages to silence them. Honda considered starting V8 production in the mid-2000s for larger Acura sedans, a new version of the high end NSX sports car (which previously used DOHC V6 engines with VTEC to achieve its high power output) and possible future ventures into the American full-size truck and SUV segment for both the Acura and Honda brands, but this was cancelled in late 2008, with Honda citing environmental and worldwide economic conditions as reasons for the termination of this project.

ASIMO is the part of Honda's Research & Development robotics program. It is the eleventh in a line of successive builds starting in 1986 with Honda E0 moving through the ensuing Honda E series and the Honda P series. Weighing 54 kilograms and standing 130 centimeters tall, ASIMO resembles a small astronaut wearing a backpack, and can walk on two feet in a manner resembling human locomotion, at up to . ASIMO is the world's only humanoid robot able to ascend and descend stairs independently. However, human motions such as climbing stairs are difficult to mimic with a machine, which ASIMO has demonstrated by taking two plunges off a staircase.

Honda's robot ASIMO (see below) as an R&D project brings together expertise to create a robot that walks, dances and navigates steps.
2010 marks the year Honda has developed a machine capable of reading a user's brainwaves to move ASIMO. The system uses a helmet covered with electroencephalography and near-infrared spectroscopy sensors that monitor electrical brainwaves and cerebral blood flow—signals that alter slightly during the human thought process. The user thinks of one of a limited number of gestures it wants from the robot, which has been fitted with a Brain Machine Interface.

Honda has also pioneered new technology in its HA-420 HondaJet, manufactured by its subsidiary Honda Aircraft Company, which allows new levels of reduced drag, increased aerodynamics and fuel efficiency thus reducing operating costs.

Honda has also built a downhill racing bicycle known as the Honda RN-01. It is not available for sale to the public. The bike has a gearbox, which replaces the standard derailleur found on most bikes.

Honda has hired several people to pilot the bike, among them Greg Minnaar. The team is known as Team G Cross Honda.

Honda also builds all-terrain vehicles (ATV).
420
450r
400ex
300ex
250r

Honda's solar cell subsidiary company Honda Soltec (Headquarters: Kikuchi-gun, Kumamoto; President and CEO: Akio Kazusa) started sales throughout Japan of thin-film solar cells for public and industrial use on 24 October 2008, after selling solar cells for residential use since October 2007. Honda announced in the end of October 2013 that Honda Soltec would cease the business operation except for support for existing customers in Spring 2014 and the subsidiary would be dissolved.

Honda has been active in motorsports, like Formula One, Motorcycle Grand Prix and others.

Honda entered Formula One as a constructor for the first time in the 1964 season at the German Grand Prix with Ronnie Bucknum at the wheel. 1965 saw the addition of Richie Ginther to the team, who scored Honda's first point at the Belgian Grand Prix, and Honda's first win at the Mexican Grand Prix. 1967 saw their next win at the Italian Grand Prix with John Surtees as their driver. In 1968, Jo Schlesser was killed in a Honda RA302 at the French Grand Prix. This racing tragedy, coupled with their commercial difficulties selling automobiles in the United States, prompted Honda to withdraw from all international motorsport that year.

After a learning year in 1965, Honda-powered Brabhams dominated the 1966 French Formula Two championship in the hands of Jack Brabham and Denny Hulme. As there was no European Championship that season, this was the top F2 championship that year. In the early 1980s Honda returned to F2, supplying engines to Ron Tauranac's Ralt team. Tauranac had designed the Brabham cars for their earlier involvement. They were again extremely successful. In a related exercise, John Judd's Engine Developments company produced a turbo "Brabham-Honda" engine for use in IndyCar racing. It won only one race, in 1988 for Bobby Rahal at Pocono.

Honda returned to Formula One in 1983, initially with another Formula Two partner, the Spirit team, before switching abruptly to Williams in 1984. Between 1986 and 1991, Honda won six consecutive Formula One Constructors' Championships as an engine manufacturer, as well as five consecutive Drivers' Championships with Nelson Piquet, Ayrton Senna and Alain Prost. Williams-Honda won the crown in 1986 and 1987. Honda switched allegiance to McLaren in 1988, and then won the title in 1988, 1989, 1990 and 1991. Honda withdrew from Formula One at the end of 1992, although the related Mugen company maintained a presence up to the end of 2000, winning four races with Ligier and Jordan.

Honda debuted in the CART IndyCar World Series as a works supplier in 1994. The engines were far from competitive at first, but after development, the company won six consecutive drivers' championships and four manufacturers' championships between 1996 and 2001. In 2003, Honda transferred its effort to the IRL IndyCar Series with Ilmor supporting HPD. In 2004, Honda-powered cars overwhelmingly dominated the IndyCar Series, winning 14 of 16 IndyCar races, including the Indianapolis 500, and claimed the IndyCar Series Manufacturers' Championship, Drivers' Championship and Rookie of the Year titles. From 2006 to 2011, Honda was the lone engine supplier for the IndyCar Series, including the Indianapolis 500. In the 2006 Indianapolis 500, for the first time in Indianapolis 500 history, the race was run without a single engine problem. Since 2012, HPD has constructed turbocharged V6 engines for its IndyCar effort, winning four Indianapolis 500's, two manufacturers' championships and two drivers' championships.

During 1998, Honda considered returning to Formula One with their own team. The project was aborted after the death of its technical director, Harvey Postlethwaite. Honda instead came back as an official engine supplier to British American Racing (BAR), and briefly to Jordan Grand Prix. Together BAR and Honda achieved 15 podium finishes and second place in the 2004 constructors' championship. Honda bought a stake in the BAR team in 2004 before buying the team outright at the end of 2005, becoming a constructor for the first time since the 1960s. Honda won the 2006 Hungarian Grand Prix with driver Jenson Button. Honda announced in December 2008, that it would be exiting Formula One with immediate effect due to the 2008 global economic crisis.

Honda has competed in the British Touring Car Championship since 1995, though not always as a works team. They have achieved over 170 race victories, seven drivers' championships, five manufacturers' championships and seven teams' championships, ranking second with most wins in the series. Honda also won the World Touring Car Championship in 2013.

Honda made an official announcement on 16 May 2013 that it planned to re-enter into Formula One in 2015 as an engine supplier to McLaren. On 15 September 2017, after a winless campaign spanning three seasons and achieving a best finish of fifth place, McLaren and Honda announced their split, with the latter going on to sign a multi-year deal to supply Toro Rosso, the junior team of Red Bull Racing. After a fairly successful season with Toro Rosso, Honda made a deal to also supply Red Bull Racing. Max Verstappen scored Honda's first win of the V6 turbo-hybrid era at the Austrian Grand Prix.

Honda Racing Corporation (HRC) was formed in 1982. The company combines participation in motorcycle races throughout the world with the development of high potential racing machines. Its racing activities are an important source for the creation of leading edge technologies used in the development of Honda motorcycles. HRC also contributes to the advancement of motorcycle sports through a range of activities that include sales of production racing motorcycles, support for satellite teams, and rider education programs.

Soichiro Honda, being a race driver himself, could not stay out of international motorsport. In 1959, Honda entered five motorcycles into the Isle of Man TT race, the most prestigious motorcycle race in the world. While always having powerful engines, it took until 1961 for Honda to tune their chassis well enough to allow Mike Hailwood to claim their first Grand Prix victories in the 125 and 250 cc classes. Hailwood would later pick up their first Senior TT wins in 1966 and 1967. Honda's race bikes were known for their "sleek & stylish design" and exotic engine configurations, such as the 5-cylinder, 22,000 rpm, 125 cc bike and their 6-cylinder 250 cc and 297 cc bikes.

In 1979, Honda returned to Grand Prix motorcycle racing with the monocoque-framed, four-stroke NR500. The FIM rules limited engines to four cylinders, so the NR500 had non-circular, 'race-track', cylinders, each with 8 valves and two connecting rods, in order to provide sufficient valve area to compete with the dominant two-stroke racers. Unfortunately, it seemed Honda tried to accomplish too much at one time and the experiment failed. For the 1982 season, Honda debuted their first two-stroke race bike, the NS500 and in , Honda won their first 500 cc Grand Prix World Championship with Freddie Spencer. Since then, Honda has become a dominant marque in motorcycle Grand Prix racing, winning a plethora of top level titles with riders such as Mick Doohan and Valentino Rossi. Honda also head the number of wins at the Isle of Man TT having notched up 227 victories in the solo classes and Sidecar TT, including Ian Hutchinson's clean sweep at the 2010 races.

The outright lap record on the Snaefell Mountain Course was held by Honda, set at the 2015 TT by John McGuinness at an average speed of on a Honda CBR1000RR, bettered the next year by Michael Dunlop on a BMW S1000RR at .

In the Motocross World Championship, Honda has claimed six world championships. In the World Enduro Championship, Honda has captured eight titles, most recently with Stefan Merriman in 2003 and with Mika Ahola from 2007 to 2010. In motorcycle trials, Honda has claimed three world championships with Belgian rider Eddy Lejeune.

The Honda Civic GX was for a long time the only purpose-built natural gas vehicle (NGV) commercially available in some parts of the U.S. The Honda Civic GX first appeared in 1998 as a factory-modified Civic LX that had been designed to run exclusively on compressed natural gas. The car looks and drives just like a contemporary Honda Civic LX, but does not run on gasoline. In 2001, the Civic GX was rated the cleanest-burning internal combustion engine in the world by the U.S. Environmental Protection Agency (EPA).

First leased to the City of Los Angeles, in 2005, Honda started offering the GX directly to the public through factory trained dealers certified to service the GX. Before that, only fleets were eligible to purchase a new Civic GX. In 2006, the Civic GX was released in New York, making it the second state where the consumer is able to buy the car.

In June 2015, Honda announced its decision to phase out the commercialization of natural-gas powered vehicles to focus on the development of a new generation of electrified vehicles such as hybrids, plug-in electric cars and hydrogen-powered fuel cell vehicles. Since 2008, Honda has sold about 16,000 natural-gas vehicles, mainly to taxi and commercial fleets.

Honda's Brazilian subsidiary launched flexible-fuel versions for the Honda Civic and Honda Fit in late 2006. As other Brazilian flex-fuel vehicles, these models run on any blend of hydrous ethanol (E100) and E20-E25 gasoline. Initially, and in order to test the market preferences, the carmaker decided to produce a limited share of the vehicles with flex-fuel engines, 33 percent of the Civic production and 28 percent of the Fit models. Also, the sale price for the flex-fuel version was higher than the respective gasoline versions, around US$1,000 premium for the Civic, and US$650 for the Fit, despite the fact that all other flex-fuel vehicles sold in Brazil had the same tag price as their gasoline versions. In July 2009, Honda launched in the Brazilian market its third flexible-fuel car, the Honda City.

During the last two months of 2006, both flex-fuel models sold 2,427 cars against 8,546 gasoline-powered automobiles, jumping to 41,990 flex-fuel cars in 2007, and reaching 93,361 in 2008. Due to the success of the flex versions, by early 2009 a hundred percent of Honda's automobile production for the Brazilian market is now flexible-fuel, and only a small percentage of gasoline version is produced in Brazil for exports.

In March 2009, Honda launched in the Brazilian market the first flex-fuel motorcycle in the world. Produced by its Brazilian subsidiary Moto Honda da Amazônia, the CG 150 Titan Mix is sold for around US$2,700.

In late 1999, Honda launched the first commercial hybrid electric car sold in the U.S. market, the Honda Insight, just one month before the introduction of the Toyota Prius, and initially sold for US$20,000. The first-generation Insight was produced from 2000 to 2006 and had a fuel economy of for the EPA's highway rating, the most fuel-efficient mass-produced car at the time. Total global sales for the Insight amounted to only around 18,000 vehicles. Cumulative global sales reached 100,000 hybrids in 2005 and 200,000 in 2007.

Honda introduced the second-generation Insight in Japan in February 2009, and released it in other markets through 2009 and in the U.S. market in April 2009. At $19,800 as a five-door hatchback it will be the least expensive hybrid available in the U.S.
Since 2002, Honda has also been selling the Honda Civic Hybrid (2003 model) in the U.S. market. It was followed by the Honda Accord Hybrid, offered in model years 2005 through 2007. Sales of the Honda CR-Z began in Japan in February 2010, becoming Honda's third hybrid electric car in the market. , Honda was producing around 200,000 hybrids a year in Japan.

Sales of the Fit Hybrid began in Japan in October 2010, at the time, the lowest price for a gasoline-hybrid electric vehicle sold in the country. The European version, called Honda Jazz Hybrid, was released in early 2011. During 2011 Honda launched three hybrid models available only in Japan, the Fit Shuttle Hybrid, Freed Hybrid and Freed Spike Hybrid.

Honda's cumulative global hybrid sales passed the 1 million unit milestone at the end of September 2012, 12 years and 11 months after sales of the first generation Insight began in Japan November 1999. A total of 187,851 hybrids were sold worldwide in 2013, and 158,696 hybrids during the first six months of 2014. , Honda has sold more than 1.35 million hybrids worldwide.

In Takanezawa, Japan, on 16 June 2008, Honda Motors produced the first assembly-line FCX Clarity, a hybrid hydrogen fuel cell vehicle. More efficient than a gas-electric hybrid vehicle, the FCX Clarity combines hydrogen and oxygen from ordinary air to generate electricity for an electric motor. In July 2014 Honda announced the end of production of the Honda FCX Clarity for the 2015 model.

The vehicle itself does not emit any pollutants and its only by products are heat and water. The FCX Clarity also has an advantage over gas-electric hybrids in that it does not use an internal combustion engine to propel itself. Like a gas-electric hybrid, it uses a lithium ion battery to assist the fuel cell during acceleration and capture energy through regenerative braking, thus improving fuel efficiency. The lack of hydrogen filling stations throughout developed countries will keep production volumes low. Honda will release the vehicle in groups of 150. California is the only U.S. market with infrastructure for fueling such a vehicle, though the number of stations is still limited. Building more stations is expensive, as the California Air Resources Board (CARB) granted $6.8 million for four H2 fueling stations, costing US$1.7 million each.

Honda views hydrogen fuel cell vehicles as the long term replacement of piston cars, not battery cars.

The all-electric Honda EV Plus was introduced in 1997 as a result of CARB's zero-emissions vehicle mandate and was available only for leasing in California. The EV plus was the first battery electric vehicle from a major automaker with non-lead–acid batteries The EV Plus had an all-electric range of . Around 276 units were sold in the U.S. and production ended in 1999.

The all-electric Honda Fit EV was introduced in 2012 and has a range of . The all-electric car was launched in the U.S. to retail customers in July 2012 with initial availability limited to California and Oregon. Production is limited to only 1,100 units over the first three years. A total of 1,007 units have been leased in the U.S. through September 2014. The Fit EV was released in Japan through leasing to local government and corporate customers in August 2012. Availability in the Japanese market is limited to 200 units during its first two years. In July 2014 Honda announced the end of production of the Fit EV for the 2015 model.

The Honda Accord Plug-in Hybrid was introduced in 2013 and has an all-electric range of Sales began in the U.S. in January 2013 and the plug-in hybrid is available only in California and New York. A total of 835 units have been sold in the U.S. through September 2014. The Accord PHEV was introduced in Japan in June 2013 and is available only for leasing, primarily to corporations and government agencies.

Starting in 1978, Honda in Japan decided to diversify its sales distribution channels and created Honda Verno, which sold established products with a higher content of standard equipment and a more sporting nature. The establishment of "Honda Verno" coincided with its new sports compact, the Honda Prelude. Later, the Honda Vigor, Honda Ballade, and Honda Quint were added to "Honda Verno" stores. This approach was implemented due to efforts in place by rival Japanese automakers Toyota and Nissan.

As sales progressed, Honda created two more sales channels, called Honda Clio in 1984, and Honda Primo in 1985. The "Honda Clio" chain sold products that were traditionally associated with Honda dealerships before 1978, like the Honda Accord, and "Honda Primo" sold the Honda Civic, kei cars such as the Honda Today, superminis like the Honda Capa, along with other Honda products, such as farm equipment, lawn mowers, portable generators, and marine equipment, plus motorcycles and scooters like the Honda Super Cub. A styling tradition was established when "Honda Primo" and "Clio" began operations in that all "Verno" products had the rear license plate installed in the rear bumper, while "Primo" and "Clio" products had the rear license plate installed on the trunk lid or rear door for minivans.

As time progressed and sales began to diminish partly due to the collapse of the Japanese "bubble economy", "supermini" and "kei" vehicles that were specific to "Honda Primo" were "badge engineered" and sold at the other two sales channels, thereby providing smaller vehicles that sold better at both "Honda Verno" and "Honda Clio" locations. As of March 2006, the three sales chains were discontinued, with the establishment of "Honda Cars" dealerships. While the network was disbanded, some Japanese Honda dealerships still use the network names, offering all Japanese market Honda cars at all locations.

Honda sells genuine accessories through a separate retail chain called "" for both their motorcycle, scooter and automobile products. In cooperation with corporate group partner Pioneer, Honda sells an aftermarket line of audio and in-car navigation equipment that can be installed in any vehicle under the brand name , which is available at Honda Access locations as well as Japanese auto parts retailers, such as Autobacs. Buyers of used vehicles are directed to a specific Honda retail chain that sells only used vehicles called "."

In the spring of 2012, Honda in Japan introduced "Honda Cars Small Store" (Japanese) which is devoted to compact cars like the Honda Fit, and "kei" vehicles like the Honda N-One and Honda S660 roadster.
In 2003, Honda released its "Cog" advertisement in the UK and on the Internet. To make the ad, the engineers at Honda constructed a Rube Goldberg Machine made entirely out of car parts from a Honda Accord Touring. To the chagrin of the engineers at Honda, all the parts were taken from two of only six hand-assembled pre-production models of the Accord. The advertisement depicted a single cog which sets off a chain of events that ends with the Honda Accord moving and Garrison Keillor speaking the tagline, "Isn't it nice when things just... work?" It took 606 takes to get it perfect.

In 2004, they produced the "Grrr" advert, usually immediately followed by a shortened version of the 2005 "Impossible Dream" advert. In December 2005, Honda released "The Impossible Dream" a two-minute panoramic advertisement filmed in New Zealand, Japan and Argentina which illustrates the founder's dream to build performance vehicles. While singing the song "Impossible Dream", a man reaches for his racing helmet, leaves his trailer on a minibike, then rides a succession of vintage Honda vehicles: a motorcycle, then a car, then a powerboat, then goes over a waterfall only to reappear piloting a hot air balloon, with Garrison Keillor saying "I couldn't have put it better myself" as the song ends. The song is from the 1960s musical "Man of La Mancha", sung by Andy Williams.

In 2006, Honda released its "Choir" advertisement, for the UK and the internet. This had a 60-person choir who sang the car noises as film of the Honda Civic are shown.

In the mid to late 2000s in the United States, during model close-out sales for the current year before the start of the new model year, Honda's advertising has had an animated character known simply as Mr. Opportunity, voiced by Rob Paulsen. The casual looking man talked about various deals offered by Honda and ended with the phrase "I'm Mr. Opportunity, and I'm knockin'", followed by him "knocking" on the television screen or "thumping" the speaker at the end of radio ads. In addition, commercials for Honda's international hatchback, the Jazz, are parodies of well-known pop culture images such as Tetris and Thomas The Tank Engine.

In late 2006, Honda released an ad with ASIMO exploring a museum, looking at the exhibits with almost childlike wonderment (spreading out its arms in the aerospace exhibit, waving hello to an astronaut suit that resembles him, etc.), while Garrison Keillor ruminates on progress. It concludes with the tagline: "More forwards please". Honda also sponsored ITV's coverage of Formula One in the UK for 2007. However they had announced that they would not continue in 2008 due to the sponsorship price requested by ITV being too high.

In May 2007, focuses on their strengths in racing and the use of the Red H badge – a symbol of what is termed as "Hondamentalism". The campaign highlights the lengths that Honda engineers go to in order to get the most out of an engine, whether it is for bikes, cars, powerboats – even lawnmowers. Honda released its Hondamentalism campaign. In the TV spot, Garrison Keillor says, "An engineer once said to build something great is like swimming in honey", while Honda engineers in white suits walk and run towards a great light, battling strong winds and flying debris, holding on to anything that will keep them from being blown away. Finally one of the engineers walks towards a red light, his hand outstretched. A web address is shown for the Hondamentalism website. The digital campaign aims to show how visitors to the site share many of the Hondamentalist characteristics.

At the beginning of 2008, Honda released – the "Problem Playground". The advert outlines Honda's environmental responsibility, demonstrating a hybrid engine, more efficient solar panels and the FCX Clarity, a hydrogen-powered car. The 90-second advert has large-scale puzzles, involving Rubik's Cubes, large shapes and a 3-dimensional puzzle. On 29 May 2008, Honda, in partnership with Channel 4, broadcast a live advertisement. It showed skydivers jumping from an aeroplane over Spain and forming the letters H, O, N, D and A in mid-air. This live advertisement is generally agreed to be the first of its kind on British television. The advert lasted three minutes.

In 2009, American Honda released the "Dream the Impossible" documentary series, a collection of 5- to 8-minute web vignettes that focus on the core philosophies of Honda. Current short films include "Failure: The Secret to Success", "Kick Out the Ladder" and "Mobility 2088". They have Honda employees as well as Danica Patrick, Christopher Guest, Ben Bova, Chee Pearlman, Joe Johnston and Orson Scott Card. The film series plays at dreams.honda.com. In the UK, national television ads feature voice-overs from American radio host Garrison Keillor, while in the US the voice of Honda commercials is actor and director Fred Savage.

In the North American market, Honda starts all of their commercials with a two-tone jingle since the mid-2010s.

The late F1 driver Ayrton Senna stated that Honda probably played the most significant role in his three world championships. He had immense respect for founder, Soichiro Honda, and had a good relationship with Nobuhiko Kawamoto, the chairman of Honda at that time. Senna once called Honda "the greatest company in the world".

As part of its marketing campaign, Honda is an official partner and sponsor of the National Hockey League, the Anaheim Ducks of the NHL, and the arena named after it: Honda Center. Honda also sponsors The Honda Classic golf tournament and is a sponsor of Major League Soccer. The "Honda Player of the Year" award is presented in United States soccer. The "Honda Sports Award" is given to the best female athlete in each of twelve college sports in the United States. One of the twelve Honda Sports Award winners is chosen to receive the Honda-Broderick Cup, as "Collegiate Woman Athlete of the Year."

Honda sponsored La Liga club Valencia CF starting from 2014–15 season.

Honda has been a presenting sponsor of the Los Angeles Marathon since 2010 in a three-year sponsorship deal, with winners of the LA Marathon receiving a free Honda Accord. Since 1989, the Honda Campus All-Star Challenge has been a quizbowl tournament for Historically black colleges and universities.

2010 Chinese labor strike happened in Guangqi Honda, Dongfeng Honda.





</doc>
<doc id="13730" url="https://en.wikipedia.org/wiki?curid=13730" title="Handball">
Handball

Handball (also known as team handball, European handball or Olympic handball) is a team sport in which two teams of seven players each (six outcourt players and a goalkeeper) pass a ball using their hands with the aim of throwing it into the goal of the other team. A standard match consists of two periods of 30 minutes, and the team that scores more goals wins.

Modern handball is played on a court of , with a goal in the middle of each end. The goals are surrounded by a zone where only the defending goalkeeper is allowed; goals must be scored by throwing the ball from outside the zone or while "diving" into it. The sport is usually played indoors, but outdoor variants exist in the forms of field handball and Czech handball (which were more common in the past) and beach handball. The game is fast and high-scoring: professional teams now typically score between 20 and 35 goals each, though lower scores were not uncommon until a few decades ago. Body contact is permitted, the defenders trying to stop the attackers from approaching the goal. No protective equipment is mandated, but players may wear soft protective bands, pads and mouth guards.

The game was codified at the end of the 19th century in Denmark. The modern set of rules was published in 1917 in Germany, and had several revisions since. The first international games were played under these rules for men in 1925 and for women in 1930. Men's handball was first played at the 1936 Summer Olympics in Berlin as outdoors, and the next time at the 1972 Summer Olympics in Munich as indoors, and has been an Olympic sport since. Women's team handball was added at the 1976 Summer Olympics.

The International Handball Federation was formed in 1946 and, , has 197 member federations. The sport is most popular in Europe, and European countries have won all medals but one in the men's world championships since 1938. In the women's world championships, only two non-European countries have won the title: South Korea and Brazil. The game also enjoys popularity in East Asia, North Africa and parts of South America.

There is evidence of ancient Roman women playing a version of handball called "expulsim ludere". There are records of handball-like games in medieval France, and among the Inuit in Greenland, in the Middle Ages. By the 19th century, there existed similar games of "håndbold" from Denmark, "házená" in the Czech Republic, "handbol" in Ukraine, and "torball" in Germany.

The team handball game of today was codified at the end of the 19th century in northern Europe: primarily in Denmark, Germany, Norway and Sweden. The first written set of team handball rules was published in 1906 by the Danish gym teacher, lieutenant and Olympic medalist Holger Nielsen from Ordrup grammar school, north of Copenhagen. The modern set of rules was published on 29 October 1917 by Max Heiser, Karl Schelenz, and Erich Konigh from Germany. After 1919 these rules were improved by Karl Schelenz. The first international games were played under these rules, between Germany and Belgium by men in 1925 and between Germany and Austria by women in 1930.

In 1926, the Congress of the International Amateur Athletics Federation nominated a committee to draw up international rules for field handball. The International Amateur Handball Federation was formed in 1928 and later the International Handball Federation was formed in 1946.

Men's field handball was played at the 1936 Summer Olympics in Berlin. During the next several decades, indoor handball flourished and evolved in the Scandinavian countries. The sport re-emerged onto the world stage as team handball for the 1972 Summer Olympics in Munich. Women's team handball was added at the 1976 Summer Olympics in Montreal. Due to its popularity in the region, the Eastern European countries that refined the event became the dominant force in the sport when it was reintroduced.

The International Handball Federation organised the men's world championship in 1938 and every four (sometimes three) years from World War II to 1995. Since the 1995 world championship in Iceland, the competition has been held every two years. The women's world championship has been held since 1957. The IHF also organizes women's and men's junior world championships. By July 2009, the IHF listed 166 member federations - approximately 795,000 teams and 19 million players.

The rules are laid out in the IHF's set of rules.

Two teams of seven players (six field players plus one goalkeeper) take the field and attempt to score points by putting the game ball into the opposing team's goal. In handling the ball, players are subject to the following restrictions:


Notable scoring opportunities can occur when attacking players jump into the goal area. For example, an attacking player may catch a pass while launching inside the goal area, and then shoot or pass before touching the floor. "Doubling" occurs when a diving attacking player passes to another diving teammate.

Handball is played on a court , with a goal in the centre of each end. The goals are surrounded by a near-semicircular area, called the zone or the crease, defined by a line six meters from the goal. A dashed near-semicircular line nine metres from the goal marks the free-throw line. Each line on the court is part of the area it encompasses. This implies that the middle line belongs to both halves at the same time.

The goals are two meters high and three meters wide. They must be securely bolted either to the floor or the wall behind.

The goal posts and the crossbar must be made out of the same material (e.g., wood or aluminium) and feature a quadratic cross section with sides of . The three sides of the beams visible from the playing field must be painted alternatingly in two contrasting colors which both have to contrast against the background. The colors on both goals must be the same.

Each goal must feature a net. This must be fastened in such a way that a ball thrown into the goal does not leave or pass the goal under normal circumstances. If necessary, a second net may be clasped to the back of the net on the inside.

The goals are surrounded by the crease. This area is delineated by two quarter circles with a radius of six metres around the far corners of each goal post and a connecting line parallel to the goal line. Only the defending goalkeeper is allowed inside this zone. However, the court players may catch and touch the ball in the air within it as long as the player starts his jump outside the zone and releases the ball before he lands (landing inside the perimeter is allowed in this case as long as the ball has been released).

If a player without the ball contacts the ground inside the goal perimeter, or the line surrounding the perimeter, he must take the most direct path out of it. However, should a player cross the zone in an attempt to gain an advantage (e.g., better position) their team cedes the ball. Similarly, violation of the zone by a defending player is penalized only if they do so in order to gain an advantage in defending.

Outside of one long edge of the court to both sides of the middle line are the substitution areas for each team. Team officials, substitutes, and suspended players must wait within this area. A team's area is the same side as the goal the team is defending; during halftime, substitution areas are swapped. Any player entering or leaving the play must cross the substitution line which is part of the side line and extends from the middle line to the team's side.

 A standard match has two 30-minute halves with a 10- to 15-minute halftime break. At half-time, teams switch sides of the court as well as benches. For youths, the length of the halves is reduced—25 minutes at ages 12 to 15, and 20 minutes at ages 8 to 11; though national federations of some countries may differ in their implementation from the official guidelines.

If a decision must be reached in a particular match (e.g., in a tournament) and it ends in a draw after regular time, there are at maximum two overtimes, each consisting of two straight 5-minute periods with a one-minute break in between. Should these not decide the game either, the winning team is determined in a penalty shootout (best-of-five rounds; if still tied, extra rounds are added until one team wins).

The referees may call "timeout" according to their sole discretion; typical reasons are injuries, suspensions, or court cleaning. Penalty throws should trigger a timeout only for lengthy delays, such as a change of the goalkeeper.

Since 2012, teams can call 3 "team timeouts" per game (up to two per half), which last one minute each. This right may only be invoked by the team in possession of the ball. Team representatives must show a green card marked with a black "T" on the timekeeper's desk. The timekeeper then immediately interrupts the game by sounding an acoustic signal and stops the time. Before 2012, teams were allowed only one timeout per half. For the purpose of calling timeouts, overtime and shootouts are extensions of the second half.

A handball match is adjudicated by two equal referees. Some national bodies allow games with only a single referee in special cases like illness on short notice. Should the referees disagree on any occasion, a decision is made on mutual agreement during a short timeout; or, in case of punishments, the more severe of the two comes into effect. The referees are obliged to make their decisions "on the basis of their observations of facts". Their judgements are final and can be appealed against only if not in compliance with the rules.

The referees position themselves in such a way that the team players are confined between them. They stand diagonally aligned so that each can observe one side line. Depending on their positions, one is called "field referee" and the other "goal referee". These positions automatically switch on ball turnover. They physically exchange their positions approximately every 10 minutes (long exchange), and change sides every five minutes (short exchange).

The IHF defines 18 hand signals for quick visual communication with players and officials. The signal for warning is accompanied by a yellow card. A disqualification for the game is indicated by a red card, followed by a blue card if the disqualification will be accompanied by a report. The referees also use whistle blows to indicate infractions or to restart the play.

The referees are supported by a "scorekeeper" and a "timekeeper" who attend to formal things such as keeping track of goals and suspensions, or starting and stopping the clock, respectively. They also keep an eye on the benches and notify the referees on substitution errors. Their desk is located between the two substitution areas.

Each team consists of seven players on court and seven substitute players on the bench. One player on the court must be the designated goalkeeper, differing in his clothing from the rest of the field players. Substitution of players can be done in any number and at any time during game play. An exchange takes place over the substitution line. A prior notification of the referees is not necessary.

Some national bodies, such as the Deutsche Handball Bund (DHB, "German Handball Federation"), allow substitution in junior teams only when in ball possession or during timeouts. This restriction is intended to prevent early specialization of players to offence or defence.

Field players are allowed to touch the ball with any part of their bodies above and including the knee. As in several other team sports, a distinction is made between catching and dribbling. A player who is in possession of the ball may stand stationary for only three seconds, and may take only three steps. They must then either shoot, pass, or dribble the ball. Taking more than three steps at any time is considered travelling, and results in a turnover. A player may dribble as many times as they want (though, since passing is faster, it is the preferred method of attack), as long as during each dribble the hand contacts only the top of the ball. Therefore, carrying is completely prohibited, and results in a turnover. After the dribble is picked up, the player has the right to another three seconds or three steps. The ball must then be passed or shot, as further holding or dribbling will result in a "double dribble" turnover and a free throw for the other team. Other offensive infractions that result in a turnover include charging and setting an illegal screen. Carrying the ball into the six-meter zone results either in ball possession by the goalkeeper (by attacker) or turnover (by defender).

Only the goalkeepers are allowed to move freely within the goal perimeter, although they may not cross the goal perimeter line while carrying or dribbling the ball. Within the zone, they are allowed to touch the ball with all parts of their bodies, including their feet, with a defensive aim (for other actions, they are subject to the same restrictions as the field players). The goalkeepers may participate in the normal play of their teammates. They may be substituted by a regular field player if their team elects to use this scheme in order to outnumber the defending players. Earlier, this field player become the designated goalkeeper on the court; and had to wear some vest or bib to be identified as such. That shirt had to be equal in colour and form to the goalkeeper's shirt, to avoid confusion. A rule change meant to make the game more offensive now allows any player to substitute with the goalkeeper. The new rule resembles the one used in ice hockey. This rule was first used in the women's world championship in December 2015 and has since been used by the men's European championship in January 2016 and by both genders in the Olympic tournament in Rio in 2016.

If either goalkeeper deflects the ball over the outer goal line, their team stays in possession of the ball, in contrast to other sports like football. The goalkeeper resumes the play with a throw from within the zone ("goalkeeper throw"). Passing to one's own goalkeeper results in a turnover. In a penalty shot, throwing the ball against the head of a goalkeeper who is not moving risks a direct disqualification ("red card").

Outside of own D-zone, the goalkeeper is treated as a current field player, and has to follow field players' rules; holding or tackling an opponent player outside the area risks a direct disqualification. The goalkeeper may not return to the area with the ball.

Each team is allowed to have a maximum of four team officials seated on the benches. An official is anybody who is neither player nor substitute. One official must be the designated representative who is usually the team manager. Since 2012, representatives can call up to 3 team timeouts (up to twice per half), and may address the scorekeeper, timekeeper, and referees (before that, it was once per half); overtime and shootouts are considered extensions of the second half. Other officials typically include physicians or managers. Neither official is allowed to enter the playing court without the permission of the referees.

 The ball is spherical and must be made either of leather or a synthetic material. It is not allowed to have a shiny or slippery surface. As the ball is intended to be operated by a single hand, its official sizes vary depending on age and gender of the participating teams.
The referees may award a special throw to a team. This usually happens after certain events such as scored goals, off-court balls, turnovers and timeouts. All of these special throws require the thrower to obtain a certain position, and pose restrictions on the positions of all other players. Sometimes the execution must wait for a whistle blow by the referee.





Penalties are given to players, in progressive format, for fouls that require more punishment than just a free-throw. Actions directed mainly at the opponent and not the ball (such as reaching around, holding, pushing, tripping, and jumping into opponent) as well as contact from the side, from behind a player or impeding the opponent's counterattack are all considered illegal and are subject to penalty. Any infraction that prevents a clear scoring opportunity will result in a seven-meter penalty shot.

Typically the referee will give a warning yellow card for an illegal action; but, if the contact was particularly dangerous, like striking the opponent in the head, neck or throat, the referee can forego the warning for an immediate two-minute suspension. Players are warned once before given a yellow card; they risk being red-carded if they draw three yellows.

A red card results in an ejection from the game and a two-minute penalty for the team. A player may receive a red card directly for particularly rough penalties. For instance, any contact from behind during a fast break is now being treated with a red card; as does any deliberate intent to injure opponents. A red-carded player has to leave the playing area completely. A player who is disqualified may be substituted with another player after the two-minute penalty is served. A coach or official can also be penalized progressively. Any coach or official who receives a two-minute suspension will have to pull out one of their players for two minutes; however, the player is not the one punished, and can be substituted in again, as the penalty consists of the team playing with one fewer player than the opposing team.

After referees award the ball to the opponents for whatever reason, the player currently in possession of the ball has to lay it down quickly, or risk a two-minute suspension. Also, gesticulating or verbally questioning the referee's order, as well as arguing with the officials' decisions, will normally risk a yellow card. If the suspended player protests further, does not walk straight off the field to the bench, or if the referee deems the tempo deliberately slow, that player risks a double yellow card. Illegal substitution (outside of the dedicated area, or if the replacement player enters too early) is prohibited; if they do, they risk a yellow card.

Players are typically referred to by the positions they are playing. The positions are always denoted from the view of the respective goalkeeper, so that a defender on the right opposes an attacker on the left. However, not all of the following positions may be occupied depending on the formation or potential suspensions.


Sometimes, the offense uses formations with two pivot players.

There are many variations in defensive formations. Usually, they are described as "n:m" formations, where "n" is the number of players defending at the goal line and "m" the number of players defending more offensive. Exceptions are the 3:2:1 defense and n+m formation (e.g. 5+1), where m players defend some offensive player in man coverage (instead of the usual zone coverage).

Attacks are played with all field players on the side of the defenders. Depending on the speed of the attack, one distinguishes between three attack "waves" with a decreasing chance of success:



The third wave evolves into the normal offensive play when all defenders not only reach the zone, but gain their accustomed positions. Some teams then substitute specialised offence players. However, this implies that these players must play in the defence should the opposing team be able to switch quickly to offence. The latter is another benefit for fast playing teams.

If the attacking team does not make sufficient progress (eventually releasing a shot on goal), the referees can call passive play (since about 1995, the referee gives a passive warning some time before the actual call by holding one hand up in the air, signalling that the attacking team should release a shot soon), turning control over to the other team. A shot on goal or an infringement leading to a yellow card or two-minute penalty will mark the start of a new attack, causing the hand to be taken down; but a shot blocked by the defense or a normal free throw will not. If it were not for this rule, it would be easy for an attacking team to stall the game indefinitely, as it is difficult to intercept a pass without at the same time conceding dangerous openings towards the goal.

The usual formations of the defense are 6–0, when all the defense players line up between the and lines to form a wall; the 5–1, when one of the players cruises outside the perimeter, usually targeting the center forwards while the other 5 line up on the line; and the less common 4–2 when there are two such defenders out front. Very fast teams will also try a 3–3 formation which is close to a switching man-to-man style. The formations vary greatly from country to country, and reflect each country's style of play. 6–0 is sometimes known as "flat defense", and all other formations are usually called "offensive defense".

Handball teams are usually organised as clubs. On a national level, the clubs are associated in federations which organize matches in leagues and tournaments.

The International Handball Federation (IHF) is the administrative and controlling body for international handball. Handball is an Olympic sport played during the Summer Olympics.

The IHF organizes world championships, held in odd-numbered years, with separate competitions for men and women.
The IHF World Men's Handball Championship 2019 title holders are Denmark. The IHF World Women's Handball Championship 2017 title holders are France.

The IHF is composed of five continental federations: Asian Handball Federation, African Handball Confederation, Pan-American Team Handball Federation, European Handball Federation and Oceania Handball Federation. These federations organize continental championships held every other second year. Handball is played during the Pan American Games, All-Africa Games, and Asian Games. It is also played at the Mediterranean Games. In addition to continental competitions between national teams, the federations arrange international tournaments between club teams.




The current worldwide attendance record for seven-a-side handball was set on September 6, 2014, during a neutral venue German league game between HSV Hamburg and the Mannheim-based Rhein-Neckar Lions. The matchup drew 44,189 spectators to Commerzbank Arena in Frankfurt, exceeding the previous record of 36,651 set at Copenhagen's Parken Stadium during the 2011 Danish Cup final.

Handball events have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Handball commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the coin, the modern athlete directs the ball in his hands towards his target, while in the background the ancient athlete is just about to throw a ball, in a game known as cheirosphaira, in a representation taken from a black-figure pottery vase of the Archaic period.

The most recent commemorative coin featuring handball is the British 50 pence coin, part of the series of coins commemorating the London 2012 Olympic Games. 
Notes



</doc>
<doc id="13733" url="https://en.wikipedia.org/wiki?curid=13733" title="Hilbert's basis theorem">
Hilbert's basis theorem

In mathematics, specifically commutative algebra, Hilbert's basis theorem says that a polynomial ring over a Noetherian ring is Noetherian.

If formula_1 is a ring, let formula_2 denote the ring of polynomials in the indeterminate formula_3 over formula_1. Hilbert proved that if formula_1 is "not too large", in the sense that if formula_1 is Noetherian, the same must be true for formula_2. Formally,

Hilbert's Basis Theorem. If formula_1 is a Noetherian ring, then formula_2 is a Noetherian ring.

Corollary. If formula_1 is a Noetherian ring, then formula_11 is a Noetherian ring.

This can be translated into algebraic geometry as follows: every algebraic set over a field can be described as the set of common roots of finitely many polynomial equations. proved the theorem (for the special case of polynomial rings over a field) in the course of his proof of finite generation of rings of invariants.

Hilbert produced an innovative proof by contradiction using mathematical induction; his method does not give an algorithm to produce the finitely many basis polynomials for a given ideal: it only shows that they must exist. One can determine basis polynomials using the method of Gröbner bases.

Remark. We will give two proofs, in both only the "left" case is considered; the proof for the right case is similar.

Suppose formula_14 is a non-finitely generated left-ideal. Then by recursion (using the axiom of dependent choice) there is a sequence formula_15 of polynomials such that if formula_16 is the left ideal generated by formula_17 then formula_18 is of minimal degree. It is clear that formula_19 is a non-decreasing sequence of naturals. Let formula_20 be the leading coefficient of formula_21 and let formula_22 be the left ideal in formula_1 generated by formula_24. Since formula_1 is Noetherian the chain of ideals

must terminate. Thus formula_27 for some integer formula_28. So in particular,

Now consider

whose leading term is equal to that of formula_31; moreover, formula_32. However, formula_33, which means that formula_34 has degree less than formula_31, contradicting the minimality.

Let formula_14 be a left-ideal. Let formula_37 be the set of leading coefficients of members of formula_38. This is obviously a left-ideal over formula_1, and so is finitely generated by the leading coefficients of finitely many members of formula_38; say formula_41. Let formula_42 be the maximum of the set formula_43, and let formula_44 be the set of leading coefficients of members of formula_38, whose degree is formula_46. As before, the formula_44 are left-ideals over formula_1, and so are finitely generated by the leading coefficients of finitely many members of formula_38, say

with degrees formula_46. Now let formula_52 be the left-ideal generated by:

We have formula_54 and claim also formula_55. Suppose for the sake of contradiction this is not so. Then let formula_56 be of minimal degree, and denote its leading coefficient by formula_57.

Thus our claim holds, and formula_73 which is finitely generated.

Note that the only reason we had to split into two cases was to ensure that the powers of formula_3 multiplying the factors were non-negative in the constructions.

Let formula_1 be a Noetherian commutative ring. Hilbert's basis theorem has some immediate corollaries.


The Mizar project has completely formalized and automatically checked a proof of Hilbert's basis theorem in the HILBASIS file.



</doc>
<doc id="13734" url="https://en.wikipedia.org/wiki?curid=13734" title="Heterocyclic compound">
Heterocyclic compound

A heterocyclic compound or ring structure is a cyclic compound that has atoms of at least two different elements as members of its ring(s). Heterocyclic chemistry is the branch of organic chemistry dealing with the synthesis, properties, and applications of these heterocycles.

Examples of heterocyclic compounds include all of the nucleic acids, the majority of drugs, most biomass (cellulose and related materials), and many natural and synthetic dyes. 59% of US FDA-approved drugs contain nitrogen heterocycles.

Although heterocyclic chemical compounds may be inorganic compounds or organic compounds, most contain at least one carbon. While atoms that are neither carbon nor hydrogen are normally referred to in organic chemistry as heteroatoms, this is usually in comparison to the all-carbon backbone. But this does not prevent a compound such as borazine (which has no carbon atoms) from being labelled "heterocyclic". IUPAC recommends the Hantzsch-Widman nomenclature for naming heterocyclic compounds.

Heterocyclic compounds can be usefully classified based on their electronic structure. The saturated heterocycles behave like the acyclic derivatives. Thus, piperidine and tetrahydrofuran are conventional amines and ethers, with modified steric profiles. Therefore, the study of heterocyclic chemistry focuses especially on unsaturated derivatives, and the preponderance of work and applications involves unstrained 5- and 6-membered rings. Included are pyridine, thiophene, pyrrole, and furan. Another large class of heterocycles are fused to benzene rings, which for pyridine, thiophene, pyrrole, and furan are quinoline, benzothiophene, indole, and benzofuran, respectively. Fusion of two benzene rings gives rise to a third large family of compounds, respectively the acridine, dibenzothiophene, carbazole, and dibenzofuran. The unsaturated rings can be classified according to the participation of the heteroatom in the conjugated system, pi system.

Heterocycles with three atoms in the ring are higher in energy and more reactive because of ring strain. Those containing one heteroatom are, in general, stable. Those with two heteroatoms are more likely to occur as reactive intermediates. The C-X-C bond angles (where X is a heteroatom) in oxiranes and aziridines are very close to 60° and the peripheral H-C-H bond angles are near to 180°.

The 5-membered ring compounds containing "two" heteroatoms, at least one of which is nitrogen, are collectively called the azoles. Thiazoles and isothiazoles contain a sulfur and a nitrogen atom in the ring. Dithiolanes have two sulfur atoms.

A large group of 5-membered ring compounds with "three" or more heteroatoms also exists. One example is the class of dithiazoles, which contain two sulfur atoms and one nitrogen atoms.

The hypothetical compound with six nitrogen heteroatoms would be hexazine.

Borazine is a six-membered ring with three nitrogen heteroatoms and three boron heteroatom.

With 7-membered rings, the heteroatom must be able to provide an empty pi orbital (e.g., boron) for "normal" aromatic stabilization to be available; otherwise, homoaromaticity may be possible. Compounds with one heteroatom include:

Those with two heteroatoms include:

Heterocyclic rings systems that are formally derived by fusion with other rings, either carbocyclic or heterocyclic, have a variety of common and systematic names. For example, with the benzo-fused unsaturated nitrogen heterocycles, pyrrole provides indole or isoindole depending on the orientation. The pyridine analog is quinoline or isoquinoline. For azepine, benzazepine is the preferred name. Likewise, the compounds with two benzene rings fused to the central heterocycle are carbazole, acridine, and dibenzoazepine. Thienothiophene are the fusion of two thiophene rings. Phosphaphenalenes are a tricyclic phosphorus-containing heterocyclic system derived from the carbocycle phenalene.

The history of heterocyclic chemistry began in the 1800s, in step with the development of organic chemistry. Some noteworthy developments:
1818: Brugnatelli isolates alloxan from uric acid
1832: Dobereiner produces furfural (a furan) by treating starch with sulfuric acid
1834: Runge obtains pyrrole ("fiery oil") by dry distillation of bones
1906: Friedlander synthesizes indigo dye, allowing synthetic chemistry to displace a large agricultural industry
1936: Treibs isolates chlorophyl derivatives from crude oil, explaining the biological origin of petroleum.
1951: Chargaff's rules are described, highlighting the role of heterocyclic compounds (purines and pyrimidines) in the genetic code.

Heterocyclic compounds are pervasive in many areas of life sciences and technology. Many drugs are heterocyclic compounds.



</doc>
<doc id="13743" url="https://en.wikipedia.org/wiki?curid=13743" title="Harry Connick Jr.">
Harry Connick Jr.

Joseph Harry Fowler Connick Jr. (born September 11, 1967) is an American singer, composer, actor, and television host. He has sold over 28million albums worldwide. Connick is ranked among the top60 best-selling male artists in the United States by the Recording Industry Association of America, with 16million in certified sales. He has had seven top20 US albums, and ten number-one US jazz albums, earning more number-one albums than any other artist in US jazz chart history.

Connick's best-selling album in the United States is his Christmas album "When My Heart Finds Christmas" (1993). His highest-charting album is his release "Only You" (2004), which reached No.5 in the US and No.6 in Britain. He has won three Grammy Awards and two Emmy Awards. He played Debra Messing's character Grace Adler’s husband, Leo Markus, on the NBC sitcom "Will & Grace" from 2002 to 2006.

Connick began his acting career as a tail gunner in the World War II film "Memphis Belle" (1990). He played a serial killer in "Copycat" (1995), before being cast as a fighter pilot in the blockbuster "Independence Day" (1996). Connick's first role as a leading man was in "Hope Floats" (1998) with Sandra Bullock. His first thriller film since "Copycat" came in the film "Basic" (2003) with John Travolta. Additionally, he played a violent ex-husband in "Bug", before two romantic comedies, "P.S. I Love You" (2007), and the leading man in "New in Town" (2009) with Renée Zellweger. In 2011, he appeared in the family film "Dolphin Tale" as Dr. Clay Haskett and in its 2014 sequel.

Harry Connick Jr. was born and raised in New Orleans, Louisiana. His mother, Anita Frances Livingston (née Levy) was a lawyer and judge in New Orleans. His father, Joseph Harry Fowler Connick Sr., was the district attorney of Orleans Parish from 1973 to 2003. His parents also owned a record store. Connick's father is a Catholic of Irish ancestry. Connick's mother, who died from ovarian cancer, was Jewish (her parents had emigrated from Minsk and Vienna). Connick and his sister, Suzanna, were raised in the Lakeview neighborhood of New Orleans.

Connick's musical talents came to the fore when he started learning keyboards at age three, playing publicly at age five, and recording with a local jazz band at ten. When he was nine years old, Connick performed the Piano Concerto No. 3 Opus 37 of Beethoven with the New Orleans Symphony Orchestra (now the Louisiana Philharmonic). Later he played a duet with Eubie Blake at the Royal Orleans Esplanade Lounge in New Orleans. The song was "I'm Just Wild About Harry". This was recorded for a Japanese documentary called "Jazz Around the World". The clip was also shown in a Bravo special, called "Worlds of Harry Connick, Junior." in 1999. His musical talents were developed at the New Orleans Center for Creative Arts and under the tutelage of Ellis Marsalis Jr. and James Booker.

Connick attended Jesuit High School, Isidore Newman School, Lakeview School, and the New Orleans Center for Creative Arts, all in New Orleans. Following an unsuccessful attempt to study jazz academically, and having given recitals in the classical and jazz piano programs at Loyola University, Connick moved to the 92nd Street YMHA in New York City to study at Hunter College and the Manhattan School of Music. There he met Columbia Records executive, Dr. George Butler, who persuaded him to sign with Columbia. His first record, "Harry Connick Jr.", was a mainly instrumental album of standards. He soon acquired a reputation in jazz because of extended stays at high-profile New York venues. His next album, "20", featured his vocals and added to this reputation.

With Connick's reputation growing, director Rob Reiner asked him to provide a soundtrack for his romantic comedy, "When Harry Met Sally..." (1989), starring Meg Ryan and Billy Crystal. The soundtrack consisted of several standards, including "It Had to Be You", "Let's Call the Whole Thing Off" and "Don't Get Around Much Anymore", and achieved double-platinum status in the United States. He won his first Grammy Award for Best Jazz Male Vocal Performance for his work on the soundtrack.

Connick made his screen debut in "Memphis Belle" (1990), a fictional story about a B-17 Flying Fortress bomber crew in World War II. In that year he began a two-year world tour. In addition, he released two albums in July 1990: the instrumental jazz trio album "Lofty's Roach Souffle" and a big-band album of mostly original songs titled "We Are in Love", which also went double platinum. "We Are in Love" earned him his second consecutive Grammy for Best Jazz Male Vocal.

"Promise Me You'll Remember", his contribution to the "Godfather III" soundtrack, was nominated for both an Academy Award and a Golden Globe Award in 1991. In a year of recognition, he was also nominated for an Emmy Award for Best Performance in a Variety Special for his PBS special "Swingin' Out Live", which was also released as a video. In October 1991, he released his third consecutive multi-platinum album, "Blue Light, Red Light", on which he wrote and arranged the songs. Also in October 1991, he starred in "Little Man Tate", directed by Jodie Foster, playing the friend of a child prodigy who goes to college.

In November 1992, Connick released "25", a solo piano collection of standards that again went platinum. He also re-released the album "Eleven". Connick contributed "A Wink and a Smile" to the "Sleepless in Seattle" soundtrack, released in 1993. His multi-platinum album of holiday songs, "When My Heart Finds Christmas", was the best-selling Christmas album in 1993.

In 1994, Connick decided to branch out. He released "She", an album of New Orleans funk that also went platinum. In addition, he released a song called "(I Could Only) Whisper Your Name" for the soundtrack of "The Mask", starring Jim Carrey, which is his most successful single in the United States to date.

Connick took his funk music on a tour of the United Kingdom in 1994, an effort that did not please some of his fans, who were expecting a jazz crooner. Connick also went on a tour of the People's Republic of China in 1995, playing at the Shanghai Center Theatre. The performance was televised live in China for what became known as the Shanghai Gumbo special. In his third film "Copycat", Connick played a serial killer who terrorizes a psychiatrist (played by Sigourney Weaver). Released in 1995, "Copycat" also starred Holly Hunter and Sigourney Weaver. The following year, he released his second funk album, "Star Turtle", which did not sell as well as previous albums, although it did reach No. 38 on the charts. However, he appeared in the most successful movie of 1996, "Independence Day", with Will Smith and Jeff Goldblum.

For his 1997 release "To See You", Connick recorded original love songs, touring the United States and Europe with a full symphony orchestra backing him and his piano in each city. As part of his tour, he played at the Nobel Peace Prize Concert in Oslo, Norway, with his final concert of that tour in Paris being recorded for a Valentine's Day special on PBS in 1998. He also continued his film career, starring in "Excess Baggage" (1997) opposite Alicia Silverstone and Benicio del Toro.

In May 1998, he had his first leading role in director Forest Whitaker's "Hope Floats", with Sandra Bullock as his female lead. He released "Come By Me", his first album of big band music in eight years in 1999, and embarked on a world tour visiting the United States, Europe, Japan and Australia. In addition, he provided the voice of Dean McCoppin in the animated film "The Iron Giant".

Connick wrote the score for Susan Stroman's Broadway musical "Thou Shalt Not", based on Émile Zola's novel "Thérèse Raquin", in 2000; it premiered in 2001. His music and lyrics earned a Tony Award nomination. He was also the narrator of the film "My Dog Skip", released in that year.

In March 2001, Connick starred in a television production of "South Pacific" with Glenn Close, televised on the ABC network. He also starred in his twelfth movie, "Mickey", featuring a screenplay by John Grisham that same year. In October 2001, he again released two albums: "Songs I Heard", featuring big band re-workings of children's show themes, and "30", featuring Connick on piano with guest appearances by several other musical artists. "Songs I Heard" won Connick another Grammy for Best Traditional Pop Album and he toured performing songs from the album, holding matinees at which each parent had to be accompanied by a child.

In 2002, he received a for a "system and method for coordinating music display among players in an orchestra." Connick appeared as Grace Adler's boyfriend (and later husband) Leo Markus on the NBC sitcom "Will & Grace" from 2002 to 2006.

In July 2003, Connick released his first instrumental album in fifteen years, "Other Hours Connick on Piano Volume 1". It was released on Branford Marsalis' new label Marsalis Music and led to a short tour of nightclubs and small theaters. Connick appeared in the film "Basic". In October 2003, he released his second Christmas album, "Harry for the Holidays", which went gold and reached No. 12 on the "Billboard" 200 albums chart. He also had a television special on NBC featuring Whoopi Goldberg, Nathan Lane, Marc Anthony and Kim Burrell. "Only You", his seventeenth album for Columbia Records, was released in February 2004. A collection of 1950s and 1960s ballads, "Only You", went top ten on both sides of the Atlantic and was certified gold in the United States in March 2004. The "Only You" tour with big band went on in America, Australia and a short trip to Asia. "Harry for the Holidays" was certified platinum in November 2004. A music DVD "Harry Connick Jr."Only You" in Concert" was released in March 2004, after it had first aired as a "Great Performances" special on PBS. The special won him an Emmy Award for Outstanding Music Direction. The DVD received a Gold & Platinum Music VideoLong Form awards from the RIAA in November 2005.

An animated holiday special, "The Happy Elf", aired on NBC in December 2005, with Connick as the composer, the narrator, and one of the executive producers. Shortly after, it was released on DVD. The holiday special was based on his original song "The Happy Elf", from his 2003 album "Harry for the Holidays". Another album from Marsalis Music was recorded in 2005, "", a duo album with Harry Connick Jr. on piano together with Branford Marsalis on saxophone. A music DVD, "A Duo Occasion", was filmed at the Ottawa International Jazz Festival 2005 in Canada, and released in November 2005.

He appeared in another episode of NBC sitcom "Will & Grace" in November 2005, and appeared in an additional three episodes in 2006.

"Bug", a film directed by William Friedkin, is a psychological thriller filmed in 2005, starring Connick, Ashley Judd, and Michael Shannon. The film was released in 2007. He starred in the Broadway revival of "The Pajama Game", produced by the Roundabout Theater Company, along with Michael McKean and Kelli O'Hara, at the "American Airlines Theatre" in 2006. It ran from February 23 to June 17, 2006, including five benefit performances running from June 13 to 17. The "Pajama Game" cast recording was nominated for a Grammy, after being released as part of Connick's double disc album Harry on Broadway, Act I.

He hosted The Weather Channel's miniseries "100 Biggest Weather Moments" which aired in 2007. He was part of the documentary , released in November 2007. He sat in on piano on Bob French's 2007 album "Marsalis Music Honors Series: Bob French". He appeared in the film "P.S. I Love You", released in December 2007. A third album in the "Connick on Piano" series, "Chanson du Vieux Carré" was released in 2007, and Connick received two Grammy nominations for the track "Ash Wednesday", for the Grammy awards in 2008. "Chanson du Vieux Carré" was released simultaneously with the album "Oh, My NOLA". Connick toured North America and Europe in 2007, and toured Asia and Australia in 2008, as part of his My New Orleans Tour. Connick did the arrangements for, wrote a couple of songs, and sang a duet on Kelli O'Hara's album that was released in May 2008. He was also the featured singer at the Concert of Hope immediately preceding Pope Benedict XVI's Mass at Yankee Stadium in April 2008. He had the starring role of Dr. Dennis Slamon in the Lifetime television film "Living Proof" (2008). His third Christmas album, "What a Night!", was released in November 2008.

Harry has a vast knowledge of musical genres and vocalists, even Gospel music. One of his favorite Gospel artists is Stellar Award winner and Grammy nominated artist Kim Burrell of Houston, Texas. "And when Harry Connick Jr. assembled a symphony orchestra for Pope Benedict XVI’s appearance at Yankee Stadium in 2008, he wanted Burrell on vocals"

The film "New in Town" starring Connick and Renée Zellweger, began filming in January 2008, and was released in January 2009. Connick's album "Your Songs" was released on CD, September 22, 2009. In contrast to Connick's previous albums, this album is a collaboration with a record company producer, the multiple Grammy Award winning music executive Clive Davis.

Connick starred in the Broadway revival of "On a Clear Day You Can See Forever", which opened at the St. James Theatre in November 2011 in previews.

Connick appeared on May 4, 2010 episode of "American Idol" season 9, where he acted as a mentor for the top 5 finalists. He appeared again the next night on May 5 to perform "And I Love Her".

On January 6, 2012, NBC president Robert Greenblatt announced at the Television Critics Association winter press tour that Connick had been cast in a four-episode arc of NBC's long-running legal drama, "" as new Executive ADA, David Haden, a prosecutor who is assigned a case with Detective Olivia Benson (Mariska Hargitay).

On June 11, 2013, Connick released a new album of all original music titled "Every Man Should Know". Connick debuted the title track live on May 2, 2013 episode of "American Idol" and appeared on "The Ellen DeGeneres Show" the following week to discuss his new project. A 2013 US summer tour was announced in support of the album.

Connick returned to "American Idol" to mentor the top four of season 12. He performed "Every Man Should Know" on the results show the following night.

On September 3, 2013, the officials of "American Idol" officially announced that Connick would be a part of the judging panel for season 13 alongside former judge Jennifer Lopez and returning judge Keith Urban.

"Angels Sing", a family Christmas movie released in November 2013 by Lionsgate, afforded Connick an onscreen collaboration with fellow musician Willie Nelson. The two wrote a special song exclusively for the movie. Shot in Austin, Texas, "Angels Sing" features actor/musicians Connie Britton, Lyle Lovett, and Kris Kristofferson and is directed by Tim McCanlies, who previously worked with Connick in The Iron Giant.

A one-hour weekday daytime talk show both starring and named "Harry", debuted on September 12, 2016.

The following musicians have toured as the Harry Connick Jr. Big Band since its inception in 1990:

In January 2019, it was announced that Connick was hired by piano instruction software company Playground Sessions as a video instructor.

On October 25, 2019 he released a new album of Cole Porter compositions rearranged by Connick himself from Porter’s The Great American Songbook including “Anything Goes” and “You Do Something To Me.” After selecting the songs, and writing and orchestrating the arrangements, he assembled and conducted the orchestra which features his longtime touring band with additional horns and a full string section.

Along with his album, Connick announced his return to Broadway on September 16, 2019 with Harry Connick Jr. — A Celebration of Cole Porter, a multimedia celebration of the Cole Porter songbook. The production was conceived and directed by Connick himself with the addition of theatrical and film elements accompanied by a company of dancers and an onstage orchestra.

Connick, a New Orleans native, is a founder of the Krewe of Orpheus, a music-based New Orleans krewe, taking its name from Orpheus of classical mythology. The Krewe of Orpheus parades on St. Charles Avenue and Canal Street in New Orleans on Lundi Gras (Fat Monday)the day before Mardi Gras (Fat Tuesday).

On September 2, 2005, Connick helped to organize, and appeared in, the NBC-sponsored live telethon concert, "A Concert for Hurricane Relief", for relief in the wake of Hurricane Katrina. He spent several days touring the city to draw attention to the plight of citizens stranded at the Ernest N. Morial Convention Center and other places. At the concert he paired with host Matt Lauer, and entertainers including Tim McGraw, Faith Hill, Kanye West, Mike Myers, and John Goodman.

On September 6, 2005, Connick was made honorary chair of Habitat for Humanity's Operation Home Delivery, a long-term rebuilding plan for families who survived Hurricane Katrina in New Orleans and along the Gulf Coast. His actions in New Orleans earned him a Jefferson Award for Public Service.

Connick's album "Oh, My NOLA", and "" were released in 2007, with a following tour called the My New Orleans Tour.

Connick and Branford Marsalis devised an initiative to help restore New Orleans' musical heritage. Habitat for Humanity and New Orleans Area Habitat for Humanity, working with Connick and Marsalis announced December 6, 2005, plans for a Musicians' Village in New Orleans. The Musicians' Village includes Habitat-constructed homes, with an "Ellis Marsalis Center for Music", as the area's centerpiece. The Habitat-built homes provide musicians, and anyone else who qualifies, the opportunity to buy decent, affordable housing.

In 2012, Connick and Marsalis received the S. Roger Horchow Award for Greatest Public Service by a Private Citizen, an award given out annually by Jefferson Awards.

On April 16, 1994, Connick married former Victoria's Secret model Jill Goodacre, originally from Texas, at the St. Louis Cathedral, New Orleans. Jill is the daughter of sculptor Glenna Goodacre, originally from Lubbock, and now Santa Fe, New Mexico. The song "Jill", on the album "Blue Light, Red Light" (1991) is about her. They have three daughters: Georgia Tatum (born April 17, 1996), Sarah Kate (born September 12, 1997), and Charlotte (born June 26, 2002). The family resides in New Canaan, Connecticut and New Orleans, Louisiana. Connick is a practicing Roman Catholic. In 2011 Harry wrote Kate's debut song "A Lot Like Me". The song was released to celebrate the debut of American Girl's newest historical characters Cecile Rey and Marie Grace Gardner. "A Lot Like Me" is available on iTunes. The proceeds from "A Lot Like Me" went towards Ellis Marsalis Center for Music. In 2014, Cecile and Marie Grace were archived with Ruthie and Ivy to make room for the return of Samantha and BeForever.

Connick is a supporter of hometown NFL franchise New Orleans Saints. He was caught on camera at the Super Bowl XLIV, which the Saints won, in Miami by the television crew of "The Ellen DeGeneres Show" during the post-game celebrations. Ellen's mother Betty was on the sidelines watching the festivities when she spotted Connick in the stands sporting a Drew Brees jersey.

Connick was arrested by the Port Authority Police in December 1992 and charged with having a 9mm pistol in his possession at JFK International Airport. After spending a day in jail, he agreed to make a public-service television commercial warning against breaking gun laws. The court agreed to drop all charges if Connick stayed out of trouble for six months.





</doc>
<doc id="13744" url="https://en.wikipedia.org/wiki?curid=13744" title="List of humorists">
List of humorists

A humorist (American English) or humourist (British English) is an intellectual who uses humor in writing or public speaking. Humorists are distinct from comedians, who are show business entertainers whose business is to make an audience laugh, though it is possible for some persons to occupy both roles in the course of their careers.

Despite the fact that the Kennedy Center for the Performing Arts annually bestows a Mark Twain Prize for American Humor (usually on comedians) since 1998, this award does not by itself qualify the recipient as a humorist. only two recipients, Steve Martin and Neil Simon, are known as humorists, being humorous playwrights.

Notable humorists include:


</doc>
<doc id="13746" url="https://en.wikipedia.org/wiki?curid=13746" title="Hydrostatic shock">
Hydrostatic shock

Hydrostatic shock is the controversial concept that a penetrating projectile (such as a bullet) can produce a pressure wave that causes "remote neural damage", "subtle damage in neural tissues" and/or "rapid incapacitating effects" in living targets. It has also been suggested that pressure wave effects can cause indirect bone fractures at a distance from the projectile path, although it was later demonstrated that indirect bone fractures are caused by temporary cavity effects (strain placed on the bone by the radial tissue displacement produced by the temporary cavity formation).

Proponents of the concept argue that hydrostatic shock can produce remote neural damage and produce incapacitation more quickly than blood loss effects. In arguments about the differences in stopping power between calibers and between cartridge models, proponents of cartridges that are "light and fast" (such as the 9×19mm Parabellum) versus cartridges that are "slow and heavy" (such as the .45 ACP) often refer to this phenomenon.

Martin Fackler has argued that sonic pressure waves do not cause tissue disruption and that temporary cavity formation is the actual cause of tissue disruption mistakenly ascribed to sonic pressure waves. One review noted that strong opinion divided papers on whether the pressure wave contributes to wound injury. It ultimately concluded that no "conclusive evidence could be found for permanent pathological effects produced by the pressure wave".

In the scientific literature, the first discussion of pressure waves created when a bullet hits a living target is presented by E. Harvey Newton and his research group at Princeton University in 1947:

Frank Chamberlin, a World War II trauma surgeon and ballistics researcher, noted remote pressure wave effects. Col. Chamberlin described what he called "explosive effects" and "hydraulic reaction" of bullets in tissue. "...liquids are put in motion by ‘shock waves’ or hydraulic effects... with liquid filled tissues, the effects and destruction of tissues extend in all directions far beyond the wound axis". He avoided the ambiguous use of the term "shock" because it can refer to either a specific kind of pressure wave associated with explosions and supersonic projectiles or to a medical condition in the body.

Col. Chamberlin recognized that many theories have been advanced in wound ballistics. During World War II he commanded an 8,500-bed hospital center that treated over 67,000 patients during the fourteen months that he operated it. P.O. Ackley estimates that 85% of the patients were suffering from gunshot wounds. Col. Chamberlin spent many hours interviewing patients as to their reactions to bullet wounds. He conducted many live animal experiments after his tour of duty. On the subject of wound ballistics theories, he wrote:
Other World War II era scientists noted remote pressure wave effects in the peripheral nerves. There was support for the idea of remote neural effects of ballistic pressure waves in the medical and scientific communities, but the phrase "’hydrostatic shock’" and similar phrases including "shock" were used mainly by gunwriters (such as Jack O'Conner) and the small arms industry (such as Roy Weatherby, and Federal "Hydra-Shok.")

Dr. Martin Fackler, a Vietnam-era trauma surgeon, wound ballistics researcher, a Colonel in the U.S. Army and the head of the Wound Ballistics Laboratory for the U.S. Army's Medical Training Center, Letterman Institute, claimed that hydrostatic shock had been disproved and that the assertion that a pressure wave plays a role in injury or incapacitation is a myth. Others expressed similar views.

Dr. Fackler based his argument on the lithotriptor, a tool commonly used to break up kidney stones. The lithotriptor uses sonic pressure waves which are stronger than those caused by most handgun bullets, yet it produces no damage to soft tissues whatsoever. Hence, Fackler argued, ballistic pressure waves cannot damage tissue either.

Dr. Fackler claimed that a study of rifle bullet wounds in Vietnam (Wound Data and Munitions Effectiveness Team) found "no cases of bones being broken, or major vessels torn, that were not hit by the penetrating bullet. In only two cases, an organ that was not hit (but was within a few cm of the projectile path), suffered some disruption." Dr. Fackler cited a personal communication with R. F. Bellamy. However, Bellamy's published findings the following year estimated that 10% of fractures in the data set might be due to indirect injuries, and one specific case is described in detail (pp. 153–154). In addition, the published analysis documents five instances of abdominal wounding in cases where the bullet did not penetrate the abdominal cavity (pp. 149–152), a case of lung contusion resulting from a hit to the shoulder (pp. 146–149), and a case of indirect effects on the central nervous system (p. 155). Fackler's critics argue that Fackler's evidence does not contradict distant injuries, as Fackler claimed, but the WDMET data from Vietnam actually provides supporting evidence for it.

A summary of the debate was published in 2009 as part of a "Historical Overview of Wound Ballistics Research."

The Wound Data and Munitions Effectiveness Team (WDMET) gathered data on wounds sustained during the Vietnam War. In their analysis of this data published in the "Textbook of Military Medicine", Ronald Bellamy and Russ Zajtchuck point out a number of cases which seem to be examples of distant injuries. Bellamy and Zajtchuck describe three mechanisms of distant wounding due to pressure transients: 1) stress waves 2) shear waves and 3) a vascular pressure impulse.

After citing Harvey's conclusion that "stress waves probably do not cause any tissue damage" (p. 136), Bellamy and Zajtchuck express their view that Harvey's interpretation might not be definitive because they write "the possibility that stress waves from a penetrating projectile might also cause tissue damage cannot be ruled out." (p. 136) The WDMET data includes a case of a lung contusion resulting from a hit to the shoulder. The caption to Figure 4-40 (p. 149) says, "The pulmonary injury may be the result of a stress wave." They describe the possibility that a hit to a soldier's trapezius muscle caused temporary paralysis due to "the stress wave passing through the soldier's neck indirectly [causing] cervical cord dysfunction." (p. 155)

In addition to stress waves, Bellamy and Zajtchuck describe shear waves as a possible mechanism of indirect injuries in the WDMET data. They estimate that 10% of bone fractures in the data may be the result of indirect injuries, that is, bones fractured by the bullet passing close to the bone without a direct impact. A Chinese experiment is cited which provides a formula estimating how pressure magnitude decreases with distance. Together with the difference between strength of human bones and strength of the animal bones in the Chinese experiment, Bellamy and Zajtchuck use this formula to estimate that assault rifle rounds "passing within a centimeter of a long bone might very well be capable of causing an indirect fracture." (p. 153) Bellamy and Zajtchuck suggest the fracture in Figures 4-46 and 4-47 is likely an indirect fracture of this type. Damage due to shear waves extends to even greater distances in abdominal injuries in the WDMET data. Bellamy and Zajtchuck write, "The abdomen is one body region in which damage from indirect effects may be common." (p. 150) Injuries to the liver and bowel shown in Figures 4-42 and 4-43 are described, "The damage shown in these examples extends far beyond the tissue that is likely to direct contact with the projectile." (p. 150)

In addition to providing examples from the WDMET data for indirect injury due to propagating shear and stress waves, Bellamy and Zajtchuck expresses an openness to the idea of pressure transients propagating via blood vessels can cause indirect injuries. "For example, pressure transients arising from an abdominal gunshot wound might propagate through the vena cavae and jugular venous system into the cranial cavity and cause a precipitous rise in intracranial pressure there, with attendant transient neurological dysfunction." (p. 154) However, no examples of this injury mechanism are presented from the WDMET data. However, the authors suggest the need for additional studies writing, "Clinical and experimental data need to be gathered before such indirect injuries can be confirmed." Distant injuries of this nature were later confirmed in the experimental data of Swedish and Chinese researchers, in the clinical findings of Krajsa and in autopsy findings from Iraq.

Proponents of the concept point to human autopsy results demonstrating brain hemorrhaging from fatal hits to the chest, including cases with handgun bullets. Thirty-three cases of fatal penetrating chest wounds by a single bullet were selected from a much larger set by excluding all other traumatic factors, including past history.
An 8-month study in Iraq performed in 2010 and published in 2011 reports on autopsies of 30 gunshot victims struck with high-velocity (greater than 2500 fps) rifle bullets. The authors determined that the lungs and chest are the most susceptible to distant wounding, followed by the abdomen. The study noted that the "sample size was so small [too small] to reach the level of statistical significance". Nevertheless, the authors conclude:

A shock wave can be created when fluid is rapidly displaced by an explosive or projectile. Tissue behaves similarly enough to water that a sonic pressure wave can be created by a bullet impact, generating pressures in excess of .

Duncan MacPherson, a former member of the International Wound Ballistics Association and author of the book, Bullet Penetration, claimed that shock waves cannot result from bullet impacts with tissue. In contrast, Brad Sturtevant, a leading researcher in shock wave physics at Caltech for many decades, found that shock waves can result from handgun bullet impacts in tissue. Other sources indicate that ballistic impacts can create shock waves in tissue.

Blast and ballistic pressure waves have physical similarities. Prior to wave reflection, they both are characterized by a steep wave front followed by a nearly exponential decay at close distances. They have similarities in how they cause neural effects in the brain. In tissue, both types of pressure waves have similar magnitudes, duration, and frequency characteristics. Both have been shown to cause damage in the hippocampus. It has been hypothesized that both reach the brain from the thoracic cavity via major blood vessels.

For example, Ibolja Cernak, a leading researcher in blast wave injury at the Applied Physics Laboratory at Johns Hopkins University, hypothesized, "alterations in brain function following blast exposure are induced by kinetic energy transfer of blast overpressure via great blood vessels in abdomen and thorax to the central nervous system." This hypothesis is supported by observations of neural effects in the brain from localized blast exposure focused on the lungs in experiments in animals.

"Hydrostatic shock" expresses the idea that organs can be damaged by the pressure wave in addition to damage from direct contact with the penetrating projectile. If one interprets the "shock" in the term "hydrostatic shock" to refer to the physiological effects rather than the physical wave characteristics, the question of whether the pressure waves satisfy the definition of "shock wave" is unimportant, and one can consider the weight of scientific evidence and various claims regarding the possibility of a ballistic pressure wave to create tissue damage and incapacitation in living targets.

A number of papers describe the physics of ballistic pressure waves created when a high-speed projectile enters a viscous medium. These results show that ballistic impacts produce pressure waves that propagate at close to the speed of sound.

Lee et al. present an analytical model showing that unreflected ballistic pressure waves are well approximated by an exponential decay, which is similar to blast pressure waves. Lee et al. note the importance of the energy transfer:

The rigorous calculations of Lee et al. require knowing the drag coefficient and frontal area of the penetrating projectile at every instant of the penetration. Since this is not generally possible with expanding handgun bullets, Courtney and Courtney developed a model for estimating the peak pressure waves of handgun bullets from the impact energy and penetration depth in ballistic gelatin. This model agrees with the more rigorous approach of Lee et al. for projectiles where they can both be applied. For expanding handgun bullets, the peak pressure wave magnitude is proportional to the bullet's kinetic energy divided by the penetration depth.

Goransson et al. were the first contemporary researchers to present compelling evidence for remote cerebral effects of extremity bullet impact. They observed changes in EEG readings from pigs shot in the thigh. A follow-up experiment by Suneson et al. implanted high-speed pressure transducers into the brain of pigs and demonstrated that a significant pressure wave reaches the brain of pigs shot in the thigh. These scientists observed apnea, depressed EEG readings, and neural damage in the brain caused by the distant effects of the ballistic pressure wave originating in the thigh.

The results of Suneson et al. were confirmed and expanded upon by a later experiment in dogs
which "confirmed that distant effect exists in the central nervous system after a high-energy missile impact to an extremity. A high-frequency oscillating pressure wave with large amplitude and short duration was found in the brain after the extremity impact of a high-energy missile..." Wang et al. observed significant damage in both the hypothalamus and hippocampus regions of the brain due to remote effects of the ballistic pressure wave.

In a study of a handgun injury, Sturtevant found that pressure waves from a bullet impact in the torso can reach the spine and that a focusing effect from concave surfaces can concentrate the pressure wave on the spinal cord producing significant injury. This is consistent with other work showing remote spinal cord injuries from ballistic impacts.

Roberts et al. present both experimental work and finite element modeling showing that there can be considerable pressure wave magnitudes in the thoracic cavity for handgun projectiles stopped by a Kevlar vest. For example, an 8 gram projectile at 360 m/s impacting a NIJ level II vest over the sternum can produce an estimated pressure wave level of nearly 2.0 MPa (280 psi) in the heart and a pressure wave level of nearly 1.5 MPa (210 psi) in the lungs. Impacting over the liver can produce an estimated pressure wave level of 2.0 MPa (280 psi) in the liver.

The work of Courtney et al. supports the role of a ballistic pressure wave in incapacitation and injury. The work of Suneson et al. and Courtney et al. suggest that remote neural effects can occur with levels of energy transfer possible with handguns, about . Using sensitive biochemical techniques, the work of Wang et al. suggests even lower impact energy thresholds for remote neural injury to the brain. In analysis of experiments of dogs shot in the thigh they report highly significant (p < 0.01), easily detectable neural effects in the hypothalamus and hippocampus with energy transfer levels close to . Wang et al. reports less significant (p < 0.05) remote effects in the hypothalamus with energy transfer just under .

Even though Wang et al. document remote neural damage for low levels of energy transfer, roughly , these levels of neural damage are probably too small to contribute to rapid incapacitation. Courtney and Courtney believe that remote neural effects only begin to make significant contributions to rapid incapacitation for ballistic pressure wave levels above (corresponds to transferring roughly in of penetration) and become easily observable above (corresponds to transferring roughly in of penetration). Incapacitating effects in this range of energy transfer are consistent with observations of remote spinal injuries, observations of suppressed EEGs and apnea in pigs and with observations of incapacitating effects of ballistic pressure waves without a wound channel.

The scientific literature contains significant other findings regarding injury mechanisms of ballistic pressure waves. Ming et al. found that ballistic pressure waves can break bones. Tikka et al. reports abdominal pressure changes produced in pigs hit in one thigh. Akimov et al. report on injuries to the nerve trunk from gunshot wounds to the extremities.

In self-defense, military, and law enforcement communities, opinions vary regarding the importance of remote wounding effects in ammunition design and selection. In his book on hostage rescuers, Leroy Thompson discusses the importance of hydrostatic shock in choosing a specific design of .357 Magnum and 9×19mm Parabellum bullets. In "Armed and Female", Paxton Quigley explains that hydrostatic shock is the real source of "stopping power." Jim Carmichael, who served as shooting editor for Outdoor Life magazine for 25 years, believes that hydrostatic shock is important to "a more immediate disabling effect" and is a key difference in the performance of .38 Special and .357 Magnum hollow point bullets. In "The search for an effective police handgun," Allen Bristow describes that police departments recognize the importance of hydrostatic shock when choosing ammunition. A research group at West Point suggests handgun loads with at least of energy and of penetration and recommends:

A number of law enforcement and military agencies have adopted the 5.7×28mm cartridge. These agencies include the Navy SEALs and the Federal Protective Service branch of the ICE. In contrast, some defense contractors, law enforcement analysts, and military analysts say that hydrostatic shock is an unimportant factor when selecting cartridges for a particular use because any incapacitating effect it may have on a target is difficult to measure and inconsistent from one individual to the next. This is in contrast to factors such as proper shot placement and massive blood loss which are almost always eventually incapacitating for nearly every individual.

The FBI recommends that loads intended for self-defense and law enforcement applications meet a minimum penetration requirement of in ballistic gelatin and explicitly advises against selecting rounds based on hydrostatic shock effects.

Hydrostatic shock is commonly considered as a factor in the selection of hunting ammunition. Peter Capstick explains that hydrostatic shock may have value for animals up to the size of white-tailed deer, but the ratio of energy transfer to animal weight is an important consideration for larger animals. If the animal's weight exceeds the bullet's energy transfer, penetration in an undeviating line to a vital organ is a much more important consideration than energy transfer and hydrostatic shock. Jim Carmichael, in contrast, describes evidence that hydrostatic shock can affect animals as large as Cape Buffalo in the results of a carefully controlled study carried out by veterinarians in a buffalo culling operation.

Dr. Randall Gilbert describes hydrostatic shock as an important factor in bullet performance on whitetail deer, "When it [a bullet] enters a whitetail’s body, huge accompanying shock waves send vast amounts of energy through nearby organs, sending them into arrest or shut down." Dave Ehrig expresses the view that hydrostatic shock depends on impact velocities above per second. Sid Evans explains the performance of the Nosler Partition bullet and Federal Cartridge Company's decision to load this bullet in terms of the large tissue cavitation and hydrostatic shock produced from the frontal diameter of the expanded bullet. The North American Hunting Club suggests big game cartridges that create enough hydrostatic shock to quickly bring animals down.




</doc>
