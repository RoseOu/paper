<doc id="18724" url="https://en.wikipedia.org/wiki?curid=18724" title="Land Rover">
Land Rover

Land Rover is a British brand of four-wheel drive cars, that exclusively offers premium and luxury sport utility vehicles. Owned by multinational car manufacturer Jaguar Land Rover (JLR), which has been owned by India's Tata Motors since 2008, JLR currently build Land Rovers in Brazil, China, India, Slovakia, and the United Kingdom.

The Land Rover name was originally used by the Rover Company for a boxy four-wheel drive, off-road model, launched in 1948—now known as the Land Rover Series, it is today regarded as a British icon. It was granted a Royal Warrant by King George VI in 1951, and fifty years later, in 2001, it received a Queen's Award for Enterprise for outstanding contribution to international trade.

Over time, Land Rover grew into its own brand (and for a while also a company), encompassing a consistently growing range of four-wheel drive, off-road capable models. Starting with the much more upmarket 1970 Range Rover, and subsequent introductions of the mid-range Discovery and entry-level Freelander line (in 1989 and 1997), as well as the 1990 Land Rover Defender refresh, the marque today includes two models of Discovery, four distinct models of Range Rover, and after a three-year hiatus, a second genaration of Defenders have gone into production for the 2020 model year—in short or long wheelbase, as before.

Originally, the vehicles was simply called the Land Rover – an off-road capable car model of the Rover Company. The "Series" indication later became a retronym model name, once 'Land Rover' had started becoming a brand, with the introduction of the Range Rover in 1970, and eventually even a British Leyland subsidiary in 1978. In 1983 and 1984, the long and the short wheelbase Land Rovers were finally given official names — the One Ten, and the Ninety respectively, and together they were badged the Defender models in 1990, after the 1989 introduction of the new Discovery model.

The design for the original vehicle was started in 1947 by Maurice Wilks. Wilks, chief designer at the Rover Company, on his farm in Newborough, Anglesey, working in conjunction with his brother Spencer who was the managing director of Rover. The design may have been influenced by the Jeep and the prototype, later nicknamed Centre Steer, was built on a Jeep chassis and axles. The early choice of colour was dictated by military surplus supplies of aircraft cockpit paint, so early vehicles only came in various shades of light green; all models until recently feature sturdy box section ladder-frame chassis. Early vehicles like the Series I were field-tested at Long Bennington and designed to be field-serviced.

After the formation of Land Rover Limited in 1978 the hyphen in Land-Rover —as shown in the logo— began to be dropped.

Land Rover as a company has existed since 1978. Prior to this, it was a product line of the Rover Company which was subsequently absorbed into the Rover-Triumph division of the British Leyland Motor Corporation (BL) following Leyland Motor Corporation’s takeover of Rover in 1967. The ongoing commercial success of the original Land Rover series models, and latterly the Range Rover in the 1970s in the midst of BL's well-documented business troubles prompted the establishment of a separate Land Rover company but still under the BL umbrella, remaining part of the subsequent Rover Group in 1988, under the ownership of British Aerospace after the remains of British Leyland were broken up and privatised.

In 1994 Rover Group plc, including Land Rover, was acquired by BMW. In 2000, Rover Group was broken up by BMW and Land Rover was sold to Ford Motor Company, becoming part of its Premier Automotive Group.

In 2006 Ford also purchased the Rover brand from BMW for around £6 million. BMW had retained ownership of the brand to protect the integrity of the Land Rover brand, with which 'Rover' might be confused in the US 4x4 market and allowed it to be used under licence by MG Rover until it collapsed in 2005, at which point it was offered to the Ford Motor Company, who by then owned Land Rover. On 11 June 2007, Ford announced that it planned to sell Land Rover along with Jaguar Cars. Private equity firms such as Alchemy Partners of the UK, TPG Capital, Ripplewood Holdings, Cerberus Capital Management and One Equity Partners of the US, Tata Motors of India and a consortium comprising Mahindra & Mahindra of India and Apollo Management all initially expressed interest in purchasing the marques from the Ford Motor Company. On 1 January 2008, Ford formally declared that Tata was the preferred bidder. In 2008, On 26 March 2008, Ford announced that it had agreed to sell its Jaguar and Land Rover operations to Tata Motors, and that it expected to complete the sale by the end of the second quarter of 2008.

On 18 January 2008, Tata Motors, a part of the Tata Group, established Jaguar Land Rover Limited as a British-registered and wholly owned subsidiary. The new company was to be used as a holding company for the acquisition of the two businesses from Ford - Jaguar Cars Limited and Land Rover. That acquisition was completed on 2 June 2008 at a cost of £1.7 billion. Included in the deal to buy Land Rover and Jaguar Cars were the rights to three other British brands: the Daimler marque, as well as two dormant brands Lanchester and Rover.

On 1 January 2013, the group, which had been operating as two separate companies (Jaguar Cars Limited and Land Rover), although on an integrated basis, underwent a fundamental restructuring. The parent company was renamed to Jaguar Land Rover Automotive PLC, Jaguar Cars Limited was renamed to Jaguar Land Rover Limited and the assets (excluding certain Chinese interests) of Land Rover were transferred to it. The consequence was that Jaguar Land Rover Limited became responsible in the UK for the design, manufacture and marketing of both Jaguar and Land Rover branded products, and Land Rover and Jaguar Cars ceased to be separate vehicle producing entities.


Jaguar Land Rover manufactures Land Rover cars in plants in five countries. In the United Kingdom the Range Rover, Range Rover Sport and Range Rover Velar are built at their Solihull plant near Birmingham and the Discovery Sport and Evoque are built at their Halewood plant near Liverpool. In October 2018 JLR openend a new plant in Nitra, Slovakia to build the Discovery, and are now also building the 2020 Defender there. In Brazil the company builds both the Discovery Sport and Evoque in their plant in Itatiaia which was opened in June 2016. JLR has been building cars since 2011 in Pune, India and currently builds the Discovery Sport and Evoque there. Under a 50/50 joint venture with Chery at Changshu in China Discovery Sports and Evoques are also built.

Historically Land Rovers were manufactured primarily at the Solihull plant until production of the Freelander was moved to the Halewood plant. The Freelander was also assembled in CKD form at Land Rover's facility in Pune, India. As of 2015, the company continued to expand by building locally in India as well as increasing the number of models made at JLR’s Chikhali facility near Pune to include the Discovery Sport and Evoque.

Defender models were assembled under licence in several locations worldwide, including Spain (Santana Motors), Iran (Pazhan Morattab), Brazil (Karmann), and Turkey (Otokar).



Range Stormer – Land Rover's first concept vehicle, unveiled at the 2004 North American International Auto Show, later became the Range Rover Sport.(Gritzinger, 2004).

Land Rover LRX – Land Rover's second concept vehicle, first unveiled at the 2008 Detroit Auto Show. Originally a vehicle with ERAD technology, the production version did not include this. The car was then launched in 2011 as the Range Rover Evoque, and was the first Range Rover branded product to be offered with front wheel drive, and no low ratio transfer box.

Land Rover DC100 – Land Rover's third concept vehicle, first unveiled at the 2011 Frankfurt Auto Show, designed to be a replacement for the Land Rover Defender, though it is unlikely that the Defender's replacement will be exactly the same as the DC100 concept.

Land Rover Discovery Vision Concept – Land Rover's fourth concept vehicle, first unveiled at the 2014, was designed to be a replacement for the Land Rover Discovery, This concept features Transparent Bonnet, Suicide doors, and Laser assisted lamps (there is a very little chance this will be included in any future production vehicles).

Models developed for the UK Ministry of Defence (MoD) include:

Models developed for the Australian Army

During the history of the Land Rover many different engines have been fitted:

, most Land Rovers in production are powered by Ford engines. Under the terms of the acquisition, Tata has the right to buy engines from Ford until 2019.

Integrated Electric Rear Axle Drive (ERAD) technology, dubbed e-terrain technology, will allow the vehicle to move off without starting the engine as well as supplying extra power over tough terrain. Land Rover's Diesel ERAD Hybrid was developed as part of a multimillion-pound project supported by the UK Government's Energy Saving Trust, under the low carbon research and development programme. ERAD programme is one of a broad range of sustainability-focused engineering programmes that Land Rover is pursuing, brought together by the company under the collective name "e TERRAIN Technologies".

Land Rover presented at the 2008 London Motor Show its new ERAD diesel-electric hybrid in a pair of Freelander 2 (LR2) prototypes. The new hybrid system is being designed as a scalable and modular system that could be applied across a variety of Land Rover models and powertrains.

Land Rover unveiled the LRX hybrid concept at the 2008 North American International Auto Show in Detroit, for it to be going into production. An ERAD will enable the car to run on electric power at speeds below .

In September 2011, the Range Rover Evoque was launched, though it was based on the LRX hybrid concept presented at the 2008 North American International Auto Show, it did not include the ERAD system, included in the original concept.

In February 2013, Land Rover unveiled at the 83rd Geneva Motor Show an All-Terrain Electric Defender that produces zero emissions. The electric vehicle was developed for research purposes following successful trials of the Defender-based electric vehicle, Leopard 1. The vehicle is capable of producing 70kW and 330Nm of torque and has a range of 80 kilometres or in low speed off-road use it can last for up to eight hours before recharging.

Power take-off (PTO) was integral to the Land Rover concept from 1948, enabling farm machinery and many other items to be run with the vehicle stationary. Maurice Wilks' original instruction was "...to have power take-offs everywhere!" The 1949 report by British National Institute of Agricultural Engineering and Scottish Machinery Testing Station contained this description: "the power take-off is driven through a Hardy Spicer propeller shaft from the main gearbox output and two interchangeable pinions giving two ratios. The PTO gearbox casing is bolted to the rear chassis cross-member and an belt pulley driven from the PTO shaft through two bevel gears can be bolted to the PTO gearbox casing." PTOs remained regular options on Series I, II and III Land Rovers up to the demise of the Series Land Rover in 1985. An agricultural PTO on a Defender is possible as a special order.

Land Rovers (the Series/Defender models) are available in a variety of body styles, from a simple canvas-topped pick-up truck to a twelve-seat fully trimmed station wagon. Both Land Rover and out-of-house contractors have offered conversions and adaptations to the basic vehicle, such as fire engines, excavators, 'cherry picker' hydraulic platforms, ambulances, snowploughs, and six-wheel-drive versions, as well as one-off special builds including amphibious Land Rovers and vehicles fitted with tracks instead of wheels.

Various Land Rover models have been used in a military capacity, most notably by the British Army and Australian Army. Modifications may include military "blackout" lights, heavy-duty suspension, uprated brakes, 24 volt electrics, convoy lights, electronic suppression of the ignition system, blackout curtains and mounts for special equipment and small arms. Dedicated military models have been produced such as the 101 Forward Control and the air-portable 1/2 ton Lightweight. Military uses include light utility vehicle; communications platform; weapon platform for recoilless rifles, Anti-tank (e.g. TOW or M40 recoilless rifle) / Surface-to-Air Guided Weapons or machine guns; ambulances and workshops. The Discovery has also been used in small numbers, mostly as liaison vehicles.

Two models that have been designed for military use from the ground up are the 101 Forward Control from the early 1970s and the Lightweight or Airportable from the late 1960s. The latter was intended to be transported under a helicopter. The Royal Air Force Mountain Rescue Service (RAFMRS) teams were early users in the late 1950s and early 1960s, and their convoys of Land Rovers and larger military trucks are a sight often seen in the mountain areas of the United Kingdom. Originally RAFMRS Land Rovers had blue bodies and bright yellow tops, to be better seen from above. In 1981, the colour scheme was changed to green with yellow stripes. More recently, vehicles have been painted white, and are issued with fittings similar to civilian UK Mountain Rescue teams.
An adaptation of Land Rovers to military purposes is the "Pink Panther" models. Approximately 100 Series IIA models were adapted to reconnaissance use by British special operations forces the SAS. For desert use they were often painted pink, hence the name. The vehicles were fitted with among other gear a sun compass, machine guns, larger fuel tanks and smoke dischargers. Similar adaptations were later made to Series IIIs and 90/110/Defenders.

The Australian Army adapted the Land Rover Series 2 into the Long Range Patrol Vehicle for use by the Special Air Service Regiment and as an anti-tank "gunbuggy" fitted with an M40 recoilless rifle.

The 75th Ranger Regiment of the United States Army also adapted twelve versions of the Land Rover that were officially designated the Ranger Special Operations Vehicle.

Series and Defender models have also been armoured. The most widespread of these is the Shorts Shorland, built by Shorts Brothers of Belfast. The first of these were delivered in 1965 to the Royal Ulster Constabulary, the Northern Ireland police force. They were originally wheelbase models with an armoured body and a turret from the Ferret armoured car. By 1990, there had been more than 1,000 produced.
In the 1970s, a more conventional armoured Land Rover was built for the Royal Ulster Constabulary in Wales called the Hotspur. The Land Rover Tangi was built by the Royal Ulster Constabulary's own vehicle engineering team during the 1990s. The British Army has used various armoured Land Rovers, first in Northern Ireland but also in more recent campaigns. They first added protective panels to Series General Service vehicles (the Vehicle Protection Kit (VPK)). Later they procured the Glover Webb APV and finally the Courtaulds (later NP Aerospace) Composite Armoured Vehicle, commonly known as Snatch. These were originally based on heavy-duty V8 110 chassis but some have recently been re-mounted on new chassis from Otokar of Turkey and fitted with diesel engines and air-conditioning for Iraq. Although these now have more in common with the 'Wolf' (Defender XD) Land Rovers that many mistakenly confuse them with, the Snatch and the Wolf are different vehicles.

The most radical conversion of a Land Rover for military purposes was the Centaur half-track. It was based on a Series III with a V8 engine and a shortened belt drive from the Alvis Scorpion light tank. A small number was manufactured, and they were used by Ghana, among others.

The Land Rover is used by military forces throughout the world. The current generation of Land Rover used by British Army, the Snatch 2, have upgraded and strengthened chassis and suspension compared to civilian-specification vehicles. There is also the Land Rover WMIK (weapon mounted installation kit) used by British Army. The WMIK consists of a driver, a raised gun, usually a Browning heavy machine gun or a grenade machine gun, this used for ground support, and a GPMG (general-purpose machine gunner) located next to the driver, this used for vehicle protection.

Highly modified Land Rovers have competed in the Dakar Rally and won the Macmillan 4x4 UK Challenge almost every year, as well as having been the vehicle used for the Camel Trophy. Now, Land Rover has its own G4 challenge.

Land Rover Experience was established in 1990, and consists of a network of centres throughout the world, set up to help customers get the most out of their vehicles' on and off-road capability. The flagship centres are Land Rover's bases at Solihull, Eastnor, Gaydon and Halewood. Courses offered include off-road driving, winching and trailer handling, along with a variety of corporate and individual 'Adventure Days'. The factory centres at Solihull and Halewood have manufacturing tours, while Gaydon has an engineering tour.

Model-by-model road accident statistics from the UK Department for Transport show that the Land Rover Defender is one of the safest cars on British roads as measured by chance of death in two-car injury accidents.
The figures, which were based on data collected by police forces following accidents between 2000 and 2004 in Great Britain, showed that Defender drivers had a 1% chance of being killed or seriously injured and a 33% chance of sustaining any kind of injury. Other four-wheel-drive vehicles scored equally highly, and collectively these vehicles were much safer for their passengers than those in other classes such as passenger cars and MPVs. These figures reflect the fact that drivers of large mass vehicles are likely to be safer, often at the expense of other drivers if they collide with smaller cars.

The original Land Rover Owners Club was set up by the Rover Company in 1954. The company published the Land Rover Owners Club Review magazine for members from 1957 to 1968 when the club became the Rover Owners Association. This original association fell away when the company merged with British Leyland.

There are many Land Rover clubs throughout the UK and internationally. Land Rover clubs break down into a number of groups of varying interests.

Single Marque Clubs – Bring together owners of a specific model or series of vehicle such as the Land Rover Series One Club, or the Discovery Owners Club. Single marque clubs have a global membership.

Special Vehicle Clubs – At various times Land Rover have produced vehicles for specific events or on a specific theme, most notable are the Camel Trophy and G4 Challenge vehicles which have been sold on to the general public, and a range of Defenders that were loosely based on the custom vehicles produced for the Tomb Raider motion picture.

Regional Clubs in the UK break down into two groups, competitive and non-competitive. The non-competitive clubs activities generally relate to social events, off-road driving or green laning on un-surfaced public highways or 'pay and play' days at off-road centres. Competitive clubs are a phenomenon almost exclusively found within the UK, who as well as the non-competitive activities detailed above run competitive events such as Tyro, Road Taxed Vehicle (RTV) and Cross Country Vehicle (CCV) trials, winch and recovery challenges or speed events such as Competitive Safaries. All UK competitive events are run within the framework of rules created by the Motor Sports Association (MSA) with further vehicle specific rules applied by the host club or association. Outside of the UK regional clubs are independent and mostly non-competitive.

A number of clubs are affiliated to the Association of Land Rover Clubs (ALRC), formerly known as the Association of Rover Clubs (ARC) the association applies its own vehicle regulations to all of its member clubs who have the opportunity to compete together at regional events and an annual national event with vehicles approved to the same standard. In recent years some non-competitive clubs have dropped their affiliation fifth ALRC. Few clubs outside of the UK are affiliated with ALRC. Other than ALRC and the short lived Association of North American Rover Clubs (ANARC), which was created 1998 to celebrate Land Rover's 50th anniversary and disbanded in 2001, other groups of Land Rover clubs have affiliated with each other.

Land Rover owners were also early adopters of virtual clubs that are entirely based online. Bill Caloccia created the original Land Rover Owner email list (LRO) as single marque offshoot of the British Cars email list in May 1990. Bill later created email lists in the mid 1990s for Range Rovers (RRO) and various regions (e.g., UK-LRO, AU-LRO, ZA-LRO, EU-LRO, IT-LRO, NL-LRO). In California members of the LRO list created mendo_recce in 1995.

In 2005, under Ford ownership, Land Rover became more interested in the club environment. An internal club was formed, The Land Rover Club, exclusive to employees of Ford's Premier Automotive Group (Now exclusive to the new 'Jaguar – Land Rover' group since the brand moved away from the Ford stable). Also, an agreement was generated to allow other clubs to use the Land Rover green oval logo under licence. In 2006, the Bedfordshire, Hertfordshire and Cambridgeshire club were the pilot licensees for the new agreement, who now benefit from a reciprocal arrangement where their own logo is trade marked and owned by Land Rover and they can refer to themselves as a 'Land Rover Approved Club'.

In 1995, Land Rover endorsed the production of a hand-made bicycle using its logo. The bicycle, called the Land Rover APB and manufactured by Pashley Cycles of Stratford-upon-Avon, was the collapsible version of Pashley Cycles' Moulton APB (All Purpose Bicycle) model, with leading link front suspension and adjustable damping and stroke. Two more models immediately followed: the Land Rover XCB V-20, aimed primarily at younger riders (children); and the Land Rover XCB D-26, also available as the M26 with hydraulic rim brakes, front suspension and suspension seat pillar.

In June 2004, Land Rover released a comprehensive 25 model range of bicycles. The three main ranges are the "Defender", the "Discovery", and the "Freelander", each with different attributes. The "Discovery" is an all-rounder bicycle suited to a variety of terrains, "Defender" is most suited to rugged terrain and off-road pursuits, whereas the "Freelander" is designed for an urban lifestyle. All bikes are made from lightweight aluminium.

In 2010 the range was relaunched in conjunction with British manufacturer 2x2.

Land Rover has had its name associated with coffee since 2005, when the Land Rover Coffee company was established.

Land Rover gave UK pram company Pegasus a licence to produce a three-wheeler range of Land Rover ATP pushchairs. The design reflected the heritage of the marque, with a light metal frame with canvas seating, held together with push-studs and tough simple parts like brakes and hinges. They could be collapsed completely flat, with wheels removed in seconds. The basic frame could be adapted with modules to allow a baby to lie flat or a bubble windscreen to completely enclose the child. The frame also came in long or short-handled versions, and could be repaired with home tools. The design was simple, light, and rugged and able to travel in all terrains (hence the ATP for all-terrain pushchair.) It came in three military looking colours: a light blue, a sand colour and olive drab. Production was discontinued in 2002.




</doc>
<doc id="18727" url="https://en.wikipedia.org/wiki?curid=18727" title="International Numbering System for Food Additives">
International Numbering System for Food Additives

The International Numbering System for Food Additives (INS) is a European-based naming system for food additives, aimed at providing a short designation of what may be a lengthy actual name. It is defined by Codex Alimentarius, the international food standards organisation of the World Health Organization (WHO) and Food and Agriculture Organization (FAO) of the United Nations (UN). The information is published in the document "Class Names and the International Numbering System for Food Additives", first published in 1989, with revisions in 2008 and 2011. The INS is an open list, "subject to the inclusion of additional additives or removal of existing ones on an ongoing basis".

INS numbers consist of three or four digits, optionally followed by an alphabetical suffix to further characterize individual additives. On packaging in the European Union (EU), approved food additives are written with a prefix of "E". Australia and New Zealand do not use a prefix letter when listing additives in the ingredients. An additive that appears in the INS does not automatically have a corresponding E number.

INS numbers are assigned by the committee to identify each food additive. INS numbers generally correspond to E numbers for the same compound, e.g. INS 102, Tartrazine, is also E102. INS numbers are not unique and, in fact, one number may be assigned to a group of like compounds.





</doc>
<doc id="18728" url="https://en.wikipedia.org/wiki?curid=18728" title="Law of dilution">
Law of dilution

Wilhelm Ostwald’s dilution law is a relationship proposed in 1888 between the dissociation constant ' and the degree of dissociation ' of a weak electrolyte. The law takes the form

Where the square brackets denote concentration, and is the total concentration of electrolyte.

Using formula_2, where formula_3 is the molar conductivity at concentration c and formula_4 is the limiting value of molar conductivity extrapolated to zero concentration or infinite dilution, this results in the following relation:

Consider a binary electrolyte AB which dissociates reversibly into A and B ions. Ostwald noted that the law of mass action can be applied to such systems as dissociating electrolytes. The equilibrium state is represented by the equation:

If ' is the fraction of dissociated electrolyte, then ' is the concentration of each ionic species. must, therefore be the fraction of "undissociated" electrolyte, and the concentration of same. The dissociation constant may therefore be given as

For very weak electrolytes (however, neglecting 'α' for most weak electrolytes yields counterproductive result) , implying that .

This gives the following results;

Thus, the degree of dissociation of a weak electrolyte is proportional to the inverse square root of the concentration, or the square root of the dilution. The concentration of any one ionic species is given by the root of the product of the dissociation constant and the concentration of the electrolyte.

The Ostwald law of dilution provides a satisfactory description of the concentration dependence of the conductivity of weak electrolytes like CHCOOH and NHOH. The variation of molar conductivity is essentially due to the incomplete dissociation of weak electrolytes into ions. 

For strong electrolytes, however, Lewis and Randall recognized that the law fails badly since the supposed equilibrium constant is actually far from constant. This is because the dissociation of strong electrolytes into ions is essentially complete below a concentration threshold value. The decrease in molar conductivity as a function of concentration is actually due to attraction between ions of opposite charge as expressed in the Debye-Hückel-Onsager equation and later revisions.

Even for weak electrolytes the equation is not exact. Chemical thermodynamics shows that the true equilibrium constant is a ratio of thermodynamic activities, and that each concentration must be multiplied by an activity coefficient. This correction is important for ionic solutions due to the strong forces between ionic charges. An estimate of their values is given by the Debye–Hückel theory at low concentrations.



</doc>
<doc id="18729" url="https://en.wikipedia.org/wiki?curid=18729" title="Lleida">
Lleida

Lleida ( , ; ) is a city in the west of Catalonia, Spain. It is the capital city of the province of Lleida.

Geographically, it is located in the Catalan Central Depression. It is also the capital city of the Segrià comarca, as well as the largest city in the province. It had 137,387 inhabitants , including the contiguous municipalities of Raimat and Sucs.

Lleida is one of the oldest towns in Catalonia, with recorded settlements dating back to the Bronze Age period. Until the Roman conquest of the Iberian Peninsula, the area served as a settlement for an Iberian people, the Ilergetes. The town became a municipality, named Ilerda, under the reign of Augustus. It was reconquered in 1149, after being ruled by the Moors for many centuries, who had conquered the town in the 8th century. In 1297, the University of Lleida was founded, becoming the third oldest in the whole of Spain. During the following centuries, the town was damaged by several wars such as the Reapers' War in the 17th century and the Spanish Civil War in the 20th century. Since then, the city has been in a constant urban, commercial and demographic growth.

In ancient times the city, named Iltrida and Ilerda, was the chief city of the Ilergetes, an Iberian tribe. Indíbil, king of the Ilergetes, and Mandoni, king of the Ausetanes, defended it against the Carthaginian and Roman invasions.

Under the Romans, the city was incorporated into the Roman province of Hispania Tarraconensis, and was a place of considerable importance, historically as well as geographically. It stood upon an eminence, on the right (west) bank of the river Sicoris (the modern Segre), the principal tributary of the Ebre, and some distance above its confluence with the Cinga (modern Cinca); thus commanding the country between those rivers, as well as the great road from Tarraco (modern Tarragona), the provincial capital, to the northwest of Spain, which here crossed the Sicoris.

Its situation induced the legates of Pompey in Spain to make it the key of their defense against Caesar, in the first year of the Civil War (49 BC). Afranius and Marcus Petreius threw themselves into the place with five legions; and their siege by Caesar himself (Battle of Ilerda), as narrated in his own words, forms one of the most interesting passages of military history. The resources exhibited by the great general, in a contest where the formation of the district and the very elements of nature seemed in league with his enemies, have been frequently extolled; but no epitome can do justice to the campaign. It ended by the capitulation of Afranius and Petreius, who were conquered as much by Caesar's generosity as by his strategy. In consequence of the battle, the Latin phrase "Ilerdam videas" is said to have been used by people who wanted to cast bad luck on someone else.

Under the Roman empire, Ilerda was a very flourishing city, and a "municipium". It minted its own coins. It had a fine stone bridge over the Sicoris, (the bridge was so sturdy that its foundations support a bridge to this day). In the time of Ausonius the city had fallen into decay; but it rose again into importance in the Middle Ages.

It was part of Visigothic and Muslim Hispania until it was conquered from the Moors by Count Ramon Berenguer IV of Barcelona in 1149.

It used to be the seat of a major university, the oldest in the Crown of Aragon, until 1717, when it was moved by Philip V to the nearby town of Cervera. The University of Lleida is nowadays active again since 1991.

During the Reapers' War, Lleida was occupied by the French and rebel forces. In 1644 the city was conquered by the Spanish under D. Felipe da Silva.

Lleida served as a key defense point for Barcelona during the Spanish Civil War, and fell to the Insurgents, whose air forces bombed it extensively, in 1937 and 1938. The November 2, 1937 Legion Condor attacks against Lleida became especially infamous since they were aimed to the school known as "Liceu Escolar de Lleida". 48 children and several teachers died in it that day, 300 people were killed on the November 2 bombings altogether, and the town would be bombed and sieged again in 1938, when it was conquered by Franco's forces.

After some decades without any kind of population growth, it met a massive migration of Andalusians who helped the town undergo a relative demographic growth. Nowadays it is home to immigrants of 146 different nationalities.

During 2007 Lleida was the year's Capital of Catalan Culture.

Lleida has a temperate semi-arid climate (Köppen "BSk"). Winters are mild and foggy though cooler than places on the coast while summers are hot and dry. Frosts are common during winter although snowfall can occasionally fall, averaging 1 or 2 days. Precipitation is low, with an annual average of with a peak in April and May and another peak in September and October. 

Lleida is divided in the following districts by the "Observatori Socioeconòmic de Lleida":

Lleida is served by the RENFE, Spanish state railway's Madrid-Barcelona high-speed rail line, serving Barcelona, Zaragoza, Calatayud, Guadalajara, and Madrid. Lleida has a new airport opened in January 2010, and a minor airfield located in Alfès. Also, the town is the western terminus of the Eix Transversal Lleida-Girona, and a railway covering the same distance ("Eix Transversal Ferroviari") is currently under planning.

Lleida's only passenger railway station is Lleida Pirineus. It is served by both Renfe and Ferrocarrils de la Generalitat de Catalunya train lines. In the future a Rodalies Lleida commuter network will connect the town with its adjacent area and the main towns of its province, improving the existing network with more train frequency and newly built infrastructure. A second railway station is Pla de la Vilanoveta in an industrial area, and only used by freight trains. A future railway museum will be located in its facilities.
Since 2008 the bulk of public transport of the Lleida's surrounding area, mainly buses operated by several companies, is managed by Autoritat Territorial de la Mobilitat de l'Àrea de Lleida.

The urban buses, coloured yellow with blue stripes and owned by "Autobusos de Lleida", include the following lines:

In addition to these, there's a tourist bus and a regular night service to nearby clubs.

Lleida has depended long time on nearby airports and had no local air transit. Lleida-Alguaire airport opened in 2010.

A tram-train system is pending approval. Using an existing but outdated passenger line, it would link Balaguer and Lleida, crossing both towns in a much needed move towards better public transportation, both inner-city and between localities.

Lleida is a traditionally Catalan-speaking city and province, with a characteristic dialect (known as Western or, more specifically, North-Western Catalan, or colloquially "lleidatà"). Most of the population is actively bilingual in Spanish.

Lleida was the Capital of Catalan Culture in 2007.

Enric Granados Auditorium is the city's concert hall and main music institution and conservatory. It is named after the composer Enric Granados, who was born in the city. CaixaForum Lleida (formerly known as "Centre Cultural de la Fundació La Caixa") includes a concert hall. Teatre Municipal de l'Escorxador is the town's main theatre; it includes a concert venue, Cafè del Teatre. A theatre and congress centre, La Llotja de Lleida, opened in 2010.

There are two important music festivals in Lleida; MÚSIQUES DISPERSES Folk Festival in March, and the jazz festival JAZZ TARDOR in November. Concerts are also a regular fixture of the two local feasts, Sant Anastasi in May, and Sant Miquel in September.

CaixaForum Lleida is the usual venue for film-related events and screenings. A Latin-American film festival is held yearly in the town ("Mostra de Cinema Llatinoamericà de Lleida"), and an animation film festival called "Animac" is held every May.

The Lleida Museum opened in 2008 and displays historical artefacts and works of art from various periods. The Institut d'Estudis Ilerdencs, a historically relevant building, exhibits both ancient and contemporary art. The Centre d'Art La Panera is a contemporary art institution. The Museu d'Art Jaume Morera displays art from the 20th and 21st centuries (as well as artwork by its namesake).

The city has a number of small municipal galleries, such as the Sala Municipal d'Exposicions de Sant Joan and the Sala Manel Garcia Sarramona. There are also several institutions dedicated to local artists, such as the Sala Leandre Cristòfol, containing artwork by the sculptor and painter Leandre Cristòfol (1908–1998); and the Sala Coma Estadella, dedicated to the sculptor and painter Albert Coma Estadella (1933–1991).

Private art galleries include the Espai Cavallers. The private foundation CaixaForum Lleida and the Public Library of Lleida also offer regular exhibits. The now defunct Petite Galerie was an innovative and influential gallery in the 1970s.

The Escola Municipal de Belles Arts provides higher education in the arts.

Traditional celebrations include the main annual town festivity: "Festa Major"; "Fira de Sant Miquel" and "L’Aplec del Caragol" (escargot-eating festival, the biggest in the world of this sort, held at the Camps Elisis since 1980).

The latter is a gastronomical festivity focused on escargot cooking and is celebrated yearly at the end of May. "L'Aplec" gathers thousands of people around the table to taste the most traditional dishes from Lleida.

Due to its strong popularity, it was declared a traditional festivity of national interest in 2002 by the Generalitat of Catalonia and two years later it was also declared as such by the Spanish Government.

The main traditional celebrations in Lleida are chaired by the twelve emblematic "Gegants de la Paeria" (Giants of the Town Hall), the two oldest made in 1840.

Lleida has a bar and clubbing area, informally known as "Els Vins". The oldest part of the quarter, known as "Els Vins Vells", has been largely replaced by "Els Vins Nous", an architecturally newer and more upscale area. Most big clubs in Lleida are located outside the town and are not easily accessible without a car, though on Saturday nights there is a bus.


Lleida has been always a city with great sport tradition. Probably the most famous sport at the present time is basketball, because the club of basketball of the city a few years ago ascended to the ACB, being the revelation team in the league, this caused a lot of boys and girls to become fond of this little well-known sport.


Lleida has sister relationships with many places worldwide:

The city is the subject of the Catalan folk song "La Presó de Lleida", "The prison of Lleida", which was already attested in the 17th century and may be even older. It is a very popular tune, covered by many artists such as Joan Manuel Serrat.



</doc>
<doc id="18730" url="https://en.wikipedia.org/wiki?curid=18730" title="Le Mans">
Le Mans

Le Mans (, ) is a city in France on the Sarthe River. Traditionally the capital of the province of Maine, it is now the capital of the Sarthe department and the seat of the Roman Catholic diocese of Le Mans. Le Mans is a part of the Pays de la Loire region.

Its inhabitants are called "Manceaux" and "Mancelles". Since 1923, the city has hosted the internationally famous 24 Hours of Le Mans endurance sports car race.

First mentioned by Claudius Ptolemy, the Roman city "Vindinium" was the capital of the Aulerci, a sub tribe of the Aedui. Le Mans is also known as "Civitas Cenomanorum" (City of the Cenomani), or "Cenomanus". Their city, seized by the Romans in 47 BC, was within the ancient Roman province of Gallia Lugdunensis. A 3rd-century amphitheatre is still visible. The "thermae" were demolished during the crisis of the third century when workers were mobilized to build the city's defensive walls. The ancient wall around Le Mans is one of the most complete circuits of Gallo-Roman city walls to survive. 

As the use of the French language replaced late Vulgar Latin in the area, "Cenomanus", with dissimilation, became known as "Celmans." "Cel-" was taken to be a form of the French word for "this" and "that", and was replaced by "le", which means "the".

Gregory of Tours mentions a Frankish sub-king Rigomer, who was killed by King Clovis I in his campaign to unite the Frankish territories.

As the principal city of Maine, Le Mans was the stage for struggles in the eleventh century between the counts of Anjou and the dukes of Normandy. When the Normans had control of Maine, William the Conqueror successfully invaded England and established an occupation. In 1069 the citizens of Maine revolted and expelled the Normans, resulting in Hugh V being proclaimed count of Maine. Geoffrey V of Anjou married Matilda of England in the cathedral. Their son Henry II Plantagenet, king of England, was born here. In 1154, during the reign of his uncle King Stephen, Henry landed in England with an army, intent on challenging Stephen for the throne. Some of the members of that feudal force were known by the surname 'del Mans' (Latin for of Mans, as the city was then known.) In medieval records pertaining to the history of Gloucester is a reference to one such man, Walter del Mans, and beside his name 'Cenomanus' was added by the medieval scribe, so that there is no doubt as to Walter's origin. In the English censuses down to the twentieth century the surname Mans (latterly often spelled Manns) was virtually confined to the counties of Gloucestershire and Herefordshire and their borderlands, reflecting the original settlement patterns in the Welsh Marches of the original followers of Henry's from Le Mans in 1154. A John Mans/Manns was escheator of Hereford 1399-1400. One family from [Le] Mans held the manor of Dodenham, Worcestershire. (Calendar of the Records of the Corporation of Gloucester, Item 96, ca.1200; Fine Roles Henry III, 23 Aug. 1233 [Hereford];'Parishes: Doddenham', A History of the County of Worcester, volume 4 (1924), pp. 260–62.) Intercourse between England and Le Mans continued throughout the Angevin period.

Wilbur Wright began official public demonstrations of the airplane he had developed with his younger brother Orville on August 8, 1908, at the Hunaudières horse racing track near Le Mans.

Soon after Le Mans was liberated by the U.S. 79th and 90th Infantry Divisions on 8 August 1944, engineers of the Ninth Air Force IX Engineering Command began construction of a combat Advanced Landing Ground outside of the town. The airfield was declared operational on 3 September and designated as "A-35". It was used by several American fighter and transport units until late November of that year in additional offensives across France; the airfield was closed.


Le Mans has an oceanic climate influenced by the mild Atlantic air travelling inland. Summers are warm and occasionally hot, whereas winters are mild and cloudy. Precipitation is relatively uniform and moderate year round.

At the 1999 French census, there were 293,159 inhabitants in the metropolitan area ("aire urbaine") of Le Mans, with 146,105 of these living in the city proper (commune).

The Gare du Mans is the main railway station of Le Mans. It takes 1 hour to reach Paris from Le Mans by TGV high speed train. There are also TGV connections to Lille, Marseille, Nantes, Rennes and Brest. Gare du Mans is also a hub for regional trains. Le Mans inaugurated a new light rail system on 17 November 2007.

The first French Grand Prix took place on a 64-mile (103 km) circuit based at Le Mans in 1906.

Since the 1920s, the city has been best known for its connection with motorsports. There are two official and separate racing tracks at Le Mans, though they share certain portions. The smaller is the Bugatti Circuit (named after Ettore Bugatti, founder of the car company bearing his name), a relatively short permanent circuit, which is used for racing throughout the year and has hosted the French motorcycle Grand Prix. The longer and more famous Circuit de la Sarthe is composed partly of public roads. These are closed to the public when the track is in use for racing. Since 1923, this route has been used for the famous 24 Hours of Le Mans sports car endurance race. Boutiques and shops are set up during the race, selling merchandise and promoting products for cars.

The "Le Mans start" was formerly used in the 24-hour race: drivers lined up across the track from their cars, ran across the track, jumped into their cars and started them to begin the race.

The 1955 Le Mans disaster was a large accident during the race that killed eighty-four spectators.

The city is home to Le Mans Sarthe Basket, 2006 Champion of the LNB Pro A, France's top professional basketball division.

The team plays its home games at the Antarès, which served as one of the host arenas of the FIBA EuroBasket 1999.



Le Mans was the birthplace of:


Notable residents include:

Died in Le Mans:

Le Mans is twinned with:

The culinary specialty of Le Mans is "rillettes", a shredded pork pâté.

Located at Mayet near Le Mans, the Le Mans-Mayet transmitter has a height of 342 m and is one of the tallest radio masts in France.






</doc>
<doc id="18733" url="https://en.wikipedia.org/wiki?curid=18733" title="Lois McMaster Bujold">
Lois McMaster Bujold

Lois McMaster Bujold ( ; born November 2, 1949) is an American speculative fiction writer. She is one of the most acclaimed writers in her field, having won the Hugo Award for best novel four times, matching Robert A. Heinlein's record (not counting his Retro Hugos). Her novella "The Mountains of Mourning" won both the Hugo Award and Nebula Award. In the fantasy genre, "The Curse of Chalion" won the Mythopoeic Award for Adult Literature and was nominated for the 2002 World Fantasy Award for best novel, and both her fourth Hugo Award and second Nebula Award were for "Paladin of Souls". In 2011 she was awarded the Skylark Award. In 2013 she was awarded the Forry Award for Lifetime Achievement, named for Forrest J. Ackerman, by the Los Angeles Science Fantasy Society. She has won two Hugo Awards for Best Series, in 2017 for the Vorkosigan Saga and in 2018 for the Chalion series. The Science Fiction Writers of America named her its 36th SFWA Grand Master in 2019.

The bulk of Bujold's works comprises three separate book series: the Vorkosigan Saga, the Chalion series, and the Sharing Knife series.

Bujold is the daughter of Robert Charles McMaster and attributes her early interest in science fiction, as well as certain aspects of the Vorkosigan Saga, to his influence. He was editor of the "Nondestructive Testing Handbook".

Bujold writes that her experience growing up with a famous father is reflected in the same experience that her characters (Miles, Fiametta) have of growing up in the shadow of a "Great Man". Having observed this tendency in both genders, she wonders why it is always called "great man's son syndrome", and never "great man's daughter's syndrome." Her brother, an engineer like their father, helped provide technical details to support her writing of "Falling Free".

She has stated that she was always a "voracious reader". She started reading adult science fiction at the age of nine, picking up the habit from her father. She became a member of science fiction fandom, joined the Central Ohio Science Fiction Society, and co-published "StarDate", a science fiction fanzine in which a story of hers appeared under the byline Lois McMaster. Her reading tastes later expanded and she stated she now reads "history, mysteries, romance, travel, war, poetry, etc".

She attended Ohio State University from 1968 to 1972. While she was interested in writing, she didn't pursue an English major, feeling it was too concerned with literary criticism instead of literary creation.

She married John Fredric Bujold in 1971, but they divorced in the early 1990s. The marriage produced two children, a daughter named Anne (born 1979) and a son named Paul (born 1981). Anne Bujold is a Portland, Oregon metal artist, welder, and vice president of the Northwest Blacksmith Association. Bujold currently lives in Minneapolis, Minnesota.

Bujold had been friends with Lillian Stewart Carl since high school, where they "collaborated on extended story lines [but where] only a fragment of the total was written out.". At one point, she even co-produced a "Star Trek" zine called "StarDate" which she wrote for. In college, she wrote a Sherlock Holmes mystery as well. However, she stopped writing after that, being busy with marriage, family, and a career in hospital patient care.

It wasn't until her thirties that she returned to writing. Bujold has credited her friend Lillian Stewart Carl's first book sales with inspiring her to return to the field: "it occurred to me that if she could do it, I could do it too." She originally planned to write as a hobby again, but discovered the amount of work required was too much for anything other than a profession, so she decided to turn professional. With support from Carl and Patricia Wrede, she was able to complete her first novel.

Lois Bujold wrote three books ("Shards of Honor", "The Warrior's Apprentice" and "Ethan of Athos") before "The Warrior's Apprentice" was finally accepted, after four rejections. "The Warrior's Apprentice" was the first book purchased, though not the first Vorkosigan book written, nor would it be the first one to be published. On the strength of "The Warrior's Apprentice", Baen Books agreed to a three-book deal to include the two bracketing novels. By 2010, Baen Books claimed to have sold 2 million copies of Bujold's books.

Bujold is best known for her Vorkosigan Saga, a series of novels featuring Miles Vorkosigan, a physically impaired interstellar spy and mercenary admiral from the planet Barrayar, set approximately 1000 years in the future. The series also includes prequels starring Miles' parents, along with companion novels centered on secondary characters. Earlier titles are generally firmly in the space opera tradition with no shortage of battles, conspiracies, and wild twists, while in more recent volumes, Miles becomes more of a detective. In "A Civil Campaign", Bujold explores yet another genre: a high-society romance with a plot that pays tribute to Regency romance novelist Georgette Heyer (as acknowledged in the dedication). It centers on a catastrophic dinner party, with misunderstandings and dialogue justifying the subtitle "A Comedy of Biology and Manners".

The author has stated that the series structure is modeled after the Horatio Hornblower books, documenting the life of a single person. In themes and echoes, they also reflect Dorothy L. Sayers' mystery character Lord Peter Wimsey. Bujold has also said that part of the challenge of writing a series is that many readers will encounter the stories in "utterly random order", so she must provide sufficient background in each of them without being excessively repetitious. Most recent printings of her Vorkosigan tales do include an appendix at the end of each book, summarizing the internal chronology of the series.

Bujold has discussed her own views on the optimum reading order for the Vorkosigan series in her blog.

Bujold also wanted to break into the fantasy genre, but met with early setbacks. Her first foray into fantasy was "The Spirit Ring". She wrote the book "on spec", shopped it around, and found low offers, sending her back to Baen Books, where Jim Baen bought it for a fair price in exchange for the promise of more Vorkosigan books. Bujold called this experience very educational; the book received little critical acclaim, and had only mediocre sales.

She would not attempt to break into the fantasy market again for almost another decade, with "The Curse of Chalion". This book was also written on spec and offered up to a book auction. This time, she met with considerable critical and commercial success by tapping into a crossover market of fantasy and romance genre fans. The fantasy world of Chalion was first conceived as a result of a University of Minnesota course she was taking about medieval Spain in her spare time.

The next fantasy world she created was the tetralogy set in the universe of "The Sharing Knife", borrowing inspiration for its landscapes and for the dialect of the "farmers" from ones she grew up with in central Ohio. She writes that her first readers who helped proofread it said she got it exactly right and they could recognize Ohio features in the descriptions and dialects.

Bujold has generally been supportive of fan fiction written about her characters and universe. Amy H. Sturgis, in her essay "From Both Sides Now: Bujold and the Fan Fiction Phenomenon", notes that this is unusual for writers of Bujold's generation, most of whom are opposed to fan fiction. Sturgis relates this to Bujold's own production of "Star Trek" and Sherlock Holmes fan fiction early in her life, which Sturgis saw as an apprenticeship for her professional writing career.

Bujold herself ties her appreciation of fan fiction to her appreciation of "active" readers. To her, good readers are the "unsung collaborators" who make the story work, by actually constructing the world and characters in their heads. Books, to her, don't actually exist until they enter the reader's head and grow there. And sometimes, the characters and stories in a book grow so much that they escape the writer's original confines and become fan fiction. To Bujold, great literature is never "sterile", stopping with only what the original author wrote. She further believes that fan fiction gives authors a unique chance to see into the minds of those "invisible collaborators", the readers.

Despite this, she no longer reads fan fiction about her own characters due to legal and financial concerns, "fascinating as [she] finds it".

Wins

Nominations

Wins

Nominations

Best Science Fiction Novel

Best Fantasy Novel

Wins

Nominations




</doc>
<doc id="18735" url="https://en.wikipedia.org/wiki?curid=18735" title="Lycopene">
Lycopene

Lycopene (from the neo-Latin "Lycopersicum", the tomato species) is a bright red carotenoid hydrocarbon found in tomatoes and other red fruits and vegetables, such as red carrots, watermelons, grapefruits, and papayas, but it is not present in strawberries or cherries. Although lycopene is chemically a carotene, it has no vitamin A activity. Foods that are not red may also contain lycopene, such as asparagus and parsley.

In plants, algae, and other photosynthetic organisms, lycopene is an intermediate in the biosynthesis of many carotenoids, including beta-carotene, which is responsible for yellow, orange, or red pigmentation, photosynthesis, and photoprotection. Like all carotenoids, lycopene is a tetraterpene. It is insoluble in water. Eleven conjugated double bonds give lycopene its deep red color. Owing to the strong color, lycopene is useful as a food coloring (registered as E160d) and is approved for use in the USA, Australia and New Zealand (registered as 160d) and the European Union.

Lycopene is a symmetrical tetraterpene assembled from eight isoprene units. It is a member of the carotenoid family of compounds, and because it consists entirely of carbon and hydrogen, is also a carotene. Isolation procedures for lycopene were first reported in 1910, and the structure of the molecule was determined by 1931. In its natural, all-"trans" form, the molecule is long and straight, constrained by its system of 11 conjugated double bonds. Each extension in this conjugated system reduces the energy required for electrons to transition to higher energy states, allowing the molecule to absorb visible light of progressively longer wavelengths. Lycopene absorbs all but the longest wavelengths of visible light, so it appears red.

Plants and photosynthetic bacteria naturally produce all-"trans" lycopene. When exposed to light or heat, lycopene can undergo isomerization to any of a number of "cis"-isomers, which have a bent rather than linear shape. Different isomers were shown to have different stabilities due to their molecular energy (highest stability: 5-cis ≥ all-trans ≥ 9-cis ≥ 13-cis > 15-cis > 7-cis > 11-cis: lowest). In human blood, various "cis"-isomers constitute more than 60% of the total lycopene concentration, but the biological effects of individual isomers have not been investigated.

Carotenoids like lycopene are found in photosynthetic pigment-protein complexes in plants, photosynthetic bacteria, fungi, and algae. They are responsible for the bright orange–red colors of fruits and vegetables, perform various functions in photosynthesis, and protect photosynthetic organisms from excessive light damage. Lycopene is a key intermediate in the biosynthesis of carotenoids, such as beta-carotene, and xanthophylls.

Dispersed lycopene molecules can be encapsulated into carbon nanotubes enhancing their optical properties. Efficient energy transfer occurs between the encapsulated dye and nanotube — light is absorbed by the dye and without significant loss is transferred to the nanotube. Encapsulation increases chemical and thermal stability of lycopene molecules; it also allows their isolation and individual characterization.

The unconditioned biosynthesis of lycopene in eukaryotic plants and in prokaryotic cyanobacteria is similar, as are the enzymes involved. Synthesis begins with mevalonic acid, which is converted into dimethylallyl pyrophosphate. This is then condensed with three molecules of isopentenyl pyrophosphate (an isomer of dimethylallyl pyrophosphate), to give the 20-carbon geranylgeranyl pyrophosphate. Two molecules of this product are then condensed in a tail-to-tail configuration to give the 40-carbon phytoene, the first committed step in carotenoid biosynthesis. Through several desaturation steps, phytoene is converted into lycopene. The two terminal isoprene groups of lycopene can be cyclized to produce beta-carotene, which can then be transformed into a wide variety of xanthophylls.

Lycopene is the pigment in tomato sauces that turns plastic cookware orange. It is insoluble in plain water, but it can be dissolved in organic solvents and oils. Because of its non-polarity, lycopene in food preparations will stain any sufficiently porous material, including most plastics. To remove this staining, the plastics may be soaked in a solution containing a small amount of chlorine bleach.
The bleach oxidizes the lycopene, thus allowing the now-polarized metabolite to dissolve.

Absorption of lycopene requires that it be combined with bile salts and fat to form micelles. Intestinal absorption of lycopene is enhanced by the presence of fat and by cooking. Lycopene dietary supplements (in oil) may be more efficiently absorbed than lycopene from food.

Lycopene is not an essential nutrient for humans, but is commonly found in the diet mainly from dishes prepared from tomatoes. The median and 99th percentile of dietary lycopene intake have been estimated to be 5.2 and 123 mg/d, respectively.

Fruits and vegetables that are high in lycopene include autumn olive, gac, tomatoes, watermelon, pink grapefruit, pink guava, papaya, seabuckthorn, wolfberry (goji, a berry relative of tomato), and rosehip. Ketchup is a common dietary source of lycopene. Although gac ("Momordica cochinchinensis" Spreng) has the highest content of lycopene of any known fruit or vegetable (multiple times more than tomatoes), tomatoes and tomato-based sauces, juices, and ketchup account for more than 85% of the dietary intake of lycopene for most people. The lycopene content of tomatoes depends on variety and increases as the fruit ripens.

Unlike other fruits and vegetables, where nutritional content such as vitamin C is diminished upon cooking, processing of tomatoes increases the concentration of bioavailable lycopene. Lycopene in tomato paste is up to four times more bioavailable than in fresh tomatoes. Processed tomato products such as pasteurized tomato juice, soup, sauce, and ketchup contain a higher concentration of bioavailable lycopene compared to raw tomatoes.

Cooking and crushing tomatoes (as in the canning process) and serving in oil-rich dishes (such as spaghetti sauce or pizza) greatly increases assimilation from the digestive tract into the bloodstream. Lycopene is fat-soluble, so the oil is said to help absorption. Gac has high lycopene content derived mainly from its seed coats. Cara cara navel, and other citrus fruit, such as pink grapefruit, also contain lycopene. Some foods that do not appear red also contain lycopene, e.g., asparagus, which contains about 30 μg of lycopene per 100-g serving (0.3 μg/g) and dried parsley and basil, which contain around 3.5–7.0 μg/g of lycopene.

In humans, the Observed Safe Level for lycopene is 75 mg/day, according to one preliminary study.

Lycopene is non-toxic and commonly found in the diet, mainly from tomato products. There are cases of intolerance or allergic reaction to dietary lycopene, which may cause diarrhea, nausea, stomach pain or cramps, gas, and loss of appetite. Lycopene may increase the risk of bleeding when taken with anticoagulant drugs. Because lycopene may cause low blood pressure, interactions with drugs that affect blood pressure may occur. Lycopene may affect the immune system, the nervous system, sensitivity to sunlight, or drugs used for stomach ailments.

Lycopenemia is an orange discoloration of the skin that is observed with high intakes of lycopene. The discoloration is expected to fade after discontinuing excessive lycopene intake.

In a review of literature on lycopene and its potential role as a dietary antioxidant, the European Food Safety Authority concluded that evidence was insufficient for lycopene having antioxidant effects in humans, particularly in skin, heart function, or vision protection from ultraviolet light.

Although lycopene from tomatoes has been tested in humans for cardiovascular diseases and prostate cancer, no effect on any disease was found. The US Food and Drug Administration (FDA), in rejecting manufacturers' requests in 2005 to allow "qualified labeling" for lycopene and the reduction of various cancer risks, provided a conclusion that remains in effect :

"...no studies provided information about whether lycopene intake may reduce the risk of any of the specific forms of cancer. Based on the above, FDA concludes that there is no credible evidence supporting a relationship between lycopene consumption, either as a food ingredient, a component of food, or as a dietary supplement, and any of these cancers."

A 2017 review concluded that tomato products and lycopene supplementation had small positive effects on cardiovascular risk factors, such as elevated blood lipids and blood pressure. A 2010 review concluded that research has been insufficient to establish whether lycopene consumption affects human health. Lycopene has been studied in basic and clinical research for its potential effects on cardiovascular diseases and prostate cancer, although results through 2017 have not changed the prevailing FDA view that evidence of benefit remains inconclusive.



</doc>
<doc id="18737" url="https://en.wikipedia.org/wiki?curid=18737" title="Landau">
Landau

Landau, or Landau in der Pfalz, is an autonomous ("kreisfrei") town surrounded by the Südliche Weinstraße ("Southern Wine Route") district of southern Rhineland-Palatinate, Germany. It is a university town (since 1990), a long-standing cultural centre, and a market and shopping town, surrounded by vineyards and wine-growing villages of the Palatinate wine region. Landau lies east of the Palatinate forest, Europe's largest contiguous forest, on the German Wine Route.

It contains the districts ("Ortsteile") of Arzheim, Dammheim, Godramstein, Mörlheim, Mörzheim, Nussdorf, Queichheim, and Wollmesheim.

Landau was first mentioned as a settlement in 1106. It was in the possession of the counts of Leiningen-Dagsburg-Landeck, whose arms, differenced by an escutcheon of the Imperial eagle, served as the arms of Landau until 1955 . The town was granted a charter in 1274 by King Rudolf I of Germany, who declared the town a Free Imperial Town in 1291; nevertheless Prince-Bishop Emich of Speyer, a major landowner in the district, seized the town in 1324. The town did not regain its ancient rights until 1511 from Maximilian I. An Augustinian monastery was founded in 1276.

Landau was later part of France from 1680 to 1815, during which it was one of the "Décapole", the ten free cities of Alsace, and received its modern fortifications by Louis XIV's military architect Vauban in 1688–99, making the little town (population in 1789 was still only approximately 5,000) one of Europe's strongest citadels. In the War of the Spanish Succession it had four Sieges. After the Siege in 1702 that was lost by the French, an Imperial garrison was installed in Landau. After the 2nd Siege from 13 October to 15 November 1703 the French had regained the city, caused by their victory in the Battle of Speyerbach. The 3rd Siege began on 12 September 1704 by Louis, Margrave of Baden-Baden, and ended on 23 November 1704 with the French defeat. During this siege King Joseph I arrived at Landau coming from Vienna in a newly developed convertible carriage. It became very popular, named Landau in English, or in German. The French got Landau back after the 4th Siege which lasted from 6 June to 20 August 1713 by Marshal General Villars.

Landau was part of Bas-Rhin department between 1789 and 1815. After Napoleon's Hundred Days following his escape from Elba, Landau, which had remained French, was granted to the Kingdom of Bavaria in 1815 and became the capital of one of the thirteen "Bezirksämter" (counties) of the Bavarian Rheinkreis, later renamed Pfalz.

In 1840 famous political cartoonist Thomas Nast was born in Landau.

Following World War II, Landau was an important barracks town for the French occupation.

Landau's large main square ("Rathausplatz") is dominated by the town hall ("Rathaus") and the market hall ("Altes Kaufhaus"). In the 19th century, the former fortifications gave way to a ring road that encircles the old town centre, from which the old industrial buildings have been excluded. A convention hall, the "Festhalle", was built in Art Nouveau style, 1905–07 on a rise overlooking the town park and facing the modernist "Bundesamt", the regional government building.
The Protestant Collegiate Church () in Landau in der Pfalz is one of the oldest buildings in the town. With the construction of the church started in the 14th century, was completed in the mid-16th century.

The zoo is located close to the center of Landau alongside the historical fortifications. Animals are held in natural enclosures. The zoo contains numerous exotic species such as tigers and cheetahs, but also seals, penguins, kangaroos and flamingos and many more.

Wine-making continues to be an important industry of Landau.

The "landau," a luxury open carriage with a pair of folding tops, was invented in the town.

A frequent Ashkenazi surname originates in this town. Probably its most famous bearer was Yechezkel Landau, an 18th-century talmudist and halakhist and the chief rabbi of Prague.



</doc>
<doc id="18738" url="https://en.wikipedia.org/wiki?curid=18738" title="Liv Ullmann">
Liv Ullmann

Liv Johanne Ullmann (born 16 December 1938) is a Norwegian actress and film director. She was one of the "muses" of Swedish director Ingmar Bergman.

Ullmann won a Golden Globe Award for Best Actress – Motion Picture Drama in 1972 for the film "The Emigrants" (1971), and has been nominated for another four. In 2000, she was nominated for the Palme d'Or for her second directorial feature film, "Faithless". She has also received two BAFTA Award nominations for her performances in "Scenes from a Marriage" (1973) and "Face to Face" (1976), and two Academy Award nominations for "The Emigrants" and "Face to Face".

Ullmann was born in Tokyo, Japan, the daughter of Erik Viggo Ullmann (1907–1945), a Norwegian aircraft engineer who was working in Tokyo at the time, and Janna Erbe (née Lund; 1910–1996), also Norwegian. Her grandfather was sent to the Dachau concentration camp during the Second World War for helping Jews escape from the town where he lived in Norway; he died in the camp. When she was two years old, the family moved to Toronto, Ontario, where her father worked at the Norwegian air force base on Toronto Island (in Lake Ontario) during the Second World War. The family moved to New York, where four years later, her father died of a brain tumor, an event that affected her greatly. Her mother worked as a bookseller, while raising two daughters. They eventually returned to Norway, settling in Trondheim.

Ullmann began her acting career as a stage actress in Norway during the mid-1950s. She continued to act in theatre for most of her career, and became noted for her portrayal of Nora in Henrik Ibsen's play "A Doll's House", but became better known once she started to work with Swedish movie director Ingmar Bergman. She later acted, with acclaim, in 10 of his movies, including "Persona" (1966), "The Passion of Anna" (1969), "Cries and Whispers" (1972), and "Autumn Sonata" (1978), in the last of which her co-actress Ingrid Bergman resumed her own Swedish cinema career. She co-acted often with Swedish actor and fellow Bergman collaborator Erland Josephson, with whom she made the Swedish television drama "Scenes from a Marriage" (1973), which was also edited to feature-movie length and distributed theatrically. Ullmann acted with Laurence Olivier in "A Bridge Too Far" (1977), directed by Richard Attenborough.

Nominated more than 40 times for awards, including various lifetime achievement awards, she won the best actress prize three times from the National Society of Film Critics, three times from the National Board of Review, received three awards from the New York Film Critics Circle, and a Golden Globe. During 1971, Ullmann was nominated for an Academy Award for Best Actress for the movie "The Emigrants", and again during 1976 for the movie "Face to Face".

Ullmann made her New York City stage debut in 1975 also in "A Doll's House". Appearances in "Anna Christie" and "Ghosts" followed, as well as the less than successful musical version of "I Remember Mama". This show, composed by Richard Rodgers, experienced numerous revisions during a long preview period, then closed after 108 performances. She also featured in the widely deprecated musical movie remake of "Lost Horizon" during 1973. In 1977, when she appeared on Broadway at the Imperial Theatre in Eugene O'Neill's "Anna Christie", she "glowed with despair and hope, and was everything one could have wished her to have been" in a performance "not to be missed and never to be forgotten", with her "grace and authority" that was "perhaps more than Garbo...born for Anna Christie:--Or more properly, Anna Christie was born for her." (Clive Barnes (1977). "Theater: Liv Ullman's 'Anna Christie'." "The New York Times", 15 April 1977)

In 1980, Brian De Palma, who directed "Carrie", wanted Liv Ullmann to play the role of Kate Miller in the erotic crime thriller "Dressed to Kill" and offered it to her, but she declined because of the violence. The role subsequently went to Angie Dickinson. In 1982 Ingmar Bergman wanted Ullmann to play Emelie Ekdahl in his last feature film, "Fanny and Alexander", and wrote the role with this in mind. She declined it, feeling the role was too sad. She later stated in interviews that turning it down was one of the few things she really regretted.

During 1984, she was chairperson of the jury at the 34th Berlin International Film Festival, and during 2002 chaired the jury of the Cannes Film Festival. She introduced her daughter, Linn Ullmann, to the audience with the words: "Here comes the woman whom Ingmar Bergman loves the most". Her daughter was there to receive the Prize of Honour on behalf of her father; she would return to serve the jury herself during 2011.

In 2003, Ullmann reprised her role for "Scenes from a Marriage" in "Saraband" (2003), Bergman's final telemovie. Her previous screen role had been in the Swedish movie "Zorn" (1994).

In 2004, Ullmann revealed that she had received an offer in November 2003 to play in three episodes of the popular American series, "Sex and the City". She was amused by the offer, and said that it was one of the few programs she regularly watched, but she turned it down. Later that year, Steven Soderbergh wrote a role in the movie "Ocean's 12" especially for her, but she also turned that down.

Ullmann narrated the Canada–Norway co-produced animated short movie "The Danish Poet" (2006), which won the Academy Award for Animated Short Film at the 79th Academy Awards during 2007. 

In 2008, she was the head of the jury at the 30th Moscow International Film Festival.

She published two autobiographies, "Changing" (1977) and "Choices" (1984).

During 2012, she attended the International Indian Film Academy Awards in Singapore, where she was honored for her "Outstanding Contributions to International Cinema" and she also showed her movie on her relationship with Ingmar Bergman.

Ullmann's first film as a director was "Sofie" (1992); her friend and former co-actor, Erland Josephson, starred on it. She later directed the Bergman-composed movie "Faithless" (2000). "Faithless" garnered nominations for both the Palme d'Or and Best Actress category at the Cannes Film Festival.

During 2006, Ullmann announced that she had been forced to end her longtime wish of making a film based on "A Doll's House". According to her statement, the Norwegian Film Fund was preventing her and writer Kjetil Bjørnstad from pursuing the project. Australian actress Cate Blanchett and British actress Kate Winslet had been cast intended in the main roles of the movie. She later directed Blanchett in the play "A Streetcar Named Desire", by Tennessee Williams, at the Sydney Theatre Company in Sydney, which was performed September through October 2009, and then continued from 29 October to 21 November 2009 at the John F. Kennedy Center for the Performing Arts in Washington, D.C., where it won a Helen Hayes Award for Outstanding Non-resident Production as well as actress and supporting performer for 2009. The play was also performed at the Brooklyn Academy of Music in Brooklyn, New York.

In 2013, Ullmann directed a film adaptation of "Miss Julie". The film, released in September 2014, stars Jessica Chastain, Colin Farrell, and Samantha Morton. It was widely praised by the Norwegian press.

In addition to Norwegian, Ullmann speaks Swedish, English, and other European languages.

She had a romantic relationship with Ingmar Bergman (1965–1970). Writer Linn Ullmann (b. 1966) is their daughter.

Following an affair with the actor John Lithgow, Ullman married Boston real estate developer Donald Saunders in 1985, and they remain married.

She is a UNICEF Goodwill Ambassador, and has traveled widely for the organization. She is also co-founder and honorary chair of the International Rescue Committee's Women's Refugee Commission.<br> In 2005, King Harald V of Norway made Ullmann a Commander with Star of the Order of St. Olav.

She received an honorary degree in 2006, Doctor of Philosophy, from the Norwegian University of Science and Technology.







</doc>
<doc id="18739" url="https://en.wikipedia.org/wiki?curid=18739" title="Laika">
Laika

Laika (; c. 1954 – 3 November 1957) was a Soviet space dog who became one of the first animals in space, and the first animal to orbit the Earth. Laika, a stray mongrel from the streets of Moscow, was selected to be the occupant of the Soviet spacecraft Sputnik 2 that was launched into outer space on 3 November 1957.

Little was known about the impact of spaceflight on living creatures at the time of Laika's mission, and the technology to de-orbit had not yet been developed, so Laika's survival was never expected. Some scientists believed humans would be unable to survive the launch or the conditions of outer space, so engineers viewed flights by animals as a necessary precursor to human missions. The experiment aimed to prove that a living passenger could survive being launched into orbit and endure a micro-g environment, paving the way for human spaceflight and providing scientists with some of the first data on how living organisms react to spaceflight environments.

Laika died within hours from overheating, possibly caused by a failure of the central R-7 sustainer to separate from the payload. The true cause and time of her death were not made public until 2002; instead, it was widely reported that she died when her oxygen ran out on day six or, as the Soviet government initially claimed, she was euthanised prior to oxygen depletion.

On 11 April 2008, Russian officials unveiled a monument to Laika. A small monument in her honour was built near the military research facility in Moscow that prepared Laika's flight to space. It portrayed a dog standing on top of a rocket. She also appears on the Monument to the Conquerors of Space in Moscow.

After the success of Sputnik 1 in October 1957, Nikita Khrushchev, the Soviet leader, wanted a spacecraft launched on 7 November 1957, the 40th anniversary of the October Revolution. Construction had already started on a more sophisticated satellite, but it would not be ready until December; this satellite would later become Sputnik 3.

Meeting the November deadline meant building a new craft. Khrushchev specifically wanted his engineers to deliver a "space spectacular", a mission that would repeat the triumph of Sputnik 1, stunning the world with Soviet prowess. Planners settled on an orbital flight with a dog. Soviet rocket engineers had long intended a canine orbit before attempting human spaceflight; since 1951, they had lofted twelve dogs into sub-orbital space on ballistic flights, working gradually toward an orbital mission set for some time in 1958. To satisfy Khrushchev's demands, they expedited the orbital canine flight for the November launch.

According to Russian sources, the official decision to launch Sputnik 2 was made on 10 or 12 October, leaving less than four weeks to design and build the spacecraft. Sputnik 2, therefore, was something of a rush job, with most elements of the spacecraft being constructed from rough sketches. Aside from the primary mission of sending a living passenger into space, Sputnik 2 also contained instrumentation for measuring solar irradiance and cosmic rays.

The craft was equipped with a life-support system consisting of an oxygen generator and devices to avoid oxygen poisoning and to absorb carbon dioxide. A fan, designed to activate whenever the cabin temperature exceeded , was added to keep the dog cool. Enough food (in a gelatinous form) was provided for a seven-day flight, and the dog was fitted with a bag to collect waste. A harness was designed to be fitted to the dog, and there were chains to restrict her movements to standing, sitting, or lying down; there was no room to turn around in the cabin. An electrocardiogram monitored heart rate and further instrumentation tracked respiration rate, maximum arterial pressure, and the dog's movements.

Laika was found as a stray wandering the streets of Moscow. Soviet scientists chose to use Moscow strays since they assumed that such animals had already learned to endure conditions of extreme cold and hunger. This specimen was a mongrel female, approximately three years old. Another account reported that she weighed about . Soviet personnel gave her several names and nicknames, among them Kudryavka (Russian for "Little Curly"), Zhuchka ("Little Bug"), and Limonchik ("Little Lemon"). Laika, the Russian name for several breeds of dogs similar to the husky, was the name popularised around the world. The American press dubbed her Muttnik ("mutt" + suffix "-nik") as a pun on Sputnik, or referred to her as "Curly". Her true pedigree is unknown, although it is generally accepted that she was part husky or other Nordic breed, and possibly part terrier. NASA refers to Laika as a "part-Samoyed terrier." A Russian magazine described her temperament as phlegmatic, saying that she did not quarrel with other dogs. Vladimir Yazdovsky, who led the program of test dogs used on rockets, in a later publication wrote that “Laika was quiet and charming”.

The Soviet Union and United States had previously sent animals only on sub-orbital flights. Three dogs were trained for the Sputnik 2 flight: Albina, Mushka, and Laika. Soviet space-life scientists Vladimir Yazdovsky and Oleg Gazenko trained the dogs.

To adapt the dogs to the confines of the tiny cabin of Sputnik 2, they were kept in progressively smaller cages for periods of up to 20 days. The extensive close confinement caused them to stop urinating or defecating, made them restless, and caused their general condition to deteriorate. Laxatives did not improve their condition, and the researchers found that only long periods of training proved effective. The dogs were placed in centrifuges that simulated the acceleration of a rocket launch and were placed in machines that simulated the noises of the spacecraft. This caused their pulses to double and their blood pressure to increase by 30–65 torr. The dogs were trained to eat a special high-nutrition gel that would be their food in space.

Before the launch, one of the mission scientists took Laika home to play with his children. In a book chronicling the story of Soviet space medicine, Dr. Vladimir Yazdovsky wrote, "Laika was quiet and charming ... I wanted to do something nice for her: She had so little time left to live."

Vladimir Yazdovsky made the final selection of dogs and their designated roles. Laika was to be the "flight dog"—a sacrifice to science on a one-way mission to space. Albina, who had already flown twice on a high-altitude test rocket, was to act as Laika's backup. The third dog Mushka was a "control dog"—she was to stay on the ground and be used to test instrumentation and life support.

Before leaving for the Baikonur Cosmodrome, Yazdovsky and Gazenko conducted surgery on the dogs, routing the cables from the transmitters to the sensors that would measure breathing, pulse, and blood pressure.

Because the existing airstrip at Turatam near the cosmodrome was small, the dogs and crew had to be first flown aboard a Tu-104 plane to Tashkent. From there, a smaller and lighter Il-14 plane took them to Turatam. Training of dogs continued upon arrival; one after another they were placed in the capsules to get familiar with the feeding system.

According to a NASA document, Laika was placed in the capsule of the satellite on 31 October 1957—three days before the start of the mission. At that time of year, the temperatures at the launch site were extremely cold, and a hose connected to a heater was used to keep her container warm. Two assistants were assigned to keep a constant watch on Laika before launch. Just prior to liftoff on 3 November 1957, from Baikonur Cosmodrome, Laika's fur was sponged in a weak alcohol solution and carefully groomed, while iodine was painted onto the areas where sensors would be placed to monitor her bodily functions.

One of the technicians preparing the capsule before final liftoff stated that "after placing Laika in the container and before closing the hatch, we kissed her nose and wished her bon voyage, knowing that she would not survive the flight."

The exact time of the liftoff varies from source to source and is mentioned as 05:30:42 Moscow Time or 07:22 Moscow Time.

At peak acceleration Laika's respiration increased to between three and four times the pre-launch rate. The sensors showed her heart rate was 103 beats/min before launch and increased to 240 beats/min during the early acceleration. After reaching orbit, Sputnik 2's nose cone was jettisoned successfully; however the "Block A" core did not separate as planned, preventing the thermal control system from operating correctly. Some of the thermal insulation tore loose, raising the cabin temperature to . After three hours of weightlessness, Laika's pulse rate had settled back to 102 beats/min, three times longer than it had taken during earlier ground tests, an indication of the stress she was under. The early telemetry indicated that Laika was agitated but eating her food. After approximately five to seven hours into the flight, no further signs of life were received from the spacecraft.

The Soviet scientists had planned to euthanise Laika with a poisoned serving of food. For many years, the Soviet Union gave conflicting statements that she had died either from asphyxia, when the batteries failed, or that she had been euthanised. Many rumours circulated about the exact manner of her death. In 1999, several Russian sources reported that Laika had died when the cabin overheated on the fourth orbit. In October 2002, Dimitri Malashenkov, one of the scientists behind the Sputnik 2 mission, revealed that Laika had died by the fourth circuit of flight from overheating. According to a paper he presented to the World Space Congress in Houston, Texas, "It turned out that it was practically impossible to create a reliable temperature control system in such limited time constraints."

Over five months later, after 2,570 orbits, Sputnik 2—including Laika's remains—disintegrated during re-entry on 14 April 1958.

Due to the overshadowing issue of the Soviet versus U.S. Space Race, the ethical issues raised by this experiment went largely unaddressed for some time. As newspaper clippings from 1957 show, the press was initially focused on reporting the political perspective, while the health and retrieval—or lack thereof—of Laika only became an issue later.

Sputnik 2 was not designed to be retrievable, and Laika had always been intended to die. The mission sparked a debate across the globe on the mistreatment of animals and animal testing in general to advance science. In the United Kingdom, the National Canine Defence League called on all dog owners to observe a minute's silence, while the Royal Society for the Prevention of Cruelty to Animals (RSPCA) received protests even before Radio Moscow had finished announcing the launch. Animal rights groups at the time called on members of the public to protest at Soviet embassies. Others demonstrated outside the United Nations in New York. These protests were largely stirred up and instrumentalized as an ideological struggle by various interest groups. Laboratory researchers in the U.S. offered some support for the Soviets, at least before the news of Laika's death.

In the Soviet Union, there was less controversy. Neither the media, books in the following years, nor the public openly questioned the decision to send a dog into space. In 1998, after the collapse of the Soviet regime, Oleg Gazenko, one of the scientists responsible for sending Laika into space, expressed regret for allowing her to die:
In other Warsaw Pact countries, open criticism of the Soviet space program was difficult because of political censorship, but there were notable cases of criticism in Polish scientific circles. A Polish scientific periodical, ""Kto, Kiedy, Dlaczego"" ("Who, When, Why"), published in 1958, discussed the mission of Sputnik 2. In the periodical's section dedicated to astronautics, Krzysztof Boruń described the Sputnik 2 mission as "regrettable" and criticised not bringing Laika back to Earth alive as "undoubtedly a great loss for science".

Laika is memorialised in the form of a statue and plaque at Star City, Russia, the Russian Cosmonaut training facility. Created in 1997, Laika is positioned behind the cosmonauts with her ears erect. The Monument to the Conquerors of Space, constructed in 1964, also includes Laika. On 11 April 2008 at the military research facility where staff had been responsible for readying Laika for the flight, officials unveiled a monument of her poised on top of a space rocket. Stamps and envelopes picturing Laika were produced, as well as branded cigarettes and matches.

Future space missions carrying dogs would be designed to be recovered. Four other dogs died in Soviet space missions: Bars and Lisichka were killed when their R-7 rocket exploded shortly after launch on 28 July 1960; Pchyolka and Mushka died when Korabl-Sputnik 3 was purposely destroyed with an explosive charge to prevent foreign powers from inspecting the capsule after a wayward atmospheric reentry trajectory on 1 December 1960.

Although never shown, Laika is prominently mentioned in the 1985 film "My Life as a Dog", in which the main character (a young Swedish boy in the late 1950s) identifies strongly with the dog. "Laika", a 2007 graphic novel by Nick Abadzis giving a fictionalized account of Laika's life, won the Eisner Award for Best Publication for Teens. Laika is also mentioned in the 2004 song "Neighborhood #2 (Laika)" by Arcade Fire, included on their debut album "Funeral". "Lajka" (in English: "Laika") is a 2017 Czech animated science fiction comedy film inspired by Laika.





</doc>
<doc id="18756" url="https://en.wikipedia.org/wiki?curid=18756" title="Master of Puppets">
Master of Puppets

Master of Puppets is the third studio album by American heavy metal band Metallica, released on March 3, 1986, by Elektra Records. Recorded at Sweet Silence Studios with producer Flemming Rasmussen, it was the band's last album to feature bassist Cliff Burton, who died in a bus accident in Sweden during the album's promotional tour.

The album's artwork, designed by Metallica and Peter Mensch and painted by Don Brautigam, depicts a cemetery field of white crosses tethered to strings, manipulated by a pair of hands in a blood-red sky. Instead of releasing a single or video in advance of the album's release, Metallica embarked on a five-month American tour in support of Ozzy Osbourne. The European leg was canceled after Burton's death in September 1986, and the band returned home to audition a new bassist.

"Master of Puppets" peaked at number 29 on the "Billboard" 200, and received widespread acclaim from critics, who praised its music and political lyrics. Today, it is widely considered to be one of the greatest heavy metal albums of all time, and is credited with consolidating the American thrash metal scene. It was certified six times platinum by the RIAA in 2003 for shipping six million copies in the United States, and was later certified six times platinum by Music Canada and gold by the BPI. In 2015, "Master of Puppets" became the first metal recording to be selected by the Library of Congress for preservation in the National Recording Registry for being "culturally, historically, or aesthetically significant."

Metallica's 1983 debut "Kill 'Em All" laid the foundation for thrash metal with its aggressive musicianship and vitriolic lyrics. The album revitalized the American underground scene, and inspired similar records by contemporaries. The band's second album "Ride the Lightning" extended the limits of the genre with its more sophisticated songwriting and improved production. The album caught the attention of Elektra Records representative Michael Alago, who signed the group to an eight-album deal in the fall of 1984, halfway through the album's promotional tour. Elektra reissued "Ride the Lightning" on November 19, and the band began touring larger venues and festivals throughout 1985. After parting with manager Jon Zazula, Metallica hired Q Prime executives Cliff Burnstein and Peter Mensch. During the summer, the band played the Monsters of Rock festival at Castle Donington, alongside Bon Jovi and Ratt to an audience of 70,000. 

Metallica was motivated to make an album that would impress critics and fans, and began writing new material in mid-1985. Lead vocalist and rhythm guitarist James Hetfield and drummer Lars Ulrich were the main songwriters on the album, already titled "Master of Puppets". The two developed ideas at a garage in El Cerrito, California, before inviting bassist Cliff Burton and guitarist Kirk Hammett for rehearsals. Hetfield and Ulrich described the songwriting process as starting with "guitar riffs, assembled and reassembled until they start to sound like a song". After that, the band came up with a song title and topic, and Hetfield wrote lyrics to match the title. "Master of Puppets" is Metallica's first album not to feature songwriting contributions from former lead guitarist Dave Mustaine. Mustaine claimed he had co-written "Leper Messiah", based on an old song called "The Hills Ran Red". The band denied this, but stated that one section incorporated Mustaine's ideas.

The band was not satisfied with the acoustics of the American studios they considered, and decided to record in Ulrich's native Denmark. Ulrich took drum lessons, and Hammett worked with Joe Satriani to learn how to record more efficiently. Ulrich was in talks with Rush's bassist and vocalist Geddy Lee to produce the album, but the collaboration never materialized because of uncoordinated schedules. Metallica recorded the album with producer Flemming Rasmussen at Sweet Silence Studios in Copenhagen, Denmark, from September 1 to December 27, 1985. The writing of all the songs except "Orion" and "The Thing That Should Not Be" was completed before the band's arrival in Copenhagen. Rasmussen stated that the band brought well-prepared demos of the songs, and only slight changes were made to the compositions in the studio. The recording took longer than the previous album because Metallica had developed a sense of perfectionism and had higher ambitions. Metallica eschewed the slick production and synthesizers of contemporary hard rock and glam metal albums. With a reputation for drinking, the band stayed sober on recording days. Hammett recalled that the group was "just making another album" at the time and "had no idea that the record would have such a range of influence that it went on to have". He also said that the group was "definitely peaking" at the time and that the album had "the sound of a band really gelling, really learning how to work well together".

Rasmussen and Metallica did not manage to complete the mixtapes as planned. Instead, the multitrack recordings were sent in January 1986 to Michael Wagener, who finished the album's mixing. The cover was designed by Metallica and Peter Mensch and painted by Don Brautigam. It depicts a cemetery field of white crosses tethered to strings, manipulated by a pair of hands in a blood-red sky. Ulrich explained that the artwork summarized the lyrical content of the album—people being subconsciously manipulated. The original artwork was sold at Rockefeller Plaza, New York City for $28,000 in 2008. The band mocked the warning stickers promoted by the PMRC with a facetious Parental Advisory label on the cover: "The only track you probably won't want to play is 'Damage, Inc.' due to the multiple use of the infamous 'F' word. Otherwise, there aren't any 'shits', 'fucks', 'pisses', 'cunts', 'motherfuckers', or 'cocksuckers' anywhere on this record".

The album was recorded with the following equipment: Hammett's guitars were a 1974 Gibson Flying V, a Jackson Randy Rhoads, and a Fernandes Stratocaster copy; Hetfield used a Jackson King V played through a Mesa Boogie Mark IIC+ amplifier modified as a pre-amp; Burton played an Aria Pro II SB1000 through Mesa Boogie amplifier heads and cabinets; Ulrich played Tama drum equipment, and borrowed a rare S.L.P. Black Brass from Def Leppard drummer Rick Allen.

"Master of Puppets" features dynamic music and thick arrangements. Metallica delivered a more refined approach and performance compared to the previous two albums, with multilayered songs and technical dexterity. This album and its predecessor "Ride the Lightning" follow a similar track sequencing: both open with an up-tempo song with an acoustic intro, followed by a lengthy title track, and a fourth track with ballad qualities. Although both albums are similarly structured, the musicianship on "Master of Puppets" is more powerful and epic in scope, with tight rhythms and delicate guitar solos. According to music writer Joel McIver, "Master of Puppets" introduced a new level of heaviness and complexity in thrash metal, displaying atmospheric and precisely executed songs. Hetfield's vocals had matured from the hoarse shouting of the first two albums to a deeper, in-control yet aggressive style. The songs explore themes such as control and the abuse of power. The lyrics describe the consequences of alienation, oppression, and feelings of powerlessness. Author Ryan Moore thought the lyrics depicted "ominous yet unnamed forces of power wielding total control over helpless human subjects". The lyrics were considered perceptive and harrowing, and were praised for being honest and socially conscious by writer Brock Helander. Referring to the epic proportions of the songs, BBC Music's Eamonn Stack stated that "at this stage in their careers Metallica weren't even doing songs, they were telling stories". The compositions and arrangements benefited from Burton's classical training and understanding of harmony.

"Battery" refers to angry violence, as in the term "assault and battery". Some critics contended that the title actually refers to an artillery battery, and interpreted it as "Hetfield of a war tactic as the aggressor" personifying destruction. The song begins with bass-heavy acoustic guitars that build upon multitracked layers until they are joined by a sonic wall of distorted electric guitars. It then breaks into fast, aggressive riffing featuring off-beat rhythms and heavily distorted minor dyads where root-fifth power chords might be expected. Hetfield improvised the riff while relaxing in London. "Master of Puppets" consists of several riffs with odd meters and a cleanly picked middle section with melodic solo. The song shares a similar structure with "The Four Horsemen" from the band's first album: two verse-chorus sets lead to a lengthy interlude to another verse-chorus set. The opening and pre-verse sections feature fast downstroked chromatic riffing at around 220 beats per minute in mostly time. Every fourth bar of each verse and the outro is cut short by more than a beat; the time signature of these bars is often inaccurately analyzed as being , but it is closer to . A lengthy interlude follows the second chorus, beginning with a clean, arpeggiated section over which Hetfield contributes a melodic solo; the riffing becomes distorted and progressively more heavy and Hammett provides a more virtuosic solo before the song eventually returns to the main verse. The song closes with a fade-out of sinister laughter. The lyrical theme is cocaine addiction.

"The Thing That Should Not Be" was inspired by the Cthulhu Mythos created by famed horror writer H.P. Lovecraft, with notable direct references to "The Shadow Over Innsmouth" and to Cthulhu himself, who is the subject matter of the song's chorus. It is considered the heaviest track on the album, with the main riff emulating a beast dragging itself into the sea. The Black Sabbath-influenced guitars are down-tuned, creating slow and moody ambience. "Welcome Home (Sanitarium)" was based on Ken Kesey's novel "One Flew Over the Cuckoo's Nest" and conveys the thoughts of a patient unjustly caged in a mental institution. The song opens with a section of clean single strings and harmonics. The clean, arpeggiated main riff is played in alternating and time signatures. The song is structured with alternating somber clean guitars in the verses, and distorted heavy riffing in the choruses, unfolding into an aggressive finale. This structure follows a pattern of power ballads Metallica set with "Fade to Black" on "Ride the Lightning" and would revisit with "One" on "...And Justice for All".

"Disposable Heroes" is an anti-war song about a young soldier whose fate is controlled by his superiors. With sections performed at 220 beats per minute, it is one of the most intense tracks on the record. The guitar passage at the end of each verse was Hammett's imitation of the sort of music he found in war films. The syncopated riffing of "Leper Messiah" challenges the hypocrisy of the televangelism that emerged in the 1980s. The song describes how people are willingly turned into blind religious followers who mindlessly do whatever they are told. The 136 beats per minute mid-tempo riffing of the verses culminates in a descending chromatic riff in the chorus; it increases to a galloping 184 beats per minute for the middle section that climaxes in a distorted scream of "Lie!". The title derives from the lyrics to the David Bowie song "Ziggy Stardust". "Orion" is a multipart instrumental highlighting Burton's bass playing. It opens with a fade-in bass section, heavily processed to resemble an orchestra. It continues with mid-tempo riffing, followed by a bass solo at half-tempo. The tempo accelerates during the latter part, and ends with music fading out. Burton arranged the middle section, which features its moody bass line and multipart guitar harmonies. "Damage, Inc." rants about senseless violence and reprisal at an unspecified target. It starts with a series of reversed bass chords based on the chorale prelude of Bach's "Come, Sweet Death". The song then jumps into a rapid rhythm with a pedal-point riff in E that Hammett says was influenced by Deep Purple.

"Master of Puppets" was hailed as a masterpiece by critics outside of the heavy metal audience and cited by some as the genre's greatest album. In a contemporary review, Tim Holmes of "Rolling Stone" asserted that the band had redefined heavy metal with the technical skill and subtlety showcased on the album, which he described as "the sound of global paranoia". "Kerrang!" wrote that "Master of Puppets" "finally put Metallica into the big leagues where they belong". Editor Tom King said Metallica was at an "incredible song-writing peak" during the recording sessions, partially because Burton contributed to the songwriting. By contrast, "Spin" magazine's Judge I-Rankin was disappointed with the album and said, although the production is exceptional and Metallica's experimentation is commendable, it eschews the less "intellectual" approach of "Kill 'Em All" for a MDC-inspired direction that is inconsistent.

In a retrospective review, AllMusic's Steve Huey viewed "Master of Puppets" as Metallica's best album and remarked that, although it was not as unexpected as "Ride the Lightning", it is a more musically and thematically consistent album. Greg Kot of the "Chicago Tribune" said the songs were the band's most intense at that point, and veer toward "the progressive tendency of Rush." Adrien Begrand of PopMatters praised the production as "a metal version of Phil Spector's Wall of Sound" and believed none of Metallica's subsequent albums could match its passionate and intense musical quality. BBC Music's Eamonn Stack called the album "hard, fast, rock with substance" and likened the songs to stories of "biblical proportions". Canadian journalist Martin Popoff compared the album to "Ride the Lightning" and found "Master of Puppets" not a remake, though similar in "awesome power and effect". Robert Christgau was more critical. Writing in "" (1990), he said the band's energy and political motivations are respectable, but the music evokes clichéd images of "revolutionary heroes" who are "male chauvinists too inexperienced to know better". The philosopher Roger Scruton, a critic of contemporary pop music, has praised Metallica, and in particular this album, writing, "I have actually been listening to quite a bit of heavy metal lately, and Metallica, I think, is genuinely talented. Master of Puppets I think has got something genuinely both poetic – violently poetic – and musical. Every now and then something like that stands out."

Released on March 3, 1986, the album had a 72-week run on the "Billboard" 200 album charts and earned the band its first gold certification. The album debuted on March 29, 1986, at number 128 and peaked at number 29 on the "Billboard" 200 chart. "Billboard" reported that 300,000 copies were sold in its first three weeks. More than 500,000 copies were sold in its first year, even with virtually no radio airplay and no music videos. In 2003, "Master of Puppets" was certified 6× platinum by the Recording Industry Association of America (RIAA), with six million copies shipped in the United States. Between the beginning of the Nielsen SoundScan era in 1991 and 2009, 4,578,000 copies were sold. The album was less successful on an international level, despite entering the top 5 on the Finnish and the top 40 on the German and Swiss album charts in its inaugural year. In 2004, it peaked within the top 15 in Sweden. In 2008, the album reached the top 40 on the Australian and Norwegian album charts. It received 6× platinum certification from Music Canada and a golden award from the British Phonographic Industry (BPI) for shipments of 600,000 and 100,000 copies, respectively.

"Master of Puppets" has appeared in several publications' best album lists. It was ranked number 167 on the list of Rolling Stone's 500 Greatest Albums of All Time, maintaining the rating in a 2012 revised list. The magazine would also later rank it second on its 2017 list of "100 Greatest Metal Albums of All Time", behind Black Sabbath's "Paranoid". "Time" included the album in its list of the 100 best albums of all time. According to the magazine's Josh Tyrangiel, "Master of Puppets" reinforced the velocity of playing in heavy metal and diminished some of its clichés. "Slant Magazine" placed the album at number 90 on its list of the best albums of the 1980s, saying "Master of Puppets" is Metallica's best and most sincere recording. The album is featured in Robert Dimery's book "1001 Albums You Must Hear Before You Die". IGN named "Master of Puppets" the best heavy metal album of all time. The website stated it was Metallica's best because it "built upon and perfected everything they had experimented with prior" and that "all the pieces come together in glorious cohesion". Music journalist Martin Popoff also ranked it the best heavy metal album. The album was voted the fourth greatest guitar album of all time by "Guitar World" in 2006, and the title track ranked number 61 on the magazine's list of the 100 greatest guitar solos. "Total Guitar" ranked the main riff of the title track at number 7 among the top 20 guitar riffs. The April 2006 edition of "Kerrang!" was dedicated to the album and gave away to readers the cover album "Master of Puppets: Remastered".

"Master of Puppets " became thrash metal's first platinum album and by the early 1990s thrash metal successfully challenged and redefined the mainstream of heavy metal. Metallica and a few other bands headlined arena concerts and appeared regularly on MTV, although radio play remained incommensurate with their popularity. "Master of Puppets" is widely accepted as the genre's most accomplished album, and paved the way for subsequent development. The album, in the words of writer Christopher Knowles, "ripped Metallica away from the underground and put them atop the metal mountain". David Hayter from "Guitar Planet" recognized the album as one of the most influential records ever made and a benchmark by which other metal albums should be judged. MTV's Kyle Anderson had similar thoughts, saying that 25 years after its release the album remained a "stone cold classic". Carlos Ramirez from Noisecreep believes that "Master of Puppets" stands as one of the most representative albums of its genre.

1986 is seen as a pinnacle year for thrash metal in which the genre broke out of the underground due to albums such as Megadeth's "Peace Sells... but Who's Buying?" and Slayer's "Reign in Blood". Anthrax released "Among the Living" in 1987, and by the end of the year these bands, alongside Metallica, were being called the "Big Four" of thrash metal. "Master of Puppets" frequently tops critic and fan polls of favorite thrash metal albums—the most frequent rival is Slayer's "Reign in Blood", also released in 1986 and also considered that band's peak. The rivalry partially stemmed from a contrast in approaches on the two albums, between the sophistication of "Master of Puppets" and the velocity of "Reign in Blood". Histories of the band tend to position "Ride the Lightning", "Master of Puppets", and "...And Justice for All" as a trilogy over the course of which the band's music progressively matured and became more sophisticated. In 2015, the album was deemed "culturally, historically, or aesthetically significant" by the Library of Congress and was selected for preservation in the National Recording Registry.

Metallica opted for extensive touring instead of releasing a single or video to promote the album. Metallica spent March to August 1986 touring as the opening act for Ozzy Osbourne in the United States, the first tour Metallica played to arena-sized audiences. During sound checks, the group played riffs from Osbourne's previous band Black Sabbath, which Osbourne perceived as a mockery toward him. Referring to that occasion, Ulrich stated that Metallica was honored to play with Osbourne, who treated the band well on the tour. Metallica was noted by the media for its excessive drinking habit while touring and earned the nickname "Alcoholica". The band members occasionally wore satirical T-shirts reading "Alcoholica/Drank 'Em All". The band usually played a 45-minute set often followed by an encore. According to Ulrich, the audiences in bigger cities were already familiar with Metallica's music, unlike in the smaller towns they've visited. "In the B-markets, people really don't know what we're all about. But after 45 or 50 minutes we can tell we've won them over. And fans who come to hear Ozzy go home liking Metallica." Metallica won over Osbourne's fans and slowly began to establish a mainstream following.

The tour, however, was notable for several incidents. Hetfield broke his wrist in a mid-tour skateboarding accident, and his guitar technician John Marshall played rhythm guitar on several dates. The European leg of the Damage, Inc. Tour commenced in September, with Anthrax as the supporting band. After the performance of September 26 in Stockholm, the band's bus rolled over on a stretch of icy road the following morning. Burton was thrown through a window and killed instantly. The driver was charged with manslaughter but was not convicted. The band returned to San Francisco and hired Flotsam and Jetsam bassist Jason Newsted to replace Burton. Many of the songs that appeared on the band's next album, "...And Justice for All", were composed during Burton's career with the band.

All of the songs have been performed live and some became permanent setlist features. Four tracks were featured on the nine-song set list for the album's promotional tour: "Battery" as opener, "Master of Puppets", "Welcome Home (Sanitarium)", and "Damage, Inc." The title track, which was issued as a single in France, became a live staple and the most played Metallica song. "Loudwire"s Chad Childers characterized the band's performance as "furious" and the song as the set's highlight. "Rolling Stone" described the live performance as "a classic in all its eight-minute glory". While filming its 3D movie "" (2013) at the Rogers Arena in Vancouver, crosses were rising from the stage during the song, reminiscent of the album's cover art.

"Welcome Home (Sanitarium)" is the second-most performed song from the album. The live performance is often accompanied by lasers, pyrotechnical effects and film screens. "Battery" is usually played at the beginning of the setlist or during the encore, accompanied by lasers and flame plumes. "Disposable Heroes" is featured in the video album "" (2009) filmed in Mexico City, in which the song was played on the second of three nights at the Foro Sol. "Orion" is the least-performed song from the album. Its first live performance was during the Escape from the Studio '06 tour, when the band performed the album in its entirety, honoring the 20th anniversary of its release. The band performed the album in the middle of the set. "Battery", "Welcome Home (Sanitarium)", "Damage, Inc." and the full-length "Master of Puppets" were revived for the band's concerts in 1998 and 1999, after having been retired for a number of years.

All lyrics written by James Hetfield and Lars Ulrich. The bonus tracks on the digital re-release were recorded live at the Seattle Coliseum, Seattle, Washington on August 29 and 30, 1989, and also appeared on the live album "" (1993).

Credits are adapted from the album's liner notes.

Metallica

Production

Artwork


</doc>
<doc id="18758" url="https://en.wikipedia.org/wiki?curid=18758" title="Ride the Lightning">
Ride the Lightning

Ride the Lightning is the second studio album by American heavy metal band Metallica, released on July 27, 1984, by the independent record label Megaforce Records. The album was recorded in three weeks with producer Flemming Rasmussen at the Sweet Silence Studios in Copenhagen, Denmark. The artwork, based on a concept by the band, depicts an electric chair being struck by lightning flowing from the band logo. The title was taken from a passage in Stephen King's novel "The Stand". Although rooted in the thrash metal genre, the album showcased the band's musical growth and lyrical sophistication. This was partly because bassist Cliff Burton introduced the basics of music theory to the rest of the band and had more input in the songwriting. Instead of relying heavily on fast tempos as on its debut "Kill 'Em All", Metallica broadened its approach by employing acoustic guitars, extended instrumentals, and more complex harmonies. The overall recording costs were paid by Metallica's European label Music for Nations because Megaforce was unable to cover it. It was the last album to feature songwriting contributions from former lead guitarist Dave Mustaine, and the first to feature contributions from his replacement, Kirk Hammett.

"Ride the Lightning" received positive response from music critics, who saw it as a more ambitious effort than its predecessor. Metallica promoted the album on the Bang That Head That Doesn't Bang European tour in late 1984, and on its North American leg in the first half of 1985. The band performed at major music festivals such as Monsters of Rock and Day on the Green later that year. Two months after its release, Elektra Records signed Metallica to a multi-year deal and reissued the album. "Ride the Lightning" peaked at number 100 on the "Billboard" 200 with no radio exposure. Although 75,000 copies were initially pressed for the American market, the album sold half a million by November 1987. It was certified 6× platinum by the Recording Industry Association of America (RIAA) in 2012 for shipping six million copies in the United States. Many rock publications have ranked "Ride the Lightning" on their best album lists, saying it had a lasting impact on the genre.

Metallica released their debut album, "Kill 'Em All", on the independent label Megaforce Records on July 25, 1983. The album helped to establish thrash metal, a heavy metal subgenre defined by its brisk riffs and intense percussion. After finishing its promotional tour, Metallica began composing new material, and from September, began performing the songs that were to make up "Ride the Lightning" at concerts. Because the band had little money, its members often ate one meal a day and stayed at fans' homes while playing at clubs across the United States. An incident occurred when part of Metallica's gear was stolen in Boston, and Anthrax lent Metallica some of its equipment to complete the remaining dates. When not gigging, the band stayed in a rented house in El Cerrito, California, called the Metallica Mansion. Frontman James Hetfield felt uneasy about performing double duty on vocals and rhythm guitar, so the band offered the job to Armored Saint singer John Bush, who turned down the offer because Armored Saint was doing well at the time. Hetfield gradually built confidence as lead vocalist and kept his original role. Metallica started recording on February 20, 1984 at Sweet Silence Studios in Copenhagen, Denmark. The album was produced by Flemming Rasmussen, the founder of Sweet Silence Studios. Drummer Lars Ulrich chose Rasmussen, because he liked his work on Rainbow's "Difficult to Cure" (1981), and was keen to record in Europe. Rasmussen, who had not heard of Metallica, agreed to work on the album, even though his studio employees questioned the band's talent. Rasmussen listened to Metallica's tapes before the members arrived and thought the band had great potential. Metallica rehearsed the album's material at Mercyful Fate's practice room in Copenhagen.

Before entering the studio, Metallica collected ideas on "riff tape" recordings of various jam sessions. Hetfield and Ulrich went through the tapes and selected the strongest riffs to assemble into songs. Instruments were recorded separately, with only Hetfield playing rhythm guitar. Rasmussen, with the support of drum roadie Flemming Larsen, taught the basics of timing and beat duration to Ulrich, who had a tendency to increase speed and had little knowledge of rhythm theory. Drums were recorded in an empty warehouse at the back of the studio, which was not soundproof, and caused reverberation. Although four tracks were already arranged, the band members were not used to creating songs in the studio, as they had not done so for "Kill 'Em All". "For Whom the Bell Tolls", "Trapped Under Ice" and "Escape" were written mostly in Copenhagen, and the band put finishing touches on "Fight Fire with Fire", "Ride the Lightning", "Creeping Death", and "The Call of Ktulu", which were already performed live. Lead guitarist Kirk Hammett took the album's name from a passage in Stephen King's novel "The Stand". The cover art, displaying an electric chair in the midst of lightning bolts, was conceived before recording began. Metallica initially had sound problems, because its gear was stolen three weeks before the band arrived in Copenhagen. The band members slept in the studio by day as they could not afford a hotel and recorded by night, because the studio was booked by other artists during the daytime. Because the group was looking for a major label deal, several A&R representatives from different labels visited the studio. At first, it seemed that Metallica was going to sign with Bronze Records, but the deal fell through, because Bronze executive Gerry Bron did not appreciate the work done at Sweet Silence Studios, and wanted the US edition to be remixed by engineer Eddie Kramer, and even considered re-recording the album in another studio. Metallica was put off by Bron's failure to share the band's artistic vision and decided to look for another label for the US release, in spite of the fact that Bronze had already advertised Metallica as one of their bands.

Metallica had to record quickly because of European shows scheduled 29 days after it entered the studio. Recording finished on March 14, and Megaforce released the album on July 27. Although the original album budget was $20,000, the final expense was above $30,000. Metallica's European label Music for Nations paid the studio costs because Megaforce owner Jon Zazula could not afford them. Metallica was unhappy with the lack of promotion by Megaforce, and decided to part ways with Zazula. Major label Elektra Records employee Michael Alago noticed Metallica at The Stone gig in San Francisco, and invited Elektra's chairman and the head of promotion to see the August show in New York. The performance at Roseland Ballroom, with Anthrax and Metallica opening for Raven, pleased the Elektra staff, and the band was offered a contract the following morning. On September 12, Metallica signed with Elektra, who re-released the album on November 19. Cliff Burnstein and Peter Mensch of Q Prime were concurrently appointed as the band's new managers. "Ride the Lightning" was the last Metallica album to feature co-writing contributions from former lead guitarist Dave Mustaine, who received credit on the title track and the instrumental "The Call of Ktulu". The album also represented the first time Hammett was given writing credits.

Music writers opine that "Ride the Lightning" exhibited greater musical maturity, with sonically broader songs than "Kill 'Em All", which was noted for its one-dimensional sound. This was partially because of bassist Cliff Burton's knowledge of music theory. He showed Hetfield how to augment core notes with complementary counter-melodies and how basic guitar harmony worked, which reflected on the song compositions. Hetfield developed more socially aware lyrics, as well as ominous and semi-philosophical references. Ulrich explained that Metallica opted not to rely strictly on fast tempos as on the previous album, but to explore other musical approaches that sounded powerful and heavy. "Grinder" magazine's Kevin Fisher summarized the album as "ultimate thrash, destruction and total blur" that reminded him of the speed and power of "Kill 'Em All". Music journalist Martin Popoff observed that "Ride the Lightning" offered "sophistication and brutality in equal measure" and was seen as something new at the time of its release. Discussing the album's lyrical content, philosopher William Irwin wrote: "After "Kill 'Em All", the rebellion and aggression became much more focused as the enemy became more clearly defined. Metallica was deeply concerned about various domains in which the common man was wrongfully yet ingeniously deceived. More precisely, they were highly critical of those in power".

The major-key acoustic introduction to "Fight Fire with Fire" displayed Metallica's evolution towards a more harmonically complex style of songwriting. The fastest Metallica song in terms of picking speed, it is driven by nimbly tremolo-picked riffs in the verses and chorus. The extended solo at the end dissolves in a sound effect of a vast nuclear explosion. The main riff was taped during the Kill 'Em All Tour and the acoustic intro was something Burton was playing on acoustic guitar at the time. The song discouraged the "eye for an eye" approach, and its lyrical themes focused on nuclear warfare and Armageddon. "Ride the Lightning" was Metallica's first song to emphasize the misery of the criminal justice system. The lyrics were written from the perspective of someone who is anticipating execution by the electric chair. The song, one of the two album tracks that credited Mustaine, begins in a mid-tempo which gradually accelerates as the song progress. One of the riffs in this song, originally composed by Mustaine, was simplified. It features an instrumental middle section highlighted by Hammett's soloing. According to Hetfield, the song was not a criticism of capital punishment, but a tale of a man sentenced to death for a crime he did not commit, as in the opening lyrics: "Guilty as charged/But damn/It ain't right".

"For Whom the Bell Tolls" begins with a bell tolling, followed by a marching riff and high-register bass melody. The chromatic introduction, which Burton wrote before he joined Metallica, is often mistaken for an electric guitar but is actually Burton's bass guitar augmented with distortion and a wah-wah pedal. The lyrics were inspired by Ernest Hemingway's 1940 novel of the same name, which explores the horror and dishonor of modern warfare. "For Whom the Bell Tolls" was released as a promotional single in two versions, an edit on side A and the album version on side B. "Fade to Black" is a power ballad whose lyrics contemplate suicide. Hetfield wrote the words because he felt powerless after the band's equipment was stolen before the January 1984 show in Boston. Musically, the song begins with an acoustic guitar introduction overlaid with electric soloing. The song becomes progressively heavier and faster, ending with multi-layered guitar solos. The ballad's arpeggiated chords and reserved singing was incongruous for thrash metal bands at the time and disappointed some of Metallica's fans. The song's structure foreshadows later Metallica ballads, "Welcome Home (Sanitarium)", "One" and "The Day That Never Comes. "Fade to Black" was released as a promotional single in 1984, in glow in the dark green.

"Trapped Under Ice" is about a person who wakes from a cryonic state. Realizing there is nowhere to go, and no-one will come to the rescue, the person helplessly awaits impending doom. The song is built on a fast-picked galloping riff, reminiscent of the album's opener. It was inspired by a track Hammett's former band Exodus had demoed called "Impaler", which was later released on that band's 2004 album "Tempo of the Damned". "Escape" was originally titled "The Hammer" and was intended to be released as a single due to its lighter riffs and conventional song structure. The intro features a counterpoint bass melody and a chugging guitar riff that resolves into a standard down-stroked riff. "Escape" is Hetfield's most hated Metallica song, due to it being the result of their record company forcing Metallica to write something more radio friendly. Authors Mick Wall and Malcolm Dome felt the song was influenced by the album-oriented rock of 1970s bands such as Journey and Foreigner, but fans perceived it as an attempt for airplay at rock radio. Metallica performed "Escape" live only once, at the 2012 Orion Music + More festival, while performing "Ride the Lightning" in its entirety.

"Creeping Death" describes the (Exodus 12:29). The lyrics deal with the ten plagues visited on Ancient Egypt; four of them are mentioned throughout the song, as well as the Passover. The title was inspired by a scene from "The Ten Commandments" while the band was watching the movie at Burton's house. The bridge, with its chant "Die, by my hand!", was originally written by Hammett for the song "Die by His Hand" while he was playing in Exodus, who recorded it as a demo but did not feature it on a studio album. Journalist Joel McIver called the song a "moshpit anthem" due to its epic lyrical themes and dramatic atmosphere. "Creeping Death" was released as a single with a B-side titled "Garage Days Revisited" made up of covers of Diamond Head's "Am I Evil?" and Blitzkrieg's "Blitzkrieg". "The Call of Ktulu", tentatively titled "When Hell Freezes Over", was inspired by H. P. Lovecraft's book "The Shadow over Innsmouth", which was introduced to the rest of the band by Burton. The title was taken from one of Lovecraft's key stories featuring Cthulhu, "The Call of Cthulhu", although the original name was modified to "Ktulu" for easier pronunciation. The track begins with a D minor chord progression in the intro, written by Mustaine (Mustaine later re-used the chord structure on Megadeth's track "Hangar 18") followed by a two-minute bass solo over a rhythmic riff pattern. Conductor Michael Kamen rearranged the piece for Metallica's 1999 "S&M" project and won a Grammy Award for Best Rock Instrumental Performance in 2001.

"Ride the Lightning" received widespread acclaim from music critics. According to "Q" magazine, the album confirmed Metallica's status as the leading heavy metal band of the modern era. The magazine credited the group for redefining the norms of thrash metal with "Fade to Black", the genre's first power ballad. British rock magazine "Kerrang!" stated that the album's maturity and musical intelligence helped Metallica expand heavy metal's boundaries. Greg Kot of the "Chicago Tribune" described "Ride the Lightning" as a more refined extension of the group's debut. In a retrospective review, Sputnikmusic's Channing Freeman named it as one of the few albums that can be charming and powerful at the same time. He praised Hetfield's vocal performance and concluded that Metallica was "firing on all cylinders". AllMusic's Steve Huey saw the album as a more ambitious and remarkable effort than "Kill 'Em All". He called "Ride the Lightning" an "all-time metal classic" because of the band's rich musical imagination and lyrics that avoided heavy metal cliches. "The Rolling Stone Album Guide" viewed the album as a great step forward for the band and as an album that established the concept for Metallica's following two records. Colin Larkin, writing in the "Encyclopedia of Popular Music", singled out "For Whom the Bell Tolls" as an example of Metallica's growing music potential. Popoff regards "Ride the Lightning" as an album where "extreme metal became art". "This literally was the first album since (Judas Priest's 1976) "Sad Wings of Destiny" where the rulebook has changed. This was a new kind of heaviness; the soft, billowy but explosive production was amazing, the speed was superhuman", stated Popoff. Reviewing the 2016 reissue, Jason Anderson of "Uncut" considers "Ride the Lightning" the second best Metallica album which set the pace for metal in the years to come.

Megaforce initially pressed 75,000 copies of the album for the US market, while Music for Nations took care of the European market. By the autumn of 1984, "Ride the Lightning" had moved 85,000 copies in Europe, resulting in Metallica's first cover story for "Kerrang!" in its December issue. After signing Metallica, Elektra released the single "Creeping Death" in a sleeve depicting a bridge and a skull painted grey and green. The album peaked at number 100 on the "Billboard" 200 with no radio exposure. In 1984, the French record label Bernett Records misprinted the color of the album cover in green, rather than blue, and 400 copies with the green cover were produced. Because of their rarity, these green albums have become collectors' items. "Ride the Lightning" went gold by November 1987 and in 2012 was certified 6× platinum by the Recording Industry Association of America (RIAA) for shipping six million copies in the US. The album, along with "Kill 'Em All", was reissued in 2016 as a boxed set including demos and live recordings. Many rock publications have ranked "Ride the Lightning" on their best album lists. The album placed fifth on IGN Music's "Top 25 Metal Albums" list. "Spin" listed it as a thrash metal essential, declaring it "the thrashiest thrash ever". According to "Guitar World", "Ride the Lightning" "didn't just change the band's trajectory—it reset the course of metal itself". Corey Deiterman of the "Houston Press" considers "Ride the Lightning" the most influential Metallica album, saying it had a lasting impact on genres such as crossover thrash and hardcore punk. In 2017, it was ranked 11th on "Rolling Stone" list of "100 Greatest Metal Albums of All Time".

After recording was completed, Music for Nations founder Martin Hooker wanted to arrange a triple bill UK tour in March / April 1984 with Exciter, Metallica, and The Rods. The Hell on Earth Tour never materialized because of poor ticket sales. To promote "Ride the Lightning", Metallica commenced the Bang That Head That Doesn't Bang European tour on November 16, in Rouen, France, with British NWOBHM band Tank as support. The tour continued with dates in Belgium, Italy, Germany, and the Nordic countries to an average crowd of 1,300. After a Christmas break, the group embarked on a 50-date North American tour, firstly as a co-headlining act with W.A.S.P. and then as headliners with Armored Saint supporting. At a gig in Portland, Oregon, Metallica covered "The Money Will Roll Right In" by Fang, with Armored Saint onstage. The American leg ended in May 1985, and the band spent the following two months working on the next studio album, "Master of Puppets", whose recording sessions were scheduled to begin in September. Metallica performed at the Monsters of Rock festival held at Castle Donington in England on August 17 in front of 70,000 fans. The band was placed between Ratt and Bon Jovi, two glam metal groups whose sound and appearance were much unlike Metallica's. At the start of the set, Hetfield pronounced to the audience: "If you came here to see spandex, eye make-up, and the words 'oh baby' in every fuckin' song, this ain't the fuckin' band!" Two weeks later, Metallica appeared on the Day on the Green festival in Oakland, California, before 90,000 people. The last show Metallica played before recording began was the Loreley Metal Hammer festival in Germany, headlined by Venom. Metallica finished 1985 with a show at the Sacramento Memorial Auditorium on December 29 opening for Y&T, and a New Year's Eve concert at the Civic Auditorium in San Francisco on a bill with Metal Church, Exodus, and Megadeth, the first time Metallica and Megadeth shared a stage. At this gig, Metallica premiered "Master of Puppets" and "Disposable Heroes", songs from the then-upcoming third studio album.

All lyrics written by James Hetfield (Kirk Hammett also contributed to lyrics for "Creeping Death"). The bonus tracks on the digital re-release were recorded live at the Seattle Coliseum, Seattle, Washington on August 29 and 30, 1989, and later appeared on the live album "" (1993).

Credits are adapted from the album's liner notes.

Metallica

Production

Packaging



</doc>
<doc id="18760" url="https://en.wikipedia.org/wiki?curid=18760" title="Metallica (album)">
Metallica (album)

Metallica (commonly known as The Black Album) is the self-titled fifth studio album by American heavy metal band Metallica, released on August 12, 1991, through Elektra Records. It was recorded in an eight-month span at One on One Recording Studios in Los Angeles. The recording of the album was troubled, however, and, during production, the band frequently came into conflict with their new producer Bob Rock. The album marked a change in the band's sound from the thrash metal style of the previous four albums to a slower and heavier one rooted in heavy metal. 

Metallica promoted "Metallica" with a series of tours. They also released five singles to promote the album; "Enter Sandman", "The Unforgiven", "Nothing Else Matters", "Wherever I May Roam", and "Sad but True", all of which have been considered to be among the band's best-known songs. The song "Don't Tread on Me" was also issued to rock radio shortly after the album's release but did not receive a commercial single release.

"Metallica" received widespread critical acclaim and became the band's best-selling album. It debuted at number one in ten countries and spent four consecutive weeks at the top of the "Billboard" 200, making it Metallica's first album to top the album charts. "Metallica" is one of the best-selling albums worldwide, and also one of the best-selling albums in the United States since Nielsen SoundScan tracking began. The album was certified 16× platinum by the Recording Industry Association of America (RIAA) in 2012, and has sold over sixteen million copies in the United States, being the first album in the SoundScan era to do so. 

Metallica played "Metallica" in its entirety during the 2012 European Black Album Tour. In 2003, the album was ranked number 255 on "Rolling Stone"s 500 greatest albums of all time. In December 2019, "Metallica" became the fourth release in American history to enter the 550 week milestone on the "Billboard" 200. It also became the second longest-charting traditional title in history, and the second to spend 550 weeks on the album charts.

At the time of "Metallica" recording, the band's songs were written mainly by frontman James Hetfield and drummer Lars Ulrich, with Hetfield being the lyricist. The duo frequently composed together at Ulrich's house in Berkeley, California. Several song ideas and concepts were conceived by other members of the band, lead guitarist Kirk Hammett and bassist Jason Newsted. For instance, Newsted wrote the main riff of "My Friend of Misery", which was originally intended to be an instrumental, one of which had been included on every previous Metallica album. The songs were written in two months in mid-1990; the ideas for some of them were originated during the Damaged Justice Tour. Metallica was impressed with Bob Rock's production work on Mötley Crüe's "Dr. Feelgood" (1989) and decided to hire him to work on their album. Initially, the band members were not interested in having Rock producing the album as well, but changed their minds. Ulrich said, "We felt that we still had our best record in us and Bob Rock could help us make it".

Four demos for the album were recorded on August 13, 1990; "Enter Sandman", "The Unforgiven", "Nothing Else Matters" and "Wherever I May Roam." The lead single "Enter Sandman" was the first song to be written and the last to receive lyrics. On October 4, 1990, a demo of "Sad but True" was recorded. In October 1990, Metallica began recording at One on One Recording Studios in Los Angeles, California, to record the album, and also at Little Mountain Sound Studios in Vancouver, British Columbia for about a week. On June 2, 1991, a demo of "Holier Than Thou" was recorded. Hetfield stated about the recording: "What we really wanted was a live feel. In the past, Lars and I constructed the rhythm parts without Kirk and Jason. This time I wanted to try playing as a band unit in the studio. It lightens things up and you get more of a vibe."

Because it was Rock's first time producing a Metallica album, he had the band make the album in different ways; he asked them to record songs collaboratively rather than individually in separate locations. He also suggested recording tracks live and using harmonic vocals for Hetfield. Rock was expecting the production to be "easy" but had trouble working with the band, leading to frequent, engaged arguments with the band members over aspects of the album. Rock wanted Hetfield to write better lyrics and found his experience recording with Metallica disappointing. Since the band was perfectionist, Rock insisted they recorded as many takes as needed to get the sound they wanted.<ref name="AY&½ITLOM (1/2)"></ref> The album was remixed three times and cost . The troubled production coincided with Ulrich, Hammett, and Newsted divorcing their wives; Hammett said this influenced their playing because they were "trying to take those feeling of guilt and failure and channel them into the music, to get something positive out of it".

Rock altered Metallica's familiar recording routine and the recording experience was so stressful that Rock briefly swore never to work with the band again. The tension between band and producer was documented in "A Year and a Half in the Life of Metallica" and "Classic Albums: Metallica – Metallica", documentaries that explore the intense recording process that resulted in "Metallica". Despite the controversies between the band and Rock, he continued to work with Metallica through the 2003 album "St. Anger". After the production of "St. Anger" (2003), the fourth and final Metallica record Rock would produce, a petition signed by 1,500 fans was posted online in an attempt to encourage the band to prohibit Rock from producing Metallica albums, saying he had too much influence on the band's sound and musical direction. Rock said the petition hurt his children's feelings; he said, "sometimes, even with a great coach, a team keeps losing. You have to get new blood in there."

According to Robert Palmer of "Rolling Stone", "tempos were often slowed down in exchange for slower BPMs, while they expand its music and expressive range". The album was a change in Metallica's direction from the thrash metal style of the band's previous four studio albums towards a more commercial, heavy metal sound, but still had characteristics of thrash metal. Many fans consider the album to be a transition from the often ostentatious compositions of Metallica's previous releases to the slower, divested style of the band's later albums, where "old" and "new" Metallica are distinguished from one another. Instruments not usually used by heavy metal bands, such as the cellos in "The Unforgiven" and the orchestra in "Nothing Else Matters", were added at Rock's insistence. Rock also raised the volume of the bass guitar, which had been nearly inaudible on the previous album "...And Justice for All". Newsted said he tried to "create a real rhythm section rather than a one-dimensional sound" with his bass. Ulrich said he tried to avoid the "progressive Peartian paradiddles which became boring to play live" in his drumming and used a basic sound similar to those of The Rolling Stones' Charlie Watts and AC/DC's Phil Rudd.

The band took a simpler approach partly because the members felt the songs on "...And Justice for All" were too long and complex. Hetfield said that radio airplay was not their intention, but because they felt "we had pretty much done the longer song format to death," and considered a good change doing songs with just two riffs and "only taking two minutes to get the point across". Ulrich added that the band was feeling a musical insecurity — "We felt inadequate as musicians and as songwriters, That made us go too far, around "Master of Puppets" and "Justice", in the direction of trying to prove ourselves. 'We'll do all this weird-ass shit sideways to prove that we are capable musicians and songwriters'" – and Hetfield added he wanted to avoid getting stale: "Sitting there and worrying about whether people are going to like the album, therefore we have to write a certain kind of song — you just end up writing for someone else. Everyone's different. If everyone was the same, it would be boring as shit."

The lyrics of "Metallica" written by James Hetfield were more personal and introspective in nature than those of previous Metallica albums; Rock said Hetfield's songwriting became more confident, and that he was inspired by Bob Dylan, Bob Marley and John Lennon. According to Chris True of AllMusic, "Enter Sandman" is about "nightmares and all that come with them". "The God That Failed" dealt with the death of Hetfield's mother from cancer and her Christian science beliefs, which kept her from seeking medical treatment. "Nothing Else Matters" was a love song Hetfield wrote about missing his girlfriend while on tour. Hetfield said the album's lyrical themes were more introspective because he wanted "lyrics that the band could stand behind – but we are four completely different individuals. So the only way to go was in."

Metallica had many discussions about the album title; the members considered calling it "Five" or using the title of one of the songs, but eventually chose an eponym because they "wanted to keep it simple." The album's cover depicts the band's logo angled against the upper left corner and a coiled snake derived from the Gadsden flag in the bottom right corner. For the initial release, both emblems were embossed so they could barely be seen against the black background, giving "Metallica" the nickname ""The Black Album"". These emblems also appear on the back cover of the album. For later and current releases, both emblems are dark gray so they stand out more prominently. The motto of the Gadsden flag, "Don't Tread on Me", is also the title of a song on the album. A folded, pageless booklet depicts the faces of the band's members against a black background. The lyrics and liner notes are also printed on a grey background. The cover is reminiscent of Spinal Tap's album "Smell the Glove", which the band jokingly acknowledged in its documentary "A Year and a Half in the Life of Metallica". Members of Spinal Tap appeared on the film and asked Metallica about it, with Lars Ulrich commenting that British rock group Status Quo was the original inspiration as that band's "Hello!" album cover was also black.

Six tracks on "Metallica" were released as singles. "Enter Sandman" was released as the lead single on July 29, 1991; it reached number 16 on the "Billboard" Hot 100 singles chart and was certified Platinum by the Recording Industry Association of America (RIAA). The follow-up single, "Don't Tread on Me", was released promotionally and peaked at number 21 on the "Billboard" Hot Mainstream Rock Tracks singles chart. "The Unforgiven" was a Top 40 hit; it peaked in the Top 10 in Australia. In 1992, "Nothing Else Matters" was released to more success, reaching number six in the United Kingdom and Ireland. The fifth single from the album was also released in 1992; "Wherever I May Roam" peaked at number two on the Mainstream Rock Tracks chart but was less successful on the Hot 100 chart, failing to reach the Top 80. In 1993, "Sad but True" did not repeat the successes of the album's previous singles, charting for one week on the "Billboard" Hot 100 at 98. Almost all singles were accompanied by music videos; the Wayne Isham-directed "Enter Sandman" promotional film won an MTV Video Music Award for Best Rock Video at the 1992 MTV Video Music Awards.

In 1991, for the fourth time, Metallica played as part of the Monsters of Rock festival tour. The last concert of the tour was held on September 28, 1991, at Tushino Airfield in Moscow; it was described as "the first free outdoor Western rock concert in Soviet history" and was attended by an estimated 150,000 to 500,000 people. Some unofficial estimates put the attendance as high as 1,600,000. The first tour directly intended to support the album, the Wherever We May Roam Tour, included a performance at the Freddie Mercury Tribute Concert, at which Metallica performed a short set list, consisting of "Enter Sandman", "Sad but True" and "Nothing Else Matters", and Hetfield performed the Queen song "Stone Cold Crazy" with John Deacon, Brian May and Roger Taylor of Queen and Tony Iommi of Black Sabbath. At one of the tour's first gigs the floor of the stage collapsed. The January 13 and 14, 1992, shows in San Diego were later released in the box set "", while the tour and the album were documented in the documentary "A Year and a Half in the Life of Metallica".

Metallica's Wherever We May Roam Tour also overlapped with Guns N' Roses' Use Your Illusion Tour. Hetfield suffered second and third degree burns to his arms, face, hands, and legs on August 8, 1992, during a Montreal show in the co-headlining Guns N' Roses/Metallica Stadium Tour. The tour included pyrotechnics, which were installed on-stage. Hetfield accidentally walked into a flame shot from a pyrotechnic during a live performance of the introduction of "Fade to Black".<ref name="AY&½ITLOM (2/2)"></ref> The show was cut short shortly after this accident, so that Guns N' Roses began their concert to malicious reactions from fans. Newsted said Hetfield's skin was "bubbling like on "The Toxic Avenger"". The tour recommenced on August 25 in Phoenix, and although Hetfield could sing, he could not play guitar for the remainder of the tour. Guitar technician John Marshall, who had previously filled in on rhythm guitar and was then playing in Metal Church, played guitar for the recovering Hetfield. Brazilian musician, Andreas Kisser from Sepultura was initially considered for play the tour, but Marshall finally was chosen.

The shows in Mexico City across February and March 1993 during the Nowhere Else to Roam tour were recorded, filmed and later also released as part of the band's first box set, which was released in November 1993 and titled "Live Shit: Binge & Purge". The collection contained three live CDs, three home videos, and a book filled with riders and letters. Pressings of the box set since November 2002 includes two DVDs, the first one being filmed at San Diego on the Wherever We May Roam Tour, and the latter at Seattle on the Damaged Justice Tour. "Binge & Purge" was packaged as a cardboard box resembling that of a typical tour equipment transport box. The box set also featured a recreated copy of an access pass to the "Snakepit" part of the tour stage, as well as a cardboard drawing/airbrush stencil for the "Scary Guy" logo. The Mexico City shows were also the first time the band met future member Robert Trujillo, who was in Suicidal Tendencies at the time.

The final tour supporting the album, the Shit Hits the Sheds Tour, included a performance at Woodstock '94 that followed Nine Inch Nails and preceded Aerosmith on August 13 in front of a crowd of 350,000. Some songs, such as "Enter Sandman", "Nothing Else Matters" and "Sad but True", became permanent staples of Metallica's concert setlists during these and subsequent tours. Other songs though, such as "Holier Than Thou", "The God That Failed", "Through the Never", and "The Unforgiven" were no longer included in performances after 1995 and would not be played again until the 2000s, when Metallica, with Robert Trujillo on bass, began performing a more extensive back catalog of songs after Trujillo joined the band upon completion of the album "St. Anger".

After touring duties for the album were finished, Metallica filed a lawsuit against Elektra Records, which tried to force the record label to terminate the band's contract and give the band ownership of their master recordings. The band based its claim on a section of the California Labor Code that allows employees to be released from a personal services contract after seven years. Metallica had sold 40 million copies worldwide upon the filing of the suit. Metallica had been signed to the label for over a decade but was still operating under the terms of its original 1984 contract, which provided a relatively low 14% royalty rate. The band members said they were taking the action because they were ambivalent about Robert Morgado's refusal to give them another record deal along with Bob Krasnow, who retired from his job at the label shortly afterwards. Elektra responded by counter-suing the band, but in December 1994, Warner Music Group United States chairman Doug Morris offered Metallica a lucrative new deal in exchange for dropping the suit, which was reported to be even more generous than the earlier Krasnow deal. In January 1995, both parties settled out of court with a non-disclosure agreement. Metallica played the album in its entirety during the 2012 European Black Album Tour.

"Metallica" was released to widespread acclaim from both heavy metal journalists and mainstream publications, including "NME", "The New York Times", and "The Village Voice". In "Entertainment Weekly", David Browne called it "rock's preeminent speed-metal cyclone", and said, "Metallica may have invented a new genre: progressive thrash". "Q" magazine's Mark Cooper said he found the album's avoidance of metal's typically clumsy metaphors and glossy production refreshing; he said, "Metallica manage to rekindle the kind of intensity that fired the likes of Black Sabbath before metal fell in love with its own cliches". "Select" magazine's David Cavanagh believed the album lacks artifice and is "disarmingly genuine". In his review for "Spin", Alec Foege found the music's harmonies vividly performed and said that Metallica showcase their "newfound versatility" on songs such as "The Unforgiven" and "Holier Than Thou". Robert Palmer, writing in "Rolling Stone", said that several songs sound like "hard-rock classics" and that, apart from "Don't Tread on Me", "Metallica" is an "exemplary album of mature but still kickass rock & roll". In his guide to Metallica's albums up to that point, Greg Kot of the "Chicago Tribune" recommended the album as "a great place for Metallica neophytes to start, with its more concise songs and explosive production." Jonathan Gold was less enthusiastic in the "Los Angeles Times". He said while Metallica embraced pop sensibilities "quite well", there was a sense the group was "no longer in love with the possibilities of its sound" on an album whose difficulty being embraced by the "metal cult" mirrored Bob Dylan going electric in the mid 1960s.

In a retrospective article, "Kerrang!" said "Metallica" is the album that "propelled [the band] out of the metal ghetto to true mainstream global rock superstardom". "Melody Maker" said that as a deliberate departure from the band's thrash style on "...And Justice for All", ""Metallica" was slower, less complicated, and probably twice as heavy as anything they'd done before". In his review for BBC Music, Sid Smith said that although staunch listeners of the band accused them of selling out, Metallica confidently departed from the style of their previous albums and transitioned "from cult metal gods to bona fide rock stars". "Classic Rock" called it "the absolute pinnacle of Metallica's long and successful career", and credited the album for inspiring 1990s post-grunge music and convincing the music industry to embrace heavy metal as a genre with mass appeal. AllMusic's Steve Huey believed the massive popularity of "Metallica" inspired other speed metal bands to also embrace a simpler, less progressive sound. He deemed the record "a good, but not quite great" album, one whose best moments deservedly captured the heavy metal crown, but whose approach also foreshadowed a creative decline for Metallica. "Village Voice" critic Robert Christgau was less enthusiastic, saying he "put James Hetfield out of his misery in under five plays" and that he "found life getting shorter with every song". In "Christgau's Consumer Guide" (2000), he later graded the album a "dud", indicating "a bad record whose details rarely merit further thought".

"Metallica" was voted the eighth best album of the year in "The Village Voice"s annual Pazz & Jop critics poll for 1991. "Melody Maker" ranked it number 16 in its December 1991 list of the year's best albums. In 1992, the album won a Grammy Award for Best Metal Performance. In 2000 it was voted number 88 in Colin Larkin's "All Time Top 1000 Albums". In 2003 and 2012, "Rolling Stone" ranked "Metallica" number 255 on its list of the 500 greatest albums of all time and 25th on their 2017 list of "100 Greatest Metal Albums of All Time". "Spin" ranked it number 52 in its 1999 list of the "90 Greatest Albums of the '90s" and said, "this record's diamond-tipped tuneage stripped the band's melancholy guitar excess down to melodic, radio-ready bullets and ballads". It was included in "Q" magazine's August 2000 list of the "Best Metal Albums of All Time"; the magazine said the album "transformed them from cult metal heroes into global superstars, bringing a little refinement to their undoubted power". In 1999, eight years after its release, the album won a Billboard Music Award for Catalog Album of the Year.

"Metallica" was released on August 12, 1991, and was the band's first album to debut at number one on the "Billboard" 200, selling 598,000 copies in its first week. It was certified platinum in two weeks and spent four consecutive weeks atop the "Billboard" 200. Logging over 488 weeks on the "Billboard" 200, it is the third longest charting album in the Nielsen SoundScan era, behind Pink Floyd's "The Dark Side of the Moon" and Carole King's "Tapestry". In 2009, it surpassed Shania Twain's "Come On Over" as the best-selling album of the SoundScan era. It became the first album in the SoundScan era to pass 16 million in sales, and with 16.4 million copies sold by 2016, "Metallica" is the best-selling album in the United States since Nielsen SoundScan tracking began in 1991. Of that sum, 5.8 million were purchased on cassette. The album never sold less than 1,000 copies in a week, and moved a weekly average of 5,000 copies in 2016. "Metallica" was certified 16× platinum by the Recording Industry Association of America (RIAA) in 2012 for shipping sixteen million copies in the US. "Metallica" sold 31 million copies worldwide on physical media. All five of "Metallica"s singles, "Enter Sandman", "The Unforgiven", "Nothing Else Matters", "Wherever I May Roam" and "Sad but True" reached the "Billboard" Hot 100.

"Metallica" debuted at number one on the UK Albums Chart, and was certified 2× platinum by the British Phonographic Industry (BPI) for shipping 600,000 copies in the UK. "Metallica" topped the charts in Australia, Canada, Germany, New Zealand, Norway, the Netherlands, Sweden, and Switzerland. It also reached the top five in Austria, Finland, and Japan, as well as the top 10 in Spain. The album failed to reach the top 20 in Ireland, having peaked at number 27. The Australian Recording Industry Association (ARIA) certified the album 12× platinum. It was given a diamond plaque from the Canadian Recording Industry Association (CRIA) and the Recorded Music NZ (RMNZ) for shipping a million and 150,000 copies, respectively.

Credits are adapted from the album's liner notes.

Metallica

Additional musicians

Production and design

!scope="row"|Worldwide




</doc>
<doc id="18761" url="https://en.wikipedia.org/wiki?curid=18761" title="Load (album)">
Load (album)

Load is the sixth studio album by the American heavy metal band Metallica, released on June 4, 1996 by Elektra Records in the United States and by Vertigo Records internationally. The album showed more of a hard rock side of Metallica than the band's typical thrash metal style, which alienated much of the band's fanbase. It also featured influences from genres such as Southern rock, blues rock, country rock and alternative rock. Drummer Lars Ulrich said about "Load" more exploratory nature, "This album and what we're doing with it – that, to me, is what Metallica are all about: exploring different things. The minute you stop exploring, then just sit down and fucking die". At 79 minutes, it is Metallica's longest studio album.

"Load" debuted and spent four consecutive weeks at number one on the US "Billboard" 200 chart. "Load" sold 680,000 units in its first week, making it the biggest opening week for Metallica as well as the biggest debut of 1996. It was certified 5× platinum by the Recording Industry Association of America (RIAA) for shipping five million copies in the United States. Four singles—"Until It Sleeps", "Hero of the Day", "Mama Said", and "King Nothing"—were released as part of the marketing campaign for the album.

"Load", released approximately five years after the commercially successful album "Metallica", saw the band shifting toward hard rock and away from their thrash metal roots. As on previous releases, the album's fourteen songs began as rough demos created by principal songwriters James Hetfield and Lars Ulrich in Ulrich's basement recording studio, "The Dungeon". In early 1995, the band took over thirty demos into The Plant Studios, where they would work for approximately one year. Metallica worked with producer Bob Rock, who had been at the helm during the recording process for "Metallica".

The songwriting dispenses almost entirely with the thrash metal style that characterized the band's sound in the 1980s. In place of staccato riffs, Hetfield and lead guitarist Kirk Hammett experimented with blues rock-based tones and styles. Additionally, Ulrich adopted a minimalist approach to his drum recording, abandoning the speed and complex double bass drumming patterns of previous albums, and using simpler techniques and playing styles. Hetfield displayed a lyrical evolution, writing what many said were his most personal and introspective lyrics. "Until It Sleeps", the album's lead single, addressed his mother's unsuccessful battle with cancer, and "Mama Said" also explores his relationship with her. All of this marked a departure from the political and social overtones of "...And Justice for All" and "Master of Puppets". Hammett, encouraged by producer Bob Rock, also played rhythm guitar on a Metallica album for the first time, having previously only played lead parts with Hetfield playing all the rhythm parts to achieve a tighter feel, in contrast to the looser feel they were looking for here. Hammett continued playing rhythm until "Death Magnetic" when Hetfield once again played all the rhythm parts. 

At 79 minutes, "Load" is Metallica's longest studio album. With the CD length at 78:59, initial pressings of the album were affixed with stickers boasting of its long playtime, simply reading "78:59". "The Outlaw Torn" had to be shortened by about one minute to fit on the album; the full version of the track was released on the single "The Memory Remains" as "The Outlaw Torn (Unencumbered by Manufacturing Restrictions Version)", with a running time of 10:48. An explanation on the single's back cover stated:

"Load" was Metallica's first album on which all tracks were down-tuned to E♭ tuning. Hammett states:

Hetfield also felt that the change to E♭ was a bonus, as it was easier to perform string bends in the riffs. 

The band had recorded songs on earlier albums in tunings lower than E; "The God That Failed" on "Metallica" which was in E♭, and the same album's "Sad but True" and "The Thing That Should Not Be" from "Master of Puppets" were in D tuning. The Australian CD release of "Load" includes a bonus interview CD that is unavailable elsewhere. 10 songs from the album have been played live including "King Nothing", "Until It Sleeps", "Ain't My Bitch", "Bleeding Me", "Wasting My Hate", "Hero of the Day", "The Outlaw Torn", "2 X 4", "Poor Twisted Me", "Mama Said". Songs that have not been played live in their entirety are "The House Jack Built", "Cure", "Thorn Within", and "Ronnie".

The cover of "Load" is an original artwork titled "Semen and Blood III". It is one of three photographic studies that Andres Serrano created in 1990 by mingling bovine blood and his own semen between two sheets of Plexiglas. The liner notes simply state "cover art by Andres Serrano" rather than listing the title of the work. In a 2009 interview with "Classic Rock", Hetfield expressed his dislike of the album cover and its inspiration:

"Load" also marked the first appearance of a new Metallica logo that rounded off the stabbing edges of the band's earlier logo, greatly simplifying its appearance. The M from the original logo was used to make a shuriken-like symbol known as the "ninja star", which was used as an alternate logo on this and future albums, and on related artwork. The album featured an expansive booklet containing photographs by Anton Corbijn. These photographs depict the band in various dress, including white A-shirts with suspenders, Cuban suits, and gothic. In the aforementioned 2009 interview, James Hetfield said:

"Load" received positive to mixed reviews from critics. "Rolling Stone" said, "The foursome dams the bombast and chugs half-speed ahead, settling into a wholly magnetizing groove that bridges old-school biker rock and the doomier side of post-grunge '90s rock." "Q" enthused, "These boys set up their tents in the darkest place of all, in the naked horror of their own heads... Metallica make existential metal and they've never needed the props... Metallica are still awesome... What is new is streamlined attack, the focus and, yes, the tunes."

"Melody Maker" expressed reservations about "Load"'s heaviness compared to its predecessors: "A Metallica album is traditionally an exhausting event. It should rock you to exhaustion, leave you brutalised and drained. This one is no exception. It is, however, the first Metallica album to make me wonder at any point, 'What the fuck was that?' It's as if the jackboot grinding the human face were to take occasional breaks for a pedicure." AllMusic considered "Load" repetitive, uninteresting and poorly executed. In "The Village Voice", Robert Christgau said "this is just a metal record with less solo room, which is good because it concentrates their chops, and more singing, which isn't because they can't."

"Some of that stuff was pretty cool," remarked Lars Ulrich of the album and its sequel. "With "Load", it was disappointing that some people's reaction to the music was biased by how they dealt with the pictures – the hair and all that crap "[see Artwork, above]". People have come up to me years afterwards and said, 'I never gave the record a fair chance because I couldn't get beyond Jason Newsted wearing eyeliner.' But 'The Outlaw Torn', some of that shit is pretty fucking awesome."
Credits are adapted from the album's liner notes.

Metallica

Production


</doc>
<doc id="18763" url="https://en.wikipedia.org/wiki?curid=18763" title="Garage Inc.">
Garage Inc.

Garage Inc. is a compilation album of cover songs by American heavy metal band Metallica. It was released on November 24, 1998, through Elektra Records. It includes cover songs, B-side covers, and "", which had gone out of print since its original release in 1987. The title is a combination of "Garage Days Revisited" and their song "Damage, Inc.", from "Master of Puppets", and the album's graphical cover draws heavily from the . The album features songs by artists that have influenced Metallica, including many bands from the new wave of British heavy metal movement, hardcore punk bands and popular songs.

The day after Metallica finished the North American leg of the Poor Re-Touring Me Tour in San Diego's Coors Amphitheatre, they hit the studio to start recording a new album of cover versions. As Lars Ulrich explained, the band wanted to do something different after "three pretty serious albums in a row, starting with the Black album and then "Load" and "ReLoad"", and the process would be easier by working with covers, especially as the band had a tradition of taking other people's songs and "turn them into something very Metallica, different from what the original artist did". Given the band had recorded many covers that were spread across various releases, such as B-sides of their singles and the 1987 EP "", the band would "put them all in a nice little packaging for easy listening" along with the newly recorded cover versions, chosen through a group decision. Only one of the eleven songs in the "New Recordings '98" disk was not done in the three-week sessions, a version of Lynyrd Skynyrd's "Tuesday's Gone" the band recorded for a radio broadcast along with friends such as Les Claypool, John Popper and Gary Rossington.

The cover for "Garage Inc." had an Anton Corbijn photograph of Metallica dressed as mechanics. The band wanted the booklet to hold a detailed account of the contents of the project, and designer Andy Airfix was allowed to search through Ulrich's catalogue of Metallica memorabilia in San Francisco to create a 32-page booklet. Airfix also did the back cover, where the front of "Garage Days Re-Revisited" was modified with headshots of Metallica in 1998 and the track list written on tracing paper.


These tracks were recorded in September–October 1998 for the "Garage Inc." album.


These tracks are a collection of B-sides from artists Metallica were inspired by, throughout the early years of the band.


Metallica

Guest musicians on "Tuesday's Gone"
Technical personnel

In the U.S., "Garage Inc." sold 426,500 units in the first week of release, making Metallica's fourth straight debut surpassing 400,000 copies. Still, the million-plus record breaking debut of Garth Brooks' "Double Live" made "Garage Inc." land only at second on the "Billboard 200".



</doc>
<doc id="18764" url="https://en.wikipedia.org/wiki?curid=18764" title="S&amp;M (album)">
S&amp;M (album)

S&M (an abbreviation of "Symphony and Metallica") is a live album by American heavy metal band Metallica, with The San Francisco Symphony conducted by Michael Kamen. It was recorded on April 21–22, 1999 at The Berkeley Community Theatre. This is the final Metallica album to feature Jason Newsted as Metallica's bassist.

"S&M" contains performances of Metallica songs with additional symphonic accompaniment, which was composed by Michael Kamen, who also conducted the orchestra during the concert. The idea to combine heavy metal with an epic classical approach, as James Hetfield has stated repeatedly, was an idea of Cliff Burton. Burton’s love of classical music, especially of Johann Sebastian Bach, can be traced back to many instrumental parts and melodic characteristics in Metallica’s songwriting including songs from "Ride The Lightning" and "Master of Puppets". Kamen, who arranged and conducted the orchestral background tracks for "Nothing Else Matters", met the band at the 1992 Grammy award show for the first time, and after hearing the "Elevator version" of the song, suggested the band to perform with a whole orchestra; the band, however, didn't take him up on the offer until 7 years later. Lars Ulrich's favourite band Deep Purple, who he colourfully inducted into the Rock and Roll Hall of Fame in 2016, is noted for having kicked off this kind of approach 30 years before, in Concerto for Group and Orchestra (1969).

In addition to songs from previous albums spanning "Ride the Lightning" through "ReLoad", there are two new compositions: "No Leaf Clover" and "−Human". "The Ecstasy of Gold" by Ennio Morricone, Metallica's entrance music, was played live by the orchestra. "No Leaf Clover" has since been performed by Metallica in concert, using a recording of the orchestral prelude.

Changes were made to the lyrics of some songs, most notably the removal of the second verse and chorus of "The Thing That Should Not Be" and playing the third verse in its place.

The "S" in the stylized "S&M" on the album cover is a backwards treble clef, while the "M" is taken from Metallica's logo.

The drum kit Ulrich used on the album currently resides in a Guitar Center in San Francisco.


"S&M" sold 300,000 units in the first week of release, and went on to sell a total of 2.5 million copies. As of 2003, the album had been certified 5× platinum. As of August 2013 the album had sold more than 8 million copies worldwide.

The album was included in the book "1001 Albums You Must Hear Before You Die".

This album was also ranked 48th on NME's list of 50 Greatest Live Albums.

After Kamen's death in 2003, Metallica hadn't revisited the S&M concept in any further performances or recording work. However, the band announced on March 18, 2019 that they would hold a concert at the Chase Center on September 6 of that year to commemorate the 20th anniversary with a single-night concert, headed by Michael Tilson Thomas as music director. They later added a second concert on September 8.

The shows included many songs from the original S&M performances, as well as renditions of songs that had been released since then. In August, it was announced that a film of the concerts would receive a limited worldwide theatrical release. The concert was given a limited release and has grossed over 5.5 million dollars. 

Metallica also filmed and released the concert in DVD and VHS with direction by Wayne Isham. The VHS set has only the concert video, while the double DVD set has 5.1 sound (also: 2.0 band+orchestra, 2.0 band-only and 2.0 orchestra-only), 41 minute documentary about the concert, and two "No Leaf Clover" music videos: "Slice & Dice" version and the "Maestro Edit". The DVD also contains four songs with multi-angles where each band member can be viewed individually: "Of Wolf and Man", "Fuel", "Sad But True", and "Enter Sandman".


</doc>
<doc id="18787" url="https://en.wikipedia.org/wiki?curid=18787" title="Metallica">
Metallica

Metallica is an American heavy metal band. The band was formed in 1981 in Los Angeles by vocalist/guitarist James Hetfield and drummer Lars Ulrich, and has been based in San Francisco for most of its career. The band's fast tempos, instrumentals and aggressive musicianship made them one of the founding "big four" bands of thrash metal, alongside Megadeth, Anthrax and Slayer. Metallica's current lineup comprises founding members and primary songwriters Hetfield and Ulrich, longtime lead guitarist Kirk Hammett and bassist Robert Trujillo. Guitarist Dave Mustaine (who went on to form Megadeth after being fired from the band) and bassists Ron McGovney, Cliff Burton (who died in a bus accident in Sweden in 1986) and Jason Newsted are former members of the band.

Metallica earned a growing fan base in the underground music community and won critical acclaim with its first five albums. The band's third album, "Master of Puppets" (1986), was described as one of the heaviest and most influential thrash metal albums. Its eponymous fifth album, "Metallica" (1991), the band's first to root predominantly in heavy metal, appealed to a more mainstream audience, achieving substantial commercial success and selling over 16 million copies in the United States to date, making it the best-selling album of the SoundScan era. After experimenting with different genres and directions in subsequent releases, the band returned to its thrash metal roots with the release of its ninth album, "Death Magnetic" (2008), which drew similar praise to that of the band's earlier albums.

In 2000, Metallica led the case against the peer-to-peer file sharing service Napster, in which the band and several other artists filed lawsuits against the service for sharing their copyright-protected material without consent; after reaching a settlement, Napster became a pay-to-use service in 2003. Metallica was the subject of the acclaimed 2004 documentary film "", which documented the troubled production of the band's eighth album, "St. Anger" (2003), and the internal struggles within the band at the time. In 2009, Metallica was inducted into the Rock and Roll Hall of Fame. The band wrote the screenplay for and starred in the 2013 IMAX concert film "", in which the band performed live against a fictional thriller storyline.

Metallica has released ten studio albums, four live albums, a cover album, five extended plays, 37 singles and 39 music videos. The band has won nine Grammy Awards from 23 nominations, and its last six studio albums (beginning with "Metallica") have consecutively debuted at number one on the "Billboard" 200. Metallica ranks as one of the most commercially successful bands of all time, having sold over 125 million albums worldwide as of 2018. Metallica has been listed as one of the greatest artists of all time by magazines such as "Rolling Stone", which ranked them at no. 61 on its "100 Greatest Artists of All Time" list. As of 2017, Metallica is the third best-selling music artist since Nielsen SoundScan began tracking sales in 1991, selling a total of 58 million albums in the United States.

Metallica was formed in Los Angeles in late 1981 when Danish-born drummer Lars Ulrich placed an advertisement in a Los Angeles newspaper, "The Recycler", which read, "Drummer looking for other metal musicians to jam with Tygers of Pan Tang, Diamond Head and Iron Maiden." Guitarists James Hetfield and Hugh Tanner of Leather Charm answered the advertisement. Although he had not formed a band, Ulrich asked Metal Blade Records founder Brian Slagel if he could record a song for the label's upcoming compilation album, "Metal Massacre". Slagel accepted, and Ulrich recruited Hetfield to sing and play rhythm guitar. The band was officially formed on October 28, 1981, five months after Ulrich and Hetfield first met.
The band name came from Ulrich's friend Ron Quintana, who was brainstorming names for a fanzine and was considering "MetalMania" or "Metallica". After hearing the two names, Ulrich wanted Metallica for his band, so he suggested Quintana use MetalMania instead. Dave Mustaine replied to an advertisement for a lead guitarist; Ulrich and Hetfield recruited him after seeing his expensive guitar equipment. In early 1982, Metallica recorded its first original song, "Hit the Lights", for the "Metal Massacre I" compilation. Hetfield played bass, rhythm guitar and sang while Lloyd Grant was credited with a guitar solo and Lars Ulrich played drums. "Metal Massacre I" was released on June 14, 1982; early pressings listed the band incorrectly as "Mettallica", angering the band. The song generated word of mouth and the band played its first live performance on March 14, 1982, at Radio City in Anaheim, California, with newly recruited bassist Ron McGovney. Their first live success came early; they were chosen to open for British heavy metal band Saxon at one gig of their 1982 US tour. This was Metallica's second gig. Metallica recorded its first demo, "Power Metal", whose name was inspired by Quintana's early business cards in early 1982.

The term "thrash metal" was coined in February 1984 by "Kerrang!" journalist Malcolm Dome in reference to Anthrax's song "Metal Thrashing Mad". Prior to this, Hetfield referred to Metallica's sound as "power metal". In late 1982, Ulrich and Hetfield attended a show at the West Hollywood nightclub Whisky a Go Go, which featured bassist Cliff Burton in the band Trauma. The two were "blown away" by Burton's use of a wah-wah pedal and asked him to join Metallica. Hetfield and Mustaine wanted McGovney to leave because they thought he "didn't contribute anything, he just followed". Although Burton initially declined the offer, by the end of the year, he had accepted on the condition the band move to El Cerrito in the San Francisco Bay Area. Metallica's first live performance with Burton was at the nightclub The Stone in March 1983, and the first recording to feature Burton was the "Megaforce" demo (1983).

Metallica was ready to record their debut album, but when Metal Blade was unable to cover the cost, they began looking for other options. Concert promoter Johny "Z" Zazula, who had heard the demo "No Life 'til Leather" (1982), offered to broker a record deal between Metallica and New York City-based record labels. After those record labels showed no interest, Zazula borrowed enough money to cover the recording budget and signed Metallica to his own label, Megaforce Records.

In May 1983, Metallica traveled to Rochester, New York to record its debut album, "Metal Up Your Ass", which was produced by Paul Curcio. The other members decided to eject Mustaine from the band because of his drug and alcohol abuse, and violent behavior just before the recording sessions on April 11, 1983. Exodus guitarist Kirk Hammett replaced Mustaine the same afternoon. Metallica's first live performance with Hammett was on April 16, 1983, at a nightclub in Dover, New Jersey called The Showplace; the support act was Anthrax's original line-up, which included Dan Lilker and Neil Turbin. This was the first time the two bands performed live together.

Mustaine, who went on to form Megadeth, has expressed his dislike for Hammett in interviews, saying Hammett "stole" his job. Mustaine was "pissed off" because he believes Hammett became popular by playing guitar leads that Mustaine himself had written. In a 1985 interview with "Metal Forces", Mustaine said, "it's real funny how Kirk Hammett ripped off every lead break I'd played on that "No Life 'til Leather" tape and got voted No. 1 guitarist in your magazine". On Megadeth's debut album "Killing Is My Business... and Business Is Good!" (1985), Mustaine included the song "Mechanix", which Metallica had previously reworked and retitled "The Four Horsemen" on "Kill 'Em All". Mustaine said he did this to "straighten Metallica up" because Metallica referred to Mustaine as a drunk and said he could not play guitar.
Because of conflicts with its record label and the distributors' refusal to release an album titled "Metal Up Your Ass", the album was renamed "Kill 'Em All". It was released on Megaforce Records in the U.S. and on Music for Nations in Europe, and peaked at number 155 on the "Billboard" 200 in 1986. Although the album was not initially a financial success, it earned Metallica a growing fan base in the underground metal scene. To support the release, Metallica embarked on the Kill 'Em All for One tour with Raven. In February 1984, Metallica supported Venom on the Seven Dates of Hell tour, during which the bands performed in front of 7,000 people at the Aardschok Festival in Zwolle, Netherlands.

Metallica recorded its second studio album, "Ride the Lightning", at Sweet Silence Studios in Copenhagen, Denmark. It was released in August 1984 and reached number 100 on the "Billboard" 200. A French printing press mistakenly printed green covers for the album, which are now considered collectors' items. Mustaine received writing credit for "Ride the Lightning" and "The Call of Ktulu".

Elektra Records A&R director Michael Alago, and co-founder of Q-Prime Management Cliff Burnstein, attended a Metallica concert in September 1984. They were impressed with the performance, signed Metallica to Elektra, and made the band a client of Q-Prime Management. Metallica's growing success was such that the band's British label Music for Nations released "Creeping Death" as a limited edition single, which sold 40,000 copies as an import in the U.S. Two of the three songs on the recordcover versions of Diamond Head's "Am I Evil?" and Blitzkrieg's "Blitzkrieg"appeared on the 1988 Elektra reissue of "Kill 'Em All". Metallica embarked on its first major European tour with Tank to an average crowd of 1,300. Returning to the U.S., it embarked upon a tour co-headlining with W.A.S.P. and supported by Armored Saint. Metallica played its largest show at the Monsters of Rock festival at Donington Park, England, on August 17, 1985, with Bon Jovi and Ratt, playing to 70,000 people. At a show in Oakland, California, at the Day on the Green festival, the band played to a crowd of 60,000.

Metallica's third studio album, "Master of Puppets", was recorded at Sweet Silence Studios and was released in March 1986. The album reached number 29 on the "Billboard" 200 and spent 72 weeks on the chart. It was the band's first album to be certified gold on November 4, 1986, and was certified six times platinum in 2003. Steve Huey of AllMusic considered the album "the band's greatest achievement". Following the release of the album, Metallica supported Ozzy Osbourne on a U.S. tour. Hetfield broke his wrist while skateboarding; he continued with the tour, performing vocals, with guitar technician John Marshall playing rhythm guitar.

On September 27, 1986, during the European leg of Metallica's Damage, Inc. Tour, members drew cards to determine which bunks on the tour bus they would sleep in. Burton won and chose to sleep in Hammett's bunk. At around sunrise near Dörarp, Sweden, the bus driver lost control and skidded, which caused the bus to overturn several times. Ulrich, Hammett, and Hetfield sustained no serious injuries; however, bassist Burton was pinned under the bus and died. Hetfield said:
I saw the bus lying right on him. I saw his legs sticking out. I freaked. The bus driver, I recall, was trying to yank the blanket out from under him to use for other people. I just went, 'Don't fucking do that!' I already wanted to kill the [bus driver]. I don't know if he was drunk or if he hit some ice. All I knew was, he was driving and Cliff wasn't alive anymore.

Burton's death left Metallica's future in doubt. The three remaining members decided Burton would want them to carry on, and with the Burton family's blessings the band sought a replacement. Roughly 40 people, including Hammett's childhood friend, Les Claypool of Primus, Troy Gregory of Prong, and Jason Newsted, formerly of Flotsam and Jetsam, auditioned for the band. Newsted learned Metallica's entire set list; after the audition Metallica invited him to Tommy's Joynt in San Francisco. Hetfield, Ulrich, and Hammett decided on Newsted as Burton's replacement; Newsted's first live performance with Metallica was at the Country Club in Reseda, California. The members initiated Newsted by tricking him into eating a ball of wasabi.

After Newsted joined Metallica, the band left its El Cerrito practice spacea suburban house formerly rented by sound engineer Mark Whitaker dubbed "the Metalli-mansion"and relocated to the adjacent cities of Berkeley and Albany before eventually settling in the Marin County city of San Rafael, north of San Francisco.

Metallica finished its tour in the early months of 1987. In March 1987, Hetfield again broke his wrist while skateboarding, forcing the band to cancel an appearance on "Saturday Night Live". In August 1987, an all-covers extended play (EP) titled "" was released. The EP was recorded in an effort to use the band's newly constructed recording studio, test Newsted's talents, and to relieve grief and stress following the death of Burton. A video titled "Cliff 'Em All" commemorating Burton's three years in Metallica was released in 1987; the video included bass solos, home videos, and pictures.

Metallica's first studio album since Burton's death, "...And Justice for All", was released in 1988. The album was a commercial success, reaching number six on the "Billboard" 200, and was the band's first album to enter the top 10. The album was certified platinum nine weeks after its release. There were complaints about the production; Steve Huey of AllMusic said Ulrich's drums were clicking more than thudding, and the guitars "buzz thinly". To promote the album, Metallica embarked on a tour called Damaged Justice.
In 1989, Metallica received its first Grammy Award nomination for "...And Justice for All" in the new Best Hard Rock/Metal Performance Vocal or Instrument category. Metallica was the favorite to win but the award was given to Jethro Tull for the album "Crest of a Knave". The award was controversial with fans and the press; Metallica was standing off-stage waiting to receive the award after performing the song "One". Jethro Tull had been advised by its manager not to attend the ceremony because he was expecting Metallica to win. The award was named in "Entertainment Weekly" "Grammy's 10 Biggest Upsets".

Following the release of "...And Justice for All", Metallica released its debut music video for the song "One", which the band performed in an abandoned warehouse. The footage was remixed with the film "Johnny Got His Gun". Rather than organize an ongoing licensing deal, Metallica purchased the rights to the film. The remixed video was submitted to MTV with an alternative, performance-only version that was held back in case MTV banned the remixed version. MTV accepted the remixed version; the video was viewers' first exposure to Metallica. In 1999, it was voted number 38 in MTV's "Top 100 Videos of All Time" countdown; it was featured in the network's 25th Anniversary edition of "ADD Video", which showcased the most popular videos on MTV in the last 25 years.

In October 1990, Metallica entered One on One Recording's studio in North Hollywood to record its next album. Bob Rock, who had worked with Aerosmith, The Cult, Bon Jovi, and Mötley Crüe, was hired as the producer. "Metallica"also known as "The Black Album"was remixed three times, cost , and ended three marriages. Although the release was delayed until 1991, "Metallica" debuted at number one in ten countries, selling 650,000 units in the U.S. during its first week. The album brought Metallica mainstream attention; it has been certified 16 times platinum in the U.S., which makes it the 25th-best-selling album in the country. The making of "Metallica" and the following tour was documented in "A Year and a Half in the Life of Metallica". The tour in support of the album, called the Wherever We May Roam Tour, lasted 14 months and included dates in the U.S., Japan, and the UK. In April 1992, Metallica appeared at The Freddie Mercury Tribute Concert and performed a three-song set. Hetfield later performed "Stone Cold Crazy" with the remaining members of Queen and Tony Iommi.

On August 8, 1992, during the co-headlining Guns N' Roses/Metallica Stadium Tour, Hetfield suffered second and third degree burns to his arms, face, hands, and legs. There had been some confusion with the new pyrotechnics setup, which resulted in Hetfield walking into a flame during "Fade to Black". Newsted said Hetfield's skin was "bubbling like on "The Toxic Avenger"". Metallica returned to the stage 17 days later with guitar technician and Metal Church member John Marshall replacing Hetfield on guitar for the remainder of the tour, although Hetfield was able to sing. Later in 1993, Metallica went on the Nowhere Else to Roam Tour, playing five shows in Mexico City. "", the band's first box set, was released in November 1993. The collection contained three live CDs, three home videos, and a book filled with riders and letters.

After almost three years of touring to promote "Metallica", including a headlining performance at Woodstock '94, Metallica returned to the studio to write and record its sixth studio album. The band went on a brief hiatus in the summer of 1995 and played a short tour, Escape from the Studio '95, comprising three outdoor shows, including a headline show at Donington Park supported by Slayer, Skid Row, Slash's Snakepit, Therapy?, and Corrosion of Conformity. The band spent about a year writing and recording new songs, resulting in the release of "Load" in 1996. "Load" debuted at number one on the "Billboard" 200 and ARIA Charts; it was the band's second number-one album. The cover art, "Blood and Semen III", was created by Andres Serrano, who pressed a mixture of his own semen and blood between sheets of plexiglass. The release marked a change in the band's musical direction and a new image; the bandmembers' hair was cut. Metallica headlined the alternative rock festival Lollapalooza festival in mid-1996.
During early production of the album, the band had recorded enough material to fill a double album. It was decided that half of the songs were to be released; the band would continue to work on the remaining songs and release them the following year. This resulted in follow-up album titled "Reload". The cover art was again created by Serrano, this time using a mixture of blood and urine. "Reload" debuted at number one on the "Billboard" 200 and reached number two on the Top Canadian Album chart. Hetfield said in the 2004 documentary film "" that the band initially thought some of the songs on these albums were of average quality; these were "polished and reworked" until judged releasable. To promote "Reload", Metallica performed "Fuel" and "The Memory Remains" with Marianne Faithfull on NBC's "Saturday Night Live" in December 1997.

In 1998, Metallica compiled a double album of cover songs, "Garage Inc." The first disc contained newly recorded covers of songs by Diamond Head, Killing Joke, the Misfits, Thin Lizzy, Mercyful Fate, Black Sabbath, and others. The second disc featured the original version of "The $5.98 E.P.: Garage Days Re-Revisited", which had become a scarce collectors' item. The album entered the "Billboard" 200 at number two.

On April 21 and 22, 1999, Metallica recorded two performances with the San Francisco Symphony conducted by Michael Kamen, who had previously worked with producer Rock on "Nothing Else Matters". Kamen approached Metallica in 1991 with the idea of pairing the band's music with a symphony orchestra. Kamen and his staff of over 100 composed additional orchestral material for Metallica songs. Metallica wrote two new Kamen-scored songs for the event, "No Leaf Clover" and "-Human". The audio recording and concert footage were released in 1999 as the album and concert film "S&M". It entered the "Billboard" 200 at number two and the Australian ARIA charts and Top Internet Albums chart at number one.

In 2000, Metallica discovered that a demo of its song "I Disappear", which was supposed to be released in combination with the , was receiving radio airplay. Tracing the source of the leak, the band found the file on the Napster peer-to-peer file-sharing network, and also found that the band's entire catalogue was freely available. Legal action was initiated against Napster; Metallica filed a lawsuit at the U.S. District Court, Central District of California, alleging that Napster violated three areas of the law: copyright infringement, unlawful use of digital audio interface device, and the Racketeer Influenced and Corrupt Organizations Act (RICO).
Ulrich provided a statement to the Senate Judiciary Committee regarding copyright infringement on July 11, 2000. Federal Judge Marilyn Hall Patel ordered the site to place a filter on the program within 72 hours or be shut down. A settlement between Metallica and Napster was reached when German media conglomerate Bertelsmann BMG showed interest in purchasing the rights to Napster for $94 million. Under the terms of settlement, Napster agreed to block users who shared music by artists who do not want their music shared. On June 3, 2002, Napster filed for Chapter 11 protection under U.S. bankruptcy laws. On September 3, 2002, an American bankruptcy judge blocked the sale of Napster to Bertelsmann and forced Napster to liquidate its assets according to Chapter 7 of the U.S. bankruptcy laws.

At the 2000 MTV Video Music Awards, Ulrich appeared with host Marlon Wayans in a skit that criticized the idea of using Napster to share music. Marlon played a college student listening to Metallica's "I Disappear". Ulrich walked in and asked for an explanation. Ulrich responded to Wayans' excuse that using Napster was just "sharing" by saying that Wayans' idea of sharing was "borrowing things that were not yours without asking". He called in the Metallica road crew, who proceeded to confiscate all of Wayans' belongings, leaving him almost naked in an empty room. Napster creator Shawn Fanning responded later in the ceremony by presenting an award wearing a Metallica shirt, saying, "I borrowed this shirt from a friend. Maybe, if I like it, I'll buy one of my own." Ulrich was later booed on stage at the award show when he introduced the final musical act, Blink-182.
Newsted left Metallica on January 17, 2001, as plans were being made to enter the recording studio. He said he left the band for "private and personal reasons, and the physical damage I have done to myself over the years while playing the music that I love". During a "Playboy" interview with Metallica, Newsted said he wanted to release an album with his side project, Echobrain. Hetfield was opposed to the idea and said, "When someone does a side project, it takes away from the strength of Metallica", and that a side project is "like cheating on your wife in a way". Newsted said Hetfield had recorded vocals for a song used in the film "", and appeared on two Corrosion of Conformity albums. Hetfield replied, "My name isn't on those records. And I'm not out trying to sell them", and raised questions such as, "Where would it end? Does he start touring with it? Does he sell shirts? Is it his band?"

In April 2001, filmmakers Joe Berlinger and Bruce Sinofsky began following Metallica to document the recording process of the band's next studio album. Over two years they recorded more than 1,000 hours of footage. On July 19, 2001, before preparations to enter the recording studio, Hetfield entered rehab to treat his "alcoholism and other addictions". All recording plans were put on hold and the band's future was in doubt. Hetfield left rehab on December 4, 2001, and the band returned to the recording studio on April 12, 2002. Hetfield was required to limit his work to four hours a day between noon and 4 pm, and to spend the rest of his time with his family. The footage recorded by Berlinger and Sinofsky was compiled into the documentary "", which premiered at the Sundance Film Festival in January 2004. In the documentary, Newsted said his former bandmates' decision to hire a therapist to help solve their problems which he felt they could have solved on their own was "really fucking lame and weak".

In June 2003, Metallica's eighth studio album, "St. Anger", debuted at number one on the "Billboard" 200, and drew mixed reactions from critics. Ulrich's "steely" sounding snare drum and the absence of guitar solos received particular criticism. Kevin Forest Moreau of "Shakingthrough.net" said, "the guitars stumble in a monotone of mid-level, processed rattle; the drums don't propel as much as struggle to disguise an all-too-turgid pace; and the rage is both unfocused and leavened with too much narcissistic navel-gazing". Brent DiCrescenzo of "Pitchfork" described it as "an utter mess". However, "Blender" magazine called it the "grimiest and grimmest of the band's Bob Rock productions", and "New York Magazine" called it "utterly raw and rocking". The title track, "St. Anger", won the Grammy Award for Best Metal Performance in 2004; it was used as the official theme song for WWE's "SummerSlam 2003".
For the duration of "St. Anger"s recording period, producer Bob Rock played bass on the album and in several live shows at which Metallica performed during that time. Once the record was completed, the band started to hold auditions for Newsted's permanent replacement. Bassists Pepper Keenan, Jeordie White, Scott Reeder, Eric Avery, Danny Lohner, and Chris Wyseamong othersauditioned for the role. After three months of auditions, Robert Trujillo, formerly of Suicidal Tendencies and Ozzy Osbourne's band, was chosen as the new bassist. Newsted, who had joined Canadian thrash metal band Voivod by that time, was Trujillo's replacement in Osbourne's band during the 2003 Ozzfest tour, which included Voivod.

Before the band's set at the 2004 Download Festival, Ulrich was rushed to the hospital after having an anxiety seizure and was unable to perform. Hetfield searched for last-minute volunteers to replace Ulrich. Slayer drummer Dave Lombardo and Slipknot drummer Joey Jordison volunteered. Lombardo performed "Battery" and "The Four Horsemen", Ulrich's drum technician Flemming Larsen performed "Fade to Black", and Jordison performed the remainder of the set. Having toured for two years in support of "St. Anger" on the Summer Sanitarium Tour 2003 and the Madly in Anger with the World Tour, with multi-platinum rock band Godsmack in support, Metallica took a break from performing and spent most of 2005 with friends and family. The band opened for The Rolling Stones at SBC Park in San Francisco on November 13 and 15, 2005.

In December 2006, Metallica released a DVD titled "The Videos 1989–2004", which sold 28,000 copies in its first week and entered the "Billboard" Top Videos chart at number three. Metallica recorded a guitar-based interpretation of Ennio Morricone's "The Ecstasy of Gold" for a tribute album titled "We All Love Ennio Morricone", which was released in February 2007. The track received a Grammy nomination at the 50th Grammy Awards for the category "Best Rock Instrumental Performance". A recording of "The Ecstasy of Gold" has been played to introduce Metallica's performances since the 1980s. Earlier that year, Metallica announced on its official website that after 15 years, long-time producer Bob Rock would not be producing the band's next studio album. Instead, the band chose to work with producer Rick Rubin. Metallica scheduled the release of "Death Magnetic" as September 12, 2008, and the band filmed a music video for the album's first single, "The Day That Never Comes".
On September 2, 2008, a record store in France began selling copies of "Death Magnetic" nearly two weeks before its scheduled worldwide release date, which resulted in the album being made available on peer-to-peer clients. This prompted the band's UK distributor Vertigo Records to officially release the album on September 10, 2008. Rumors of Metallica or Warner Bros. taking legal action against the French retailer were unconfirmed, though drummer Lars Ulrich responded to the leak by saying, "...We're ten days from release. I mean, from here, we're golden. If this thing leaks all over the world today or tomorrow, happy days. Happy days. Trust me", and, "By 2008 standards, that's a victory. If you'd told me six months ago that our record wouldn't leak until 10 days out, I would have signed up for that."

"Death Magnetic" debuted at number one in the U.S. selling 490,000 units; Metallica became the first band to have five consecutive studio albums debut at number one in the history of the "Billboard" 200. A week after its release, "Death Magnetic" remained at number one on the "Billboard" 200 and the European album chart; it also became the fastest selling album of 2008 in Australia. "Death Magnetic" remained at number one on the "Billboard" 200 album chart for three consecutive weeks. Metallica was one of two artists whose albumthe other being Jack Johnson's album "Sleep Through the Static"remained on the "Billboard" 200 for three consecutive weeks at number one in 2008. "Death Magnetic" also remained at number one on "Billboard"'s Hard Rock, Modern Rock/Alternative and Rock album charts for five consecutive weeks. The album reached number one in 32 countries outside the U.S., including the UK, Canada, and Australia. In November 2008, Metallica's record deal with Warner Bros. ended and the band considered releasing its next album through the internet.

On January 14, 2009, it was announced that Metallica would be inducted into the Rock and Roll Hall of Fame on April 4, 2009, and that former bassist Jason Newstedwho left the band in 2001would perform with the band at the ceremony. Initially, it was announced that the matter had been discussed and that bassist Trujillo had agreed not to play because he "wanted to see the Black Album band". However, during the band's set of "Master of Puppets" and "Enter Sandman", both Trujillo and Newsted were on stage. Ray Burton, father of the late Cliff Burton, accepted the honor on his behalf. Although he was not to be inducted with them, Metallica invited Dave Mustaine to take part in the induction ceremony. Mustaine declined because of his touring commitments in Europe.
Metallica, Slayer, Megadeth, and Anthrax performed on the same bill for the first time on June 16, 2010, at Warsaw Babice Airport, Warsaw, as a part of the Sonisphere Festival series. The show in Sofia, Bulgaria, on June 22, 2010, was broadcast via satellite to cinemas. The bands also played concerts in Bucharest on June 26, 2010, and Istanbul on June 27, 2010. On June 28, 2010, "Death Magnetic" was certified double platinum by the RIAA. Metallica's World Magnetic Tour ended in Melbourne on November 21, 2010. The band had been touring for over two years in support of "Death Magnetic". To accompany the final tour dates in Australia and New Zealand, a live, limited edition EP of past performances in Australia called "Six Feet Down Under" was released. The EP was followed by "Six Feet Down Under (Part II)", which was released on November 12, 2010. Part 2 contains a further eight songs recorded during the first two Oceanic Legs of the World Magnetic Tour. On November 26, 2010, Metallica released a live EP titled "Live at Grimey's", which was recorded in June 2008 at Grimey's Record Store, just before the band's appearance at Bonnaroo Music Festival that year.

In a June 2009 interview with Italy's Rock TV, Ulrich said Metallica was planning to continue touring until August 2010, and that there were no plans for a tenth album. He said he was sure the band would collaborate with producer Rick Rubin again. According to Blabbermouth.net, the band was considering recording its next album in the second half of 2011. In November 2010, during an interview with The Pulse of Radio, Ulrich said Metallica would return to writing in 2011. Ulrich said, "There's a bunch of balls in the air for 2011, but I think the main one is we really want to get back to writing again. We haven't really written since, what, '06, '07, and we want to get back to kind of just being creative again. Right now we are going to just chill out and then probably start up again in, I'd say, March or April, and start probably putting the creative cap back on and start writing some songs."
In an interview at the April 2011 Big Four concert, Robert Trujillo said Metallica will work with Rick Rubin again as producer for the new album and were "really excited to write some new music. There's no shortage of riffage in Metallica world right now." He added, "The first album with Rick was also the first album for me, so in a lot of ways, you're kind of testing the water. Now that we're comfortable with Rick and his incredible engineer, Greg Fidelman, who worked with Slayer, actually, on this last recordit's my heroit's a great team. And it's only gonna better; I really believe that. So I'm super-excited." In June 2011, Rubin said Metallica had begun writing its new album. On November 9, 2010, Metallica announced it would be headlining the Rock in Rio festival in Rio de Janeiro on September 25, 2011.

On December 13, 2010, the band announced it would again play as part of the "big four" during the Sonisphere Festival at Knebworth House, Hertfordshire, on July 8, 2011. It was the first time all of the "big four" members played on the same stage in the UK. On December 17, 2010, Another "big four" Sonisphere performance that would take place in France on July 9 was announced. On January 25, 2011, another "big four" performance on April 23, 2011, at the Empire Polo Club in Indio, California, was announced. It was the first time all of the "big four" members played on the same stage in the U.S. On February 17, 2011, a show in Gelsenkirchen, Germany, on July 2, 2011, was announced. On February 22, a "big four" show in Milan on July 6, 2011, was announced. On March 2, 2011, another "big four" concert, which took place in Gothenburg on July 3, 2011, was announced. The final "big four" concert was in New York City, at Yankee Stadium, on September 14, 2011.

On June 15, 2011, Metallica announced that recording sessions with singer-songwriter Lou Reed had concluded. The album, which was titled "Lulu", was recorded over several months and comprised ten songs based on Frank Wedekind's "Lulu" plays "Earth Spirit" and "Pandora's Box". The album was released on October 31, 2011. The recording of the album was problematic at times; Lars Ulrich later said Lou Reed challenged him to a "street fight". On October 16, 2011, Robert Trujillo confirmed that the band was back in the studio and writing new material. He said, "The writing process for the new Metallica album has begun. We've been in the studio with Rick Rubin, working on a couple of things, and we're going to be recording during the most of next year."
Metallica was due to make its first appearance in India at the "India Rocks" concert, supporting the 2011 Indian Grand Prix. However, the concert was canceled when the venue was proven to be unsafe. Fans raided the stage during the event and the organizers were later arrested for fraud. Metallica made its Indian debut in Bangalore on October 30, 2011. On November 10, it was announced that Metallica would headline the main stage on Saturday June 9, 2012, at the Download Festival at Donington Park and that the band would play "The Black Album" in its entirety. Metallica celebrated its 30th anniversary by playing four shows at the Fillmore in San Francisco in December 2011. The shows were exclusive to Met Club members and tickets were charged at $6 each or $19.81 for all four nights. The shows consisted of songs from the band's career and featured guest appearances by artists who had either helped or had influenced Metallica. These shows were notable because Lloyd Grant, Dave Mustaine, Jason Newsted, Glenn Danzig, Ozzy Osbourne, Jerry Cantrell, Apocalyptica, members of Diamond Head, and King Diamond joined Metallica on stage for all appropriate songs. In December 2011, Metallica began releasing songs that were written for "Death Magnetic" but were not included on the album online. On December 13, 2011, the band released "Beyond Magnetic", a digital EP release exclusively on iTunes. It was released on CD in January 2012.

On February 7, 2012, Metallica announced that it would start a new music festival called Orion Music + More, which took place on June 23 and 24, 2012, in Atlantic City. Metallica also confirmed that it would headline the festival on both days and would perform two of its most critically acclaimed albums in their entirety: "The Black Album" on one night, and "Ride the Lightning" on the other. In a July 2012 interview with Canadian radio station 99.3 The Fox, Ulrich said Metallica would not release its new album until at least early 2014. In November 2012, Metallica left Warner Bros. Records and launched an independent record label, Blackened Recordings, which will produce the band's future releases. The band has acquired the rights to all of its studio albums, which will be reissued through the new label. Blackened releases will be licensed through Warner subsidiary Rhino Entertainment in North America and internationally through Universal Music. On September 20, 2012, Metallica announced via its official website that a new DVD containing footage of shows it performed in Quebec in 2009 would be released that December; fans would get the chance to vote for two setlists that would appear on the DVD. The film, titled "Quebec Magnetic", was released in the U.S. on December 10, 2012.
In an interview with "Classic Rock" on January 8, 2013, Ulrich said regarding the band's upcoming album, "What we're doing now certainly sounds like a continuation [of "Death Magnetic"]". He also said, "I love Rick [Rubin]. We all love Rick. We're in touch with Rick constantly. We'll see where it goes. It would stun me if the record came out in 2013." Also in 2013, the band starred in a 3D concert film titled "", which was directed by Antal Nimród and was released in IMAX theaters on September 27. In an interview dated July 22, 2013, Ulrich told "Ultimate Guitar", "2014 will be all about making a new Metallica record"; he said the album will most likely be released during 2015. Kirk Hammett and Robert Trujillo later confirmed the band's intention to enter the studio. At the second Orion Music + More festival held in Detroit, the band played under the name "Dehaan"a reference to actor Dane DeHaan, who starred in "Metallica: Through the Never". The band performed its debut album "Kill 'Em All" in its entirety, celebrating the 30th anniversary of its release. On December 8, 2013, the band played a show called "Freeze 'Em All" in Antarctica, becoming the first band to play on all seven continents. The performance was filmed and released as a live album the same month.

At the 56th Annual Grammy Awards in January 2014, Metallica performed "One" with Chinese pianist Lang Lang. In March 2014, Metallica began a tour called "Metallica By Request", in which fans request songs for the band to perform. A new song, titled "Lords of Summer" was written for the concerts and released as a "first take" demo in June 2014. In June 2014, the band headlined the Glastonbury Festival in an attempt to attract new fans. Ulrich said, "We have one shot, you never know if you'll be invited back". In November 2014, Metallica performed at the closing ceremony of BlizzCon 2014. In January 2015, Metallica announced a "Metallica Night" with the San Jose Sharks, which featured a Q&A session with the band and a charity auction benefiting the San Francisco Bay Chapter of the Sierra Club, but no performances. They were announced to headline Lollapalooza in March 2015, returning to perform there for the first time in 20 years. On May 2, 2015, Metallica performed their third annual Metallica Day at AT&T Park. Metallica were also announced to play at X Games for the first time at X Games Austin 2015 in Austin, Texas. On June 14, 2015, Hetfield and Hammett performed The Star-Spangled Banner live via electric guitars prior to game 5 of the NBA Finals between the Cleveland Cavaliers and Golden State Warriors at Oracle Arena in Oakland, California. In late October, the band unveiled a new website with an introduction from Ulrich containing footage from the studio of the band working on new material. On November 2, Metallica were announced to play "The Night Before" Super Bowl 50 at AT&T Park. Metallica announced they would be opening the U.S. Bank Stadium on August 20, 2016, with Avenged Sevenfold and Volbeat as support.
In April 2016, during the week leading up to Record Store Day, for which the band was its ambassador for 2016, Ulrich told "Billboard" that the band's expanded role within the music industry had played a part in the amount of time that it had taken to write and record the album. "The way we do things now is very different than the way we did things back in the days of "Kill 'Em All" and "Ride the Lightning". Nowadays we like to do so many different things." Ulrich was also optimistic that production of the album had almost reached its completion. "Unless something radical happens it would be difficult for me to believe that it won't come out in 2016". On August 18, 2016, the band announced via their website that their tenth studio album, "Hardwired... to Self-Destruct", would be released worldwide on November 18, 2016, via their independent label, Blackened Recordings. They also unveiled the track listing, album artwork, and released a music video for the album's first single, "Hardwired". The album was released as scheduled and debuted at number one on the "Billboard" 200.

Metallica announced they would be touring the US in summer of 2017 for the WorldWired Tour. The stadium tour also includes Avenged Sevenfold, Volbeat and Gojira as supporting acts. On August 7, 2017, Metallica was invited by the San Francisco Giants again for the fifth annual "Metallica Night" with Hammett and Hetfield performing the national anthem. In January 2018, the band announced that they would be reissuing "The $5.98 E.P.: Garage Days Re-Revisited" on April 13 for Record Store Day, and the sixth annual "Metallica Night" was also announced a few weeks later, this time in April, with all proceeds going to the All Within My Hands Foundation, which the band created in late 2017. In February 2018, the band announced a second set of North American tour dates, most of which for cities that they had not visited in up to thirty years.

In an interview with Australian magazine "The Music"s official podcast in March 2019, Trujillo said that Metallica had begun jamming on new material for its next studio album. "I'm excited about the next record because I believe it will also be a culmination of the two [previous] records and another journey. There's no shortage of original ideas, that's the beauty of being in this band." He estimated that the album would be released "a lot sooner than the previous two did... this time around I think we'll be able to jump on it a lot quicker and jump in the studio and start working. We've all vowed to get this one going sooner than later." In an interview with Australian magazine "Mixdown" the following month, Hammett said that the band had tentative plans to enter the studio after the conclusion of its WorldWired Tour. He stated, "We're in our third year since "Hardwired". Maybe we can get a bit more focus and go into the studio a bit sooner." After not contributing any writing to "Hardwired... to Self-Destruct", Hammett said regarding his ideas for the new album, "I have a ton of material. I've over-compensated, so I'm ready to go anytime."

In March 2019, Metallica announced that its WorldWired Tour would continue into Australia and New Zealand in October with Slipknot in support. Later that month, the band announced that it would perform at the grand opening of San Francisco's new Chase Center with the San Francisco Symphony in September to celebrate the twenty-year anniversary of "S&M". The commemorative shows, titled "S&M2", were screened in over 3,000 theaters worldwide on October 9; the event featured arrangements from the original "S&M" concerts as well as new arrangements for songs recorded since then and a cover of the Alexander Mosolov piece "Iron Foundry", and were conducted by Edwin Outwater and San Francisco Symphony music director Michael Tilson Thomas. "S&M2" went on to earn $5.5 million at the box office, making it the biggest global rock event cinema release of all time; a second screening was later announced for October 30 as a result.

In July 2019, Metallica announced a set of South American tour dates for April 2020 with Greta Van Fleet in support. In September, ahead of that year's Global Citizen Festival, it was announced that Metallica would perform at the following year's festival in September 2020 alongside artists such as Billie Eilish, Miley Cyrus and Coldplay, in what would be the final event of Global Poverty Project's year-long Global Goal Live: The Possible Dream campaign. The following day, on September 27, Metallica announced that Hetfield had re-entered a rehabilitation program and that its Australia/New Zealand tour would be postponed. In a statement by Ulrich, Hammett and Trujillo, the band spoke of the devastation of the news, saying that Hetfield "[had] been struggling with addiction on and off for many years" and that all tickets would be fully refunded. Ulrich later added that Hetfield was "in the process of healing himself", and that the band hoped to return to Australia and New Zealand in 2020. The band's other commitments, including a benefit concert in March 2020, were still expected to continue as planned; a further five US festival appearances were announced in October.

Metallica was influenced by early heavy metal and hard rock bands and artists Black Sabbath, Deep Purple, Kiss, Led Zeppelin, Queen, Ted Nugent, AC/DC, Rush, Aerosmith, Judas Priest, Scorpions and by new wave of British heavy metal (NWOBHM) bands Venom, Motörhead, Saxon, Diamond Head, Blitzkrieg, and Iron Maiden, and early punk rock bands Ramones, Sex Pistols, and the Misfits also influenced Metallica's style as did post-punk band Killing Joke. The band's early releases contained fast tempos, harmonized leads, and nine-minute instrumental tracks. Steve Huey of AllMusic said "Ride the Lightning" featured "extended, progressive epics; tight, concise groove-rockers". Huey said Metallica expanded its compositional technique and range of expression to take on a more aggressive approach in following releases, and lyrics dealt with personal and socially conscious issues. Religious and military leaders, rage, insanity, monsters, and drugsamong other themeswere explored on "Master of Puppets".

In 1991, Huey said Metallica with new producer Bob Rock simplified and streamlined its music for a more commercial approach to appeal to mainstream audiences. Robert Palmer of "Rolling Stone" said the band abandoned its aggressive, fast tempos to expand its music and expressive range. The change in direction proved commercially successful; "Metallica" was the band's first album to peak at number one on the "Billboard" 200. Metallica noticed changes to the rock scene created by the grunge movement of the early 1990s. In "Load"—an album that has been described as having "an almost alternative rock" approach—the band changed musical direction and focused on non-metal influences. Metallica's new lyrical approach moved away from drugs and monsters, and focused on anger, loss, and retribution. Some fans and critics were not pleased with this change, which included haircuts, the cover art of "Load", and headlining the Lollapalooza festival of 1996. David Fricke of "Rolling Stone" described the move as "goodbye to the moldy stricture and dead-end Puritanism of no-frills thrash", and called "Load" the heaviest record of 1996. With the release of "ReLoad" in 1997, the band displayed blues and early hard rock influences, incorporating more rhythm and harmony in song structures.

"St. Anger" marked another large change in the band's sound. Guitar solos were excluded from the album, leaving a "raw and unpolished sound". The band used drop C tuning; Ulrich's snare drum received particular criticism. "New York Magazine"s Ethan Brown said it "reverberates with a thwong". The album's lyrics deal with Hetfield's drug rehabilitation and include references to the devil, anti-drug themes, claustrophobia, impending doom, and religious hypocrisy. At the advice of producer Rick Rubin, for its ninth studio album "Death Magnetic", the band returned to standard tuning and guitar solos. As a return to Metallica's thrash roots, "Death Magnetic" was a riff-oriented album featuring intense guitar solos and subtle lyrics dealing with suicide and redemption.

Metallica has become one of the most influential heavy metal bands of all time, and is credited as one of the "big four" of thrash metal, along with Slayer, Anthrax, and Megadeth. The band has sold more than 125 million records worldwide, including an RIAA-certified 66 million and Nielsen SoundScan-reported 58,000,000 in the US, making Metallica one of the most commercially successful bands of all time. The writers of "The Rolling Stone Encyclopedia of Rock & Roll" said Metallica gave heavy metal "a much-needed charge". Stephen Thomas Erlewine and Greg Prato of Allmusic said Metallica "expanded the limits of thrash, using speed and volume not for their own sake, but to enhance their intricately structured compositions", and called the band "easily the best, most influential heavy metal band of the '80s, responsible for bringing the music back to Earth".

Jonathan Davis of Korn said he respects Metallica as his favorite band; he said, "I love that they've done things their own way and they've persevered over the years and they're still relevant to this day. I think they're one of the greatest bands ever." Godsmack drummer Shannon Larkin said Metallica has been the biggest influence on the band, stating, "they really changed my life when I was 16 years oldI'd never heard anything that heavy". Vocalist and guitarist Robb Flynn of Machine Head said that when creating the band's 2007 album, "The Blackening", "What we mean is an album that has the power, influence and epic grandeur of that album "Master of Puppets"and the staying powera timeless record like that". Trivium guitarists Corey Beaulieu and Matt Heafy said that when they heard Metallica they wanted to start playing guitar. M. Shadows of Avenged Sevenfold said touring with Metallica was the band's career highlight, and said, "Selling tons of records and playing huge shows will never compare to meeting your idols "Metallica"". God Forbid guitarists Doc and Dallas Coyle were inspired by Metallica as they grew up, and the band's bassist John Outcalt admires Burton as a "rocker". Ill Niño drummer Dave Chavarri said he finds early Metallica releases are "heavy, raw, rebellious. It said, 'fuck you'", and Adema drummer Kris Kohls said the band is influenced by Metallica.

On April 4, 2009, Metallica were inducted into the Rock And Roll Hall Of Fame. They entered the Rock and Roll Hall of Fame the second year they were eligible and first year they were nominated. Metallica's induction into the Hall included its current lineup, James Hetfield, Kirk Hammett, Robert Trujillo, and Lars Ulrich, and former members Jason Newsted and Cliff Burton.

MTV ranked Metallica the third "Greatest Heavy Metal Band in History". Metallica was ranked 42nd on VH1's "100 Greatest Artists Of All Time", was listed fifth on VH1's "100 Greatest Artists of Hard Rock", and the band was number one on VH1's "20 Greatest Metal Bands" list. "Rolling Stone" placed the band 61st on its list of "The 100 Greatest Artists of All Time"; its albums "Master of Puppets" and "Metallica" were ranked at numbers 167 and 252 respectively on the magazine's list of "The 500 Greatest Albums of All Time". "Master of Puppets" was named in "Q Magazine" "50 Heaviest Albums of All Time", and was ranked number one on IGN's "Top 25 Metal Albums", and number one on Metal-rules.com's "Top 100 Heavy Metal Albums" list. "Enter Sandman" was ranked number 399 on "Rolling Stone" "500 Greatest Songs of All Time".

"Kerrang!" released a tribute album titled "Master of Puppets: Remastered" with the April 8, 2006, edition of the magazine to celebrate the 20th anniversary of "Master of Puppets". The album featured cover versions of Metallica songs by Machine Head, Bullet for My Valentine, Chimaira, Mastodon, Mendeed, and Triviumall of which are influenced by Metallica. At least 15 Metallica tribute albums have been released. On September 10, 2006, Metallica guest starred on "The Simpsons" eighteenth-season premiere, "The Mook, the Chef, the Wife and Her Homer". Hammett's and Hetfield's voices were used in three episodes of the animated television series "Metalocalypse". Finnish cello metal band Apocalyptica released a tribute album titled "Plays Metallica by Four Cellos", which features eight Metallica songs played on cellos. A parody band named Beatallica plays music using a combination of The Beatles and Metallica songs. Beatallica faced legal troubles when Sony, which owns The Beatles' catalog, issued a cease and desist order, claiming "substantial and irreparable injury" and ordering the group to pay damages. Ulrich, a fan of Beatallica, asked Metallica's lawyer Peter Paterno to help settle the case.

On March 7, 1999, Metallica was inducted into the San Francisco Walk of Fame. The mayor of San Francisco, Willie Brown, proclaimed the day "Official Metallica Day". The band was awarded the MTV Icon award in 2003, and a concert paying tribute to the band with artists performing its songs was held. Performances included Sum 41 and a medley of "For Whom the Bell Tolls", "Enter Sandman", and "Master of Puppets". Staind covered "Nothing Else Matters", Avril Lavigne played "Fuel", hip-hop artist Snoop Dogg performed "Sad but True", Korn played "One", and Limp Bizkit performed "Welcome Home (Sanitarium)".

The "Guitar Hero" video game series included several of Metallica's songs. "One" was used in "Guitar Hero III". The album "Death Magnetic" was later released as purchasable, downloadable content for the game. "Trapped Under Ice" was featured in the sequel, "Guitar Hero World Tour". In 2009, Metallica collaborated with the game's developers to make "", which included a number of Metallica's songs. Harmonix' video game series "Rock Band" included "Enter Sandman" and "Battery"; "Ride the Lightning", "Blackened", and "...And Justice for All" were released as downloadable tracks. In 2013, due to expiring content licenses, "Ride the Lightning", "Blackened", and "...And Justice for All" are no longer available for download.

Current members
Former members

Session/touring members

Grammy Awards


The 1988 re-issue of "Kill 'Em All" on Elektra Records also charted on the Billboard 200, peaking at number 120.




</doc>
<doc id="18816" url="https://en.wikipedia.org/wiki?curid=18816" title="Mural">
Mural

A mural is any piece of artwork painted or applied directly on a wall, ceiling or other permanent surfaces. A distinguishing characteristic of mural painting is that the architectural elements of the given space are harmoniously incorporated into the picture.

Some wall paintings are painted on large canvases, which are then attached to the wall (e.g., with marouflage), but the technique has been in common use since the late 19th century.

Murals of sorts date to Upper Paleolithic times such as the cave paintings in the Lubang Jeriji Saléh cave in Borneo (40,000-52,000 BP), Chauvet Cave in Ardèche department of southern France (around 32,000 BP). Many ancient murals have been found within ancient Egyptian tombs (around 3150 BC), the Minoan palaces (Middle period III of the Neopalatial period, 1700–1600 BC), the Oxtotitlán cave and Juxtlahuaca in Mexico (around 1200-900 BC) and in Pompeii (around 100 BC – AD 79).

During the Middle Ages murals were usually executed on dry plaster (secco). The huge collection of Kerala mural painting dating from the 14th century are examples of fresco secco. In Italy, circa 1300, the technique of painting of frescos on wet plaster was reintroduced and led to a significant increase in the quality of mural painting.
In modern times, the term became more well-known with the Mexican muralism art movement (Diego Rivera, David Siqueiros and José Orozco). There are many different styles and techniques. The best-known is probably "fresco", which uses water-soluble paints with a damp lime wash, rapid use of the resulting mixture over a large surface, and often in parts (but with a sense of the whole). The colors lighten as they dry. The "marouflage" method has also been used for millennia.

Murals today are painted in a variety of ways, using oil or water-based media. The styles can vary from abstract to "trompe-l'œil" (a French term for "fool" or "trick the eye"). Initiated by the works of mural artists like Graham Rust or Rainer Maria Latzke in the 1980s, trompe-l'oeil painting has experienced a renaissance in private and public buildings in Europe.
Today, the beauty of a wall mural has become much more widely available with a technique whereby a painting or photographic image is transferred to poster paper or canvas which is then pasted to a wall surface "(see wallpaper, Frescography)" to give the effect of either a hand-painted mural or realistic scene.

A special type of mural painting is Lüftlmalerei, still practised today in the villages of the Alpine valleys. Well-known examples of such façade designs from the 18th and 19th centuries can be found in Mittenwald, Garmisch, Unter- and Oberammergau.

In the history of mural several methods have been used:

"A fresco" painting, from the Italian word "affresco" which derives from the adjective "fresco" ("fresh"), describes a method in which the paint is applied on plaster on walls or ceilings. 

The "buon fresco" technique consists of painting in pigment mixed with water on a thin layer of wet, fresh, lime mortar or plaster. The pigment is then absorbed by the wet plaster; after a number of hours, the plaster dries and reacts with the air: it is this chemical reaction which fixes the pigment particles in the plaster. After this the painting stays for a long time up to centuries in fresh and brilliant colors.

"Fresco-secco" painting is done on dry plaster ("secco" is "dry" in Italian). The pigments thus require a binding medium, such as egg (tempera), glue or oil to attach the pigment to the wall.

"Mezzo-fresco" is painted on nearly-dry plaster, and was defined by the sixteenth-century author Ignazio Pozzo as "firm enough not to take a thumb-print" so that the pigment only penetrates slightly into the plaster. By the end of the sixteenth century this had largely displaced the "buon fresco" method, and was used by painters such as Gianbattista Tiepolo or Michelangelo. This technique had, in reduced form, the advantages of "a secco" work.

In Greco-Roman times, mostly encaustic colors applied in a cold state were used.

Tempera painting is one of the oldest known methods in mural painting. In tempera, the pigments are bound in an albuminous medium such as egg yolk or egg white diluted in water.

In 16th-century Europe, oil painting on canvas arose as an easier method for mural painting. The advantage was that the artwork could be completed in the artist's studio and later transported to its destination and there attached to the wall or ceiling. Oil paint may be a less satisfactory medium for murals because of its lack of brilliance in colour. Also, the pigments are yellowed by the binder or are more easily affected by atmospheric conditions. 

Different muralists tend to become experts in their preferred medium and application, whether that be oil paints, emulsion or acrylic paints applied by brush, roller or airbrush/aerosols. Clients will often ask for a particular style and the artist may adjust to the appropriate technique.

A consultation usually leads to detailed design and layout of the proposed mural with a price quote that the client approves before the muralist starts on the work. The area to be painted can be gridded to match the design allowing the image to be scaled accurately step by step. In some cases, the design is projected straight onto the wall and traced with pencil before painting begins. Some muralists will paint directly without any prior sketching, preferring the spontaneous technique.

Once completed the mural can be given coats of varnish or protective acrylic glaze to protect the work from UV rays and surface damage.

In modern, quick form of muralling, young enthusiasts also use POP clay mixed with glue or bond to give desired models on canvas board. The canvas is later set aside to let the clay dry. Once dried, the canvas and the shape can be painted with your choice of colors and later coated with varnish.

As an alternative to a hand-painted or airbrushed mural, digitally printed murals can also be applied to surfaces. Already existing murals can be photographed and then be reproduced in near-to-original quality.

The disadvantages of pre-fabricated murals and decals are that they are often mass-produced and lack the allure and exclusivity of original artwork. They are often not fitted to the individual wall sizes of the client and their personal ideas or wishes cannot be added to the mural as it progresses. The Frescography technique, a digital manufacturing method (CAM) invented by Rainer Maria Latzke addresses some of the personalisation and size restrictions.

Digital techniques are commonly used in advertisements. A "wallscape" is a large advertisement on or attached to the outside wall of a building. Wallscapes can be painted directly on the wall as a mural, or printed on vinyl and securely attached to the wall in the manner of a billboard. Although not strictly classed as murals, large scale printed media are often referred to as such. Advertising murals were traditionally painted onto buildings and shops by sign-writers, later as large scale poster billboards.

Murals are important in that they bring art into the public sphere. Due to the size, cost, and work involved in creating a mural, muralists must often be commissioned by a sponsor. Often it is the local government or a business, but many murals have been paid for with grants of patronage. For artists, their work gets a wide audience who otherwise might not set foot in an art gallery. A city benefits by the beauty of a work of art.

Murals can be a relatively effective tool of social emancipation or achieving a political goal. Murals have sometimes been created against the law, or have been commissioned by local bars and coffee shops. Often, the visual effects are an enticement to attract public attention to social issues.
State-sponsored public art expressions, particularly murals, are often used by totalitarian regimes as a tool of propaganda. However, despite the propagandist character of that works, some of them still have an artistic value.

Murals can have a dramatic impact whether consciously or subconsciously on the attitudes of passers-by, when they are added to areas where people live and work. It can also be argued that the presence of large, public murals can add aesthetic improvement to the daily lives of residents or that of employees at a corporate venue. Large-format hand-painted murals were the norm for advertisements in cities across America, before the introduction of vinyl and digital posters. It was an expensive form of advertising with strict signage laws but gained attention and improved local aesthetics.

Other world-famous murals can be found in Mexico, New York City, Philadelphia, Belfast, Derry, Los Angeles, Nicaragua, Cuba and in India. They have functioned as an important means of communication for members of socially, ethnically and racially divided communities in times of conflict. They also proved to be an effective tool in establishing a dialogue and hence solving the cleavage in the long run.
The Indian state Kerala has exclusive murals. These Kerala mural painting are on walls of Hindu temples. They can be dated from 9th century AD.

The San Bartolo murals of the Maya civilization in Guatemala, are the oldest example of this art in Mesoamerica and are dated at 300 BC.

Many rural towns have begun using murals to create tourist attractions in order to boost economic income. Colquitt, Georgia was chosen to host the 2010 Global Mural Conference. The town had more than twelve murals completed, and hosted the Conference along with Dothan, Alabama, and Blakely, Georgia.

The Mexican mural movement in the 1930s brought new prominence to murals as a social and political tool. Diego Rivera, José Orozco and David Siqueiros were the most famous artists of the movement. Between 1932 and 1940, Rivera also painted murals in San Francisco, Detroit, and New York City. In 1933, he completed a famous series of twenty-seven fresco panels entitled "Detroit Industry" on the walls of an inner court at the Detroit Institute of Arts. During the McCarthyism of the 1950s, a was placed in the courtyard defending the artistic merit of the murals while attacking his politics as "detestable."

In 1948, the Colombian government hosted the IX Pan-American Conference to establish the Marshall plan for the Americas. The director of the OEA and the Colombian government commissioned master Santiago Martinez Delgado, to paint a mural in the Colombian congress building to commemorate the event. Martinez decided to make it about the Cúcuta Congress, and painted Bolívar in front of Santander, making liberals upset; so, due to the murder of Jorge Elieser Gaitan the mobs of el bogotazo tried to burn the capitol, but the Colombian Army stopped them. Years later, in the 1980s, with liberals in charge of the Congress, they passed a resolution to turn the whole chamber in the Elliptic Room 90 degrees to put the main mural on the side and commissioned Alejandro Obregon to paint a non-partisan mural in the surrealist style.

Northern Ireland contains some of the most famous political murals in the world. Almost 2,000 murals have been documented in Northern Ireland since the 1970s. In recent times, many murals are non-sectarian, concerning political and social issues such as racism and environmentalism, and many are completely apolitical, depicting children at play and scenes from everyday life. (See Northern Irish murals.)

A not political, but social related mural covers a wall in an old building, once a prison, at the top of a cliff in Bardiyah, in Libya. It was painted and signed by the artist in April 1942, weeks before his death on the first day of the First Battle of El Alamein. Known as the Bardia Mural, it was created by English artist, private John Frederick Brill.

In 1961 East Germany began to erect a wall between East and West Berlin, which became famous as the Berlin Wall. While on the East Berlin side painting was not allowed, artists painted on the Western side of the Wall from the 80s until the fall of the Wall in 1989.

Many unknown and known artists such as Thierry Noir and Keith Haring painted on the Wall, the "World's longest canvas". The sometimes detailed artwork were often painted over within hours or days. On the Western side the Wall was not protected, so everybody could paint on the Wall. After the fall of the Berlin Wall in 1989, the Eastern side of the Wall became also a popular "canvas" for many mural and graffiti artists.
Orgosolo, in Sardinia, is a most important center of murals politics.

It is also common for mural graffiti to be used as a memoir. In the 2001 book "Somebody Told Me", Rick Bragg writes about a series of communities, mainly located in New York, that have walls dedicated to the people who died. These memorials, both written word and mural style, provide the deceased to be present in the communities in which they lived. Bragg states that the "murals have woven themselves in the fabric of the neighborhoods, and the city." These memorials remind people of the deaths caused by inner city violence.

Many people like to express their individuality by commissioning an artist to paint a mural in their home. This is not an activity exclusively for owners of large houses. A mural artist is only limited by the fee and therefore the time spent on the painting; dictating the level of detail; a simple mural can be added to the smallest of walls.

Private commissions can be for dining rooms, bathrooms, living rooms or, as is often the case- children's bedrooms. A child's room can be transformed into the 'fantasy world' of a forest or racing track, encouraging imaginative play and an awareness of art.

The current trend for feature walls has increased commissions for muralists in the UK. A large hand-painted mural can be designed on a specific theme, incorporate personal images and elements and may be altered during the course of painting it. The personal interaction between client and muralist is often a unique experience for an individual not usually involved in the arts.

In the 1980s, illusionary wall painting experienced a renaissance in private homes. The reason for this revival in interior design could, in some cases be attributed to the reduction in living space for the individual. Faux architectural features, as well as natural scenery and views, can have the effect of 'opening out' the walls. Densely built-up areas of housing may also contribute to people's feelings of being cut off from nature in its free form. A mural commission of this sort may be an attempt by some people to re-establish a balance with nature.

Commissions of murals in schools, hospitals, and retirement homes can achieve a pleasing and welcoming atmosphere in these caring institutions. Murals in other public buildings, such as public houses are also common.

Recently, graffiti and street art have played a key role in contemporary wall painting. Such graffiti/street artists as Keith Haring, Shepard Fairey, Above, Mint&Serf, Futura 2000, Os Gemeos, and Faile among others have successfully transcended their street art aesthetic beyond the walls of urban landscape and onto walls of private and corporate clients. As graffiti/street art became more mainstream in the late 1990s, youth-oriented brands such as Nike and Red Bull, with Wieden Kennedy, have turned to graffiti/street artists to decorate walls of their respective offices. This trend continued through 2000's with graffiti/street art gaining more recognition from art institutions worldwide.

Many homeowners choose to display the traditional art and culture of their society or events from their history in their homes. Ethnic murals have become an important form of interior decoration. Warli painting murals are becoming a preferred mode of wall decor in India. Warli painting is an ancient Indian art form in which the tribal people used to depict different phases of their life on the walls of their mud houses.

Tile murals are murals made out of stone, ceramic, porcelain, glass and or metal tiles that are installed within, or added onto the surface of an existing wall. They are also inlaid into floors. Mural tiles are painted, glazed, sublimation printed (as described below) or more traditionally cut or broken into pieces. Unlike the traditional painted murals described above, tile murals are always made with the use of tiles.

Mosaic murals are made by combining small 1/4" to 2" size pieces of colorful stone, ceramic, or glass tiles which are then laid out to create a picture. Modern day technology has allowed commercial mosaic mural makers to use computer programs to separate photographs into colors that are automatically cut and glued onto sheets of mesh creating precise murals fast and in large quantities.

The azulejo (, ) refers to a typical form of Portuguese or Spanish painted, tin-glazed, ceramic tilework. They have become a typical aspect of Portuguese culture, manifesting without interruption during five centuries, the consecutive trends in art.

Azulejos can be found inside and outside churches, palaces, ordinary houses and even railway stations or subway stations.

They were not only used as an ornamental art form, but also had a specific functional capacity like temperature control in homes. Many "azulejos" chronicle major historical and cultural aspects of Portuguese history.

Custom-printed tile murals can be produced using digital images for kitchen splashbacks, wall displays, and flooring. Digital photos and artwork can be resized and printed to accommodate the desired size for the area to be decorated. Custom tile printing uses a variety of techniques including dye sublimation and ceramic-type laser toners. The latter technique can yield fade-resistant custom tiles which are suitable for long term exterior exposure.





</doc>
<doc id="18819" url="https://en.wikipedia.org/wiki?curid=18819" title="Microeconomics">
Microeconomics

Microeconomics (from Greek prefix "mikro-" meaning "small" + "economics") is a branch of economics that studies the behaviour of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these individuals and firms.

One goal of microeconomics is to analyze the market mechanisms that establish relative prices among goods and services and allocate limited resources among alternative uses. Microeconomics shows conditions under which free markets lead to desirable allocations. It also analyzes market failure, where markets fail to produce efficient results.

While microeconomics focuses on firms and individuals, macroeconomics focuses on the sum total of economic activity, dealing with the issues of growth, inflation, and unemployment and with national policies relating to these issues. Microeconomics also deals with the effects of economic policies (such as changing taxation levels) on microeconomic behavior and thus on the aforementioned aspects of the economy. Particularly in the wake of the Lucas critique, much of modern macroeconomic theories has been built upon microfoundations—i.e. based upon basic assumptions about micro-level behavior.

Microeconomic theory typically begins with the study of a single rational and utility maximizing individual. To economists, rationality means an individual possesses stable preferences that are both complete and transitive.

The technical assumption that preference relations are continuous is needed to ensure the existence of a utility function. Although microeconomic theory can continue without this assumption, it would make comparative statics impossible since there is no guarantee that the resulting utility function would be differentiable.

Microeconomic theory progresses by defining a competitive budget set which is a subset of the consumption set. It is at this point that economists make the technical assumption that preferences are locally non-satiated. Without the assumption of LNS (local non-satiation) there is no 100% guarantee but there would be a rational rise 
in individual utility. With the necessary tools and assumptions in place the utility maximization problem (UMP) is developed.

The utility maximization problem is the heart of consumer theory. The utility maximization problem attempts to explain the action axiom by imposing rationality axioms on consumer preferences and then mathematically modeling and analyzing the consequences. The utility maximization problem serves not only as the mathematical foundation of consumer theory but as a metaphysical explanation of it as well. That is, the utility maximization problem is used by economists to not only explain "what" or "how" individuals make choices but "why" individuals make choices as well.

The utility maximization problem is a constrained optimization problem in which an individual seeks to maximize utility subject to a budget constraint. Economists use the extreme value theorem to guarantee that a solution to the utility maximization problem exists. That is, since the budget constraint is both bounded and closed, a solution to the utility maximization problem exists. Economists call the solution to the utility maximization problem a Walrasian demand function or correspondence.

The utility maximization problem has so far been developed by taking consumer tastes (i.e. consumer utility) as the primitive. However, an alternative way to develop microeconomic theory is by taking consumer choice as the primitive. This model of microeconomic theory is referred to as revealed preference theory.

The theory of supply and demand usually assumes that markets are perfectly competitive. This implies that there are many buyers and sellers in the market and none of them have the capacity to significantly influence prices of goods and services. In many real-life transactions, the assumption fails because some individual buyers or sellers have the ability to influence prices. Quite often, a sophisticated analysis is required to understand the demand-supply equation of a good model. However, the theory works well in situations meeting these assumptions.

Mainstream economics does not assume "a priori" that markets are preferable to other forms of social organization. In fact, much analysis is devoted to cases where market failures lead to resource allocation that is suboptimal and creates deadweight loss. A classic example of suboptimal resource allocation is that of a public good. In such cases, economists may attempt to find policies that avoid waste, either directly by government control, indirectly by regulation that induces market participants to act in a manner consistent with optimal welfare, or by creating "missing markets" to enable efficient trading where none had previously existed.

This is studied in the field of collective action and public choice theory. "Optimal welfare" usually takes on a Paretian norm, which is a mathematical application of the Kaldor–Hicks method. This can diverge from the Utilitarian goal of maximizing utility because it does not consider the distribution of goods between people. Market failure in positive economics (microeconomics) is limited in implications without mixing the belief of the economist and their theory.

The demand for various commodities by individuals is generally thought of as the outcome of a utility-maximizing process, with each individual trying to maximize their own utility under a budget constraint and a given consumption set.

Economists commonly consider themselves microeconomists or macroeconomists. The difference between microeconomics and macroeconomics was introduced in 1933 by the Norwegian economist Ragnar Frisch, the co-recipient of the first Nobel Memorial Prize in Economic Sciences in 1969.

Consumer demand theory relates preferences for the consumption of both goods and services to the consumption expenditures; ultimately, this relationship between preferences and consumption expenditures is used to relate preferences to consumer demand curves. The link between personal preferences, consumption and the demand curve is one of the most closely studied relations in economics. It is a way of analyzing how consumers may achieve equilibrium between preferences and expenditures by maximizing utility subject to consumer budget constraints.

Production theory is the study of production, or the economic process of converting inputs into outputs. Production uses resources to create a good or service that is suitable for use, gift-giving in a gift economy, or exchange in a market economy. This can include manufacturing, storing, shipping, and packaging. Some economists define production broadly as all economic activity other than consumption. They see every commercial activity other than the final purchase as some form of production.

The cost-of-production theory of value states that the price of an object or condition is determined by the sum of the cost of the resources that went into making it. The cost can comprise any of the factors of production (including labor, capital, or land) and taxation. Technology can be viewed either as a form of fixed capital (e.g. an industrial plant) or circulating capital (e.g. intermediate goods).

In the mathematical model for the cost of production, the short-run total cost is equal to fixed cost plus total variable cost. The fixed cost refers to the cost that is incurred regardless of how much the firm produces. The variable cost is a function of the quantity of an object being produced. The cost function can be used to characterize production through the duality theory in economics, developed mainly by Ronald Shephard (1953, 1970) and other scholars (Sickles & Zelenyuk, 2019, ch.2).

Opportunity cost is closely related to the idea of time constraints. One can do only one thing at a time, which means that, inevitably, one is always giving up other things. The opportunity cost of any activity is the value of the next-best alternative thing one may have done instead. Opportunity cost depends only on the value of the next-best alternative. It doesn't matter whether one has five alternatives or 5,000.

Opportunity costs can tell when "not" to do something as well as when to do something. For example, one may like waffles, but like chocolate even more. If someone offers only waffles, one would take it. But if offered waffles or chocolate, one would take the chocolate. The opportunity cost of eating waffles is sacrificing the chance to eat chocolate. Because the cost of not eating the chocolate is higher than the benefits of eating the waffles, it makes no sense to choose waffles. Of course, if one chooses chocolate, they are still faced with the opportunity cost of giving up having waffles. But one is willing to do that because the waffle's opportunity cost is lower than the benefits of the chocolate. Opportunity costs are unavoidable constraints on behaviour because one has to decide what's best and give up the next-best alternative.

Supply and demand is an economic model of price determination in a perfectly competitive market. It concludes that in a perfectly competitive market with no externalities, per unit taxes, or price controls, the unit price for a particular good is the price at which the quantity demanded by consumers equals the quantity supplied by producers. This price results in a stable economic equilibrium.

Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.

For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is "constrained utility maximization" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.

The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.

"Supply" is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be "profit maximizers", meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.

That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The "Law of Supply" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.

Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.

For a given quantity of a consumer good, the point on the demand curve indicates the value, or marginal utility, to consumers for that unit. It measures what the consumer would be prepared to pay for that unit. The corresponding point on the supply curve measures marginal cost, the increase in total cost to the supplier for the corresponding unit of the good. The price in equilibrium is determined by supply and demand. In a perfectly competitive market, supply and demand equate marginal cost and marginal utility at equilibrium.

On the supply side of the market, some factors of production are described as (relatively) "variable" in the short run, which affects the cost of changing output levels. Their usage rates can be changed easily, such as electrical power, raw-material inputs, and over-time and temp work. Other inputs are relatively "fixed", such as plant and equipment and key personnel. In the long run, all inputs may be adjusted by management. These distinctions translate to differences in the elasticity (responsiveness) of the supply curve in the short and long runs and corresponding differences in the price-quantity change from a shift on the supply or demand side of the market.

Marginalist theory, such as above, describes the consumers as attempting to reach most-preferred positions, subject to income and wealth constraints while producers attempt to maximize profits subject to their own constraints, including demand for goods produced, technology, and the price of inputs. For the consumer, that point comes where marginal utility of a good, net of price, reaches zero, leaving no net gain from further consumption increases. Analogously, the producer compares marginal revenue (identical to price for the perfect competitor) against the marginal cost of a good, with "marginal profit" the difference. At the point where marginal profit reaches zero, further increases in production of the good stop. For movement to market equilibrium and for changes in equilibrium, price and quantity also change "at the margin": more-or-less of something, rather than necessarily all-or-nothing.

Other applications of demand and supply include the distribution of income among the factors of production, including labour and capital, through factor markets. In a competitive labour market for example the quantity of labour employed and the price of labour (the wage rate) depends on the demand for labour (from employers for production) and supply of labour (from potential workers). Labour economics examines the interaction of workers and employers through such markets to explain patterns and changes of wages and other labour income, labour mobility, and (un)employment, productivity through human capital, and related public-policy issues.

Demand-and-supply analysis is used to explain the behaviour of perfectly competitive markets, but as a standard of comparison it can be extended to any type of market. It can also be generalized to explain variables across the economy, for example, total output (estimated as real GDP) and the general price level, as studied in macroeconomics. Tracing the qualitative and quantitative effects of variables that change supply and demand, whether in the short or long run, is a standard exercise in applied economics. Economic theory may also specify conditions such that supply and demand through the market is an efficient mechanism for allocating resources.

Market structure refers to features of a market, including the number of firms in the market, the distribution of market shares between them, product uniformity across firms, how easy it is for firms to enter and exit the market, and forms of competition in the market. A market structure can have several types of interacting market systems. 
Different forms of markets are a feature of capitalism and market socialism, with advocates of state socialism often criticizing markets and aiming to substitute or replace markets with varying degrees of government-directed economic planning.

Competition acts as a regulatory mechanism for market systems, with government providing regulations where the market cannot be expected to regulate itself. One example of this is with regards to building codes, which if absent in a purely competition regulated market system, might result in several horrific injuries or deaths to be required before companies would begin improving structural safety, as consumers may at first not be as concerned or aware of safety issues to begin putting pressure on companies to provide them, and companies would be motivated not to provide proper safety features due to how it would cut into their profits.

The concept of "market type" is different from the concept of "market structure." Nevertheless, it is worth noting here that there are a variety of types of markets.

Perfect competition is a situation in which numerous small firms producing identical products compete against each other in a given industry. Perfect competition leads to firms producing the socially optimal output level at the minimum possible cost per unit. Firms in perfect competition are "price takers" (they do not have enough market power to profitably increase the price of their goods or services). A good example would be that of digital marketplaces, such as eBay, on which many different sellers sell similar products to many different buyers. Consumers in a perfect competitive market have perfect knowledge about the products that are being sold in this market.

Imperfect competition is a type of market structure showing some but not all features of competitive markets.

Monopolistic competition is a situation in which many firms with slightly different products compete. Production costs are above what may be achieved by perfectly competitive firms, but society benefits from the product differentiation. Examples of industries with market structures similar to monopolistic competition include restaurants, cereal, clothing, shoes, and service industries in large cities.

A monopoly is a market structure in which a market or industry is dominated by a single supplier of a particular good or service. Because monopolies have no competition they tend to sell goods and services at a higher price and produce below the socially optimal output level. However, not all monopolies are a bad thing, especially in industries where multiple firms would result in more costs than benefits (i.e. natural monopolies).


An oligopoly is a market structure in which a market or industry is dominated by a small number of firms (oligopolists). Oligopolies can create the incentive for firms to engage in collusion and form cartels that reduce competition leading to higher prices for consumers and less overall market output. Alternatively, oligopolies can be fiercely competitive and engage in flamboyant advertising campaigns.


A monopsony is a market where there is only one buyer and many sellers.

A bilateral monopoly is a market consisting of both a monopoly (a single seller) and a monopsony (a single buyer).

An oligopsony is a market where there are a few buyers and many sellers.

Game theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. The term "game" here implies the study of any strategic interaction between people. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems, and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.

Information economics is a branch of microeconomic theory that studies how information and information systems affect an economy and economic decisions. Information has special characteristics. It is easy to create but hard to trust. It is easy to spread but hard to control. It influences many decisions. These special characteristics (as compared with other types of goods) complicate many standard economic theories. The economics of information has recently become of great interest to many - possibly due to the rise of information based companies inside the technology industry. From a game theory approach, we can loosen the usual constraints that agents have complete information to further examine the consequences of having incomplete information. This gives rise to many results which are applicable to real life situations. For example, if one does loosen this assumption, then it is possible to scrutinize the actions of agents in situations of uncertainty. It is also possible to more fully understand the impacts – both positive and negative – of agents seeking out or acquiring information.

Applied microeconomics includes a range of specialized areas of study, many of which draw on methods from other fields.






</doc>
<doc id="18820" url="https://en.wikipedia.org/wiki?curid=18820" title="Macroeconomics">
Macroeconomics

Macroeconomics (from the Greek prefix "makro-" meaning "large" + "economics") is a branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole. This includes regional, national, and global economies. 

While macroeconomics is a broad field of study, there are two areas of research that are emblematic of the discipline: the attempt to understand the causes and consequences of short-run fluctuations in national income (the business cycle), and the attempt to understand the determinants of long-run economic growth (increases in national income).

Macroeconomic models and their forecasts are used by governments to assist in the development and evaluation of economic policy.

Macroeconomists study aggregated indicators such as GDP, unemployment rates, national income, price indices, and the interrelations among the different sectors of the economy to better understand how the whole economy functions. They also develop models that explain the relationship between such factors as national income, output, consumption, unemployment, inflation, saving, investment, energy, international trade, and international finance.

Macroeconomics and microeconomics, a pair of terms coined by Ragnar Frisch, are the two most general fields in economics. In contrast to macroeconomics, microeconomics is the branch of economics that studies the behavior of individuals and firms in making decisions and the interactions among these individuals and firms in narrowly-defined markets. The central problems of an economy are 1. What to produce ? 2. How to produce ? 3. For whom to produce ?

Macroeconomics descended from the once divided fields of business cycle theory and monetary theory. The quantity theory of money was particularly influential prior to World War II. It took many forms, including the version based on the work of Irving Fisher:

In the typical view of the quantity theory, money velocity (V) and the quantity of goods produced (Q) would be constant, so any increase in money supply (M) would lead to a direct increase in price level (P). The quantity theory of money was a central part of the classical theory of the economy that prevailed in the early twentieth century.

Ludwig Von Mises's work "Theory of Money and Credit", published in 1912, was one of the first books from the Austrian School to deal with macroeconomic topics.

Macroeconomics, at least in its modern form, began with the publication of John Maynard Keynes's "General Theory of Employment, Interest and Money". When the Great Depression struck, classical economists had difficulty explaining how goods could go unsold and workers could be left unemployed. In classical theory, prices and wages would drop until the market cleared, and all goods and labor were sold. Keynes offered a new theory of economics that explained why markets might not clear, which would evolve (later in the 20th century) into a group of macroeconomic schools of thought known as Keynesian economics – also called Keynesianism or Keynesian theory.

In Keynes's theory, the quantity theory broke down because people and businesses tend to hold on to their cash in tough economic times – a phenomenon he described in terms of liquidity preferences. Keynes also explained how the multiplier effect would magnify a small decrease in consumption or investment and cause declines throughout the economy. Keynes also noted the role uncertainty and animal spirits can play in the economy.

The generation following Keynes combined the macroeconomics of the "General Theory" with neoclassical microeconomics to create the neoclassical synthesis. By the 1950s, most economists had accepted the synthesis view of the macroeconomy. Economists like Paul Samuelson, Franco Modigliani, James Tobin, and Robert Solow developed formal Keynesian models and contributed formal theories of consumption, investment, and money demand that fleshed out the Keynesian framework.

Milton Friedman updated the quantity theory of money to include a role for money demand. He argued that the role of money in the economy was sufficient to explain the Great Depression, and that aggregate demand oriented explanations were not necessary. Friedman also argued that monetary policy was more effective than fiscal policy; however, Friedman doubted the government's ability to "fine-tune" the economy with monetary policy. He generally favored a policy of steady growth in money supply instead of frequent intervention.

Friedman also challenged the Phillips curve relationship between inflation and unemployment. Friedman and Edmund Phelps (who was not a monetarist) proposed an "augmented" version of the Phillips curve that excluded the possibility of a stable, long-run tradeoff between inflation and unemployment. When the oil shocks of the 1970s created a high unemployment and high inflation, Friedman and Phelps were vindicated. Monetarism was particularly influential in the early 1980s. Monetarism fell out of favor when central banks found it difficult to target money supply instead of interest rates as monetarists recommended. Monetarism also became politically unpopular when the central banks created recessions in order to slow inflation.

New classical macroeconomics further challenged the Keynesian school. A central development in new classical thought came when Robert Lucas introduced rational expectations to macroeconomics. Prior to Lucas, economists had generally used adaptive expectations where agents were assumed to look at the recent past to make expectations about the future. Under rational expectations, agents are assumed to be more sophisticated. A consumer will not simply assume a 2% inflation rate just because that has been the average the past few years; she will look at current monetary policy and economic conditions to make an informed forecast. When new classical economists introduced rational expectations into their models, they showed that monetary policy could only have a limited impact.

Lucas also made an influential critique of Keynesian empirical models. He argued that forecasting models based on empirical relationships would keep producing the same predictions even as the underlying model generating the data changed. He advocated models based on fundamental economic theory that would, in principle, be structurally accurate as economies changed. Following Lucas's critique, new classical economists, led by Edward C. Prescott and Finn E. Kydland, created real business cycle (RB C) models of the macro economy.

RB C models were created by combining fundamental equations from neo-classical microeconomics. In order to generate macroeconomic fluctuations, RB C models explained recessions and unemployment with changes in technology instead of changes in the markets for goods or money. Critics of RB C models argue that money clearly plays an important role in the economy, and the idea that technological regress can explain recent recessions is implausible. However, technological shocks are only the more prominent of a myriad of possible shocks to the system that can be modeled. Despite questions about the theory behind RB C models, they have clearly been influential in economic methodology.

New Keynesian economists responded to the new classical school by adopting rational expectations and focusing on developing micro-founded models that are immune to the Lucas critique. Stanley Fischer and John B. Taylor produced early work in this area by showing that monetary policy could be effective even in models with rational expectations when contracts locked in wages for workers. Other new Keynesian economists, including Olivier Blanchard, Julio Rotemberg, Greg Mankiw, David Romer, and Michael Woodford, expanded on this work and demonstrated other cases where inflexible prices and wages led to monetary and fiscal policy having real effects.

Like classical models, new classical models had assumed that prices would be able to adjust perfectly and monetary policy would only lead to price changes. New Keynesian models investigated sources of sticky prices and wages due to imperfect competition, which would not adjust, allowing monetary policy to impact quantities instead of prices.

By the late 1990s economists had reached a rough consensus. The nominal rigidity of new Keynesian theory was combined with rational expectations and the RBC methodology to produce dynamic stochastic general equilibrium (DSGE) models. The fusion of elements from different schools of thought has been dubbed the new neoclassical synthesis. These models are now used by many central banks and are a core part of contemporary macroeconomics.

New Keynesian economics, which developed partly in response to new classical economics, strives to provide microeconomic foundations to Keynesian economics by showing how imperfect markets can justify demand management.

The AD-AS model has become the standard textbook model for explaining the macroeconomy. This model shows the price level and level of real output given the equilibrium in aggregate demand and aggregate supply. The aggregate demand curve's downward slope means that more output is demanded at lower price levels. The downward slope is the result of three effects: the Pigou or real balance effect, which states that as real prices fall, real wealth increases, resulting in higher consumer demand of goods; the Keynes or interest rate effect, which states that as prices fall, the demand for money decreases, causing interest rates to decline and borrowing for investment and consumption to increase; and the net export effect, which states that as prices rise, domestic goods become comparatively more expensive to foreign consumers, leading to a decline in exports.

In the conventional Keynesian use of the AS-AD model, the aggregate supply curve is horizontal at low levels of output and becomes inelastic near the point of potential output, which corresponds with full employment. Since the economy cannot produce beyond the potential output, any AD expansion will lead to higher price levels instead of higher output.

The AD–AS diagram can model a variety of macroeconomic phenomena, including inflation. Changes in the non-price level factors or determinants cause changes in aggregate demand and shifts of the entire aggregate demand (AD) curve. When demand for goods exceeds supply there is an inflationary gap where demand-pull inflation occurs and the AD curve shifts upward to a higher price level. When the economy faces higher costs, cost-push inflation occurs and the AS curve shifts upward to higher price levels. The AS–AD diagram is also widely used as a pedagogical tool to model the effects of various macroeconomic policies.

The IS–LM model gives the underpinnings of aggregate demand (itself discussed above). It answers the question “At any given price level, what is the quantity of goods demanded?” This model shows represents what combination of interest rates and output will ensure equilibrium in both the goods and money markets. The goods market is modeled as giving equality between investment and public and private saving (IS), and the money market is modeled as giving equilibrium between the money supply and liquidity preference. 

The IS curve consists of the points (combinations of income and interest rate) where investment, given the interest rate, is equal to public and private saving, given output The IS curve is downward sloping because output and the interest rate have an inverse relationship in the goods market: as output increases, more income is saved, which means interest rates must be lower to spur enough investment to match saving. 

The LM curve is upward sloping because the interest rate and output have a positive relationship in the money market: as income (identically equal to output) increases, the demand for money increases, resulting in a rise in the interest rate in order to just offset the insipient rise in money demand.

The IS-LM model is often used to demonstrate the effects of monetary and fiscal policy. Textbooks frequently use the IS-LM model, but it does not feature the complexities of most modern macroeconomic models. Nevertheless, these models still feature similar relationships to those in IS-LM.

The neoclassical growth model of Robert Solow has become a common textbook model for explaining economic growth in the long-run. The model begins with a production function where national output is the product of two inputs: capital and labor. The Solow model assumes that labor and capital are used at constant rates without the fluctuations in unemployment and capital utilization commonly seen in business cycles.

An increase in output, or economic growth, can only occur because of an increase in the capital stock, a larger population, or technological advancements that lead to higher productivity (total factor productivity). An increase in the savings rate leads to a temporary increase as the economy creates more capital, which adds to output. However, eventually the depreciation rate will limit the expansion of capital: savings will be used up replacing depreciated capital, and no savings will remain to pay for an additional expansion in capital. Solow's model suggests that economic growth in terms of output per capita depends solely on technological advances that enhance productivity.

In the 1980s and 1990s endogenous growth theory arose to challenge neoclassical growth theory. This group of models explains economic growth through other factors, such as increasing returns to scale for capital and learning-by-doing, that are endogenously determined instead of the exogenous technological improvement used to explain growth in Solow's model.

Macroeconomics encompasses a variety of concepts and variables, but there are three central topics for macroeconomic research. Macroeconomic theories usually relate the phenomena of output, unemployment, and inflation. Outside of macroeconomic theory, these topics are also important to all economic agents including workers, consumers, and producers.

National output is the total amount of everything a country produces in a given period of time. Everything that is produced and sold generates an equal amount of income. The total output of the economy is measured GDP per person. The output and income are usually considered equivalent and the two terms are often used interchangeably, output changes into income. Output can be measured or it can be viewed from the production side and measured as the total value of final goods and services or the sum of all value added in the economy.

Macroeconomic output is usually measured by gross domestic product (GDP) or one of the other national accounts. Economists interested in long-run increases in output study economic growth. Advances in technology, accumulation of machinery and other capital, and better education and human capital are all factors that lead to increase economic output over time. However, output does not always increase consistently over time. Business cycles can cause short-term drops in output called recessions. Economists look for macroeconomic policies that prevent economies from slipping into recessions and that lead to faster long-term growth.

The amount of unemployment in an economy is measured by the unemployment rate, i.e. the percentage of workers without jobs in the labor force. The unemployment rate in the labor force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded.

Unemployment can be generally broken down into several types that are related to different causes.

A general price increase across the entire economy is called inflation. When prices decrease, there is deflation. Economists measure these changes in prices with price indexes. Inflation can occur when an economy becomes overheated and grows too quickly. Similarly, a declining economy can lead to deflation.

Central bankers, who manage a country's money supply, try to avoid changes in price level by using monetary policy. Raising interest rates or reducing the supply of money in an economy will reduce inflation. Inflation can lead to increased uncertainty and other negative consequences. Deflation can lower economic output. Central bankers try to stabilize prices to protect economies from the negative consequences of price changes.

Changes in price level may be the result of several factors. The quantity theory of money holds that changes in price level are directly related to changes in the money supply. Most economists believe that this relationship explains long-run changes in the price level. Short-run fluctuations may also be related to monetary factors, but changes in aggregate demand and aggregate supply can also influence price level. For example, a decrease in demand due to a recession can lead to lower price levels and deflation. A negative supply shock, such as an oil crisis, lowers aggregate supply and can cause inflation.

Macroeconomic policy is usually implemented through two sets of tools: fiscal and monetary policy. Both forms of policy are used to stabilize the economy, which can mean boosting the economy to the level of GDP consistent with full employment. Macroeconomic policy focuses on limiting the effects of the business cycle to achieve the economic goals of price stability, full employment, and growth.

Central banks implement monetary policy by controlling the money supply through several mechanisms. Typically, central banks take action by issuing money to buy bonds (or other assets), which boosts the supply of money and lowers interest rates, or, in the case of contractionary monetary policy, banks sell bonds and take money out of circulation. Usually policy is not implemented by directly targeting the supply of money.

Central banks continuously shift the money supply to maintain a targeted fixed interest rate. Some of them allow the interest rate to fluctuate and focus on targeting inflation rates instead. Central banks generally try to achieve high output without letting loose monetary policy that create large amounts of inflation.

Conventional monetary policy can be ineffective in situations such as a liquidity trap. When interest rates and inflation are near zero, the central bank cannot loosen monetary policy through conventional means.

Central banks can use unconventional monetary policy such as quantitative easing to help increase output. Instead of buying government bonds, central banks can implement quantitative easing by buying not only government bonds, but also other assets such as corporate bonds, stocks, and other securities. This allows lower interest rates for a broader class of assets beyond government bonds. In another example of unconventional monetary policy, the United States Federal Reserve recently made an attempt at such a policy with Operation Twist. Unable to lower current interest rates, the Federal Reserve lowered long-term interest rates by buying long-term bonds and selling short-term bonds to create a flat yield curve.

Fiscal policy is the use of government's revenue and expenditure as instruments to influence the economy. Examples of such tools are expenditure, taxes, debt.

For example, if the economy is producing less than potential output, government spending can be used to employ idle resources and boost output. Government spending does not have to make up for the entire output gap. There is a multiplier effect that boosts the impact of government spending. For instance, when the government pays for a bridge, the project not only adds the value of the bridge to output, but also allows the bridge workers to increase their consumption and investment, which helps to close the output gap.

The effects of fiscal policy can be limited by crowding out. When the government takes on spending projects, it limits the amount of resources available for the private sector to use. Crowding out occurs when government spending simply replaces private sector output instead of adding additional output to the economy. Crowding out also occurs when government spending raises interest rates, which limits investment. Defenders of fiscal stimulus argue that crowding out is not a concern when the economy is depressed, plenty of resources are left idle, and interest rates are low.

Fiscal policy can be implemented through automatic stabilizers. Automatic stabilizers do not suffer from the policy lags of discretionary fiscal policy. Automatic stabilizers use conventional fiscal mechanisms but take effect as soon as the economy takes a downturn: spending on unemployment benefits automatically increases when unemployment rises and, in a progressive income tax system, the effective tax rate automatically falls when incomes decline.

Economists usually favor monetary over fiscal policy because it has two major advantages. First, monetary policy is generally implemented by independent central banks instead of the political institutions that control fiscal policy. Independent central banks are less likely to make decisions based on political motives. Second, monetary policy suffers shorter inside lags and outside lags than fiscal policy. Central banks can quickly make and implement decisions while discretionary fiscal policy may take time to pass and even longer to carry out.




</doc>
<doc id="18823" url="https://en.wikipedia.org/wiki?curid=18823" title="Mary Pickford">
Mary Pickford

Gladys Louise Smith (April 8, 1892 – May 29, 1979), known professionally as Mary Pickford, was a Canadian-born American film actress and producer. With a career spanning 50 years, she was a co-founder of both the Pickford–Fairbanks Studio (along with Douglas Fairbanks) and, later, the United Artists film studio (with Fairbanks, Charlie Chaplin and D. W. Griffith), and one of the original 36 founders of the Academy of Motion Picture Arts and Sciences who present the yearly "Oscar" award ceremony.

Pickford was known in her prime as "America's Sweetheart" and the "girl with the curls". She was one of the Canadian pioneers in early Hollywood and a significant figure in the development of film acting. Pickford was one of the earliest stars to be billed under her own name, and was one of the most popular actresses of the 1910s and 1920s, earning the nickname "Queen of the Movies". She is credited as having defined the archetype in cinema.

She was awarded the second ever Academy Award for Best Actress for her first sound-film role in "Coquette" (1929) and also received an honorary Academy Award in 1976. In consideration of her contributions to American cinema, the American Film Institute ranked Pickford as 24th in its 1999 list of greatest female stars of classic Hollywood Cinema.

Mary Pickford was born Gladys Louise Smith in 1892 (although she later claimed 1893 or 1894 as her year of birth) at 211 University Avenue, Toronto, Ontario. Her father, John Charles Smith, was the son of English Methodist immigrants, and worked a variety of odd jobs. Her mother, Charlotte Hennessey, was of Irish Catholic descent and worked for a time as a seamstress. She had two younger siblings, Charlotte, called "Lottie" (born 1893), and John Charles, called "Jack" (born 1896), who also became actors. To please her husband's relatives, Pickford's mother baptized her children as Methodists, the religion of their father. John Charles Smith was an alcoholic; he abandoned the family and died on February 11, 1898, from a fatal blood clot caused by a workplace accident when he was a purser with Niagara Steamship.

When Gladys was age four, her household was under infectious quarantine, a public health measure. Their devoutly Catholic maternal grandmother (Catherine Faeley Hennessey) asked a visiting Roman Catholic priest to baptize the children. Pickford was at this time baptized as Gladys Marie Smith.

After being widowed in 1899, Charlotte Smith began taking in boarders, one of whom was a Mr. Murphy, the theatrical stage manager for Cummings Stock Company, who soon suggested that Gladys, then age seven, and Lotti, then age six, be given two small theatrical roles – Gladys portrayed a girl and a boy, while Lottie was cast in a silent part in the company's production of "The Silver King" at Toronto's Princess Theatre (destroyed by fire in 1915, rebuilt, demolished in 1931), while their mother played the organ. Pickford subsequently acted in many melodramas with Toronto's Valentine Stock Company, finally playing the major child role in its version of "The Silver King". She capped her short career in Toronto with the starring role of Little Eva in the Valentine production of "Uncle Tom's Cabin", adapted from the 1852 novel.

By the early 1900s, theatre had become a family enterprise. Gladys, her mother and two younger siblings toured the United States by rail, performing in third-rate companies and plays. After six impoverished years, Pickford allowed one more summer to land a leading role on Broadway, planning to quit acting if she failed. In 1906 Gladys, Lottie and Jack Smith supported singer Chauncey Olcott on Broadway in "Edmund Burke". Gladys finally landed a supporting role in a 1907 Broadway play, "The Warrens of Virginia". The play was written by William C. deMille, whose brother, Cecil, appeared in the cast. David Belasco, the producer of the play, insisted that Gladys Smith assume the stage name Mary Pickford. After completing the Broadway run and touring the play, however, Pickford was again out of work.

On April 19, 1909, the Biograph Company director D. W. Griffith screen-tested her at the company's New York studio for a role in the nickelodeon film "Pippa Passes". The role went to someone else but Griffith was immediately taken with Pickford. She quickly grasped that movie acting was simpler than the stylized stage acting of the day. Most Biograph actors earned $5 a day but, after Pickford's single day in the studio, Griffith agreed to pay her $10 a day against a guarantee of $40 a week.

Pickford, like all actors at Biograph, played both bit parts and leading roles, including mothers, ingenues, charwomen, spitfires, slaves, Native Americans, spurned women, and a prostitute. As Pickford said of her success at Biograph:I played scrubwomen and secretaries and women of all nationalities ... I decided that if I could get into as many pictures as possible, I'd become known, and there would be a demand for my work. She appeared in 51 films in 1909 – almost one a week. While at Biograph, she suggested to Florence La Badie to "try pictures", invited her to the studio and later introduced her to D. W. Griffith, who launched La Badie's career.

In January 1910, Pickford traveled with a Biograph crew to Los Angeles. Many other film companies wintered on the West Coast, escaping the weak light and short days that hampered winter shooting in the East. Pickford added to her 1909 Biographs ("Sweet and Twenty", "They Would Elope," and "To Save Her Soul", to name a few) with films made in California.

Actors were not listed in the credits in Griffith's company. Audiences noticed and identified Pickford within weeks of her first film appearance. Exhibitors, in turn, capitalized on her popularity by advertising on sandwich boards that a film featuring "The Girl with the Golden Curls", "Blondilocks", or "The Biograph Girl" was inside.

Pickford left Biograph in December 1910. The following year, she starred in films at Carl Laemmle's Independent Moving Pictures Company (IMP). IMP was absorbed into Universal Pictures in 1912, along with Majestic. Unhappy with their creative standards, Pickford returned to work with Griffith in 1912. Some of her best performances were in his films, such as "Friends", "The Mender of Nets", "Just Like a Woman", and "The Female of the Species". That year, Pickford also introduced Dorothy and Lillian Gish– whom she had befriended as new neighbors from Ohio –to Griffith, and each became major silent film stars, in comedy and tragedy, respectively. Pickford made her last Biograph picture, "The New York Hat", in late 1912.

She returned to Broadway in the David Belasco production of "A Good Little Devil" (1912). This was a major turning point in her career. Pickford, who had always hoped to conquer the Broadway stage, discovered how deeply she missed film acting. In 1913, she decided to work exclusively in film. The previous year, Adolph Zukor had formed Famous Players in Famous Plays. It was later known as Famous Players-Lasky and then Paramount Pictures, one of the first American feature film companies.
Pickford left the stage to join Zukor's roster of stars. Zukor believed film's potential lay in recording theatrical players in replicas of their most famous stage roles and productions. Zukor first filmed Pickford in a silent version of "A Good Little Devil". The film, produced in 1913, showed the play's Broadway actors reciting every line of dialogue, resulting in a stiff film that Pickford later called "one of the worst [features] I ever made ... it was deadly". Zukor agreed; he held the film back from distribution for a year.

Pickford's work in material written for the camera by that time had attracted a strong following. Comedy-dramas, such as "In the Bishop's Carriage" (1913), "Caprice" (1913), and especially "Hearts Adrift" (1914), made her irresistible to moviegoers. "Hearts Adrift" was so popular that Pickford asked for the first of her many publicized pay raises based on the profits and reviews. The film marked the first time Pickford's name was featured above the title on movie marquees. "Tess of the Storm Country" was released five weeks later. Biographer Kevin Brownlow observed that the film "sent her career into orbit and made her the most popular actress in America, if not the world".

Her appeal was summed up two years later by the February 1916 issue of "Photoplay" as "luminous tenderness in a steel band of gutter ferocity". Only Charlie Chaplin, who slightly surpassed Pickford's popularity in 1916, had a similarly spellbinding pull with critics and the audience. Each enjoyed a level of fame far exceeding that of other actors. Throughout the 1910s and 1920s, Pickford was believed to be the most famous woman in the world, or, as a silent-film journalist described her, "the best known woman who has ever lived, the woman who was known to more people and loved by more people than any other woman that has been in all history".

 Pickford starred in 52 features throughout her career. On June 24, 1916, Pickford signed a new contract with Zukor that granted her full authority over production of the films in which she starred, and a record-breaking salary of $10,000 a week. In addition, Pickford's compensation was half of a film's profits, with a guarantee of $1,040,000 (US$ in 2020).

Occasionally, she played a child, in films such as "The Poor Little Rich Girl" (1917), "Rebecca of Sunnybrook Farm" (1917), "Daddy-Long-Legs" (1919) and "Pollyanna" (1920). Pickford's fans were devoted to these "little girl" roles, but they were not typical of her career. Due to her lack of a normal childhood, she enjoyed making these pictures. Given how small she was at under five feet, and her naturalistic acting abilities, she was very successful in these roles. Douglas Fairbanks Jr., when he first met her in person as a boy, assumed she was a new playmate for him, and asked her to come and play trains with him, which she obligingly did.

In August 1918, Pickford's contract expired and, when refusing Zukor's terms for a renewal, she was offered $250,000 to leave the motion picture business. She declined, and went to First National Pictures, which agreed to her terms. In 1919, Pickford, along with D.W. Griffith, Charlie Chaplin, and Douglas Fairbanks, formed the independent film production company United Artists. Through United Artists, Pickford continued to produce and perform in her own movies; she could also distribute them as she chose. In 1920, Pickford's film "Pollyanna" grossed around $1,100,000. The following year, Pickford's film "Little Lord Fauntleroy" was also a success, and in 1923, "Rosita" grossed over $1,000,000 as well. During this period, she also made "Little Annie Rooney" (1925), another film in which Pickford played a child, "Sparrows" (1926), which blended the Dickensian with newly minted German expressionist style, and "My Best Girl" (1927), a romantic comedy featuring her future husband Buddy Rogers.

The arrival of sound was her undoing. Pickford underestimated the value of adding sound to movies, claiming that "adding sound to movies would be like putting lipstick on the Venus de Milo".

She played a reckless socialite in "Coquette" (1929), a role for which her famous ringlets were cut into a 1920s' bob. Pickford had already cut her hair in the wake of her mother's death in 1928. Fans were shocked at the transformation. Pickford's hair had become a symbol of female virtue, and when she cut it, the act made front-page news in "The New York Times" and other papers. "Coquette" was a success and won her an Academy Award for Best Actress, although this was highly controversial. The public failed to respond to her in the more sophisticated roles. Like most movie stars of the silent era, Pickford found her career fading as talkies became more popular among audiences.

Her next film, "The Taming of The Shrew", made with husband Douglas Fairbanks, was not well received at the box office. Established Hollywood actors were panicked by the impending arrival of the talkies. On March 29, 1928, "The Dodge Brothers Hour" was broadcast from Pickford's bungalow, featuring Fairbanks, Chaplin, Norma Talmadge, Gloria Swanson, John Barrymore, D.W. Griffith, and Dolores del Rio, among others. They spoke on the radio show to prove that they could meet the challenge of talking movies.

A transition in the roles Pickford selected came when she was in her late 30s, no longer able to play the children, teenage spitfires, and feisty young women so adored by her fans, and was not suited for the glamorous and vampish heroines of early sound. In 1933, she underwent a Technicolor screen test for an animated/live action film version of "Alice in Wonderland", but Walt Disney discarded the project when Paramount released its own version of the book. Only one Technicolor still of her screen test still exists. She retired from acting in 1933; her last acting film was released in 1934. She continued to produce for others, however, including "Sleep, My Love" (1948; with Claudette Colbert) and "Love Happy" (1949), with the Marx Brothers).

Pickford used her stature in the movie industry to promote a variety of causes. Although her image depicted fragility and innocence, Pickford proved to be a worthy businesswoman who took control of her career in a cutthroat industry.

During World War I, she promoted the sale of Liberty Bonds, making an intensive series of fund-raising speeches that kicked off in Washington, D.C., where she sold bonds alongside Charlie Chaplin, Douglas Fairbanks, Theda Bara, and Marie Dressler. Five days later she spoke on Wall Street to an estimated 50,000 people. Though Canadian-born, she was a powerful symbol of Americana, kissing the American flag for cameras and auctioning one of her world-famous curls for $15,000. In a single speech in Chicago she sold an estimated five million dollars' worth of bonds. She was christened the U.S. Navy's official "Little Sister"; the Army named two cannons after her and made her an honorary colonel.

At the end of World War I, Pickford conceived of the Motion Picture Relief Fund, an organization to help financially needy actors. Leftover funds from her work selling Liberty Bonds were put toward its creation, and in 1921, the Motion Picture Relief Fund (MPRF) was officially incorporated, with Joseph Schenck voted its first president and Pickford its vice president. In 1932, Pickford spearheaded the "Payroll Pledge Program", a payroll-deduction plan for studio workers who gave one half of one percent of their earnings to the MPRF. As a result, in 1940, the Fund was able to purchase land and build the Motion Picture Country House and Hospital, in Woodland Hills, California.

An astute businesswoman, Pickford became her own producer within three years of her start in features. According to her Foundation, "she oversaw every aspect of the making of her films, from hiring talent and crew to overseeing the script, the shooting, the editing, to the final release and promotion of each project". She demanded (and received) these powers in 1916, when she was under contract to Zukor's Famous Players in Famous Plays (later Paramount). Zukor acquiesced to her refusal to participate in block-booking, the widespread practice of forcing an exhibitor to show a bad film of the studio's choosing to also be able to show a Pickford film. In 1916, Pickford's films were distributed, singly, through a special distribution unit called Artcraft. The Mary Pickford Corporation was briefly Pickford's motion-picture production company.
In 1919, she increased her power by co-founding United Artists (UA) with Charlie Chaplin, D. W. Griffith, and her soon-to-be husband, Douglas Fairbanks. Before UA's creation, Hollywood studios were vertically integrated, not only producing films but forming chains of theaters. Distributors (also part of the studios) arranged for company productions to be shown in the company's movie venues. Filmmakers relied on the studios for bookings; in return they put up with what many considered creative interference.

United Artists broke from this tradition. It was solely a distribution company, offering independent film producers access to its own screens as well as the rental of temporarily unbooked cinemas owned by other companies. Pickford and Fairbanks produced and shot their films after 1920 at the jointly owned Pickford-Fairbanks studio on Santa Monica Boulevard. The producers who signed with UA were true independents, producing, creating and controlling their work to an unprecedented degree. As a co-founder, as well as the producer and star of her own films, Pickford became the most powerful woman who has ever worked in Hollywood. By 1930, Pickford's acting career had largely faded. After retiring three years later, however, she continued to produce films for United Artists. She and Chaplin remained partners in the company for decades. Chaplin left the company in 1955, and Pickford followed suit in 1956, selling her remaining shares for three million dollars.

Pickford was married three times. She married Owen Moore, an Irish-born silent film actor, on January 7, 1911. It is rumored she became pregnant by Moore in the early 1910s and had a miscarriage or an abortion. Some accounts suggest this resulted in her later inability to have children. The couple's marriage was strained by Moore's alcoholism, insecurity about living in the shadow of Pickford's fame, and bouts of domestic violence. The couple lived together on-and-off for several years.

Pickford became secretly involved in a relationship with Douglas Fairbanks. They toured the U.S. together in 1918 to promote Liberty Bond sales for the World War I effort. Around this time, Pickford also suffered from the flu during the 1918 flu pandemic. Pickford divorced Moore on March 2, 1920, after she agreed to his $100,000 demand for a settlement. She married Fairbanks just days later on March 28, 1920. They went to Europe for their honeymoon; fans in London and in Paris caused riots trying to get to the famous couple. The couple's triumphant return to Hollywood was witnessed by vast crowds who turned out to hail them at railway stations across the United States.

"The Mark of Zorro" (1920) and a series of other swashbucklers gave the popular Fairbanks a more romantic, heroic image. Pickford continued to epitomize the virtuous but fiery girl next door. Even at private parties, people instinctively stood up when Pickford entered a room; she and her husband were often referred to as "Hollywood royalty". Their international reputations were broad. Foreign heads of state and dignitaries who visited the White House often asked if they could also visit Pickfair, the couple's mansion in Beverly Hills.

Dinners at Pickfair became celebrity events. Charlie Chaplin, Fairbanks' best friend, was often present. Other guests included George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder, and Meher Baba, among others. The public nature of Pickford's second marriage strained it to the breaking point. Both she and Fairbanks had little time off from producing and acting in their films. They were also constantly on display as America's unofficial ambassadors to the world, leading parades, cutting ribbons, and making speeches. When their film careers both began to flounder at the end of the silent era, Fairbanks' restless nature prompted him to overseas travel (something which Pickford did not enjoy). When Fairbanks' romance with Sylvia, Lady Ashley became public in the early 1930s, he and Pickford separated. They divorced January 10, 1936. Fairbanks' son by his first wife, Douglas Fairbanks Jr., claimed his father and Pickford long regretted their inability to reconcile.

On June 24, 1937, Pickford married her third and last husband, actor and band leader Buddy Rogers. They adopted two children: Roxanne (born 1944, adopted 1944) and Ronald Charles (born 1937, adopted 1943, a.k.a. Ronnie Pickford Rogers). A PBS "American Experience" documentary described Pickford's relationship with her children as tense. She criticized their physical imperfections, including Ronnie's small stature and Roxanne's crooked teeth. Both children later said their mother was too self-absorbed to provide real maternal love. In 2003, Ronnie recalled that "Things didn't work out that much, you know. But I'll never forget her. I think that she was a good woman."

After retiring from the screen, Pickford became an alcoholic, as her father had been. Her mother Charlotte died of breast cancer in March 1928. Her siblings, Lottie and Jack, both died of alcohol-related causes. These deaths, her divorce from Fairbanks, and the end of silent films left Pickford deeply depressed. Her relationship with her adopted children, Roxanne and Ronald, was turbulent at best. Pickford withdrew and gradually became a recluse, remaining almost entirely at Pickfair and allowing visits only from Lillian Gish, her stepson Douglas Fairbanks, Jr., and few other people. She appeared in court in 1959, in a matter pertaining to her co-ownership of North Carolina TV station WSJS-TV. The court date coincided with the date of her 67th birthday; under oath, when asked to give her age, Pickford replied: "I'm 21, going on 20."

In the mid-1960s, Pickford often received visitors only by telephone, speaking to them from her bedroom. Buddy Rogers often gave guests tours of Pickfair, including views of a genuine western bar Pickford had bought for Douglas Fairbanks, and a portrait of Pickford in the drawing room. A print of this image now hangs in the Library of Congress. In addition to her Oscar as best actress for "Coquette" (1929), Mary Pickford received an Academy Honorary Award in 1976 for lifetime achievement. The Academy sent a TV crew to her house to record her short statement of thanks – offering the public a very rare glimpse into Pickfair Manor.

Pickford believed that she had ceased to be a British subject when she married an American citizen upon her marriage to Fairbanks in 1920. Thus, she never acquired Canadian citizenship when it was first created in 1947. However, Pickford held and traveled under a British/Canadian passport which she renewed regularly at the British/Canadian consulates in Los Angeles, and she did not take out papers for American citizenship. She also owned a house in Toronto, Canada. Toward the end of her life, Pickford made arrangements with the Canadian Department of Citizenship to officially acquire Canadian citizenship because she wished to "die as a Canadian". Canadian authorities were not sure that she had ever lost her Canadian citizenship, given her passport status, but her request was approved and she officially became a Canadian citizen.
On May 29, 1979, Pickford died at a Santa Monica, California, hospital of complications from a cerebral hemorrhage she had suffered the week before. She was interred in the Garden of Memory of the Forest Lawn Memorial Park cemetery in Glendale, California.






</doc>
<doc id="18824" url="https://en.wikipedia.org/wiki?curid=18824" title="Mack Sennett">
Mack Sennett

Mack Sennett (born Michael Sinnott; January 17, 1880 – November 5, 1960) was a Canadian-American film actor, director, and producer, and studio head, known as the King of Comedy.

Born in Melbourne, Quebec, in 1880, he started in films in the Biograph company of New York, and later opened Keystone Studios in Edendale, California in 1912. It was the first fully enclosed film stage, and Sennett became famous as the originator of slapstick routines such as pie-throwing and car-chases, as seen in the Keystone Cops films. He also produced short features that displayed his Bathing Beauties, many of whom went on to develop successful acting careers.
Sennett's work in sound-movies was less successful and he was bankrupted in 1933. He was presented with an honorary Academy Award for his contribution to film comedy.

Born Michael Sinnott in Richmond Ste-Bibiane Parish, Quebec, he was the son of Irish Catholic John Sinnott and Catherine Foy. His parents married 1879 in Tingwick, Québec and moved the same year to Richmond, where John Sinnott was hired as a laborer. By 1883, when Michael's brother George was born, John Sinnott was working in Richmond as an innkeeper, a position he held for many years. His parents had all their children and raised their family in Richmond, then a small Eastern Townships village. At that time, Michael's grandparents were living in Danville, Québec. Michael Sinnott moved to Connecticut when he was 17 years old.

He lived for a while in Northampton, Massachusetts, where, according to his autobiography, Sennett first got the idea to become an opera singer after seeing a vaudeville show. He claimed that the most respected lawyer in town, Northampton mayor (and future President of the United States) Calvin Coolidge, as well as Sennett's mother, tried to talk him out of his musical ambitions.
In New York City, Sennett became an actor, singer, dancer, clown, set designer, and director for Biograph. A major distinction in his acting career, often overlooked, is that he played Sherlock Holmes 11 times, albeit as a parody, between 1911 and 1913.

With financial backing from Adam Kessel and Charles O. Bauman of the New York Motion Picture Company, Michael "Mack" Sennett founded Keystone Studios in Edendale, California in 1912 (which is now a part of Echo Park). The original main building which was the first totally enclosed film stage and studio ever constructed, is still there today. Many important actors cemented their film careers with Sennett, including Marie Dressler, Mabel Normand, Charles Chaplin, Harry Langdon, Roscoe Arbuckle, Harold Lloyd, Raymond Griffith, Gloria Swanson, Ford Sterling, Andy Clyde, Chester Conklin, Polly Moran, Louise Fazenda, The Keystone Cops, Bing Crosby, and W. C. Fields.

Sennett's slapstick comedies were noted for their wild car chases and custard pie warfare, especially in the "Keystone Cops "series. Sennett's first female comedian was Mabel Normand, who became a major star under his direction and with whom he embarked on a tumultuous romantic relationship. Sennett also developed the "Kid Comedies", a forerunner of the "Our Gang" films, and in a short time, his name became synonymous with screen comedy which were called "flickers" at the time. In 1915, Keystone Studios became an autonomous production unit of the ambitious Triangle Film Corporation, as Sennett joined forces with D. W. Griffith and Thomas Ince, both powerful figures in the film industry.

Also beginning in 1915, Sennett assembled a bevy of women known as the Sennett Bathing Beauties to appear in provocative bathing costumes in comedy short subjects, in promotional material, and in promotional events such as Venice Beach beauty contests. The Sennett Bathing Beauties continued to appear through 1928.

In 1917, Sennett gave up the Keystone trademark and organized his own company, Mack Sennett Comedies Corporation. (Sennett's bosses retained the Keystone trademark and produced a cheap series of comedy shorts that were "Keystones" in name only: they were unsuccessful, and Sennett had no connection with them.) Sennett went on to produce more ambitious comedy short films and a few feature-length films.
During the 1920s, his short subjects were in much demand, featuring stars such as Louise Fazenda, Billy Bevan, Andy Clyde, Harry Gribbon, Vernon Dent, Alice Day, Ralph Graves, Charlie Murray, and Harry Langdon. He produced several features with his brightest stars such as Ben Turpin and Mabel Normand.

Many of Sennett's films of the early 1920s were inherited by Warner Bros. Studio. Warner Bros. merged with the original distributor, First National, and added music and commentary to several of these short subjects. Unfortunately, many of the films of this period were destroyed due to inadequate storage. As a result, many of Sennett's films from his most productive and creative period no longer exist.

In the mid-1920s, Sennett moved to Pathé Exchange distribution. Pathé had a huge market share, but made bad corporate decisions, such as attempting to sell too many comedies at once (including those of Sennett's main competitor, Hal Roach). In 1927, Hollywood's two most successful studios Metro-Goldwyn-Mayer and Paramount Pictures, took note of the profits being made by smaller companies such as Pathé Exchange and Educational Pictures. MGM and Paramount resumed the production and distribution of short subjects. Hal Roach signed with MGM, but Mack Sennett remained with Pathé Exchange even during hard times, which were brought on by the competition. Hundreds of other independent exhibitors and movie houses of this period had switched from Pathe' to the new MGM or Paramount films and short subjects.

Sennett made a reasonably smooth transition to sound films, releasing them through Earle Hammons's Educational Pictures. Sennett occasionally experimented with color. Plus, he was the first to get a talkie short subject on the market in 1928. In 1932, he was nominated for the Academy Award for Live Action Short Film in the comedy division for producing "The Loud Mouth" (with Matt McHugh, in the sports-heckler role later taken in Columbia Pictures remakes by Charley Chase and Shemp Howard). Sennett also won an Academy Award in the novelty division for his film "Wrestling Swordfish" also in 1932. On March 25, 1932, he became a United States citizen.

Sennett often clung to outmoded techniques, making his early-1930s films seem dated and quaint. This doomed his attempt to re-enter the feature-film market with "Hypnotized" (starring blackface comedians Moran and Mack, "The Two Black Crows"). However, Sennett enjoyed great success with short comedies starring Bing Crosby, which were more than likely instrumental in Sennett's product being picked up by a major studio, Paramount Pictures. W. C. Fields conceived and starred in four famous Sennett-Paramount comedies. Fields himself recalled that he "made seven comedies for the Irishman", his original deal called for one film and an option for six more, but ultimately only four were made with Fields as star. Two other Sennett shorts were made with Fields scripts: The Singing Boxer (1933) with Donald Novis and Too Many Highballs (1933) with Lloyd Hamilton.

Sennett's studio did not survive the Great Depression. His partnership with Paramount lasted only one year and he was forced into bankruptcy in November 1933.

On January 12, 1934, Sennett was injured in an automobile accident that killed blackface performer Charles Mack in Mesa, Arizona.

His last work, in 1935, was as a producer-director for Educational Pictures, in which he directed Buster Keaton in "The Timid Young Man" and Joan Davis in "Way Up Thar". (The 1935 Vitaphone short subject "Keystone Hotel" is not a Sennett production, although it featured several alumni from the Mack Sennett Studios. Actually, Sennett was not involved in the making of this film.)

Mack Sennett went into semiretirement at the age of 55, having produced more than 1,000 silent films and several dozen talkies during a 25-year career. His studio property was purchased by Mascot Pictures (later part of Republic Pictures), and many of his former staffers found work at Columbia Pictures.

In March 1938, Sennett was presented with an honorary Academy Award: "for his lasting contribution to the comedy technique of the screen, the basic principles of which are as important today as when they were first put into practice, the Academy presents a Special Award to that master of fun, discoverer of stars, sympathetic, kindly, understanding comedy genius – Mack Sennett."

Rumors abounded that Sennett would be returning to film production (a 1938 publicity release indicated that he would be working with Stan Laurel of Laurel and Hardy), but apart from Sennett reissuing a couple of his Bing Crosby two-reelers to theaters, nothing happened. Sennett did appear in front of the camera, however, in "Hollywood Cavalcade" (1939), itself a thinly disguised version of the Mack Sennett-Mabel Normand romance. In 1949, he provided film footage for and also appeared in the first full-length comedy compilation called "Down Memory Lane" (1949), which was written and narrated by Steve Allen. Sennett was profiled in the television series "This is Your Life" in 1954. and made a cameo appearance (for $1,000) in "Abbott and Costello Meet the Keystone Kops" (1955). His last contribution worth noting was to the NBC radio program "Biography in Sound" relating memories of working with W.C. Fields, which was broadcast February 28, 1956.

Sennett died on November 5, 1960, in Woodland Hills, California, aged 80. He was interred in the Holy Cross Cemetery in Culver City, California.

For his contribution to the motion picture industry, Sennett was honored with a star on the Hollywood Walk of Fame at 6712 Hollywood Boulevard. He was also inducted into Canada's Walk of Fame in 2014.

A line in a Henry Kuttner science-fiction short story "Piggy Bank" reads, "Within seconds the scene resembled a Mack Sennett pie-throwing comedy."

In "A Story of Water", a 1961 short film by Jean-Luc Godard and François Truffaut, the directors dedicate the film to Mack Sennett.

Henry Mancini's score for the 1963 film "The Pink Panther", the original entry in the series, contains a segment called "Shades of Sennett". It is played on a silent film era style "barrel house" piano, and accompanies a climactic scene in which the incompetent police detective Inspector Clouseau is involved in a multi-vehicle chase with the antagonists.

In 1974, Michael Stewart and Jerry Herman wrote the musical "Mack & Mabel", chronicling the romance between Sennett and Mabel Normand.

Sennett also was a leading character in "The Biograph Girl", a 1980 musical about the silent-film era.

Peter Lovesey's 1983 novel "Keystone" is a whodunnit set in the Keystone Studios and involving (among others), Mack Sennett, Mabel Normand, Roscoe Arbuckle, and the Keystone Cops.

Dan Aykroyd portrayed Mack Sennett in the 1992 movie "Chaplin". Marisa Tomei played Mabel Normand and Robert Downey, Jr. starred as Charlie Chaplin.

Joseph Beattie and Andrea Deck portrayed Mack Sennett and Mabel Normand, respectively, in episode eight of series two of ITV's "Mr. Selfridge".





</doc>
<doc id="18825" url="https://en.wikipedia.org/wiki?curid=18825" title="Motion Picture Patents Company">
Motion Picture Patents Company

The Motion Picture Patents Company (MPPC, also known as the Edison Trust), founded in December 1908 and terminated seven years later in 1915 after conflicts within the industry, was a trust of all the major US film companies and local foreign-branches (Edison, Biograph, Vitagraph, Essanay, Selig Polyscope, Lubin Manufacturing, Kalem Company, Star Film Paris, American Pathé), the leading film distributor (George Kleine) and the biggest supplier of raw film stock, Eastman Kodak. The MPPC ended the domination of foreign films on US screens, standardized the manner in which films were distributed and exhibited within the US, and improved the quality of US motion pictures by internal competition. But it also discouraged its members' entry into feature film production, and the use of outside financing, both to its members' eventual detriment.

The MPPC was preceded by the Edison licensing system, in effect in 1907–1908, on which the MPPC was modeled. During the 1890s, Thomas Edison owned most of the major US patents relating to motion picture cameras. The Edison Manufacturing Company's patent lawsuits against each of its domestic competitors crippled the US film industry, reducing production mainly to two companies: Edison and Biograph, which used a different camera design. This left Edison's other rivals with little recourse but to import French and British films.

Since 1902, Edison had also been notifying distributors and exhibitors that if they did not use Edison machines and films exclusively, they would be subject to litigation for supporting filmmaking that infringed Edison's patents. Exhausted by the lawsuits, Edison's competitors — Essanay, Kalem, Pathé Frères, Selig, and Vitagraph — approached him in 1907 to negotiate a licensing agreement, which Lubin was also invited to join. The one notable filmmaker excluded from the licensing agreement was Biograph, which Edison hoped to squeeze out of the market. No further applicants could become licensees. The purpose of the licensing agreement, according to an Edison lawyer, was to "preserve the business of present manufacturers and not to throw the field open to all competitors."

In February 1909, major European producers held the Paris Film Congress in an attempt to create a similar European organisation. This group also included MPPC members Pathé and Vitagraph, which had extensive European production and distribution interests. This proposed European cartel ultimately failed when Pathé, then still the largest company in the world, withdrew in April.

Biograph retaliated for being frozen out of the trust agreement by purchasing the patent to the Latham film loop, a key feature of virtually all motion picture cameras then in use. Edison sued to gain control of the patent; however, after a federal court upheld the validity of the patent in 1907, Edison began negotiation with Biograph in May 1908 to reorganize the Edison licensing system. The resulting trust pooled 16 motion picture patents. Ten were considered of minor importance; the remaining key six pertained one each to films, cameras, and the Latham loop, and three to projectors.

The MPPC eliminated the outright sale of films to distributors and exhibitors, replacing it with rentals, which allowed quality control over prints that had formerly been exhibited long past their prime. The trust also established a uniform rental rate for all licensed films, thereby removing price as a factor for the exhibitor in film selection, in favor of selection made on quality, which in turn encouraged the upgrading of production values.

However, the MPPC also established a monopoly on all aspects of filmmaking. Eastman Kodak, which owned the patent on raw film stock, was a member of the trust and thus agreed to sell stock only to other members. Likewise, the trust's control of patents on motion picture cameras ensured that only MPPC studios were able to film, and the projector patents allowed the trust to make licensing agreements with distributors and theaters – and thus determine who screened their films and where.

The patents owned by the MPPC allowed them to use federal law enforcement officials to enforce their licensing agreements and to prevent unauthorized use of their cameras, films, projectors, and other equipment. In some cases, however, the MPPC made use of hired thugs and mob connections to violently disrupt productions that were not licensed by the trust.

The MPPC also strictly regulated the production content of their films, primarily as a means of cost control. Films were initially limited to one reel in length (13–17 minutes), although competition by independent and foreign producers by 1912 led to the introduction of two-reelers, and by 1913, three- and four-reelers.

Many independent filmmakers, who controlled from one-quarter to one-third of the domestic marketplace, responded to the creation of the MPPC by moving their operations to Hollywood, whose distance from Edison's home base of New Jersey made it more difficult for the MPPC to enforce its patents. The Ninth Circuit Court of Appeals, which is headquartered in San Francisco, California, and covers the area, was averse to enforcing patent claims. Southern California was also chosen because of its beautiful year-round weather and varied countryside; its topography, semi-arid climate and widespread irrigation gave its landscapes the ability to offer motion picture shooting scenes set in deserts, jungles and great mountains. Hollywood had one additional advantage: if a non-licensed studio was sued, it was only a hundred miles to "run for the border" and get out of the US to Mexico, where the trust's patents were not in effect and thus equipment could not be seized.

The reasons for the MPPC's decline are manifold. The first blow came in 1911 when Eastman Kodak modified its exclusive contract with the MPPC to allow Kodak, which led the industry in quality and price, to sell its raw film stock to unlicensed independents. The number of theaters exhibiting independent films grew by 33 percent within twelve months, to half of all houses.

Another reason was the MPPC's overestimation of the efficiency of controlling the motion picture industry through patent litigation and the exclusion of independents from licensing. The slow process of using detectives to investigate patent infringements, and of obtaining injunctions against the infringers, was outpaced by the dynamic rise of new companies in diverse locations.

Despite the rise in popularity of feature films in 1912–1913 from independent producers and foreign imports, the MPPC was very reluctant to make the changes necessary to distribute such longer films. Edison, Biograph, Essanay, and Vitagraph did not release their first features until 1914, after dozens, if not hundreds, of feature films, had been released by independents.

Patent royalties to the MPPC ended in September 1913 with the expiration of the last of the patents filed in the mid-1890s at the dawn of commercial film production and exhibition. Thus the MPPC lost the ability to control the American film industry through patent licensing and had to rely instead on its subsidiary, the General Film Company, formed in 1910, which monopolized film distribution in US.

The outbreak of World War I in 1914 cut off most of the European market, which played a much more significant part of the revenue and profit for MPPC members than for the independents, which concentrated on Westerns produced for a primarily US market.

The end came with a federal court decision in "United States v. Motion Picture Patents Co." on October 1, 1915, which ruled that the MPPC's acts went "far beyond what was necessary to protect the use of patents or the monopoly which went with them" and was, therefore, an illegal restraint of trade under the Sherman Antitrust Act. An appellate court dismissed the MPPC's appeal, and officially terminated the company in 1918.




</doc>
<doc id="18826" url="https://en.wikipedia.org/wiki?curid=18826" title="MD5">
MD5

The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value. Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. It can still be used as a checksum to verify data integrity, but only against unintentional corruption. It remains suitable for other non-cryptographic purposes, for example for determining the partition for a particular key in a partitioned database.

MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321.
One basic requirement of any cryptographic hash function is that it should be computationally infeasible to find two distinct messages that hash to the same value. MD5 fails this requirement catastrophically; such collisions can be found in seconds on an ordinary home computer.

The weaknesses of MD5 have been exploited in the field, most infamously by the Flame malware in 2012. The CMU Software Engineering Institute considers MD5 essentially "cryptographically broken and unsuitable for further use".

, MD5 continues to be widely used, in spite of its well-documented weaknesses and deprecation by security experts.

MD5 is one in a series of message digest algorithms designed by Professor Ronald Rivest of MIT (Rivest, 1992). When analytic work indicated that MD5's predecessor MD4 was likely to be insecure, Rivest designed MD5 in 1991 as a secure replacement. (Hans Dobbertin did indeed later find weaknesses in MD4.)

In 1993, Den Boer and Bosselaers gave an early, although limited, result of finding a "pseudo-collision" of the MD5 compression function; that is, two different initialization vectors that produce an identical digest.

In 1996, Dobbertin announced a collision of the compression function of MD5 (Dobbertin, 1996). While this was not an attack on the full MD5 hash function, it was close enough for cryptographers to recommend switching to a replacement, such as SHA-1 or RIPEMD-160.

The size of the hash value (128 bits) is small enough to contemplate a birthday attack. MD5CRK was a distributed project started in March 2004 with the aim of demonstrating that MD5 is practically insecure by finding a collision using a birthday attack.

MD5CRK ended shortly after 17 August 2004, when collisions for the full MD5 were announced by Xiaoyun Wang, Dengguo Feng, Xuejia Lai, and Hongbo Yu. Their analytical attack was reported to take only one hour on an IBM p690 cluster.

On 1 March 2005, Arjen Lenstra, Xiaoyun Wang, and Benne de Weger demonstrated construction of two X.509 certificates with different public keys and the same MD5 hash value, a demonstrably practical collision. The construction included private keys for both public keys. A few days later, Vlastimil Klima described an improved algorithm, able to construct MD5 collisions in a few hours on a single notebook computer. On 18 March 2006, Klima published an algorithm that could find a collision within one minute on a single notebook computer, using a method he calls tunneling.

Various MD5-related RFC errata have been published. 
In 2009, the United States Cyber Command used an MD5 hash value of their mission statement as a part of their official emblem.

On 24 December 2010, Tao Xie and Dengguo Feng announced the first published single-block (512-bit) MD5 collision. (Previous collision discoveries had relied on multi-block attacks.) For "security reasons", Xie and Feng did not disclose the new attack method. They issued a challenge to the cryptographic community, offering a US$10,000 reward to the first finder of a different 64-byte collision before 1 January 2013. Marc Stevens responded to the challenge and published colliding single-block messages as well as the construction algorithm and sources.

In 2011 an informational RFC 6151 was approved to update the security considerations in MD5 and HMAC-MD5.

The security of the MD5 hash function is severely compromised. A collision attack exists that can find collisions within seconds on a computer with a 2.6 GHz Pentium 4 processor (complexity of 2). Further, there is also a chosen-prefix collision attack that can produce a collision for two inputs with specified prefixes within seconds, using off-the-shelf computing hardware (complexity 2).
The ability to find collisions has been greatly aided by the use of off-the-shelf GPUs. On an NVIDIA GeForce 8400GS graphics processor, 16–18 million hashes per second can be computed. An NVIDIA GeForce 8800 Ultra can calculate more than 200 million hashes per second.

These hash and collision attacks have been demonstrated in the public in various situations, including colliding document files and digital certificates. As of 2015, MD5 was demonstrated to be still quite widely used, most notably by security research and antivirus companies.

As of 2019, one quarter of widely used content management systems were reported to still use MD5 for password hashing.

In 1996, a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1, which has since been found to be vulnerable as well.
In 2004 it was shown that MD5 is not collision-resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity.

As of 2010, the CMU Software Engineering Institute considers MD5 "cryptographically broken and unsuitable for further use", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.

In 1996, collisions were found in the compression function of MD5, and Hans Dobbertin wrote in the RSA Laboratories technical newsletter, "The presented attack does not yet threaten practical applications of MD5, but it comes rather close ... in the future MD5 should no longer be implemented ... where a collision-resistant hash function is required."

In 2005, researchers were able to create pairs of PostScript documents and X.509 certificates with the same hash. Later that year, MD5's designer Ron Rivest wrote that "md5 and sha1 are both clearly broken (in terms of collision-resistance)".

On 30 December 2008, a group of researchers announced at the 25th Chaos Communication Congress how they had used MD5 collisions to create an intermediate certificate authority certificate that appeared to be legitimate when checked by its MD5 hash. The researchers used a cluster of Sony PlayStation 3 units at the EPFL in Lausanne, Switzerland to change a normal SSL certificate issued by RapidSSL into a working CA certificate for that issuer, which could then be used to create other certificates that would appear to be legitimate and issued by RapidSSL. VeriSign, the issuers of RapidSSL certificates, said they stopped issuing new certificates using MD5 as their checksum algorithm for RapidSSL once the vulnerability was announced. Although Verisign declined to revoke existing certificates signed using MD5, their response was considered adequate by the authors of the exploit (Alexander Sotirov, Marc Stevens, Jacob Appelbaum, Arjen Lenstra, David Molnar, Dag Arne Osvik, and Benne de Weger). Bruce Schneier wrote of the attack that "we already knew that MD5 is a broken hash function" and that "no one should be using MD5 anymore". The SSL researchers wrote, "Our desired impact is that Certification Authorities will stop using MD5 in issuing new certificates. We also hope that use of MD5 in other applications will be reconsidered as well."

In 2012, according to Microsoft, the authors of the Flame malware used an MD5 collision to forge a Windows code-signing certificate.

MD5 uses the Merkle–Damgård construction, so if two prefixes with the same hash can be constructed, a common suffix can be added to both to make the collision more likely to be accepted as valid data by the application using it. Furthermore, current collision-finding techniques allow to specify an arbitrary "prefix": an attacker can create two colliding files that both begin with the same content. All the attacker needs to generate two colliding files is a template file with a 128-byte block of data, aligned on a 64-byte boundary that can be changed freely by the collision-finding algorithm. An example MD5 collision, with the two messages differing in 6 bits, is:

Both produce the MD5 hash codice_1.
The difference between the two samples is that the leading bit in each nibble has been flipped. For example, the 20th byte (offset 0x13) in the top sample, 0x87, is 10000111 in binary. The leading bit in the byte (also the leading bit in the first nibble) is flipped to make 00000111, which is 0x07, as shown in the lower sample.

Later it was also found to be possible to construct collisions between two files with separately chosen prefixes. This technique was used in the creation of the rogue CA certificate in 2008. A new variant of parallelized collision searching using MPI was proposed by Anton Kuznetsov in 2014, which allowed to find a collision in 11 hours on a computing cluster.

In April 2009, an attack against MD5 was published that breaks MD5's preimage resistance. This attack is only theoretical, with a computational complexity of 2 for full preimage.

MD5 digests have been widely used in the software world to provide some assurance that a transferred file has arrived intact. For example, file servers often provide a pre-computed MD5 (known as md5sum) checksum for the files, so that a user can compare the checksum of the downloaded file to it. Most unix-based operating systems include MD5 sum utilities in their distribution packages; Windows users may use the included PowerShell function "Get-FileHash", install a Microsoft utility, or use third-party applications. Android ROMs also use this type of checksum.

As it is easy to generate MD5 collisions, it is possible for the person who created the file to create a second file with the same checksum, so this technique cannot protect against some forms of malicious tampering. In some cases, the checksum cannot be trusted (for example, if it was obtained over the same channel as the downloaded file), in which case MD5 can only provide error-checking functionality: it will recognize a corrupt or incomplete download, which becomes more likely when downloading larger files.

Historically, MD5 has been used to store a one-way hash of a password, often with key stretching. NIST does not include MD5 in their list of recommended hashes for password storage.

MD5 is also used in the field of electronic discovery, in order to provide a unique identifier for each document that is exchanged during the legal discovery process. This method can be used to replace the Bates stamp numbering system that has been used for decades during the exchange of paper documents. As above, this usage should be discouraged due to the ease of collision attacks.

MD5 processes a variable-length message into a fixed-length output of 128 bits. The input message is broken up into chunks of 512-bit blocks (sixteen 32-bit words); the message is padded so that its length is divisible by 512. The padding works as follows: first a single bit, 1, is appended to the end of the message. This is followed by as many zeros as are required to bring the length of the message up to 64 bits fewer than a multiple of 512. The remaining bits are filled up with 64 bits representing the length of the original message, modulo 2.

The main MD5 algorithm operates on a 128-bit state, divided into four 32-bit words, denoted , , , and . These are initialized to certain fixed constants. The main algorithm then uses each 512-bit message block in turn to modify the state. The processing of a message block consists of four similar stages, termed "rounds"; each round is composed of 16 similar operations based on a non-linear function , modular addition, and left rotation. Figure 1 illustrates one operation within a round. There are four possible functions; a different one is used in each round:

formula_2 denote the XOR, AND, OR and NOT operations respectively.

The MD5 hash is calculated according to this algorithm. All values are in little-endian.

"Note: Instead of the formulation from the original RFC 1321 shown, the following may be used for improved efficiency (useful if assembly language is being used – otherwise, the compiler will generally optimize the above code. Since each computation is dependent on another in these formulations, this is often slower than the above method where the nand/and can be parallelised):"

The 128-bit (16-byte) MD5 hashes (also termed "message digests") are typically represented as a sequence of 32 hexadecimal digits. The following demonstrates a 43-byte ASCII input and the corresponding MD5 hash:

Even a small change in the message will (with overwhelming probability) result in a mostly different hash, due to the avalanche effect. For example, adding a period to the end of the sentence:

The hash of the zero-length string is:

The MD5 algorithm is specified for messages consisting of any number of bits; it is not limited to multiples of eight bits (octets, bytes). Some MD5 implementations such as md5sum might be limited to octets, or they might not support "streaming" for messages of an initially undetermined length.

Below is a list of cryptography libraries that support MD5:






</doc>
<doc id="18830" url="https://en.wikipedia.org/wiki?curid=18830" title="Magic: The Gathering">
Magic: The Gathering

Magic: The Gathering (colloquially known as Magic" cards, Magic or just MTG") is a collectible and digital collectible card game created by Richard Garfield. Released in 1993 by Wizards of the Coast (now a subsidiary of Hasbro), "Magic" was the first trading card game and has approximately twenty million players , and over twenty billion "Magic" cards produced in the period from 2008 to 2016, during which time it grew in popularity.
Each game of "Magic" represents a battle between wizards known as "planeswalkers" who cast spells, use artifacts, and summon creatures as depicted on individual cards in order to defeat their opponents, typically, but not always, by draining them of their 20 starting "life points" in the standard format. Although the original concept of the game drew heavily from the motifs of traditional fantasy role-playing games such as "Dungeons & Dragons", the gameplay bears little similarity to pencil-and-paper adventure games, while simultaneously having substantially more cards and more complex rules than many other card games.

"Magic" can be played by two or more players, either in person with printed cards or on a computer, smartphone or tablet with virtual cards through the Internet-based software "" or such as "". It can be played in various rule formats, which fall into two categories: "constructed" and "limited". Limited formats involve players building a deck spontaneously out of a pool of random cards with a minimum deck size of 40 cards; in constructed formats, players create decks from cards they own, usually with a minimum of 60 cards per deck.

New cards are released on a regular basis through . An organized tournament system (the DCI) played at the international level and a worldwide community of has developed, as well as a substantial resale market for "Magic" cards. Certain cards can be monetarily valuable due to their rarity in production and utility in gameplay, with prices ranging from a few cents to thousands of dollars.

Richard Garfield was a doctoral candidate in combinatorial mathematics at University of Pennsylvania when he first started to design the game. During his free time he worked with local volunteer playtesters to help refine the game. He had been brought on as an adjunct professor at Whitman College in 1991 when Peter Adkison (then CEO of Wizards of the Coast games company) first met with Garfield to discuss Garfield's new game "RoboRally". Adkison saw the game as very promising, but declined to produce it as Wizards of the Coast lacked the resources. He did like Garfield's ideas and mentioned that he was looking for a portable game that could be played in the downtime that frequently occurs at gaming conventions. Garfield returned and presented the general outline of the concept of a trading card game, based on his earlier game "Five Magics" from 1982. Adkison immediately saw the potential of this idea and agreed to produce it. "Magic: The Gathering" underwent a general release on August 5, 1993.

While the game was simply called "Magic" through most of playtesting, when the game had to be officially named a lawyer informed them that the name "Magic" was too generic to be trademarked. "Mana Clash" was instead chosen to be the name used in the first solicitation of the game. However, everybody involved with the game continued to refer to it simply as "Magic". After further legal consultation, it was decided to rename the game "Magic: The Gathering", thus enabling the name to be trademarked.

A patent was granted to Wizards of the Coast in 1997 for "a novel method of game play and game components that in one embodiment are in the form of trading cards" that includes claims covering games whose rules include many of "Magic"'s elements in combination, including concepts such as changing orientation of a game component to indicate use (referred to in the rules of "Magic" and later of Garfield's games such as "" as "tapping") and constructing a deck by selecting cards from a larger pool. The patent has aroused criticism from some observers, who believe some of its claims to be invalid. In 2003, the patent was an element of a larger legal dispute between Wizards of the Coast and Nintendo, regarding trade secrets related to Nintendo's "Pokémon Trading Card Game". The legal action was settled out of court, and its terms were not disclosed.

"Magic" was an immediate success for Wizards of the Coast. Early on they were even reluctant to advertise the game because they were unable to keep pace with existing demand. Initially "Magic" attracted many "Dungeons & Dragons" players, but the following included all types of other people as well. The success of the game quickly led to the creation of similar games by other companies as well as Wizards of the Coast themselves. Companion Games produced the Galactic Empires CCG (the first science fiction trading card game), which allowed players to pay for and design their own promotional cards, while TSR created the "Spellfire" game, which eventually included five editions in six languages, plus twelve expansion sets. Wizards of the Coast produced "" (now called "Vampire: The Eternal Struggle"), a game about modern-day vampires. Other similar games included trading card games based on "Star Trek" and "Star Wars". "Magic" is often cited as an example of a 1990s collecting fad, though the game's makers were able to overcome the bubble traditionally associated with collecting fads.

The success of the initial edition prompted a reissue later in 1993, along with expansions to the game. "" was released as the first in December 1993. New expansions and revisions of the base game ("Core Sets") have since been released on a regular basis, amounting to four releases a year. By the end of 1994, the game had printed over a billion cards. Until the release of "" in 1996, expansions were released on an irregular basis. Beginning in 2009 one revision of the core set and a set of three related expansions called a "block" were released every year. This system was revised in 2015, with the Core Set being eliminated and blocks now consisting of two sets, released biannually. A further revision occurred in 2018, reversing the elimination of the core sets and no longer constraining sets to blocks. While the essence of the game has always stayed the same, the rules of "Magic" have undergone three major revisions with the release of the "" in 1994, in 1999, and "Magic 2010" in July 2009. With the release of the "" in 2003, "Magic" also received a major visual redesign.

In 1996, Wizards of the Coast established the "", a circuit of tournaments where players can compete for sizeable cash prizes over the course of a single weekend-long tournament. In 2009 the top prize at a single tournament was US$40,000. Sanctioned through the DCI, the tournaments added an element of prestige to the game by virtue of the cash payouts and media coverage from within the community. For a brief period of time, ESPN2 televised the tournaments.

By April 1997, cards had been sold. In 1999, Wizards of The Coast was acquired by Hasbro for $325 million, making "Magic" a Hasbro game.

While unofficial methods of online play existed previously, "" (often shortened to "MTGO" or "Modo"), an official online version of the game, was released in 2002. A new, updated version of "Magic Online" was released in April 2008.

In January 2014, Hasbro announced a franchise film deal with 20th Century Fox for "Magic: The Gathering", saying that they wanted "to launch a massive franchise on the scale of "Harry Potter" and "The Lord of the Rings"." Simon Kinberg was to serve as a producer for the project. In June 2014, Fox hired screenwriter Bryan Cogman to write the script for the film. As of 2019, no film has entered production.

In February 2018, Wizards noted that between the years of 2008 and 2016 they had printed over 20 billion "Magic: the Gathering" cards.

In June 2019, it was announced that the Russo Brothers are producing an animated series for "Magic: The Gathering" on Netflix.

Scott Haring reviewed "Magic: The Gathering" in "Pyramid" #4 (Nov./Dec., 1993), and stated that "Not only is "Magic" the best gaming bargain to come down the pike in memory; not only is it the most original idea in years; it's also a delightfully addictive game that you and your friends will find impossible to put down."

A 2004 article in "USA Today" suggested that playing "Magic" might help improve the social and mental skills of some of the players. The article interviewed players' parents who believe that the game, similar to sports, teaches children how to more gracefully win and lose. "Magic" also contains a great amount of strategy and vocabulary that children may not be exposed to on a regular basis. Parents also claimed that playing "Magic" helped keep their children out of trouble, such as using illegal drugs or joining criminal gangs. On the other hand, the article also briefly mentions that "Magic" can be highly addictive, leading to parents worried about their children's "Magic" obsession. In addition, until 2007, some of the better players had opportunities to compete for a small number of scholarships.

Jordan Weisman, an American game designer and entrepreneur, commented, "I love games that challenge and change our definition of adventure gaming, and "Magic: The Gathering" is definitely one of a very short list of titles that has accomplished that elusive goal. By combining the collecting and trading elements of baseball cards with the fantasy play dynamics of role-playing games, "Magic" created a whole new genre of product that changed our industry forever."In 2015, "The Guardian" reported that an estimated 20 million people played "Magic" around the world and that the game had a thriving tournament scene, a professional league and a weekly organized game program called Friday Night Magic.

A July 2019 article in "Bloomberg" reported that ""Magic" is part of the [Hasbro’s] 'franchise brands,' a segment that accounted for $2.45 billion in net revenue for the company last year, bigger than its emerging, partner and gaming brand units combined. [Chris] Cocks said "Magic" accounts for a 'meaningful portion' of that, with KeyBanc estimating the game’s contribution is already more than $500 million—including both the physical cards and the nascent digital version. Of the franchise brands, only "Magic" and Monopoly logged revenue gains last year". "", in open beta testing since September 2018, is a free-to-play digital collectible card game with microtransaction purchases based on "Magic". Brett Andress, an analyst at KeyBanc Capital Markets, predicts "Magic: The Gathering Arena" adding as much as 98 cents a share in incremental earnings to results by 2021 (which is at least a 20% boost). Joe Deaux, for "Bloomberg", wrote that "nearly 3 million active users will be playing Arena by the end of this year, KeyBanc estimates, and that could swell to nearly 11 million by 2021 according to its bull case scenario—especially if it expands from PCs to mobile. That’s just active users, and registered users could be higher by the millions. Already, according to Hasbro, a billion games have been played online".

In addition, several individuals including Richard Garfield and Donato Giancola won personal awards for their contributions to "Magic".

A game of "Magic" involves two or more players who are engaged in a battle acting as powerful wizards called "planeswalkers". Each player has their own deck, either one previously constructed or made from a limited pool of cards for the event. A player starts the game with twenty "life points" and loses the game when their life total is reduced to zero. A player can also lose if they must draw from an empty deck. In addition, some cards specify other ways to win or lose the game. Garfield has stated that two major influences in his creation of "Magic: the Gathering" were the games "Cosmic Encounter", which first used the concept that normal rules could sometimes be overridden, and "Dungeons & Dragons". The "Golden Rule of "Magic"" states that "Whenever a card's text directly contradicts the rules, the card takes precedence." The "Comprehensive Rules", a detailed rulebook, exists to clarify conflicts.

Players begin the game by shuffling their decks and then drawing seven cards. Players draw one card at the beginning of each of their turns, except the first player on their first turn unless there are more than 2 players. Players alternate turns. The two basic kinds of cards are "spells" and "lands". Lands provide "mana", or magical energy, which is used as magical fuel when the player attempts to cast spells. Players may only play one land per turn. More powerful spells cost more mana, so as the game progresses more mana becomes available, and the quantity and relative power of the spells played tends to increase. Spells come in several varieties: "sorceries" and "instants" have a single, one-time effect before they go to the "graveyard" (discard pile); "enchantments" and "artifacts" are "permanents" that remain in play after being cast to provide a lasting magical effect; "creature" spells (also a type of permanent) summon creatures that can attack and damage an opponent. The set "Lorwyn" introduced the new "planeswalker" card type, which represents powerful allies who fight with their own magic abilities.

In most Constructed tournament formats, decks are required to be a minimum of sixty cards, with no upper limit, provided the player can shuffle their deck unaided. Players may use no more than four copies of any named card, with the exception of "basic lands", which act as a standard resource in "Magic", and some specific cards that state otherwise. For example, the card Relentless Rats states that a deck may contain any number of itself. Certain formats such as may limit the number of iterations of a single card players may have in their decks to 1 (excluding basic lands). These are colloquially known as singleton formats.

In most Constructed formats, there exists a list of individual cards which have been "restricted" (the card is limited to a single copy per deck) or "banned" (the card is no longer legal for tournament play). These limitations are usually for balance of power reasons, but have been occasionally made because of gameplay mechanics.

In "" tournament formats, a small number of cards are opened for play from booster packs or tournament packs, and a minimum deck size of forty cards is enforced. The most popular limited format is Booster Draft, in which players open a booster pack, choose a card from it, and pass it to the player seated next to them. This continues until all the cards have been picked, and then a new pack is opened. Three packs are opened altogether, and the direction of passing alternates left-right-left. Once the draft is done, players create 40-card decks out of the cards they picked and play games with the players they drafted with.

Deck building requires strategy as players must choose among thousands of cards which they want to play. This requires players to evaluate the power of their cards, as well as the possible synergies between them, and their possible interactions with the cards they expect to play against (this "metagame" can vary in different locations or time periods). The choice of cards is usually narrowed by the player deciding which colors they want to include in the deck. This decision is a key part of creating a deck. In general, reducing the number of colors used increases the consistency of play and the probability of drawing the lands needed to cast one's spells, at the expense of restricting the range of tactics available to the player.

Most spells come in one of five colors: each with a specific lore, personality, philosophy, or style of play as a prelude to strategy. The colors can be seen on the back of the cards, in a pentagonal design, called the "Color Wheel" or "Color Pie". Clockwise from the top, they are: white (W), blue (U), black (B), red (R), and green (G). To play a spell of a given color, at least one mana of that color is required. This mana is normally generated by a basic land: plains for white, island for blue, swamp for black, mountain for red, and forest for green. The balances and distinctions among the five colors form one of the defining aspects of the game. Each color has strengths and weaknesses based on the "style" of magic it represents.

The colors adjacent to each other on the pentagon are "allied" and often have similar, complementary abilities. For example, Blue has a relatively large number of creatures with the "flying" ability, as do White and Black, which are next to it. The two non-adjacent colors to a particular color are "enemy" colors, and are thematically opposed. For instance, Red tends to be very aggressive, while White and Blue are often more defensive in nature. The Research and Development (R&D) team at Wizards of the Coast aims to balance power and abilities among the five colors by using the "Color Pie" to differentiate the strengths and weaknesses of each. This guideline lays out the capabilities, themes, and mechanics of each color and allows for every color to have its own distinct attributes and gameplay. The Color Pie is used to ensure new cards are thematically in the correct color and do not infringe on the territory of other colors.







"Magic", like many other games, combines chance and skill. One frequent complaint about the game involves the notion that there is too much luck involved, especially concerning possessing too many or too few lands. Early in the game especially, too many or too few lands could ruin a player's chance at victory without the player having made a mistake. This in-game statistical variance can be minimized by proper deck construction, as an appropriate land count can reduce mana problems. In "Duels of the Planeswalkers 2012", the land count is automatically adjusted to 40% of the total deck size.

A "mulligan" rule was introduced into the game, first informally in casual play and then in the official game rules. The most current mulligan rule allows players to shuffle an unsatisfactory opening hand back into the deck at the start of the game, draw a new hand with the same number of cards, and repeat until satisfied, after which any player who has mulliganed, will put cards from the hand they kept on the bottom of their deck equal to the number of times they mulliganed. In multiplayer, a player may take one mulligan without penalty, while subsequent mulligans will still cost one card (a rule known as "Partial Paris mulligan"). The original mulligan allowed a player a single redraw of seven new cards if that player's initial hand contained seven or zero lands. A variation of this rule called a "forced mulligan" is still used in some casual play circles and in multiplayer formats on "Magic Online", and allows a single "free" redraw of seven new cards if a player's initial hand contains seven, six, one or zero lands. With the release of the Core Set 2020 a new mulligan system was introduced for competitive play known as the London Mulligan. 

Confessing his love for games combining both luck and skill, "Magic" creator Richard Garfield admitted its influence in his design of "Magic". In addressing the complaint about luck influencing a game, Garfield states that new and casual players tend to appreciate luck as a leveling effect, since randomness can increase their chances of winning against a more skilled player. Meanwhile, a player with higher skills appreciates a game with less chance, as the higher degree of control increases their chances of winning. According to Garfield, "Magic" has and would likely continue decreasing its degree of luck as the game matured. The "Mulligan rule", as well as card design, past vs. present, are good examples of this trend. He feels that this is a universal trend for maturing games. Garfield explained using chess as an example, that unlike modern chess, in predecessors, players would use dice to determine which chess piece to move.

The original set of rules prescribed that all games were to be played for ante. Garfield was partly inspired by the game of marbles and wanted folks to play with the cards rather than collect them. For Magic, each player removed a card at random from the deck they wished to play with and the two cards would be set aside as the ante. At the end of the match, the winner would take and keep both cards. Early sets included a few cards with rules designed to interact with this gambling aspect, allowing replacements of cards up for ante, adding more cards to the ante, or even permanently trading ownership of cards in play.

The ante concept became controversial because many regions had restrictions on games of chance. The rule was later made optional because of these restrictions and because of players' reluctance to possibly lose a card that they owned. The gambling rule is forbidden at sanctioned events and is now mostly a relic of the past, though it still sees occasional usage in friendly games as well as the format. The last card to mention ante was printed in the 1995 expansion set "".

"Magic" tournaments regularly occur in gaming stores and other venues. Larger tournaments with hundreds of competitors from around the globe sponsored by Wizards of the Coast are arranged many times every year, with substantial cash prizes for the top finishers. A number of websites report on tournament news, give complete lists for the most currently popular decks, and feature articles on current issues of debate about the game. The DCI, which is owned and operated by Wizards of the Coast, is the organizing body for sanctioned "Magic" events. The two major categories of tournament play are "Constructed" and "Limited".

In "Constructed" tournaments, each player arrives with a pre-built deck, which must have a minimum of sixty cards and follow other deck construction rules. The deck may also have up to a fifteen card sideboard, which allows players to modify their deck. Normally the first player to win two games is the winner of the match.

Different formats of Constructed "Magic" exist, each allowing different cards. The DCI maintains a "Banned and Restricted List" for each format; players may not use banned cards at all, and restricted cards are limited to one copy per deck. The DCI bans cards that it determines are damaging the health of a format; it seeks to use this remedy as infrequently as possible, and only a handful of cards have been banned in recent years.



In "Limited" tournaments, players construct decks using booster packs plus any additional basic lands of their choice. The decks in Limited tournaments must be a minimum of forty cards. All unused cards function as the sideboard, which, as in "Constructed" formats, can be freely exchanged between games of a match, as long as the deck continues to adhere to the forty card minimum. The rule that a player may use only four copies of any given card does not apply.
Players often create their own formats based on any number of criteria. Sometimes these can be based on limiting the financial value of a deck, mixing and matching different blocks or sets, or taking an existing format and modifying the DCI Banned List. Commander (formerly Elder Dragon Highlander) was one such format, before becoming officially supported by Wizards of the Coast. One of the most popular player created formats for Limited is Cube Drafting. Similar in structure to Draft, players will instead use a collection of pre-selected cards instead of random boosters to draft from. Since 2014 player created formats are allowed as Friday Night Magic events, so long as they follow basic Magic Tournament Rules (no fake cards, no gambling etc.)

The DCI maintains a set of rules for being able to sanction tournaments, as well as runs its own circuit. Local shops often offer "Friday Night Magic" tournaments as a stepping-stone to more competitive play. The DCI runs the as a series of major tournaments to attract interest. The right to compete in a Pro Tour has to be earned by either winning a Pro Tour Qualifier Tournament or being successful in a previous tournament on a similar level. A Pro Tour is usually structured into two days of individual competition played in the Swiss format. On the final day, the top eight players compete with each other in an elimination format to select the winner.

At the end of the competition in a Pro Tour, players are awarded depending on their finishing place. If the player finishes high enough, they will also be awarded prize money. Frequent winners of these events have made names for themselves in the "Magic" community, such as Gabriel Nassif, Kai Budde and Jon Finkel. As a promotional tool, the DCI launched the in 2005 to honor selected players.

At the end of the year the is held. The World Championship functions like a Pro Tour, except that competitors have to present their skill in three different formats (usually Standard, booster draft and a second constructed format) rather than one. Another difference is that invitation to the World Championship can be gained not through Pro Tour Qualifiers, but via the national championship of a country. Most countries send their top four players of the tournament as representatives, though nations with minor "Magic" playing communities may send just one player. The World Championship also has a team-based competition, where the national teams compete with each other.

At the beginning of the World Championship, new members are inducted into the Hall of Fame. The tournament also concludes the current season of tournament play and at the end of the event, the player who earned the most Pro Points during the year is awarded the title "". The player who earned the most Pro Points and did not compete in any previous season is awarded the title "".

Invitation to a Pro Tour, Pro Points and prize money can also be earned in lesser tournaments called that are open to the general public and are held more frequently throughout the year. Grand Prix events are usually the largest "Magic" tournaments, sometimes drawing more than 2,000 players. The largest "Magic" tournament ever held was Grand Prix: Las Vegas in June 2013 with a total of 4,500 players.

"Magic: The Gathering" cards are produced in much the same way as normal playing cards. Each "Magic" card, approximately 63 × 88 mm in size (2.5 by 3.5 inches), has a face which displays the card's name and rules text as well as an illustration appropriate to the card's concept. 18,970 unique cards have been produced for the game ,
many of them with variant editions, artwork, or layouts, and 600–1000 new ones are added each year. The first "Magic" cards were printed exclusively in English, but current sets are also printed in Simplified Chinese, Traditional Chinese, French, German, Italian, Japanese, Korean, Portuguese, Russian, and Spanish.

The overwhelming majority of "Magic" cards are issued and marketed in the form of sets. For the majority of its history there were two types: the Core Set and the themed expansion sets. Under Wizards of the Coast's current production and marketing scheme, a new set is released quarterly. Various products are released with each set to appeal to different segments of the "Magic" playing community:
"Shards of Alara" also debuted mythic rares (red-orange), which replace one in eight rare cards on average. There are also premium versions of every card with holographic foil, randomly inserted into some boosters in place of a common, which replace about one in seventy cards.

As of 2018, the number of consecutive sets set on the same world varies. For example, although "Dominaria" takes place in one set, the "Guilds of Ravnica" block will take place over three sets. In addition, small sets have been removed due to developmental problems and all sets are now large. Prior to this change, sets were put into two-set blocks, starting with a large set and ending with a smaller one three months later. Prior to 2016, expansion sets were released in a three-set block (again, beginning with a larger set followed by two smaller sets). These sets consist almost exclusively of newly designed cards. Contrasting with the wide-ranging Core Set, each expansion is focused around a subset of mechanics and ties into a set storyline. Expansions also dedicate several cards to a handful of particular, often newly introduced, game mechanics.

The Core Sets began to be released annually (previously biennially) in July 2009 coinciding with the name change from to "Magic 2010". This shift also introduced new, never before printed cards into the core set, something that previously had never been done. However, core sets were discontinued following the release of "Magic Origins", on July 17, 2015, at the same time that two-set blocks were introduced. Wizards of Coast announced on June 12, 2017 that they plan on revamping and reintroducing a revamped core set, and Core Set 2019 was released on July 13, 2018.

In addition to the quarterly set releases, "Magic" cards are released in other products as well, such as the "Planechase" and "" spin-off games. These combine reprinted "Magic" cards with new, oversized cards with new functionality. "Magic" cards are also printed specifically for collectors, such as the "From the Vault" and "Premium Deck Series" sets, which contain exclusively premium foil cards.

In 2003, starting with the "" Core Set, the game went through its biggest visual change since its creation—a new card frame layout was developed to allow more rules text and larger art on the cards, while reducing the thick, colored border to a minimum. The new frame design aimed to improve contrast and readability using black type instead of the previous white, a new font, and partitioned areas for the name, card type, and power and toughness. The card frame was changed once again in Core Set 2015, which maintained the same templating, but made the card sleeker and added a holo-foil stamp to every rare and mythic card to curtail counterfeiting.

For the first few years of its production, "Magic: The Gathering" featured a small number of cards with names or artwork with demonic or occultist themes, in 1995 the company elected to remove such references from the game. In 2002, believing that the depiction of demons was becoming less controversial and that the game had established itself sufficiently, Wizards of the Coast reversed this policy and resumed printing cards with "demon" in their names.

In 2019, starting with "Throne of Eldraine", booster packs have a chance of containing an alternate art "showcase card". This is to increase the reward of buying boosters and making it more exciting.

"Magic: The Gathering" video games, comics, and books have been produced under licensing or directly by Wizards of the Coast.

In September 2011, Hasbro and IDW Publishing accorded to make a four-issue mini-series about "Magic: The Gathering" with a new story but heavily based on MTG elements and with a new Planeswalker called "Dack Fayden", the story of which mainly developed in the planes of Ravnica and Innistrad. The ongoing series started in February 2012.

In 2015 Wizards of the Coast and Hasbro published "Magic: The Gathering – Arena of the Planeswalkers". Arena of the Planeswalkers is a tactical boardgame where the players maneuver miniatures over a customizable board game, and the ruleset and terrain is based on Heroscape, but with an addition of spell cards and summoning. The original master set includes miniatures that represent the five Planeswalkers Gideon, Jace, Liliana, Chandra, and Nissa as well as select creatures from the Magic: The Gathering universe. They later released an expansion "Battle for Zendikar" featuring multi-color Planeswalkers Kiora and Ob Nixilis and a colorless Eldrazi Ruiner, and a second master set Shadows Over Innistrad which has 4 new Planeswalkers and also includes the addition of cryptoliths.

While comics and books have mostly been supplements to develop a background story for the game, several video games have been produced which lean in varying degree on the original game. For the first computer games Wizards of the Coast had sold licenses to Acclaim and MicroProse roughly at the same time. While MicroProse's "" received favorable reviews, Acclaim's "" was mostly dismissed with negative reaction.

With "" or MTGO for short, Wizards developed and released a computer version of the game themselves that allows players to compete online against other players using the original "Magic" cards and rules. Players purchase digital cards, and are able to play online against each other using their digital collections. "Magic: The Gathering Online" is the closest to paper magic of the digital alternatives.

A stripped-down version of MTGO is "" which was developed by Stainless Games and released for the Xbox 360 in June 2009. The game was ported to Windows in June of the next year. Six months after the PC release of "Duels of the Planeswalkers", the game was ported to the PlayStation 3 platform. The game was the most-played Xbox Live title for two weeks after its release. Stainless continued to release yearly updates to this, culminating in "Magic Duels", a free-to-play title released in 2015.

Hiberium and D3 Publisher licensed "Magic: the Gathering" for its mobile game, "", combining deck building with match-3-style casual gaming. This was released in December 2015 and continues to be updated with new card sets from the physical game.

Cryptic Studios and Perfect World Entertainment have announced plans to create a "Magic: The Gathering" massively multiplayer online role-playing game, to be released for personal computers and consoles.

On November 3, 2017, "", the successor to Duels of the Planeswalkers, entered its first closed stress test. On December 2017, the game entered closed beta, before entering open beta on September 27, 2018. "Arena" was used for an invitational event held on March 28–31 at PAX East 2019.

In January 2014, 20th Century Fox acquired the rights to produce a "Magic: The Gathering" film with Simon Kinberg as producer and TSG Entertainment (its co-financing partner), and Allspark Pictures as co-financers, after Universal Pictures allegedly dropped the film from their schedule (Both Universal and Hasbro had been developing the original "Magic: The Gathering" film since 2009). In 2019 following Disney's acquisition of 21st Century Fox's assets, the film along with numerous other properties in development at Fox were cancelled.

In April 2016, "Enter the Battlefield", a documentary about life on the Magic Pro Tour was released. The film was written by Greg Collins, Nathan Holt, and Shawn Kornhauser.

In June 2019, Variety reported that Joe and Anthony Russo, Wizards of the Coast, and Hasbro's Allspark Animation have teamed with Netflix for a flash animated "Magic: The Gathering" television series. In July 2019 at the San Diego Comic Con, the Russos revealed the logo of the animated series and spoke about doing an live action series.

In 1998, PGI Limited created "", which was a parody of "Magic: The Gathering". Wizards of the Coast, which owned the rights to "Magic: The Gathering", took active steps to hinder the distribution of the game and successfully shut out PGI Limited from attending GenCon in July 1998. In an attempt to avoid breaching copyright and Richard Garfield's patent, each starter deck of "Havic" had printed on the back side, "This is a Parody", and on the bottom of the rule card was printed, ""Do not have each player": construct their own library of predetermined number of game components by examining and selecting [the] game components from [a] reservoir of game components or you may infringe on U.S. Patent No. 5,662,332 to Garfield."

Three official parody expansions of "Magic" exist: "", "", and "". Most of the cards in these sets feature silver borders and humorous themes. The silver-bordered cards are not legal for play in DCI-sanctioned tournaments.

There is an active secondary market in individual cards among players and game shops. Many physical and online stores sell single cards or "playsets" of four of a card. Common cards rarely sell for more than a few cents and are usually sold in bulk. Uncommon cards and weak rare cards typically sell from 10¢ up to $1. The more expensive cards in Standard tournament play - a rotating format featuring the newest cards designed to be fairer and more accessible to newer players - are typically priced between $1 to $25. A second format, Modern, comprising an intermediate level of power and allowing most cards released since roughly 2003, has staple cards that often value between $5 and $100, with higher rarity and demand but reprints every few years intended to keep the format affordable. Foil versions of rare and mythic rare cards are typically priced at about twice as much as the regular versions. Some of the more sought-after rare and mythic rare cards can have foil versions that cost up to three or four times more than the non-foil versions.

A few of the oldest cards, due to smaller printings and limited distribution, are highly valued and rare. This is partly due to the Reserved List, a list of cards from the sets "Alpha" to "Urza's Destiny" (1994–1999) that Wizards has promised never to reprint. Legacy-only cards on the Reserved List, which are barred from reprint under a voluntary but genuine legal obligation, are in short supply due to smaller print runs of the game in its oldest days, and may be worth $200 to $1,000 or higher. And certain Vintage cards - the oldest cards in Magic, with most on the Reserved List, such as the so-called "Power Nine" - can easily cost more than $1,000 apiece. 
The most expensive card that was in regular print (as opposed to being a promotional or special printing) is the "Black Lotus", copies of which are worth thousands of dollars at minimum. In 2019, a "Pristine 9.5 grade" Beckett Grading Services graded Alpha Black Lotus was bought by an anonymous buyer, for a record $166,100.

The secondary market started with comic book stores, and hobby shops displaying and selling cards, with the cards' values determined somewhat arbitrarily by the employees of the store. With the expansion of the internet, prices of cards were determined by the number of tournament deck lists a given card would appear in. If a card was played in a tournament more frequently, the cost of the card would be higher (in addition to the market availability of the card). When eBay, Amazon, and other large online markets started to gain popularity, the "Magic" secondary market evolved substantially. Buying and selling "Magic" cards online became a source of income for people who learned how to manipulate the market. Today, the secondary market is so large and complex, it has become an area of study for consumer research, and some people make a career out of market manipulation, creating mathematical models to analyze the growth of cards' worth, and predict the market value of both individual cards, and entire sets of cards. This is called .

As of late 2013, Wizards of the Coast has expressed concern over the increasing number of counterfeit cards in the secondary market. Wizards of the Coast has since made an effort to counteract the rise of counterfeits by introducing a new holofoil stamp on all rare and mythic rare cards as of Magic 2015.

Each card has an illustration to represent the flavor of the card, often reflecting the setting of the expansion for which it was designed. Much of "Magic"<nowiki>'</nowiki>s early artwork was commissioned with little specific direction or concern for visual cohesion. One infamous example was the printing of the creature Whippoorwill without the "flying" ability even though its art showed a bird in flight. The art direction team later decided to impose a few constraints so that the artistic vision more closely aligned with the design and development of the cards. Each block of cards now has its own style guide with sketches and descriptions of the various races and places featured in the setting.

A few early sets experimented with alternate art for cards. However, Wizards came to believe that this impeded easy recognition of a card and that having multiple versions caused confusion when identifying a card at a glance. Consequently, alternate art is now only used sparingly and mostly for promotional cards. When older cards are reprinted in new sets, however, Wizards of the Coast usually prints them with new art to make the older cards more collectible, though they sometimes reuse well-received artwork if it makes sense thematically.

As "Magic" has expanded across the globe, its artwork has had to change for its international audience. Artwork has been edited or given alternate art to comply with the governmental standards. For example, the portrayal of skeletons and most undead in artwork was prohibited by the Chinese government until 2008.

The way "Magic" storylines are conceived and deployed has changed considerably over the years. The main premise of "Magic" is that countless possible worlds (planes) exist in the , and only unique and rare beings called Planeswalkers are capable of traversing the Multiverse. This allows the game to frequently change worlds so as to renew its mechanical inspiration, while maintaining planeswalkers as recurrent, common elements across worlds. An intricate storyline underlies the cards released in each expansion and is shown in the art and flavor text of the cards, as well as in novels and anthologies published by Wizards of the Coast (and formerly by Harper Prism). Important storyline characters, objects and locations often appear as cards in "Magic" sets, usually as "Legendary" creatures, artifacts, and lands, or as "Planeswalker" cards.

The original "Magic: The Gathering Limited Edition" has no overarching storyline, and the cards only have unconnected bits of lore and trivia to give the cards some individual depth. In the early expansion sets until "" there is usually no real story arc either. Instead, some of these sets are inspired from mythologies of various cultures. This is most apparent in , that takes some of the One Thousand and One Nights characters and makes them into "Magic" cards. Norse mythological influences can be seen worked into and African influences into . However, not all of the early sets can be linked as directly to Earth mythology.

Beginning with the "Weatherlight" expansion there was a shift in the way "Magic" storylines were used. For the blocks "Weatherlight" through "", the story was laid out in a character driven story, following the events of the Weatherlight ship and its crew. With help of the planeswalking capabilities of the Weatherlight, the protagonists travel through the multiverse to fight Yawgmoth and his army of Phyrexians.

"" through "" are an unconnected storyline set 100 years later on Dominaria where multiple factions battle for control of the Mirari, a powerful magical artifact left by Karn.

After "", "Magic" storylines have mostly panned away from Dominaria. New planes were created to set the scene for new storylines. In contrast to the previous character driven stories, these releases focused on thematic worlds. This was the model from through , a world split into five magically and culturally distinct "shards" but later reunited. During this block of time, "Time Spiral" block was released, in which several Dominarian planeswalkers attempted to stop the time rifts that threatened to destroy Dominaria. This block contains the Multiverse-wide event known as the Mending, which powered down the current, godlike planeswalkers to mere mortals that happened to be able to travel to other planes. The event also set up the introduction of the Planeswalker type in Lorwyn block.

After Alara, "Magic" visited , a world used as a prison to entrap a race of interplanar parasitic monsters called the Eldrazi, which were inspired by H. P. Lovecraft's Old Ones. Beginning with "Zendikar" the world-centric storytelling was complemented by an overlying story layer. Planeswalker cards had been introduced in "Lorwyn" and these Planeswalker characters were used to give the overarching storyline a sense of continuity, despite the constant change of setting. The block following Zendikar, "Scars of Mirrodin", revisited the plane of Mirrodin, where the Mirran natives battled against an invading Phyrexian corruption unwittilingly left by Karn (again interconnecting various storylines). To further integrate the storyline into the gameplay, certain events for the second set, "Mirrodin Besieged", encouraged players to affiliate themselves with either the Mirran or Phyrexian faction. Much of the recent focus has been on both integrating the play experience with the story line and on making mechanics and individual cards which represent pivotal points in the story.

On "Innistrad", a plane inspired heavily by gothic horror, its guardian angel has gone missing. Darkness has started to consume the plane, and the players discover that the Helvault, a magical prison, has been holding the archangel Avacyn as well as demons. Thalia, a cathar of the Church of Avacyn, broke open the Helvault and released Avacyn as well as all of the demons. In the "Return to Ravnica" block, players were encouraged to affiliate themselves with a guild and take control of the city of Ravnica by completing the maze discovered by Niv-Mizzet.

"Theros" was a plane inspired by Greek mythology, containing many references to Greek mythological figures such as Prometheus and the pantheon of gods.

"Tarkir" would have been a plane where dragons had long since died, controlled by five clans ruled by khans. Through time travel, the result of the struggle between the ancient clans and the dragons was reversed and the dragons now reign over each of the five clans, which are both similar and different to their alternate-timeline predecessors.

"Battle for Zendikar" was a return to the plane of Zendikar, which had been ravaged by the Eldrazi horrors. This marks a change in "Magic"s storytelling, where each block's story is shown from the perspective of a group of planeswalkers called the Gatewatch.

"Shadows Over Innistrad" was a return to Innistrad, where Avacyn has been corrupted. The next set, "Eldritch Moon", focuses on the fact that Emrakul, the most powerful Eldrazi titan that had been missing from the "Battle for Zendikar" storyline, is now on Innistrad. Together, the Gatewatch must find a way to save the plane from Emrakul's influence. This story also focuses on cosmic horror instead of the traditional gothic horror of old Innistrad.

"Kaladesh" had the Gatewatch go to Chandra Nalaar's home world, the titular plane of Kaladesh, where she finds her mother (presumed dead) and almost kills Tezzeret. Tezzeret later kidnaps Rashmi, winner of the famous Inventor's Fair, and begins a dastardly plot to control the ruling Consulate. With the Consulate imprisoning inventors and confiscating their devices following the Fair, tensions between the populace and the government reach a boiling point, as depicted in "Aether Revolt". The block focuses on a Steampunk aesthetic, with the steam replaced by aether, a powerful material that works in nearly every part of life.

"Amonkhet" had the Gatewatch set out to destroy the evil dragon planeswalker Nicol Bolas after learning of his dominion over the titular desert plane Amonkhet. In the desert, they find a city (Naktamun) teeming with food, water, and life, ruled by five gods, with Nicol Bolas seemingly absent altogether. The people of Naktamun train their entire lives to die in ritual combat, hoping to experience pure bliss in the afterlife when the God-Pharaoh (Bolas) returns to Amonkhet. In "Hour of Devastation", Bolas returns as prophesied, only to raze Naktamun and reveal the true purpose of the training and combat: to create an army of physically-perfect and combat-adept mummies that serve as an unquestionably loyal army. Although the Gatewatch attempt to defeat Bolas, they are utterly defeated and are forced to flee from Amonkhet. The block's setting is based on ancient Egypt, with themes of social hierarchy and the contrast between life and death.

"Ixalan" centered around the quest for the Golden City of Orazca and the artifact known as the Immortal Sun that lies within it. The tribes vying for the Immortal Sun include: the Sun Empire, an army of dinosaur-riding warriors; the River Heralds, a group of merfolk shamans; the Legion of Dusk, a coalition of vampire conquistadors; and the Brazen Coalition, a fleet of seafaring pirates. The story also follows Jace Beleren, who is stranded on Ixalan without any of his memories after the events of "Hour of Devastation", and Vraska, a gorgon agent of Nicol Bolas posing as a pirate on the high seas. In "Rivals of Ixalan", the quest for the Immortal Sun reaches its climax as all four tribes attempt to seize the Golden City and as Jace and Vraska attempt to defeat the devious sphinx Azor, the founder of the Azorius guild on Ravnica and the creator of the Immortal Sun.

"Dominaria" was a return to the plane of the same name, which has not been seen in over a decade. The story starts right after the events of "Hour of Devastation," and involves Liliana, Chandra, and Gideon on their mission to kill the Demonlord Belzenlok, the final demon that Liliana made a pact with to secure her youth and power. The story arc culminates on Ravnica with "Guilds of Ravnica", "Ravnica Allegiance", with the guilds in disorder and chaos and Bolas manipulating them in preparation for an invasion and finale in "War of the Spark".

There are several examples of academic, peer-reviewed research concerning different aspects of "Magic: The Gathering". One study examined how players use their imaginations when playing. This research studied hobby players and showed how players sought to create and participate in an epic fantasy narrative. Another example used online auctions for "Magic" cards to test revenue outcomes for various auction types. A third example uses probability to examine "Magic" card-collecting strategies. Using a specific set of cards in a specialized manner has shown "Magic: The Gathering" to be Turing complete.




</doc>
<doc id="18831" url="https://en.wikipedia.org/wiki?curid=18831" title="Mathematics">
Mathematics

Mathematics (from Greek μάθημα "máthēma", "knowledge, study, learning") includes the study of such topics as quantity (number theory), structure (algebra), space (geometry), and change (mathematical analysis). It has no generally accepted definition.

Mathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, mathematical reasoning can be used to provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.

Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's "Elements". Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.

Mathematics is essential in many fields, including natural science, engineering, medicine, finance, and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics (mathematics for its own sake) without having any application in mind, but practical applications for what began as pure mathematics are often discovered later.

The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.

As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.

Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system which is still in use today for measuring angles and time.

Beginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics. Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook "Elements" is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea (2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).

The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.
During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.

During the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system — if powerful enough to describe arithmetic — will contain true propositions that cannot be proved.

Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the "Bulletin of the American Mathematical Society", "The number of papers and books included in the "Mathematical Reviews" database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."

The word "mathematics" comes from Ancient Greek μάθημα ("máthēma"), meaning "that which is learnt", "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times. Its adjective is ("mathēmatikós"), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, ("mathēmatikḗ tékhnē"), , meant "the mathematical art".

Similarly, one of the two main schools of thought in Pythagoreanism was known as the "mathēmatikoi" (μαθηματικοί)—which at the time meant "learners" rather than "mathematicians" in the modern sense.

In Latin, and in English until around 1700, the term "mathematics" more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of "mathematici", meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.

The apparent plural form in English, like the French plural form (and the less commonly used singular derivative ), goes back to the Latin neuter plural (Cicero), based on the Greek plural ("ta mathēmatiká"), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical", although it is plausible that English borrowed only the adjective "mathematic(al)" and formed the noun "mathematics" anew, after the pattern of "physics" and "metaphysics", which were inherited from Greek. In English, the noun "mathematics" takes a singular verb. It is often shortened to "maths" or, in North America, "math".
Mathematics has no generally accepted definition. Aristotle defined mathematics as "the science of quantity" and this definition prevailed until the 18th century. In the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Three leading types of definition of mathematics today are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe flaws, none has widespread acceptance, and no reconciliation seems possible.

An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870). In the "Principia Mathematica", Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).

Intuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other." A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct. Intuitionists also reject the law of excluded middle — a stance which forces them to reject proof by contradiction as a viable proof method as well.

Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems". A formal system is a set of symbols, or "tokens", and some "rules" on how the tokens are to be combined into "formulas". In formal systems, the word "axiom" has a special meaning different from the ordinary meaning of "a self-evident truth", and is used to refer to a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.

A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, "Mathematics is what mathematicians do."

The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences". More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery". In the original Latin "Regina Scientiarum", as well as in German "Königin der Wissenschaften", the word corresponding to "science" means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to "natural science" follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation from the external world is arguably negligible in mathematics, especially when compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."

Some modern philosophers consider that mathematics is not a science. The philosopher Karl Popper observed that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."

Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.

The opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is "created" (as in art) or "discovered" (as in science). It is common to see universities divided into sections that include a division of "Science and Mathematics", indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.

Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.

Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.

For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the "elegance" of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in "A Mathematician's Apology" expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematical research often seeks critical features of a mathematical object. A theorem expressed as a characterization of the object by these features is the prize. Examples of particularly succinct and revelatory mathematical arguments has been published in "Proofs from THE BOOK".

The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. And at the other social extreme, philosophers continue to find problems in philosophy of mathematics, such as the nature of mathematical proof.

Most of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more "abstract" and more "encrypted" than those of natural language. Unlike natural language, where people can often equate a word (such as "cow") with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog. Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.

Mathematical language can be difficult to understand for beginners because even common terms, such as "or" and "only", have a more precise meaning than they have in everyday speech, and other terms such as "open" and "field" refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as "homeomorphism" and "integrable" that have no meaning outside of mathematics. Additionally, shorthand phrases such as "iff" for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".

Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may be erroneous if the used computer program is erroneous. On the other hand, proof assistants allow verifying all details that cannot be given in a hand-written proof, and provide certainty of the correctness of long proofs such as that of Feit–Thompson theorem.

Axioms in traditional thought were "self-evident truths", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.

Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.

Discrete mathematics conventionally groups together the fields of mathematics which study mathematical structures that are fundamentally discrete rather than continuous.

In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.

Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if "sound" (meaning that all theorems that can be proved are true), is necessarily "incomplete" (meaning that there are true theorems which cannot be proved "in that system"). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.

Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.

The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.

As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.

Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.

By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.

The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.

Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.

Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, "applied mathematics" focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.

In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.

Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.

Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.

Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretisation broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.

Arguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.

The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.

A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward.




</doc>
<doc id="18835" url="https://en.wikipedia.org/wiki?curid=18835" title="Manhattan (disambiguation)">
Manhattan (disambiguation)

Manhattan is a borough of New York City.

Manhattan may also refer to:











</doc>
<doc id="18836" url="https://en.wikipedia.org/wiki?curid=18836" title="Middle Ages">
Middle Ages

In the history of Europe, the Middle Ages (or medieval period) lasted from the 5th to the 15th century. It began with the fall of the Western Roman Empire and merged into the Renaissance and the Age of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history: classical antiquity, the medieval period, and the modern period. The medieval period is itself subdivided into the Early, High, and Late Middle Ages.

Population decline, counterurbanisation, collapse of centralized authority, invasions, and mass migrations of tribes, which had begun in Late Antiquity, continued in the Early Middle Ages. The large-scale movements of the Migration Period, including various Germanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the 7th century, North Africa and the Middle East—once part of the Byzantine Empire—came under the rule of the Umayyad Caliphate, an Islamic empire, after conquest by Muhammad's successors. Although there were substantial changes in society and political structures, the break with classical antiquity was not complete. The still-sizeable Byzantine Empire, Rome's direct continuation, survived in the Eastern Mediterranean and remained a major power. The empire's law code, the "Corpus Juris Civilis" or "Code of Justinian", was rediscovered in Northern Italy in 1070 and became widely admired later in the Middle Ages. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns to Christianise pagan Europe continued. The Franks, under the Carolingian dynasty, briefly established the Carolingian Empire during the later 8th and early 9th centuries. It covered much of Western Europe but later succumbed to the pressures of internal civil wars combined with external invasions: Vikings from the north, Magyars from the east, and Saracens from the south.

During the High Middle Ages, which began after 1000, the population of Europe increased greatly as technological and agricultural innovations allowed trade to flourish and the Medieval Warm Period climate change allowed crop yields to increase. Manorialism, the organisation of peasants into villages that owed rent and labour services to the nobles, and feudalism, the political structure whereby knights and lower-status nobles owed military service to their overlords in return for the right to rent from lands and manors, were two of the ways society was organised in the High Middle Ages. The Crusades, first preached in 1095, were military attempts by Western European Christians to regain control of the Holy Land from Muslims. Kings became the heads of centralised nation-states, reducing crime and violence but making the ideal of a unified Christendom more distant. Intellectual life was marked by scholasticism, a philosophy that emphasised joining faith to reason, and by the founding of universities. The theology of Thomas Aquinas, the paintings of Giotto, the poetry of Dante and Chaucer, the travels of Marco Polo, and the Gothic architecture of cathedrals such as Chartres are among the outstanding achievements toward the end of this period and into the Late Middle Ages.

The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and the Western Schism within the Catholic Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.

The Middle Ages is one of the three major periods in the most enduring scheme for analysing European history: classical civilisation, or Antiquity; the Middle Ages; and the Modern Period. The "Middle Ages" first appears in Latin in 1469 as "media tempestas" or "middle season". In early usage, there were many variants, including "medium aevum", or "middle age", first recorded in 1604, and "media saecula", or "middle centuries", first recorded in 1625. The adjective "medieval" (or sometimes "mediaeval" or "mediæval"), meaning pertaining to the Middle Ages, derives from "medium aevum".

Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires", and considered their time to be the last before the end of the world. When referring to their own times, they spoke of them as being "modern". In the 1330s, the humanist and poet Petrarch referred to pre-Christian times as "antiqua" (or "ancient") and to the Christian period as "nova" (or "new"). Leonardo Bruni was the first historian to use tripartite periodisation in his "History of the Florentine People" (1442), with a middle period "between the fall of the Roman Empire and the revival of city life sometime in late eleventh and twelfth centuries". Tripartite periodisation became standard after the 17th-century German historian Christoph Cellarius divided history into three periods: ancient, medieval, and modern.

The most commonly given starting point for the Middle Ages is around 500, with the date of 476 first used by Bruni. Later starting dates are sometimes used in the outer parts of Europe. For Europe as a whole, 1500 is often considered to be the end of the Middle Ages, but there is no universally agreed upon end date. Depending on the context, events such as the conquest of Constantinople by the Turks in 1453, Christopher Columbus's first voyage to the Americas in 1492, or the Protestant Reformation in 1517 are sometimes used. English historians often use the Battle of Bosworth Field in 1485 to mark the end of the period. For Spain, dates commonly used are the death of King Ferdinand II in 1516, the death of Queen Isabella I of Castile in 1504, or the conquest of Granada in 1492.

Historians from Romance-speaking countries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late". In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages", but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.

The Roman Empire reached its greatest territorial extent during the 2nd century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories. Economic issues, including inflation, and external pressure on the frontiers combined to create the Crisis of the Third Century, with emperors coming to the throne only to be rapidly replaced by new usurpers. Military expenses increased steadily during the 3rd century, mainly in response to the war with the Sasanian Empire, which revived in the middle of the 3rd century. The army doubled in size, and cavalry and smaller units replaced the Roman legion as the main tactical unit. The need for revenue led to increased taxes and a decline in numbers of the curial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns. More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.

The Emperor Diocletian (r. 284–305) split the empire into separately administered eastern and western halves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrative promulgations in one division were considered valid in the other. In 330, after a period of civil war, Constantine the Great (r. 306–337) refounded the city of Byzantium as the newly renamed eastern capital, Constantinople. Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others. Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowing invaders to encroach. For much of the 4th century, Roman society stabilised in a new form that differed from the earlier classical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns. Another change was the Christianisation, or conversion of the empire to Christianity, a gradual process that lasted from the 2nd to the 5th centuries.

In 376, the Goths, fleeing from the Huns, received permission from Emperor Valens (r. 364–378) to settle in the Roman province of Thracia in the Balkans. The settlement did not go smoothly, and when Roman officials mishandled the situation, the Goths began to raid and plunder. Valens, attempting to put down the disorder, was killed fighting the Goths at the Battle of Adrianople on 9 August 378. As well as the threat from such tribal confederacies from the north, internal divisions within the empire, especially within the Christian Church, caused problems. In 400, the Visigoths invaded the Western Roman Empire and, although briefly forced back from Italy, in 410 sacked the city of Rome. In 406 the Alans, Vandals, and Suevi crossed into Gaul; over the next three years they spread across Gaul and in 409 crossed the Pyrenees Mountains into modern-day Spain. The Migration Period began, when various peoples, initially largely Germanic peoples, moved across Europe. The Franks, Alemanni, and the Burgundians all ended up in northern Gaul while the Angles, Saxons, and Jutes settled in Britain, and the Vandals went on to cross the strait of Gibraltar after which they conquered the province of Africa. In the 430s the Huns began invading the empire; their king Attila (r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452. The Hunnic threat remained until Attila's death in 453, when the Hunnic confederation he led fell apart. These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.

By the end of the 5th century the western section of the empire was divided into smaller political units, ruled by the tribes that had invaded in the early part of the century. The deposition of the last emperor of the west, Romulus Augustulus, in 476 has traditionally marked the end of the Western Roman Empire. By 493 the Italian peninsula was conquered by the Ostrogoths. The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. The Byzantine emperors maintained a claim over the territory, but while none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Mediterranean periphery and the Italian Peninsula (Gothic War) in the reign of Justinian (r. 527–565) was the sole, and temporary, exception.

The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the Western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration. The emperors of the 5th century were often controlled by military strongmen such as Stilicho (d. 408), Aetius (d. 454), Aspar (d. 471), Ricimer (d. 472), or Gundobad (d. 516), who were partly or fully of non-Roman background. When the line of Western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common. This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state. Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects. Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions. An important difference was the gradual loss of tax revenue by the new polities. Many of the new political entities no longer supported their armies through taxes, instead relying on granting them land or rents. This meant there was less need for large tax revenues and so the taxation systems decayed. Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.
Between the 5th and 8th centuries, new peoples and individuals filled the political void left by Roman centralised government. The Ostrogoths, a Gothic tribe, settled in Roman Italy in the late fifth century under Theoderic the Great (d. 526) and set up a kingdom marked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign. The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436 formed a new kingdom in the 440s. Between today's Geneva and Lyon, it grew to become the realm of Burgundy in the late 5th and early 6th centuries. Elsewhere in Gaul, the Franks and Celtic Britons set up small polities. Francia was centred in northern Gaul, and the first king of whom much is known is Childeric I (d. 481). His grave was discovered in 1653 and is remarkable for its grave goods, which included weapons and a large quantity of gold.

Under Childeric's son Clovis I (r. 509–511), the founder of the Merovingian dynasty, the Frankish kingdom expanded and converted to Christianity. The Britons, related to the natives of Britannia – modern-day Great Britain – settled in what is now Brittany. Other monarchies were established by the Visigothic Kingdom in the Iberian Peninsula, the Suebi in northwestern Iberia, and the Vandal Kingdom in North Africa. In the sixth century, the Lombards settled in Northern Italy, replacing the Ostrogothic kingdom with a grouping of duchies that occasionally selected a king to rule over them all. By the late sixth century, this arrangement had been replaced by a permanent monarchy, the Kingdom of the Lombards.

The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul for instance, the invaders settled much more extensively in the north-east than in the south-west. Slavs settled in Central and Eastern Europe and the Balkan Peninsula. The settlement of peoples was accompanied by changes in languages. Latin, the literary language of the Western Roman Empire, was gradually replaced by vernacular languages which evolved from Latin, but were distinct from it, collectively known as Romance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs added Slavic languages to Eastern Europe.

As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with the Sasanian Empire, the traditional enemy of Rome, lasted throughout most of the 5th century. The Eastern Empire was marked by closer relations between the political state and Christian Church, with doctrinal matters assuming an importance in Eastern politics that they did not have in Western Europe. Legal developments included the codification of Roman law; the first effort—the "Codex Theodosianus"—was completed in 438. Under Emperor Justinian (r. 527–565), another compilation took place—the "Corpus Juris Civilis". Justinian also oversaw the construction of the Hagia Sophia in Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths, under Belisarius (d. 565). The conquest of Italy was not complete, as a deadly outbreak of plague in 542 led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests.

At the Emperor's death, the Byzantines had control of most of Italy, North Africa, and a small foothold in southern Spain. Justinian's reconquests have been criticised by historians for overextending his realm and setting the stage for the early Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.

In the Eastern Empire the slow infiltration of the Balkans by the Slavs added a further difficulty for Justinian's successors. It began gradually, but by the late 540s Slavic tribes were in Thrace and Illyrium, and had defeated an imperial army near Adrianople in 551. In the 560s the Avars began to expand from their base on the north bank of the Danube; by the end of the 6th-century, they were the dominant power in Central Europe and routinely able to force the Eastern emperors to pay tribute. They remained a strong power until 796.

An additional problem to face the empire came as a result of the involvement of Emperor Maurice (r. 582–602) in Persian politics when he intervened in a succession dispute. This led to a period of peace, but when Maurice was overthrown, the Persians invaded and during the reign of Emperor Heraclius (r. 610–641) controlled large chunks of the empire, including Egypt, Syria, and Anatolia until Heraclius' successful counterattack. In 628 the empire secured a peace treaty and recovered all of its lost territories.

In Western Europe, some of the older Roman elite families died out while others became more involved with ecclesiastical than secular affairs. Values attached to Latin scholarship and education mostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century, Jerome (d. 420) dreamed that God rebuked him for spending more time reading Cicero than the Bible. By the 6th century, Gregory of Tours (d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learning shorthand. By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book. Most intellectual efforts went towards imitating classical scholarship, but some original works were created, along with now-lost oral compositions. The writings of Sidonius Apollinaris (d. 489), Cassiodorus (d. c. 585), and Boethius (d. c. 525) were typical of the age.

Changes also took place among laymen, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported entourages of fighters who formed the backbone of the military forces. Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of the feud in aristocratic society, examples of which included those related by Gregory of Tours that took place in Merovingian Gaul. Most feuds seem to have ended quickly with the payment of some sort of compensation. Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. In Anglo-Saxon society the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played by abbesses of monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.

Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes from archaeology; few detailed written records documenting peasant life remain from before the 9th century. Most of the descriptions of the lower classes come from either law codes or writers from the upper classes. Landholding patterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having a great deal of autonomy. Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and still others lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more of those systems. Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and it was possible for a free peasant's family to rise into the aristocracy over several generations through military service to a powerful lord.

Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. Rome, for instance, shrank from a population of hundreds of thousands to around 30,000 by the end of the 6th century. Roman temples were converted into Christian churches and city walls remained in use. In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals. Although there had been Jewish communities in many Roman cities, the Jews suffered periods of persecution after the conversion of the empire to Christianity. Officially they were tolerated, if subject to conversion efforts, and at times were even encouraged to settle in new areas.

Religious beliefs in the Eastern Empire and Iran were in flux during the late sixth and early seventh centuries. Judaism was an active proselytising faith, and at least one Arab political leader converted to it. Christianity had active missions competing with the Persians' Zoroastrianism in seeking converts, especially among residents of the Arabian Peninsula. All these strands came together with the emergence of Islam in Arabia during the lifetime of Muhammad (d. 632). After his death, Islamic forces conquered much of the Eastern Empire and Persia, starting with Syria in 634–635 and reaching Egypt in 640–641, Persia between 637 and 642, North Africa in the later seventh century, and the Iberian Peninsula in 711. By 714, Islamic forces controlled much of the peninsula in a region they called Al-Andalus.

The Islamic conquests reached their peak in the mid-eighth century. The defeat of Muslim forces at the Battle of Tours in 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of the Umayyad Caliphate and its replacement by the Abbasid Caliphate. The Abbasids moved their capital to Baghdad and were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, the Aghlabids controlled North Africa, and the Tulunids became rulers of Egypt. By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the old Roman economy. Franks traded timber, furs, swords and slaves in return for silks and other fabrics, spices, and precious metals from the Arabs.

The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and by the 7th century found only in a few cities such as Rome or Naples. By the end of the 7th century, under the impact of the Muslim conquests, African products were no longer found in Western Europe. The replacement of goods from long-range trade with local products was a trend throughout the old Roman lands that happened in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In the northern parts of Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.

The various Germanic states in the west all had coinages that imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century in 693-94 when it was replaced by silver in the Merovingian kindgon. The basic Frankish silver coin was the denarius or denier, while the Anglo-Saxon version was called a penny. From these areas, the denier or penny spread throughout Europe during the centuries from 700 to 1000. Copper or bronze coins were not struck, nor were gold except in Southern Europe. No silver coins denominated in multiple units were minted.

Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly the Byzantine Church differed in language, practices, and liturgy from the Western Church. The Eastern Church used Greek instead of the Western Latin. Theological and political differences emerged, and by the early and middle 8th century issues such as iconoclasm, clerical marriage, and state control of the Church had widened to the extent that the cultural and religious differences were greater than the similarities. The formal break, known as the East–West Schism, came in 1054, when the papacy and the patriarchy of Constantinople clashed over papal supremacy and excommunicated each other, which led to the division of Christianity into two Churches—the Western branch became the Roman Catholic Church and the Eastern branch the Eastern Orthodox Church.

The ecclesiastical structure of the Roman Empire survived the movements and invasions in the west mostly intact, but the papacy was little regarded, and few of the Western bishops looked to the bishop of Rome for religious or political leadership. Many of the popes prior to 750 were more concerned with Byzantine affairs and Eastern theological controversies. The register, or archived copies of the letters, of Pope Gregory the Great (pope 590–604) survived, and of those more than 850 letters, the vast majority were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent the Gregorian mission in 597 to convert the Anglo-Saxons to Christianity. Irish missionaries were most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under such monks as Columba (d. 597) and Columbanus (d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.

The Early Middle Ages witnessed the rise of monasticism in the West. The shape of European monasticism was determined by traditions and ideas that originated with the Desert Fathers of Egypt and Syria. Most European monasteries were of the type that focuses on community experience of the spiritual life, called cenobitism, which was pioneered by Pachomius (d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries through hagiographical literature such as the "Life of Anthony". Benedict of Nursia (d. 547) wrote the Benedictine Rule for Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by an abbot. Monks and monasteries had a deep effect on the religious and political life of the Early Middle Ages, in various cases acting as land trusts for powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation. They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latin classics were copied in monasteries in the Early Middle Ages. Monks were also the authors of new works, including history, theology, and other subjects, written by authors such as Bede (d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.

The Frankish kingdom in northern Gaul split into kingdoms called Austrasia, Neustria, and Burgundy during the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria. Such warfare was exploited by Pippin (d. 640), the Mayor of the Palace for Austrasia who became the power behind the Austrasian throne. Later members of his family inherited the office, acting as advisers and regents. One of his descendants, Charles Martel (d. 741), won the Battle of Poitiers in 732, halting the advance of Muslim armies across the Pyrenees. Great Britain was divided into small states dominated by the kingdoms of Northumbria, Mercia, Wessex, and East Anglia, which were descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons and Picts. Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as 150 local kings in Ireland, of varying importance.

The Carolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led by (r. 752–768). A contemporary chronicle claims that Pippin sought, and gained, authority for this coup from Pope (pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) and Carloman (r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great or Charlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, and Saxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land. In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of the Papal States.

The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire, since the new emperor ruled over much of the area previously controlled by the Western emperors. It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state. There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural in character, with only a few small cities. Most of the people were peasants settled on small farms. Little trade existed and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean. The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials called counts, who administered the counties the empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials called "missi dominici", who served as roving inspectors and troubleshooters.

Charlemagne's court in Aachen was the centre of the cultural revival sometimes referred to as the "Carolingian Renaissance". Literacy increased, as did development in the arts, architecture and jurisprudence, as well as liturgical and scriptural studies. The English monk Alcuin (d. 804) was invited to Aachen and brought the education available in the monasteries of Northumbria. Charlemagne's chancery—or writing office—made use of a new script today known as Carolingian minuscule, allowing a common writing style that advanced communication across much of Europe. Charlemagne sponsored changes in church liturgy, imposing the Roman form of church service on his domains, as well as the Gregorian chant in liturgical music for the churches. An important activity for scholars during this period was the copying, correcting, and dissemination of basic works on religious and secular topics, with the aim of encouraging learning. New works on religious topics and schoolbooks were also produced. Grammarians of the period modified the Latin language, changing it from the Classical Latin of the Roman Empire into a more flexible form to fit the needs of the Church and government. By the reign of Charlemagne, the language had so diverged from the classical that it was later called Medieval Latin.

Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs, but was unable to do so as only one son, Louis the Pious (r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Louis's reign of 26 years was marked by numerous divisions of the empire among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest son (d. 855) as emperor and gave him Italy. Louis divided the rest of the empire between Lothair and Charles the Bald (d. 877), his youngest son. Lothair took East Francia, comprising both banks of the Rhine and eastwards, leaving Charles West Francia with the empire to the west of the Rhineland and the Alps. Louis the German (d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under the suzerainty of his elder brother. The division was disputed. of Aquitaine (d. after 864), the emperor's grandson, rebelled in a contest for Aquitaine, while Louis the German tried to annex all of East Francia. Louis the Pious died in 840, with the empire still in chaos.

A three-year civil war followed his death. By the Treaty of Verdun (843), a kingdom between the Rhine and Rhone rivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German was in control of Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France. Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost. In 987 the Carolingian dynasty was replaced in the western lands, with the crowning of Hugh Capet (r. 987–996) as king. In the eastern lands the dynasty had died out earlier, in 911, with the death of Louis the Child, and the selection of the unrelated Conrad I (r. 911–918) as king.

The breakup of the Carolingian Empire was accompanied by invasions, migrations, and raids by external foes. The Atlantic and northern shores were harassed by the Vikings, who also raided the British Isles and settled there as well as in Iceland. In 911, the Viking chieftain Rollo (d. c. 931) received permission from the Frankish King Charles the Simple (r. 898–922) to settle in what became Normandy. The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continual Magyar assault until the invader's defeat at the Battle of Lechfeld in 955. The breakup of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.

Efforts by local kings to fight the invaders led to the formation of new political entities. In Anglo-Saxon England, King Alfred the Great (r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting in Danish settlements in Northumbria, Mercia, and parts of East Anglia. By the middle of the 10th century, Alfred's successors had conquered Northumbria, and restored English control over most of the southern part of Great Britain. In northern Britain, Kenneth MacAlpin (d. c. 860) united the Picts and the Scots into the Kingdom of Alba. In the early 10th century, the Ottonian dynasty had established itself in Germany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 of (r. 936–973) as Holy Roman Emperor. In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his son Otto II (r. 967–983) to Theophanu (d. 991), daughter of an earlier Byzantine Emperor Romanos II (r. 959–963). By the late 10th century Italy had been drawn into the Ottonian sphere after a period of instability; Otto III (r. 996–1002) spent much of his later reign in the kingdom. The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.

Missionary efforts to Scandinavia during the 9th and 10th centuries helped strengthen the growth of kingdoms such as Sweden, Denmark, and Norway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what became Russia and in Iceland. Swedish traders and raiders ranged down the rivers of the Russian steppe, and even attempted to seize Constantinople in 860 and 907. Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms of Asturias and León.

In Eastern Europe, Byzantium revived its fortunes under Emperor Basil I (r. 867–886) and his successors Leo VI (r. 886–912) and Constantine VII (r. 913–959), members of the Macedonian dynasty. Commerce revived and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperors John I (r. 969–976) and Basil II (r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as the Macedonian Renaissance. Writers such as John Geometres (fl. early 10th century) composed new hymns, poems, and other works. Missionary efforts by both Eastern and Western clergy resulted in the conversion of the Moravians, Bulgars, Bohemians, Poles, Magyars, and Slavic inhabitants of the Kievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states of Moravia, Bulgaria, Bohemia, Poland, Hungary, and the Kievan Rus'. Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea. By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.

Few large stone buildings were constructed between the Constantinian basilicas of the 4th century and the 8th century, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture. One feature of the basilica is the use of a transept, or the "arms" of a cross-shaped building that are perpendicular to the long nave. Other new features of religious architecture include the crossing tower and a monumental entrance to the church, usually at the west end of the building.

Carolingian art was produced for a small group of figures around the court, and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman and Byzantine art, but was also influenced by the Insular art of the British Isles. Insular art integrated the energy of Irish Celtic and Anglo-Saxon Germanic styles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostly illuminated manuscripts and carved ivories, originally made for metalwork that has since been melted down. Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as the Cross of Lothair, several reliquaries, and finds such as the Anglo-Saxon burial at Sutton Hoo and the hoards of Gourdon from Merovingian France, Guarrazar from Visigothic Spain and Nagyszentmiklós near Byzantine territory. There are survivals from the large brooches in fibula or penannular form that were a key piece of personal adornment for elites, including the Irish Tara Brooch. Highly decorated books were mostly Gospel Books and these have survived in larger numbers, including the Insular Book of Kells, the Book of Lindisfarne, and the imperial Codex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels. Charlemagne's court seems to have been responsible for the acceptance of figurative monumental sculpture in Christian art, and by the end of the period near life-sized figures such as the Gero Cross were common in important churches.

During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force as well as the continued development of highly specialised types of troops. The creation of heavily armoured cataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphases on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths, who had a high proportion of cavalry in their armies. During the early invasion period, the stirrup had not been introduced into warfare, which limited the usefulness of cavalry as shock troops because it was not possible to put the full force of the horse and rider behind blows struck by the rider. The greatest change in military affairs during the invasion period was the adoption of the Hunnic composite bow in place of the earlier, and weaker, Scythian composite bow. Another development was the increasing use of longswords and the progressive replacement of scale armour by mail armour and lamellar armour.

The importance of infantry and light cavalry began to decline during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use of militia-type levies of the free population declined over the Carolingian period. Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have been mounted infantry, rather than true cavalry. One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as the "fyrd", which were led by the local elites. In military technology, one of the main changes was the return of the crossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages. Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was the horseshoe, which allowed horses to be used in rocky terrain.

The High Middle Ages was a period of tremendous expansion of population. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, a more clement climate and the lack of invasion have all been suggested. As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known as manors or villages. These peasants were often subject to noble overlords and owed them rents and other services, in a system known as manorialism. There remained a few free peasants throughout this period and beyond, with more of them in the regions of Southern Europe than in the north. The practice of assarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to the expansion of population.

The open-field system of agriculture was commonly practiced in most of Europe, especially in "northwestern and central Europe". Such agricultural communities had three basic characteristics: individual peasant holdings in the form of strips of land were scattered among the different fields belonging to the manor; crops were rotated from year to year to preserve soil fertility; and common land was used for grazing livestock and other purposes. Some regions used a three-field system of crop rotation, others retained the older two-field system.

Other sections of society included the nobility, clergy, and townsmen. Nobles, both the titled nobility and simple knights, exploited the manors and the peasants, although they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system of feudalism. During the 11th and 12th centuries, these lands, or fiefs, came to be considered hereditary, and in most areas they were no longer divisible between all the heirs as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son. The dominance of the nobility was built upon its control of the land, its military service as heavy cavalry, control of castles, and various immunities from taxes or other impositions. Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and provided protection from invaders as well as allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords. Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller areas of land and fewer people. Knights were the lowest level of nobility; they controlled but did not own land, and had to serve other nobles.

The clergy was divided into two types: the secular clergy, who lived out in the world, and the regular clergy, who lived under a religious rule and were usually monks. Throughout the period monks remained a very small proportion of the population, usually less than one percent. Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The local parish priests were often drawn from the peasant class. Townsmen were in a somewhat unusual position, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townsmen expanded greatly as existing towns grew and new population centres were founded. But throughout the Middle Ages the population of the towns probably never exceeded 10 percent of the total population.

Jews also spread across Europe during the period. Communities were established in Germany and England in the 11th and 12th centuries, but Spanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity. Most Jews were confined to the cities, as they were not allowed to own land or be peasants. Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.

Women in the Middle Ages were officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows, who were often allowed much control over their own lives, were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for taking care of the household, child-care, as well as gardening and animal husbandry near the house. They could supplement the household income by spinning or brewing at home. At harvest-time, they were also expected to help with field-work. Townswomen, like peasant women, were responsible for the household, and could also engage in trade. What trades were open to women varied by country and period. Noblewomen were responsible for running a household, and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that of nuns, as they were unable to become priests.

In central and northern Italy and in Flanders, the rise of towns that were to a degree self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as the Hanseatic League, and the Italian Maritime republics such as Venice, Genoa, and Pisa expanded their trade throughout the Mediterranean. Great trading fairs were established and flourished in northern France during the period, allowing Italian and German merchants to trade with each other as well as local merchants. In the late 13th century new land and sea routes to the Far East were pioneered, famously described in "The Travels of Marco Polo" written by one of the traders, Marco Polo (d. 1324). Besides new trading opportunities, agricultural and technological improvements enabled an increase in crop yields, which in turn allowed the trade networks to expand. Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, allowing risk to be shared among merchants. Accounting methods improved, partly through the use of double-entry bookkeeping; letters of credit also appeared, allowing easy transmission of money.

The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power, and set up lasting governing institutions. New kingdoms such as Hungary and Poland, after their conversion to Christianity, became Central European powers. The Magyars settled Hungary around 900 under King Árpád (d. c. 907) after a series of invasions in the 9th century. The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; the Papal Monarchy reached its apogee in the early 13th century under the pontificate of (pope 1198–1216). Northern Crusades and the advance of Christian kingdoms and military orders into previously pagan regions in the Baltic and Finnic north-east brought the forced assimilation of numerous native peoples into European culture.

During the early High Middle Ages, Germany was ruled by the Ottonian dynasty, which struggled to control the powerful dukes ruling over territorial duchies tracing back to the Migration period. In 1024, they were replaced by the Salian dynasty, who famously clashed with the papacy under Emperor (r. 1084–1105) over Church appointments as part of the Investiture Controversy. His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of Emperor (r. 1111–25), who died without heirs, until Barbarossa (r. 1155–90) took the imperial throne. Although he ruled effectively, the basic problems remained, and his successors continued to struggle into the 13th century. Barbarossa's grandson Frederick II (r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars and he was often accused of heresy. He and his successors faced many difficulties, including the invasion of the Mongols into Europe in the mid-13th century. Mongols first shattered the Kievan Rus' principalities and then invaded Eastern Europe in 1241, 1259, and 1287.

Under the Capetian dynasty the French monarchy slowly began to expand its authority over the nobility, growing out of the Île-de-France to exert control over more of the country in the 11th and 12th centuries. They faced a powerful rival in the Dukes of Normandy, who in 1066 under William the Conqueror (duke 1035–1087), conquered England (r. 1066–87) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages. Normans also settled in Sicily and southern Italy, when Robert Guiscard (d. 1085) landed there in 1059 and established a duchy that later became the Kingdom of Sicily. Under the Angevin dynasty of (r. 1154–89) and his son Richard I (r. 1189–99), the kings of England ruled over England and large areas of France, brought to the family by Henry II's marriage to Eleanor of Aquitaine (d. 1204), heiress to much of southern France. Richard's younger brother John (r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French King Philip II Augustus (r. 1180–1223). This led to dissension among the English nobility, while John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 to "Magna Carta", a charter that confirmed the rights and privileges of free men in England. Under (r. 1216–72), John's son, further concessions were made to the nobility, and royal power was diminished. The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under the king's personal rule and centralising the royal administration. Under Louis IX (r. 1226–70), royal prestige rose to new heights as Louis served as a mediator for most of Europe.

In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as the "Reconquista". By about 1150, the Christian north had coalesced into the five major kingdoms of León, Castile, Aragon, Navarre, and Portugal. Southern Iberia remained under control of Islamic states, initially under the Caliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known as "taifas", who fought with the Christians until the Almohad Caliphate re-established centralised rule over Southern Iberia in the 1170s. Christian forces advanced again in the early 13th century, culminating in the capture of Seville in 1248.

In the 11th century, the Seljuk Turks took over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at the Battle of Manzikert and captured the Byzantine Emperor Romanus IV (r. 1068–71). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to the Fatimids of Egypt and suffering from a series of internal civil wars. The Byzantines also faced a revived Bulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.

The crusades were intended to seize Jerusalem from Muslim control. The First Crusade was proclaimed by Pope Urban II (pope 1088–99) at the Council of Clermont in 1095 in response to a request from the Byzantine Emperor Alexios I Komnenos (r. 1081–1118) for aid against further Muslim advances. Urban promised indulgence to anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099. One feature of the crusades was the pogroms against local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade, when the Jewish communities in Cologne, Mainz, and Worms were destroyed, as well as other communities in cities between the rivers Seine and the Rhine. Another outgrowth of the crusades was the foundation of a new type of monastic order, the military orders of the Templars and Hospitallers, which fused monastic life with military service.

The crusaders consolidated their conquests into crusader states. During the 12th and 13th centuries, there were a series of conflicts between them and the surrounding Islamic states. Appeals from the crusader states to the papacy led to further crusades, such as the Third Crusade, called to try to regain Jerusalem, which had been captured by Saladin (d. 1193) in 1187. In 1203, the Fourth Crusade was diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up a Latin Empire of Constantinople and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261, but never regained their former strength. By 1291 all the crusader states had been captured or forced from the mainland, although a titular Kingdom of Jerusalem survived on the island of Cyprus for several years afterwards.

Popes called for crusades to take place elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic. The Spanish crusades became fused with the "Reconquista" of Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders of Calatrava and Santiago by the beginning of the 12th century. Northern Europe also remained outside Christian influence until the 11th century or later, and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, the Order of the Sword Brothers. Another order, the Teutonic Knights, although founded in the crusader states, focused much of its activity in the Baltic after 1225, and in 1309 moved its headquarters to Marienburg in Prussia.

During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was debate between the realists and the nominalists over the concept of "universals". Philosophical discourse was stimulated by the rediscovery of Aristotle and his emphasis on empiricism and rationalism. Scholars such as Peter Abelard (d. 1142) and Peter Lombard (d. 1164) introduced Aristotelian logic into theology. In the late 11th and early 12th centuries cathedral schools spread throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns. Cathedral schools were in turn replaced by the universities established in major European cities. Philosophy and theology fused in scholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason and culminated in the thought of Thomas Aquinas (d. 1274), who wrote the "Summa Theologica", or "Summary of Theology".

Chivalry and the ethos of courtly love developed in royal and noble courts. This culture was expressed in the vernacular languages rather than Latin, and comprised poems, stories, legends, and popular songs spread by troubadours, or wandering minstrels. Often the stories were written down in the "chansons de geste", or "songs of great deeds", such as "The Song of Roland" or "The Song of Hildebrand". Secular and religious histories were also produced. Geoffrey of Monmouth (d. c. 1155) composed his "Historia Regum Britanniae", a collection of stories and legends about Arthur. Other works were more clearly history, such as Otto von Freising's (d. 1158) "Gesta Friderici Imperatoris" detailing the deeds of Emperor Frederick Barbarossa, or William of Malmesbury's (d. c. 1143) "Gesta Regum" on the kings of England.

Legal studies advanced during the 12th century. Both secular law and canon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was advanced greatly by the discovery of the "Corpus Juris Civilis" in the 11th century, and by 1100 Roman law was being taught at Bologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140 a monk named Gratian (fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—the "Decretum".

Among the results of the Greek and Islamic influence on this period in European history was the replacement of Roman numerals with the decimal positional number system and the invention of algebra, which allowed more advanced mathematics. Astronomy advanced following the translation of Ptolemy's "Almagest" from Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced the school at Salerno.

In the 12th and 13th centuries, Europe experienced economic growth and innovations in methods of production. Major technological advances included the invention of the windmill, the first mechanical clocks, the manufacture of distilled spirits, and the use of the astrolabe. Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.

The development of a three-field rotation system for planting crops increased the usage of land from one half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production. The development of the heavy plough allowed heavier soils to be farmed more efficiently, aided by the spread of the horse collar, which led to the use of draught horses in place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system. Legumes – such as peas, beans, or lentils – were grown more widely as crops, in addition to the usual cereal crops of wheat, oats, barley, and rye.

The construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. Ancillary structures included new town halls, houses, bridges, and tithe barns. Shipbuilding improved with the use of the rib and plank method rather than the old Roman system of mortise and tenon. Other improvements to ships included the use of lateen sails and the stern-post rudder, both of which increased the speed at which ships could be sailed.

In military affairs, the use of infantry with specialised roles increased. Along with the still-dominant heavy cavalry, armies often included mounted and infantry crossbowmen, as well as sappers and engineers. Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase in siege warfare in the 10th and 11th centuries. The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-face helmets, heavy body armour, as well as horse armour. Gunpowder was known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304, although it was merely used as an explosive and not as a weapon. Cannon were being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.

In the 10th century the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" is derived. Where available, Roman brick and stone buildings were recycled for their materials. From the tentative beginnings known as the First Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000 there was a great wave of building stone churches all over Europe. Romanesque buildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults. The large portal with coloured sculpture in high relief became a central feature of façades, especially in France, and the capitals of columns were often carved with narrative scenes of imaginative monsters and animals. According to art historian C. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive. Simultaneous with the development in church architecture, the distinctive European form of the castle was developed and became crucial to politics and warfare.

Romanesque art, especially metalwork, was at its most sophisticated in Mosan art, in which distinct artistic personalities including Nicholas of Verdun (d. 1205) become apparent, and an almost classical style is seen in works such as a font at Liège, contrasting with the writhing animals of the exactly contemporary Gloucester Candlestick. Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a "Last Judgement" on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.

From the early 12th century, French builders developed the Gothic style, marked by the use of rib vaults, pointed arches, flying buttresses, and large stained glass windows. It was used mainly in churches and cathedrals and continued in use until the 16th century in much of Europe. Classic examples of Gothic architecture include Chartres Cathedral and Reims Cathedral in France as well as Salisbury Cathedral in England. Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.

During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton "by 1300 most monks bought their books in shops", and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses. In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco. Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.

Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life. Cluny Abbey, founded in the Mâcon region of France in 909, was established as part of the Cluniac Reforms, a larger movement of monastic reform in response to this fear. Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.

Monastic reform inspired change in the secular Church. The ideals upon which it was based were brought to the papacy by Pope Leo IX (pope 1049–1054), and provided the ideology of clerical independence that led to the Investiture Controversy in the late 11th century. This involved Pope Gregory VII (pope 1073–85) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas of investiture, clerical marriage, and simony. The emperor saw the protection of the Church as one of his responsibilities as well as wanting to preserve the right to appoint his own choices as bishops within his lands, but the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122 known as the Concordat of Worms. The dispute represents a significant stage in the creation of a papal monarchy separate from and equal to lay authorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.

The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including the Carthusians and the Cistercians. The latter especially expanded rapidly in their early years under the guidance of Bernard of Clairvaux (d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who along with those wishing to enter the religious life wanted a return to the simpler hermetical monasticism of early Christianity, or to live an Apostolic life. Religious pilgrimages were also encouraged. Old pilgrimage sites such as Rome, Jerusalem, and Compostela received increasing numbers of visitors, and new sites such as Monte Gargano and Bari rose to prominence.

In the 13th century mendicant orders—the Franciscans and the Dominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy. Religious groups such as the Waldensians and the Humiliati also attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, but they were condemned as heretical by the papacy. Others joined the Cathars, another heretical movement condemned by the papacy. In 1209, a crusade was preached against the Cathars, the Albigensian Crusade, which in combination with the medieval Inquisition, eliminated them.

The first years of the 14th century were marked by famines, culminating in the Great Famine of 1315–17. The causes of the Great Famine included the slow transition from the Medieval Warm Period to the Little Ice Age, which left the population vulnerable when bad weather caused crop failures. The years 1313–14 and 1317–21 were excessively rainy throughout Europe, resulting in widespread crop failures. The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.

These troubles were followed in 1347 by the Black Death, a pandemic that spread throughout Europe during the following three years. The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions. Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice the reduced number of available workers to their fields. Further problems were lower rents and lower demand for food, both of which cut into agricultural income. Urban workers also felt that they had a right to greater earnings, and popular uprisings broke out across Europe. Among the uprisings were the "jacquerie" in France, the Peasants' Revolt in England, and revolts in the cities of Florence in Italy and Ghent and Bruges in Flanders. The trauma of the plague led to an increased piety throughout Europe, manifested by the foundation of new charities, the self-mortification of the flagellants, and the scapegoating of Jews. Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.

Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned, as the survivors were able to acquire more fertile areas. Although serfdom declined in Western Europe it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free. Most peasants in Western Europe managed to change the work they had previously owed to their landlords into cash rents. The percentage of serfs amongst the peasantry declined from a high of 90 to closer to 50 percent by the end of the period. Landlords also became more conscious of common interests with other landholders, and they joined together to extort privileges from their governments. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death. Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.

Jewish communities were expelled from England in 1290 and from France in 1306. Although some were allowed back into France, most were not, and many Jews emigrated eastwards, and Hungary. The Jews were expelled from Spain in 1492, and dispersed to Turkey, France, Italy, and Holland. The rise of banking in Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many banking firms loaned money to royalty, at great risk, as some were bankrupted when kings defaulted on their loans.

Strong, royalty-based nation states rose throughout Europe in the Late Middle Ages, particularly in England, France, and the Christian kingdoms of the Iberian Peninsula: Aragon, Castile, and Portugal. The long conflicts of the period strengthened royal control over their kingdoms and were extremely hard on the peasantry. Kings profited from warfare that extended royal legislation and increased the lands they directly controlled. Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased. The requirement to obtain the consent of taxpayers allowed representative bodies such as the English Parliament and the French Estates General to gain power and authority.

Throughout the 14th century, French kings sought to expand their influence at the expense of the territorial holdings of the nobility. They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to the Hundred Years' War, waged from 1337 to 1453. Early in the war the English under Edward III (r. 1327–77) and his son Edward, the Black Prince (d. 1376), won the battles of Crécy and Poitiers, captured the city of Calais, and won control of much of France. The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war. In the early 15th century, France again came close to dissolving, but in the late 1420s the military successes of Joan of Arc (d. 1431) led to the victory of the French and the capture of the last English possessions in southern France in 1453. The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars had a positive effect on English national identity, doing much to fuse the various local identities into a national English ideal. The conflict with France also helped create a national culture in England separate from French culture, which had previously been the dominant influence. The dominance of the English longbow began during early stages of the Hundred Years' War, and cannon appeared on the battlefield at Crécy in 1346.

In modern-day Germany, the Holy Roman Empire continued to rule, but the elective nature of the imperial crown meant there was no enduring dynasty around which a strong state could form. Further east, the kingdoms of Poland, Hungary, and Bohemia grew powerful. In Iberia, the Christian kingdoms continued to gain land from the Muslim kingdoms of the peninsula; Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over royal succession and other concerns. After losing the Hundred Years' War, England went on to suffer a long civil war known as the Wars of the Roses, which lasted into the 1490s and only ended when Henry Tudor (r. 1485–1509 as Henry VII) became king and consolidated power with his victory over Richard III (r. 1483–85) at Bosworth in 1485. In Scandinavia, Margaret I of Denmark (r. in Denmark 1387–1412) consolidated Norway, Denmark, and Sweden in the Union of Kalmar, which continued until 1523. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city-states that traded from Western Europe to Russia. Scotland emerged from English domination under Robert the Bruce (r. 1306–29), who secured papal recognition of his kingship in 1328.

Although the Palaeologi emperors recaptured Constantinople from the Western Europeans in 1261, they were never able to regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on the Black Sea and around the Aegean Sea. The former Byzantine lands in the Balkans were divided between the new Kingdom of Serbia, the Second Bulgarian Empire and the city-state of Venice. The power of the Byzantine emperors was threatened by a new Turkish tribe, the Ottomans, who established themselves in Anatolia in the 13th century and steadily expanded throughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at the Battle of Kosovo in 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at the Battle of Nicopolis. Constantinople was finally captured by the Ottomans in 1453.

During the tumultuous 14th century, disputes within the leadership of the Church led to the Avignon Papacy of 1309–76, also called the "Babylonian Captivity of the Papacy" (a reference to the Babylonian captivity of the Jews), and then to the Great Schism, lasting from 1378 to 1418, when there were two and later three rival popes, each supported by several states. Ecclesiastical officials convened at the Council of Constance in 1414, and in the following year the council deposed one of the rival popes, leaving only two claimants. Further depositions followed, and in November 1417 the council elected Martin V (pope 1417–31) as pope.

Besides the schism, the Western Church was riven by theological controversies, some of which turned into heresies. John Wycliffe (d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as for holding views on the Eucharist that were contrary to Church doctrine. Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages: Lollardy in England and Hussitism in Bohemia. The Bohemian movement initiated with the teaching of Jan Hus, who was burned at the stake in 1415 after being condemned as a heretic by the Council of Constance. The Hussite Church, although the target of a crusade, survived beyond the Middle Ages. Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312 and the division of their great wealth between the French King Philip IV (r. 1285–1314) and the Hospitallers.

The papacy further refined the practice in the Mass in the Late Middle Ages, holding that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such as Meister Eckhart (d. 1327) and Thomas à Kempis (d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread, and by the late 15th century the Church had begun to lend credence to populist fears of witchcraft with its condemnation of witches in 1484 and the publication in 1486 of the "Malleus Maleficarum", the most popular handbook for witch-hunters.

During the Later Middle Ages, theologians such as John Duns Scotus (d. 1308) and William of Ockham (d. c. 1348), led a reaction against intellectualist scholasticism, objecting to the application of reason to faith. Their efforts undermined the prevailing Platonic idea of universals. Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy. Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed by customary law. The lone exception to this trend was in England, where the common law remained pre-eminent. Other countries codified their laws; legal codes were promulgated in Castile, Poland, and Lithuania.

Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of the trivium—grammar, rhetoric, logic—were studied in cathedral schools or in schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. Lay literacy rates rose, but were still low; one estimate gave a literacy rate of ten percent of males and one percent of females in 1500.

The publication of vernacular literature increased, with Dante (d. 1321), Petrarch (d. 1374) and Giovanni Boccaccio (d. 1375) in 14th-century Italy, Geoffrey Chaucer (d. 1400) and William Langland (d. c. 1386) in England, and François Villon (d. 1464) and Christine de Pizan (d. c. 1430) in France. Much literature remained religious in character, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages. This was fed by the growth of the "Devotio Moderna" movement, most prominently in the formation of the Brethren of the Common Life, but also in the works of German mystics such as Meister Eckhart and Johannes Tauler (d. 1361). Theatre also developed in the guise of miracle plays put on by the Church. At the end of the period, the development of the printing press in about 1450 led to the establishment of publishing houses throughout Europe by 1500.

In the early 15th century, the countries of the Iberian peninsula began to sponsor exploration beyond the boundaries of Europe. Prince Henry the Navigator of Portugal (d. 1460) sent expeditions that discovered the Canary Islands, the Azores, and Cape Verde during his lifetime. After his death, exploration continued; Bartolomeu Dias (d. 1500) went around the Cape of Good Hope in 1486 and Vasco da Gama (d. 1524) sailed around Africa to India in 1498. The combined Spanish monarchies of Castile and Aragon sponsored the voyage of exploration by Christopher Columbus (d. 1506) in 1492 that discovered the Americas. The English crown under Henry VII sponsored the voyage of John Cabot (d. 1498) in 1497, which landed on Cape Breton Island.

One of the major developments in the military sphere during the Late Middle Ages was the increased use of infantry and light cavalry. The English also employed longbowmen, but other countries were unable to create similar forces with the same success. Armour continued to advance, spurred by the increasing power of crossbows, and plate armour was developed to protect soldiers from crossbows as well as the hand-held guns that were developed. Pole arms reached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.

In agriculture, the increased usage of sheep with long-fibred wool allowed a stronger thread to be spun. In addition, the spinning wheel replaced the traditional distaff for spinning wool, tripling production. A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer. Windmills were refined with the creation of the tower mill, allowing the upper part of the windmill to be spun around to face the direction from which the wind was blowing. The blast furnace appeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality. The first patent law in 1447 in Venice protected the rights of inventors to their inventions.

The Late Middle Ages in Europe as a whole correspond to the Trecento and Early Renaissance cultural periods in Italy. Northern Europe and Spain continued to use Gothic styles, which became increasingly elaborate in the 15th century, until almost the end of the period. International Gothic was a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as the Très Riches Heures du Duc de Berry. All over Europe secular art continued to increase in quantity and quality, and in the 15th century the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery, ivory caskets, cassone chests, and maiolica pottery. These objects also included the Hispano-Moresque ware produced by mostly Mudéjar potters in Spain. Although royalty owned huge collections of plate, little survives except for the Royal Gold Cup. Italian silk manufacture developed, so that Western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders tapestry weaving of sets like "The Lady and the Unicorn" became a major luxury industry.

The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in the Pulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden relief altarpieces became common, especially as churches created many side-chapels. Early Netherlandish painting by artists such as Jan van Eyck (d. 1441) and Rogier van der Weyden (d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450 printed books rapidly became popular, though still expensive. There were around 30,000 different editions of incunabula, or works printed before 1500, by which time illuminated manuscripts were commissioned only by royalty and a few others. Very small woodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensive engravings supplied a wealthier market with a variety of images.

The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity." This is a legacy from both the Renaissance and Enlightenment when scholars favourably contrasted their intellectual cultures with those of the medieval period. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world; Enlightenment scholars saw reason as superior to faith, and thus viewed the Middle Ages as a time of ignorance and superstition.

Others argue that reason was generally held in high regard during the Middle Ages. Science historian Edward Grant writes, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities". Also, contrary to common belief, David Lindberg writes, "the late medieval scholar rarely experienced the coercive power of the Church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".

The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century and still very common, is that all people in the Middle Ages believed that the Earth was flat. This is untrue, as lecturers in the medieval universities commonly argued that evidence showed the Earth was a sphere. Lindberg and Ronald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference". Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian Church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by historical research.



</doc>
<doc id="18837" url="https://en.wikipedia.org/wiki?curid=18837" title="Median">
Median

The median is the value separating the higher half from the lower half of a data sample (a population or a probability distribution). For a data set, it may be thought of as the "middle" value. For example, in the data set [1, 3, 3, 6, 7, 8, 9], the median is 6, the fourth largest, and also the fourth smallest, number in the sample. For a continuous probability distribution, the median is the value such that a number is equally likely to fall above or below it.

The median is a commonly used measure of the properties of a data set in statistics and probability theory. The basic advantage of the median in describing data compared to the mean (often simply described as the "average") is that it is not skewed so much by a small proportion of extremely large or small values, and so it may give a better idea of a "typical" value. For example, in understanding statistics like household income or assets, which vary greatly, the mean may be skewed by a small number of extremely high or low values. Median income, for example, may be a better way to suggest what a "typical" income is.

Because of this, the median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large or small result.

The median of a finite list of numbers can be found by arranging all the numbers from smallest to greatest.

If there is an odd number of numbers, the middle one is picked. For example, consider the list of numbers

This list contains seven numbers. The median is the fourth of them, which is 6.

If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values. For example, in the data set

the median is the mean of the middle two numbers: this is formula_1, which is formula_2. (In more technical terms, this interprets the median as the fully trimmed mid-range).

The formula used to find the index of the middle number of a data set of "n" numerically ordered numbers is formula_3. This either gives the middle number (for an odd number of values) or the halfway point between the two middle values. For example, with 14 values, the formula will give an index of 7.5, and the median will be taken by averaging the seventh (the floor of this index) and eighth (the ceiling of this index) values. So the median can be represented by the following formula:

where formula_5 is an ordered list of numbers, formula_6 denotes its length, and formula_7 and formula_8 denotes the floor and ceiling function, respectively.

One can find the median using the Stem-and-Leaf Plot.

There is no widely accepted standard notation for the median, but some authors represent the median of a variable "x" either as "x͂" or as "μ" sometimes also "M". In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.

The median is used primarily for skewed distributions, which it summarizes differently from the arithmetic mean. Consider the multiset { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the mode), and it might be seen as a better indication of central tendency (less susceptible to the exceptionally large value in data) than the arithmetic mean of 4.

The median is a popular summary statistic used in descriptive statistics, since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of outlier values than is the mean. The widely cited empirical relationship between the relative locations of the mean and the median for skewed distributions is, however, not generally true. There are, however, various relationships for the "absolute" difference between them; see below.

With an even number of observations (as shown above) no value need be exactly at the value of the median. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid.

In a population, at most half have values strictly less than the median and at most half have values strictly greater than it. If each set contains less than half the population, then some of the population is exactly equal to the median. For example, if "a" < "b" < "c", then the median of the list {"a", "b", "c"} is "b", and, if "a" < "b" < "c" < "d", then the median of the list {"a", "b", "c", "d"} is the mean of "b" and "c"; i.e., it is ("b" + "c")/2. As a median is based on the middle data in a set, it is not necessary to know the value of extreme results in order to calculate it. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.

The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.

A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.

The median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible location parameter. The median is the 2nd quartile, 5th decile, and 50th percentile. Since the median is the same as the "second quartile", its calculation is illustrated in the article on quartiles. A median can be worked out for ranked but not numerical classes (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between grades if there is an even number of cases.

When the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the range, the interquartile range, the mean absolute deviation, and the median absolute deviation.

For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see asymptotic efficiency and references therein.

Aho et. al. give a divide-and-conquer algorithm to compute the formula_9th smallest element of an unordered list formula_5 in linear time, which is faster than sorting. Running it with formula_11 computes the median of formula_5.

For any probability distribution on the real line R with cumulative distribution function "F", regardless of whether it is any kind of continuous probability distribution, in particular an absolutely continuous distribution (which has a probability density function), or a discrete probability distribution, a median is by definition any real number "m" that satisfies the inequalities

or, equivalently, the inequalities

in which a Lebesgue–Stieltjes integral is used. For an absolutely continuous probability distribution with probability density function "ƒ", the median satisfies

Any probability distribution on R has at least one median, but in specific cases there may be more than one median. Specifically, if a probability density is zero on an interval ["a", "b"], and the cumulative distribution function at "a" is 1/2, any value between "a" and "b" will also be a median.

The medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the Cauchy distribution:

The "mean absolute error" of a real variable "c" with respect to the random variable "X" is
Provided that the probability distribution of "X" is such that the above expectation exists, then "m" is a median of "X" if and only if "m" is a minimizer of the mean absolute error with respect to "X". In particular, "m" is a sample median if and only if "m" minimizes the arithmetic mean of the absolute deviations.

More generally, a median is defined as a minimum of
as discussed below in the section on multivariate medians (specifically, the spatial median).

This optimization-based definition of the median is useful in statistical data-analysis, for example, in "k"-medians clustering.

It can be shown for a unimodal distribution that the median formula_18 and the mean formula_19 lie within (3/5) ≈ 0.7746 standard deviations of each other. In symbols,

where |·| is the absolute value.

A similar relation holds between the median and the mode: they lie within 3 ≈ 1.732 standard deviations of each other:

If the distribution has finite variance, then the distance between the median and the mean is bounded by one standard deviation.

This bound was proved by Mallows, who used Jensen's inequality twice, as follows. We have

The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex. The second inequality comes from the fact that a median minimizes the absolute deviation function

This proof also follows directly from Cantelli's inequality.
The result can be generalized to obtain a multivariate version of the inequality, as follows:

where "m" is a spatial median, that is, a minimizer of the function
formula_25 The spatial median is unique when the data-set's dimension is two or more. An alternative proof uses the one-sided Chebyshev inequality; it appears in .

Jensen's inequality states that for any random variable "x" with a finite expectation "E"("x") and for any convex function "f"

It has been shown that if "x" is a real variable with a unique median "m" and "f" is a C function then

A C function is a real valued function, defined on the set of real numbers "R", with the property that for any real "t"

is a closed interval, a singleton or an empty set.

Even though comparison-sorting "n" items requires operations, selection algorithms can compute the 'th-smallest of items with only operations. This includes the median, which is the 'th order statistic (or for an even number of samples, the arithmetic mean of the two middle order statistics).

Selection algorithms still have the downside of requiring memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the quicksort sorting algorithm, which uses an estimate of its input's median. A more robust estimator is Tukey's "ninther", which is the median of three rule applied with limited recursion: if is the sample laid out as an array, and

then

The "remedian" is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.

In individual series (if number of observation is very low) first one must arrange all the observations in order. Then count("n") is the total number of observation in given data.

If "n" is odd then Median ("M") = value of (("n" + 1)/2)th item term.

If "n" is even then Median ("M") = value of [("n"/2)th item term + ("n"/2 + 1)th item term]/2


As an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.

Start by sorting the values: 1, 2, 5, 7, 8.

In this case, the median is 5 since it is the middle observation in the ordered list.

The median is the (("n" + 1)/2)th item, where "n" is the number of values. For example, for the list {1, 2, 5, 7, 8}, we have "n" = 5, so the median is the ((5 + 1)/2)th item.


As an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.

Start by sorting the values: 1, 2, 2, 6, 7, 8.

In this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.

The distributions of both the sample mean and the sample median were determined by Laplace. The distribution of the sample median from a population with a density function formula_29 is asymptotically normal with mean formula_30 and variance

where formula_30 is the median of formula_29 and formula_34 is the sample size.
For normal samples, the density is formula_35, thus for large samples the variance of the median equals formula_36

These results have also been extended. It is now known for the formula_37-th quantile that the distribution of the sample formula_37-th quantile is asymptotically normal around the formula_37-th quantile with variance equal to
where formula_41 is the value of the distribution density at the formula_37-th quantile.

In the case of a discrete variable, the sampling distribution of the median for small-samples can be investigated as follows. We take the sample size to be an odd number formula_43. If a given value formula_44 is to be the median of the sample then two conditions must be satisfied. The first is that at most formula_45 observations can have a value of formula_46 or less. The second is that at most formula_45 observations can have a value of formula_48 or more. Let formula_49 be the number of observations that have a value of formula_46 or less and let formula_51 be the number of observations that have a value of formula_48 or more. Then formula_49 and formula_51 both have a minimum value of 0 and a maximum of formula_45. If an observation has a value below formula_44, it is not relevant how far below formula_44 it is and conversely, if an observation has a value above formula_44, it is not relevant how far above formula_44 it is. We can therefore represent the observations as following a trinomial distribution with probabilities formula_60, formula_61 and formula_62. The probability that the median formula_63 will have a value formula_44 is then given by

Summing this over all values of formula_44 defines a proper distribution and gives a unit sum. In practice, the function formula_61 will often not be known but it can be estimated from an observed frequency distribution. An example is given in the following table where the actual distribution is not known but a sample of 3,800 observations allows a sufficiently accurate assessment of formula_61.

Using these data it is possible to investigate the effect of sample size on the standard errors of the mean and median. The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174. The following table gives some comparison statistics. The standard error of the median is given both from the above expression for formula_69 and from the asymptotic approximation given earlier.
The expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size. The asymptotic approximation errs on the side of caution by overestimating the standard error.

In the case of a continuous variable, the following argument can be used. If a given value formula_44 is to be the median, then one observation must take the value formula_44. The elemental probability of this is formula_72. Then, of the remaining formula_73 observations, exactly formula_45 of them must be above formula_44 and the remaining formula_45 below. The probability of this is the formula_45th term of a binomial distribution with parameters formula_78 and formula_73. Finally we multiply by formula_80 since any of the observations in the sample can be the median observation. Hence the elemental probability of the median at the point formula_44 is given by

Now we introduce the beta function. For integer arguments formula_83 and formula_84, this can be expressed as formula_85. Also, formula_86. Using these relationships and setting both formula_83 and formula_84 equal to formula_89 allows the last expression to be written as

Hence the density function of the median is a symmetric beta distribution over the unit interval which supports formula_78. Its mean, as we would expect, is 0.5 and its variance is formula_92. The corresponding variance of the sample median is

However this finding can only be used if the density function formula_61 is known or can be assumed. As this will not always be the case, the median variance has to be estimated sometimes from the sample data.


The value of formula_95—the asymptotic value of formula_96 where formula_97 is the population median—has been studied by several authors. The standard "delete one" jackknife method produces inconsistent results. An alternative—the "delete k" method—where formula_9 grows with the sample size has been shown to be asymptotically consistent. This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent, but converges very slowly (order of formula_99). Other methods have been proposed but their behavior may differ between large and small samples.


The efficiency of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size formula_100 from the normal distribution, the efficiency for large N is

The efficiency tends to formula_102 as formula_103 tends to infinity.

In other words, the relative variance of the median will be formula_104, or 57% greater than the variance of the mean – the standard error of the median will be 25% greater than that of the mean.

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median.

If data are represented by a statistical model specifying a particular family of probability distributions, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.

The coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data. It is a statistical measure used by the states of Iowa, New York and South Dakota in estimating dues taxes. In symbols

where "n" is the sample size, "m" is the sample median and "x" is a variate. The sum is taken over the whole sample.

Confidence intervals for a two-sample test in which the sample sizes are large have been derived by Bonett and Seier. This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by

where "t" is the mean absolute deviation of the "j" sample, var() is the variance and "z" is the value from the normal distribution for the chosen value of "α": for "α" = 0.05, "z" = 1.96. The following formulae are used in the derivation of these confidence intervals

where "r" is the Pearson correlation coefficient between the squared deviation scores

"a" and "b" here are constants equal to 1 and 2, "x" is a variate and "s" is the standard deviation of the sample.

Previously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.

The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.

The geometric median of a discrete set of sample points formula_111 in a Euclidean space is point minimizing the sum of distances to the sample points. 

In contrast to the marginal median, the geometric median is equivariant with respect to Euclidean similarity transformations such as translations and rotations.

An alternative generalization of the median in higher dimensions is the centerpoint.

When dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median formula_63 is 3 since the median is the smallest value of formula_114 for which formula_115 is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50. First we add half of the interval width formula_116 to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark. In other words, we split up the interval width pro rata to the numbers of observations. In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values formula_117 are known, the interpolated median can be calculated from

Alternatively, if in an observed sample there are formula_51 scores above the median category, formula_120 scores in it and formula_49 scores below it then the interpolated median is given by

For univariate distributions that are "symmetric" about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population "pseudo-median", which is the median of a symmetrized distribution and which is close to the population median. The Hodges–Lehmann estimator has been generalized to multivariate distributions.

The Theil–Sen estimator is a method for robust linear regression based on finding medians of slopes.

In the context of image processing of monochrome raster images there is a type of noise, known as the salt and pepper noise, when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively reduce noise in this case.

In cluster analysis, the k-medians clustering algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in k-means clustering, is replaced by maximising the distance between cluster-medians.

This is a method of robust regression. The idea dates back to Wald in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter formula_123: a left half with values less than the median and a right half with values greater than the median. He suggested taking the means of the dependent formula_124 and independent formula_123 variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.

Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples. Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.

Any "mean"-unbiased estimator minimizes the risk (expected loss) with respect to the squared-error loss function, as observed by Gauss. A "median"-unbiased estimator minimizes the risk with respect to the absolute-deviation loss function, as observed by Laplace. Other loss functions are used in statistical theory, particularly in robust statistics.

The theory of median-unbiased estimators was revived by George W. Brown in 1947:

Further properties of median-unbiased estimators have been reported. Median-unbiased estimators are invariant under one-to-one transformations.

There are methods of constructing median-unbiased estimators that are optimal (in a sense analogous to the minimum-variance property for mean-unbiased estimators). Such constructions exist for probability distributions having monotone likelihood-functions. One such procedure is an analogue of the Rao–Blackwell procedure for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao—Blackwell procedure but for a larger class of loss functions.

The idea of the median appeared in the 13th century in the Talmud (further for possible older mentions)

The idea of the median also appeared later in Edward Wright's book on navigation ("Certaine Errors in Navigation") in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.

In 1757, Roger Joseph Boscovich developed a regression method based on the "L" norm and therefore implicitly on the median.

In 1774, Laplace suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criterion was to minimize the expected magnitude of the error; formula_126 where formula_127 is the estimate and formula_128 is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the least squares method of Gauss and Legendre which minimizes formula_129 to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.

Antoine Augustin Cournot in 1843 was the first to use the term "median" ("valeur médiane") for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median ("Centralwerth") in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data, although it had been used previously by Laplace.

Francis Galton used the English term "median" in 1881, having earlier used the terms "middle-most value" in 1869, and the "medium" in 1880.




</doc>
<doc id="18838" url="https://en.wikipedia.org/wiki?curid=18838" title="Mammal">
Mammal

Mammals (from Latin "mamma" "breast") are vertebrate animals constituting the class Mammalia (), and characterized by the presence of mammary glands which in females (and sometimes males) produce milk for feeding (nursing) their young, a neocortex (a region of the brain), fur or hair, and three middle ear bones. These characteristics distinguish them from reptiles and birds, from which they diverged in the late Triassic, 201–227 million years ago. There are around 5,450 species of mammals. The largest orders are the rodents, bats and Soricomorpha (shrews and others). The next three are the Primates (apes, monkeys, and others), the Cetartiodactyla (cetaceans and even-toed ungulates), and the Carnivora (cats, dogs, seals, and others).

Like birds, mammals are endothermic. The trait evolved independently in these two classes and is an example of convergent evolution.

In terms of cladistics, which reflects evolutionary history, mammals are the only living members of the Synapsida; this clade, together with Sauropsida (reptiles and birds) together constitute the larger Amniota clade. The early synapsid mammalian ancestors were sphenacodont pelycosaurs, a group that included the non-mammalian "Dimetrodon". At the end of the Carboniferous period around 300 million years ago, this group diverged from the sauropsid line that led to today's reptiles and birds. The line following the stem group Sphenacodontia split into several diverse groups of non-mammalian synapsids—sometimes incorrectly referred to as mammal-like reptiles—before giving rise to Therapsida in the Early Permian period. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of non-avian dinosaurs, and have been among the dominant terrestrial animal groups from 66 million years ago to the present.

The basic body type is quadruped, and most mammals use their four extremities for terrestrial locomotion; but in some, the extremities are adapted for life at sea, in the air, in trees, underground, or on two legs. Mammals range in size from the bumblebee bat to the blue whale—possibly the largest animal to have ever lived. Maximum lifespan varies from two years for the shrew to 211 years for the bowhead whale. All modern mammals give birth to live young, except the five species of monotremes, which are egg-laying mammals. The most species-rich group of mammals, the cohort called placentals, have a placenta, which enables the feeding of the fetus during gestation.

Most mammals are intelligent, with some possessing large brains, self-awareness, and tool use. Mammals can communicate and vocalize in several different ways, including the production of ultrasound, scent-marking, alarm signals, singing, and echolocation. Mammals can organize themselves into fission-fusion societies, harems, and hierarchies—but can also be solitary and territorial. Most mammals are polygynous, but some can be monogamous or polyandrous.

Domestication of many types of mammals by humans played a major role in the Neolithic revolution, and resulted in farming replacing hunting and gathering as the primary source of food for humans. This led to a major restructuring of human societies from nomadic to sedentary, with more co-operation among larger and larger groups, and ultimately the development of the first civilizations. Domesticated mammals provided, and continue to provide, power for transport and agriculture, as well as food (meat and dairy products), fur, and leather. Mammals are also hunted and raced for sport, and are used as model organisms in science. Mammals have been depicted in art since Palaeolithic times, and appear in literature, film, mythology, and religion. Decline in numbers and extinction of many mammals is primarily driven by human poaching and habitat destruction, primarily deforestation.

Mammal classification has been through several iterations since Carl Linnaeus initially defined the class. No classification system is universally accepted; McKenna & Bell (1997) and Wilson & Reader (2005) provide useful recent compendiums. George Gaylord Simpson's "Principles of Classification and a Classification of Mammals" (AMNH "Bulletin" v. 85, 1945) provides systematics of mammal origins and relationships that were universally taught until the end of the 20th century. Since Simpson's classification, the paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though field work gradually made Simpson's classification outdated, it remains the closest thing to an official classification of mammals.

Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers of species are Rodentia: mice, rats, porcupines, beavers, capybaras and other gnawing mammals; Chiroptera: bats; and Soricomorpha: shrews, moles and solenodons. The next three biggest orders, depending on the biological classification scheme used, are the Primates including the apes, monkeys and lemurs; the Cetartiodactyla including whales and even-toed ungulates; and the Carnivora which includes cats, dogs, weasels, bears, seals and allies. According to "Mammal Species of the World", 5,416 species were identified in 2006. These were grouped into 1,229 genera, 153 families and 29 orders. In 2008, the International Union for Conservation of Nature (IUCN) completed a five-year Global Mammal Assessment for its IUCN Red List, which counted 5,488 species. According to research published in the "Journal of Mammalogy" in 2018, the number of recognized mammal species is 6,495 including 96 recently extinct.

The word "mammal" is modern, from the scientific name "Mammalia" coined by Carl Linnaeus in 1758, derived from the Latin "mamma" ("teat, pap"). In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group of mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and therian mammals (marsupials and placentals) and all descendants of that ancestor. Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century. If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. "Ambondro" is more closely related to monotremes than to therian mammals while "Amphilestes" and "Amphitherium" are more closely related to the therians; as fossils of all three genera are dated about in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group.

T. S. Kemp has provided a more traditional definition: "synapsids that possess a dentary–squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of "Sinoconodon" and living mammals. The earliest known synapsid satisfying Kemp's definitions is "Tikitherium", dated , so the appearance of mammals in this broader sense can be given this Late Triassic date.

In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. Their 1997 book, "Classification of Mammals above the Species Level", is a comprehensive work on the systematics, relationships and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though molecular genetic data challenge several of the higher level groupings. The authors worked together as paleontologists at the American Museum of Natural History, New York. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa that reflects the historical genealogy of Mammalia.

Extinct groups are represented by a dagger (†).

Class Mammalia

Molecular studies based on DNA analysis have suggested new relationships among mammal families over the last few years. Most of these findings have been independently validated by retrotransposon presence/absence data. Classification systems based on molecular studies reveal three major groups or lineages of placental mammals—Afrotheria, Xenarthra and Boreoeutheria—which diverged in the Cretaceous. The relationships between these three lineages is contentious, and all three possible different hypotheses have been proposed with respect to which group is basal. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra) and Exafroplacentalia (basal Afrotheria). Boreoeutheria in turn contains two major lineages—Euarchontoglires and Laurasiatheria.

Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on the type of DNA used (such as nuclear or mitochondrial) and varying interpretations of paleogeographic data.

The cladogram above is based on Tarver "et al". (2016)

Group I: Superorder Afrotheria
Group II: Superorder Xenarthra
Group III: Magnaorder Boreoeutheria

Synapsida, a clade that contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod (~323 million to ~300 million years ago), when they split from reptilian and avian lineages. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic. The cladogram takes Mammalia to be the crown group.

The first fully terrestrial vertebrates were amniotes. Like their amphibious tetrapod predecessors, they had lungs and limbs. Amniotic eggs, however, have internal membranes that allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.

The first amniotes apparently arose in the Pennsylvanian subperiod of the Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods, which lived on land that was already inhabited by insects and other invertebrates as well as ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which now include turtles, lizards, snakes, crocodilians, dinosaurs and birds. Synapsids have a single hole (temporal fenestra) low on each side of the skull. One synapsid group, the pelycosaurs, included the largest and fiercest animals of the early Permian. Nonmammalian synapsids are sometimes (inaccurately) called "mammal-like reptiles".

Therapsids, a group of synapsids, descended from pelycosaurs in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates. They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger skulls and incisors which are equal in size in therapsids, but not for eupelycosaurs. The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very similar to their pelycosaur ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:

The Permian–Triassic extinction event about 252 million years ago, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of carnivorous therapsids. In the early Triassic, most medium to large land carnivore niches were taken over by archosaurs which, over an extended period (35 million years), came to include the crocodylomorphs, the pterosaurs and the dinosaurs; however, large cynodonts like "Trucidocynodon" and traversodontids still occupied large sized carnivorous and herbivorous niches respectively. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.

The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards; The Jurassic "Castorocauda", for example, was a close relative of true mammals that had adaptations for swimming, digging and catching fish. Most, if not all, are thought to have remained nocturnal (the nocturnal bottleneck), accounting for much of the typical mammalian traits. The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids. The earliest known metatherian is "Sinodelphys", found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province. The fossil is nearly complete and includes tufts of fur and imprints of soft tissues.

The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike "Juramaia sinensis", or "Jurassic mother from China", dated to 160 million years ago in the late Jurassic. A later eutherian relative, "Eomaia", dated to 125 million years ago in the early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage. In particular, the epipubic bones extend forwards from the pelvis. These are not found in any modern placental, but they are found in marsupials, monotremes, other nontherian mammals and "Ukhaatherium", an early Cretaceous animal in the eutherian order Asioryctitheria. This also applies to the multituberculates. They are apparently an ancestral feature, which subsequently disappeared in the placental lineage. These epipubic bones seem to function by stiffening the muscles during locomotion, reducing the amount of space being presented, which placentals require to contain their fetus during gestation periods. A narrow pelvic outlet indicates that the young were very small at birth and therefore pregnancy was short, as in modern marsupials. This suggests that the placenta was a later development.

One of the earliest known monotremes was "Teinolophos", which lived about 120 million years ago in Australia. Monotremes have some features which may be inherited from the original amniotes such as the same orifice to urinate, defecate and reproduce (cloaca)—as lizards and birds also do— and they lay eggs which are leathery and uncalcified.

"Hadrocodium", whose fossils date from approximately 195 million years ago, in the early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids.
The earliest clear evidence of hair or fur is in fossils of "Castorocauda" and "Megaconus", from 164 million years ago in the mid-Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur; it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard "Tupinambis" has foramina that are almost identical to those found in the nonmammalian cynodont "Thrinaxodon". Popular sources, nevertheless, continue to attribute whiskers to "Thrinaxodon". Studies on Permian coprolites suggest that non-mammalian synapsids of the epoch already had fur, setting the evolution of hairs possibly as far back as dicynodonts.

When endothermy first appeared in the evolution of mammals is uncertain, though it is generally agreed to have first evolved in non-mammalian therapsids. Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals, but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians. Likewise, some modern therians like afrotheres and xenarthrans have secondarily developed lower body temperatures.

The evolution of erect limbs in mammals is incomplete—living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the late Jurassic or early Cretaceous; it is found in the eutherian "Eomaia" and the metatherian "Sinodelphys", both dated to 125 million years ago. Epipubic bones, a feature that strongly influenced the reproduction of most mammal clades, are first found in Tritylodontidae, suggesting that it is a synapomorphy between them and mammaliformes. They are omnipresent in non-placental mammaliformes, though "Megazostrodon" and "Erythrotherium" appear to have lacked them.

It has been suggested that the original function of lactation (milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals.

Therian mammals took over the medium- to large-sized ecological niches in the Cenozoic, after the Cretaceous–Paleogene extinction event approximately 66 million years ago emptied ecological space once filled by non-avian dinosaurs and other groups of reptiles, as well as various other mammal groups, and underwent an exponential increase in body size (megafauna). Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity. For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the dinosaurs.

Molecular phylogenetic studies initially suggested that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene. However, no placental fossils have been found from before the end of the Cretaceous. The earliest undisputed fossils of placentals comes from the early Paleocene, after the extinction of the dinosaurs. In particular, scientists have identified an early Paleocene animal named "Protungulatum donnae" as one of the first placental mammals. however it has been reclassified as a non-placental eutherian. Recalibrations of genetic and morphological diversity rates have suggested a Late Cretaceous origin for placentals, and a Paleocene origin for most modern clades.

The earliest known ancestor of primates is "Archicebus achilles" from around 55 million years ago. This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.

Living mammal species can be identified by the presence of sweat glands, including those that are specialized to produce milk to nourish their young. In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils.

Many traits shared by all living mammals appeared among the earliest members of the group:

For the most part, these characteristics were not present in the Triassic ancestors of the mammals. Nearly all mammaliaforms possess an epipubic bone, the exception being modern placentals.

The majority of mammals have seven cervical vertebrae (bones in the neck), including bats, giraffes, whales and humans. The exceptions are the manatee and the two-toed sloth, which have just six, and the three-toed sloth which has nine cervical vertebrae. All mammalian brains possess a neocortex, a brain region unique to mammals. Placental mammals have a corpus callosum, unlike monotremes and marsupials.

The lungs of mammals are spongy and honeycombed. Breathing is mainly achieved with the diaphragm, which divides the thorax from the abdominal cavity, forming a dome convex to the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the lung cavity. Air enters through the oral and nasal cavities, and travels through the larynx, trachea and bronchi, and expands the alveoli. Relaxing the diaphragm has the opposite effect, decreasing the volume of the lung cavity, causing air to be pushed out of the lungs. During exercise, the abdominal wall contracts, increasing pressure on the diaphragm, which forces air out quicker and more forcefully. The rib cage is able to expand and contract the chest cavity through the action of other respiratory muscles. Consequently, air is sucked into or expelled out of the lungs, always moving down its pressure gradient. This type of lung is known as a bellows lung due to its resemblance to blacksmith bellows.

The mammalian heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers. The heart has four valves, which separate its chambers and ensures blood flows in the correct direction through the heart (preventing backflow). After gas exchange in the pulmonary capillaries (blood vessels in the lungs), oxygen-rich blood returns to the left atrium via one of the four pulmonary veins. Blood flows nearly continuously back into the atrium, which acts as the receiving chamber, and from here through an opening into the left ventricle. Most blood flows passively into the heart while both the atria and ventricles are relaxed, but toward the end of the ventricular relaxation period, the left atrium will contract, pumping blood into the ventricle. The heart also requires nutrients and oxygen found in blood like other muscles, and is supplied via coronary arteries.

The integumentary system is made up of three layers: the outermost epidermis, the dermis and the hypodermis. The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue, which stores lipids and provides cushioning and insulation. The thickness of this layer varies widely from species to species; marine mammals require a thick hypodermis (blubber) for insulation, and right whales have the thickest blubber at . Although other animals have features such as whiskers, feathers, setae, or cilia that superficially resemble it, no animals other than mammals have hair. It is a definitive characteristic of the class. Though some mammals have very little, careful examination reveals the characteristic, often in obscure parts of their bodies.
Herbivores have developed a diverse range of physical structures to facilitate the consumption of plant material. To break up intact plant tissues, mammals have developed teeth structures that reflect their feeding preferences. For instance, frugivores (animals that feed primarily on fruit) and herbivores that feed on soft foliage have low-crowned teeth specialized for grinding foliage and seeds. Grazing animals that tend to eat hard, silica-rich grasses, have high-crowned teeth, which are capable of grinding tough plant tissues and do not wear down as quickly as low-crowned teeth. Most carnivorous mammals have carnassialiforme teeth (of varying length depending on diet), long canines and similar tooth replacement patterns.

The stomach of Artiodactyls is divided into four sections: the rumen, the reticulum, the omasum and the abomasum (only ruminants have a rumen). After the plant material is consumed, it is mixed with saliva in the rumen and reticulum and separates into solid and liquid material. The solids lump together to form a bolus (or cud), and is regurgitated. When the bolus enters the mouth, the fluid is squeezed out with the tongue and swallowed again. Ingested food passes to the rumen and reticulum where cellulytic microbes (bacteria, protozoa and fungi) produce cellulase, which is needed to break down the cellulose in plants. Perissodactyls, in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum, where it is fermented by bacteria. Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the large intestine is not sacculated or much wider than the small intestine.

The mammalian excretory system involves many components. Like most other land animals, mammals are ureotelic, and convert ammonia into urea, which is done by the liver as part of the urea cycle. Bilirubin, a waste product derived from blood cells, is passed through bile and urine with the help of enzymes excreted by the liver. The passing of bilirubin via bile through the intestinal tract gives mammalian feces a distinctive brown coloration. Distinctive features of the mammalian kidney include the presence of the renal pelvis and renal pyramids, and of a clearly distinguishable cortex and medulla, which is due to the presence of elongated loops of Henle. Only the mammalian kidney has a bean shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds, cetaceans and bears. Most adult placental mammals have no remaining trace of the cloaca. In the embryo, the embryonic cloaca divides into a posterior region that becomes part of the anus, and an anterior region that has different fates depending on the sex of the individual: in females, it develops into the vestibule that receives the urethra and vagina, while in males it forms the entirety of the penile urethra. However, the tenrecs, golden moles, and some shrews retain a cloaca as adults. In marsupials, the genital tract is separate from the anus, but a trace of the original cloaca does remain externally. Monotremes, which translates from Greek into "single hole", have a true cloaca.

As in all other tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal vocal tract which filters this sound. The lungs and surrounding musculature provide the air stream and pressure required to phonate. The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially closed larynx. Other mammals phonate using vocal folds, as opposed to the vocal cords seen in birds and reptiles. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming. Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to form both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud.

Some mammals have a large larynx and thus a low-pitched voice, namely the hammer-headed bat ("Hypsignathus monstrosus") where the larynx can take up the entirety of the thoracic cavity while pushing the lungs, heart, and trachea into the abdomen. Large vocal pads can also lower the pitch, as in the low-pitched roars of big cats. The production of infrasound is possible in some mammals such as the African elephant ("Loxodonta" spp.) and baleen whales. Small mammals with small larynxes have the ability to produce ultrasound, which can be detected by modifications to the middle ear and cochlea. Ultrasound is inaudible to birds and reptiles, which might have been important during the Mesozoic, when birds and reptiles were the dominant predators. This private channel is used by some rodents in, for example, mother-to-pup communication, and by bats when echolocating. Toothed whales also use echolocation, but, as opposed to the vocal membrane that extends upward from the vocal folds, they have a melon to manipulate sounds. Some mammals, namely the primates, have air sacs attached to the larynx, which may function to lower the resonances or increase the volume of sound.

The vocal production system is controlled by the cranial nerve nuclei in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve, branches of the vagus nerve. The vocal tract is supplied by the hypoglossal nerve and facial nerves. Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, elephants and possibly bats; in humans, this is the result of a direct connection between the motor cortex, which controls movement, and the motor neurons in the spinal cord.

The primary function of the fur of mammals is thermoregulation. Others include protection, sensory purposes, waterproofing, and camouflage.


Hair length is not a factor in thermoregulation: for example, some tropical mammals such as sloths have the same length of fur length as some arctic mammals but with less insulation; and, conversely, other tropical mammals with short hair have the same insulating value as arctic mammals. The denseness of fur can increase an animal's insulation value, and arctic mammals especially have dense fur; for example, the musk ox has guard hairs measuring as well as a dense underfur, which forms an airtight coat, allowing them to survive in temperatures of . Some desert mammals, such as camels, use dense fur to prevent solar heat from reaching their skin, allowing the animal to stay cool; a camel's fur may reach in the summer, but the skin stays at . Aquatic mammals, conversely, trap air in their fur to conserve heat by keeping the skin dry.

Mammalian coats are colored for a variety of reasons, the major selective pressures including camouflage, sexual selection, communication, and thermoregulation. Coloration in both the hair and skin of mammals is mainly determined by the type and amount of melanin; eumelanins for brown and black colors and pheomelanin for a range of yellow to reddish-brown colors, giving mammals an earth tone. Some mammals, like the mandrill, have more vibrant colors due to structural coloration. Many sloths appear green because their fur hosts green algae; this may be a symbiotic relation that affords camouflage to the sloths.

Camouflage is a powerful influence in a large number of mammals, as it helps to conceal individuals from predators or prey. In arctic and subarctic mammals such as the arctic fox ("Alopex lagopus"), collared lemming ("Dicrostonyx groenlandicus"), stoat ("Mustela erminea"), and snowshoe hare ("Lepus americanus"), seasonal color change between brown in summer and white in winter is driven largely by camouflage. Some arboreal mammals, notably primates and marsupials, have shades of violet, green, or blue skin on parts of their bodies, indicating some distinct advantage in their largely arboreal habitat due to convergent evolution.

Aposematism, warning off possible predators, is the most likely explanation of the black-and-white pelage of many mammals which are able to defend themselves, such as in the foul-smelling skunk and the powerful and aggressive honey badger. Coat color is sometimes sexually dimorphic, as in many primate species. Differences in female and male coat color may indicate nutrition and hormone levels, important in mate selection. Coat color may influence the ability to retain heat, depending on how much light is reflected. Mammals with a darker colored coat can absorb more heat from solar radiation, and stay warmer, and some smaller mammals, such as voles, have darker fur in the winter. The white, pigmentless fur of arctic mammals, such as the polar bear, may reflect more solar radiation directly onto the skin. The dazzling black-and-white striping of zebras appear to provide some protection from biting flies.

In male placentals, the penis is used both for urination and copulation. Depending on the species, an erection may be fueled by blood flow into vascular, spongy tissue or by muscular action. A penis may be contained in a prepuce when not erect, and some placentals also have a penis bone (baculum). Marsupials typically have forked penises, while the echidna penis generally has four heads with only two functioning. The testes of most mammals descend into the scrotum which is typically posterior to the penis but is often anterior in marsupials. Female mammals generally have a clitoris, labia majora and labia minora on the outside, while the internal system contains paired oviducts, 1-2 uteri, 1-2 cervices and a vagina. Marsupials have two lateral vaginas and a medial vagina. The "vagina" of monotremes is better understood as a "urogenital sinus". The uterine systems of placental mammals can vary between a duplex, were there are two uteri and cervices which open into the vagina, a bipartite, were two uterine horns have a single cervix that connects to the vagina, a bicornuate, which consists where two uterine horns that are connected distally but separate medially creating a Y-shape, and a simplex, which has a single uterus.
The ancestral condition for mammal reproduction is the birthing of relatively undeveloped, either through direct vivipary or a short period as soft-shelled eggs. This is likely due to the fact that the torso could not expand due to the presence of epipubic bones. The oldest demonstration of this reproductive style is with "Kayentatherium", which produced undeveloped perinates, but at much higher litter sizes than any modern mammal, 38 specimens. Most modern mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypus and the four species of echidna, lay eggs. The monotremes have a sex determination system different from that of most other mammals. In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal.

Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. Marsupials have a short gestation period, typically shorter than its estrous cycle and gives birth to an undeveloped newborn that then undergoes further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesiomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy. Even non-placental eutherians probably reproduced this way. The placentals give birth to relatively complete and developed young, usually after long gestation periods. They get their name from the placenta, which connects the developing fetus to the uterine wall to allow nutrient uptake. In placental mammals, the epipubic is either completely lost or converted into the baculum; allowing the torso to be able to expand and thus birth developed offspring.

The mammary glands of mammals are specialized to produce milk, the primary source of nutrition for newborns. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly. Compared to placental mammals, the milk of marsupials changes greatly in both production rate and in nutrient composition, due to the underdeveloped young. In addition, the mammary glands have more autonomy allowing them to supply separate milks to young at different development stages. Lactose is the main sugar in placental mammal milk while monotreme and marsupial milk is dominated by oligosaccharides. Weaning is the process in which a mammal becomes less dependent on their mother's milk and more on solid food.

Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic ("cold-blooded") reptiles and insects. Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles. Small insectivorous mammals eat prodigious amounts for their size. A rare exception, the naked mole-rat produces little metabolic heat, so it is considered an operational poikilotherm. Birds are also endothermic, so endothermy is not unique to mammals.

Among mammals, species maximum lifespan varies significantly (for example the shrew has a lifespan of two years, whereas the oldest bowhead whale is recorded to be 211 years). Although the underlying basis for these lifespan differences is still uncertain, numerous studies indicate that the ability to repair DNA damage is an important determinant of mammalian lifespan. In a 1974 study by Hart and Setlow, it was found that DNA excision repair capability increased systematically with species lifespan among seven mammalian species. Species lifespan was observed to be robustly correlated with the capacity to recognize DNA double-strand breaks as well as the level of the DNA repair protein Ku80. In a study of the cells from sixteen mammalian species, genes employed in DNA repair were found to be up-regulated in the longer-lived species. The cellular level of the DNA repair enzyme poly ADP ribose polymerase was found to correlate with species lifespan in a study of 13 mammalian species. Three additional studies of a variety of mammalian species also reported a correlation between species lifespan and DNA repair capability.

Most vertebrates—the amphibians, the reptiles and some mammals such as humans and bears—are plantigrade, walking on the whole of the underside of the foot. Many mammals, such as cats and dogs, are digitigrade, walking on their toes, the greater stride length allowing more speed. Digitigrade mammals are also often adept at quiet movement. Some animals such as horses are unguligrade, walking on the tips of their toes. This even further increases their stride length and thus their speed. A few mammals, namely the great apes, are also known to walk on their knuckles, at least for their front legs. Giant anteaters and platypuses are also knuckle-walkers. Some mammals are bipeds, using only two limbs for locomotion, which can be seen in, for example, humans and the great apes. Bipedal species have a larger field of vision than quadrupeds, conserve more energy and have the ability to manipulate objects with their hands, which aids in foraging. Instead of walking, some bipeds hop, such as kangaroos and kangaroo rats.

Animals will use different gaits for different speeds, terrain and situations. For example, horses show four natural gaits, the slowest horse gait is the walk, then there are three faster gaits which, from slowest to fastest, are the trot, the canter and the gallop. Animals may also have unusual gaits that are used occasionally, such as for moving sideways or backwards. For example, the main human gaits are bipedal walking and running, but they employ many other gaits occasionally, including a four-legged crawl in tight spaces. Mammals show a vast range of gaits, the order that they place and lift their appendages in locomotion. Gaits can be grouped into categories according to their patterns of support sequence. For quadrupeds, there are three main categories: walking gaits, running gaits and leaping gaits. Walking is the most common gait, where some feet are on the ground at any given time, and found in almost all legged animals. Running is considered to occur when at some points in the stride all feet are off the ground in a moment of suspension.

Arboreal animals frequently have elongated limbs that help them cross gaps, reach fruit or other resources, test the firmness of support ahead and, in some cases, to brachiate (swing between trees). Many arboreal species, such as tree porcupines, silky anteaters, spider monkeys, and possums, use prehensile tails to grasp branches. In the spider monkey, the tip of the tail has either a bare patch or adhesive pad, which provides increased friction. Claws can be used to interact with rough substrates and reorient the direction of forces the animal applies. This is what allows squirrels to climb tree trunks that are so large to be essentially flat from the perspective of such a small animal. However, claws can interfere with an animal's ability to grasp very small branches, as they may wrap too far around and prick the animal's own paw. Frictional gripping is used by primates, relying upon hairless fingertips. Squeezing the branch between the fingertips generates frictional force that holds the animal's hand to the branch. However, this type of grip depends upon the angle of the frictional force, thus upon the diameter of the branch, with larger branches resulting in reduced gripping ability. To control descent, especially down large diameter branches, some arboreal animals such as squirrels have evolved highly mobile ankle joints that permit rotating the foot into a 'reversed' posture. This allows the claws to hook into the rough surface of the bark, opposing the force of gravity. Small size provides many advantages to arboreal species: such as increasing the relative size of branches to the animal, lower center of mass, increased stability, lower mass (allowing movement on smaller branches) and the ability to move through more cluttered habitat. Size relating to weight affects gliding animals such as the sugar glider. Some species of primate, bat and all species of sloth achieve passive stability by hanging beneath the branch. Both pitching and tipping become irrelevant, as the only method of failure would be losing their grip.

Bats are the only mammals that can truly fly. They fly through the air at a constant speed by moving their wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of the wings, generates a faster airflow moving over the wing. This generates a lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole.

The wings of bats are much thinner and consist of more bones than those of birds, allowing bats to maneuver more accurately and fly with more lift and less drag. By folding the wings inwards towards their body on the upstroke, they use 35% less energy during flight than birds. The membranes are delicate, ripping easily; however, the tissue of the bat's membrane is able to regrow, such that small tears can heal quickly. The surface of their wings is equipped with touch-sensitive receptors on small bumps called Merkel cells, also found on human fingertips. These sensitive areas are different in bats, as each bump has a tiny hair in the center, making it even more sensitive and allowing the bat to detect and collect information about the air flowing over its wings, and to fly more efficiently by changing the shape of its wings in response.

A fossorial (from Latin "fossor", meaning "digger") is an animal adapted to digging which lives primarily, but not solely, underground. Some examples are badgers, and naked mole-rats. Many rodent species are also considered fossorial because they live in burrows for most but not all of the day. Species that live exclusively underground are described as "subterranean". Some organisms are fossorial to aid in temperature regulation while others use the underground habitat for protection from predators or for food storage. An animal is said to be "sub-fossorial" if it shows limited adaptations to a fossorial lifestyle.

Fossorial mammals have a fusiform body, thickest at the shoulders and tapering off at the tail and nose. Unable to see in the dark burrows, most have degenerated eyes, but degeneration varies between species; pocket gophers, for example, are only semi-fossorial and have very small yet functional eyes, in the fully fossorial marsupial mole the eyes are degenerated and useless, talpa moles have vestigial eyes and the cape golden mole has a layer of skin covering the eyes. External ears flaps are also very small or absent. Truly fossorial mammals have short, stout legs as strength is more important than speed to a burrowing mammal, but semi-fossorial mammals have cursorial legs. The front paws are broad and have strong claws to help in loosening dirt while excavating burrows, and the back paws have webbing, as well as claws, which aids in throwing loosened dirt backwards. Most have large incisors to prevent dirt from flying into their mouth.

Many fossorial mammals such as shrews, hedgehogs, and moles were classified under the now obsolete order Insectivora.

Fully aquatic mammals, the cetaceans and sirenians, have lost their legs and have a tail fin to propel themselves through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin to prevent themselves from turning upside-down in the water. The flukes of sirenians are raised up and down in long strokes to move the animal forward, and can be twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing.

Semi-aquatic mammals, like pinnipeds, have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. The elbows and ankles are enclosed within the body. Pinnipeds have several adaptions for reducing drag. In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and make it easier for them to slip through water. They also lack arrector pili, so their fur can be streamlined as they swim. They rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles. Fore-flipper movement is not continuous, and the animal glides between each stroke. Compared to terrestrial carnivorans, the fore-limbs are reduced in length, which gives the locomotor muscles at the shoulder and elbow joints greater mechanical advantage; the hind-flippers serve as stabilizers. Other semi-aquatic mammals include beavers, hippopotamuses, otters and platypuses. Hippos are very large semi-aquatic mammals, and their barrel-shaped bodies have graviportal skeletal structures, adapted to carrying their enormous weight, and their specific gravity allows them to sink and move along the bottom of a river.

Many mammals communicate by vocalizing. Vocal communication serves many purposes, including in mating rituals, as warning calls, to indicate food sources, and for social purposes. Males often call during mating rituals to ward off other males and to attract females, as in the roaring of lions and red deer. The songs of the humpback whale may be signals to females; they have different dialects in different regions of the ocean. Social vocalizations include the territorial calls of gibbons, and the use of frequency in greater spear-nosed bats to distinguish between groups. The vervet monkey gives a distinct alarm call for each of at least four different predators, and the reactions of other monkeys vary according to the call. For example, if an alarm call signals a python, the monkeys climb into the trees, whereas the eagle alarm causes monkeys to seek a hiding place on the ground. Prairie dogs similarly have complex calls that signal the type, size, and speed of an approaching predator. Elephants communicate socially with a variety of sounds including snorting, screaming, trumpeting, roaring and rumbling. Some of the rumbling calls are infrasonic, below the hearing range of humans, and can be heard by other elephants up to away at still times near sunrise and sunset.
Mammals signal by a variety of means. Many give visual anti-predator signals, as when deer and gazelle stot, honestly indicating their fit condition and their ability to escape, or when white-tailed deer and other prey mammals flag with conspicuous tail markings when alarmed, informing the predator that it has been detected. Many mammals make use of scent-marking, sometimes possibly to help defend territory, but probably with a range of functions both within and between species. Microbats and toothed whales including oceanic dolphins vocalize both socially and in echolocation.

To maintain a high constant body temperature is energy expensive—mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals—this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants, which contain complex carbohydrates such as cellulose. An herbivorous diet includes subtypes such as granivory (seed eating), folivory (leaf eating), frugivory (fruit eating), nectarivory (nectar eating), gummivory (gum eating) and mycophagy (fungus eating). The digestive tract of an herbivore is host to bacteria that ferment these complex substances, and make them available for digestion, which are either housed in the multichambered stomach or in a large cecum. Some mammals are coprophagous, consuming feces to absorb the nutrients not digested when the food was first ingested. An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract because the proteins, lipids and minerals found in meat require little in the way of specialized digestion. Exceptions to this include baleen whales who also house gut flora in a multi-chambered stomach, like terrestrial herbivores.

The size of an animal is also a factor in determining diet type (Allen's rule). Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about are mostly insectivorous because they cannot tolerate the slow, complex digestive process of an herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (carnivores that feed on larger vertebrates) or a slower digestive process (herbivores). Furthermore, mammals that weigh more than usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).
Some mammals are omnivores and display varying degrees of carnivory and herbivory, generally leaning in favor of one more than the other. Since plants and meat are digested differently, there is a preference for one over the other, as in bears where some species may be mostly carnivorous and others mostly herbivorous. They are grouped into three categories: mesocarnivory (50–70% meat), hypercarnivory (70% and greater of meat), and hypocarnivory (50% or less of meat). The dentition of hypocarnivores consists of dull, triangular carnassial teeth meant for grinding food. Hypercarnivores, however, have conical teeth and sharp carnassials meant for slashing, and in some cases strong jaws for bone-crushing, as in the case of hyenas, allowing them to consume bones; some extinct groups, notably the Machairodontinae, had saber-shaped canines.

Some physiological carnivores consume plant matter and some physiological herbivores consume meat. From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy. Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification. For example, it is well documented that some ungulates such as giraffes, camels, and cattle, will gnaw on bones to consume particular minerals and nutrients. Also, cats, which are generally regarded as obligate carnivores, occasionally eat grass to regurgitate indigestible material (such as hairballs), aid with hemoglobin production, and as a laxative.

Many mammals, in the absence of sufficient food requirements in an environment, suppress their metabolism and conserve energy in a process known as hibernation. In the period preceding hibernation, larger mammals, such as bears, become polyphagic to increase fat stores, whereas smaller mammals prefer to collect and stash food. The slowing of the metabolism is accompanied by a decreased heart and respiratory rate, as well as a drop in internal temperatures, which can be around ambient temperature in some cases. For example, the internal temperatures of hibernating arctic ground squirrels can drop to , however the head and neck always stay above . A few mammals in hot environments aestivate in times of drought or extreme heat, namely the fat-tailed dwarf lemur ("Cheirogaleus medius").

In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.

Tool use by animals may indicate different levels of learning and cognition. The sea otter uses rocks as essential and regular parts of its foraging behaviour (smashing abalone from rocks or breaking open shells), with some populations spending 21% of their time making tools. Other tool use, such as chimpanzees using twigs to "fish" for termites, may be developed by watching others use tools and may even be a true example of animal teaching. Tools may even be used in solving puzzles in which the animal appears to experience a "Eureka moment". Other mammals that do not use tools, such as dogs, can also experience a Eureka moment.

Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the or exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males.

Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning. The traditional method for measuring this is the mirror test, which determines if an animal possesses the ability of self-recognition. Mammals that have passed the mirror test include Asian elephants (some pass, some do not); chimpanzees; bonobos; orangutans; humans, from 18 months (mirror stage); bottlenose dolphins killer whales; and false killer whales.

 
Eusociality is the highest level of social organization. These societies have an overlap of adult generations, the division of reproductive labor and cooperative caring of young. Usually insects, such as bees, ants and termites, have eusocial behavior, but it is demonstrated in two rodent species: the naked mole-rat and the Damaraland mole-rat.

Presociality is when animals exhibit more than just sexual interactions with members of the same species, but fall short of qualifying as eusocial. That is, presocial animals can display communal living, cooperative care of young, or primitive division of reproductive labor, but they do not display all of the three essential traits of eusocial animals. Humans and some species of Callitrichidae (marmosets and tamarins) are unique among primates in their degree of cooperative care of young. Harry Harlow set up an experiment with rhesus monkeys, presocial primates, in 1958; the results from this study showed that social encounters are necessary in order for the young monkeys to develop both mentally and sexually.

A fission-fusion society is a society that changes frequently in its size and composition, making up a permanent social group called the "parent group". Permanent social networks consist of all individual members of a community and often varies to track changes in their environment. In a fission–fusion society, the main parent group can fracture (fission) into smaller stable subgroups or individuals to adapt to environmental or social circumstances. For example, a number of males may break off from the main group in order to hunt or forage for food during the day, but at night they may return to join (fusion) the primary group to share food and partake in other activities. Many mammals exhibit this, such as primates (for example orangutans and spider monkeys), elephants, spotted hyenas, lions, and dolphins.

Solitary animals defend a territory and avoid social interactions with the members of its species, except during breeding season. This is to avoid resource competition, as two individuals of the same species would occupy the same niche, and to prevent depletion of food. A solitary animal, while foraging, can also be less conspicuous to predators or prey.

In a hierarchy, individuals are either dominant or submissive. A despotic hierarchy is where one individual is dominant while the others are submissive, as in wolves and lemurs, and a pecking order is a linear ranking of individuals where there is a top individual and a bottom individual. Pecking orders may also be ranked by sex, where the lowest individual of a sex has a higher ranking than the top individual of the other sex, as in hyenas. Dominant individuals, or alphas, have a high chance of reproductive success, especially in harems where one or a few males (resident males) have exclusive breeding rights to females in a group. Non-resident males can also be accepted in harems, but some species, such as the common vampire bat ("Desmodus rotundus"), may be more strict.

Some mammals are perfectly monogamous, meaning that they mate for life and take no other partners (even after the original mate's death), as with wolves, Eurasian beavers, and otters. There are three types of polygamy: either one or multiple dominant males have breeding rights (polygyny), multiple males that females mate with (polyandry), or multiple males have exclusive relations with multiple females (polygynandry). It is much more common for polygynous mating to happen, which, excluding leks, are estimated to occur in up to 90% of mammals. Lek mating occurs when males congregate around females and try to attract them with various courtship displays and vocalizations, as in harbor seals.

All higher mammals (excluding monotremes) share two major adaptations for care of the young: live birth and lactation. These imply a group-wide choice of a degree of parental care. They may build nests and dig burrows to raise their young in, or feed and guard them often for a prolonged period of time. Many mammals are K-selected, and invest more time and energy into their young than do r-selected animals. When two animals mate, they both share an interest in the success of the offspring, though often to different extremes. Mammalian females exhibit some degree of maternal aggression, another example of parental care, which may be targeted against other females of the species or the young of other females; however, some mammals may "aunt" the infants of other females, and care for them. Mammalian males may play a role in child rearing, as with tenrecs, however this varies species to species, even within the same genus. For example, the males of the southern pig-tailed macaque ("Macaca nemestrina") do not participate in child care, whereas the males of the Japanese macaque ("M. fuscata") do.

Non-human mammals play a wide variety of roles in human culture. They are the most popular of pets, with tens of millions of dogs, cats and other animals including rabbits and mice kept by families around the world. Mammals such as mammoths, horses and deer are among the earliest subjects of art, being found in Upper Paleolithic cave paintings such as at Lascaux. Major artists such as Albrecht Dürer, George Stubbs and Edwin Landseer are known for their portraits of mammals. Many species of mammals have been hunted for sport and for food; deer and wild boar are especially popular as game animals. Mammals such as horses and dogs are widely raced for sport, often combined with betting on the outcome. There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own. Mammals further play a wide variety of roles in literature, film, mythology, and religion.

Domestic mammals form a large part of the livestock raised for meat across the world. They include (2009) around 1.4 billion cattle, 1 billion sheep, 1 billion domestic pigs, and (1985) over 700 million rabbits. Working domestic animals including cattle and horses have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods.
Mammal skins provide leather for shoes, clothing and upholstery. Wool from mammals including sheep, goats and alpacas has been used for centuries for clothing. Mammals serve a major role in science as experimental animals, both in fundamental biological research, such as in genetics, and in the development of new medicines, which must be tested exhaustively to demonstrate their safety. Millions of mammals, especially mice and rats, are used in experiments each year. A knockout mouse is a genetically modified mouse with an inactivated gene, replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown. A small percentage of the mammals are non-human primates, used in research for their similarity to humans.

Charles Darwin, Jared Diamond and others have noted the importance of domesticated mammals in the Neolithic development of agriculture and of civilization, causing farmers to replace hunter-gatherers around the world. This transition from hunting and gathering to herding flocks and growing crops was a major step in human history. The new agricultural economies, based on domesticated mammals, caused "radical restructuring of human societies, worldwide alterations in biodiversity, and significant changes in the Earth's landforms and its atmosphere... momentous outcomes".

Hybrids are offspring resulting from the breeding of two genetically distinct individuals, which usually will result in a high degree of heterozygosity, though hybrid and heterozygous are not synonymous. The deliberate or accidental hybridizing of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown for economic purposes. Hybrids between different subspecies within a species (such as between the Bengal tiger and Siberian tiger) are known as intra-specific hybrids. Hybrids between different species within the same genus (such as between lions and tigers) are known as interspecific hybrids or crosses. Hybrids between different genera (such as between sheep and goats) are known as intergeneric hybrids. Natural hybrids will occur in hybrid zones, where two populations of species within the same genera or species living in the same or adjacent areas will interbreed with each other. Some hybrids have been recognized as species, such as the red wolf (though this is controversial).

Artificial selection, the deliberate selective breeding of domestic animals, is being used to breed back recently extinct animals in an attempt to achieve an animal breed with a phenotype that resembles that extinct wildtype ancestor. A breeding-back (intraspecific) hybrid may be very similar to the extinct wildtype in appearance, ecological niche and to some extent genetics, but the initial gene pool of that wild type is lost forever with its extinction. As a result, bred-back breeds are at best vague look-alikes of extinct wildtypes, as Heck cattle are of the aurochs.

Purebred wild species evolved to a specific ecology can be threatened with extinction through the process of genetic pollution, the uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the heterosic hybrid species. When new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact, extinction in some species, especially rare varieties, is possible. Interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool. For example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the domestic water buffalo. Such extinctions are not always apparent from a morphological standpoint. Some degree of gene flow is a normal evolutionary process, nevertheless, hybridization threatens the existence of rare species.

The loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. In the Quaternary extinction event, the mass die-off of megafaunal variety coincided with the appearance of humans, suggesting a human influence. One hypothesis is that humans hunted large mammals, such as the woolly mammoth, into extinction. The 2019 "Global Assessment Report on Biodiversity and Ecosystem Services" by IPBES states that the total biomass of wild mammals has declined by 82 percent since the beginning of human civilization.

Various species are predicted to become extinct in the near future, among them the rhinoceros, primates, pangolins, and giraffes. Hunting alone threatens hundreds of mammalian species around the world. Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon rainforest, are being converted to agricultural land for meat production. According to the World Wildlife Fund's 2016 Living Planet Index, global wildlife populations have declined 58% since 1970, primarily due to habitat destruction, over-hunting and pollution. They project that if current trends continue, 67% of wildlife could disappear by 2020. Another influence is over-hunting and poaching, which can reduce the overall population of game animals, especially those located near villages, as in the case of peccaries. The effects of poaching can especially be seen in the ivory trade with African elephants. Marine mammals are at risk from entanglement from fishing gear, notably cetaceans, with discard mortalities ranging from 65,000 to 86,000 individuals annually.

Attention is being given to endangered species globally, notably through the Convention on Biological Diversity, otherwise known as the Rio Accord, which includes 189 signatory countries that are focused on identifying endangered species and habitats. Another notable conservation organization is the IUCN, which has a membership of over 1,200 governmental and non-governmental organizations.

Recent extinctions can be directly attributed to human influences. The IUCN characterizes 'recent' extinction as those that have occurred past the cut-off point of 1500, and around 80 mammal species have gone extinct since that time and 2015. Some species, such as the Père David's deer are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existing elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic.



</doc>
<doc id="18839" url="https://en.wikipedia.org/wiki?curid=18839" title="Music">
Music

Music is an art form and cultural activity whose medium is sound organized in time. General definitions of music include common elements such as pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the "color" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek μουσική ("mousike"; "art of the Muses").
See glossary of musical terminology.

In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as "the harmony of the spheres" and "it is music to my ears" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, "There is no noise, only sound."

The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as "not being music", including Beethoven's "Grosse Fuge" string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions–such as Classical music symphonies from the 1700s and 1800s, through to spontaneously played improvisational music such as jazz, and avant-garde styles of chance-based contemporary music from the 20th and 21st centuries.

Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. For example, it can be hard to draw the line between some early 1980s hard rock and heavy metal. Within the arts, music may be classified as a performing art, a fine art or as an auditory art. Music may be played or sung and heard live at a rock concert or orchestra performance, heard live as part of a dramatic work (a music theater show or opera), or it may be recorded and listened to on a radio, MP3 player, CD player, smartphone or as film score or TV show.

In many cultures, music is an important part of people's way of life, as it plays a key role in religious rituals, rite of passage ceremonies (e.g., graduation and marriage), social activities (e.g., dancing) and cultural activities ranging from amateur karaoke singing to playing in an amateur funk band or singing in a community choir. People may make music as a hobby, like a teen playing cello in a youth orchestra, or work as a professional musician or singer. The music industry includes the individuals who create new songs and musical pieces (such as songwriters and composers), individuals who perform music (which include orchestra, jazz band and rock band musicians, singers and conductors), individuals who record music (music producers and sound engineers), individuals who organize concert tours, and individuals who sell recordings, sheet music, and scores to customers.

The word derives from Greek μουσική ("mousike"; "art of the Muses"). In Greek mythology, the nine Muses were the goddesses who inspired literature, science, and the arts and who were the source of the knowledge embodied in the poetry, song-lyrics, and myths in the Greek culture. According to the "Online Etymological Dictionary", the term "music" is derived from "mid-13c., musike, from Old French "musique" (12c.) and directly from Latin "musica" "the art of music," also including poetry (also [the] source of Spanish "música", Italian "musica", Old High German "mosica", German "Musik", Dutch "muziek", Danish "musik")." This is derived from the "...Greek "mousike (techne)" "(art) of the Muses," from fem. of mousikos "pertaining to the Muses," from Mousa "Muse" (see muse (n.)). Modern spelling [dates] from [the] 1630s. In classical Greece, [the term "music" refers to] any art in which the Muses presided, but especially music and lyric poetry."

Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of sound recording, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of their favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of their favorite songs, which serve as a "self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved."

Amateur musicians can compose or perform music for their own pleasure, and derive their income elsewhere. Professional musicians are employed by a range of institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras.

A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings.

"Composition" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music "score", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music.

Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.

Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as "Aus den sieben Tagen", to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski. A more commonly known example of chance-based music is the sound of wind chimes jingling in a breeze.

The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation also often provides instructions on how to perform the music. For example, the sheet music for a song may state that the song is a "slow blues" or a "fast swing", which indicates the tempo and the genre. To read music notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre.

Written notation varies with style and period of music. In the 2000s, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre (e.g., jazz or country music).

In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument.

Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework or chord progression. Improvisation is the act of instantaneous composition by performers, where compositional techniques are employed with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines and accompaniment parts. In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments and basso continuo keyboard players improvised chord voicings based on figured bass notation. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts. However, in the 20th and early 21st century, as "common practice" Western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role. At the same time, some modern composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances.

Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived.

Music has many different fundamentals or elements. Depending on the definition of "element" being used, these can include: pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form and structure. The elements of music feature prominently in the music curriculums of Australia, UK and US. All three curriculums identify pitch, dynamics, timbre and texture as elements, but the other identified elements of music are far from universally agreed. Below is a list of the three official versions of the "elements of music":

In relation to the UK curriculum, in 2013 the term: "appropriate musical notations" was added to their list of elements and the title of the list was changed from the "elements of music" to the "inter-related dimensions of music". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure and appropriate musical notations.

The phrase "the elements of music" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the "rudimentary elements of music" and the "perceptual elements of music".

In the 1800s, the phrases "the elements of music" and "the rudiments of music" were used interchangeably. The elements described in these documents refer to aspects of music that are needed in order to become a musician, Recent writers such as Estrella seem to be using the phrase "elements of music" in a similar manner. A definition which most accurately reflects this usage is: "the rudimentary principles of an art, science, etc.: the elements of grammar." The UK's curriculum switch to the "inter-related dimensions of music" seems to be a move back to using the rudimentary elements of music.

Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we "hear" music than how we learn to play it or study it. C.E. Seashore, in his book "Psychology of Music", identified four "psychological attributes of sound". These were: "pitch, loudness, time, and timbre" (p. 3). He did not call them the "elements of music" but referred to them as "elemental components" (p. 2). Nonetheless these elemental components link precisely with four of the most common musical elements: "Pitch" and "timbre" match exactly, "loudness" links with dynamics and "time" links with the time-based elements of rhythm, duration and tempo. This usage of the phrase "the elements of music" links more closely with "Webster's New 20th Century Dictionary" definition of an element as: "a substance which cannot be divided into a simpler form by known methods" and educational institutions' lists of elements generally align with this definition as well.

Although writers of lists of "rudimentary elements of music" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area.

Some styles of music place an emphasis on certain of these fundamentals, while others place less emphasis on certain elements. To give one example, while Bebop-era jazz makes use of very complex chords, including altered dominants and challenging chord progressions, with chords changing two or more times per bar and keys changing several times in a tune, funk places most of its emphasis on rhythm and groove, with entire songs based around a vamp on a single chord. While Romantic era classical music from the mid- to late-1800s makes great use of dramatic changes of dynamics, from whispering pianissimo sections to thunderous fortissimo sections, some entire Baroque dance suites for harpsichord from the early 1700s may use a single dynamic. To give another example, while some art music pieces, such as symphonies are very long, some pop songs are just a few minutes long.

Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is "higher" or "lower" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck.
A melody (also called a "tune") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the "white notes" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the "white notes" and "black notes" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F, G and A). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.

Harmony refers to the "vertical" sounds of pitches in music, which means pitches that are played or sung together at the same time to create a chord. Usually this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality ("keys"), which includes most classical music written from 1600 to 1900 and most Western pop, rock and traditional music, the key of a piece determines the scale used, which centres around the "home note" or tonic of the key. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or "home note" of a song may change every four bars or even every two bars.

Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player.

Musical texture is the overall sound of a piece of music or song. The texture of a piece or sing is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One of these layers could be a string section, or another brass. The thickness also is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music:

Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a "thicker" or "denser" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello).

Timbre, sometimes called "color" or "tone color" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently).

The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars.

Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter).

Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements in order to convey "an indication of mood, spirit, character etc." and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music.

In music, form describes how the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA 32 bar form, in which the A sections repeated the same eight bar melody and the B section provided a contrasting melody and/or harmony for 8 bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which is based around a sequence of verse and chorus ("refrain") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues.

In the tenth edition of "The Oxford Companion to Music", Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo.
Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.)

Where a piece cannot readily be broken down into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu, etc. Professor Charles Keil classified forms and formal detail as "sectional, developmental, or variational."

This form is built from a sequence of clear-cut units that may be referred to by letters but also often have generic names such as introduction and coda, exposition, development and recapitulation, verse, chorus or refrain, and bridge. Introductions and codas, when they are no more than that, are frequently excluded from formal analysis. All such units may typically be eight measures long. Sectional forms include:

This form is defined by its "unrelieved repetition" (AAAA...).


Medley, potpourri is the extreme opposite, that of "unrelieved variation": it is simply an indefinite sequence of self-contained sections (ABCD...), sometimes with repeats (AABBCCDD...). Examples include orchestral overtures, which are sometimes no more than a string of the best tunes of the musical theatre show or opera to come.

This form uses two sections (AB...), each often repeated (AABB...). In 18th-century Western classical music, "simple binary" form was often used for dances and carried with it the convention that the two sections should be in different musical keys but same rhythm, duration and tone. The alternation of two tunes gives enough variety to permit a dance to be extended for as long as desired.
b
This form has three parts. In Western classical music a simple ternary form has a third section that is a recapitulation of the first (ABA). Often, the first section is repeated (AABA). This approach was popular in the 18th-century operatic aria, and was called "da capo" (i.e. "repeat from the top") form. Later, it gave rise to the 32-bar song, with the B section then often referred to as the "middle eight". A song has more need than a dance of a self-contained form with a beginning and an end of course.

This form has a recurring theme alternating with different (usually contrasting) sections called "episodes". It may be asymmetrical (ABACADAEA) or symmetrical (ABACABA). A recurring section, especially the main theme, is sometimes more thoroughly varied, or else one episode may be a "development" of it. A similar arrangement is the ritornello form of the Baroque concerto grosso. Arch form (ABCBA) resembles a symmetrical rondo without intermediate repetitions of the main theme. It is normally used in a round.

Variational forms are those in which variation is an important formative element.

Theme and Variations: a theme, which in itself can be of any shorter form (binary, ternary, etc.), forms the only "section" and is repeated indefinitely (as in strophic form) but is varied each time (A, B, A, F, Z, A), so as to make a sort of sectional chain form. An important variant of this, much used in 17th-century British music and in the Passacaglia and Chaconne, was that of the ground bass – a repeating bass theme or "basso ostinato" over and around which the rest of the structure unfolds, often, but not always, spinning polyphonic or contrapuntal threads, or improvising divisions and descants. This is said by Scholes (1977) to be the form "par excellence" of unaccompanied or accompanied solo instrumental music. The Rondo is often found with sections varied (AABACABA) or (ABACABA).

Developmental forms are built directly from smaller units, such as motifs. A well-known Classical piece with a motif is Beethoven's fifth symphony, which starts with three short repeated notes and then a long note. In Classical pieces that are based on motifs, the motif is usually combined, varied and worked out in different ways, perhaps having a symmetrical or arch-like underpinning and a progressive development from beginning to end. By far the most important developmental form in Western classical music is Sonata form.
This form, also known as "sonata" form, first movement form, compound binary, ternary and a variety of other names, developed from the binary-formed dance movement described above but is almost always cast in a greater ternary form having the nominal subdivisions of "Exposition, Development" and "Recapitulation". Usually, but not always, the "A" parts (Exposition and Recapitulation, respectively) may be subdivided into two or three themes or theme groups which are taken asunder and recombined to form the "B" part (the development) – thus e. g. (AabB[dev. of a and/or b]Aab+coda). This developmental form is generally confined to certain sections of the piece, as to the middle section of the first movement of a sonata, though 19th-century composers such as Berlioz, Liszt and Wagner made valiant efforts to derive large-scale works purely or mainly from the motif.

Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. Flutes are often discovered, carved from bones in which lateral holes have been pierced; these are thought to have been blown at one end like the Japanese shakuhachi. The Divje Babe flute, carved from a cave bear femur, is thought to be at least 40,000 years old. Instruments such as the seven-holed flute and various types of stringed instruments, such as the Ravanahatha, have been recovered from the Indus Valley Civilization archaeological sites. India has one of the oldest musical traditions in the world—references to Indian classical music ("marga") are found in the Vedas, ancient scriptures of the Hindu tradition. The earliest and largest collection of prehistoric musical instruments was found in China and dates back to between 7000 and 6600 BC. The "Hurrian Hymn to Nikkal", found on clay tablets that date back to approximately 1400 BC, is the oldest surviving notated work of music.

The ancient Egyptians credited one of their gods, Thoth, with the invention of music, with Osiris in turn used as part of his effort to civilize the world. The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi "dhikr" rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments.

Indian classical music is one of the oldest musical traditions in the world. The Indus Valley civilization has sculptures that show dance and old musical instruments, like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler. The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. "Silappadhikaram" by Ilango Adigal provides information about how new scales can be formed by modal shifting of the tonic from an existing scale. Hindi music was influenced by the Persian performance practices of the Afghan Mughals. Carnatic music, popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are also many songs emphasising love and other social issues.

Asian music covers the music cultures of Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Chinese classical music, the traditional art or court music of China, has a history stretching over around three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music. Persian music is the music of Persia and Persian language countries: "musiqi", the science and art of music, and "muzik", the sound and performance of music (Sakata 1983).

Music and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture have discovered common links in theatrical and musical activity between the classical cultures of the Hebrews and those of later Greeks and Romans. The common area of performance is found in a "social phenomenon called litany," a form of prayer consisting of a series of invocations or supplications. "The Journal of Religion and Theatre" notes that among the earliest forms of litany, "Hebrew litany was accompanied by a rich musical tradition:"

Music was an important part of social and cultural life in ancient Greece. Musicians and singers played a prominent role in Greek theater. Mixed-gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed "aulos" and a plucked string instrument, the "lyre", principally the special kind called a "kithara". Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created a flowering of music development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world. The oldest surviving work written on the subject of music theory is "Harmonika Stoicheia" by Aristoxenus.

The medieval era (476 to 1400), which took place during the Middle Ages, started with the introduction of monophonic (single melodic line) chanting into Roman Catholic Church services. Musical notation was used since Ancient times in Greek culture, but in the Middle Ages, notation was first introduced by the Catholic church so that the chant melodies could be written down, to facilitate the use of the same melodies for religious music across the entire Catholic empire. The only European Medieval repertory that has been found in written form from before 800 is the monophonic liturgical plainsong chant of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin, Guillaume de Machaut, and Walther von der Vogelweide.

Renaissance music (c. 1400 to 1600) was more focused on secular (non-religious) themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the printing press, all notated music was hand-copied). The increased availability of sheet music helped to spread musical styles more quickly and across a larger area. Musicians and singers often worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Dufay, Giovanni Pierluigi da Palestrina, Thomas Morley, and Orlande de Lassus. As musical activity shifted from the church to the aristocratic courts, kings, queens and princes competed for the finest composers. Many leading important composers came from the Netherlands, Belgium, and northern France. They are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain.

The Baroque era of music took place from 1600 to 1750, as the Baroque artistic style flourished across Europe; and during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the Medieval era). German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach, George Frideric Handel, Georg Philipp Telemann and Antonio Lucio Vivaldi.

The music of the Classical period (1730 to 1820) aimed to imitate what were seen as the key elements of the art and philosophy of Ancient Greece and Rome: the ideals of balance, proportion and disciplined expression. (Note: the music from the Classical period should not be confused with Classical music in general, a term which refers to Western art music from the 5th century to the 2000s, which includes the Classical period as one of a number of periods). Music from the Classical period has a lighter, clearer and considerably simpler texture than the Baroque music which preceded it. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct. Classical instrumental melodies tended to be almost voicelike and singable. New genres were developed, and the fortepiano, the forerunner to the modern piano, replaced the Baroque era harpsichord and pipe organ as the main keyboard instrument.

Importance was given to instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Others main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata. The instruments used chamber music and orchestra became more standardized. In place of the basso continuo group of the Baroque era, which consisted of harpsichord, organ or lute along with a number of bass instruments selected at the discretion of the group leader (e.g., viol, cello, theorbo, serpent), Classical chamber groups used specified, standardized instruments (e.g., a string quartet would be performed by two violins, a viola and a cello). The Baroque era improvised chord-playing of the continuo keyboardist or lute player was gradually phased out between 1750 and 1800.

One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres such as opera and oratorio became more popular.

The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism.

Romantic music (c. 1810 to 1900) from the 19th century had many elements in common with the Romantic styles in literature and painting of the era. Romanticism was an artistic, literary, and intellectual movement was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature. Romantic music expanded beyond the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces and songs. Romantic composers such as Wagner and Brahms attempted to increase emotional expression and power in their music to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. The emotional and expressive qualities of music came to take precedence over tradition.

Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, new approaches were explored for existing genres, forms, and functions. Also, new forms were created that were deemed better suited to the new subject matter. Composers continued to develop opera and ballet music, exploring new styles and themes.

In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colours". The late 19th century saw a dramatic expansion in the size of the orchestra, and the industrial revolution helped to create better instruments, creating a more powerful sound. Public concerts became an important part of well-to-do urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre.

In the 19th century, one of the key ways that new compositions became known to the public was by the sales of sheet music, which middle class amateur music lovers would perform at home on their piano or other common instruments, such as violin. With 20th-century music, the invention of new electric technologies such as radio broadcasting and the mass market availability of gramophone records meant that sound recordings of songs and pieces heard by listeners (either on the radio or on their record player) became the main way to learn about new songs and pieces. There was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music, because whereas in the 19th century, the focus on sheet music restricted access to new music to the middle class and upper-class people who could read music and who owned pianos and instruments, in the 20th century, anyone with a radio or record player could hear operas, symphonies and big bands right in their own living room. This allowed lower-income people, who would never be able to afford an opera or symphony concert ticket to hear this music. It also meant that people could hear music from different parts of the country, or even different parts of the world, even if they could not afford to travel to these locations. This helped to spread musical styles.

The focus of art music in the 20th century was characterized by exploration of new rhythms, styles, and sounds. The horrors of World War I influenced many of the arts, including music, and some composers began exploring darker, harsher sounds. Traditional music styles such as jazz and folk music were used by composers as a source of ideas for classical music. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenre of classical music, including the acousmatic and Musique concrète schools of electronic composition. Sound recording was also a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do much more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance.

Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half of that century, rock music did the same. Jazz is an American musical artform that originated in the beginning of the 20th century in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note.

Rock music is a genre of popular music that developed in the 1960s from 1950s rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric guitar or acoustic guitar, and it uses a strong back beat laid down by a rhythm section of electric bass guitar, drums, and keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers and digital ones and computers since the 1990s. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form," it "has three chords, a strong, insistent back beat, and a catchy melody."

Performance is the physical expression of music, which occurs when a song is sung or when a piano piece, electric guitar melody, symphony, drum beat or other musical part is played by musicians. In classical music, a musical work is written in music notation by a composer and then it is performed once the composer is satisfied with its structure and instrumentation. However, as it gets performed, the interpretation of a song or piece can evolve and change. In classical music, instrumental performers, singers or conductors may gradually make changes to the phrasing or tempo of a piece. In popular and traditional music, the performers have a lot more freedom to make changes to the form of a song or piece. As such, in popular and traditional music styles, even when a band plays a cover song, they can make changes to it such as adding a guitar solo to or inserting an introduction.

A performance can either be planned out and rehearsed (practiced)—which is the norm in classical music, with jazz big bands and many popular music styles–or improvised over a chord progression (a sequence of chords), which is the norm in small jazz and blues groups. Rehearsals of orchestras, concert bands and choirs are led by a conductor. Rock, blues and jazz bands are usually led by the bandleader. A rehearsal is a structured repetition of a song or piece by the performers until it can be sung and/or played correctly and, if it is a song or piece for more than one musician, until the parts are together from a rhythmic and tuning perspective. Improvisation is the creation of a musical idea–a melody or other musical line–created on the spot, often based on scales or pre-existing melodic riffs.

Many cultures have strong traditions of solo performance (in which one singer or instrumentalist performs), such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing to highly planned and organised performances such as the modern classical concert, religious processions, classical music festivals or music competitions. Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than large symphonic works.

Many types of music, such as traditional blues and folk music were not written down in sheet music; instead, they were originally preserved in the memory of performers, and the songs were handed down orally, from one musician or singer to another, or aurally, in which a performer learns a song "by ear". When the composer of a song or piece is no longer known, this music is often classified as "traditional" or as a "folk song". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history and stories may also be passed on by ear through song.

In music, an "ornament" is a decoration to a melody, bassline or other musical part. The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th centuries required performers to have a great deal of contextual knowledge about performing styles. For example, in the 17th and 18th centuries, music notated for solo performers typically indicated a simple, unadorned melody. However, performers were expected to know how to add stylistically appropriate ornaments to add interest to the music, such as trills and turns.

In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. The performer was expected to know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece.

Philosophy of music is a subfield of philosophy. The philosophy of music is the study of fundamental questions regarding music. The philosophical study of music has many connections with philosophical questions in metaphysics and aesthetics.
Some basic questions in the philosophy of music are:


In ancient times, such as with the Ancient Greeks, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the 18th century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment ("plaisir" and "jouissance") of music. The origin of this philosophic shift is sometimes attributed to Alexander Gottlieb Baumgarten in the 18th century, followed by Immanuel Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present-day connotation. In the 2000s, philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been a central issue.

In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner regarding whether music can express meaning. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Also many modern composers like La Monte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation.

It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in "The Republic" that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state (Book VII).

There has been a strong tendency in the aesthetics of music to emphasize the paramount importance of compositional structure; however, other issues concerning the aesthetics of music include lyricism, harmony, hypnotism, emotiveness, temporal dynamics, resonance, playfulness, and color (see also musical development).

Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.

Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET).

Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science.

This interdisciplinary field investigates topics such as the parallels between language and music in the brain. Biologically inspired models of computation are often included in research, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated.

Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics.

Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers.

An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music.

Many ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats.

Other types of music—including, but not limited to, jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls.

However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomics standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.

When composers introduce styles of music that break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.

Women have played a major role in music throughout history, as composers, songwriters, instrumental performers, singers, conductors, music scholars, music educators, music critics/music journalists and other musical professions. As well, it describes music movements, events and genres related to women, women's issues and feminism. In the 2010s, while women comprise a significant proportion of popular music and classical music singers, and a significant proportion of songwriters (many of them being singer-songwriters), there are few women record producers, rock critics and rock instrumentalists. Although there have been a huge number of women composers in classical music, from the Medieval period to the present day, women composers are significantly underrepresented in the commonly performed classical music repertoire, music history textbooks and music encyclopedias; for example, in the "Concise Oxford History of Music", Clara Schumann is one of the only female composers who is mentioned.

Women comprise a significant proportion of instrumental soloists in classical music and the percentage of women in orchestras is increasing. A 2015 article on concerto soloists in major Canadian orchestras, however, indicated that 84% of the soloists with the Orchestre Symphonique de Montreal were men. In 2012, women still made up just 6% of the top-ranked Vienna Philharmonic orchestra. Women are less common as instrumental players in popular music genres such as rock and heavy metal, although there have been a number of notable female instrumentalists and all-female bands. Women are particularly underrepresented in extreme metal genres. In the 1960s pop-music scene, "[l]ike most aspects of the...music business, [in the 1960s,] songwriting was a male-dominated field. Though there were plenty of female singers on the radio, women ...were primarily seen as consumers:... Singing was sometimes an acceptable pastime for a girl, but playing an instrument, writing songs, or producing records simply wasn't done." Young women "...were not socialized to see themselves as people who create [music]."

Women are also underrepresented in orchestral conducting, music criticism/music journalism, music producing, and sound engineering. While women were discouraged from composing in the 19th century, and there are few women musicologists, women became involved in music education "...to such a degree that women dominated [this field] during the later half of the 19th century and well into the 20th century."

According to Jessica Duchen, a music writer for London's "The Independent", women musicians in classical music are "...too often judged for their appearances, rather than their talent" and they face pressure "...to look sexy onstage and in photos." Duchen states that while "[t]here are women musicians who refuse to play on their looks...the ones who do tend to be more materially successful."
According to the UK's Radio 3 editor, Edwina Wolstencroft, the music industry has long been open to having women in performance or entertainment roles, but women are much less likely to have positions of authority, such as being the leader of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process. One of the most recorded artists is Asha Bhosle, an Indian singer best known as a playback singer in Hindi cinema.

The music that composers make can be heard through several media; the most traditional way is to hear it live, in the presence of the musicians (or as one of the musicians), in an outdoor or indoor space such as an amphitheatre, concert hall, cabaret room or theatre. Since the 20th century, live music can also be broadcast over the radio, television or the Internet, or recorded and listened to on a CD player or Mp3 player. Some musical styles focus on producing a sound for a performance, while others focus on producing a recording that mixes together sounds that were never played "live." Recording, even of essentially live styles such as rock, often uses the ability to edit and splice to produce recordings that may be considered "better" than the actual performance.

Technology has had an influence on music since prehistoric times, when cave people used simple tools to bore holes into bone flutes 41,000 years ago. Technology continued to influence music throughout the history of music, as it enabled new instruments and music notation reproduction systems to be used, with one of the watershed moments in music notation being the invention of the printing press in the 1400s, which meant music scores no longer had to be hand copied. In the 19th century, music technology led to the development of a more powerful, louder piano and led to the development of new valves brass instruments. In the early 20th century (in the late 1920s), as talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s, live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the "Pittsburgh Press" features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever"

Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and Internet in a form that is commonly known as Music-On-Demand.

In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often communal. In industrialized countries, listening to music through a recorded form, such as sound recording or watching a music video, became more common than experiencing live performance, roughly in the middle of the 20th century.

Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also "become" performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.

The advent of the Internet and widespread high-speed broadband access has transformed the experience of music, partly through the increased ease of access to recordings of music via streaming video and vastly increased choice of music for consumers. Chris Anderson, in his book "", suggests that while the traditional economic model of supply and demand describes scarcity, the Internet retail model is based on abundance. Digital storage costs are low, so a company can afford to make its whole recording inventory available online, giving customers as much choice as possible. It has thus become economically viable to offer music recordings that very few people are interested in. Consumers' growing awareness of their increased choice results in a closer association between listening tastes and social identity, and the creation of thousands of niche markets.

Another effect of the Internet arose with online communities and social media websites like YouTube and Facebook, a social networking service. These sites make it easier for aspiring singers and amateur bands to distribute videos of their songs, connect with other musicians, and gain audience interest. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book "Wikinomics", there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates content and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans.

The music industry refers to the businesses connected with the creation and sale of music. It consists of songwriters and composers who create new songs and musical pieces, music producers and sound engineers who record songs and pieces, record labels and publishers that distribute recorded music products and sheet music internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups. In the 2000s, the increasing popularity of listening to music as digital music files on MP3 players, iPods, or computers, and of trading music on file sharing websites or buying it online in the form of digital files had a major impact on the traditional music business. Many smaller independent CD stores went out of business as music buyers decreased their purchases of CDs, and many labels had lower CD sales. Some companies did well with the change to a digital format, though, such as Apple's iTunes, an online music store that sells digital files of songs over the Internet.

In spite of some international copyright treaties, determining which music is in the public domain is complicated by of national copyright laws that may be applicable. US copyright law formerly protected printed music published after 1923 for 28 years and with renewal for another 28 years, but the Copyright Act of 1976 made renewal automatic, and the Digital Millennium Copyright Act changed the calculation of the copyright term to 70 years after the death of the creator. Recorded sound falls under mechanical licensing, often covered by a confusing patchwork of state laws; most cover versions are licensed through the Harry Fox Agency. Performance rights may be obtained by either performers or the performance venue; the two major organizations for licensing are BMI and ASCAP. Two online sources for public domain music are IMSLP (International Music Score Library Project) and Choral Public Domain Library (CPDL).

The incorporation of some music or singing training into general education from preschool to post secondary education is common in North America and Europe. Involvement in playing and singing music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music and traditional music. Some elementary school children also learn about popular music styles. In religious schools, children sing hymns and other religious music. In secondary schools (and less commonly in elementary schools), students may have the opportunity to perform in some types of musical ensembles, such as choirs (a group of singers), marching bands, concert bands, jazz bands, or orchestras. In some school systems, music lessons on how to play instruments may be provided. Some students also take private music lessons after school with a singing teacher or instrument teacher. Amateur musicians typically learn basic musical rudiments (e.g., learning about musical notation for musical scales and rhythms) and beginner- to intermediate-level singing or instrument-playing techniques.

At the university level, students in most arts and humanities programs can receive credit for taking a few music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some types of musical ensembles that students in arts and humanities are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).

Individuals aiming to become professional musicians, singers, composers, songwriters, music teachers and practitioners of other music-related professions such as music history professors, sound engineers, and so on study in specialized post-secondary programs offered by colleges, universities and music conservatories. Some institutions that train individuals for careers in music offer training in a wide range of professions, as is the case with many of the top U.S. universities, which offer degrees in music performance (including singing and playing instruments), music history, music theory, music composition, music education (for individuals aiming to become elementary or high school music teachers) and, in some cases, conducting. On the other hand, some small colleges may only offer training in a single profession (e.g., sound recording).

While most university and conservatory music programs focus on training students in classical music, there are a number of universities and colleges that train musicians for careers as jazz or popular music musicians and composers, with notable U.S. examples including the Manhattan School of Music and the Berklee College of Music. Two important schools in Canada which offer professional jazz training are McGill University and Humber College. Individuals aiming at careers in some types of music, such as heavy metal music, country music or blues are less likely to become professionals by completing degrees or diplomas in colleges or universities. Instead, they typically learn about their style of music by singing and/or playing in many bands (often beginning in amateur bands, cover bands and tribute bands), studying recordings available on CD, DVD and the Internet and working with already-established professionals in their style of music, either through informal mentoring or regular music lessons. Since the 2000s, the increasing popularity and availability of Internet forums and YouTube "how-to" videos have enabled many singers and musicians from metal, blues and similar genres to improve their skills. Many pop, rock and country singers train informally with vocal coaches and singing teachers.

Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take about four years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program. Graduates of undergraduate music programs can seek employment or go on to further study in music graduate programs. Bachelor's degree graduates are also eligible to apply to some graduate programs and professional schools outside of music (e.g., public administration, business administration, library science, and, in some jurisdictions, teacher's college, law school or medical school).

Graduate music degrees include the Master of Music, the Master of Arts (in musicology, music theory or another music field), the Doctor of Philosophy (Ph.D.) (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice (singing) or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, music theory or ethnomusicology.

The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The DMA is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions.

Musicology, the academic study of the subject of music, is studied in universities and music conservatories. The earliest definitions from the 19th century defined three sub-disciplines of musicology: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In 2010-era scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-Western cultures, and the cultural study of music, is called ethnomusicology. Students can pursue the undergraduate study of musicology, ethnomusicology, music history, and music theory through several different types of degrees, including bachelor's degrees, master's degrees and PhD degrees.

Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. "Speculative music theory", contrasted with "analytic music theory", is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.

Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's "Musique, mythe, nature, ou les Dauphins d'Arion" (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's "Langage, musique, poésie" (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."

In the West, much of the history of music that is taught deals with the Western civilization's art music, which is known as classical music. The history of music in non-Western cultures ("world music" or the field of "ethnomusicology"), which typically covers music from
Africa and Asia is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular or folk styles of music in non-Western countries varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, techniques, singing styles and uses for music. Music has been used for entertainment, ceremonies, rituals, religious purposes and for practical and artistic communication. Non-Western music has also been used for propaganda purposes, as was the case with Chinese opera during the Cultural Revolution.

There is a host of music classifications for non-Western music, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including non-Western styles of rock, country, and pop music-related styles). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz-related music).

As world cultures have come into greater global contact, their indigenous musical styles have often merged with other styles, which produces new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic "melting pot" society. Some types of world music contain a mixture of non-Western indigenous styles with Western pop music elements. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's "Rhapsody in Blue", are claimed by both jazz and classical music, while Gershwin's "Porgy and Bess" and Leonard Bernstein's "West Side Story" are claimed by both opera and the Broadway musical tradition. Many current music festivals for non-Western music include bands and singers from a particular musical genre, such as world music.

Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India.

Music therapy is an interpersonal process in which a trained therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical disabilities, sensory impairments, developmental disabilities, substance abuse issues, communication disorders, interpersonal problems, and aging. It is also used to improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities. Music therapists may encourage clients to sing, play instruments, create songs, or do other musical activities.

One of the earliest mentions of music therapy was in Al-Farabi's (c. 872–950) treatise "Meanings of the Intellect", which described the therapeutic effects of music on the soul. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's "The Anatomy of Melancholy" argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In the Ottoman Empire, mental illnesses were treated with music. In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients.

Albert Einstein had a lifelong love of music (particularly the works of Bach and Mozart), once stating that life without playing music would be inconceivable to him. In some interviews Einstein even attributed much of his scientific intuition to music, with his son Hans recounting that "whenever he felt that he had come to the end of the road or into a difficult situation in his work, he would take refuge in music, and that would usually resolve all his difficulties." Something in the music, according to Michele and Robert Root-Bernstein in "Psychology Today", "would guide his thoughts in new and creative directions." It has been said that Einstein considered Mozart's music to reveal a universal harmony that Einstein believed existed in the universe, "as if the great Wolfgang Amadeus did not 'create' his beautifully clear music at all, but simply discovered it already made. This perspective parallels, remarkably, Einstein’s views on the ultimate simplicity of nature and its explanation and statement via essentially simple mathematical expressions." A review suggests that music may be effective for improving subjective sleep quality in adults with insomnia symptoms. Music is also being used in clinical rehabilitation of cognitive and motor disorders.





</doc>
<doc id="18842" url="https://en.wikipedia.org/wiki?curid=18842" title="Mode">
Mode

Mode ( meaning "manner, tune, measure, due measure, rhythm, melody") may refer to:











</doc>
<doc id="18845" url="https://en.wikipedia.org/wiki?curid=18845" title="Mouse">
Mouse

A mouse, plural mice, is a small rodent characteristically having a pointed snout, small rounded ears, a body-length scaly tail, and a high breeding rate. The best known mouse species is the common house mouse ("Mus musculus"). It is also a popular pet. In some places, certain kinds of field mice are locally common. They are known to invade homes for food and shelter.

Species of mice are mostly classified in Rodentia, and are present throughout the order. Typical mice are classified in the genus Mus.

Mice are typically distinguished from rats by their size. Generally, when someone discovers a smaller muroid rodent, its common name includes the term "mouse", while if it is larger, the name includes the term "rat". Common terms "rat" and "mouse" are not taxonomically specific. Scientifically, the term "mouse" is not confined to members of "Mus" for example, but also applies to species from other genera such as the deer mouse, "Peromyscus".

Domestic mice sold as pets often differ substantially in size from the common house mouse. This is attributable both to breeding and to different conditions in the wild. The best-known strain, the white lab mouse, has more uniform traits that are appropriate to its use in research.

Cats, wild dogs, foxes, birds of prey, snakes and even certain kinds of arthropods have been known to prey heavily upon mice. Nevertheless, because of its remarkable adaptability to almost any environment, the mouse is one of the most successful mammalian genera living on Earth today.

Mice, in certain contexts, can be considered vermin which are a major source of crop damage, causing structural damage and spreading diseases through their parasites and feces. In North America, breathing dust that has come in contact with mouse excrement has been linked to hantavirus, which may lead to hantavirus pulmonary syndrome (HPS).

Primarily nocturnal animals, mice compensate for their poor eyesight with a keen sense of hearing, and rely especially on their sense of smell to locate food and avoid predators.

Mice build long intricate burrows in the wild. These typically have long entrances and are equipped with escape tunnels or routes. In at least one species, the architectural design of a burrow is a genetic trait.

The most common mice are murines, in the same clade as common rats. They are murids, along with gerbils and other close relatives. 

Mice are common experimental animals in laboratory research of biology and psychology fields primarily because they are mammals, and also because they share a high degree of homology with humans. They are the most commonly used mammalian model organism, more common than rats. The mouse genome has been sequenced, and virtually all mouse genes have human homologs. The mouse has approximately 2.7 billion base pairs and 20 pairs of chromosomes.
They can also be manipulated in ways that are illegal with humans, although animal rights activists often object. A knockout mouse is a genetically modified mouse that has had one or more of its genes made inoperable through a gene knockout.

Reasons for common selection of mice are that they are small and inexpensive, have a widely varied diet, are easily maintained, and can reproduce quickly. Several generations of mice can be observed in a relatively short time. Mice are generally very docile if raised from birth and given sufficient human contact. However, certain strains have been known to be quite temperamental. Mice and rats have the same organs in the same places, with the difference of size.

Many people buy mice as companion pets. They can be playful, loving and can grow used to being handled. Like pet rats, pet mice should not be left unsupervised outside as they have many natural predators, including (but not limited to) birds, snakes, lizards, cats, and dogs. Male mice tend to have a stronger odor than the females. However, mice are careful groomers and as pets they never need bathing. Well looked-after mice can make ideal pets. Some common mouse care products are:

In nature, mice are largely herbivores, consuming any kind of fruit or grain from plants. However, mice adapt well to urban areas and are known for eating almost all types of food scraps. In captivity, mice are commonly fed commercial pelleted mouse diet. These diets are nutritionally complete, but they still need a large variety of vegetables.

Mice are a staple in the diet of many small carnivores. Humans have eaten mice since prehistoric times and still eat them as a delicacy throughout eastern Zambia and northern Malawi, where they are a seasonal source of protein. Mice are no longer routinely consumed by humans elsewhere. However, in Victorian Britain, fried mice were still given to children as a folk remedy for bed-wetting; while Jared Diamond reports creamed mice being used in England as a dietary supplement during W. W. II rationing.

Prescribed cures in Ancient Egypt included mice as medicine. In Ancient Egypt, when infants were ill, mice were eaten as treatment by their mothers. It was believed that mouse eating by the mother would help heal the baby who was ill.

In various countries mice are used as food for pets such as snakes, lizards, frogs, tarantulas and birds of prey, and many pet stores carry mice for this purpose.

Common terms used to refer to different ages/sizes of mice when sold for pet food are "pinkies", "fuzzies", "crawlers", "hoppers", and "adults". Pinkies are newborn mice that have not yet grown fur; fuzzies have some fur but are not very mobile; hoppers have a full coat of hair and are fully mobile but are smaller than adult mice. Mice without fur are easier for the animal to consume; however, mice with fur may be more convincing as animal feed. These terms are also used to refer to the various growth stages of rats (see Fancy rat).




</doc>
<doc id="18847" url="https://en.wikipedia.org/wiki?curid=18847" title="Multics">
Multics

Multics (Multiplexed Information and Computing Service) is an influential early time-sharing operating system which is based on the concept of a single-level memory. Virtually all modern operating systems were heavily influenced by Multics – often through Unix, which was created by some of the people who had worked on Multics – either directly (Linux and macOS) or indirectly (Windows NT).

Initial planning and development for Multics started in 1964, in Cambridge, Massachusetts. Originally it was a cooperative project led by MIT (Project MAC with Fernando Corbató) along with General Electric and Bell Labs. It was developed on the GE 645 computer, which was specially designed for it; the first one was delivered to MIT in January, 1967.

Multics was conceived as a commercial product for General Electric, and became one for Honeywell, albeit not very successfully. Due to its many novel and valuable ideas, Multics had a significant impact on computer science despite its faults.

Multics had numerous features intended to ensure high availability so that it would support a computing utility similar to the telephone and electricity utilities. Modular hardware structure and software architecture were used to achieve this. The system could grow in size by simply adding more of the appropriate resource, be it computing power, main memory, or disk storage. Separate access control lists on every file provided flexible information sharing, but complete privacy when needed. Multics had a number of standard mechanisms to allow engineers to analyze the performance of the system, as well as a number of adaptive performance optimization mechanisms.

Multics implemented a single-level store for data access, discarding the clear distinction between files (called "segments" in Multics) and "process memory". The memory of a process consisted solely of segments that were mapped into its address space. To read or write to them, the process simply used normal central processing unit (CPU) instructions, and the operating system took care of making sure that all the modifications were saved to disk. In POSIX terminology, it was as if every file were codice_1ed; however, in Multics there was no concept of "process memory", separate from the memory used to hold mapped-in files, as Unix has. "All" memory in the system was part of "some" segment, which appeared in the file system; this included the temporary scratch memory of the process, its kernel stack, etc.

One disadvantage of this was that the size of segments was limited to 256 kilowords, just over 1 MiB. This was due to the particular hardware architecture of the machines on which Multics ran, having a 36-bit word size and index registers (used to address within segments) of half that size (18 bits). Extra code had to be used to work on files larger than this, called multisegment files. In the days when one megabyte of memory was prohibitively expensive, and before large databases and later huge bitmap graphics, this limit was rarely encountered.

Another major new idea of Multics was dynamic linking, in which a running process could request that other segments be added to its address space, segments which could contain code that it could then execute. This allowed applications to automatically use the latest version of any external routine they called, since those routines were kept in other segments, which were dynamically linked only when a process first tried to begin execution in them. Since different processes could use different search rules, different users could end up using different versions of external routines automatically. Equally importantly, with the appropriate settings on the Multics security facilities, the code in the other segment could then gain access to data structures maintained in a different process.

Thus, to interact with an application running in part as a daemon (in another process), a user's process simply performed a normal procedure-call instruction to a code segment to which it had dynamically linked (a code segment that implemented some operation associated with the daemon). The code in that segment could then modify data maintained and used in the daemon. When the action necessary to commence the request was completed, a simple procedure return instruction returned control of the user's process to the user's code.

Multics also supported extremely aggressive on-line reconfiguration: central processing units, memory banks, disk drives, etc. could be added and removed while the system continued operating. At the MIT system, where most early software development was done, it was common practice to split the multiprocessor system into two separate systems during off-hours by incrementally removing enough components to form a second working system, leaving the rest still running the original logged-in users. System software development testing could be done on the second system, then the components of the second system were added back to the main user system, without ever having shut it down. Multics supported multiple CPUs; it was one of the earliest multiprocessor systems.

Multics was the first major operating system to be designed as a secure system from the outset. Despite this, early versions of Multics were broken into repeatedly. This led to further work that made the system much more secure and prefigured modern security engineering techniques. Break-ins became very rare once the second-generation hardware base was adopted; it had hardware support for ring-oriented security, a multilevel refinement of the concept of master mode.

Multics was the first operating system to provide a hierarchical file system, and file names could be of almost arbitrary length and syntax. A given file or directory could have multiple names (typically a long and short form), and symbolic links between directories were also supported. Multics was the first to use the now-standard concept of per-process stacks in the kernel, with a separate stack for each security ring. It was also the first to have a command processor implemented as ordinary user code – an idea later used in the Unix shell. It was also one of the first written in a high-level language (Multics PL/I), after the Burroughs MCP system written in ALGOL.

In 1964, Multics was developed initially for the GE-645 mainframe, a 36-bit system. GE's computer business, including Multics, was taken over by Honeywell in 1970; around 1973, Multics was supported on the Honeywell 6180 machines, which included security improvements including hardware support for protection rings.

Bell Labs pulled out of the project in 1969; some of the people who had worked on it there went on to create the Unix system. Multics development continued at MIT and General Electric.

Honeywell continued system development until 1985. About 80 multimillion-dollar sites were installed, at universities, industry, and government sites. The French university system had several installations in the early 1980s. After Honeywell stopped supporting Multics, users migrated to other systems like Unix.

In 1985, Multics was issued certification as a B2 level secure operating system using the Trusted Computer System Evaluation Criteria from the National Computer Security Center (NCSC) a division of the NSA, the first operating system evaluated to this level.

Multics was distributed from 1975 to 2000 by Groupe Bull in Europe, and by Bull HN Information Systems Inc. in the United States. In 2006, Bull SAS open sourced Multics versions MR10.2, MR11.0, MR12.0, MR12.1, MR12.2, MR12.3, MR12.4 & MR12.5.

The last known Multics installation running natively on Honeywell hardware was shut down on October 30, 2000, at the Canadian Department of National Defence in Halifax, Nova Scotia, Canada.

In 2006 Bull HN released the source code for MR12.5, the final 1992 Multics release, to MIT. Most of the system is now available as open-source software with the exception of some optional pieces such as TCP/IP.

In 2014 Multics was successfully run on current hardware using a emulator. The 1.0 release of the emulator is now available. Release 12.6f of Multics accompanies the 1.0 release of the emulator, and adds a few new features, including command line recall and editing using the video system.

Peter H. Salus, author of a book covering Unix's early years, stated one position: "With Multics they tried to have a much more versatile and flexible operating system, and it failed miserably". This position, however, has been widely discredited in the computing community because many of Multics' technical innovations are used in modern commercial computing systems.

The permanently resident kernel of Multics, a system derided in its day as being too large and complex, was only 135 KB of code. In comparison, a Linux system in 2007 might have occupied 18 MB. The first MIT GE-645 had 512 kilowords of memory (2 MiB), a truly enormous amount at the time, and the kernel used only a moderate portion of Multics main memory.

The entire system, including the operating system and the complex PL/1 compiler, user commands, and subroutine libraries, consisted of about 1500 source modules. These averaged roughly 200 lines of source code each, and compiled to produce a total of roughly 4.5 MiB of procedure code, which was fairly large by the standards of the day.

Multics compilers generally optimised more for code density than CPU performance, for example using small sub-routines called "operators" for short standard code sequences, which makes comparison of object code size with modern systems less useful. High code density was a good optimisation choice for Multics as a multi-user system with expensive main memory.

The design and features of Multics greatly influenced the Unix operating system, which was originally written by two Multics programmers, Ken Thompson and Dennis Ritchie. Superficial influence of Multics on Unix is evident in many areas, including the naming of some commands. But the internal design philosophy was quite different, focusing on keeping the system small and simple, and so correcting some deficiencies of Multics because of its high resource demands on the limited computer hardware of the time.

The name "Unix" (originally "Unics") is itself a pun on "Multics". The "U" in Unix is rumored to stand for "uniplexed" as opposed to the "multiplexed" of Multics, further underscoring the designers' rejections of Multics' complexity in favor of a more straightforward and workable approach for smaller computers. (Garfinkel and Abelson cite an alternative origin: Peter Neumann at Bell Labs, watching a demonstration of the prototype, suggested the pun name UNICS – pronounced "eunuchs" – as a "castrated Multics", although Dennis Ritchie is said to have denied this.)

Ken Thompson, in a transcribed 2007 interview with Peter Seibel refers to Multics as "overdesigned and overbuilt and over everything. It was close to unusable. They [Massachusetts Institute of Technology] still claim it's a monstrous success, but it just clearly wasn't". He admitted however, that "the things that I liked enough (about Multics) to actually take were the hierarchical file system and the shell — a separate process that you can replace with some other process".

The Prime Computer operating system, PRIMOS, was referred to as "Multics in a shoebox" by William Poduska, a founder of the company. Poduska later moved on to found Apollo Computer, whose AEGIS and later Domain/OS operating systems, sometimes called "Multics in a matchbox", extended the Multics design to a heavily networked graphics workstation environment.

The Stratus VOS operating system of Stratus Computer (now Stratus Technologies) was very strongly influenced by Multics, and both its external user interface and internal structure bear many close resemblances to the older project. The high-reliability, availability, and security features of Multics were extended in Stratus VOS to support a new line of fault tolerant computer systems supporting secure, reliable transaction processing. Stratus VOS is the most directly-related descendant of Multics still in active development and production usage today.

The protection architecture of Multics, restricting the ability of code at one level of the system to access resources at another, was adopted as the basis for the security features of ICL's VME operating system.


The literature contains a large number of papers about Multics, and various components of it; a fairly complete list is available at the Multics Bibliography page. The most important and/or informative ones are listed below.





</doc>
<doc id="18849" url="https://en.wikipedia.org/wiki?curid=18849" title="Marxist film theory">
Marxist film theory

Marxist film theory is one of the oldest forms of film theory. 

Sergei Eisenstein and many other Soviet filmmakers in the 1920s expressed ideas of Marxism through film. In fact, the Hegelian dialectic was considered best displayed in film editing through the Kuleshov Experiment and the development of montage.

While this structuralist approach to Marxism and filmmaking was used, the more vociferous complaint that the Russian filmmakers had was with the narrative structure of the cinema of the United States.

Eisenstein's solution was to shun narrative structure by eliminating the individual protagonist and tell stories where the action is moved by the group and the story is told through a clash of one image against the next (whether in composition, motion, or idea) so that the audience is never lulled into believing that they are watching something that has not been worked over.

Eisenstein himself, however, was accused by the Soviet authorities under Joseph Stalin of "formalist error", of highlighting form as a thing of beauty instead of portraying the worker nobly.

French Marxist film makers, such as Jean-Luc Godard, would employ radical editing and choice of subject matter as well as subversive parody to heighten class consciousness and promote Marxist ideas.

Situationist film maker Guy Debord, author of "The Society of the Spectacle", began his film "In girum imus nocte et consumimur igni" [Wandering around in the night we are consumed by fire] with a radical critique of the spectator who goes to the cinema to forget about his dispossessed daily life.

Situationist film makers produced a number of important films, where the only contribution by the situationist film cooperative was the sound-track. In "Can dialectics break bricks?" (1973), a Chinese Kung Fu film was transformed by redubbing into an epistle on state capitalism and Proletarian revolution. The intellectual technique of using capitalism's own structures against itself is known as détournement. 

Marxist film theory has developed from these precise and historical beginnings and is now sometimes viewed in a wider way to refer to any power relationships or structures within a moving image text. 




</doc>
<doc id="18851" url="https://en.wikipedia.org/wiki?curid=18851" title="Mars (disambiguation)">
Mars (disambiguation)

Mars is a planet in the Solar System.

Mars also commonly refers to:

Mars may also refer to:






















</doc>
<doc id="18852" url="https://en.wikipedia.org/wiki?curid=18852" title="Morpheme">
Morpheme

A morpheme is the smallest meaningful unit in a language. A morpheme is not identical to a word. The main difference between them is that a morpheme sometimes does not stand alone, but a word, by definition, always stands alone. The linguistics field of study dedicated to morphemes is called morphology. When a morpheme stands by itself, it is considered as a root because it has a meaning of its own (such as the morpheme "cat"). When it depends on another morpheme to express an idea, it is an affix because it has a grammatical function (such as the "–s" in "cats" to indicate that it is plural). Every word comprises one or more morphemes.

Every morpheme can be classified as either free or bound. Since the categories are mutually exclusive, a given morpheme will belong to exactly one of them.

Bound morphemes can be further classified as derivational or inflectional morphemes. The main difference between derivational morphemes and inflectional morphemes is their function for words.



Allomorphs are variants of a morpheme that differ in pronunciation but are semantically identical. For example, the English plural marker "-(e)s" of regular nouns can be pronounced ("bats"), , ("bugs"), or , ("buses"), depending on the final sound of the noun's plural form.

Generally, these types of morphemes have no visible changes. For instance, "sheep" is both the singular and the plural form. The intended meaning is thus derived from the Co-occurrence determiner (in this case, "some-" or "a-").

Content morphemes express a concrete meaning or "content", and function morphemes have more of a grammatical role. For example, the morphemes "fast" and "sad" can be considered content morphemes. On the other hand, the suffix "-ed" is a function morpheme since it has the grammatical function of indicating past tense.

Both categories may seem very clear and intuitive, but the idea behind them is occasionally harder to grasp since they overlap with each other. Examples of ambiguous situations are the preposition "over" and the determiner "your", which seem to have concrete meanings but are considered function morphemes since their role is to connect ideas grammatically. Here is a general rule to determine the category of a morpheme:


Roots are composed of only one morpheme, while stems can be composed of more than one morpheme. Any additional affixes are considered morphemes. For example, in the word "quirkiness", the root is "quirk", but the stem is "quirky", which has two morphemes.

Moreover, some pairs of affixes have the same phonological form but have a different meaning. For example, the suffix "–er" can be either derivative (e.g. "sell" ⇒ "seller") or inflectional (e.g. "small" ⇒ "smaller"). Such morphemes are called homophonous.

Some words might seem to be composed of multiple morphemes but are not. Therefore, not only form but also meaning must be considered when identifying morphemes. For example, the word "relate" might seem to be composed of two morphemes, "re-" (prefix) and the word "late", but it is not. Those morphemes have no relationship with the definitions relevant to the word like "to feel sympathy," "to narrate," or "to be connected by blood or marriage."

Furthermore, the length of a word does not determine whether or not it has multiple morphemes. The word "Madagascar" is long and might seem to have morphemes like "mad", "gas", and "car", but it does not. Conversely, some short words have multiple morphemes (e.g. "dogs" = "dog" + "s").

Morphological Icons are images, patterns or symbols that relates to a specific Morpheme. For children with Dyslexia it has been shown to be an effective way of building up a word, The word 'inviting' as an example is made up of two commonly used Morphemes, 'In' and 'Ing'. A Morphological Icons for 'In' could be an arrow going into a cup, and 'Ing' could be an arrow going forward to symbolise that something is in action, (Being, Running, Fishing). 

The concept of combining visual aid Icons with Morpheme teaching methods was pioneered from the mid 1980's by Dr. Neville Brown, He founded Maple Hayes school for dyslexia in 1981, He later improved the method alongside his son Dr. Daryl Brown, the schools curriculum uses Morphological Icons. 

In natural language processing for Korean, Japanese, Chinese, and other languages, morphological analysis is the process of segmenting a sentence into a row of morphemes. Morphological analysis is closely related to part-of-speech tagging, but word segmentation is required for these languages because word boundaries are not indicated by blank spaces.

The purpose of morphological analysis is to determine the minimal units of meaning in a language or morphemes by using comparisons of similar forms: for example, comparing forms such as "She is walking" and "They are walking," rather than comparing either with something completely different like "You are reading." Thus, the forms can be effectively broken down into parts and the different morphemes can be distinguished. 

Similarly, both meaning and form are equally important for the identification of morphemes. For instance, an agent morpheme is an affix like "-er" that transforms a verb into a noun (e.g. "teach" → "teacher"). On the other hand, "–er" can also be a comparative morpheme that changes an adjective into another degree of the same adjective (eg.. "small" → "smaller"). Although the form is the same, the meaning of both morphemes is different. Also, the opposite can occur, with the meaning being the same but the form being different.

In generative grammar, the definition of a morpheme depends heavily on whether syntactic trees have morphemes as leaves or features as leaves.

Given the definition of a morpheme as "the smallest meaningful unit," nanosyntax aims to account for idioms in which an entire syntactic tree often contributes "the smallest meaningful unit." An example idiom is "Don't let the cat out of the bag." Here, the idiom is composed of "let the cat out of the bag." This might be considered a semantic morpheme that is itself composed of many syntactic morphemes. Other cases of the "smallest meaningful unit" being longer than a word include some collocations such as "in view of" and "business intelligence", in which the words together have a specific meaning.

The definition of morphemes also plays a significant role in the interfaces of generative grammar in the following theoretical constructs:





</doc>
<doc id="18856" url="https://en.wikipedia.org/wiki?curid=18856" title="MTV">
MTV

MTV (originally an initialism of Music Television) is an American pay television channel launched on August 1, 1981, based in New York City that serves as the flagship property of the ViacomCBS Domestic Media Networks division of ViacomCBS headquartered in New York City.

MTV originally aired music videos as guided by television personalities known as "video jockeys" (VJs), but in the years since its inception, the network significantly toned down its focus on music in favor of original reality programming targeting teenagers and young adults. Despite its success, the network's programming and influence have been the subject of controversy over the years.

MTV has spawned numerous sister channels in the U.S. and affiliated channels internationally, some of which have gone independent, with approximately 90.6 million American households in the United States receiving the channel as of January 2016.

Several earlier concepts for music video-based television programming had been around since the early 1960s. The Beatles had used music videos to promote their records starting in the mid-1960s. The creative use of music videos within their 1964 film "A Hard Day's Night," particularly the performance of the song "Can't Buy Me Love", led MTV later to honor the film's director Richard Lester with an award for "basically inventing the music video".

In his book "The Mason Williams FCC Rapport", author Mason Williams states that he pitched an idea to CBS for a television program that featured "video-radio", where disc jockeys would play avant-garde art pieces set to music. CBS rejected the idea, but Williams premiered his own musical composition "Classical Gas" on the "Smothers Brothers Comedy Hour", where he was head writer. In 1970, Philadelphia-based disc jockey Bob Whitney created "The Now Explosion", a television series filmed in Atlanta and broadcast in syndication to other local television stations throughout the United States. The series featured promotional clips from various popular artists, but was canceled by its distributor in 1971. Several music programs originating outside of the US, including Australia's "Countdown" and the United Kingdom's "Top of the Pops", which had initially aired music videos in lieu of performances from artists who were not available to perform live, began to feature them regularly by the mid-1970s.

In 1974, Gary Van Haas, vice president of Televak Corporation, introduced a concept to distribute a music video channel to record stores across the United States, and promoted the channel, named Music Video TV, to distributors and retailers in a May 1974 issue of "Billboard". The channel, which featured video disc jockeys, signed a deal with US Cable in 1978 to expand its audience from retail to cable television. The service was no longer active by the time MTV launched in 1981.

In 1977, Warner Cable a division of Warner Communications and the precursor of Warner-Amex Satellite Entertainment launched the first two-way interactive cable television system named QUBE in Columbus, Ohio. The QUBE system offered many specialized channels. One of these specialized channels was Sight on Sound, a music channel that featured concert footage and music-oriented television programs. With the interactive QUBE service, viewers could vote for their favorite songs and artists.

The original programming format of MTV was created by media executive Robert W. Pittman, who later became president and chief executive officer (CEO) of MTV Networks. Pittman had test-driven the music format by producing and hosting a 15-minute show, "Album Tracks", on New York City television station WNBC-TV in the late 1970s.

Pittman's boss Warner-Amex executive vice president John Lack had shepherded "PopClips", a television series created by former Monkee-turned solo artist Michael Nesmith, whose attention had turned to the music video format in the late 1970s. The inspiration for "PopClips" came from a similar program on New Zealand's TVNZ network named "Radio with Pictures", which premiered in 1976. The concept itself had been in the works since 1966, when major record companies began supplying the creator of New Zealand Broadcasting Corporation with promotional music clips to play on the air at no charge. Few artists made the long trip to New Zealand to appear live.

From its launch on August 1, 1981 until March 18, 2009, the "Music Television" caption was included underneath the logo. Many fans recognize this logo because of its unique design that allows it to become anything.

On Saturday, August 1, 1981, at 12:01 AMEastern Time, MTV was launched with the words "Ladies and gentlemen, rock and roll," spoken by John Lack and played over footage of the first Space Shuttle launch countdown of "Columbia" (which took place earlier that year) and of the launch of Apollo 11. Those words were immediately followed by the original MTV theme song, a crunching rock tune composed by Jonathan Elias and John Petersen, playing over the American flag changed to show MTV's logo changing into various textures and designs. MTV producers Alan Goodman and Fred Seibert used this public domain footage as a concept; Seibert said that they had originally planned to use Neil Armstrong's "One small step" quote, but lawyers said that Armstrong owned his name and likeness and that he had refused, so the quote was replaced with a beeping sound. A shortened version of the shuttle launch ID ran at the top of every hour in various forms, from MTV's first day until it was pulled in early 1986 in the wake of the "Challenger" disaster.

The first music video shown on MTV was The Buggles' "Video Killed the Radio Star", originally available only to homes in New Jersey. This was followed by the video for Pat Benatar's "You Better Run". The screen went black sporadically when an employee at MTV inserted a tape into a VCR. MTV's lower third graphics that appeared near the beginning and end of music videos eventually used the recognizable Kabel typeface for about 25 years, but these graphics differed on MTV's first day of broadcast; they were set in a different typeface and included information such as the year and record label name.

As programming chief, Robert W. Pittman recruited and managed a team for the launch that included Tom Freston (who succeeded Pittman as CEO of MTV Networks), Fred Seibert, John Sykes, Carolyn Baker (original head of talent and acquisition), Marshall Cohen (original head of research), Gail Sparrow (of talent and acquisition), Sue Steinberg (executive producer), Julian Goldberg, Steve Lawrence, Geoff Bolton; studio producers and MTV News writers/associate producers Liz Nealon, Nancy LaPook and Robin Zorn; Steve Casey (creator of the name "MTV" and its first program director), Marcy Brafman, Ronald E. "Buzz" Brindle, and Robert Morton. Kenneth M. Miller is credited as being the first technical director to officially launch MTV from its New York City-based network operations facility.

MTV's effect was immediate in areas where the new music video channel was carried. Within two months, record stores in areas where MTV was available were selling music that local radio stations were not playing, such as Men at Work, Bow Wow Wow and the Human League. MTV sparked the Second British Invasion, with British acts, who had been accustomed to using music videos for half a decade (some of which appeared on the BBC’s "Top of the Pops"), featuring heavily on the channel.

MTV targeted an audience between the ages of twelve to thirty-four. However, according to MTV's self conducted research over 50% of its audience was between twelve and twenty-four. Furthermore, this particular group watched MTV for an average of thirty minutes to two hours a day.

The original purpose of MTV was to be "music television", playing music videos 24 hours a day and seven days a week, guided by on-air personalities known as VJs, or video jockeys. The original slogans of the channel were "You'll never look at music the same way again", and "On cable. In stereo."

MTV's earliest format was modeled after AOR (album-oriented rock) radio; MTV underwent a transition to mimic a full Top 40 station in 1984. Fresh-faced young men and women were hired to host the channel's programming and to introduce music videos that were being played. The term VJ was coined, which was a play on the initialism DJ (disc jockey). Many VJs eventually became celebrities in their own right. The original five MTV VJs in 1981 were Nina Blackwood, Mark Goodman, Alan Hunter, J.J. Jackson and Martha Quinn. Initially popular New York DJ Meg Griffin was going to be a VJ, but decided against it at the last minute. The VJs were hired to fit certain demographics the channel was trying to obtain (Goodman was the affable every man, Hunter the popular jock, Jackson the hip radio veteran, Blackwood the bombshell vixen and Quinn the girl next door). Due to the uncertainty of how successful the channel would be, the VJs were told not to buy permanent residencies and to keep their second jobs.

The VJs recorded intro and outro voiceovers before broadcast, along with music news, interviews, concert dates and promotions. These segments appeared to air live and debut across the MTV program schedule 24 hours a day and seven days a week, although the segments themselves were pre-taped within a regular work week at MTV's studios.

The early music videos that made up the bulk of MTV's programming in the 1980s were promotional videos (or "promos", a term that originated in the United Kingdom) that record companies had commissioned for international use or concert clips from any available sources.

Rock bands and performers of the 1980s who had airplay on MTV ranged from new wave to hard rock or heavy metal bands such as Adam Ant, Bryan Adams, The Pretenders, Blondie, Eurythmics, Tom Petty and the Heartbreakers, Culture Club, Mötley Crüe, Split Enz, Prince, Ultravox, Duran Duran, Van Halen, Bon Jovi, RATT, Def Leppard, The Police, and The Cars. The channel also rotated the music videos of "Weird Al" Yankovic, who made a career out of parodying other artists' videos. MTV also aired several specials by "Weird Al" in the 1980s and 1990s under the title "Al TV".

MTV also played classic rock acts from the 1980s and earlier decades, including David Bowie, Dire Straits (whose 1985 song and video "Money for Nothing" both referenced MTV and also included the slogan "I want my MTV" in its lyrics), Journey, Rush, Linda Ronstadt, Genesis, Billy Squier, Aerosmith, The Rolling Stones, Eric Clapton, The Moody Blues, John Mellencamp, Daryl Hall & John Oates, Billy Joel, Robert Palmer, Rod Stewart, The Who, and ZZ Top; newly solo acts such as Peter Gabriel, Robert Plant, Phil Collins, Paul McCartney, David Lee Roth, and Pete Townshend; supergroup acts such as Asia, The Power Station, Yes, The Firm, and Traveling Wilburys, as well as forgotten acts such as Michael Stanley Band, Shoes, Blotto, Ph.D., Rockpile, Bootcamp, Silicon Teens and Taxxi. The hard rock band Kiss publicly appeared without their trademark makeup for the first time on MTV in 1983. The first country-music video aired on MTV was "Angel of the Morning" by Juice Newton, which first aired on MTV's premiere date. (Newton's video was the third video by a solo female artist to air on MTV, after Pat Benatar and Carly Simon.)

During the early days of the channel, MTV occasionally let other stars take over the channel within an hour as "guest VJs". These guests included musicians such as Adam Ant, Billy Idol, Phil Collins, Simon LeBon, and Nick Rhodes of Duran Duran, Tina Turner; and comedians such as Eddie Murphy, Martin Short, Dan Aykroyd, and Steven Wright; as they chose their favorite music videos.

The 1983 film "Flashdance" was the first film in which its promoters excerpted musical segments from it and supplied them to MTV as music videos, which the channel then aired in regular rotation.

In addition to bringing lesser-known artists into view, MTV was instrumental in adding to the booming eighties dance wave. Videos' budgets increased, and artists began to add fully choreographed dance sections. Michael Jackson's music became synonymous with dance. In addition to learning the lyrics, fans also learned his choreography so they could dance along. Madonna capitalized on dance in her videos, using classically trained jazz and break-dancers. Along with extensive costuming and make-up, Duran Duran used tribal elements, pulled from Dunham technique, in "The Wild Boys", and Kate Bush used a modern dance duet in "Running Up That Hill". MTV brought more than music into public view, it added to the ever-growing resurgence of dance in the early 1980s that has carried through to today.

In 1984, more record companies and artists began making video clips for their music than in the past, realizing the popularity of MTV and the growing medium. In keeping with the influx of videos, MTV announced changes to its playlists in the November 3, 1984, issue of "Billboard" magazine, that would take effect the following week. The playlist categories would be expanded to seven, from three (light, medium, heavy); including New, Light, Breakout, Medium, Active, Heavy and Power. This would ensure artists with hit records on the charts would be get the exposure they deserved, with Medium being a home for the established hits still on the climb up to the top 10; and Heavy being a home for the big hitswithout the bells and whistlesjust the exposure they commanded.

In 1985, MTV spearheaded a safe-sex initiative as a response to the AIDS epidemic that continues to influence sexual health currently. In this light, MTV pushed teens to pay more attention to safe-sex because they were most likely more willing to hear this message from MTV than their parents. This showed that MTV was not always influencing youth negatively. Even though in other aspects, MTV was provocative, they had this campaign to showcase their positive influence on youths and safe sexa campaign that still is alive today: "Its Your Sex Life".

During MTV's first few years on the air, very few black artists were included in rotation on the channel. The select few who were in MTV's rotation were Michael Jackson, Prince, Eddy Grant, Donna Summer, Joan Armatrading, Musical Youth, and Herbie Hancock. The very first people of color to perform on MTV was the British band The Specials, which featured an integrated line-up of white and black musicians and vocalists. The Specials' video "Rat Race" was played as the 58th video on the station's first day of broadcasting.

MTV rejected other black artists' videos, such as Rick James' "Super Freak", because they did not fit the channel's carefully selected album-oriented rock format at the time. The exclusion enraged James; he publicly advocated the addition of more black artists' videos on the channel. Rock legend David Bowie also questioned MTV's lack of black artists during an on-air interview with VJ Mark Goodman in 1983. MTV's original head of talent and acquisition, Carolyn B. Baker, who was black, had questioned why the definition of music had to be so narrow, as had a few others outside the network. "The party line at MTV was that we weren't playing black music because of the 'research, said Baker years later. "But the research was based on ignorance ... we were young, we were cutting edge. We didn't have to be on the cutting edge of racism." Nevertheless, it was Baker who had personally rejected Rick James' video for "Super Freak" "because there were half-naked women in it, and it was a piece of crap. As a black woman, I did not want that representing my people as the first black video on MTV."

The network's director of music programming, Buzz Brindle, told an interviewer in 2006, "MTV was originally designed to be a rock music channel. It was difficult for MTV to find African American artists whose music fit the channel's format that leaned toward rock at the outset." Writers Craig Marks and Rob Tannenbaum noted that the channel "aired videos by plenty of white artists who didn't play rock." Andrew Goodwin later wrote, "[MTV] denied racism, on the grounds that it merely followed the rules of the rock business." MTV senior executive vice president Les Garland complained decades later, "The worst thing was that 'racism' bullshit... there were hardly any videos being made by black artists. Record companies weren't funding them. "They" never got charged with racism." However, critics of that defense pointed out that record companies were not funding videos for black artists because they knew that they would have difficulty persuading MTV to play them.

Before 1983, Michael Jackson also struggled to receive airtime on MTV. To resolve the struggle and finally "break the color barrier", the president of CBS Records at the time, Walter Yetnikoff, denounced MTV in a strong, profane statement, threatening to take away MTV's ability to play any of the record label's music videos. However, Les Garland, then acquisitions head, said he decided to air Jackson's "Billie Jean" video without pressure from CBS. This was contradicted by CBS head of Business Affairs David Benjamin in Vanity Fair.

According to "The Austin Chronicle", Jackson's video for the song "Billie Jean" was "the video that broke the color barrier, even though the channel itself was responsible for erecting that barrier in the first place." But change was not immediate. "Billie Jean" was not added to MTV's "medium rotation" playlist (two to three airings per day) until after it had already reached #1 on the "Billboard" Hot 100 chart. In the final week of March, it was in "heavy rotation", one week before the MTV debut of Jackson's "Beat It" video. Prince's "Little Red Corvette" joined both videos in heavy rotation at the end of April. At the beginning of June, "Electric Avenue" by Eddy Grant joined "Billie Jean", which was still in heavy rotation until mid-June. At the end of August, "She Works Hard for the Money" by Donna Summer was in heavy rotation on the channel. Herbie Hancock's "Rockit" and Lionel Richie's "All Night Long" was placed in heavy rotation at the end of October and the beginning of November respectively. In final week of November, Donna Summer's "Unconditional Love" was in heavy rotation. When Jackson's elaborate video for "Thriller" was released late in the year, which raised the ambition bar for what a video could be, the network's support for it was total; subsequently, more pop and R&B videos were played on MTV.

Regardless of the timeline, many black artists had their videos played in "heavy" rotation the following year (1984). Along with Herbie Hancock, Prince, Donna Summer, other black artists such as Billy Ocean, Stevie Wonder, Tina Turner, Lionel Richie, Ray Parker Jr, Rockwell, The Pointer Sisters, The Jacksons, Sheila E and Deniece Williams all had videos played in heavy rotation on MTV.

Eventually, videos from the emerging genre of rap and hip hop also began to enter rotation on MTV. A majority of the rap artists appearing on MTV in the mid-1980s such as Run-DMC, The Fat Boys, Whodini, LL Cool J, and the Beastie Boys were from the East Coast.

In 1984, the channel produced its first "MTV Video Music Awards" show, or VMAs. The first award show, in 1984, was punctuated by a live performance by Madonna of "Like A Virgin". The statuettes that are handed out at the "Video Music Awards" are of the MTV moonman, the channel's original image from its first broadcast in 1981. Presently, the "Video Music Awards" are MTV's most watched annual event.

MTV began its annual "Spring Break" coverage in 1986, setting up temporary operations in Daytona Beach, Florida, for a week in March, broadcasting live eight hours per day. "Spring break is a youth culture event", MTV's vice president Doug Herzog said at the time. "We wanted to be part of it for that reason. It makes good sense for us to come down and go live from the center of it, because obviously the people there are the kinds of people who watch MTV." The channel's coverage featured numerous live performances from artists and bands on location. The annual tradition continued into the 2000s, when it became de-emphasized and handed off to mtvU, the spin-off channel of MTV targeted at college campuses.

The channel later expanded its beach-themed events to the summer, dedicating most of each summer season to broadcasting live from a beach house at various locations away from New York City, eventually leading to channel-wide branding throughout the summer in the 1990s and early 2000s such as "Motel California", "Summer Share", "Isle of MTV", "SoCal Summer", "Summer in the Keys", and "Shore Thing". MTV VJs hosted blocks of music videos, interview artists and bands, and introduced live performances and other programs from the beach house location each summer. In the 2000s, as the channel reduced its airtime for music videos and eliminated much of its in-house programming, its annual summer-long events came to an end.

MTV also held week-long music events that took over the presentation of the channel. Examples from the 1990s and 2000s include "All Access Week", a week in the summer dedicated to live concerts and festivals; "Spankin' New Music Week", a week in the fall dedicated to brand new music videos; and week-long specials that culminated in a particular live event, such as "Wanna be a VJ" and the "Video Music Awards".

At the end of each year, MTV takes advantage of its home location in New York City to broadcast live coverage on New Year's Eve in Times Square. Several live music performances are featured alongside interviews with artists and bands that were influential throughout the year. For many years from the 1980s to the 2000s, the channel upheld a tradition of having a band perform a cover song at midnight immediately following the beginning of the new year.

Throughout its history, MTV has covered global benefit concert series live. For most of July 13, 1985, MTV showed the Live Aid concerts, held in London and Philadelphia and organized by Bob Geldof and Midge Ure to raise funds for famine relief in Ethiopia. While the ABC network showed only selected highlights during primetime, MTV broadcast 16 hours of coverage.

Along with VH1, MTV broadcast the Live 8 concerts, a series of concerts set in the G8 states and South Africa, on July 2, 2005. Live 8 preceded the 31st G8 summit and the 20th anniversary of Live Aid. MTV drew heavy criticism for its coverage of Live 8. The network cut to commercials, VJ commentary, or other performances during performances. Complaints surfaced on the Internet over MTV interrupting the reunion of Pink Floyd. In response, MTV president Van Toeffler stated that he wanted to broadcast highlights from every venue of Live 8 on MTV and VH1, and clarified that network hosts talked over performances only in transition to commercials, informative segments or other musical performances. Toeffler acknowledged that "MTV should not have placed such a high priority on showing so many acts, at the expense of airing complete sets by key artists." He also blamed the Pink Floyd interruption on a mandatory cable affiliate break. MTV averaged 1.4million viewers for its original July 2 broadcast of Live 8. Consequently, MTV and VH1 aired five hours of uninterrupted Live 8 coverage on July 9, with each channel airing different blocks of artists.

MTV had debuted "Dial MTV" in 1986, a daily top ten music video countdown show for which viewers could call the toll-free telephone number 1-800-DIAL-MTV to request a music video. The show was replaced by "MTV Most Wanted" in 1991, which ran until 1996, and later saw a spiritual successor in "Total Request Live". The phone number remained in use for video requests until 2006.

1986 also brought the departures of the first two VJs, as J.J. Jackson moved back to Los Angeles and returned to radio, while Nina Blackwood moved on to pursue new roles in television. Downtown Julie Brown was hired as the first new VJ as a replacement.

Also in 1986, the channel introduced "120 Minutes", a show that featured low-rotation, alternative rock and other "underground" videos for the next 14 years on MTV and three additional years on sister channel MTV2. The program then became known as "Subterranean" on MTV2. Eight years later, on July 31, 2011, "120 Minutes" was resurrected with Matt Pinfield taking over hosting duties once again and airing monthly on MTV2.

Another late night music video show was added in 1987, "Headbangers Ball", which featured heavy metal music and news. Before its abrupt cancellation in 1995, it featured several hosts including Riki Rachtman and Adam Curry. A weekly block of music videos with the name "Headbangers Ball" aired from 2003 to 2011 on sister channel MTV2, before spending an additional two years as a web-only series on MTV2's website, until "Headbangers Ball" was discontinued once again in 2013. Mark Goodman and Alan Hunter departed the network in 1987. 

In 1988, MTV debuted "Yo! MTV Raps", a hip hop/rap formatted program. The program continued until August 1995. It was renamed to simply "Yo!" and aired as a one-hour program from 1995 to 1999. The concept was reintroduced as "Direct Effect" in 2000, which became "Sucker Free" in 2006 and was cancelled in 2008, after briefly celebrating the 20th anniversary of "Yo! MTV Raps" throughout the months of April and May 2008. Despite its cancellation on MTV, a weekly countdown of hip hop videos known as "Sucker Free" still airs on MTV2 through the present day.

Martha Quinn's contract wasn't renewed in 1988 and she departed the network. She was brought back a year later in 1989 and stayed until 1993.

In 1989, MTV began to premiere music-based specials such as "MTV Unplugged", an acoustic performance show, which has featured dozens of acts as its guests and has remained active in numerous iterations on various platforms for over 20 years.

To further cater to the growing success of R&B, MTV introduced the weekly "Fade to Black" in the summer of 1991, which was hosted by Al B. Sure!. The show was reformatted into the better known "MTV Jams" the following year, which incorporated mainstream hip-hop into the playlist. Bill Bellamy became the new and ongoing host. The show became so successful it spawned its own Most Wanted spinoff titled "Most Wanted Jams".

By the early 1990s, MTV was playing a combination of pop-friendly hard rock acts, chart-topping metal and hard rock acts such as Metallica, Nirvana and Guns N' Roses, pop singers such as Michael Jackson, Madonna, Janet Jackson, and New Kids on the Block, and R&B groups such as New Edition, En Vogue, Bell Biv Devoe, SWV, Tony! Toni! Toné!, TLC, and Boyz II Men, while introducing hit rappers Vanilla Ice and MC Hammer. MTV progressively increased its airing of hip hop acts, by way of the program "Yo! MTV Raps", such as LL Cool J, Queen Latifah, Salt-n-Pepa, Naughty By Nature, Onyx, MC Lyte, and Sir-Mix-A-Lot, and by 1993, the channel added West Coast rappers previously associated with gangsta rap, with a less pop-friendly sound, such as Tupac Shakur, Ice Cube, Warren G, Ice-T, Dr. Dre, Tone Loc, and Snoop Doggy Dogg.

To accompany the new sounds, a new form of music videos came about: more creative, funny, artistic, experimental, and technically accomplished than those in the 1980s. Several noted film directors got their start creating music videos. After pressure from the Music Video Production Association, MTV began listing the names of the videos' directors at the bottom of the credits by December 1992. As a result, MTV's viewers became familiar with the names of Spike Jonze, Michel Gondry, David Fincher, Mary Lambert, Samuel Bayer, Matt Mahurin, Mark Romanek, Jonathan Dayton and Valerie Faris, Anton Corbijn, Mark Pellington, Tarsem, Hype Williams, Jake Scott, Jonathan Glazer, Marcus Nispel, F. Gary Gray, Jim Yukich, Russell Mulcahy, Steve Barron, Marty Callner, and Michael Bay, among others.

As the PBS series "Frontline" explored, MTV was a driving force that catapulted music videos to a mainstream audience, turning music videos into an art form as well as a marketing machine that became beneficial to artists. Danny Goldberg, chairman and CEO of Artemis Records, said the following about the art of music videos: "I know when I worked with Nirvana, Kurt Cobain cared as much about the videos as he did about the records. He wrote the scripts for them, he was in the editing room, and they were part of his art. And I think they stand up as part of his art, and I think that's true of the great artists today. Not every artist is a great artist and not every video is a good video, but in general having it available as a tool, to me, adds to the business. And I wish there had been music videos in the heyday of The Beatles, and The Rolling Stones. I think they would've added to their creative contribution, not subtracted from it."

Nirvana led a sweeping transition into the rise of alternative rock music on MTV in 1991 with their video for "Smells Like Teen Spirit". By late 1991 going into 1992, MTV began frequently airing videos from their heavily promoted "Buzz Bin", such as Nirvana, Pearl Jam, Alice in Chains, Soundgarden, Nine Inch Nails, Tori Amos, PM Dawn, Arrested Development, Björk, and Gin Blossoms. MTV increased rotation of its weekly alternative music program "120 Minutes" and added the daily "Alternative Nation" to play videos of these and other underground music acts. Subsequently, grunge and alternative rock had a rise in mainstream tastes, while 1980s-style glam bands and traditional rockers were phased out, with some exceptions such as Aerosmith and Tom Petty. Older acts such as R.E.M. and U2 remained relevant by making their music more experimental or unexpected.

They also played many hard rock acts such as Pantera, Death and other heavy/death metal acts at the time period, which saw rotation especially in "Headbangers Ball", a program geared for the style.

In 1993, more hit alternative rock acts were on heavy rotation, such as Stone Temple Pilots, Soul Asylum, Rage Against the Machine, Marilyn Manson, Tool, Beck, Therapy?, Radiohead, and The Smashing Pumpkins. Other hit acts such as Weezer, Collective Soul, Blind Melon, The Cranberries, Bush, and Silverchair followed in the next couple of years. Alternative bands that appeared on "Beavis and Butt-Head" included White Zombie.

By the next few years, 1994 through 1997, MTV began promoting new power pop acts, most successfully Green Day and The Offspring, and ska-rock acts such as No Doubt, The Mighty Mighty Bosstones and Sublime. Pop singers were added to the rotation with success as long as they were considered "alternative," such as Alanis Morissette, Jewel, Fiona Apple, and Sarah McLachlan. On October 20, 1996, the channel aired a previously unaired episode of The Ren and Stimpy Show titled "Sammy and Me; The Last Temptation."

By 1997, MTV focused heavily on introducing electronica acts into the mainstream, adding them to its musical rotation, including The Prodigy, The Chemical Brothers, Moby, Aphex Twin, Pendulum, Daft Punk, The Crystal Method, Butthole Surfers and Fatboy Slim. Some musicians who proceeded to experiment with electronica were still played on MTV including Madonna, U2, David Bowie, Radiohead, and Smashing Pumpkins. That year, MTV also attempted to introduce neo-swing bands, but they did not meet with much success.

However, in late 1997, MTV began shifting more progressively towards pop music, inspired by the success of the Spice Girls and the rise of boy bands in Europe. Between 1998 and 1999, MTV's musical content consisted heavily of videos of boy bands such as Backstreet Boys and NSYNC, as well as teen pop "princesses" such as Britney Spears, Christina Aguilera, Lynda Thomas, Mandy Moore, and Jessica Simpson. Airplay of rock, electronica, and alternative acts was reduced. Hip-hop music continued in heavy rotation, through the likes of Puff Daddy, Jermaine Dupri, Master P, DMX, Busta Rhymes, Lil' Kim, Jay-Z, Missy Elliott, Lauryn Hill, Eminem, Foxy Brown, Ja Rule, Nas, Timbaland, and their associates. R&B was also heavily represented with acts such as Aaliyah, Janet Jackson, Destiny's Child, 702, Monica, and Brandy.

Beginning in late 1997, MTV progressively reduced its airing of rock music videos, leading to the slogan among skeptics, "Rock is dead." The facts that at the time rock music fans were less materialistic, and bought less music based on television suggestion, were cited as reasons that MTV abandoned its once staple music. MTV instead devoted its musical airtime mostly to pop and hip hop/R&B music. All rock-centric shows were eliminated and the rock-related categories of the "Video Music Awards" were pared down to one.

From this time until 2004, MTV made some periodic efforts to reintroduce pop rock music videos to the channel. By 1998 through 1999, the punk-rock band Blink-182 received regular airtime on MTV due in large part to their "All the Small Things" video that made fun of the boy bands that MTV was airing at the time. Meanwhile, some rock bands that were not receiving MTV support, such as Korn and Creed, continued to sell albums. Then, upon the release of Korn's rock/rap hybrid album "Follow the Leader", MTV began playing Korn's videos "Got the Life" and "Freak on a Leash".

A band sponsored by Korn, Limp Bizkit, received airtime for its cover of George Michael's "Faith", which became a hit. Subsequently, MTV began airing more rap/rock hybrid acts, such as Limp Bizkit and Kid Rock. Some rock acts with more comical videos, such as Rob Zombie, Red Hot Chili Peppers and Foo Fighters, also received airtime.

In the fall of 1999, MTV announced a special "Return of the Rock" weekend, in which new rock acts received airtime, after which a compilation album was released. System of a Down, Staind, Godsmack, Green Day, Incubus, Papa Roach, P.O.D., Sevendust, Powerman 5000, Slipknot, Kittie, Static X, and CKY were among the featured bands. These bands received some airtime on MTV and more so on MTV2, though both channels gave emphasis to the rock/rap acts.

By 2000, Sum 41, Linkin Park, Jimmy Eat World, Mudvayne, Cold, At the Drive-In, Alien Ant Farm, and other acts were added to the musical rotation. MTV also launched subscription channel MTVX to play rock music videos exclusively, an experiment that lasted until 2002. A daily music video program on MTV that carried the name "Return of the Rock" ran through early 2001, replaced by a successor, "All Things Rock", from 2002 until 2004.

MTV created four shows in the late 1990s that centered on music videos: "MTV Live", "Total Request", "Say What?", and"12 Angry Viewers". In 1997, MTV introduced its new studios in Times Square.

A year later, in 1998, MTV merged "Total Request" and "MTV Live" into a live daily top ten countdown show, "Total Request Live", which became known as "TRL." The original host was Carson Daly. The show included a live studio audience and was filmed in a windowed studio that allowed crowds to look in. According to Nielsen, the average audience for the show was at its highest in 1999 and continued with strong numbers through 2001. The program played the top ten pop, rock, R&B, and hip hop music videos, and featured live interviews with artists and celebrities.

From 1998 to 2001, MTV also aired several other music video programs from its studios. These programs included "Say What? Karaoke", a game show hosted by Dave Holmes. In the early 2000s MTV aired "VJ for a Day", hosted by Ray Munns.

MTV also aired "Hot Zone", hosted by Ananda Lewis, which featured pop music videos during the midday time period. Other programs at the time included "Sucker Free", "BeatSuite", and blocks of music videos hosted by VJs simply called "Music Television".

In 2002, Carson Daly left MTV and "TRL" to pursue a late night talk show on NBC. The series came to an end with a special finale episode, "Total Finale Live", which aired November 16, 2008, and featured hosts and guests that previously appeared on the show.

Around 1999 through 2001, as MTV aired fewer music videos throughout the day, it regularly aired compilation specials from its then 20-year history to look back on its roots. An all-encompassing special, "MTV Uncensored", premiered in 1999 and was later released as a book.

MTV celebrated its 20th anniversary on August 1, 2001, beginning with a 12-hour long retrospective called "MTV20: Buggles to Bizkit", which featured over 100 classic videos played chronologically, hosted by various VJs in reproductions of MTV's old studios. The day of programming culminated in a three-hour celebratory live event called "MTV20: Live and Almost Legal", which was hosted by Carson Daly and featured numerous guests from MTV's history, including the original VJs from 1981. Various other related "MTV20" specials aired in the months surrounding the event.

Janet Jackson became the inaugural honoree of the "MTV Icon" award, "an annual recognition of artists who have made significant contributions to music, music video and pop culture while tremendously impacting the MTV generation." Subsequent recipients included Aerosmith, Metallica, and The Cure.

Five years later, on August 1, 2006, MTV celebrated its 25th anniversary. On their website, MTV.com, visitors could watch the very first hour of MTV, including airing the original promos and commercials from Mountain Dew, Atari, Chewels gum, and Jovan. Videos were also shown from The Buggles, Pat Benatar, Rod Stewart, and others. The introduction of the first five VJs was also shown. Additionally, MTV.com put together a "yearbook" consisting of the greatest videos of each year from 1981 to 2006. MTV itself only mentioned the anniversary once on "TRL".

Although MTV reached its 30th year of broadcasting in 2011, the channel itself passed over this milestone in favor of its current programming schedule. The channel instead aired its 30th anniversary celebrations on its sister networks MTV2 and VH1 Classic. Nathaniel Brown, senior vice president of communications for MTV, confirmed that there were no plans for an on-air MTV celebration similar to the channel's 20th anniversary. Brown explained, "MTV as a brand doesn't age with our viewers. We are really focused on our current viewers, and our feeling was that our anniversary wasn't something that would be meaningful to them, many of whom weren't even alive in 1981."

From 1995 to 2000, MTV played 36.5% fewer music videos. MTV president Van Toeffler explained: "Clearly, the novelty of just showing music videos has worn off. It's required us to reinvent ourselves to a contemporary audience." Despite targeted efforts to play certain types of music videos in limited rotation, MTV greatly reduced its overall rotation of music videos by the mid-2000s. While music videos were featured on MTV up to eight hours per day in 2000, the year 2008 saw an average of just three hours of music videos per day on MTV. The rise of social media and websites like YouTube as a convenient outlet for the promotion and viewing of music videos signaled this reduction.

As the decade progressed, MTV continued to play some music videos instead of relegating them exclusively to its sister channels, but around this time, the channel began to air music videos only in the early morning hours or in a condensed form on "Total Request Live". As a result of these programming changes, Justin Timberlake implored MTV to "play more damn videos!" while giving an acceptance speech at the 2007 MTV Video Music Awards.

Despite the challenge from Timberlake, MTV continued to decrease its total rotation time for music videos in 2007, and the channel eliminated its long-running special tags for music videos such as "Buzzworthy" (for under-represented artists), "Breakthrough" (for visually stunning videos), and "Spankin' New" (for brand new videos). Additionally, the historic Kabel typeface, which MTV displayed at the beginning and end of all music videos since 1981, was phased out in favor of larger text and less information about the video's record label and director. The classic font can still be seen in "prechyroned" versions of old videos on sister network MTV Classic, which had their title information recorded onto the same tape as the video itself.

Prior to its finale in 2008, MTV's main source of music videos was "Total Request Live", airing four times per week, featuring short clips of music videos along with VJs and guests. MTV was experimenting at the time with new ideas for music programs to replace the purpose of "TRL" but with a new format.

In mid-2008, MTV premiered new music video programming blocks called "FNMTV" and a weekly special event called "FNMTV Premieres", hosted from Los Angeles by Pete Wentz of the band Fall Out Boy, which was designed to premiere new music videos and have viewers provide instantaneous feedback.

The "FNMTV Premieres" event ended before the 2008 MTV Video Music Awards in September. With the exception of a holiday themed episode in December 2008 and an unrelated "Spring Break" special in March 2009 with the same title, "FNMTV Premieres" never returned to the channel's regular program schedule, leaving MTV without any music video programs hosted by VJs for the first time in its history.
Music video programming returned to MTV in March 2009 as "AMTV", an early morning block of music videos that originally aired from 3am to 9am on most weekdays. It was renamed "Music Feed" in 2013 with a reduced schedule. Unlike the "FNMTV" block that preceded it, "Music Feed" featured many full-length music videos, including some older videos that had been out of regular rotation for many years on MTV. It also featured music news updates, interviews, and performances. For many years, "Music Feed" was the only program on MTV's main channel that was dedicated to music videos.

During the rest of the day, MTV used to play excerpts from music videos in split screen format during the closing credits of most programs, along with the address of a website to encourage the viewer to watch the full video online. MTV positioned its website, MTV.com, as its primary destination for music videos, but this strategy was abandoned.

MTV again resurrected the long-running series "MTV Unplugged" in 2009 with performances from acts such as Adele and Paramore. However, unlike past "Unplugged" specials, these new recordings usually only aired in their entirety on MTV's website, MTV.com. Nevertheless, short clips of the specials were shown on MTV during the "AMTV" block of music videos in the early morning hours. On June 12, 2011, MTV aired a traditional television premiere of a new installment of "MTV Unplugged" instead of a web debut. The featured artist was rapper Lil Wayne and the show debuted both on MTV and MTV2. The channel followed up with a similar television premiere of "MTV Unplugged" with Florence and the Machine on April 8, 2012.

MTV launched "10 on Top" in May 2010 with little promotion throughout its run, a weekly program airing on Saturdays and hosted by Lenay Dunn, that counted down the top 10 most trending and talked about topics of the week (generally focused on entertainment). Dunn also appeared in segments between MTV's shows throughout the day as a recognizable personality and face of the channel in the absence of traditional VJs aside from its MTV News correspondents.

The animated series "Beavis and Butt-head" returned to MTV in October 2011, with new episodes. As with the original version of the series that ran from 1993 to 1997, the modern-day "Beavis and Butt-head" features segments in which its main characters watch and criticize music videos.

Sometime in 2012, MTV debuted "Clubland", which previously existed as an hour of EDM videos during the "AMTV" video block. The show had no host, but most editorial content was pushed online by the show's Tumblr and other social media outlets like Facebook and Twitter.

MTV launched a new talk show based on hip hop music on April 12, 2012, called "Hip Hop POV", hosted by Amanda Seales, Bu Thiam, Charlamagne, Devi Dev, and Sowmya Krishnamurthy. The show featured hosted commentary on the headlines in hip hop culture, providing opinions on new music, granting insider access to major events, and including artist interviews. "Hip Hip POV" lasted several episodes before going on hiatus. The show was supposed to return in Fall 2012, but was moved to MTV2 instead, where it was rebranded and merged with "Sucker Free Countdown". The new show debuted as "The Week in Jams" on October 28, 2012.

MTV launched a live talk show, "It's On with Alexa Chung", on June 15, 2009. The host of the program, Alexa Chung, was described as a "younger, more Web 2.0" version of Jimmy Fallon. Although it was filmed in the same Times Square studio where "TRL" used to be broadcast, the network stated that "the only thing the two shows have in common is the studio location." "It's On" was cancelled in December of the same year, which again eliminated the only live in-studio programming from MTV's schedule, just one year after "TRL" was also cancelled.

Shortly after Michael Jackson died on June 25, 2009, the channel aired several hours of Jackson's music videos, accompanied by live news specials featuring reactions from MTV personalities and other celebrities. The temporary shift in MTV's programming culminated the following week with the channel's live coverage of Jackson's memorial service. MTV aired similar one-hour live specials with music videos and news updates following the death of Whitney Houston on February 11, 2012, and the death of Adam Yauch of the Beastie Boys on May 4, 2012.

The channel tried its hand again at live programming with the premiere of a half-hour program called "The Seven" in September 2010. The program counted down seven entertainment-related stories of interest to viewers (and included some interview segments among them), having aired weekdays at 5pm with a weekend wrap-up at 10 am ET. Shortly after its debut, the show was slightly retooled as it dropped co-host Julie Alexandria but kept fellow co-host Kevin Manno; the Saturday recap show was eliminated as well. "The Seven" was cancelled on June 13, 2011. Manno's only assignment at MTV post-"Seven" was conducting an interview with a band which only aired on MTV.com. Manno is no longer employed with MTV and has since appeared as an occasional correspondent on the LXTV-produced NBC series "1st Look".

Presently, MTV airs sporadic live specials called "MTV First". The short program, produced by MTV News, debuted in early 2011 and continues to air typically once every couple of weeks on any given weekday. The specials usually begin at 7:53 pm. ET, led by one of MTV News' correspondents who will conduct a live interview with a featured artist or actor who has come to MTV to premiere a music video or movie trailer. MTV starts its next scheduled program at 8:00 pm, while the interview and chat with fans continues on MTV.com for another 30 to 60 minutes. Since its debut in 2011, "MTV First" has featured high-profile acts such as Lady Gaga, Katy Perry, Usher, and Justin Bieber. In the absence of daily live programs such as "TRL", "It's On with Alexa Chung", and "The Seven" to facilitate such segments, the channel now uses "MTV First" as its newest approach to present music video premieres and bring viewers from its main television channel to its website for real-time interaction with artists and celebrities.

In April 2016, then-appointed MTV president Sean Atkins announced plans to restore music programming to the channel. On April 21, 2016, MTV announced that new "Unplugged" episodes will begin airing, as well as a new weekly performance series called "Wonderland". On that same day, immediately after the death of Prince, MTV interrupted its usual programming to air Prince's music videos. In July 2017, it was announced that "TRL" would be returning to the network on October 2, 2017. As of 2019, the show currently airs on Saturday mornings as "TRL Top 10".

As MTV expanded, music videos and VJ-guided programming were no longer the centerpiece of its programming. Today, MTV's programming covers a wide variety of genres and formats aimed primarily at young adults and millennials. In addition to its original programming, MTV has also aired original and syndicated programs from Viacom-owned siblings and third-party networks.

MTV is also a producer of films aimed at young adults through its production label, MTV Films, and has aired both its own theatrically-released films and original made-for-television movies from MTV Studios in addition to acquired films.

In 2010, a study by the Gay and Lesbian Alliance Against Defamation found that of 207.5 hours of prime time programming on MTV, 42% included content reflecting the lives of gay, bisexual and transgender people. This was the highest in the industry and the highest percentage ever.

In 2018, MTV launched a new production unit under the "MTV Studios" name focused on producing new versions of MTV's library shows.

In 1985, Viacom bought Warner-Amex Satellite Entertainment, which owned MTV and Nickelodeon, renaming the company MTV Networks and beginning this expansion. Before 1987, MTV featured almost exclusively music videos, but as time passed, they introduced a variety of other shows, including some that were originally intended for other channels.

Non-music video programming began in the late 1980s, with the introduction of a music news show "The Week in Rock", which was also the beginning of MTV's news division, MTV News. Around this time, MTV also introduced a fashion news show, "House of Style"; a dance show, "Club MTV"; and a game show, "Remote Control". Programs like these did not feature music videos, but they were still largely based around the world of music.

Following the success of the "MTV Video Music Awards", in an effort to branch out from music into movies and broader pop culture, MTV started the "MTV Movie & TV Awards" in 1992, which continues presently. MTV also created an award show for Europe after the success of the "Video Music Awards". The "MTV Europe Music Awards", or the EMAs, were created in 1994, ten years after the debut of the VMAs.

These new shows were just the beginning of new genres of shows to make an impact on MTV. As the format of the network continued to evolve, more genres of shows began to appear. In the early 1990s, MTV debuted its first reality shows, "Real World" and "Road Rules".

During the latter half of the 1990s and early 2000s, MTV placed a stronger focus on reality shows and related series, building on the success of "The Real World" and "Road Rules". The first round of these shows came in the mid-1990s, with game shows such as "Singled Out", reality-based comedy shows such as "Buzzkill", and late-night talk shows such as "The Jon Stewart Show" and "Loveline".

The next round of these shows came in approximately the late 1990s, as MTV shifted its focus to prank/comedic shows such as "The Tom Green Show" and "Jackass", and game shows such as "The Challenge" (aka "Real World/Road Rules Challenge"), "The Blame Game", "webRIOT", and "Say What? Karaoke". A year later, in 2000, "MTV's Fear" became one of the first scare-based reality shows and the first reality show in which contestants filmed themselves.

MTV continued to experiment with late night talk shows in the early 2000s with relatively short-lived programs such as "Kathy's So-Called Reality", starring Kathy Griffin; and "The Tom Green Show".

Some of the reality shows on the network also followed the lives of musicians. "The Osbournes", a reality show based on the everyday life of Black Sabbath frontman Ozzy Osbourne, his wife Sharon, and two of their children, Jack and Kelly, premiered on MTV in 2002. The show went on to become one of the network's biggest-ever successes and was also recognized for the Osbourne family members' heavy use of profanity, which MTV censored for broadcast. It also kick-started a musical career for Kelly Osbourne, while Sharon Osbourne went on to host her own self-titled talk show on US television. Production ended on "The Osbournes" in November 2004. In the fall of 2004, Ozzy Osbourne's reality show "Battle for Ozzfest" aired; the show hosted competitions between bands vying to play as part of Ozzfest, a yearly heavy metal music tour across the United States hosted by Osbourne.

In 2003, MTV added "Punk'd", a project by Ashton Kutcher to play pranks on various celebrities, and "Pimp My Ride", a show about adding aesthetic and functional modifications to cars and other vehicles. Another show was "", a reality series that followed the lives of pop singers Jessica Simpson and Nick Lachey, a music celebrity couple. It began in 2003 and ran for four seasons, ending in early 2005; the couple later divorced. The success of "Newlyweds" was followed in June 2004 by "The Ashlee Simpson Show", which documented the beginnings of the music career of Ashlee Simpson, Jessica Simpson's younger sister.

In 2005 and 2006, MTV continued its focus on reality shows, with the debuts of shows such as "8th & Ocean", "", "Next", "The Hills", "Two-A-Days", "My Super Sweet 16", "Parental Control", and "Viva La Bam", featuring Bam Margera.

In 2007, MTV aired the reality show "A Shot at Love with Tila Tequila", chronicling MySpace sensation Tila Tequila's journey to find a companion. Her bisexuality played into the seriesboth male and female contestants were vying for loveand was the subject of criticism. It was the #2 show airing on MTV at that time, behind "The Hills". A spin-off series from "A Shot at Love", titled "That's Amoré!", followed a similar pursuit from previous "A Shot at Love" contestant Domenico Nesci.

MTV also welcomed Paris Hilton to its lineup in October 2008, with the launch of her new reality series, "Paris Hilton's My New BFF". In 2009, MTV aired Snoop Dogg's second program with the channel, "Dogg After Dark", and the show "College Life", based at the University of Wisconsin–Madison.
In late 2009, MTV shifted its focus back to "Real World"-style reality programming with the premiere of "Jersey Shore", a program that brought high ratings to the channel and also caused controversy due to some of its content.

With backlash towards what some consider too much superficial content on the network, a 2009 "New York Times" article also stated the intention of MTV to shift its focus towards more socially conscious media, which the article labels "MTV for the Obama era." Shows in that vein included "T.I.'s Road to Redemption" and Fonzworth Bentley's finishing school show "From G's to Gents".

The channel also aired a new show around this time titled "16 and Pregnant", which documented the lives of teenagers expecting babies. This had a follow-up show after the first season titled "Teen Mom", which follows some of the teens through the first stages with their newborns.

MTV found further success with "The Buried Life", a program about four friends traveling across the country to check off a list of "100 things to do before I die" and helping others along the way. Another recent reality program is MTV's "Hired", which follows the employment interviewing process; candidates meet with career coach Ryan Kahn from "Dream Careers" and at the end of each episode one candidate lands the job of their dreams. In 2012, "Punk'd" returned with a revolving door of new hosts per episode. Meanwhile, spin-offs from "Jersey Shore" such as "The Pauly D Project" and "Snooki & JWoww" were produced. MTV announced plans to re-enter the late-night comedy space in 2012, with "Nikki & Sara Live", an unscripted series by comedians Nikki Glaser and Sara Schaefer. The program initially aired weekly from MTV's studios in Times Square.

In a continuing bid to become a more diverse network focusing on youth and culture as well as music, MTV added animated shows to its lineup in the early 1990s. The animation showcase "Liquid Television" (a co-production between BBC and MTV produced in San Francisco by Colossal Pictures) was one of the channel's first programs to focus on the medium. In addition to airing original shows created specifically for MTV, the channel also occasionally aired episodes of original cartoon series produced by sister channel Nickelodeon ("Nicktoons") in the early 1990s.

MTV has a history of cartoons with mature themes including "Beavis and Butt-Head", "Æon Flux", "The Brothers Grunt", "Celebrity Deathmatch", "Undergrads", "Clone High", and "Daria". Although the channel has gone on to debut many other animated shows, few of MTV's other cartoon series have been renewed for additional seasons, regardless of their reception.

In September 2009, the channel aired "Popzilla", which showcased and imitated celebrities in an animated form. MTV again reintroduced animated programming to its lineup with the return of "Beavis and Butt-Head" in 2011 after 14 years off the air, alongside brand new animated program "Good Vibes". In January 2016, MTV returned to animation with "Greatest Party Story Ever".

MTV has a long history of airing both comedy and drama programs with scripted or improvised premises. Examples from the 1990s and 2000s include sketch-based comedies such as "Just Say Julie", "The Ben Stiller Show", "The State", "The Jenny McCarthy Show", "The Lyricist Lounge Show", and "Doggy Fizzle Televizzle", as well as soap operas such as "Undressed" and "Spyder Games".

The channel expanded its programming focus in late 2000s and early 2010s to include more scripted programs. The resurgence of scripted programming on MTV saw the introduction of comedy shows such as "Awkward." and "The Hard Times of RJ Berger", and dramas such as "Skins" and "Teen Wolf". In June 2012, MTV confirmed that it would develop a series based on the "Scream" franchise. The series is now in its third season. On June 24, 2019, it was announced that "Scream" would be moving to VH1 ahead of the premiere of the third season.

MTV's now-iconic logo was designed in 1981 by Manhattan Design (a collective formed by Frank Olinsky, Pat Gorman and Patty Rogoff) under the guidance of original creative director Fred Seibert. The block letter "M" was sketched by Rogoff, with the scribbled word "TV" spraypainted by Olinksky. The primary variant of MTV's logo at the time had the "M" in yellow and the "TV" in red. But unlike most television networks' logos at the time, the logo was constantly branded with different colors, patterns and images on a variety of station IDs. The only constant aspects of MTV's logo at the time were its general shape and proportions, with everything else being dynamic.

MTV launched on August 1, 1981, with an extended network ID featuring the first landing on the moon (with still images acquired directly from NASA), which was a concept of Seibert's executed by Buzz Potamkin and Perpetual Motion Pictures. The ID then cut to the American flag planted on the moon's surface changed to show the MTV logo on it, which rapidly changed into different colors and patterns several times per second as the network's original guitar-driven jingle was played for the first time. After MTV's launch, the "moon landing" ID was edited to show only its ending, and was shown at the top of every hour until early 1986, when the ID was scrapped in light of the Space Shuttle Challenger disaster. The ID ran "more than 75,000 times each year (48 times each day), at the top and bottom of every hour every day" according to Seibert.

From the late 1990s to the early 2000s, MTV updated its on-air appearance at the beginning of every year and each summer, creating a consistent brand across all of its music-related shows. This style of channel-wide branding came to an end as MTV drastically reduced its number of music-related shows in the early to mid 2000s. Around this time, MTV introduced a static and single color digital on-screen graphic to be shown during all of its programming.

Starting with the premiere of the short-lived program "FNMTV" in 2008, MTV started using a revised and chopped down version of its original logo during most of its on-air programming. It became MTV's official logo on February 8, 2010 and officially debuted on its website. The channel's full name "Music Television" was officially dropped, with the revised logo largely the same as the original logo, but without the initialism, the bottom of the "M" being cropped and the "V" in "TV" being branched off. This change was most likely made to reflect MTV's more prominent focus on reality and comedy programming and less on music-related programming. However, much like the original logo, the new logo was designed to be filled in with a seemingly unlimited variety of images. It is used worldwide, but not everywhere existentially. The new logo was first used on MTV Films logo with the 2010 film "Jackass 3D". MTV's rebranding was overseen by Popkern.

On June 25, 2015, MTV International rebranded its on-air look with a new vaporwave and seapunk-inspired graphics package. It included a series of new station IDs featuring 3D renderings of objects and people, much akin to vaporwave and seapunk "aesthetics". Many have derided MTV's choice of rebranding, insisting that the artistic style was centered on denouncing corporate capitalism (many aesthetic pieces heavily incorporate corporate logos of the 1970s, 80s and 90s, which coincidentally include MTV's original logo) rather than being embraced by major corporations like MTV. Many have also suggested that MTV made an attempt to be relevant in the modern entertainment world with the rebrand. In addition to this, the rebrand was made on exactly the same day that the social media site Tumblr introduced Tumblr TV, an animated GIF viewer which featured branding inspired by MTV's original 1980s on-air look. Tumblr has been cited as a prominent location of aesthetic art, and thus many have suggested MTV and Tumblr "switched identities". The rebrand also incorporated a modified version of MTV's classic "I Want My MTV!" slogan, changed to read "I Am My MTV". "Vice" has suggested that the slogan change represents "the current generation's movement towards self-examination, identity politics and apparent narcissism." MTV also introduced MTV Bump, a website that allows Instagram and Vine users to submit videos to be aired during commercial breaks, as well as MTV Canvas, an online program where users submit custom IDs to also be aired during commercial breaks.

The channel's iconic "I want my MTV!" advertising campaign was launched in 1982. It was first developed by George Lois and was based on a cereal commercial from the 1950s with the slogan "I want my Maypo!" that Lois adapted unsuccessfully from the original created by animator John Hubley.

Lois's first pitch to the network was roundly rejected when Lois insisted that rock stars like Mick Jagger should be crying when they said the tag line, not unlike his failed 'Maypo' revamp. His associate, and Seibert mentor Dale Pon took over the campaign, strategically and creatively, and was able to get the campaign greenlit when he laughed the tears out of the spots. From then on –with the exception of the closely logos on the first round of commercials– Pon was the primary creative force.

All the commercials were produced by Buzz Potamkin and his new company Buzzco Productions, directed first by Thomas Schlamme and Alan Goodman and eventually by Candy Kugel.

The campaign featured popular artists and celebrities, including Pete Townshend, Pat Benatar, Adam Ant, David Bowie, The Police, Kiss, Culture Club, Billy Idol, Hall & Oates, Cyndi Lauper, Madonna, Lionel Richie, Ric Ocasek, John Mellencamp, Peter Wolf, Joe Elliot, Stevie Nicks, Rick Springfield, and Mick Jagger, interacting with the MTV logo on-air and encouraging viewers to call their pay television providers and request that MTV be added to their local channel lineups. Eventually, the slogan became so ubiquitous that it made an appearance as a lyric sung by Sting on the Dire Straits song "Money for Nothing", whose music video aired in regular rotation on MTV when it was first released in 1985.

The channel has been a target of criticism by various groups about programming choices, social issues, political correctness, sensitivity, censorship, and a perceived negative social influence on young people. Portions of the content of MTV's programs and productions have come under controversy in the general news media and among social groups that have taken offense. Some within the music industry criticized what they saw as MTV's homogenization of rock 'n' roll, including the punk band the Dead Kennedys, whose song "M.T.V.Get Off the Air" was released on their 1985 album "Frankenchrist", just as MTV's influence over the music industry was being solidified. MTV was also the major influence on the growth of music videos during the 1980s.

HBO also had a 30-minute program of music videos called "Video Jukebox", that first aired around the time of MTV's launch and lasted until late 1986. Also around this time, HBO, as well as other premium channels such as Cinemax, Showtime and The Movie Channel, occasionally played one or a few music videos between movies.

SuperStation WTBS launched "Night Tracks" on June 3, 1983, with up to 14 hours of music video airplay each late night weekend by 1985. Its most noticeable difference was that black artists that MTV initially ignored received airplay. The program ran until the end of May 1992.

A few markets also launched music-only channels including Las Vegas' KVMY (channel 21), which debuted in the summer of 1984 as KRLR-TV and branded as "Vusic 21". The first video played on that channel was "Video Killed the Radio Star", following in the footsteps of MTV.

Shortly after TBS began "Night Tracks", NBC launched a music video program called "Friday Night Videos", which was considered network television's answer to MTV. Later renamed simply "Friday Night", the program ran from 1983 to 2002. ABC's contribution to the music video program genre in 1984, "ABC Rocks", was far less successful, lasting only a year.

TBS founder Ted Turner started the Cable Music Channel in 1984, designed to play a broader mix of music videos than MTV's rock format allowed. But after one month as a money-losing venture, Turner sold it to MTV, who redeveloped the channel into VH1.

Shortly after its launch, The Disney Channel aired a program called "D-TV", a play on the MTV acronym. The program used music cuts, both from current and past artists. Instead of music videos, the program used clips of various vintage Disney cartoons and animated films to go with the songs. The program aired in multiple formats, sometimes between shows, sometimes as its own program, and other times as one-off specials. The specials tended to air both on The Disney Channel and NBC. The program aired at various times between 1984 and 1999. In 2009, Disney Channel revived the "D-TV" concept with a new series of short-form segments called "Re-Micks".

MTV has edited a number of music videos to remove references to drugs, sex, violence, weapons, racism, homophobia, and/or advertising. Many music videos aired on the channel were either censored, moved to late-night rotation, or banned entirely from the channel.

In the 1980s, parent media watchdog groups such as the Parents Music Resource Center (PMRC) criticized MTV over certain music videos that were claimed to have explicit imagery of satanism. As a result, MTV developed a strict policy on refusal to air videos that may depict Satanism or anti-religious themes. This policy led MTV to ban music videos such as "Jesus Christ Pose" by Soundgarden in 1991 and "Megalomaniac" by Incubus in 2004; however, the controversial band Marilyn Manson was among the most popular rock bands on MTV during the late 1990s and early 2000s.

On September 28, 2016, on an AfterBuzz TV live stream, Scout Durwood said that MTV had a "no appropriation policy" that forbid her from wearing her hair in cornrows in an episode of "Mary + Jane". She said, "I wanted to cornrow my hair, and they were like, 'That's racist.'"

During the 1989 MTV Video Music Awards ceremony, comedian Andrew Dice Clay did his usual "adult nursery rhymes" routine (which he had done in his stand-up acts), after which the network executives imposed a lifetime ban. Billy Idol's music video for the song "Cradle of Love" originally had scenes from Clay's film "The Adventures of Ford Fairlane" when it was originally aired; scenes from the film were later excised. During the 2011 MTV Video Music Awards, Clay was in attendance where he confirmed that the channel lifted the ban.

In the wake of controversy that involved a child burning down his house after allegedly watching "Beavis and Butt-head", MTV moved the show from its original 7p.m. time slot to an 11p.m. time slot. Also, Beavis' tendency to flick a lighter and yell "fire" was removed from new episodes, and controversial scenes were removed from existing episodes before their rebroadcast. Some extensive edits were noted by series creator Mike Judge after compiling his , saying that "some of those episodes may not even exist actually in their original form."

A pilot for a show called "Dude, This Sucks" was canceled after teens attending a taping at the Snow Summit Ski Resort in January 2001 were sprayed with liquidized fecal matter by a group known as "The Shower Rangers". The teens later sued, with MTV later apologizing and ordering the segment's removal.

After Viacom's purchase of CBS, MTV was selected to produce the Super Bowl XXXV halftime show in 2001, airing on CBS and featuring Britney Spears, NSYNC, and Aerosmith. Due to its success, MTV was invited back to produce another halftime show in 2004; this sparked a nationwide debate and controversy that drastically changed Super Bowl halftime shows, MTV's programming, and radio censorship.

When CBS aired Super Bowl XXXVIII in 2004, MTV was again chosen to produce the halftime show, with performances by such artists as Nelly, P. Diddy, Janet Jackson, and Justin Timberlake. The show became controversial, however, after Timberlake tore off part of Jackson's outfit while performing "Rock Your Body" with her, revealing her right breast. All involved parties apologized for the incident, and Timberlake referred to the incident as a "wardrobe malfunction".

Michael Powell, former chairman of the Federal Communications Commission, ordered an investigation the day after broadcast. In the weeks following the halftime show, MTV censored much of its programming. Several music videos, including "This Love" and "I Miss You", were edited for sexual content. In September 2004, the FCC ruled that the halftime show was indecent and fined CBS $550,000. The FCC upheld it in 2006, but federal judges reversed the fine in 2008.

Timberlake and Jackson's controversial event gave way to a "wave of self-censorship on American television unrivaled since the McCarthy era". After the sudden event, names surfaced such as nipplegate, Janet moment, and boobgate, and this spread politically, furthering the discussion into the 2004 presidential election surrounding "moral values" and "media decency".

The Christian right organization American Family Association has also criticized MTV from perceptions of negative moral influence, describing MTV as promoting a "pro-sex, anti-family, pro-choice, drug culture".

In 2005, the Parents Television Council (PTC) released a study titled "MTV Smut Peddlers", which sought to expose excessive sexual, profane, and violent content on the channel, based on MTV's spring break programming from 2004. Jeanette Kedas, an MTV network executive, called the PTC report "unfair and inaccurate" and "underestimating young people's intellect and level of sophistication", while L. Brent Bozell III, then-president of the PTC, stated: "the incessant sleaze on MTV presents the most compelling case yet for consumer cable choice", referring to the practice of pay television companies to allow consumers to pay for channels "à la carte".

In April 2008, PTC released "The Rap on Rap", a study covering hip-hop and R&B music videos rotated on programs "106 & Park" and "Rap City", both shown on BET, and "Sucker Free" on MTV. PTC urged advertisers to withdraw sponsorship of those programs, whose videos PTC stated targeted children and teenagers containing adult content.

MTV received significant criticism from Italian American organizations for "Jersey Shore", which premiered in 2009. The controversy was due in large part to the manner in which MTV marketed the show, as it liberally used the word "guido" to describe the cast members. The word "guido" is generally regarded as an ethnic slur when referring to Italians and Italian Americans. One promotion stated that the show was to follow, "eight of the hottest, tannest, craziest Guidos," while yet another advertisement stated, ""Jersey Shore" exposes one of the tri-state area's most misunderstood species ... the GUIDO. Yes, they really do exist! Our Guidos and Guidettes will move into the ultimate beach house rental and indulge in everything the Seaside Heights, New Jersey scene has to offer."

Prior to the series debut, Unico National formally requested that MTV cancel the show. In a formal letter, the company called the show a "direct, deliberate and disgraceful attack on Italian Americans." Unico National President Andre DiMino said in a statement, "MTV has festooned the 'bordello-like' house set with Italian flags and red, white and green maps of New Jersey while every other cutaway shot is of Italian signs and symbols. They are blatantly as well as subliminally bashing Italian Americans with every technique possible." Around this time, other Italian organizations joined the fight, including the NIAF and the Order Sons of Italy in America.

MTV responded by issuing a press release which stated in part, "The Italian American cast takes pride in their ethnicity. We understand that this show is not intended for every audience and depicts just one aspect of youth culture." Following the calls for the show's removal, several sponsors requested that their ads not be aired during the show. These sponsors included Dell, Domino's Pizza, and American Family Insurance. Despite the loss of certain advertisers, MTV did not cancel the show. Moreover, the show saw its audience increase from its premiere in 2009, and continued to place as MTV's top-rated programs during "Jersey Shore's" six-season run, ending in 2012.

In December 2016, MTV online published a social justice-oriented New Year's resolution-themed video directed towards white men. The video caused widespread outrage online, including video responses from well-known online personas, and was deleted from MTV's YouTube channel. The video was then reuploaded to their channel, with MTV claiming the new video contained "updated graphical elements". The new video quickly received over 10,000 dislikes and fewer than 100 likes from only 20,000 views, and MTV deleted the video for a second time.

In addition to its regular programming, MTV has a long history of promoting social, political, and environmental activism in young people. The channel's vehicles for this activism have been "Choose or Lose", encompassing political causes and encouraging viewers to vote in elections; "Fight For Your Rights", encompassing anti-violence and anti-discrimination causes; "think MTV"; and "MTV Act" and "Power of 12", the newest umbrellas for MTV's social activism.

In 1992, MTV started a pro-democracy campaign called "Choose or Lose", to encourage over 20 million people to register to vote, and the channel hosted a town hall forum for then-candidate Bill Clinton.

In recent years, other politically diverse programs on MTV have included "True Life", which documents people's lives and problems, and MTV News specials, which center on very current events in both the music industry and the world. One special show covered the 2004 US Presidential election, airing programs focused on the issues and opinions of young people, including a program where viewers could ask questions of Senator John Kerry. MTV worked with P. Diddy's "Citizen Change" campaign, designed to encourage young people to vote.

Additionally, MTV aired a documentary covering a trip by the musical group Sum 41 to the Democratic Republic of the Congo, documenting the conflict there. The group ended up being caught in the midst of an attack outside of the hotel and were subsequently flown out of the country.

The channel also began showing presidential campaign commercials for the first time during the 2008 US presidential election. This has led to criticism, with Jonah Goldberg opining that "MTV serves as the Democrats' main youth outreach program."

MTV is aligned with Rock the Vote, a campaign to motivate young adults to register and vote.

In 2012, MTV launched "MTV Act" and "Power of 12", its current social activism campaigns. "MTV Act" focuses on a wide array of social issues, while "Power of 12" was a replacement for MTV's "Choose or Lose" and focused on the 2012 US presidential election.

In 2016, MTV continued its pro-democracy campaign with "Elect This", an issue-oriented look at the 2016 election targeting Millennials. Original content under the "Elect This" umbrella includes "Infographica," short animations summarizing MTV News polls; "Robo-Roundtable," a digital series hosted by animatronic robots; "The Racket," a multi-weekly digital series; and "The Stakes," a weekly political podcast.

Since its launch in 1981, the brand "MTV" has expanded to include many additional properties beyond the original MTV channel, including a variety of sister channels in the US, dozens of affiliated channels around the world, and an Internet presence through MTV.com and related websites.

MTV operates a group of channels under MTV Networksa name that continues to be used for the individual units of the now Viacom Media Networks, a division of corporate parent Viacom. In 1985, MTV saw the introduction of its first true sister channel, VH1, which was originally an acronym for "Video Hits One" and was designed to play adult contemporary music videos. Today, VH1 is aimed at celebrity and popular culture programming which include many reality shows. Another sister channel, CMT, targets the country music and southern culture market.

The advent of satellite television and digital cable brought MTV greater channel diversity, including its current sister channels MTV2 and MTV Tr3́s (now Tr3́s), which initially played music videos exclusively but now focus on other programming. MTV also formerly broadcast MTVU on campuses at various universities until 2018, when the MTv Networks on Campus division was sold, and the channel remained as a digital cable channel only. MTV used to also have MTV Hits and MTVX channels until these were converted into NickMusic and MTV Jams, respectively. MTV Jams later was rebranded as BET Jams.

In the 2000s, MTV launched MTV HD, a 1080i high definition simulcast feed of MTV. Until Viacom's main master control was upgraded in 2013, only the network's original series after 2010 (with some pre-2010 content) are broadcast in high definition, while music videos, despite being among the first television works to convert to high definition presentation in the early 2000s, were presented in 4:3 standard definition, forcing them into a windowboxing type of presentation; since that time all music videos are presented in HD, and are framed to their director's preference. "Jersey Shore", despite being shot with widescreen HD cameras, was also presented with SD windowboxing (though the 2018 "" revival is in full HD). The vast majority of providers carry MTV HD.

MTV Networks also operates MTV Live, a high-definition channel that features original HD music programming and HD versions of music related programs from MTV, VH1 and CMT. The channel was launched in January 2006 as MHD (Music: High Definition). The channel was officially rebranded as MTV Live on February 1, 2016.

In 2005 and 2006, MTV launched a series of channels for Asian Americans. The first channel was MTV Desi, launched in July 2005, dedicated toward South-Asian Americans. Next was MTV Chi, in December 2005, which catered to Chinese Americans. The third was MTV K, launched in June 2006 and targeted toward Korean Americans. Each of these channels featured music videos and shows from MTV's international affiliates as well as original US programming, promos, and packaging. All three of these channels ceased broadcasting on April 30, 2007.

On August 1, 2016, the 35th anniversary of the original MTV's launch, VH1 Classic was rebranded as MTV Classic. The channel's programming focused on classic music videos and programming (including notable episodes of "MTV Unplugged" and "VH1 Storytellers"), but skews more towards the 1980s, 1990s and 2000s. The network aired encores of 2000s MTV series such as "Beavis and Butt-Head" and "". The network's relaunch included a broadcast of MTV's first hour on the air, which was also simulcast on MTV and online via Facebook live streaming. MTV Classic only retained three original VH1 Classic programs, which were "That Metal Show", "Metal Evolution," and "Behind the Music Remastered", although repeats of current and former VH1 programs such as "Pop-Up Video" and "VH1 Storytellers" remained on the schedule. However, the rebranded MTV Classic had few viewers, and declined quickly to become the least-watched English-language subscription network rated by Nielsen at the end of 2016. At the start of 2017, it was reprogrammed into an all-video network.

In the late 1980s, before the World Wide Web, MTV VJ Adam Curry began experimenting on the Internet. He registered the then-unclaimed domain name "MTV.com" in 1993 with the idea of being MTV's unofficial new voice on the Internet. Although this move was sanctioned by his supervisors at MTV Networks at the time, when Curry left to start his own web-portal design and hosting company, MTV subsequently sued him for the domain name, which led to an out-of-court settlement.

The service hosted at the domain name was originally branded "MTV Online" during MTV's first few years of control over it in the mid-1990s. It served as a counterpart to the America Online portal for MTV content, which existed at AOL keyword MTV until approximately the end of the 1990s. After this time, the website became known as simply "MTV.com" and served as the Internet hub for all MTV and MTV News content.

MTV.com experimented with entirely video-based layouts between 2005 and 2007. The experiment began in April 2005 as "MTV Overdrive", a streaming video service that supplemented the regular MTV.com website. Shortly after the 2006 MTV Video Music Awards, which were streamed on MTV.com and heavily utilized the "MTV Overdrive" features, MTV introduced a massive change for MTV.com, transforming the entire site into a Flash video-based entity. Much of users' feedback about the Flash-based site was negative, demonstrating a dissatisfaction with videos that played automatically, commercials that could not be skipped or stopped, and the slower speed of the entire website. The experiment ended in February 2006 as MTV.com reverted to a traditional HTML-based website design with embedded video clips, in the style of YouTube and some other video-based websites.

From 2006 to 2007, MTV operated an online channel, MTV International, targeted to the broad international market. The purpose of the online channel was to air commercial-free music videos once the television channels started concentrating on shows unrelated to music videos or music-related programming.

The channel responded to the rise of the Internet as the new central place to watch music videos in October 2008 by launching MTV Music (later called MTV Hive), a website that featured thousands of music videos from MTV and VH1's video libraries, dating back to the earliest videos from 1981.

A newly created division of the company, MTV New Media, announced in 2008 that it would produce its own original web series, in an attempt to create a bridge between old and new media. The programming is available to viewers via personal computers, cell phones, iPods, and other digital devices.

In the summer of 2012, MTV launched a music discovery web site called the MTV Artists Platform (also known as Artists.MTV). MTV stated, "While technology has made it way easier for artists to produce and distribute their own music on their own terms, it hasn't made it any simpler to find a way to cut through all the Internet noise and speak directly to all of their potential fans. The summer launch of the platform is an attempt to help music junkies and musicians close the gap by providing a one-stop place where fans can listen to and buy music and purchase concert tickets and merchandise."

Today, MTV.com remains the official website of MTV, and it expands on the channel's broadcasts by bringing additional content to its viewers. The site's features include an online version of MTV News, podcasts, a commercial streaming service, movie features, profiles and interviews with recording artists and from MTV's television programs.

MTV Networks has launched numerous native-language regional variants of MTV-branded channels to countries around the world.




</doc>
<doc id="18857" url="https://en.wikipedia.org/wiki?curid=18857" title="Mustelidae">
Mustelidae

The Mustelidae (; from Latin "mustela", weasel) are a family of carnivorous mammals, including weasels, badgers, otters, ferrets, martens, minks, and wolverines, among others. Mustelids () are a diverse group and form the largest family in the order Carnivora, suborder Caniformia. Mustelidae comprises about 56–60 species across eight subfamilies.

Mustelids vary greatly in size and behaviour. The least weasel can be under a foot in length, while the giant otter of Amazonian South America can measure up to and sea otters can exceed in weight. The wolverine can crush bones as thick as the femur of a moose to get at the marrow, and has been seen attempting to drive bears away from their kills. The sea otter uses rocks to break open shellfish to eat. The marten is largely arboreal, while the European badger digs extensive networks of tunnels, called setts. Some mustelids have been domesticated: the ferret and the tayra are kept as pets (although the tayra requires a Dangerous Wild Animals licence in the UK), or as working animals for hunting or vermin control. Others have been important in the fur trade—the mink is often raised for its fur.

As well as being one of the most species-rich families in the order Carnivora, the family Mustelidae is one of the oldest. Mustelid-like forms first appeared about 40 million years ago, roughly coinciding with the appearance of rodents. The direct ancestors of the modern mustelids first appeared about 15 million years ago.

Within a large range of variation, the mustelids exhibit some common characteristics. They are typically small animals with elongated bodies, short legs, short, round ears, and thick fur. Most mustelids are solitary, nocturnal animals, and are active year-round.

With the exception of the sea otter, they have anal scent glands that produce a strong-smelling secretion the animals use for sexual signaling and for marking territory.
Most mustelid reproduction involves embryonic diapause. The embryo does not immediately implant in the uterus, but remains dormant for some time. No development takes place as long as the embryo remains unattached to the uterine lining. As a result, the normal gestation period is extended, sometimes up to a year. This allows the young to be born under more favorable environmental conditions. Reproduction has a large energy cost and it is to a female's benefit to have available food and mild weather. The young are more likely to survive if birth occurs after previous offspring have been weaned.

Mustelids are predominantly carnivorous, although some eat vegetable matter at times. While not all mustelids share an identical dentition, they all possess teeth adapted for eating flesh, including the presence of shearing carnassials. With variation between species, the most common dental formula is .

The martens, fisher and tayra are partially arboreal, while badgers are fossorial. A number of mustelids have aquatic lifestyles, ranging from semiaquatic minks and the several species of river otters to the fully aquatic sea otter. The sea otter is one of the few nonprimate mammals known to use a tool while foraging. It uses "anvil" stones to crack open the shellfish that form a significant part of its diet. It is a "keystone species", keeping its prey populations in balance so some do not outcompete the others and destroy the kelp in which they live.

The black-footed ferret is entirely dependent on another keystone species, the prairie dog. A family of four ferrets eats 250 prairie dogs in a year; this requires a stable population of prairie dogs from an area of some .

The skunks were formerly included as a subfamily of the mustelids, but are now regarded as a separate family (Mephitidae). The mongoose and the meerkat bear a striking resemblance to many mustelids, but belong to a distinctly different suborder—the Feliformia (all those carnivores sharing more recent origins with the cats) and not the Caniformia (those sharing more recent origins with the dogs). Because the mongooses and the mustelids occupy similar ecological niches, convergent evolution has led to some similarity in form and behavior.

The oldest known mustelid from North America is "Corumictis wolsani" from the early and late Oligocene (early and late Arikareean, Ar1–Ar3) of Oregon. Middle Oligocene "Mustelictis" from Europe might be a mustelid as well. Other early fossils of the mustelids were dated at the end of the Oligocene to the beginning of the Miocene. “There is debate regarding which fossils from these epochs represent possible ancestral forms that led to Mustelidae and which fossils represent the first modern mustelids.”(Wund, M. 2005. "Mustelidae" (On-line), Animal Diversity Web.) From the fossil record we can see that Mustelids appeared in the late Oligocene period (33 mya) in Eurasia and migrated throughout the continents. The Mustelids inhabit every continent except Antarctica and Australia. The mustelids migrated all throughout the continents that were connected during the early Miocene. The Mustelids made their way to North and South America via the Bering land bridge.

Several mustelids, including the mink, the sable (a type of marten) and the stoat (ermine), boast exquisite and valuable furs, and have been accordingly hunted since prehistoric times. Since the early Middle Ages, the trade in furs was of great economic importance for northern and eastern European nations with large native populations of fur-bearing mustelids, and was a major economic impetus behind Russian expansion into Siberia and French and English expansion in North America. In recent centuries, fur farming, notably of mink, has also become widespread and provides the majority of the fur brought to market.

One species, the sea mink ("Neovison macrodon") of New England and Canada, was driven to extinction by fur trappers. Its appearance and habits are almost unknown today because no complete specimens can be found and no systematic contemporary studies were conducted.

The sea otter, which has the densest fur of any animal, narrowly escaped the fate of the sea mink. The discovery of large populations in the North Pacific was the major economic driving force behind Russian expansion into Kamchatka, the Aleutian Islands, and Alaska, as well as a cause for conflict with Japan and foreign hunters in the Kuril Islands. Together with widespread hunting in California and British Columbia, the species was brought to the brink of extinction until an international moratorium came into effect in 1911.

Today, some mustelids are threatened for other reasons. Sea otters are vulnerable to oil spills and the indirect effects of overfishing; the black-footed ferret, a relative of the European polecat, suffers from the loss of American prairie; and wolverine populations are slowly declining because of habitat destruction and persecution. The rare European mink "Mustela lutreola" is one of the most endangered mustelid species.

One mustelid, the ferret, has been domesticated and is a fairly common pet.

The 56 living mustelids are classified into eight subfamilies in 22 genera.
Subfamily Taxidiinae

Subfamily Mellivorinae

Subfamily Melinae

Subfamily Helictidinae

Subfamily Guloninae

Subfamily Ictonychinae

Subfamily Lutrinae (otters)

Subfamily Mustelinae

Fossil mustelids
Extinct genera of the family Mustelidae include:

Multigene phylogenies constructed by Koepfli et al. (2008) and Law et al. (2018) found that Mustelidae comprises eight subfamilies. The early mustelids appear to have undergone two rapid bursts of diversification in Eurasia, with the resulting species only spreading to other continents later.

Mustelid species diversity is often attributed to an adaptive radiation coinciding with the Mid-Miocene Climate Transition. Contrary to expectations, Law et al. (2018) found no evidence for rapid bursts of lineage diversification at the origin of Mustelidae, and further analyses of lineage diversification rates using molecular and fossil-based methods did not find associations between rates of lineage diversification and Mid-Miocene Climate Transition as previously hypothesized.



</doc>
<doc id="18858" url="https://en.wikipedia.org/wiki?curid=18858" title="Maryland">
Maryland

Maryland ( ) is a state in the Mid-Atlantic region of the eastern United States, bordering Virginia, West Virginia, and the District of Columbia to its south and west; Pennsylvania to its north; and Delaware and the Atlantic Ocean to its east. The state's largest city is Baltimore, and its capital is Annapolis. Among its occasional nicknames are "Old Line State", the "Free State", and the "Chesapeake Bay State". It is named after the English queen Henrietta Maria, known in England as Queen Mary, who was the wife of King Charles I.

Sixteen of Maryland's twenty-three counties, as well as the city of Baltimore, border the tidal waters of the Chesapeake Bay estuary and its many tributaries, which combined total more than 4,000 miles of shoreline. Although one of the smallest states in the U.S., it features a variety of climates and topographical features that have earned it the moniker of "America in Miniature". In a similar vein, Maryland's geography, culture, and history combine elements of the Mid-Atlantic, Northeastern, and South Atlantic regions of the country.

Before its coastline was explored by Europeans in the 16th century, Maryland was inhabited by several groups of Native Americans, mostly by the Algonquin, and to a lesser degree by the Iroquois and Sioux. As one of the original Thirteen Colonies of Great Britain, Maryland was founded by George Calvert, a Catholic convert who sought to provide a religious haven for Catholics persecuted in England. In 1632, Charles I of England granted Calvert a colonial charter, naming the colony after his wife, Queen Mary (Henrietta Maria of France). Unlike the Pilgrims and Puritans, who rejected Catholicism in their settlements, Calvert envisioned a colony where people of different religious sects would coexist under the principle of toleration. Accordingly, in 1649 the Maryland General Assembly passed an Act Concerning Religion, which enshrined this principle by penalizing anyone who "reproached" a fellow Marylander based on religious affiliation. Nevertheless, religious strife was common in the early years, and Catholics remained a minority, albeit in greater numbers than in any other English colony.

Maryland's early settlements and population centers clustered around rivers and other waterways that empty into the Chesapeake Bay. Its economy was heavily plantation-based, centered mostly on the cultivation of tobacco. The need for cheap labor led to a rapid expansion of indentured servants, penal labor, and African slaves. In 1760, Maryland's current boundaries took form following the settlement of a long-running border dispute with Pennsylvania. Maryland was an active participant in the events leading up to the American Revolution, and by 1776 its delegates signed the Declaration of Independence. Many of its citizens subsequently played key political and military roles in the war. In 1790, the state ceded land for the establishment of the U.S. capital of Washington, D.C.

Although then a slave state, Maryland remained in the Union during the American Civil War, its strategic location giving it a significant role in the conflict. After the war, Maryland took part in the Industrial Revolution, driven by its seaports, railroad networks, and mass immigration from Europe. Since the Second World War, the state's population has grown rapidly, to approximately six million residents, and it is among the most densely populated U.S. states. , Maryland had the highest median household income of any state, owing in large part to its close proximity to Washington, D.C. and a highly diversified economy spanning manufacturing, services, higher education, and biotechnology. The state's central role in U.S. history is reflected by its hosting of some of the highest numbers of historic landmarks per capita.

Maryland has an area of and is comparable in overall area with Belgium []. It is the 42nd largest and 9th smallest state and is closest in size to the state of Hawaii [], the next smaller state. The next larger state, its neighbor West Virginia, is almost twice the size of Maryland [].

Maryland possesses a variety of topography within its borders, contributing to its nickname "America in Miniature". It ranges from sandy dunes dotted with seagrass in the east, to low marshlands teeming with wildlife and large bald cypress near the Chesapeake Bay, to gently rolling hills of oak forests in the Piedmont Region, and pine groves in the Maryland mountains to the west.

Maryland is bounded on its north by Pennsylvania, on its west by West Virginia, on its east by Delaware and the Atlantic Ocean, and on its south, across the Potomac River, by West Virginia and Virginia. The mid-portion of this border is interrupted by District of Columbia, which sits on land that was originally part of Montgomery and Prince George's counties and including the town of Georgetown, Maryland. This land was ceded to the United States Federal Government in 1790 to form the District of Columbia. (The Commonwealth of Virginia gave land south of the Potomac, including the town of Alexandria, Virginia, however Virginia retroceded its portion in 1846). The Chesapeake Bay nearly bisects the state and the counties east of the bay are known collectively as the "Eastern Shore".

Most of the state's waterways are part of the Chesapeake Bay watershed, with the exceptions of a tiny portion of extreme western Garrett County (drained by the Youghiogheny River as part of the watershed of the Mississippi River), the eastern half of Worcester County (which drains into Maryland's Atlantic coastal bays), and a small portion of the state's northeast corner (which drains into the Delaware River watershed). So prominent is the Chesapeake in Maryland's geography and economic life that there has been periodic agitation to change the state's official nickname to the "Bay State", a nickname that has been used by Massachusetts for decades.

The highest point in Maryland, with an elevation of , is Hoye Crest on Backbone Mountain, in the southwest corner of Garrett County, near the border with West Virginia, and near the headwaters of the North Branch of the Potomac River. Close to the small town of Hancock, in western Maryland, about two-thirds of the way across the state, there are between its borders. This geographical curiosity makes Maryland the narrowest state, bordered by the Mason–Dixon line to the north, and the northwards-arching Potomac River to the south.

Portions of Maryland are included in various official and unofficial geographic regions. For example, the Delmarva Peninsula is composed of the Eastern Shore counties of Maryland, the entire state of Delaware, and the two counties that make up the Eastern Shore of Virginia, whereas the westernmost counties of Maryland are considered part of Appalachia. Much of the Baltimore–Washington corridor lies just south of the Piedmont in the Coastal Plain, though it straddles the border between the two regions.

Earthquakes in Maryland are infrequent and small due to the state's distance from seismic/earthquake zones. The M5.8 Virginia earthquake in 2011 was felt moderately throughout Maryland. Buildings in the state are not well-designed for earthquakes and can suffer damage easily.

Maryland has no natural lakes, mostly due to the lack of glacial history in the area. All lakes in the state today were constructed, mostly via dams. Buckel's Bog is believed by geologists to have been a remnant of a former natural lake.

Maryland has shale formations containing natural gas, where fracking is theoretically possible.

As is typical of states on the East Coast, Maryland's plant life is abundant and healthy. A modest volume of annual precipitation helps to support many types of plants, including seagrass and various reeds at the smaller end of the spectrum to the gigantic Wye Oak, a huge example of white oak, the state tree, which can grow in excess of tall.

Middle Atlantic coastal forests, typical of the southeastern Atlantic coastal plain, grow around Chesapeake Bay and on the Delmarva Peninsula. Moving west, a mixture of Northeastern coastal forests and Southeastern mixed forests cover the central part of the state. The Appalachian Mountains of western Maryland are home to Appalachian-Blue Ridge forests. These give way to Appalachian mixed mesophytic forests near the West Virginia border.

Many foreign species are cultivated in the state, some as ornamentals, others as novelty species. Included among these are the crape myrtle, Italian cypress, southern magnolia, live oak in the warmer parts of the state, and even hardy palm trees in the warmer central and eastern parts of the state. USDA plant hardiness zones in the state range from Zones 5 and 6 in the extreme western part of the state to Zone 7 in the central part, and Zone 8 around the southern part of the coast, the bay area, and parts of metropolitan Baltimore. Invasive plant species, such as kudzu, tree of heaven, multiflora rose, and Japanese stiltgrass, stifle growth of endemic plant life. Maryland's state flower, the black-eyed susan, grows in abundance in wild flower groups throughout the state.

The state harbors a great number of white-tailed deer, especially in the woody and mountainous west of the state, and overpopulation can become a problem. Mammals can be found ranging from the mountains in the west to the central areas and include black bears, bobcats, foxes, coyotes, raccoons, and otters.

There is a population of rare wild (feral) horses found on Assateague Island. They are believed to be descended from horses who escaped from Spanish galleon shipwrecks. Every year during the last week of July, they are captured and swim across a shallow bay for sale at Chincoteague, Virginia, a conservation technique which ensures the tiny island is not overrun by the horses. The ponies and their sale were popularized by the children's book, "Misty of Chincoteague."

The purebred Chesapeake Bay Retriever dog was bred specifically for water sports, hunting and search and rescue in the Chesapeake area. In 1878 the Chesapeake Bay Retriever was the first individual retriever breed recognized by the American Kennel Club. and was later adopted by the University of Maryland, Baltimore County as their mascot.

Maryland's reptile and amphibian population includes the diamondback terrapin turtle, which was adopted as the mascot of University of Maryland, College Park, as well as the threatened Eastern box turtle 
. The state is part of the territory of the Baltimore oriole, which is the official state bird and mascot of the MLB team the Baltimore Orioles. Aside from the oriole, 435 other species of birds have been reported from Maryland.

The state insect is the Baltimore checkerspot butterfly, although it is not as common in Maryland as it is in the southern edge of its range.

Maryland joined with neighboring states during the end of the 20th century to improve the health of the Chesapeake Bay. The bay's aquatic life and seafood industry have been threatened by development and by fertilizer and livestock waste entering the bay.

In 2007, Forbes.com rated Maryland as the fifth "Greenest" state in the country behind three of the Pacific States and Vermont. Maryland ranks 40th in total energy consumption nationwide, and it managed less toxic waste per capita than all but six states in 2005. In April 2007 Maryland joined the Regional Greenhouse Gas Initiative (RGGI)—a regional initiative formed by all of the Northeastern states, Washington D.C., and three Canadian provinces to reduce greenhouse gas emissions. In March 2017, Maryland became the first state with proven gas reserves to ban fracking by passing a law against it. Vermont has such a law, but no shale gas, and New York has such a ban, though it was made by executive order.

Maryland has a wide array of climates, due to local variances in elevation, proximity to water, and protection from colder weather due to downslope winds.

The eastern half of Maryland—which includes the cities of Ocean City, Salisbury, Annapolis, and the southern and eastern suburbs of Washington, D.C. and Baltimore—lies on the Atlantic Coastal Plain, with flat topography and sandy or muddy soil. This region has a humid subtropical climate (Köppen "Cfa"), with hot, humid summers and a short, mild to cool winter; it falls under USDA Hardiness zone 8a.

The Piedmont region—which includes northern and western greater Baltimore, Westminster, Gaithersburg, Frederick, and Hagerstown—has average seasonal snowfall totals generally exceeding and, as part of USDA Hardiness zones 7b and 7a, temperatures below are less rare. From the Cumberland Valley on westward, the climate begins to transition to a humid continental climate (Köppen "Dfa").

In western Maryland, the higher elevations of Allegany and Garrett counties—including the cities of Cumberland, Frostburg, and Oakland—display more characteristics of the humid continental zone, due in part to elevation. They fall under USDA Hardiness zones 6b and below.

Precipitation in the state is characteristic of the East Coast. Annual rainfall ranges from with more in higher elevations. Nearly every part of Maryland receives per month of rain. Average annual snowfall varies from in the coastal areas to over in the western mountains of the state.

Because of its location near the Atlantic Coast, Maryland is somewhat vulnerable to tropical cyclones, although the Delmarva Peninsula and the outer banks of North Carolina provide a large buffer, such that strikes from major hurricanes (category 3 or above) occur infrequently. More often, Maryland gets the remnants of a tropical system which has already come ashore and released most of its energy. Maryland averages around 30–40 days of thunderstorms a year, and averages around six tornado strikes annually.

George Calvert, 1st Lord Baltimore (1579–1632), sought a charter from King Charles I for the territory between Massachusetts to the north and Virginia to the immediate south.
After the first Lord Baltimore died in April 1632, the charter was granted to his son, Cecilius Calvert, 2nd Baron Baltimore (1605–1675), on June 20, 1632. Officially, the new "Maryland Colony" was named in honor of Henrietta Maria of France, wife of Charles I of England. George Calvert initially proposed the name "Crescentia", the land of growth or increase, but "the King proposed Terra Mariae [Mary Land], which was concluded on and Inserted in the bill."

The original capital of Maryland was St. Mary's City, on the north shore of the Potomac River, and the county surrounding it, the first erected/created in the province, was first called Augusta Carolina, after the King, and later named St. Mary's County.

Lord Baltimore's first settlers arrived in the new colony in March 1634, with his younger brother Leonard Calvert (1606–1647), as first provincial Governor of Maryland. They made their first permanent settlement at St. Mary's City in what is now St. Mary's County. They purchased the site from the paramount chief of the region, who was eager to establish trade. St. Mary's became the first capital of Maryland, and remained so for 60 years until 1695. More settlers soon followed. Their tobacco crops were successful and quickly made the new colony profitable. However, given the incidence of malaria, yellow fever and typhoid, life expectancy in Maryland was about 10 years less than in New England.

Maryland was founded for the purpose of providing religious toleration of England's Roman Catholic minority.
Although Maryland was the most heavily Catholic of the England mainland colonies, this religious group was still in the minority, consisting of less than 10% of the total population.

In 1642 a number of Puritans left Virginia for Maryland and founded Providence (now called Annapolis) on the western shore of the upper Chesapeake Bay. A dispute with traders from Virginia over Kent Island in the Chesapeake led to armed conflict. In 1644 William Claiborne, a Puritan, seized Kent Island while his associate, the pro-Parliament Puritan Richard Ingle, took over St. Mary's. Both used religion as a tool to gain popular support. The two years from 1644–1646 that Claiborne and his Puritan associates held sway were known as "The Plundering Time". They captured Jesuit priests, imprisoned them, then sent them back to England.

In 1646 Leonard Calvert returned with troops, recaptured St. Mary's City, and restored order. The House of Delegates passed the "Act concerning Religion" in 1649 granting religious liberty to all Trinitarian Christians.

In 1650 the Puritans revolted against the proprietary government. "Protestants swept the Catholics out of the legislature ...and religious strife returned".
The Puritans set up a new government prohibiting both Roman Catholicism and Anglicanism. The Puritan revolutionary government persecuted Maryland Catholics during its reign, known as the "plundering time". Mobs burned down all the original Catholic churches of southern Maryland. The Puritan rule lasted until 1658 when the Calvert family and Lord Baltimore regained proprietary control and re-enacted the Toleration Act.

After England's "Glorious Revolution" of 1688, Maryland outlawed Catholicism. In 1704, the Maryland General Assembly prohibited Catholics from operating schools, limited the corporate ownership of property to hamper religious orders from expanding or supporting themselves, and encouraged the conversion of Catholic children. The celebration of the Catholic sacraments was also officially restricted. This state of affairs lasted until after the American Revolutionary War (1775–1783). Wealthy Catholic planters built chapels on their land to practice their religion in relative secrecy.

Into the 18th century, individual priests and lay leaders claimed Maryland farms belonging to the Jesuits as personal property and bequeathed these and other properties to other religious or lay people in order to evade the legal restrictions on religious organizations – such as the Society of Jesus – owning property.

The royal charter granted Maryland the land north of the Potomac River up to the 40th parallel. A problem arose when Charles II granted a charter for Pennsylvania. The grant defined Pennsylvania's southern border as identical to Maryland's northern border, the 40th parallel. But the grant indicated that Charles II and William Penn assumed the 40th parallel would pass close to New Castle, Delaware when it falls north of Philadelphia, the site of which Penn had already selected for his colony's capital city. Negotiations ensued after the problem was discovered in 1681.

A compromise proposed by Charles II in 1682 was undermined by Penn's receiving the additional grant of what is now Delaware. Penn successfully argued that the Maryland charter entitled Lord Baltimore only to unsettled lands, and Dutch settlement in Delaware predated his charter. The dispute remained unresolved for nearly a century, carried on by the descendants of William Penn and Lord Baltimore — the Calvert family, which controlled Maryland, and the Penn family, which controlled Pennsylvania.

The border dispute with Pennsylvania led to Cresap's War in the 1730s. Hostilities erupted in 1730 and escalated through the first half of the decade, culminating in the deployment of military forces by Maryland in 1736 and by Pennsylvania in 1737. The armed phase of the conflict ended in May 1738 with the intervention of King George II, who compelled the negotiation of a cease-fire. A provisional agreement had been established in 1732.

Negotiations continued until a final agreement was signed in 1760. The agreement defined the border between Maryland and Pennsylvania as the line of latitude now known as the Mason–Dixon line. Maryland's border with Delaware was based on a Transpeninsular Line and the Twelve-Mile Circle around New Castle.

Most of the English colonists arrived in Maryland as indentured servants, and had to serve a several years' term as laborers to pay for their passage. In the early years, the line between indentured servants and African slaves or laborers was fluid, and white and black laborers commonly lived and worked together, and formed unions. Mixed-race children born to white mothers were considered free by the principle of "partus sequitur ventrem", by which children took the social status of their mothers, a principle of slave law that was adopted throughout the colonies, following Virginia in 1662. During the colonial era, families of free people of color were formed most often by unions of white women and African men.
Many of the free black families migrated to Delaware, where land was cheaper. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, planters in Maryland imported thousands more slaves and racial caste lines hardened. The economy's growth and prosperity was based on slave labor, devoted first to the production of tobacco as the commodity crop.

Maryland was one of the thirteen colonies that revolted against British rule in the American Revolution. Near the end of the American Revolutionary War (1775–1783), on February 2, 1781, Maryland became the last and 13th state to approve the ratification of the Articles of Confederation and Perpetual Union, first proposed in 1776 and adopted by the Second Continental Congress in 1778, which brought into being the United States as a united, sovereign and national state. It also became the seventh state admitted to the Union after ratifying the new federal Constitution in 1788. In December 1790, Maryland donated land selected by first President George Washington to the federal government for the creation of the new national capital of Washington, D.C. The land was provided along the north shore of the Potomac River from Montgomery and Prince George's counties, as well as from Fairfax County and Alexandria on the south shore of the Potomac in Virginia; however, the land donated by the Commonwealth of Virginia was later returned to that state by the District of Columbia retrocession in 1846.

Influenced by a changing economy, revolutionary ideals, and preaching by ministers, numerous planters in Maryland freed their slaves in the 20 years after the Revolutionary War. Across the Upper South the free black population increased from less than 1% before the war to 14% by 1810. Abolitionist Harriet Tubman was born a slave during this time in Dorchester County, Maryland.

During the War of 1812, the British military attempted to capture Baltimore, which was protected by Fort McHenry. During this bombardment the song "Star Spangled Banner" was written by Francis Scott Key; it was later adopted as the national anthem.

The National Road (U.S. Hwy 40 today) was authorized in 1817 and ran from Baltimore to St. Louis – the first federal highway.
The Baltimore and Ohio Railroad (B&O) was the first chartered railroad in the United States. It opened its first section of track for regular operation in 1830 between Baltimore and Ellicott City, and in 1852 it became the first rail line to reach the Ohio River from the eastern seaboard.

The state remained with the Union during the Civil War, due in significant part to demographics and Federal intervention. The 1860 census, held shortly before the outbreak of the civil war, showed that 49% of Maryland's African Americans were free blacks.

Governor Thomas Holliday Hicks suspended the state legislature, and to help ensure the election of a new pro-union governor and legislature, President Abraham Lincoln had a number of its pro-slavery politicians arrested, including the Mayor of Baltimore, George William Brown; suspended several civil liberties, including "habeas corpus"; and ordered artillery placed on Federal Hill overlooking Baltimore. Historians debate the constitutionality of these wartime actions, and the suspension of civil liberties was later deemed illegal by the U.S. Supreme Court.

In April 1861 Federal units and state regiments were attacked as they marched through Baltimore, sparking the Baltimore riot of 1861, the first bloodshed in the Civil War. Of the 115,000 men from Maryland who joined the military during the Civil War, 85,000, or 77%, joined the Union army, while the remainder joined the Confederate Army. The largest and most significant battle in the state was the Battle of Antietam on September 17, 1862, near Sharpsburg. Although a tactical draw, the battle was considered a strategic Union victory and a turning point of the war.

A new state constitution in 1864 abolished slavery and Maryland was first recognized as a "Free State" in that context. Following passage of constitutional amendments that granted voting rights to freedmen, in 1867 the state extended suffrage to non-white males.

The Democratic Party rapidly regained power in the state from Republicans. Democrats replaced the Constitution of 1864 with the Constitution of 1867. Following the end of Reconstruction in 1877, Democrats devised means of disfranchising blacks, initially by physical intimidation and voter fraud, later by constitutional amendments and laws.
Blacks and immigrants, however, resisted Democratic Party disfranchisement efforts in the state. Maryland blacks were part of a biracial Republican coalition elected to state government in 1896–1904 and comprised 20% of the electorate.

Compared to some other states, blacks were better established both before and after the civil war. Nearly half the black population was free before the war, and some had accumulated property. Half the population lived in cities. Literacy was high among blacks and, as Democrats crafted means to exclude them, suffrage campaigns helped reach blacks and teach them how to resist. Whites did impose racial segregation in public facilities and Jim Crow laws, which effectively lasted until passage of federal civil rights legislation in the mid-1960s.

Baltimore grew significantly during the Industrial Revolution, due in large part to its seaport and good railroad connections, attracting European immigrant labor. Many manufacturing businesses were established in the Baltimore area after the Civil War. Baltimore businessmen, including Johns Hopkins, Enoch Pratt, George Peabody, and Henry Walters, founded notable city institutions that bear their names, including a university, library, music school and art museum.

Cumberland was Maryland's second-largest city in the 19th century. Nearby supplies of natural resources along with railroads fostered its growth into a major manufacturing center.

The Progressive Era of the late 19th and early 20th centuries brought political reforms. In a series of laws passed between 1892 and 1908, reformers worked for standard state-issued ballots (rather than those distributed and marked by the parties); obtained closed voting booths to prevent party workers from "assisting" voters; initiated primary elections to keep party bosses from selecting candidates; and had candidates listed without party symbols, which discouraged the illiterate from participating. These measures worked against ill-educated whites and blacks. Blacks resisted such efforts, with suffrage groups conducting voter education.
Blacks defeated three efforts to disfranchise them, making alliances with immigrants to resist various Democratic campaigns. Disfranchising bills in 1905, 1907, and 1911 were rebuffed, in large part because of black opposition. Blacks comprised 20% of the electorate and immigrants comprised 15%, and the legislature had difficulty devising requirements against blacks that did not also disadvantage immigrants.

The Progressive Era also brought reforms in working conditions for Maryland's labor force. In 1902 the state regulated conditions in mines; outlawed child laborers under the age of 12; mandated compulsory school attendance; and enacted the nation's first workers' compensation law. The workers' compensation law was overturned in the courts, but was redrafted and finally enacted in 1910.

The Great Baltimore Fire of 1904 burned for more than 30 hours, destroying 1,526 buildings and spanning 70 city blocks. More than 1,231 firefighters worked to bring the blaze under control. 

With the nation's entry into World War I in 1917, new military bases such as Camp Meade, the Aberdeen Proving Ground, and the Edgewood Arsenal were established. Existing facilities, including Fort McHenry, were greatly expanded.

After Georgia congressman William D. Upshaw criticized Maryland openly in 1923 for not passing Prohibition laws, "Baltimore Sun" editor Hamilton Owens coined the "Free State" nickname for Maryland in that context, which was popularized by H. L. Mencken in a series of newspaper editorials.

Maryland's urban and rural communities had different experiences during the Great Depression. The "Bonus Army" marched through the state in 1932 on its way to Washington, D.C. Maryland instituted its first ever income tax in 1937 to generate revenue for schools and welfare.

Passenger and freight steamboat service, once important throughout Chesapeake Bay and its many tributary rivers, ended in 1938.

Baltimore was a major war production center during World War II. The biggest operations were Bethlehem Steel's Fairfield Yard, which built Liberty ships; and Glenn Martin, an aircraft manufacturer.

Maryland experienced population growth following World War II. Beginning in the 1960s, as suburban growth took hold around Washington DC and Baltimore, the state began to take on a more mid-Atlantic culture as opposed to the traditionally Southern and Tidewater culture that previously dominated most of the state. Agricultural tracts gave way to residential communities, some of them carefully planned such as Columbia, St. Charles, and Montgomery Village. Concurrently the Interstate Highway System was built throughout the state, most notably I-95, I-695, and the Capital Beltway, altering travel patterns. In 1952 the eastern and western halves of Maryland were linked for the first time by the Chesapeake Bay Bridge, which replaced a nearby ferry service.

Maryland's regions experienced economic changes following WWII. Heavy manufacturing declined in Baltimore. In Maryland's four westernmost counties, industrial, railroad, and coal mining jobs declined. On the lower Eastern Shore, family farms were bought up by major concerns and large-scale poultry farms and vegetable farming became prevalent. In Southern Maryland, tobacco farming nearly vanished due to suburban development and a state tobacco buy-out program in the 1990s.

In an effort to reverse depopulation due to the loss of working-class industries, Baltimore initiated urban renewal projects in the 1960s with Charles Center and the Baltimore World Trade Center. Some resulted in the break-up of intact residential neighborhoods, producing social volatility, and some older residential areas around the harbor have had units renovated and have become popular with new populations.

The United States Census Bureau estimates that the population of Maryland was 6,042,718 on July 1, 2018, a 4.71% increase since the 2010 United States Census and an increase of 2,962, from the prior year. This includes a natural increase since the last census of 269,166 people (that is 464,251 births minus 275,093 deaths) and an increase due to net migration of 116,713 people into the state. Immigration from outside the United States resulted in a net increase of 129,730 people, and migration within the country produced a net loss of 13,017 people.

The center of population of Maryland is located on the county line between Anne Arundel County and Howard County, in the unincorporated community of Jessup.

Maryland's history as a border state has led it to exhibit characteristics of both the Northern and Southern regions of the United States. Generally, rural Western Maryland between the West Virginian Panhandle and Pennsylvania has an Appalachian culture; the Southern and Eastern Shore regions of Maryland embody a Southern culture,
while densely populated Central Maryland—radiating outward from Baltimore and Washington, D.C.—has more in common with that of the Northeast.
The U.S. Census Bureau designates Maryland as one of the South Atlantic States, but it is commonly associated with the Mid-Atlantic States and/or Northeastern United States by other federal agencies, the media, and some residents.

As of 2011, 58.0 percent of Maryland's population younger than age 1 were non-white.

"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."

Spanish (including Spanish Creole) is the second most spoken language in Maryland, after English. The third and fourth most spoken languages are French (including Patois and Cajun) and Chinese. Other commonly spoken languages include various African languages, Korean, German, Tagalog, Russian, Vietnamese, Italian, various Asian languages, Persian, Hindi and other Indic languages, Greek and Arabic.

Most of the population of Maryland lives in the central region of the state, in the Baltimore metropolitan area and Washington metropolitan area, both of which are part of the Baltimore–Washington metropolitan area.
The majority of Maryland's population is concentrated in the cities and suburbs surrounding Washington, D.C., as well as in and around Maryland's most populous city, Baltimore. Historically, these and many other Maryland cities developed along the Fall Line, the line along which rivers, brooks, and streams are interrupted by rapids and/or waterfalls. Maryland's capital city, Annapolis, is one exception to this pattern, since it lies along the banks of the Severn River, close to where it empties into the Chesapeake Bay.

The Eastern Shore is less populous and more rural, as are the counties of western Maryland. The two westernmost counties of Maryland, Allegany and Garrett, are mountainous and sparsely populated, resembling West Virginia and Appalachia more than they do the rest of the state. Both eastern and western Maryland are, however, dotted with cities of regional importance, such as Ocean City, Princess Anne, and Salisbury on the Eastern Shore and Cumberland, Frostburg, and Hancock in Western Maryland.
Southern Maryland is still somewhat rural, but suburbanization from Washington, D.C. has encroached significantly since the 1960s; important local population centers include Lexington Park, Prince Frederick, and Waldorf.

In 1970 the Census Bureau reported Maryland's population as 17.8 percent African-American and 80.4 percent non-Hispanic White.

African Americans form a sizable portion of the state's population – nearly 30 percent in 2010. Most are descendants of people transported to the area as slaves from West Africa, and many are of mixed race, including European and Native American ancestry. Concentrations of African Americans live in Baltimore City, Prince George's County, a suburb of Washington, D.C., where many work; Charles County, western parts of Baltimore County, and the southern Eastern Shore. New residents of African descent include 20th-century and later immigrants from Nigeria, particularly of the Igbo and Yoruba tribes. Maryland also hosts populations from other African and Caribbean nations. Many immigrants from the Horn of Africa have settled in Maryland, with large communities existing in the suburbs of Washington, D.C. (particularly Montgomery County and Prince George's County) and the city of Baltimore. The Greater Washington area has the largest population of Ethiopians outside of Africa. The Ethiopian community of Greater DC was historically based in Washington D.C.'s Adams Morgan and Shaw neighborhoods, but as the community has grown, many Ethiopians have settled in Silver Spring. The Washington, D.C. metropolitan area is also home to large Eritrean and Somali communities.

The top reported ancestries by Maryland residents are: German (15%), Irish (11%), English (8%), American (7%), Italian (6%), and Polish (3%).

Irish American populations can be found throughout the Baltimore area, and the Northern and Eastern suburbs of Washington D.C. in Maryland (descendants of those who moved out to the suburbs of Washington's once predominantly Irish neighborhoods), as well as Western Maryland, where Irish immigrant laborers helped to build the B & O Railroad. Smaller but much older Irish populations can be found in Southern Maryland, with some roots dating as far back as the early Maryland colony. This population, however, still remains culturally very active and yearly festivals are held.

A large percentage of the population of the Eastern Shore and Southern Maryland are descendants of British American ancestry. The Eastern Shore was settled by Protestants, chiefly Methodist and the southern counties were initially settled by English Catholics. Western and northern Maryland have large German-American populations. More recent European immigrants of the late 19th and early 20th century settled first in Baltimore, attracted to its industrial jobs. Many of their ethnic Italian, Polish, Czech, Lithuanian, and Greek descendants still live in the area.

Large ethnic minorities include Eastern Europeans such as Croatians, Belarusians, Russians and Ukrainians. The shares of European immigrants born in Eastern Europe increased significantly between 1990 and 2010. Following the dissolution of the Soviet Union, Yugoslavia, and Czechoslovakia, many immigrants from Eastern Europe came to the United States - 12 percent of which currently reside in Maryland.

Hispanic immigrants of the later 20th century have settled in Aspen Hill, Hyattsville/Langley Park, Glenmont/Wheaton, Bladensburg, Riverdale Park, Gaithersburg, as well as Highlandtown and Greektown in East Baltimore. Salvadorans are the largest Hispanic group in Maryland. Other Hispanic groups with significant populations in the state include Mexicans and Puerto Ricans and Hondurans. Though the Salvadoran population is more concentrated in the area around Washington, D.C., and the Puerto Rican population is more concentrated in the Baltimore area, all other major Hispanic groups in the state are evenly dispersed between these two areas. Maryland has one of the most diverse Hispanic populations in the country, with significant populations from various Caribbean and Central American nations.

Jews are numerous throughout Montgomery County and in Pikesville and Owings Mills northwest of Baltimore. An estimated 81,500 Jewish Americans live in Montgomery County, constituting approximately 10% of the total population.

Asian Americans are concentrated in the suburban counties surrounding Washington, D.C. and in Howard County, with Korean American and Taiwanese American communities in Rockville, Gaithersburg, and Germantown and a Filipino American community in Fort Washington. Numerous Indian Americans live across the state, especially in central Maryland. Amish/Mennonite communities are found in St. Mary's, Garrett, and Cecil counties.

Attracting educated Asians and Africans to the professional jobs in the region, Maryland has the fifth-largest proportions of racial minorities in the country.

In 2006 645,744 were counted as foreign born, which represents mainly people from Latin America and Asia. About 4.0 percent are undocumented immigrants. Maryland also has a large Korean American population. In fact, 1.7 percent are Korean, while as a whole, almost 6.0 percent are Asian.

According to The Williams Institute's analysis of the 2010 U.S. Census, 12,538 same-sex couples are living in Maryland, representing 5.8 same-sex couples per 1,000 households.

As of 2018, non-Hispanic white Americans were 50.5% of Maryland's population (White Americans, including White Hispanics, were 58.8%), making Maryland on the verge of becoming a majority minority state. 49.5% of Maryland's population is non-white and/or Hispanic/Latino, the highest percentage of any state on the East Coast and the highest percentage after the majority minority states of Hawaii, New Mexico, Texas, California and Nevada. Non-Hispanic White Americans in Maryland, the majority as of 2016, are expected to become the plurality ethnic group within 5 years of 2015. After Nevada in 2016, Maryland is projected to be the next state to become majority minority due to growing African-American, Asian and Latino populations. By 2031, minorities are projected to become the majority of voting eligible residents of Maryland.

Maryland has been historically prominent to American Catholic tradition because the English colony of Maryland was intended by George Calvert as a haven for English Catholics. Baltimore was the seat of the first Catholic bishop in the U.S. (1789), and Emmitsburg was the home and burial place of the first American-born citizen to be canonized, St. Elizabeth Ann Seton. Georgetown University, the first Catholic University, was founded in 1789 in what was then part of Maryland. The Basilica of the National Shrine of the Assumption of the Virgin Mary in Baltimore was the first Roman Catholic cathedral built in the United States, and the Archbishop of Baltimore is, albeit without formal primacy, the United States' quasi-primate, and often a cardinal. Among the immigrants of the 19th and 20th century from eastern and southern Europe were many Catholics.

Despite its historic relevance to the Catholic Church in the United States, the percentage of Catholics in the state of Maryland is below the national average of 20%. Demographically, both Protestants and those identifying no religion are more numerous than Catholics.

According to Pew research 69 percent of Maryland's population identifies as Christian. The largest religious groups in Maryland as of 2010 were the Catholic Church with 837,338 adherents in Maryland, followed by non-denominational Evangelical Protestants with 298,921 members, and the United Methodist Church with 238,774. The Southern Baptist Convention has 150,345 members.
Judaism is the largest non-Christian religion in Maryland with 241,000 adherents, or 4 percent of the total population.
The Seventh-day Adventist Church's world headquarters and Ahmadiyya Muslims' national headquarters are located in Silver Spring, just outside the District of Columbia.

The Bureau of Economic Analysis estimates that Maryland's gross state product in 2016 was $382.4 billion. However, Maryland has been using Genuine Progress Indicator, an indicator of well-being, to guide the state's development, rather than relying only on growth indicators like GDP. According to the U.S. Census Bureau, Maryland households are currently the wealthiest in the country, with a 2013 median household income of $72,483 which puts it ahead of New Jersey and Connecticut, which are second and third respectively. Two of Maryland's counties, Howard and Montgomery, are the second and eleventh wealthiest counties in the nation respectively. Maryland ranked No. 1 with the most millionaires per capita in 2013, with a ratio of 7.7 percent. Also, the state's poverty rate of 7.8 percent is the lowest in the country. per capita personal income in 2006 was $43,500, fifth in the nation. As of February 2018, the state's unemployment rate was 4.2 percent.

Maryland's economy benefits from the state's close proximity to the federal government in Washington, D.C. with an emphasis on technical and administrative tasks for the defense/aerospace industry and bio-research laboratories, as well as staffing of satellite government headquarters in the suburban or exurban Baltimore/Washington area. Ft. Meade serves as the headquarters of the Defense Information Systems Agency, United States Cyber Command, and the National Security Agency/Central Security Service. In addition, a number of educational and medical research institutions are located in the state. In fact, the various components of The Johns Hopkins University and its medical research facilities are now the largest single employer in the Baltimore area. Altogether, white collar technical and administrative workers comprise 25 percent of Maryland's labor force, attributable in part to nearby Maryland being a part of the Washington Metro Area where the federal government office employment is relatively high.

Manufacturing, while large in dollar value, is highly diversified with no sub-sector contributing over 20 percent of the total. Typical forms of manufacturing include electronics, computer equipment, and chemicals. The once mighty primary metals sub-sector, which at one time included what was then the largest steel factory in the world at Sparrows Point, still exists, but is pressed with foreign competition, bankruptcies, and mergers. During World War II the Glenn Martin Company (now part of Lockheed Martin) airplane factory employed some 40,000 people.

Mining other than construction materials is virtually limited to coal, which is located in the mountainous western part of the state. The brownstone quarries in the east, which gave Baltimore and Washington much of their characteristic architecture in the mid-19th century, were once a predominant natural resource. Historically, there used to be small gold-mining operations in Maryland, some near Washington, but these no longer exist.

One major service activity is transportation, centered on the Port of Baltimore and its related rail and trucking access. The port ranked 17th in the U.S. by tonnage in 2008. Although the port handles a wide variety of products, the most typical imports are raw materials and bulk commodities, such as iron ore, petroleum, sugar, and fertilizers, often distributed to the relatively close manufacturing centers of the inland Midwest via good overland transportation. The port also receives several different brands of imported motor vehicles and is the number one auto port in the U.S.

Baltimore City is the eighth largest port in the nation, and was at the center of the February 2006 controversy over the Dubai Ports World deal because it was considered to be of such strategic importance. The state as a whole is heavily industrialized, with a booming economy and influential technology centers. Its computer industries are some of the most sophisticated in the United States, and the federal government has invested heavily in the area. Maryland is home to several large military bases and scores of high level government jobs.

The Chesapeake and Delaware Canal is a canal on the Eastern Shore that connects the waters of the Delaware River with those of the Chesapeake Bay, and in particular with the Port of Baltimore, carrying 40 percent of the port's ship traffic.

Maryland has a large food-production sector. A large component of this is commercial fishing, centered in the Chesapeake Bay, but also including activity off the short Atlantic seacoast. The largest catches by species are the blue crab, oysters, striped bass, and menhaden. The Bay also has overwintering waterfowl in its wildlife refuges. The waterfowl support a tourism sector of sportsmen.
Maryland has large areas of fertile agricultural land in its coastal and Piedmont zones, though this land use is being encroached upon by urbanization. Agriculture is oriented to dairy farming (especially in foothill and piedmont areas) for nearby large city milksheads plus specialty perishable horticulture crops, such as cucumbers, watermelons, sweet corn, tomatoes, muskmelons, squash, and peas (Source:USDA Crop Profiles). In addition, the southern counties of the western shoreline of Chesapeake Bay are warm enough to support a tobacco cash crop zone, which has existed since early Colonial times but declined greatly after a state government buyout in the 1990s. There is also a large automated chicken-farming sector in the state's southeastern part; Salisbury is home to Perdue Farms. Maryland's food-processing plants are the most significant type of manufacturing by value in the state.

Maryland is a major center for life sciences research and development. With more than 400 biotechnology companies located there, Maryland is the fourth-largest nexus in this field in the United States.

Institutions and government agencies with an interest in research and development located in Maryland include the Johns Hopkins University, the Johns Hopkins Applied Physics Laboratory, more than one campus of the University System of Maryland, Goddard Space Flight Center, the United States Census Bureau, the National Institutes of Health (NIH), the National Institute of Standards and Technology (NIST), the National Institute of Mental Health (NIMH), the Walter Reed National Military Medical Center, the federal Food and Drug Administration (FDA), the Howard Hughes Medical Institute, the Celera Genomics company, the J. Craig Venter Institute (JCVI), and MedImmune – recently purchased by AstraZeneca.

Maryland is home to defense contractor Emergent BioSolutions, which manufactures and provides an anthrax vaccine to U.S. government military personnel.

Tourism is popular in Maryland, with tourists visiting the city of Baltimore, the beaches of the Eastern Shore, and the nature of western Maryland, as well as many passing through on the way to Washington, D.C. Baltimore attractions include the Harborplace, the Baltimore Aquarium, Fort McHenry, as well as the Camden Yards baseball stadium.
Ocean City on the Atlantic Coast has been a popular beach destination in summer, particularly since the Chesapeake Bay Bridge was built in 1952 connecting the Eastern Shore to the more populated Maryland cities. The state capital of Annapolis offers sites such as the state capitol building, the historic district, and the waterfront.
Maryland also has several sites of interest to military history, given Maryland's role in the American Civil War and in the War of 1812. Other attractions include the historic and picturesque towns along the Chesapeake Bay, such as Saint Mary's, Maryland's first colonial settlement and original capital.

As of 2017, the top two health insurers including all types of insurance were CareFirst BlueCross BlueShield with 47% market share followed by UnitedHealth Group at 15%.

Maryland has experimented with healthcare payment reforms, notably beginning in the 1970s with an all-payer rate setting program regulated by the Health Services Cost Review Commission. In 2014, it switched over to a global budget revenue system, whereby hospitals receive a capitated payment to care for their population.

The Maryland Department of Transportation oversees most transportation in the state through its various administration-level agencies. The independent Maryland Transportation Authority maintains and operates the state's eight toll facilities.

Maryland's Interstate highways include of Interstate 95 (I-95), which enters the northeast portion of the state, travels through Baltimore, and becomes part of the eastern section of the Capital Beltway to the Woodrow Wilson Bridge. I-68 travels , connecting the western portions of the state to I-70 at the small town of Hancock. I-70 enters from Pennsylvania north of Hancock and continues east for to Baltimore, connecting Hagerstown and Frederick along the way.

I-83 has in Maryland and connects Baltimore to southern central Pennsylvania (Harrisburg and York, Pennsylvania). Maryland also has an portion of I-81 that travels through the state near Hagerstown. I-97, fully contained within Anne Arundel County and the shortest () one- or two-digit interstate highway in the contiguous US, connects the Baltimore area to the Annapolis area.

There are also several auxiliary Interstate highways in Maryland. Among them are two beltways encircling the major cities of the region: I-695, the McKeldin (Baltimore) Beltway, which encircles Baltimore; and a portion of I-495, the Capital Beltway, which encircles Washington, D.C. I-270, which connects the Frederick area with Northern Virginia and the District of Columbia through major suburbs to the northwest of Washington, is a major commuter route and is as wide as fourteen lanes at points. I-895, also known as the Harbor Tunnel Thruway, provides an alternate route to I-95 across the Baltimore Harbor.

Both I-270 and the Capital Beltway were extremely congested; however, the Intercounty Connector (ICC; MD 200) has alleviated some of the congestion over time. Construction of the ICC was a major part of the campaign platform of former Governor Robert Ehrlich, who was in office from 2003 until 2007, and of Governor Martin O'Malley, who succeeded him. I-595, which is an unsigned highway concurrent with US 50/US 301, is the longest unsigned interstate in the country and connects Prince George's County and Washington D.C. with Annapolis and the Eastern Shore via the Chesapeake Bay Bridge.

Maryland also has a state highway system that contains routes numbered from 2 through 999, however most of the higher-numbered routes are either unsigned or are relatively short. Major state highways include Routes 2 (Governor Ritchie Highway/Solomons Island Road/Southern Maryland Blvd.), 4 (Pennsylvania Avenue/Southern Maryland Blvd./Patuxent Beach Road/St. Andrew's Church Road), 5 (Branch Avenue/Leonardtown Road/Point Lookout Road), 32, 45 (York Road), 97 (Georgia Avenue), 100 (Paul T. Pitcher Memorial Highway), 210 (Indian Head Highway), 235 (Three Notch Road), 295 (Baltimore-Washington Parkway), 355 (Wisconsin Avenue/Rockville Pike/Frederick Road), 404 (Queen Anne Highway/ Shore Highway), and 650 (New Hampshire Avenue).

Maryland's largest airport is Baltimore-Washington International Thurgood Marshall Airport, more commonly referred to as BWI. The airport is named for the Baltimore-born Thurgood Marshall, the first African-American Supreme Court justice. The only other airports with commercial service are at Hagerstown and Salisbury.

The Maryland suburbs of Washington, D.C. are also served by the other two airports in the region, Ronald Reagan Washington National Airport and Dulles International Airport, both in Northern Virginia. The College Park Airport is the nation's oldest, founded in 1909, and is still used. Wilbur Wright trained military aviators at this location.

Amtrak trains, including the high speed Acela Express serve Baltimore's Penn Station, BWI Airport, New Carrollton, and Aberdeen along the Washington D.C. to Boston Northeast Corridor. In addition, train service is provided to Rockville and Cumberland by Amtrak's Washington, D.C., to Chicago Capitol Limited.
The WMATA's Metrorail rapid transit and Metrobus local bus systems (the 2nd and 6th busiest in the nation of their respective modes) provide service in Montgomery and Prince George's counties and connect them to Washington D.C., with the express Metrobus "Route B30" serving BWI Airport. The Maryland Transit Administration (often abbreviated as "MTA Maryland"), a state agency part of the Maryland Department of Transportation also provides transit services within the state. Headquartered in Baltimore, MTA's transit services are largely focused on central Maryland, as well as some portions of the Eastern Shore and Southern MD. Baltimore's Light RailLink and Metro SubwayLink systems serve its densely populated inner-city and the surrounding suburbs. The MTA also serves the city and its suburbs with its local bus service (the 9th largest system in the nation). The MTA's Commuter Bus system provides express coach service on longer routes connecting Washington D.C. and Baltimore to parts of Central and Southern MD as well as the Eastern Shore. The commuter rail service, known as MARC, operates three lines which all terminate at Washington Union Station and provide service to Baltimore's Penn and Camden stations, Perryville, Frederick, and Martinsburg, WV. In addition, many suburban counties operate their own local bus systems which connect to and complement the larger MTA and WMATA/Metro services.

Freight rail transport is handled principally by two Class I railroads, as well as several smaller regional and local carriers. CSX Transportation has more extensive trackage throughout the state, with , followed by Norfolk Southern Railway. Major rail yards are located in Baltimore and Cumberland, with an intermodal terminal (rail, truck and marine) in Baltimore.

The government of Maryland is conducted according to the state constitution. The government of Maryland, like the other 49 state governments, has exclusive authority over matters that lie entirely within the state's borders, except as limited by the Constitution of the United States.

Power in Maryland is divided among three branches of government: executive, legislative, and judicial. The Maryland General Assembly is composed of the Maryland House of Delegates and the Maryland Senate. Maryland's governor is unique in the United States as the office is vested with significant authority in budgeting. The legislature may not increase the governor's proposed budget expenditures. Unlike many other states, significant autonomy is granted to many of Maryland's counties.

Most of the business of government is conducted in Annapolis, the state capital. Elections for governor and most statewide offices, as well as most county elections, are held in midterm-election years (even-numbered years not divisible by four).

The judicial branch of state government consists of one united District Court of Maryland that sits in every county and Baltimore City, as well as 24 Circuit Courts sitting in each County and Baltimore City, the latter being courts of general jurisdiction for all civil disputes over $30,000.00, all equitable jurisdiction and major criminal proceedings. The intermediate appellate court is known as the Court of Special Appeals and the state supreme court is the Court of Appeals. The appearance of the judges of the Maryland Court of Appeals is unique; Maryland is the only state whose judges wear red robes.

Maryland imposes five income tax brackets, ranging from 2 to 6.25 percent of personal income. The city of Baltimore and Maryland's 23 counties levy local "piggyback" income taxes at rates between 1.25 and 3.2 percent of Maryland taxable income. Local officials set the rates and the revenue is returned to the local governments quarterly. The top income tax bracket of 9.45 percent is the fifth highest combined state and local income tax rates in the country, behind New York City's 11.35 percent, California's 10.3 percent, Rhode Island's 9.9 percent, and Vermont's 9.5 percent.

Maryland's state sales tax is 6 percent. All real property in Maryland is subject to the property tax. Generally, properties that are owned and used by religious, charitable, or educational organizations or property owned by the federal, state or local governments are exempt. Property tax rates vary widely. No restrictions or limitations on property taxes are imposed by the state, meaning cities and counties can set tax rates at the level they deem necessary to fund governmental services.

Since before the Civil War, Maryland's elections have been largely controlled by the Democrats, which account for 54.9% of all registered voters as of May 2017.

State elections are dominated by Baltimore and the populous suburban counties bordering Washington, D.C. and Baltimore: Montgomery, Prince George's, Anne Arundel, and Baltimore counties. As of July 2017, sixty-six percent of the state's population resides in these six jurisdictions, most of which contain large, traditionally Democratic voting bloc(s): African Americans in Baltimore City and Prince George's, federal employees in Prince George's, Anne Arundel, and Montgomery, and postgraduates in Montgomery. The remainder of the state, particularly Western Maryland and the Eastern Shore, is more supportive of Republicans. One of Maryland's best known political figures is a Republican – former Governor Spiro Agnew, who served under President Richard Nixon as the U.S. Vice President from 1969 to 1973, when he resigned in the aftermath of findings that he had taken bribes while he was Governor of Maryland. Though claiming innocence, Agnew negotiated a plea bargain and appeared before the federal court in Baltimore in October 1973, where he pled no contest to one tax evasion felony charge and submitted his letter of resignation.

In 1980, Maryland was one of six states to vote for Jimmy Carter. In 1992, Bill Clinton fared better in Maryland than any other state except his home state of Arkansas. In 1996, Maryland was Clinton's sixth best; in 2000, Maryland ranked fourth for Gore; and in 2004, John Kerry showed his fifth-best performance in Maryland. In 2008, Barack Obama won the state's 10 electoral votes with 61.9 percent of the vote to John McCain's 36.5 percent.

In 2002, former Governor Robert Ehrlich was the first Republican to be elected to that office in four decades, and after one term lost his seat to Baltimore Mayor and Democrat Martin O'Malley. Ehrlich ran again for governor in 2010, losing again to O'Malley.

The 2006 election brought no change in the pattern of Democratic dominance. After Democratic Senator Paul Sarbanes announced that he was retiring, Democratic Congressman Benjamin Cardin defeated Republican Lieutenant Governor Michael S. Steele, with 55 percent of the vote, against Steele's 44 percent.

While Republicans usually win more counties, by piling up large margins in the west and east, they are also usually swamped by the more densely populated and heavily Democratic Baltimore–Washington axis. In 2008, for instance, McCain won 17 counties to Obama's six; Obama also carried Baltimore City. While McCain won most of the western and eastern counties by margins of 2-to-1 or more, he was almost completely shut out in the larger counties surrounding Baltimore and Washington; every large county except Anne Arundel went for Obama.

From 2007 to 2011, U.S. Congressman Steny Hoyer (MD-5), a Democrat, was elected as Majority Leader for the 110th Congress and 111th Congress of the House of Representatives, serving in that post again starting in 2019. In addition, Hoyer served as House Minority Whip from 2003 to 2006 and 2012–2018. His district covers parts of Anne Arundel and Prince George's counties, in addition to all of Charles, Calvert and St. Mary's counties in southern Maryland.

In 2010, Republicans won control of most counties. The Democratic Party remained in control of eight county governments including Baltimore.

In 2014, Larry Hogan, a Republican, was elected Governor of Maryland. Hogan is the second Republican to become the Governor of Maryland after Spiro Agnew, who resigned in 1969 to become Vice President. In 2018, Hogan was reelected to a second term of office.

In February 2010, Attorney General Doug Gansler issued an opinion stating that Maryland law should honor same-sex marriages from out of state. At the time, the state Supreme Court wrote a decision upholding marriage discrimination.

On March 1, 2012, Maryland Governor Martin O'Malley signed the freedom to marry bill into law after it passed in the state legislature. Immediately after, opponents of same-sex marriage began collecting signatures to overturn the law. The law was scheduled to face a referendum, as Question 6, in the November 2012 election.

In May 2012, Maryland's Court of Appeals ruled that the state will recognize marriages of same-sex couples who married out-of-state, no matter the outcome of the November election.

Voters voted 52% to 48% for Question 6 on November 6, 2012. Same-sex couples began marrying in Maryland on January 1, 2013.

A large majority (57%) of Maryland voters said they would vote to uphold the freedom to marry at the ballot in November 2012, with 37% saying they would vote against marriage for all couples. This is consistent with a January 2011 Gonzales Research & Marketing Strategies poll showing 51% support for marriage in the state.

A well known newspaper is "The Baltimore Sun".

The most populous areas are served by either Baltimore or Washington, D.C. broadcast stations. The Eastern Shore is served primarily by broadcast media based around the Delmarva Peninsula; the northeastern section receives both Baltimore and Philadelphia stations. Garrett County, which is mountainous, is served by stations from Pittsburgh, and requires cable or satellite for reception. Maryland is served by state-wide PBS member station Maryland Public Television (MPT).

Education Week ranked Maryland #1 in its nationwide 2009–2013 Quality Counts reports. The College Board's 9th Annual AP Report to the Nation also ranked Maryland first. Primary and secondary education in Maryland is overseen by the Maryland State Department of Education, which is headquartered in Baltimore. The highest educational official in the state is the State Superintendent of Schools, who is appointed by the State Board of Education to a four-year term of office. The Maryland General Assembly has given the Superintendent and State Board autonomy to make educationally related decisions, limiting its own influence on the day-to-day functions of public education. Each county and county-equivalent in Maryland has a local Board of Education charged with running the public schools in that particular jurisdiction.

The budget for education was $5.5 billion in 2009, representing about 40 percent of the state's general fund.

Maryland has a broad range of private primary and secondary schools. Many of these are affiliated with various religious sects, including parochial schools of the Catholic Church, Quaker schools, Seventh-day Adventist schools, and Jewish schools. In 2003, Maryland law was changed to allow for the creation of publicly funded charter schools, although the charter schools must be approved by their local Board of Education and are not exempt from state laws on education, including collective bargaining laws.

In 2008, the state led the entire country in the percentage of students passing Advanced Placement examinations. 23.4 percent of students earned passing grades on the AP tests given in May 2008. This marks the first year that Maryland earned this honor. Three Maryland high schools (in Montgomery County) were ranked among the top 100 in the country by US News in 2009, based in large part on AP test scores.

Maryland has several historic and renowned private colleges and universities, the most prominent of which is Johns Hopkins University, founded in 1876 with a grant from Baltimore entrepreneur Johns Hopkins.

The first public university in the state is the University of Maryland, Baltimore, which was founded in 1807 and contains the University of Maryland's only public academic health, human services, and one of two law centers (the other being the University of Baltimore School of Law). Seven professional and graduate schools train the majority of the state's physicians, nurses, dentists, lawyers, social workers, and pharmacists. The flagship university and largest undergraduate institution in Maryland is the University of Maryland, College Park which was founded as the Maryland Agricultural College in 1856 and became a public land grant college in 1864. Towson University, founded in 1866, is the state's second largest university. Baltimore is home to the University of Maryland, Baltimore County and the Maryland Institute College of Art. The majority of public universities in the state are affiliated with the University System of Maryland. Two state-funded institutions, Morgan State University and St. Mary's College of Maryland, as well as two federally funded institutions, the Uniformed Services University of the Health Sciences and the United States Naval Academy, are not affiliated with the University System of Maryland.

St. John's College in Annapolis, Maryland and Washington College in Chestertown, Maryland, both private institutions, are the two oldest colleges in the state, and are among the oldest in the country. Other private institutions include Mount St. Mary's University, McDaniel College (formerly known as Western Maryland College), Hood College, Stevenson University (formerly known as Villa Julie College), Loyola University Maryland, and Goucher College, among others.

Maryland's 24 public library systems deliver public education for everyone in the state of Maryland through a curriculum that comprises three pillars: Self-Directed Education (books and materials in all formats, e-resources), Research Assistance & Instruction (individualized research assistance, classes for students of all ages), and Instructive & Enlightening Experiences (e.g., Summer Reading Clubs, author events).

Maryland's library systems include, in part:


Many of the library systems have established formalized partnerships with other educational institutions in their counties and regions.

With two major metropolitan areas, Maryland has a number of major and minor professional sports franchises. Two National Football League teams play in Maryland, the Baltimore Ravens in Baltimore and the Washington Redskins in Landover. The Baltimore Colts represented the NFL in Baltimore from 1953 to 1983 before moving to Indianapolis.

The Baltimore Orioles are the state's Major League Baseball franchise. The National Hockey League's Washington Capitals and the National Basketball Association's Washington Wizards formerly played in Maryland, until the construction of an arena in Washington, D.C. in 1997 (now known as Capital One Arena).

Maryland enjoys considerable historical repute for the talented sports players of its past, including Cal Ripken Jr. and Babe Ruth. In 2012, "The Baltimore Sun" published a list of Maryland's top ten athletes in the state's history. The list includes Babe Ruth, Cal Ripken Jr, Johnny Unitas, Brooks Robinson, Frank Robinson, Ray Lewis, Michael Phelps, Jimmie Foxx, Jim Parker, and Wes Unseld.

Other professional sports franchises in the state include five affiliated minor league baseball teams, one independent league baseball team, the Baltimore Blast indoor soccer team, two indoor football teams, three low-level outdoor soccer teams, and the Chesapeake Bayhawks of Major League Lacrosse. Maryland is also home to one of the three races in horse racing's annual Triple Crown, the Preakness Stakes, which is run every spring at Pimlico Race Course in Baltimore.

The Congressional Country Club has hosted three golf tournaments for the U.S. Open and a PGA Championship.

The official state sport of Maryland, since 1962, is jousting; the official team sport since 2004 is lacrosse. The National Lacrosse Hall of Fame is located on the Johns Hopkins University campus in Baltimore. In 2008, intending to promote physical fitness for all ages, walking became the official state exercise. Maryland is the first state with an official state exercise.



</doc>
<doc id="18859" url="https://en.wikipedia.org/wiki?curid=18859" title="Michigan">
Michigan

Michigan () is a state in the Great Lakes and Midwestern regions of the United States. Its name originates from the Ojibwe word "mishigamaa", meaning "large water" or "large lake". With a population of just under 10 million, Michigan is the tenth most populous of the 50 United States, with the 11th most extensive total area, and is the largest state by total area east of the Mississippi River. Its capital is Lansing, and its largest city is Detroit. Metro Detroit is among the nation's most populous and largest metropolitan economies.

Michigan is the only state to consist of two peninsulas. The Lower Peninsula is often noted as being shaped like a mitten. The Upper Peninsula (often called "the U.P.") is separated from the Lower Peninsula by the Straits of Mackinac, a channel that joins Lake Huron to Lake Michigan. The Mackinac Bridge connects the peninsulas. The state has the longest freshwater coastline of any political subdivision in the world, being bounded by four of the five Great Lakes, plus Lake Saint Clair. Michigan also has 64,980 inland lakes and ponds.

The area was first occupied by a succession of Native American tribes over thousands of years. Inhabited by Natives, Métis, and French explorers in the 17th century, it was claimed as part of New France colony. After France's defeat in the French and Indian War in 1762, the region came under British rule. Britain ceded this territory to the newly independent United States after Britain's defeat in the American Revolutionary War. The area was part of the larger Northwest Territory until 1800, when western Michigan became part of the Indiana Territory. Michigan Territory was formed in 1805, but some of the northern border with Canada was not agreed upon until after the War of 1812. Michigan was admitted into the Union in 1837 as the 26th state, a free one. It soon became an important center of industry and trade in the Great Lakes region and a popular immigrant destination in the late 19th and early 20th centuries.

Although Michigan developed a diverse economy, it is widely known as the center of the U.S. automotive industry, which developed as a major economic force in the early 20th century. It is home to the country's three major automobile companies (whose headquarters are all within the Detroit metropolitan area). While sparsely populated, the Upper Peninsula is important for tourism thanks to its abundance of natural resources, while the Lower Peninsula is a center of manufacturing, forestry, agriculture, services, and high-tech industry.

When the first European explorers arrived, the most populous tribes were Algonquian peoples, which include the Anishinaabe groups of Ojibwe (referred to as "Chippewa" in the United States), Odaawaa/Odawa (Ottawa), and the Boodewaadamii/Bodéwadmi (Potawatomi). The three nations co-existed peacefully as part of a loose confederation called the Council of Three Fires. The Ojibwe, whose numbers are estimated to have been between 25,000 and 35,000, were the largest.

The Ojibwe were established in Michigan's Upper Peninsula and northern and central Michigan, and also inhabited Ontario and southern Manitoba, Canada; and northern Wisconsin, and northern and north-central Minnesota. The Ottawa lived primarily south of the Straits of Mackinac in northern, western and southern Michigan, but also in southern Ontario, northern Ohio and eastern Wisconsin. The Potawatomi were in southern and western Michigan, in addition to northern and central Indiana, northern Illinois, southern Wisconsin, and southern Ontario. Other Algonquian tribes in Michigan, in the south and east, were the Mascouten, the Menominee, the Miami, the Sac (or Sauk), and the Fox. The Wyandot were an Iroquoian-speaking people in this area; they were historically known as the Huron by the French.

French "voyageurs" and "coureurs des bois" explored and settled in Michigan in the 17th century. The first Europeans to reach what became Michigan were those of Étienne Brûlé's expedition in 1622. The first permanent European settlement was founded in 1668 on the site where Père Jacques Marquette established Sault Ste. Marie, Michigan as a base for Catholic missions. Missionaries in 1671–75 founded outlying stations at Saint Ignace and Marquette. Jesuit missionaries were well received by the area's Indian populations, with few difficulties or hostilities. In 1679, Robert Cavelier, Sieur de la Salle built Fort Miami at present-day St. Joseph. In 1691, the French established a trading post and Fort St. Joseph along the St. Joseph River at the present-day city of Niles.

In 1701, French explorer and army officer Antoine de la Mothe Cadillac founded Fort Pontchartrain du Détroit or "Fort Pontchartrain on-the-Strait" on the strait, known as the Detroit River, between lakes Saint Clair and Erie. Cadillac had convinced King Louis XIV's chief minister, Louis Phélypeaux, Comte de Pontchartrain, that a permanent community there would strengthen French control over the upper Great Lakes and discourage British aspirations.

The hundred soldiers and workers who accompanied Cadillac built a fort enclosing one arpent (about , the equivalent of just under per side) and named it Fort Pontchartrain. Cadillac's wife, Marie Thérèse Guyon, soon moved to Detroit, becoming one of the first European women to settle in what was considered the wilderness of Michigan. The town quickly became a major fur-trading and shipping post. The "Église de Saint-Anne" (Church of Saint Ann) was founded the same year. While the original building does not survive, the congregation remains active. Cadillac later departed to serve as the French governor of Louisiana from 1710 to 1716. French attempts to consolidate the fur trade led to the Fox Wars involving the Meskwaki (Fox) and their allies versus the French and their Native allies.

At the same time, the French strengthened Fort Michilimackinac at the Straits of Mackinac to better control their lucrative fur-trading empire. By the mid-18th century, the French also occupied forts at present-day Niles and Sault Ste. Marie, though most of the rest of the region remained unsettled by Europeans. France offered free land to attract families to Detroit, which grew to 800 people in 1765, and was the largest city between Montreal and New Orleans. French settlers also established small farms south of the Detroit River opposite the fort, near a Jesuit mission and Huron village.

From 1660 until the end of French rule, Michigan was part of the Royal Province of New France. In 1760, Montreal fell to the British forces ending the French and Indian War (1754–1763). Under the 1763 Treaty of Paris, Michigan and the rest of New France east of the Mississippi River passed to Great Britain. After the Quebec Act was passed in 1774, Michigan became part of the British Province of Quebec. By 1778, Detroit's population was up to 2,144 and it was the third-largest city in Quebec.

During the American Revolutionary War, Detroit was an important British supply center. Most of the inhabitants were French-Canadians or Native Americans, many of whom had been allied with the French because of long trading ties. Because of imprecise cartography and unclear language defining the boundaries in the 1783 Treaty of Paris, the British retained control of Detroit and Michigan after the American Revolution. When Quebec split into Lower and Upper Canada in 1791, Michigan was part of Kent County, Upper Canada. It held its first democratic elections in August 1792 to send delegates to the new provincial parliament at Newark (now Niagara-on-the-Lake).

Under terms negotiated in the 1794 Jay Treaty, Britain withdrew from Detroit and Michilimackinac in 1796. It retained control of territory east and south of the Detroit River, which are now included in Ontario, Canada. Questions remained over the boundary for many years, and the United States did not have uncontested control of the Upper Peninsula and Drummond Island until 1818 and 1847, respectively.

During the War of 1812, the United States forces at Fort Detroit surrendered Michigan Territory (effectively consisting of Detroit and the surrounding area) after a nearly bloodless siege in 1812. A US attempt to retake Detroit resulted in a severe American defeat in the River Raisin Massacre. This battle, still ranked as the bloodiest ever fought in the state, had the highest number of American casualties of any battle in the war.

Michigan was recaptured by the Americans in 1813 after the Battle of Lake Erie. They used Michigan as a base to launch an invasion of Canada, which culminated in the Battle of the Thames. But the more northern areas of Michigan were held by the British until the peace treaty restored the old boundaries. A number of forts, including Fort Wayne, were built by the United States in Michigan during the 19th century out of fears of renewed fighting with Britain.

The population grew slowly until the opening in 1825 of the Erie Canal through the Mohawk Valley in New York, connecting the Great Lakes to the Hudson River and New York City. The new route attracted a large influx of settlers to the Michigan territory. They worked as farmers, lumbermen, shipbuilders, and merchants and shipped out grain, lumber, and iron ore. By the 1830s, Michigan had 80,000 residents, more than enough to apply and qualify for statehood.

A Constitutional Convention of Assent, led by Gershom Mott Williams, was held to lead the territory to statehood.
In October 1835 the people approved the Constitution of 1835, thereby forming a state government, although Congressional recognition was delayed pending resolution of a boundary dispute with Ohio known as the Toledo War. Congress awarded the "Toledo Strip" to Ohio. Michigan received the western part of the Upper Peninsula as a concession and formally entered the Union as a free state on January 26, 1837. The Upper Peninsula proved to be a rich source of lumber, iron, and copper. Michigan led the nation in lumber production from the 1850s to the 1880s. Railroads became a major engine of growth from the 1850s onward, with Detroit the chief hub.

A second wave of French-Canadian immigrants settled in Michigan during the late 19th to early 20th century, working in lumbering areas in counties on the Lake Huron side of the Lower Peninsula, such as the Saginaw Valley, Alpena, and Cheboygan counties, as well as throughout the Upper Peninsula, with large concentrations in Escanaba and the Keweenaw Peninsula. This was also a period of development of the gypsum industry in Alabaster, Michigan, which became nationally prominent.

The first statewide meeting of the Republican Party took place July 6, 1854, in Jackson, Michigan, where the party adopted its platform. The state was heavily Republican until the 1930s. Michigan made a significant contribution to the Union in the American Civil War and sent more than forty regiments of volunteers to the federal armies.

Modernizers and boosters set up systems for public education, including founding the University of Michigan (1817, moved to Ann Arbor in 1837) for a classical academic education; and Michigan State Normal School (1849), now Eastern Michigan University, for the training of teachers. It adopted this model from the German educational system. In 1899, Michigan State became the first normal college in the nation to offer a four-year curriculum. Michigan Agricultural College (1855), now Michigan State University in East Lansing, was founded as the pioneer land-grant college, a model for those authorized under the Morrill Act (1862). Many private colleges were founded as well, and the smaller cities established high schools late in the century.

Michigan's economy underwent a transformation at the turn of the 20th century. Many individuals, including Ransom E. Olds, John and Horace Dodge, Henry Leland, David Dunbar Buick, Henry Joy, Charles King, and Henry Ford, provided the concentration of engineering know-how and technological enthusiasm to develop the automotive industry. Ford's development of the moving assembly line in Highland Park marked a new era in transportation. Like the steamship and railroad, mass production of automobiles was a far-reaching development. More than the forms of public transportation, the affordable automobile transformed private life. Automobile production became the major industry of Detroit and Michigan, and permanently altered the socio-economic life of the United States and much of the world.

With the growth, the auto industry created jobs in Detroit that attracted immigrants from Europe and migrants from across the United States, including both blacks and whites from the rural South. By 1920, Detroit was the fourth-largest city in the US. Residential housing was in short supply, and it took years for the market to catch up with the population boom. By the 1930s, so many immigrants had arrived that more than 30 languages were spoken in the public schools, and ethnic communities celebrated in annual heritage festivals. Over the years immigrants and migrants contributed greatly to Detroit's diverse urban culture, including popular music trends. The influential Motown Sound of the 1960s was led by a variety of individual singers and groups.

Grand Rapids, the second-largest city in Michigan, is also an important center of manufacturing. Since 1838, the city has been noted for its furniture industry. In the 21st century, it is home to five of the world's leading office furniture companies. Grand Rapids is home to a number of major companies including Steelcase, Amway, and Meijer. Grand Rapids is also an important center for GE Aviation Systems.

Michigan held its first United States presidential primary election in 1910. With its rapid growth in industry, it was an important center of industry-wide union organizing, such as the rise of the United Auto Workers.

In 1920 WWJ (AM) in Detroit became the first radio station in the United States to regularly broadcast commercial programs. Throughout that decade, some of the country's largest and most ornate skyscrapers were built in the city. Particularly noteworthy are the Fisher Building, Cadillac Place, and the Guardian Building, each of which has been designated as a National Historic Landmark (NHL).

In 1927 a school bombing took place in Clinton County. The Bath School disaster, perpetrated by an adult man, resulted in the deaths of 38 schoolchildren and constitutes the deadliest mass murder in a school in U.S. history.

Michigan converted much of its manufacturing to satisfy defense needs during World War II; it manufactured 10.9 percent of the United States military armaments produced during the war, ranking second (behind New York) among the 48 states.

Detroit continued to expand through the 1950s, at one point doubling its population in a decade. After World War II, housing was developed in suburban areas outside city cores to meet demand for residences. The federal government subsidized the construction of interstate highways, which were intended to strengthen military access, but also allowed commuters and business traffic to travel the region more easily. Since 1960, modern advances in the auto industry have led to increased automation, high-tech industry, and increased suburban growth.

Michigan is the leading auto-producing state in the US, with the industry primarily located throughout the Midwestern United States, Ontario, Canada, and the Southern United States. With almost ten million residents, Michigan is a large and influential state, ranking tenth in population among the fifty states. Detroit is the centrally located metropolitan area of the Great Lakes Megalopolis and the second-largest metropolitan area in the U.S. (after Chicago) linking the Great Lakes system.
The Metro Detroit area in Southeast Michigan is the state's largest metropolitan area (roughly 50% of the population resides there) and the eleventh largest in the USA. The Grand Rapids metropolitan area in Western Michigan is the state's fastest-growing metro area, with over 1.3 million residents . Metro Detroit receives more than 15 million visitors each year. Michigan has many popular tourist destinations, including areas such as Frankenmuth in The Thumb, and Traverse City on the Grand Traverse Bay in Northern Michigan. Tourists spend about $17 billion annually in Michigan supporting 193,000 jobs.

Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the US. The state's leading research institutions include the University of Michigan, Michigan State University, and Wayne State University, which are important partners in the state's economy and the state's University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. Agriculture also serves a significant role, making the state a leading grower of fruit in the US, including blueberries, cherries, apples, grapes, and peaches.

On November 6, 2018, the state electorate voted to pass Proposal 2 that amended the state constitution to create an independent redistrict commission.

Michigan is governed as a republic, with three branches of government: the executive branch consisting of the Governor of Michigan and the other independently elected constitutional officers; the legislative branch consisting of the House of Representatives and Senate; and the judicial branch. The Michigan Constitution allows for the direct participation of the electorate by statutory initiative and referendum, recall, and constitutional initiative and referral (Article II, § 9, defined as "the power to propose laws and to enact and reject laws, called the initiative, and the power to approve or reject laws enacted by the legislature, called the referendum. The power of initiative extends only to laws which the legislature may enact under this constitution"). Lansing is the state capital and is home to all three branches of state government.

The governor and the other state constitutional officers serve four-year terms and may be re-elected only once. The governor is Gretchen Whitmer. Michigan has two official Governor's Residences; one is in Lansing, and the other is at Mackinac Island. The other constitutionally elected executive officers are the lieutenant governor, who is elected on a joint ticket with the governor, the secretary of state, and the attorney general. The lieutenant governor presides over the Senate but only voting when ties occur, and is also a member of the cabinet. The secretary of state is the chief elections officer and is charged with running many licensure programs including motor vehicles, all of which are done through the branch offices of the secretary of state.

The Michigan Legislature consists of a 38-member Senate and 110-member House of Representatives. Members of both houses of the legislature are elected through first past the post elections by single-member electoral districts of near-equal population that often have boundaries which coincide with county and municipal lines. Senators serve four-year terms concurrent to those of the governor, while representatives serve two-year terms. The Michigan State Capitol was dedicated in 1879 and has hosted the executive and legislative branches of the state ever since.

The Michigan judiciary consists of two courts with primary jurisdiction (the Circuit Courts and the District Courts), one intermediate level appellate court (the Michigan Court of Appeals), and the Michigan Supreme Court. There are several administrative courts and specialized courts. District courts are trial courts of limited jurisdiction, handling most traffic violations, small claims, misdemeanors, and civil suits where the amount contended is below $25,000. District courts are often responsible for handling the preliminary examination and for setting bail in felony cases. District court judges are elected to terms of six years. In a few locations, municipal courts have been retained to the exclusion of the establishment of district courts. There are 57 circuit courts in the State of Michigan, which have original jurisdiction over all civil suits where the amount contended in the case exceeds $25,000 and all criminal cases involving felonies. Circuit courts are also the only trial courts in the State of Michigan which possess the power to issue equitable remedies. Circuit courts have appellate jurisdiction from district and municipal courts, as well as from decisions and decrees of state agencies. Most counties have their own circuit court, but sparsely populated counties often share them. Circuit court judges are elected to terms of six years. State appellate court judges are elected to terms of six years, but vacancies are filled by an appointment by the governor. There are four divisions of the Court of Appeals in Detroit, Grand Rapids, Lansing, and Marquette. Cases are heard by the Court of Appeals by panels of three judges, who examine the application of the law and not the facts of the case unless there has been grievous error pertaining to questions of fact. The Michigan Supreme Court consists of seven members who are elected on non-partisan ballots for staggered eight-year terms. The Supreme Court has original jurisdiction only in narrow circumstances but holds appellate jurisdiction over the entire state judicial system.

Michigan has had four constitutions, the first of which was ratified on October 5 and 6, 1835. There were also constitutions from 1850 and 1908, in addition to the current constitution from 1963. The current document has a preamble, 11 articles, and one section consisting of a schedule and temporary provisions. Michigan, like every U.S. state except Louisiana, has a common law legal system.

Voters in the state elect candidates from both major parties. Economic issues are important in Michigan elections.

The three-term Republican Governor John Engler (1991–2003) preceded the former two-term Democratic Governor Jennifer Granholm (2003–2011). The state elected successive Republican attorneys general from 2003 until 2019, when Democratic incumbent Dana Nessel took office. The Republican Party holds a majority in both the House and Senate of the Michigan Legislature. Michigan supported the election of Republican Presidents Ronald Reagan, George H. W. Bush, and Donald Trump. The Governor Gretchen Whitmer (2019–present) is a Democrat.

In contrast, the state supported Democratic candidates in each presidential election from 1992 to 2012. In 2012, Barack Obama carried the state over Mitt Romney, winning Michigan's 16 electoral votes with 54% of the vote. Michigan's two US Senators are both Democrats, while Republicans held a majority of the state's US House seats from 2011 to 2019. As a result of the 2018 elections, the House delegation became evenly divided between Democrats and Republicans at seven members each. The current standing of the delegation changed again on July 4, 2019, when Rep. Justin Amash changed parties to become an independent, and as a result, Democrats now hold a majority of the U.S. House seats for the state, something which has not occurred in over a decade. Michigan's senior U.S. Senator Debbie Stabenow, a Democrat, has served since 2001 after narrowly beating former Republican U.S. Senator Spencer Abraham in the 2000 elections. Democratic U.S. Senator Gary Peters was elected in 2014, beating former Republican Michigan Secretary of State Terri Lynn Land. Congressman Fred Upton, a Republican, serves as Chairman of the US House Committee on Energy and Commerce. Congresswoman Debbie Dingell, a Democrat, became the first person to succeed a living spouse when she replaced former Dean of the House of Representatives John Dingell in 2015.

Republican strongholds of the state include rural areas of Western and Northern Michigan, the Grand Rapids metropolitan area, and Livingston County. Areas of Democratic strength include Wayne County, home to Detroit, Washtenaw County (Ann Arbor), Ingham County (Lansing), and Genesee County (Flint). Much of suburban Detroit—which includes parts of Oakland, Macomb, and Wayne counties—is politically competitive between the two parties.

Historically, the first county-level meeting of the Republican Party took place in Jackson on July 6, 1854, and the party thereafter dominated Michigan until the Great Depression. In the 1912 election, Michigan was one of the six states to support progressive Republican and third-party candidate Theodore Roosevelt for president after he lost the Republican nomination to William Howard Taft.

Michigan remained fairly reliably Republican at the presidential level for much of the 20th century. It was part of Greater New England, the northern tier of states settled chiefly by migrants from New England who carried their culture with them. The state was one of only a handful to back Wendell Willkie over Franklin Roosevelt in 1940, and supported Thomas E. Dewey in his losing bid against Harry S. Truman in 1948. Michigan went to the Democrats in presidential elections during the 1960s and voted for the Republican candidate in every election from 1972 to 1988. Between 1992 and 2012 it supported the Democrats; early on in 2016, it was pegged as a swing state, and was narrowly won by the G.O.P. candidate, Donald Trump.

Michigan was the home of Gerald Ford, the 38th President of the United States. Born in Nebraska, he moved as an infant to Grand Rapids. The Gerald R. Ford Museum is in Grand Rapids, and the Gerald R. Ford Presidential Library is on the campus of his alma mater, the University of Michigan in Ann Arbor.

In 1846 Michigan became the first state in the Union, as well as the first English-speaking government in the world, to abolish the death penalty. Historian David Chardavoyne has suggested the movement to abolish capital punishment in Michigan grew as a result of enmity toward the state's neighbor, Canada. Under British rule, it made public executions a regular practice.

Michigan has recognized and performed same-sex marriages since June 26, 2015, following the Supreme Court ruling in "Obergefell v. Hodges". Previously, such unions were prohibited under a 2004 state constitutional amendment.

Michigan has approved plans to expand Medicaid coverage in 2014 to adults with incomes up to 133% of the federal poverty level (approximately $15,500 for a single adult in 2014).

On November 6, 2018, Michigan approved a proposal to legalize the recreational use of marijuana, which took effect on December 6, 2018.

State government is decentralized among three tiers—statewide, county and township. Counties are administrative divisions of the state, and townships are administrative divisions of a county. Both of them exercise state government authority, localized to meet the particular needs of their jurisdictions, as provided by state law. There are 83 counties in Michigan.

Cities, state universities, and villages are vested with home rule powers of varying degrees. Home rule cities can generally do anything not prohibited by law. The fifteen state universities have broad power and can do anything within the parameters of their status as educational institutions that is not prohibited by the state constitution. Villages, by contrast, have limited home rule and are not completely autonomous from the county and township in which they are located.

There are two types of township in Michigan: "general law" township and "charter". Charter township status was created by the Legislature in 1947 and grants additional powers and stream-lined administration in order to provide greater protection against annexation by a city. , there were 127 charter townships in Michigan. In general, charter townships have many of the same powers as a city but without the same level of obligations. For example, a charter township can have its own fire department, water and sewer department, police department, and so on—just like a city—but it is not "required" to have those things, whereas cities "must" provide those services. Charter townships can opt to use county-wide services instead, such as deputies from the county sheriff's office instead of a home-based force of ordinance officers.

Michigan consists of two peninsulas that lie between 82°30' to about 90°30' west longitude, and are separated by the Straits of Mackinac. The 45th parallel north runs through the state—marked by highway signs and the Polar-Equator Trail—along a line including Mission Point Light near Traverse City, the towns of Gaylord and Alpena in the Lower Peninsula and Menominee in the Upper Peninsula. With the exception of two small areas that are drained by the Mississippi River by way of the Wisconsin River in the Upper Peninsula and by way of the Kankakee-Illinois River in the Lower Peninsula, Michigan is drained by the Great Lakes-St. Lawrence watershed and is the only state with the majority of its land thus drained. No point in the state is more than from a natural water source or more than from a Great Lakes shoreline.

The Great Lakes that border Michigan from east to west are Lake Erie, Lake Huron, Lake Michigan and Lake Superior. As a result, it is one of the leading U.S. states for recreational boating. The state is bounded on the south by the states of Ohio and Indiana, sharing land and water boundaries with both. Michigan's western boundaries are almost entirely water boundaries, from south to north, with Illinois and Wisconsin in Lake Michigan; then a land boundary with Wisconsin and the Upper Peninsula, that is principally demarcated by the Menominee and Montreal Rivers; then water boundaries again, in Lake Superior, with Wisconsin and Minnesota to the west, capped around by the Canadian province of Ontario to the north and east.
The heavily forested Upper Peninsula is relatively mountainous in the west. The Porcupine Mountains, which are part of one of the oldest mountain chains in the world, rise to an altitude of almost above sea level and form the watershed between the streams flowing into Lake Superior and Lake Michigan. The surface on either side of this range is rugged. The state's highest point, in the Huron Mountains northwest of Marquette, is Mount Arvon at . The peninsula is as large as Connecticut, Delaware, Massachusetts, and Rhode Island combined but has fewer than 330,000 inhabitants. They are sometimes called "Yoopers" (from "U.P.'ers"), and their speech (the "Yooper dialect") has been heavily influenced by the numerous Scandinavian and Canadian immigrants who settled the area during the lumbering and mining boom of the late 19th century.
The Lower Peninsula is shaped like a mitten and many residents hold up a hand to depict where they are from. It is long from north to south and from east to west and occupies nearly two-thirds of the state's land area. The surface of the peninsula is generally level, broken by conical hills and glacial moraines usually not more than a few hundred feet tall. It is divided by a low water divide running north and south. The larger portion of the state is on the west of this and gradually slopes toward Lake Michigan. The highest point in the Lower Peninsula is either Briar Hill at , or one of several points nearby in the vicinity of Cadillac. The lowest point is the surface of Lake Erie at .

The geographic orientation of Michigan's peninsulas makes for a long distance between the ends of the state. Ironwood, in the far western Upper Peninsula, lies by highway from Lambertville in the Lower Peninsula's southeastern corner. The geographic isolation of the Upper Peninsula from Michigan's political and population centers makes the U.P. culturally and economically distinct. Occasionally U.P. residents have called for secession from Michigan and establishment as a new state to be called "Superior".

A feature of Michigan that gives it the distinct shape of a mitten is the Thumb. This peninsula projects out into Lake Huron and the Saginaw Bay. The geography of the Thumb is mainly flat with a few rolling hills. Other peninsulas of Michigan include the Keweenaw Peninsula, making up the Copper Country region of the state. The Leelanau Peninsula lies in the Northern Lower Michigan region. "See Also Michigan Regions"

Numerous lakes and marshes mark both peninsulas, and the coast is much indented. Keweenaw Bay, Whitefish Bay, and the Big and Little Bays De Noc are the principal indentations on the Upper Peninsula. The Grand and Little Traverse, Thunder, and Saginaw bays indent the Lower Peninsula. Michigan has the second longest shoreline of any state—, including of island shoreline.
The state has numerous large islands, the principal ones being the North Manitou and South Manitou, Beaver, and Fox groups in Lake Michigan; Isle Royale and Grande Isle in Lake Superior; Marquette, Bois Blanc, and Mackinac islands in Lake Huron; and Neebish, Sugar, and Drummond islands in St. Mary's River. Michigan has about 150 lighthouses, the most of any U.S. state. The first lighthouses in Michigan were built between 1818 and 1822. They were built to project light at night and to serve as a landmark during the day to safely guide the passenger ships and freighters traveling the Great Lakes. See Lighthouses in the United States.

The state's rivers are generally small, short and shallow, and few are navigable. The principal ones include the Detroit River, St. Marys River, and St. Clair River which connect the Great Lakes; the Au Sable, Cheboygan, and Saginaw, which flow into Lake Huron; the Ontonagon, and Tahquamenon, which flow into Lake Superior; and the St. Joseph, Kalamazoo, Grand, Muskegon, Manistee, and Escanaba, which flow into Lake Michigan. The state has 11,037 inland lakes—totaling of inland water—in addition to of Great Lakes waters. No point in Michigan is more than from an inland lake or more than from one of the Great Lakes.

The state is home to several areas maintained by the National Park Service including: Isle Royale National Park, in Lake Superior, about southeast of Thunder Bay, Ontario. Other national protected areas in the state include: Keweenaw National Historical Park, Pictured Rocks National Lakeshore, Sleeping Bear Dunes National Lakeshore, Huron National Forest, Manistee National Forest, Hiawatha National Forest, Ottawa National Forest and Father Marquette National Memorial. The largest section of the North Country National Scenic Trail passes through Michigan.

With 78 state parks, 19 state recreation areas, and 6 state forests, Michigan has the largest state park and state forest system of any state. These parks and forests include Holland State Park, Mackinac Island State Park, Au Sable State Forest, and Mackinaw State Forest.

Michigan has a continental climate, although there are two distinct regions. The southern and central parts of the Lower Peninsula (south of Saginaw Bay and from the Grand Rapids area southward) have a warmer climate (Köppen climate classification "Dfa") with hot summers and cold winters. The northern part of Lower Peninsula and the entire Upper Peninsula has a more severe climate (Köppen "Dfb"), with warm, but shorter summers and longer, cold to very cold winters. Some parts of the state average high temperatures below freezing from December through February, and into early March in the far northern parts. During the winter through the middle of February, the state is frequently subjected to heavy lake-effect snow. The state averages from of precipitation annually; however, some areas in the northern lower peninsula and the upper peninsula average almost of snowfall per year. Michigan's highest recorded temperature is at Mio on July 13, 1936, and the coldest recorded temperature is at Vanderbilt on February 9, 1934.

The state averages 30 days of thunderstorm activity per year. These can be severe, especially in the southern part of the state. The state averages 17 tornadoes per year, which are more common in the state's extreme southern section. Portions of the southern border have been almost as vulnerable historically as states further west and in Tornado Alley. For this reason, many communities in the very southern portions of the state have tornado sirens to warn residents of approaching tornadoes. Farther north, in Central Michigan, Northern Michigan, and the Upper Peninsula, tornadoes are rare.

The geological formation of the state is greatly varied, with the Michigan Basin being the most major formation. Primary boulders are found over the entire surface of the Upper Peninsula (being principally of primitive origin), while Secondary deposits cover the entire Lower Peninsula. The Upper Peninsula exhibits Lower Silurian sandstones, limestones, copper and iron bearing rocks, corresponding to the Huronian system of Canada. The central portion of the Lower Peninsula contains coal measures and rocks of the Pennsylvanian period. Devonian and sub-Carboniferous deposits are scattered over the entire state.

Michigan rarely experiences earthquakes, thus far mostly smaller ones that do not cause significant damage. A 4.6-magnitude earthquake struck in August 1947. More recently, a 4.2-magnitude earthquake occurred on Saturday, May 2, 2015, shortly after noon, about 5 miles south of Galesburg, Michigan (9 miles southeast of Kalamazoo) in central Michigan, about 140 miles west of Detroit, according to the Colorado-based U.S. Geological Survey's National Earthquake Information Center. No major damage or injuries were reported, according to Governor Rick Snyder's office.

The United States Census Bureau estimates the population of Michigan was 9,986,857 on July 1, 2019, an increase of 1.04% from 9,883,635 recorded at the 2010 United States Census.

The center of population of Michigan is in Shiawassee County, in the southeastern corner of the civil township of Bennington, which is northwest of the village of Morrice.

As of the 2010 American Community Survey for the U.S. Census, the state had a foreign-born population of 592,212, or 6.0% of the total. Michigan has the largest Dutch, Finnish, and Macedonian populations in the United States.

The 2010 Census reported:

In the same year Hispanics or Latinos (of any race) made up 4.4% of the population.

The large majority of Michigan's population is Caucasian. Americans of European descent live throughout Michigan and most of Metro Detroit. Large European American groups include those of German, British, Irish, Polish and Belgian ancestry. People of Scandinavian descent, and those of Finnish ancestry, have a notable presence in the Upper Peninsula. Western Michigan is known for the Dutch heritage of many residents (the highest concentration of any state), especially in Holland and metropolitan Grand Rapids.

African-Americans, who came to Detroit and other northern cities in the Great Migration of the early 20th century, form a majority of the population of the city of Detroit and of other cities, including Flint and Benton Harbor.

, almost 8,000 Hmong people lived in the State of Michigan, about double their 1999 presence in the state. most lived in northeastern Detroit, but they had been increasingly moving to Pontiac and Warren. By 2015 the number of Hmong in the Detroit city limits had significantly declined. Lansing hosts a statewide Hmong New Year Festival. The Hmong community also had a prominent portrayal in the 2008 film "Gran Torino", which was set in Detroit.

, 80% of Michigan's Japanese population lived in the counties of Macomb, Oakland, Washtenaw, and Wayne in the Detroit and Ann Arbor areas. , the largest Japanese national population is in Novi, with 2,666 Japanese residents, and the next largest populations are respectively in Ann Arbor, West Bloomfield Township, Farmington Hills, and Battle Creek. The state has 481 Japanese employment facilities providing 35,554 local jobs. 391 of them are in Southeast Michigan, providing 20,816 jobs, and the 90 in other regions in the state provide 14,738 jobs. The Japanese Direct Investment Survey of the Consulate-General of Japan, Detroit stated over 2,208 additional Japanese residents were employed in the State of Michigan , than in 2011. During the 1990s the Japanese population of Michigan experienced an increase, and many Japanese people with children moved to particular areas for their proximity to Japanese grocery stores and high-performing schools.

A person from Michigan is called a Michigander or Michiganian; also at times, but rarely, a "Michiganite". Residents of the Upper Peninsula are sometimes referred to as "Yoopers" (a phonetic pronunciation of "U.P.ers"), and they sometimes refer to those from the Lower Peninsula as "trolls" because they live below the bridge (see Three Billy Goats Gruff).

, 34.3% of Michigan's children under the age of one belonged to racial or ethnic minority groups, meaning they had at least one parent who was not non-Hispanic white.

"Note: Percentages in the table can exceed 100% as Hispanics are counted both by their ethnicity and by their race."


, 91.11% (8,507,947) of Michigan residents age five and older spoke only English at home, while 2.93% (273,981) spoke Spanish, 1.04% (97,559) Arabic, 0.44% (41,189) German, 0.36% (33,648) Chinese (which includes Mandarin), 0.31% (28,891) French, 0.29% (27,019) Polish, and Syriac languages (such as Modern Aramaic and Northeastern Neo-Aramaic) was spoken as a main language by 0.25% (23,420) of the population over the age of five. In total, 8.89% (830,281) of Michigan's population age 5 and older spoke a mother language other than English.

The Roman Catholic Church has six dioceses and one archdiocese in Michigan; Gaylord, Grand Rapids, Kalamazoo, Lansing, Marquette, Saginaw and Detroit. The Roman Catholic Church is the largest denomination by number of adherents, according to the Association of Religion Data Archives (ARDA) 2010 survey, with 1,717,296 adherents. The Roman Catholic Church was the only organized religion in Michigan until the 19th century, reflecting the territory's French colonial roots. Detroit's Saint Anne's parish, established in 1701 by Antoine de la Mothe Cadillac, is the second-oldest Roman Catholic parish in the United States. On March 8, 1833, the Holy See formally established a diocese in the Michigan territory, which included all of Michigan, Wisconsin, Minnesota, and the Dakotas east of the Mississippi River. When Michigan became a state in 1837, the boundary of the Diocese of Detroit was redrawn to coincide with that of the State; the other dioceses were later carved out from the Diocese of Detroit but remain part of the Ecclesiastical Province of Detroit.

In 2010, the largest Protestant denominations were the United Methodist Church with 228,521 adherents; followed by the Lutheran Church–Missouri Synod with 219,618, and the Evangelical Lutheran Church in America with 120,598 adherents. The Christian Reformed Church in North America had almost 100,000 members and over 230 congregations in Michigan. The Reformed Church in America had 76,000 members and 154 congregations in the state. In the same survey, Jewish adherents in the state of Michigan were estimated at 44,382, and Muslims at 120,351. The Lutheran Church was introduced by German and Scandinavian immigrants; Lutheranism is the second largest religious denomination in the state. The first Jewish synagogue in the state was Temple Beth El, founded by twelve German Jewish families in Detroit in 1850. In West Michigan, Dutch immigrants fled from the specter of religious persecution and famine in the Netherlands around 1850 and settled in and around what is now Holland, Michigan, establishing a "colony" on American soil that fervently held onto Calvinist doctrine that established a significant presence of Reformed churches. Islam was introduced by immigrants from the Near East during the 20th century. Michigan is home to the largest mosque in North America, the Islamic Center of America in Dearborn. Battle Creek, Michigan is also the birthplace of the Seventh-day Adventist Church, which was founded on May 21, 1863.

Total employment 2016
Number of employer establishments 

The U.S. Bureau of Economic Analysis estimated Michigan's Q3 2018 gross state product to be $538 billion, ranking 14th out of the 50 states. According to the Bureau of Labor Statistics, , the state's seasonally adjusted unemployment rate was estimated at 4.0%.

Products and services include automobiles, food products, information technology, aerospace, military equipment, furniture, and mining of copper and iron ore. Michigan is the third leading grower of Christmas trees with of land dedicated to Christmas tree farming. The beverage Vernors was invented in Michigan in 1866, sharing the title of oldest soft drink with Hires Root Beer. Faygo was founded in Detroit on November 4, 1907. Two of the top four pizza chains were founded in Michigan and are headquartered there: Domino's Pizza by Tom Monaghan and Little Caesars Pizza by Mike Ilitch. Michigan became the 24th Right to Work state in U.S. in 2012.

Since 2009, GM, Ford and Chrysler have managed a significant reorganization of their benefit funds structure after a volatile stock market which followed the September 11 attacks and early 2000s recession impacted their respective U.S. pension and benefit funds (OPEB). General Motors, Ford, and Chrysler reached agreements with the United Auto Workers Union to transfer the liabilities for their respective health care and benefit funds to a 501(c)(9) Voluntary Employee Beneficiary Association (VEBA). Manufacturing in the state grew 6.6% from 2001 to 2006, but the high speculative price of oil became a factor for the U.S. auto industry during the economic crisis of 2008 impacting industry revenues. In 2009, GM and Chrysler emerged from Chapter 11 restructurings with financing provided in part by the U.S. and Canadian governments. GM began its initial public offering (IPO) of stock in 2010. For 2010, the Big Three domestic automakers have reported significant profits indicating the beginning of rebound.

, Michigan ranked fourth in the U.S. in high tech employment with 568,000 high tech workers, which includes 70,000 in the automotive industry. Michigan typically ranks third or fourth in overall Research & development (R&D) expenditures in the United States. Its research and development, which includes automotive, comprises a higher percentage of the state's overall gross domestic product than for any other U.S. state. The state is an important source of engineering job opportunities. The domestic auto industry accounts directly and indirectly for one of every ten jobs in the U.S.

Michigan was second in the U.S. in 2004 for new corporate facilities and expansions. From 1997 to 2004, Michigan was the only state to top the 10,000 mark for the number of major new developments; however, the effects of the late 2000s recession have slowed the state's economy. In 2008, Michigan placed third in a site selection survey among the states for luring new business which measured capital investment and new job creation per one million population. In August 2009, Michigan and Detroit's auto industry received $1.36 B in grants from the U.S. Department of Energy for the manufacture of electric vehicle technologies which is expected to generate 6,800 immediate jobs and employ 40,000 in the state by 2020. From 2007 to 2009, Michigan ranked 3rd in the U.S. for new corporate facilities and expansions.

As leading research institutions, the University of Michigan, Michigan State University, and Wayne State University are important partners in the state's economy and its University Research Corridor. Michigan's public universities attract more than $1.5 B in research and development grants each year. The National Superconducting Cyclotron Laboratory is at Michigan State University. Michigan's workforce is well-educated and highly skilled, making it attractive to companies. It has the third highest number of engineering graduates nationally.

Detroit Metropolitan Airport is one of the nation's most recently expanded and modernized airports with six major runways, and large aircraft maintenance facilities capable of servicing and repairing a Boeing 747 and is a major hub for Delta Air Lines. Michigan's schools and colleges rank among the nation's best. The state has maintained its early commitment to public education. The state's infrastructure gives it a competitive edge; Michigan has 38 deep water ports. In 2007, Bank of America announced that it would commit $25 billion to community development in Michigan following its acquisition of LaSalle Bank in Troy.

Michigan led the nation in job creation improvement in 2010.

Michigan's personal income tax is set to a flat rate of 4.25%. In addition, 22 cities impose income taxes; rates are set at 1% for residents and 0.5% for non-residents in all but four cities. Michigan's state sales tax is 6%, though items such as food and medication are exempted from sales tax. Property taxes are assessed on the local level, but every property owner's local assessment contributes six mills (a rate of $6 per $1000 of property value) to the statutory State Education Tax. Property taxes are appealable to local boards of review and need the approval of the local electorate to exceed millage rates prescribed by state law and local charters. In 2011, the state repealed its business tax and replaced it with a 6% corporate income tax which substantially reduced taxes on business. Article IX of the Constitution of the State of Michigan also provides limitations on how much the state can tax.

The state also levies a 6% sales tax within the state and a Use tax on goods purchased outside the state (that are brought in and used in state). The use tax applies to internet sales/purchases from outside Michigan and is equivalent to the sales tax.

A wide variety of commodity crops, fruits, and vegetables are grown in Michigan, making it second only to California among U.S. states in the diversity of its agriculture. The state has 54,800 farms utilizing of land which sold $6.49 billion worth of products in 2010. The most valuable agricultural product is milk. Leading crops include corn, soybeans, flowers, wheat, sugar beets, and potatoes. Livestock in the state included 1 million cattle, 1 million hogs, 78,000 sheep and over 3 million chickens. Livestock products accounted for 38% of the value of agricultural products while crops accounted for the majority.

Michigan is a leading grower of fruit in the U.S., including blueberries, tart cherries, apples, grapes, and peaches. Plums, pears, and strawberries are also grown in Michigan. These fruits are mainly grown in West Michigan due to the moderating effect of Lake Michigan on the climate. There is also significant fruit production, especially cherries, but also grapes, apples, and other fruits, in Northwest Michigan along Lake Michigan. Michigan produces wines, beers and a multitude of processed food products. Kellogg's cereal is based in Battle Creek, Michigan and processes many locally grown foods. Thornapple Valley, Ball Park Franks, Koegel Meat Company, and Hebrew National sausage companies are all based in Michigan.

Michigan is home to very fertile land in the Saginaw Valley and "Thumb" areas. Products grown there include corn, sugar beets, navy beans, and soybeans. Sugar beet harvesting usually begins the first of October. It takes the sugar factories about five months to process the 3.7 million tons of sugarbeets into 485,000 tons of pure, white sugar. Michigan's largest sugar refiner, Michigan Sugar Company is the largest east of the Mississippi River and the fourth largest in the nation. Michigan Sugar brand names are Pioneer Sugar and the newly incorporated Big Chief Sugar. Potatoes are grown in Northern Michigan, and corn is dominant in Central Michigan. Alfalfa, cucumbers, and asparagus are also grown.

Michigan's tourists spend $17.2 billion per year in the state, supporting 193,000 tourism jobs. Michigan's tourism website ranks among the busiest in the nation. Destinations draw vacationers, hunters, and nature enthusiasts from across the United States and Canada. Michigan is fifty percent forest land, much of it quite remote. The forests, lakes and thousands of miles of beaches are top attractions. Event tourism draws large numbers to occasions like the Tulip Time Festival and the National Cherry Festival.
In 2006, the Michigan State Board of Education mandated all public schools in the state hold their first day of school after the Labor Day holiday, in accordance with the new Post Labor Day School law. A survey found 70% of all tourism business comes directly from Michigan residents, and the Michigan Hotel, Motel, & Resort Association claimed the shorter summer in between school years cut into the annual tourism season in the state.

Tourism in metropolitan Detroit draws visitors to leading attractions, especially The Henry Ford, the Detroit Institute of Arts, the Detroit Zoo, and to sports in Detroit. Other museums include the Detroit Historical Museum, the Charles H. Wright Museum of African American History, museums in the Cranbrook Educational Community, and the Arab American National Museum. The metro area offers four major casinos, MGM Grand Detroit, Greektown, Motor City, and Caesars Windsor in Windsor, Ontario, Canada; moreover, Detroit is the largest American city and metropolitan region to offer casino resorts.

Hunting and fishing are significant industries in the state. Charter boats are based in many Great Lakes cities to fish for salmon, trout, walleye, and perch. Michigan ranks first in the nation in licensed hunters (over one million) who contribute $2 billion annually to its economy. Over three-quarters of a million hunters participate in white-tailed deer season alone. Many school districts in rural areas of Michigan cancel school on the opening day of firearm deer season, because of attendance concerns.

Michigan's Department of Natural Resources manages the largest dedicated state forest system in the nation. The forest products industry and recreational users contribute $12 billion and 200,000 associated jobs annually to the state's economy. Public hiking and hunting access has also been secured in extensive commercial forests. The state has the highest number of golf courses and registered snowmobiles in the nation.

The state has numerous historical markers, which can themselves become the center of a tour. The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the St. Lawrence River.

With its position in relation to the Great Lakes and the countless ships that have foundered over the many years they have been used as a transport route for people and bulk cargo, Michigan is a world-class scuba diving destination. The Michigan Underwater Preserves are 11 underwater areas where wrecks are protected for the benefit of sport divers.

Michigan has nine international road crossings with Ontario, Canada:

A second international bridge between Detroit and Windsor is under consideration.

Michigan is served by four Class I railroads: the Canadian National Railway, the Canadian Pacific Railway, CSX Transportation, and the Norfolk Southern Railway. These are augmented by several dozen short line railroads. The vast majority of rail service in Michigan is devoted to freight, with Amtrak and various scenic railroads the exceptions.
Amtrak passenger rail services the state, connecting many southern and western Michigan cities to Chicago, Illinois. There are plans for commuter rail for Detroit and its suburbs (see SEMCOG Commuter Rail).


The Detroit Metropolitan Wayne County Airport, in the western suburb of Romulus, was in 2010 the 16th busiest airfield in North America measured by passenger traffic. The Gerald R. Ford International Airport in Grand Rapids is the next busiest airport in the state, served by eight airlines to 23 destinations. Flint Bishop International Airport is the third largest airport in the state, served by four airlines to several primary hubs. Cherry Capital Airport is in Traverse City. Alpena County Regional Airport services Alpena and the northeastern lower peninsula. MBS International Airport serves Midland, Bay City and Saginaw. Smaller regional and local airports are located throughout the state including on several islands.

Other economically significant cities include:

Half of the wealthiest communities in the state are in Oakland County, just north of Detroit. Another wealthy community is just east of the city, in Grosse Pointe. Only three of these cities are outside of Metro Detroit. The city of Detroit, with a per capita income of $14,717, ranks 517th on the list of Michigan locations by per capita income. Benton Harbor is the poorest city in Michigan, with a per capita income of $8,965, while Barton Hills is the richest with a per capita income of $110,683.

Michigan's education system provides services to 1.6 million K-12 students in public schools. More than 124,000 students attend private schools and an uncounted number are home-schooled under certain legal requirements. The public school system has a $14.5 billion budget in 2008–2009. Michigan has a number of public universities spread throughout the state and numerous private colleges as well. Michigan State University has the eighth largest campus population of any U.S. school. Seven of the state's universities—Central Michigan University, University of Michigan, Michigan State University, Michigan Technological University, Oakland University, Wayne State University, and Western Michigan University—are classified as research universities by the Carnegie Foundation.

Michigan music is known for three music trends: early punk rock, Motown/soul music and techno music. Michigan musicians include Bill Haley & His Comets, The Supremes, The Marvelettes, The Temptations, The Four Tops, Stevie Wonder, Marvin Gaye "The Prince of Soul", Smokey Robinson and the Miracles, Aretha Franklin, Mary Wells, Tommy James and the Shondells, ? and the Mysterians, Al Green, The Spinners, Grand Funk Railroad, The Stooges, the MC5, The Knack, Madonna "The Queen of Pop", Bob Seger, Ray Parker Jr., Aaliyah, Eminem, Kid Rock, Jack White and Meg White (The White Stripes), Big Sean, and Alice Cooper.

Major theaters in Michigan include the Fox Theatre, Music Hall, Gem Theatre, Masonic Temple Theatre, the Detroit Opera House, Fisher Theatre, The Fillmore Detroit, Saint Andrew's Hall, Majestic Theater, and Orchestra Hall.

The Nederlander Organization, the largest controller of Broadway productions in New York City, originated in Detroit. Detroit Symphony Orchestra

Motown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus.

Michigan's major-league sports teams include: Detroit Tigers baseball team, Detroit Lions football team, Detroit Red Wings ice hockey team, and the Detroit Pistons men's basketball team. All of Michigan's major league teams play in the Metro Detroit area.

The Pistons played at Detroit's Cobo Arena until 1978 and at the Pontiac Silverdome until 1988 when they moved into The Palace of Auburn Hills. In 2017, the team moved to the newly built Little Caesars Arena in downtown Detroit. The Detroit Lions played at Tiger Stadium in Detroit until 1974, then moved to the Pontiac Silverdome where they played for 27 years between 1975 and 2002 before moving to Ford Field in Detroit in 2002. The Detroit Tigers played at Tiger Stadium (formerly known as Navin Field and Briggs Stadium) from 1912 to 1999. In 2000 they moved to Comerica Park. The Red Wings played at Olympia Stadium before moving to Joe Louis Arena in 1979. They later moved to Little Caesars Arena to join the Pistons as tenants in 2017. Professional hockey got its start in Houghton, when the Portage Lakers were formed.

The Michigan International Speedway is the site of NASCAR races and Detroit was formerly the site of a Formula One World Championship Grand Prix race. From 1959 to 1961, Detroit Dragway hosted the NHRA's U.S. Nationals. Michigan is home to one of the major canoeing marathons: the Au Sable River Canoe Marathon. The Port Huron to Mackinac Boat Race is also a favorite.

Twenty-time Grand Slam champion Serena Williams was born in Saginaw. The 2011 World Champion for Women's Artistic Gymnastics, Jordyn Wieber is from DeWitt. Wieber was also a member of the gold medal winning team at the London Olympics in 2012.

Collegiate sports in Michigan are popular in addition to professional sports. The state's two largest athletic programs are the Michigan Wolverines and Michigan State Spartans, which play in the NCAA Big Ten Conference. Michigan Stadium in Ann Arbor, home to the Michigan Wolverines football team, is the largest stadium in the Western Hemisphere and the second-largest stadium worldwide behind Rungrado May Day Stadium in Pyongyang, North Korea.

The Michigan High School Athletic Association features around 300,000 participants.

Michigan is, by tradition, known as "The Wolverine State," and the University of Michigan takes the wolverine as its mascot. The association is well and long established: for example, many Detroiters volunteered to fight during the American Civil War and George Armstrong Custer, who led the Michigan Brigade, called them the "Wolverines". The origins of this association are obscure; it may derive from a busy trade in wolverine furs in Sault Ste. Marie in the 18th century or may recall a disparagement intended to compare early settlers in Michigan with the vicious mammal. Wolverines are, however, extremely rare in Michigan. A sighting in February 2004 near Ubly was the first confirmed sighting in Michigan in 200 years. The animal was found dead in 2010.





</doc>
<doc id="18862" url="https://en.wikipedia.org/wiki?curid=18862" title="Minimum wage">
Minimum wage

A minimum wage is the lowest remuneration that employers can legally pay their workers—the price floor below which workers may not sell their labor. Most countries had introduced minimum wage legislation by the end of the 20th century.

Supply and demand models suggest that there may be welfare and employment losses from minimum wages. However, if the labor market is in a state of monopsony (with only one employer available who is hiring), minimum wages can increase the efficiency of the market. There is debate about the full effects of minimum wages.

The movement for minimum wages was first motivated as a way to stop the exploitation of workers in sweatshops, by employers who were thought to have unfair bargaining power over them. Over time, minimum wages came to be seen as a way to help lower-income families. Modern national laws enforcing compulsory union membership which prescribed minimum wages for their members were first passed in New Zealand and Australia in the 1890s.

Although minimum wage laws are in effect in many jurisdictions, differences of opinion exist about the benefits and drawbacks of a minimum wage. Supporters of the minimum wage say it increases the standard of living of workers, reduces poverty, reduces inequality, and boosts morale. In contrast, opponents of the minimum wage say it increases poverty, increases unemployment because some low-wage workers "will be unable to find work...[and] will be pushed into the ranks of the unemployed" and is damaging to businesses, because excessively high minimum wages require businesses to raise the prices of their product or service to accommodate the extra expense of paying a higher wage.

Modern minimum wage laws trace their origin to the Ordinance of Labourers (1349), which was a decree by King Edward III that set a" maximum wage" for laborers in medieval England. King Edward III, who was a wealthy landowner, was dependent, like his lords, on serfs to work the land. In the autumn of 1348, the Black Plague reached England and decimated the population. The severe shortage of labor caused wages to soar and encouraged King Edward III to set a wage ceiling. Subsequent amendments to the ordinance, such as the Statute of Labourers (1351), increased the penalties for paying a wage above the set rates.

While the laws governing wages initially set a ceiling on compensation, they were eventually used to set a living wage. An amendment to the Statute of Labourers in 1389 effectively fixed wages to the price of food. As time passed, the Justice of the Peace, who was charged with setting the maximum wage, also began to set formal minimum wages. The practice was eventually formalized with the passage of the Act Fixing a Minimum Wage in 1604 by King James I for workers in the textile industry.

By the early 19th century, the Statutes of Labourers was repealed as increasingly capitalistic England embraced "laissez-faire" policies which disfavored regulations of wages (whether upper or lower limits). The subsequent 19th century saw significant labor unrest affect many industrial nations. As trade unions were decriminalized during the century, attempts to control wages through collective agreement were made. However, this meant that a uniform minimum wage was not possible. In "Principles of Political Economy" in 1848, John Stuart Mill argued that because of the collective action problems that workers faced in organisation, it was a justified departure from "laissez-faire" policies (or freedom of contract) to regulate people's wages and hours by the law.

It was not until the 1890s that the first modern legislative attempts to regulate minimum wages were seen in New Zealand and Australia. The movement for a minimum wage was initially focused on stopping sweatshop labor and controlling the proliferation of sweatshops in manufacturing industries. The sweatshops employed large numbers of women and young workers, paying them what were considered to be substandard wages. The sweatshop owners were thought to have unfair bargaining power over their employees, and a minimum wage was proposed as a means to make them pay fairly. Over time, the focus changed to helping people, especially families, become more self-sufficient.

The first modern national minimum wages were enacted by the government recognition of unions which in turn established minimum wage policy among their members, as in New Zealand in 1894, followed by Australia in 1896 and the United Kingdom in 1909. In the United States, statutory minimum wages were first introduced nationally in 1938, and they were reintroduced and expanded in the United Kingdom in 1998. There is now legislation or binding collective bargaining regarding minimum wage in more than 90 percent of all countries. In the European Union, 22 member states out of 28 currently have national minimum wages. Other countries, such as Sweden, Finland, Denmark, Switzerland, Austria, and Italy, have no minimum wage laws, but rely on employer groups and trade unions to set minimum earnings through collective bargaining.

Minimum wage rates vary greatly across many different jurisdictions, not only in setting a particular amount of money—for example $7.25 per hour ($14,500 per year) under certain US state laws (or $2.13 for employees who receive tips, which is known as the tipped minimum wage), $11.00 in the US state of Washington, or £7.83 (for those aged 25+) in the United Kingdom—but also in terms of which pay period (for example Russia and China set monthly minimum wages) or the scope of coverage. Currently the United States federal minimum wage is $7.25 per hour. However, some states do not recognize the minimum wage law, such as Louisiana and Tennessee. Other states operate below the federal minimum wage such as Georgia and Wyoming. Some jurisdictions allow employers to count tips given to their workers as credit towards the minimum wage levels. India was one of the first developing countries to introduce minimum wage policy in its law in 1948. However, it is rarely implemented, even by contractors of government agencies. In Mumbai, as of 2017, the minimum wage was Rs. 348/day.
India also has one of the most complicated systems with more than 1,200 minimum wage rates depending on the geographical region.

Customs and extra-legal pressures from governments or labor unions can produce a "de facto" minimum wage. So can international public opinion, by pressuring multinational companies to pay Third World workers wages usually found in more industrialized countries. The latter situation in Southeast Asia and Latin America was publicized in the 2000s, but it existed with companies in West Africa in the middle of the 20th century.

Among the indicators that might be used to establish an initial minimum wage rate are ones that minimize the loss of jobs while preserving international competitiveness. Among these are general economic conditions as measured by real and nominal gross domestic product; inflation; labor supply and demand; wage levels, distribution and differentials; employment terms; productivity growth; labor costs; business operating costs; the number and trend of bankruptcies; economic freedom rankings; standards of living and the prevailing average wage rate.

In the business sector, concerns include the expected increased cost of doing business, threats to profitability, rising levels of unemployment (and subsequent higher government expenditure on welfare benefits raising tax rates), and the possible knock-on effects to the wages of more experienced workers who might already be earning the new statutory minimum wage, or slightly more. Among workers and their representatives, political considerations weigh in as labor leaders seek to win support by demanding the highest possible rate. Other concerns include purchasing power, inflation indexing and standardized working hours.

In the United States, the minimum wage have been set under the Fair Labor Standards Act of 1938. According to the Economic Policy Institute, the minimum wage in the United States would have been $18.28 in 2013 if the minimum wage had kept pace with labor productivity. To adjust for increased rates of worker productivity in the United States, raising the minimum wage to $22 (or more) an hour has been presented.

According to the supply and demand model of the labor market shown in many economics textbooks, increasing the minimum wage decreases the employment of minimum-wage workers. One such textbook states:
A firm's cost is an increasing function of the wage rate. The higher the wage rate, the fewer hours an employer will demand of employees. This is because, as the wage rate rises, it becomes more expensive for firms to hire workers and so firms hire fewer workers (or hire them for fewer hours). The demand of labor curve is therefore shown as a line moving down and to the right. Since higher wages increase the quantity supplied, the supply of labor curve is upward sloping, and is shown as a line moving up and to the right. If no minimum wage is in place, wages will adjust until quantity of labor demanded is equal to quantity supplied, reaching equilibrium, where the supply and demand curves intersect. Minimum wage behaves as a classical price floor on labor. Standard theory says that, if set above the equilibrium price, more labor will be willing to be provided by workers than will be demanded by employers, creating a surplus of labor, i.e. unemployment. The economic model of markets predicts the same of other commodities (like milk and wheat, for example): Artificially raising the price of the commodity tends to cause an increase in quantity supplied and a decrease in quantity demanded. The result is a surplus of the commodity. When there is a wheat surplus, the government buys it. Since the government does not hire surplus labor, the labor surplus takes the form of unemployment, which tends to be higher with minimum wage laws than without them.

The supply and demand model implies that by mandating a price floor above the equilibrium wage, minimum wage laws will cause unemployment. This is because a greater number of people are willing to work at the higher wage while a smaller number of jobs will be available at the higher wage. Companies can be more selective in those whom they employ thus the least skilled and least experienced will typically be excluded. An imposition or increase of a minimum wage will generally only affect employment in the low-skill labor market, as the equilibrium wage is already at or below the minimum wage, whereas in higher skill labor markets the equilibrium wage is too high for a change in minimum wage to affect employment.

The supply and demand model predicts that raising the minimum wage helps workers whose wages are raised, and hurts people who are not hired (or lose their jobs) when companies cut back on employment. But proponents of the minimum wage hold that the situation is much more complicated than the model can account for. One complicating factor is possible monopsony in the labor market, whereby the individual employer has some market power in determining wages paid. Thus it is at least theoretically possible that the minimum wage may boost employment. Though single employer market power is unlikely to exist in most labor markets in the sense of the traditional 'company town,' asymmetric information, imperfect mobility, and the personal element of the labor transaction give some degree of wage-setting power to most firms.

Modern economic theory predicts that although an excessive minimum wage may raise unemployment as it fixes a price above most demand for labor, a minimum wage at a more reasonable level can increase employment, and enhance growth and efficiency. This is because labor markets are monopsonistic and workers persistently lack bargaining power. When poorer workers have more to spend it stimulates effective aggregate demand for goods and services.

The argument that a minimum wage decreases employment is based on a simple supply and demand model of the labor market. A number of economists (for example Pierangelo Garegnani, Robert L. Vienneau, and Arrigo Opocher & Ian Steedman), building on the work of Piero Sraffa, argue that that model, even given all its assumptions, is logically incoherent. Michael Anyadike-Danes and Wynne Godley argue, based on simulation results, that little of the empirical work done with the textbook model constitutes a potentially falsifiable theory, and consequently empirical evidence hardly exists for that model. Graham White argues, partially on the basis of Sraffianism, that the policy of increased labor market flexibility, including the reduction of minimum wages, does not have an "intellectually coherent" argument in economic theory.
Gary Fields, Professor of Labor Economics and Economics at Cornell University, argues that the standard textbook model for the minimum wage is ambiguous, and that the standard theoretical arguments incorrectly measure only a one-sector market. Fields says a two-sector market, where "the self-employed, service workers, and farm workers are typically excluded from minimum-wage coverage... [and with] one sector with minimum-wage coverage and the other without it [and possible mobility between the two]," is the basis for better analysis. Through this model, Fields shows the typical theoretical argument to be ambiguous and says "the predictions derived from the textbook model definitely do not carry over to the two-sector case. Therefore, since a non-covered sector exists nearly everywhere, the predictions of the textbook model simply cannot be relied on."

An alternate view of the labor market has low-wage labor markets characterized as monopsonistic competition wherein buyers (employers) have significantly more market power than do sellers (workers). This monopsony could be a result of intentional collusion between employers, or naturalistic factors such as segmented markets, search costs, information costs, imperfect mobility and the personal element of labor markets. In such a case a simple supply and demand graph would not yield the quantity of labor clearing and the wage rate. This is because while the upward sloping aggregate labor supply would remain unchanged, instead of using the upward labor supply curve shown in a supply and demand diagram, monopsonistic employers would use a steeper upward sloping curve corresponding to marginal expenditures to yield the intersection with the supply curve resulting in a wage rate lower than would be the case under competition. Also, the amount of labor sold would also be lower than the competitive optimal allocation.

Such a case is a type of market failure and results in workers being paid less than their marginal value. Under the monopsonistic assumption, an appropriately set minimum wage could increase both wages and employment, with the optimal level being equal to the marginal product of labor. This view emphasizes the role of minimum wages as a market regulation policy akin to antitrust policies, as opposed to an illusory "free lunch" for low-wage workers.

Another reason minimum wage may not affect employment in certain industries is that the demand for the product the employees produce is highly inelastic. For example, if management is forced to increase wages, management can pass on the increase in wage to consumers in the form of higher prices. Since demand for the product is highly inelastic, consumers continue to buy the product at the higher price and so the manager is not forced to lay off workers. Economist Paul Krugman argues this explanation neglects to explain why the firm was not charging this higher price absent the minimum wage.

Three other possible reasons minimum wages do not affect employment were suggested by Alan Blinder: higher wages may reduce turnover, and hence training costs; raising the minimum wage may "render moot" the potential problem of recruiting workers at a higher wage than current workers; and minimum wage workers might represent such a small proportion of a business's cost that the increase is too small to matter. He admits that he does not know if these are correct, but argues that "the list demonstrates that one can accept the new empirical findings and still be a card-carrying economist."

The following mathematical models are more quantitative in orientation, and highlight some of the difficulties in determining the impact of the minimum wage on labor market outcomes. Specifically, these models focus on labor markets with frictions.

Assume that the decision to participate in the labor market results from a trade-off between being an unemployed job seeker and not participating at all. All individuals whose expected utility outside the labor market is less than the expected utility of an unemployed person formula_1 decide to participate in the labor market. In the basic search and matching model, the expected utility of unemployed persons formula_1 and that of employed persons formula_3 are defined by:

formula_4Let formula_5 be the wage, formula_6 the interest rate, formula_7 the instantaneous income of unemployed persons, formula_8 the exogenous job destruction rate, formula_9 the labor market tightness, and formula_10 the job finding rate. The profits formula_11 and formula_12 expected from a filled job and a vacant one are:formula_13where formula_14 is the cost of a vacant job and formula_15 is the productivity. When the "free entry condition" formula_16 is satisfied, these two equalities yield the following relationship between the wage formula_5 and labor market tightness formula_9:

formula_19If formula_5 represents a minimum wage that applies to all workers, this equation completely determines the equilibrium value of the labor market tightness formula_9. There are two conditions associated with the matching function:formula_22This implies that formula_9 is a decreasing function of the minimum wage formula_5, and so is the job finding rate formula_25. A hike in the minimum wage degrades the profitability of a job, so firms post fewer vacancies and the job finding rate falls off. Now let's rewrite formula_26 to be:formula_27Using the relationship between the wage and labor market tightness to eliminate the wage from the last equation gives us:
formula_28 If we maximize formula_26 in this equation, with respect to the labor market tightness, we find that:formula_30where formula_31 is the elasticity of the matching function:formula_32This result shows that the expected utility of unemployed workers is maximized when the minimum wage is set at a level that corresponds to the wage level of the decentralized economy in which the bargaining power parameter is equal to the elasticity formula_31.  The level of the negotiated wage is formula_34.

If formula_35, then an increase in the minimum wage increases participation "and" the unemployment rate, with an ambiguous impact on employment. When the bargaining power of workers is less than formula_31, an increases in the minimum wage improves the welfare of the unemployed - this suggests that minimum wage hikes can improve labor market efficiency, at least up to the point when bargaining power equals formula_31. On the other hand, if formula_38, any increases in the minimum wage entails a decline in labor market participation and an increase in unemployment.

In the model just presented, we found that the minimum wage always increases unemployment. This result does not necessarily hold when the search effort of workers in endogenous.

Consider a model where the intensity of the job search is designated by the scalar formula_39, which can be interpreted as the amount of time and/or intensity of the effort devoted to search. Assume that the arrival rate of job offers is formula_40 and that the wage distribution is degenerated to a single wage formula_5. Denote formula_42 to be the cost arising from the search effort, with formula_43. Then the discounted utilities are given by:formula_44Therefore, the optimal search effort is such that the marginal cost of performing the search is equation to the marginal return:formula_45This implies that the optimal search effort increases as the difference between the expected utility of the job holder and the expected utility of the job seeker grows. In fact, this difference actually grows with the wage. To see this, take the difference of the two discounted utilities to find:formula_46Then differentiating with respect to formula_5 and rearranging gives us:formula_48where formula_49 is the optimal search effort. This implies that a wage increase drives up job search effort and, therefore, the job finding rate. Additionally, the unemployment rate formula_50 at equilibrium is given by:formula_51A hike in the wage, which increases the search effort and the job finding rate, decreases the unemployment rate. So it is possible that a hike in the minimum wage "may", by boosting the search effort of job seekers, boost employment. Taken in sum with the previous section, the minimum wage in labor markets with frictions can improve employment and decrease the unemployment rate when it is sufficiently low. However, a high minimum wage is detrimental to employment and increases the unemployment rate.

Economists disagree as to the measurable impact of minimum wages in practice. This disagreement usually takes the form of competing empirical tests of the elasticities of supply and demand in labor markets and the degree to which markets differ from the efficiency that models of perfect competition predict.

Economists have done empirical studies on different aspects of the minimum wage, including:
Until the mid-1990s, a general consensus existed among economists, both conservative and liberal, that the minimum wage reduced employment, especially among younger and low-skill workers. In addition to the basic supply-demand intuition, there were a number of empirical studies that supported this view. For example, Gramlich (1976) found that many of the benefits went to higher income families, and that teenagers were made worse off by the unemployment associated with the minimum wage.

Brown et al. (1983) noted that time series studies to that point had found that for a 10 percent increase in the minimum wage, there was a decrease in teenage employment of 1–3 percent. However, the studies found wider variation, from 0 to over 3 percent, in their estimates for the effect on teenage unemployment (teenagers without a job and looking for one). In contrast to the simple supply and demand diagram, it was commonly found that teenagers withdrew from the labor force in response to the minimum wage, which produced the possibility of equal reductions in the supply as well as the demand for labor at a higher minimum wage and hence no impact on the unemployment rate. Using a variety of specifications of the employment and unemployment equations (using ordinary least squares vs. generalized least squares regression procedures, and linear vs. logarithmic specifications), they found that a 10 percent increase in the minimum wage caused a 1 percent decrease in teenage employment, and no change in the teenage unemployment rate. The study also found a small, but statistically significant, increase in unemployment for adults aged 20–24.

Wellington (1991) updated Brown et al.'s research with data through 1986 to provide new estimates encompassing a period when the real (i.e., inflation-adjusted) value of the minimum wage was declining, because it had not increased since 1981. She found that a 10% increase in the minimum wage decreased the absolute teenage employment by 0.6%, with no effect on the teen or young adult unemployment rates.

Some research suggests that the unemployment effects of small minimum wage increases are dominated by other factors. In Florida, where voters approved an increase in 2004, a follow-up comprehensive study after the increase confirmed a strong economy with increased employment above previous years in Florida and better than in the US as a whole. When it comes to on-the-job training, some believe the increase in wages is taken out of training expenses. A 2001 empirical study found that there is "no evidence that minimum wages reduce training, and little evidence that they tend to increase training."

Some empirical studies have tried to ascertain the benefits of a minimum wage beyond employment effects. In an analysis of census data, Joseph Sabia and Robert Nielson found no statistically significant evidence that minimum wage increases helped reduce financial, housing, health, or food insecurity. This study was undertaken by the Employment Policies Institute, a think tank funded by the food, beverage and hospitality industries. In 2012, Michael Reich published an economic analysis that suggested that a proposed minimum wage hike in San Diego might stimulate the city's economy by about $190 million.

"The Economist" wrote in December 2013: "A minimum wage, providing it is not set too high, could thus boost pay with no ill effects on jobs...America's federal minimum wage, at 38% of median income, is one of the rich world's lowest. Some studies find no harm to employment from federal or state minimum wages, others see a small one, but none finds any serious damage. ... High minimum wages, however, particularly in rigid labour markets, do appear to hit employment. France has the rich world’s highest wage floor, at more than 60% of the median for adults and a far bigger fraction of the typical wage for the young. This helps explain why France also has shockingly high rates of youth unemployment: 26% for 15- to 24-year-olds."

The restaurant industry is commonly studied because of its high number of minimum wage workers. A 2018 study from the Center on Wage and Employment Dynamics at the University of California, Berkeley focusing on food services showed that minimum wage increases in Washington, Chicago, Seattle, San Francisco, Oakland, and San Jose gave workers higher pay without hampering job growth. A 2017 study of restaurants in the San Francisco Bay Area examined the period 2008-2016 and the effect that a minimum wage increase had on the probability of restaurants going out of business, and broke out results based on the restaurant's rating on the review site Yelp. The study found no effect for 5-star (highest rated) restaurants (regardless of the expensiveness of the cuisine) but those with increasingly lower ratings were increasingly likely to go out of business (for example a 14% increase at 3.5 stars for a $1 per hour minimum wage increase). It also noted that the Yelp star rating was correlated with likelihood of minority ownership and minority customer base. Importantly, it noted that restaurants below 4 star in rating were proportionally more likely to hire low-skilled workers. The minimum wage increases during this period did not prevent growth in the industry overall – the number of restaurants in San Francisco went from 3,600 in 2012 to 7,600 in 2016. An August 2019 study from The New School's Center for New York City Affairs and the think tank National Employment Law Project, which advocates for raising the minimum wage, found that the restaurant industry in New York City has been "thriving" following an increase in the minimum wage to $15 an hour.

A 2019 study in the "Quarterly Journal of Economics" found that minimum wage increases did not have an impact on the overall number of low-wage jobs in the five years subsequent to the wage increase. However, it did find disemployment in 'tradeable' sectors, defined as those sectors most reliant on entry level or low skilled labor.

In another study, which shared authors with the above, published in the "American Economic Review" found that a large and persistent increase in the minimum wage in Hungary produced some disemployment with the large majority of additional cost being passed on to consumers. The authors also found that firms began substituting capital for labor over time.

In 1992, the minimum wage in New Jersey increased from $4.25 to $5.05 per hour (an 18.8% increase), while in the adjacent state of Pennsylvania it remained at $4.25. David Card and Alan Krueger gathered information on fast food restaurants in New Jersey and eastern Pennsylvania in an attempt to see what effect this increase had on employment within New Jersey. A basic supply and demand model predicts that relative employment should have decreased in New Jersey. Card and Krueger surveyed employers before the April 1992 New Jersey increase, and again in November–December 1992, asking managers for data on the full-time equivalent staff level of their restaurants both times. Based on data from the employers' responses, the authors concluded that the increase in the minimum wage slightly increased employment in the New Jersey restaurants.

Card and Krueger expanded on this initial article in their 1995 book "Myth and Measurement: The New Economics of the Minimum Wage". They argued that the negative employment effects of minimum wage laws are minimal if not non-existent. For example, they look at the 1992 increase in New Jersey's minimum wage, the 1988 rise in California's minimum wage, and the 1990–91 increases in the federal minimum wage. In addition to their own findings, they reanalyzed earlier studies with updated data, generally finding that the older results of a negative employment effect did not hold up in the larger datasets.

In 1996, David Neumark and William Wascher reexamined Card and Krueger's result using administrative payroll records from a sample of large fast food restaurant chains, and reported that minimum wage increases were followed by decreases in employment. An assessment of data collected and analyzed by Neumark and Wascher did not initially contradict the Card and Krueger results, but in a later edited version they found a four percent decrease in employment, and reported that "the estimated disemployment effects in the payroll data are often statistically significant at the 5- or 10-percent level although there are some estimators and subsamples that yield insignificant—although almost always negative" employment effects. Neumark and Wascher's conclusions were subsequently rebutted in a 2000 paper by Card and Krueger. A 2011 paper has reconciled the difference between Card and Krueger's survey data and Neumark and Wascher's payroll-based data. The paper shows that both datasets evidence conditional employment effects that are positive for small restaurants, but are negative for large fast-food restaurants. A 2014 analysis based on panel data found that the minimum wage reduces employment among teenagers.

In 1996 and 1997, the federal minimum wage was increased from $4.25 to $5.15, thereby increasing the minimum wage by $0.90 in Pennsylvania but by just $0.10 in New Jersey; this allowed for an examination of the effects of minimum wage increases in the same area, subsequent to the 1992 change studied by Card and Krueger. A study by Hoffman and Trace found the result anticipated by traditional theory: a detrimental effect on employment.

Further application of the methodology used by Card and Krueger by other researchers yielded results similar to their original findings, across additional data sets. A 2010 study by three economists (Arindrajit Dube of the University of Massachusetts Amherst, William Lester of the University of North Carolina at Chapel Hill, and Michael Reich of the University of California, Berkeley), compared adjacent counties in different states where the minimum wage had been raised in one of the states. They analyzed employment trends for several categories of low-wage workers from 1990 to 2006 and found that increases in minimum wages had no negative effects on low-wage employment and successfully increased the income of workers in food services and retail employment, as well as the narrower category of workers in restaurants.

However, a 2011 study by Baskaya and Rubinstein of Brown University found that at the federal level, "a rise in minimum wage have ["sic"] an instantaneous impact on wage rates and a corresponding negative impact on employment", stating, "Minimum wage increases boost teenage wage rates and reduce teenage employment." Another 2011 study by Sen, Rybczynski, and Van De Waal found that "a 10% increase in the minimum wage is significantly correlated with a 3−5% drop in teen employment." A 2012 study by Sabia, Hansen, and Burkhauser found that "minimum wage increases can have substantial adverse labor demand effects for low-skilled individuals", with the largest effects on those aged 16 to 24.

A 2013 study by Meer and West concluded that "the minimum wage reduces net job growth, primarily through its effect on job creation by expanding establishments ... most pronounced for younger workers and in industries with a higher proportion of low-wage workers." This study by Meer and West was later critiqued for its trends of assumption in the context of narrowly defined low-wage groups. The authors replied to the critiques and released additional data which addressed the criticism of their methodology, but did not resolve the issue of whether their data showed a causal relationship. A 2019 paper published in the "Quarterly Journal of Economics" by Cengiz, Dube, Lindner and Zipperer argues that the job losses found using a Meer and West type methodology "tend to be driven by an unrealistically large drop in the number of jobs at the upper tail of the wage distribution, which is unlikely to be a causal effect of the minimum wage." Another 2013 study by Suzana Laporšek of the University of Primorska, on youth unemployment in Europe claimed there was "a negative, statistically significant impact of minimum wage on youth employment." A 2013 study by labor economists Tony Fang and Carl Lin which studied minimum wages and employment in China, found that "minimum wage changes have significant adverse effects on employment in the Eastern and Central regions of China, and result in disemployment for females, young adults, and low-skilled workers".

A 2017 study found that in Seattle, increasing the minimum wage to $13 per hour lowered income of low-wage workers by $125 per month, due to the resulting reduction in hours worked, as industries made changes to make their businesses less labor intensive. The authors argue that previous research that found no negative effects on hours worked are flawed because they only look at select industries, or only look at teenagers, instead of entire economies.

Finally, a study by Overstreet in 2019 examined increases to the minimum wage in Arizona. Utilizing data spanning from 1976 to 2017, Overstreet found that a 1% increase in the minimum wage was significantly correlated with a 1.13% increase in per capita income in Arizona. This study could show that smaller increases in minimum wage may not distort labor market as significantly as larger increases experienced in other cities and states. Thus, the small increases experienced in Arizona may have actually led to a slight increase in economic growth.

In 2019, economists from Georgia Tech published a study that found a strong correlation between incresases to the minimum wage and detectable harm to the financial conditions of small businesses, including a higher rate of bankruptcy, lower hiring rates, lower credit scores, and higher interest payments. The researchers noted that these small businesses were also correlated with minority ownership and minority customer bases. 

In July 2019, the Congressional Budget Office published the impact on proposed national $15/hour legislation. It noted that workers who retained full employment would see a modest improvement in take home pay offset by a small decrease in working conditions and non-pecuniary benefits. However, this benefit is offset by three primary factors; the reduction in hours worked, the reduction in total employment, and the increased cost of goods and services. Those factors result in a decrease of about $33 Billion in total income and nearly 1.7-3.7 million lost jobs in the first three years (the CBO also noted this figure increases over time). 

In response to an April 2016 Council of Economic Advisers (CEA) report advocating the raising of the minimum wage to deter crime, economists used data from the 1998-2016 Uniform Crime Reports (UCR), National Incident-Based Reporting System (NIBRS), and National Longitudinal Study of Youth (NLSY) to assess the impact of the minimum wage on crime. They found that increasing the minimum wage resulted in increased property crime arrests among those ages 16-to-24. They estimated that an increase of the Federal minimum wage to $15/hour would "generate criminal externality costs of nearly $2.4 billion." 

Economists in Denmark, relying on a discontinuity in wage rates when a worker turns 18, found that employment fell by 33% and total hours fell by 45% when the minimum wage law was in effect. 

Several researchers have conducted statistical meta-analyses of the employment effects of the minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and concluded that there was clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower standard errors, did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the .05 level). Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas Leonard noted, "The silence is fairly deafening."

In 2005, T.D. Stanley showed that Card and Krueger's results could signify either publication bias or the absence of a minimum wage effect. However, using a different methodology, Stanley concluded that there is evidence of publication bias and that correction of this bias shows no relationship between the minimum wage and unemployment. In 2008, Hristos Doucouliagos and T.D. Stanley conducted a similar meta-analysis of 64 U.S. studies on disemployment effects and concluded that Card and Krueger's initial claim of publication bias is still correct. Moreover, they concluded, "Once this publication selection is corrected, little or no evidence of a negative association between minimum wages and employment remains." In 2013, a meta-analysis of 16 UK studies found no significant effects on employment attributable to the minimum wage. a 2007 meta-analyses by David Neumark of 96 studies found a consistent, but not always statistically significant, negative effect on employment from increases in the minimum wage.

Minimum wage laws affect workers in most low-paid fields of employment and have usually been judged against the criterion of reducing poverty. Minimum wage laws receive less support from economists than from the general public. Despite decades of experience and economic research, debates about the costs and benefits of minimum wages continue today.

Various groups have great ideological, political, financial, and emotional investments in issues surrounding minimum wage laws. For example, agencies that administer the laws have a vested interest in showing that "their" laws do not create unemployment, as do labor unions whose members' finances are protected by minimum wage laws. On the other side of the issue, low-wage employers such as restaurants finance the Employment Policies Institute, which has released numerous studies opposing the minimum wage. The presence of these powerful groups and factors means that the debate on the issue is not always based on dispassionate analysis. Additionally, it is extraordinarily difficult to separate the effects of minimum wage from all the other variables that affect employment.

The following table summarizes the arguments made by those for and against minimum wage laws:

A widely circulated argument that the minimum wage was ineffective at reducing poverty was provided by George Stigler in 1949:
In 2006, the International Labour Organization (ILO) argued that the minimum wage could not be directly linked to unemployment in countries that have suffered job losses. In April 2010, the Organisation for Economic Co-operation and Development (OECD) released a report arguing that countries could alleviate teen unemployment by "lowering the cost of employing low-skilled youth" through a sub-minimum training wage. A study of U.S. states showed that businesses' annual and average payrolls grow faster and employment grew at a faster rate in states with a minimum wage. The study showed a correlation, but did not claim to prove causation.

Although strongly opposed by both the business community and the Conservative Party when introduced in the UK in 1999, the Conservatives reversed their opposition in 2000. Accounts differ as to the effects of the minimum wage. The Centre for Economic Performance found no discernible impact on employment levels from the wage increases, while the Low Pay Commission found that employers had reduced their rate of hiring and employee hours employed, and found ways to cause current workers to be more productive (especially service companies). The Institute for the Study of Labor found prices in the minimum wage sector rose significantly faster than prices in non-minimum wage sectors, in the four years following the implementation of the minimum wage. Neither trade unions nor employer organizations contest the minimum wage, although the latter had especially done so heavily until 1999.

In 2014, supporters of minimum wage cited a study that found that job creation within the United States is faster in states that raised their minimum wages. In 2014, supporters of minimum wage cited news organizations who reported the state with the highest minimum-wage garnered more job creation than the rest of the United States.

In 2014, in Seattle, Washington, liberal and progressive business owners who had supported the city's new $15 minimum wage said they might hold off on expanding their businesses and thus creating new jobs, due to the uncertain timescale of the wage increase implementation. However, subsequently at least two of the business owners quoted did expand.

The dollar value of the minimum wage loses purchasing power over time due to inflation. Minimum wage laws, for instance proposals to index the minimum wage to average wages, have the potential to keep the dollar value of the minimum wage relevant and predictable.

With regard to the economic effects of introducing minimum wage legislation in Germany in January 2015, recent developments have shown that the feared increase in unemployment has not materialized, however, in some economic sectors and regions of the country, it came to a decline in job opportunities particularly for temporary and part-time workers, and some low-wage jobs have disappeared entirely. Because of this overall positive development, the Deutsche Bundesbank revised its opinion, and ascertained that “the impact of the introduction of the minimum wage on the total volume of work appears to be very limited in the present business cycle”.

A 2019 study published in the American Journal of Preventive Medicine showed that in the United States, those states which have implemented a higher minimum wage saw a decline in the growth of suicide rates. The researchers say that for every one dollar increase, the annual suicide growth rate fell by 1.9%. The study covers all 50 states for the years 2006 to 2016.

According to a 1978 article in the "American Economic Review", 90% of the economists surveyed agreed that the minimum wage increases unemployment among low-skilled workers. By 1992 the survey found 79% of economists in agreement with that statement, and by 2000, 46% were in full agreement with the statement and 28% agreed with provisos (74% total). The authors of the 2000 study also reweighted data from a 1990 sample to show that at that time 62% of academic economists agreed with the statement above, while 20% agreed with provisos and 18% disagreed. They state that the reduction on consensus on this question is "likely" due to the Card and Krueger research and subsequent debate.

A similar survey in 2006 by Robert Whaples polled PhD members of the American Economic Association (AEA). Whaples found that 47% respondents wanted the minimum wage eliminated, 38% supported an increase, 14% wanted it kept at the current level, and 1% wanted it decreased. Another survey in 2007 conducted by the University of New Hampshire Survey Center found that 73% of labor economists surveyed in the United States believed 150% of the then-current minimum wage would result in employment losses and 68% believed a mandated minimum wage would cause an increase in hiring of workers with greater skills. 31% felt that no hiring changes would result.

Surveys of labor economists have found a sharp split on the minimum wage. Fuchs et al. (1998) polled labor economists at the top 40 research universities in the United States on a variety of questions in the summer of 1996. Their 65 respondents were nearly evenly divided when asked if the minimum wage should be increased. They argued that the different policy views were not related to views on whether raising the minimum wage would reduce teen employment (the median economist said there would be a reduction of 1%), but on value differences such as income redistribution. Daniel B. Klein and Stewart Dompe conclude, on the basis of previous surveys, "the average level of support for the minimum wage is somewhat higher among labor economists than among AEA members."

In 2007, Klein and Dompe conducted a non-anonymous survey of supporters of the minimum wage who had signed the "Raise the Minimum Wage" statement published by the Economic Policy Institute. 95 of the 605 signatories responded. They found that a majority signed on the grounds that it transferred income from employers to workers, or equalized bargaining power between them in the labor market. In addition, a majority considered disemployment to be a moderate potential drawback to the increase they supported.

In 2013, a diverse group of 37 economics professors was surveyed on their view of the minimum wage's impact on employment. 34% of respondents agreed with the statement, "Raising the federal minimum wage to $9 per hour would make it noticeably harder for low-skilled workers to find employment." 32% disagreed and the remaining respondents were uncertain or had no opinion on the question. 47% agreed with the statement, "The distortionary costs of raising the federal minimum wage to $9 per hour and indexing it to inflation are sufficiently small compared with the benefits to low-skilled workers who can find employment that this would be a desirable policy", while 11% disagreed.

Economists and other political commentators have proposed alternatives to the minimum wage. They argue that these alternatives may address the issue of poverty better than a minimum wage, as it would benefit a broader population of low wage earners, not cause any unemployment, and distribute the costs widely rather than concentrating it on employers of low wage workers.

A basic income (or negative income tax - NIT) is a system of social security that periodically provides each citizen with a sum of money that is sufficient to live on frugally. Supporters of the basic-income idea argue that recipients of the basic income would have considerably more bargaining power when negotiating a wage with an employer, as there would be no risk of destitution for not taking the employment. As a result, jobseekers could spend more time looking for a more appropriate or satisfying job, or they could wait until a higher-paying job appeared. Alternatively, they could spend more time increasing their skills (via education and training), which would make them more suitable for higher-paying jobs, as well as provide numerous other benefits. Experiments on Basic Income and NIT in Canada and the USA show that people spent more time studying while the program was running.

Proponents argue that a basic income that is based on a broad tax base would be more economically efficient than a minimum wage, as the minimum wage effectively imposes a high marginal tax on employers, causing losses in efficiency.

A guaranteed minimum income is another proposed system of social welfare provision. It is similar to a basic income or negative income tax system, except that it is normally conditional and subject to a means test. Some proposals also stipulate a willingness to participate in the labor market, or a willingness to perform community services.

A refundable tax credit is a mechanism whereby the tax system can reduce the tax owed by a household to below zero, and result in a net payment to the taxpayer beyond their own payments into the tax system. Examples of refundable tax credits include the earned income tax credit and the additional child tax credit in the US, and working tax credits and child tax credits in the UK. Such a system is slightly different from a negative income tax, in that the refundable tax credit is usually only paid to households that have earned at least some income. This policy is more targeted against poverty than the minimum wage, because it avoids subsidizing low-income workers who are supported by high-income households (for example, teenagers still living with their parents).

In the United States, earned income tax credit rates, also known as EITC or EIC, vary by state—some are refundable while other states do not allow a refundable tax credit. The federal EITC program has been expanded by a number of presidents including Jimmy Carter, Ronald Reagan, George H.W. Bush, and Bill Clinton. In 1986, President Reagan described the EITC as "the best anti poverty, the best pro-family, the best job creation measure to come out of Congress." The ability of the earned income tax credit to deliver larger monetary benefits to the poor workers than an increase in the minimum wage and at a lower cost to society was documented in a 2007 report by the Congressional Budget Office.

The Adam Smith Institute prefers cutting taxes on the poor and middle class instead of raising wages as an alternative to the minimum wage.

Italy, Sweden, Norway, Finland, and Denmark are examples of developed nations where there is no minimum wage that is required by legislation. Such nations, particularly the Nordics, have very high union participation rates. Instead, minimum wage standards in different sectors are set by collective bargaining.

Some economists such as Scott Sumner and Edmund Phelps advocate a wage subsidy program. A wage subsidy is a payment made by a government for work people do. It is based either on an hourly basis or by income earned. Advocates argue that the primary deficiencies of the EITC and the minimum wage are best avoided by a wage subsidy. However, the wage subsidy in the United States suffers from a lack of political support from either major political party.

Providing education or funding apprenticeships or technical training can provide a bridge for low skilled workers to move into wages above a minimum wage. For example, Germany has adopted a state funded apprenticeship program that combines on-the-job and classroom training. Having more skills makes workers more valuable and more productive, but having a high minimum wage for low-skill jobs reduces the incentive to seek education and training. Moving some workers to higher-paying jobs will decrease the supply of workers willing to accept low-skill jobs, increasing the market wage for those low skilled jobs (assuming a stable labor market). However, in that solution the wage will still not increase above the marginal return for the role and will likely promote automation or business closure.

In January 2014, seven Nobel economists—Kenneth Arrow, Peter Diamond, Eric Maskin, Thomas Schelling, Robert Solow, Michael Spence, and Joseph Stiglitz—and 600 other economists wrote a letter to the US Congress and the US President urging that, by 2016, the US government should raise the minimum wage to $10.10. They endorsed the Minimum Wage Fairness Act which was introduced by US Senator Tom Harkin in 2013. U.S. Senator Bernie Sanders introduced a bill in 2015 that would raise the minimum wage to $15, and in his 2016 campaign for president ran on a platform of increasing it. Although Sanders did not become the nominee, the Democratic National Committee adopted his $15 minimum wage push in their 2016 party platform.

Reactions from former McDonald's USA Ed Rensi about raising minimum wage to $15 is to completely push humans out of the picture when it comes to labor if they are to pay minimum wage at $15 they would look into replacing humans with machines as that would be the more cost-effective than having employees that are ineffective. During an interview on FOX Business Network’s Mornings with Maria, he stated that he believes an increase to $15 an hour would cause job loss at an extraordinary level. Rensi also believes it does not only affect the fast food industry, franchising he sees as the best business model in the United States, it is dependent on people that have low job skills that have to grow and if you cannot pay them a reasonable wage then they are going to be replaced with machines.

In late March 2016, Governor of California Jerry Brown reached a deal to raise the minimum wage to $15 by 2022 for big businesses and 2023 for smaller businesses.

In contrast, the relatively high minimum wage in Puerto Rico has been blamed by various politicians and commentators as a highly significant factor in the Puerto Rican government-debt crisis. One study concluded that "Employers are disinclined to hire workers because the US federal minimum wage is very high relative to the local average".

, unions were exempt from recent minimum wage increases in Chicago, Illinois, SeaTac, Washington, and Milwaukee County, Wisconsin, as well as the California cities of Los Angeles, San Francisco, Long Beach, San Jose, Richmond, and Oakland.

Scholars found in 2019 that, in America, "Between 1990 and 2015, raising the minimum wage by $1 in each state might have saved more than 27,000 lives, according to a report published this week in the "Journal of Epidemiology & Community Health." An increase of $2 in each state's minimum wage could have prevented more than 57,000 suicides."  The researchers stated, "The effect of a US$1 increase in the minimum wage ranged from a 3.4% decrease (95% CI 0.4 to 6.4) to a 5.9% decrease (95% CI 1.4 to 10.2) in the suicide rate among adults aged 18–64 years with a high school education or less. We detected significant effect modification by unemployment rate, with the largest effects of minimum wage on reducing suicides observed at higher unemployment levels." They concluded, "Minimum wage increases appear to reduce the suicide rate among those with a high school education or less, and may reduce disparities between socioeconomic groups. Effects appear greatest during periods of high unemployment."




</doc>
<doc id="18864" url="https://en.wikipedia.org/wiki?curid=18864" title="Mullet">
Mullet

Mullet may refer to:







</doc>
